You are evaluating a system's answer to a technical question about a codebase.

Question: {question}

Ground Truth Answer: {ground_truth}

Core Facts: {facts}

System Answer (DeepWiki): {deepwiki_answer}

Evaluate the system answer along these three dimensions:

1. FACTUAL CORRECTNESS (0-10)
   Does the answer contain accurate information? Are there any factual errors or hallucinations?
   - 9-10: All statements are factually correct
   - 7-8: Mostly correct with minor inaccuracies
   - 4-6: Mix of correct and incorrect information
   - 1-3: Primarily incorrect or hallucinated information
   - 0: Completely wrong or nonsensical

2. FACT COVERAGE (0-10)
   Of the core facts that are RELEVANT to answering this specific question, how many does the system answer include?
   - First, identify which core facts are pertinent to this question
   - Then, score based on coverage of those pertinent facts
   - 9-10: Covers all pertinent facts
   - 7-8: Covers most pertinent facts, minor omissions
   - 4-6: Covers some pertinent facts, significant gaps
   - 1-3: Misses most pertinent facts
   - 0: Covers none of the pertinent facts

3. SPECIFICITY (0-10)
   Does the answer provide concrete code references (file paths, function names, class names, line numbers) when appropriate?
   - 9-10: Provides specific, accurate code references throughout (e.g., "src/auth/login.py:45", "authenticate_user()")
   - 7-8: Good specificity but missing some details or locations
   - 4-6: Mix of specific and vague references (e.g., "in the auth module")
   - 1-3: Mostly vague descriptions without concrete references
   - 0: No code references at all, purely abstract
   - N/A: Question doesn't require code references (e.g., high-level design questions)

IMPORTANT: Think step-by-step before scoring:
- First, list which core facts are actually relevant to this question
- Then analyze what the system got right and wrong
- Consider whether the question requires code-level specificity
- Finally, assign scores

Respond in JSON format:
{{
  "reasoning": {{
    "pertinent_facts": ["list", "of", "facts", "relevant", "to", "question"],
    "facts_found": ["facts", "the", "system", "covered"],
    "facts_missing": ["pertinent", "facts", "not", "covered"],
    "errors_or_hallucinations": ["any", "incorrect", "statements"],
    "code_references_provided": ["list", "of", "files/functions", "mentioned"],
    "specificity_note": "<brief comment on level of detail>"
  }},
  "scores": {{
    "factual_correctness": <0-10>,
    "fact_coverage": <0-10>,
    "specificity": <0-10 or "N/A">
  }},
  "summary": "<2-3 sentence summary of the evaluation>"
}}