You are helping to create ground-truth facts for a codebase question-answering evaluation dataset. Given a question and its answer about a codebase, extract the core factual statements that represent the key information needed to answer the question.

# Guidelines

- Extract 2-5 atomic, verifiable facts from the answer
- Each fact should be a single, self-contained statement
- Facts should be specific and grounded in the code (mention specific classes, methods, parameters)
- Facts should be independent - each can be verified separately
- Focus on facts that are essential to answering the question, not peripheral details
- Use precise technical language and include specific identifiers (class names, method names, etc.)
- Keep facts concise - typically 1-2 sentences maximum

# Examples

**Question**: "How does the cache system coordinate state management between encoder and decoder components?"

**Answer**: "The EncoderDecoderCache contains two separate DynamicCache instances - one for self_attention_cache and one for cross_attention_cache. The BltModel.forward() method extracts these caches and passes them to the respective components."

**Facts**:
1. EncoderDecoderCache contains two separate DynamicCache instances
2. One DynamicCache is used for self_attention_cache
3. Another DynamicCache is used for cross_attention_cache
4. BltModel.forward() extracts these caches from the EncoderDecoderCache
5. The extracted caches are passed to respective encoder/decoder components

---

**Question**: "What mechanism ensures that attention computations have access to historical key-value pairs?"

**Answer**: "The past_key_values.update() method concatenates new key-value states with previously cached ones and returns the full accumulated tensors. This ensures attention is computed over the full sequence history."

**Facts**:
1. The past_key_values.update() method concatenates new key-value states with cached ones
2. The update() method returns full accumulated key-value tensors
3. These accumulated tensors represent the full sequence history
4. Attention computations use these accumulated tensors to attend over full context

---

# Task

Given the question and answer below, extract the core facts.

**Question**: {question}

**Answer**: {answer}

**Key Files**: {key_files}

Return your response as JSON:
```json
{{
  "facts": [
    "fact 1",
    "fact 2",
    "fact 3"
  ]
}}
```
