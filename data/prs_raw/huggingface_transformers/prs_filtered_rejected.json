[
  {
    "pr_number": 41923,
    "title": "fix some ut failures on XPU w/ torch 2.9",
    "body": "cases are below, all passed. @ydshieh , pls help review, thx very much.\r\n\r\n> tests/models/aya_vision/test_modeling_aya_vision.py::AyaVisionIntegrationTest::test_small_model_integration_generate_text_only\r\n> tests/models/aya_vision/test_modeling_aya_vision.py::AyaVisionIntegrationTest::test_small_model_integration_forward\r\n> tests/models/aya_vision/test_modeling_aya_vision.py::AyaVisionIntegrationTest::test_small_model_integration_batched_generate_multi_image\r\n> tests/pipelines/test_pipelines_automatic_speech_recognition.py::AutomaticSpeechRecognitionPipelineTests::test_whisper_longform\r\n> tests/test_pipeline_mixin.py::AutomaticSpeechRecognitionPipelineTests::test_whisper_longform\r\n> tests/models/aria/test_modeling_aria.py::AriaForConditionalGenerationIntegrationTest::test_generation_no_images\r\n\r\n> tests/models/gemma3/test_modeling_gemma3.py::Gemma3IntegrationTest::test_model_4b_bf16\r\n> tests/models/gemma3/test_modeling_gemma3.py::Gemma3IntegrationTest::test_model_4b_crops\r\n> tests/models/glm4v/test_modeling_glm4v.py::Glm4vIntegrationTest::test_small_model_integration_test_expand\r\n> tests/models/mistral3/test_modeling_mistral3.py::Mistral3IntegrationTest::test_mistral3_integration_generate\r\n> tests/models/mllama/test_modeling_mllama.py::MllamaForConditionalGenerationIntegrationTest::test_11b_model_integration_generate_text_only",
    "html_url": "https://github.com/huggingface/transformers/pull/41923",
    "created_at": "2025-10-28T21:37:42Z",
    "merged_at": "2025-10-29T15:15:34Z",
    "merge_commit_sha": "a43b36cf802f00616800e0bd4d748679236123ee",
    "base_ref": "main",
    "head_sha": "ced44924dde51f50cc71b6c771ab311818217658",
    "user": "yao-matrix",
    "files": [
      {
        "filename": "tests/models/aria/test_modeling_aria.py",
        "status": "modified",
        "additions": 0,
        "deletions": 1,
        "changes": 1,
        "patch": "@@ -520,7 +520,6 @@ def test_generation_no_images(self):\n             quantization_config=BitsAndBytesConfig(load_in_4bit=True, llm_int8_skip_modules=[\"multihead_attn\"]),\n         )\n         processor = AutoProcessor.from_pretrained(model_id)\n-        assert model.device.type == \"cuda\", \"This test is only supported on CUDA\"  # TODO: remove this\n         # Prepare inputs with no images\n         inputs = processor(text=\"Hello, I am\", return_tensors=\"pt\").to(torch_device)\n "
      },
      {
        "filename": "tests/models/aya_vision/test_modeling_aya_vision.py",
        "status": "modified",
        "additions": 3,
        "deletions": 3,
        "changes": 6,
        "patch": "@@ -267,7 +267,7 @@ def test_small_model_integration_forward(self):\n \n         EXPECTED_LOGITS = Expectations(\n             {\n-                (\"xpu\", 3): [0.4109, 0.1532, 0.8018, 2.1328, 0.5483],\n+                (\"xpu\", 3): [1.6699, 0.6260, 3.2266, 8.5547, 2.209],\n                 # 4-bit\n                 (\"cuda\", 7): [0.1097, 0.3481, 3.8340, 9.7969, 2.0488],\n                 (\"cuda\", 8): [1.6396, 0.6094, 3.1992, 8.5234, 2.1875],\n@@ -308,7 +308,7 @@ def test_small_model_integration_generate_text_only(self):\n \n         expected_outputs = Expectations(\n             {\n-                (\"xpu\", 3): \"Whispers on the breeze,\\nLeaves dance under moonlit skies,\\nNature's quiet song.\",\n+                (\"xpu\", 3): \"Whispers on the breeze,\\nLeaves dance under moonlit sky,\\nNature's quiet song.\",\n                 # 4-bit\n                 (\"cuda\", 7): \"Sure, here's a haiku for you:\\n\\nMorning dew sparkles,\\nPetals unfold in sunlight,\\n\",\n                 (\"cuda\", 8): \"Whispers on the breeze,\\nLeaves dance under moonlit skies,\\nNature's quiet song.\",\n@@ -474,7 +474,7 @@ def test_small_model_integration_batched_generate_multi_image(self):\n         # Batching seems to alter the output slightly, but it is also the case in the original implementation. This seems to be expected: https://github.com/huggingface/transformers/issues/23017#issuecomment-1649630232\n         expected_outputs = Expectations(\n             {\n-                (\"xpu\", 3): \"Wooden path to water,\\nMountains echo in stillness,\\nPeaceful forest lake.\",\n+                (\"xpu\", 3): \"Wooden path to water,\\nMountains echo in stillness,\\nPeaceful forest scene.\",\n                 (\"cuda\", 7): 'Wooden bridge stretches\\nMirrored lake below, mountains rise\\nPeaceful, serene',\n                 (\"cuda\", 8): 'Wooden path to water,\\nMountains echo in stillness,\\nPeaceful forest scene.',\n             }"
      },
      {
        "filename": "tests/models/gemma3/test_modeling_gemma3.py",
        "status": "modified",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "patch": "@@ -499,7 +499,7 @@ def test_model_4b_bf16(self):\n \n         EXPECTED_TEXTS = Expectations(\n             {\n-                (\"xpu\", 3): ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nCertainly! \\n\\nThe image shows a brown and white cow standing on a sandy beach with turquoise water in the background. It looks like a lovely,'],\n+                (\"xpu\", 3): ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nCertainly! \\n\\nThe image shows a brown cow standing on a sandy beach with turquoise water and a blue sky in the background. It looks like a'],\n                 (\"cuda\", (8, 0)): ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nCertainly! \\n\\nThe image shows a brown cow standing on a sandy beach with clear turquoise water and a blue sky in the background. It looks like'],\n                 (\"cuda\", (8, 6)): ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nCertainly! \\n\\nThe image shows a brown cow standing on a sandy beach with clear blue water and a blue sky in the background. It looks like'],\n                 (\"rocm\", (9, 4)): ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nCertainly! \\n\\nThe image shows a brown cow standing on a sandy beach with turquoise water and a blue sky in the background. It looks like a'],\n@@ -610,7 +610,7 @@ def test_model_4b_crops(self):\n         EXPECTED_NUM_IMAGES = 3  # one for the origin image and two crops of images\n         EXPECTED_TEXTS = Expectations(\n             {\n-                (\"xpu\", 3): ['user\\nYou are a helpful assistant.\\n\\nHere is the original image \\n\\n\\n\\n and here are some crops to help you see better \\n\\n\\n\\n \\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nThe image shows a brown cow standing on a sandy beach next to a turquoise ocean. There are clouds in the blue sky above.'],\n+                (\"xpu\", 3): [\"user\\nYou are a helpful assistant.\\n\\nHere is the original image \\n\\n\\n\\n and here are some crops to help you see better \\n\\n\\n\\n \\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nThe image shows a brown cow standing on a sandy beach next to a turquoise ocean. There's a bright blue sky with some white clouds in the\"],\n                 (\"cuda\", 7): [],\n                 (\"cuda\", (8, 6)): [\"user\\nYou are a helpful assistant.\\n\\nHere is the original image \\n\\n\\n\\n and here are some crops to help you see better \\n\\n\\n\\n \\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nThe image shows a brown cow standing on a sandy beach next to a turquoise ocean. There's a clear blue sky with some white clouds above.\"],\n                 (\"cuda\", (8, 0)): [\"user\\nYou are a helpful assistant.\\n\\nHere is the original image \\n\\n\\n\\n and here are some crops to help you see better \\n\\n\\n\\n \\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nThe image shows a brown cow standing on a sandy beach next to a turquoise ocean. There's a blue sky with some white clouds in the background\"],"
      },
      {
        "filename": "tests/models/glm4v/test_modeling_glm4v.py",
        "status": "modified",
        "additions": 19,
        "deletions": 7,
        "changes": 26,
        "patch": "@@ -24,7 +24,9 @@\n     is_torch_available,\n )\n from transformers.testing_utils import (\n+    Expectations,\n     cleanup,\n+    require_deterministic_for_xpu,\n     require_flash_attn,\n     require_torch,\n     require_torch_gpu,\n@@ -413,6 +415,7 @@ def test_small_model_integration_test_with_video(self):\n         )\n \n     @slow\n+    @require_deterministic_for_xpu\n     def test_small_model_integration_test_expand(self):\n         model = Glm4vForConditionalGeneration.from_pretrained(\n             \"THUDM/GLM-4.1V-9B-Thinking\", dtype=\"auto\", device_map=\"auto\"\n@@ -426,14 +429,23 @@ def test_small_model_integration_test_expand(self):\n \n         output = model.generate(**inputs, max_new_tokens=30, do_sample=False, num_beams=2, num_return_sequences=2)\n \n-        EXPECTED_DECODED_TEXT = [\n-            \"\\nWhat kind of dog is this?\\n<think>Got it, let's look at the image. The animal in the picture doesn't look like a dog; it's actually a cat. Specifically\",\n-            \"\\nWhat kind of dog is this?\\n<think>Got it, let's look at the image. The animal in the picture doesn't look like a dog; it's actually a cat, specifically\"\n-        ]  # fmt: skip\n-        self.assertEqual(\n-            self.processor.batch_decode(output, skip_special_tokens=True),\n-            EXPECTED_DECODED_TEXT,\n+        # fmt: off\n+        EXPECTED_DECODED_TEXTS = Expectations(\n+            {\n+\n+                (None, None): [\"\\nWhat kind of dog is this?\\n<think>Got it, let's look at the image. The animal in the picture doesn't look like a dog; it's actually a cat. Specifically\",\n+                               \"\\nWhat kind of dog is this?\\n<think>Got it, let's look at the image. The animal in the picture doesn't look like a dog; it's actually a cat, specifically\"\n+                              ],\n+                (\"xpu\", None): [\"\\nWhat kind of dog is this?\\n<think>Got it, let's look at the image. The animal in the picture is not a dog; it's a cat. Specifically, it looks\",\n+                                \"\\nWhat kind of dog is this?\\n<think>Got it, let's look at the image. The animal in the picture is not a dog; it's a cat, specifically a Pallas\"\n+                               ],\n+            }\n         )\n+        # fmt: on\n+        EXPECTED_DECODED_TEXT = EXPECTED_DECODED_TEXTS.get_expectation()\n+\n+        decoded_text = self.processor.batch_decode(output, skip_special_tokens=True)\n+        self.assertEqual(decoded_text, EXPECTED_DECODED_TEXT)\n \n     @slow\n     def test_small_model_integration_test_batch_wo_image(self):"
      },
      {
        "filename": "tests/models/mistral3/test_modeling_mistral3.py",
        "status": "modified",
        "additions": 2,
        "deletions": 1,
        "changes": 3,
        "patch": "@@ -275,6 +275,7 @@ def test_mistral3_integration_generate_text_only(self):\n         self.assertEqual(decoded_output, expected_output)\n \n     @require_read_token\n+    @require_deterministic_for_xpu\n     def test_mistral3_integration_generate(self):\n         processor = AutoProcessor.from_pretrained(self.model_checkpoint)\n         processor.chat_template = processor.chat_template.replace('strftime_now(\"%Y-%m-%d\")', '\"2025-06-20\"')\n@@ -299,7 +300,7 @@ def test_mistral3_integration_generate(self):\n \n         expected_outputs = Expectations(\n             {\n-                (\"xpu\", 3): \"The image features two cats resting on a pink blanket. The cat on the left is a kitten\",\n+                (\"xpu\", 3): \"The image features two tabby cats lying on a pink surface, which appears to be a cushion or\",\n                 (\"cuda\", 8): 'The image features two cats lying on a pink surface, which appears to be a couch or a bed',\n                 (\"rocm\", (9, 4)): \"The image features two cats lying on a pink surface, which appears to be a couch or a bed\",\n                 (\"rocm\", (9, 5)): \"The image features two tabby cats lying on a pink surface, which appears to be a cushion or\""
      },
      {
        "filename": "tests/models/mllama/test_modeling_mllama.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -547,7 +547,7 @@ def test_11b_model_integration_generate_text_only(self):\n         decoded_output = processor.decode(output[0], skip_special_tokens=True)\n         expected_outputs = Expectations(\n                 {\n-                    (\"xpu\", 3): \"If I had to write a haiku about my life, I would write:\\nLife is a messy tapestry\\n Threads of joy and sorrow\\nWeft of memories\",\n+                    (\"xpu\", 3): \"If I had to write a haiku about my life, I would write:\\nLife is a messy stream\\nRipples of joy and pain\\nFlowing, ever\",\n                     (\"cuda\", 7): \"If I had to write a haiku about my life, I would write:\\nLife is a messy stream\\nRipples of joy and pain\\nFlowing, ever\",\n                     (\"cuda\", 8): \"If I had to write a haiku about my life, I would write:\\nLife is a messy stream\\nRipples of joy and pain\\nFlowing, ever\",\n                 }"
      },
      {
        "filename": "tests/pipelines/test_pipelines_automatic_speech_recognition.py",
        "status": "modified",
        "additions": 8,
        "deletions": 1,
        "changes": 9,
        "patch": "@@ -35,6 +35,7 @@\n from transformers.pipelines.audio_utils import chunk_bytes_iter, ffmpeg_microphone_live\n from transformers.pipelines.automatic_speech_recognition import chunk_iter\n from transformers.testing_utils import (\n+    Expectations,\n     compare_pipeline_output_to_hub_spec,\n     is_pipeline_test,\n     is_torch_available,\n@@ -1443,8 +1444,14 @@ def test_whisper_prompted(self):\n     @slow\n     def test_whisper_longform(self):\n         # fmt: off\n-        EXPECTED_RESULT = \" Folks, if you watch the show, you know, I spent a lot of time right over there. Patiently and astutely scrutinizing the boxwood and mahogany chest set of the day's biggest stories developing the central headline pawns, definitely maneuvering an oso topical night to F6, fainting a classic Sicilian, nade door variation on the news, all the while seeing eight moves deep and patiently marshalling the latest press releases into a fisher's shows in Lip Nitsky attack that culminates in the elegant lethal slow-played, all-passant checkmate that is my nightly monologue. But sometimes, sometimes, folks, I. CHEERING AND APPLAUSE Sometimes I startle away, cubside down in the monkey bars of a condemned playground on a super fun site. Get all hept up on goofballs. Rummage that were discarded tag bag of defective toys. Yank out a fist bowl of disembodied doll limbs, toss them on Saturday, Rusty Cargo, container down by the Wharf, and challenge toothless drifters to the godless bughouse lets of tournament that is my segment. MUSIC Meanwhile!\"\n+        EXPECTED_RESULTS = Expectations(\n+            {\n+                (None, None): \" Folks, if you watch the show, you know, I spent a lot of time right over there. Patiently and astutely scrutinizing the boxwood and mahogany chest set of the day's biggest stories developing the central headline pawns, definitely maneuvering an oso topical night to F6, fainting a classic Sicilian, nade door variation on the news, all the while seeing eight moves deep and patiently marshalling the latest press releases into a fisher's shows in Lip Nitsky attack that culminates in the elegant lethal slow-played, all-passant checkmate that is my nightly monologue. But sometimes, sometimes, folks, I. CHEERING AND APPLAUSE Sometimes I startle away, cubside down in the monkey bars of a condemned playground on a super fun site. Get all hept up on goofballs. Rummage that were discarded tag bag of defective toys. Yank out a fist bowl of disembodied doll limbs, toss them on Saturday, Rusty Cargo, container down by the Wharf, and challenge toothless drifters to the godless bughouse lets of tournament that is my segment. MUSIC Meanwhile!\",\n+                (\"xpu\", None): \" Folks, if you watch the show, you know, I spent a lot of time right over there. Patiently and astutely scrutinizing the boxwood and mahogany chest set of the day's biggest stories developing the central headline pawns, definitely maneuvering an oso topical night to F6, fainting of classics, Sicilian, nade door variation on the news, all the while seeing eight moves deep and patiently marshalling the latest press releases into a Fisher shows in Lip Nitsky attack that culminates in the elegant lethal slow-played, all-passant checkmate that is my nightly monologue. But sometimes, sometimes, folks, I... APPLAUSE Sometimes I... Startle away, upside down on the monkey bars of a condemned playground on a superfund site. Get all heaped up on goofballs, rummaged that would discard a tag bag of defective toys, yank out a fist bowl of disembodied doll limbs, toss them on a stain kid's place mat from a defunct denys, set up a table inside a rusty cargo container down by the Wharf and challenge toothless drifters to the godless bug house blitz of tournament that is my segment.\",\n+            }\n+        )\n         # fmt: on\n+        EXPECTED_RESULT = EXPECTED_RESULTS.get_expectation()\n \n         processor = AutoProcessor.from_pretrained(\"openai/whisper-tiny.en\")\n         model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny.en\")"
      }
    ],
    "num_files": 7,
    "scraped_at": "2025-11-16T21:16:32.226620",
    "filter_decision": {
      "accept": false,
      "reasoning": "This PR is purely a test update that adjusts expected output values and adds device-specific test expectations for XPU compatibility with PyTorch 2.9. There are no logic changes, algorithm modifications, or architectural decisions\u2014only updates to hardcoded expected values in test assertions and removal of a TODO comment. This is a configuration/maintenance update that would not help developers understand the codebase's functionality.",
      "substance_level": "low"
    }
  },
  {
    "pr_number": 41892,
    "title": "Update some workflow files",
    "body": "# What does this PR do?\r\n\r\nMostly:\r\n\r\n- Make `docker/transformers-all-latest-gpu/Dockerfile` more readable and clean as we now need to handle `torchcodec` (using `cpu`) along with `torch` (`cuda`)\r\n\r\n- Remove `push-ci` stuff. We are not paying any attention to it. We have something running on a very small subset now.\r\n- Separate CI workflows and their docker images: with `flash-attn` and without it\r\n\r\n",
    "html_url": "https://github.com/huggingface/transformers/pull/41892",
    "created_at": "2025-10-27T12:26:34Z",
    "merged_at": "2025-10-29T13:42:05Z",
    "merge_commit_sha": "10d557123b42236dabfb70d40cf3d9ef57a445d0",
    "base_ref": "main",
    "head_sha": "7677b10bf70765721881edd2edd4b71739924645",
    "user": "ydshieh",
    "files": [
      {
        "filename": ".github/workflows/benchmark.yml",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -28,7 +28,7 @@ jobs:\n       (github.event_name == 'pull_request' && contains( github.event.pull_request.labels.*.name, 'run-benchmark') )||\r\n       (github.event_name == 'push' && github.ref == 'refs/heads/main')\r\n     container:\r\n-      image: huggingface/transformers-pytorch-gpu\r\n+      image: huggingface/transformers-all-latest-gpu\r\n       options: --gpus all --privileged --ipc host\r\n     steps:\r\n       - name: Get repo\r"
      },
      {
        "filename": ".github/workflows/benchmark_v2_a10_caller.yml",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -9,7 +9,7 @@ jobs:\n     uses: ./.github/workflows/benchmark_v2.yml\n     with:\n       runner: aws-g5-4xlarge-cache-use1-public-80\n-      container_image: huggingface/transformers-pytorch-gpu\n+      container_image: huggingface/transformers-all-latest-gpu\n       container_options: --gpus all --privileged --ipc host --shm-size \"16gb\"\n       commit_sha: ${{ github.sha }}\n       run_id: ${{ github.run_id }}"
      },
      {
        "filename": ".github/workflows/build-docker-images.yml",
        "status": "modified",
        "additions": 20,
        "deletions": 103,
        "changes": 123,
        "patch": "@@ -45,33 +45,20 @@ jobs:\n             REF=main\n           push: true\n           tags: huggingface/transformers-all-latest-gpu${{ inputs.image_postfix }}\n-      # Push CI images still need to be re-built daily\n-      -\n-        name: Build and push (for Push CI) in a daily basis\n-        # This condition allows `schedule` events, or `push` events that trigger this workflow NOT via `workflow_call`.\n-        # The later case is useful for manual image building for debugging purpose. Use another tag in this case!\n-        if: inputs.image_postfix != '-push-ci'\n-        uses: docker/build-push-action@v5\n-        with:\n-          context: ./docker/transformers-all-latest-gpu\n-          build-args: |\n-            REF=main\n-          push: true\n-          tags: huggingface/transformers-all-latest-gpu-push-ci\n \n       - name: Post to Slack\n         if: always()\n         uses: huggingface/hf-workflows/.github/actions/post-slack@main\n         with:\n           slack_channel: ${{ secrets.CI_SLACK_CHANNEL_DOCKER }}\n-          title: \ud83e\udd17 Results of the transformers-all-latest-gpu-push-ci docker build\n+          title: \ud83e\udd17 Results of the transformers-all-latest-gpu docker build\n           status: ${{ job.status }}\n           slack_token: ${{ secrets.SLACK_CIFEEDBACK_BOT_TOKEN }}\n \n-  latest-torch-deepspeed-docker:\n-    name: \"Latest PyTorch + DeepSpeed\"\n+  flash-attn-ci-image:\n+    name: \"PyTorch with Flash Attn [dev]\"\n     runs-on:\n-      group: aws-g4dn-2xlarge-cache\n+      group: aws-general-8-plus\n     steps:\n       -\n         name: Set up Docker Buildx\n@@ -89,26 +76,28 @@ jobs:\n         name: Build and push\n         uses: docker/build-push-action@v5\n         with:\n-          context: ./docker/transformers-pytorch-deepspeed-latest-gpu\n+          context: ./docker/transformers-all-latest-gpu\n           build-args: |\n-            REF=main\n+            REF=update_dockerfile\n+            PYTORCH=2.8.0\n+            TORCHCODEC=0.7.0\n+            FLASH_ATTN=yes\n           push: true\n-          tags: huggingface/transformers-pytorch-deepspeed-latest-gpu${{ inputs.image_postfix }}\n+          tags: huggingface/transformers-all-latest-gpu${{ inputs.image_postfix }}:flash-attn\n \n       - name: Post to Slack\n         if: always()\n         uses: huggingface/hf-workflows/.github/actions/post-slack@main\n         with:\n-          slack_channel: ${{ secrets.CI_SLACK_CHANNEL_DOCKER}}\n-          title: \ud83e\udd17 Results of the transformers-pytorch-deepspeed-latest-gpu docker build\n+          slack_channel: ${{ secrets.CI_SLACK_CHANNEL_DOCKER }}\n+          title: \ud83e\udd17 Results of the transformers-all-latest-gpu docker build\n           status: ${{ job.status }}\n           slack_token: ${{ secrets.SLACK_CIFEEDBACK_BOT_TOKEN }}\n \n-  # Can't build 2 images in a single job `latest-torch-deepspeed-docker` (for `nvcr.io/nvidia`)\n-  latest-torch-deepspeed-docker-for-push-ci-daily-build:\n-    name: \"Latest PyTorch + DeepSpeed (Push CI - Daily Build)\"\n+  latest-torch-deepspeed-docker:\n+    name: \"Latest PyTorch + DeepSpeed\"\n     runs-on:\n-      group: aws-general-8-plus\n+      group: aws-g4dn-2xlarge-cache\n     steps:\n       -\n         name: Set up Docker Buildx\n@@ -122,33 +111,27 @@ jobs:\n         with:\n           username: ${{ secrets.DOCKERHUB_USERNAME }}\n           password: ${{ secrets.DOCKERHUB_PASSWORD }}\n-      # Push CI images still need to be re-built daily\n       -\n-        name: Build and push (for Push CI) in a daily basis\n-        # This condition allows `schedule` events, or `push` events that trigger this workflow NOT via `workflow_call`.\n-        # The later case is useful for manual image building for debugging purpose. Use another tag in this case!\n-        if: inputs.image_postfix != '-push-ci'\n+        name: Build and push\n         uses: docker/build-push-action@v5\n         with:\n           context: ./docker/transformers-pytorch-deepspeed-latest-gpu\n           build-args: |\n             REF=main\n           push: true\n-          tags: huggingface/transformers-pytorch-deepspeed-latest-gpu-push-ci\n+          tags: huggingface/transformers-pytorch-deepspeed-latest-gpu${{ inputs.image_postfix }}\n \n       - name: Post to Slack\n         if: always()\n         uses: huggingface/hf-workflows/.github/actions/post-slack@main\n         with:\n-          slack_channel: ${{ secrets.CI_SLACK_CHANNEL_DOCKER }}\n-          title: \ud83e\udd17 Results of the transformers-pytorch-deepspeed-latest-gpu-push-ci docker build\n+          slack_channel: ${{ secrets.CI_SLACK_CHANNEL_DOCKER}}\n+          title: \ud83e\udd17 Results of the transformers-pytorch-deepspeed-latest-gpu docker build\n           status: ${{ job.status }}\n           slack_token: ${{ secrets.SLACK_CIFEEDBACK_BOT_TOKEN }}\n \n   doc-builder:\n     name: \"Doc builder\"\n-    # Push CI doesn't need this image\n-    if: inputs.image_postfix != '-push-ci'\n     runs-on:\n       group: aws-general-8-plus\n     steps:\n@@ -181,44 +164,6 @@ jobs:\n           status: ${{ job.status }}\n           slack_token: ${{ secrets.SLACK_CIFEEDBACK_BOT_TOKEN }}\n \n-  latest-pytorch:\n-    name: \"Latest PyTorch [dev]\"\n-    # Push CI doesn't need this image\n-    if: inputs.image_postfix != '-push-ci'\n-    runs-on:\n-      group: aws-general-8-plus\n-    steps:\n-      -\n-        name: Set up Docker Buildx\n-        uses: docker/setup-buildx-action@v3\n-      -\n-        name: Check out code\n-        uses: actions/checkout@v4\n-      -\n-        name: Login to DockerHub\n-        uses: docker/login-action@v3\n-        with:\n-          username: ${{ secrets.DOCKERHUB_USERNAME }}\n-          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n-      -\n-        name: Build and push\n-        uses: docker/build-push-action@v5\n-        with:\n-          context: ./docker/transformers-pytorch-gpu\n-          build-args: |\n-            REF=main\n-          push: true\n-          tags: huggingface/transformers-pytorch-gpu\n-\n-      - name: Post to Slack\n-        if: always()\n-        uses: huggingface/hf-workflows/.github/actions/post-slack@main\n-        with:\n-          slack_channel: ${{ secrets.CI_SLACK_CHANNEL_DOCKER }}\n-          title: \ud83e\udd17 Results of the huggingface/transformers-pytorch-gpudocker build\n-          status: ${{ job.status }}\n-          slack_token: ${{ secrets.SLACK_CIFEEDBACK_BOT_TOKEN }}\n-\n   latest-pytorch-amd:\n     name: \"Latest PyTorch (AMD) [dev]\"\n     runs-on:\n@@ -245,26 +190,13 @@ jobs:\n             REF=main\n           push: true\n           tags: huggingface/transformers-pytorch-amd-gpu${{ inputs.image_postfix }}\n-      # Push CI images still need to be re-built daily\n-      -\n-        name: Build and push (for Push CI) in a daily basis\n-        # This condition allows `schedule` events, or `push` events that trigger this workflow NOT via `workflow_call`.\n-        # The later case is useful for manual image building for debugging purpose. Use another tag in this case!\n-        if: inputs.image_postfix != '-push-ci'\n-        uses: docker/build-push-action@v5\n-        with:\n-          context: ./docker/transformers-pytorch-amd-gpu\n-          build-args: |\n-            REF=main\n-          push: true\n-          tags: huggingface/transformers-pytorch-amd-gpu-push-ci\n \n       - name: Post to Slack\n         if: always()\n         uses: huggingface/hf-workflows/.github/actions/post-slack@main\n         with:\n           slack_channel: ${{ secrets.CI_SLACK_CHANNEL_DOCKER }}\n-          title: \ud83e\udd17 Results of the huggingface/transformers-pytorch-amd-gpu-push-ci build\n+          title: \ud83e\udd17 Results of the huggingface/transformers-pytorch-amd-gpu build\n           status: ${{ job.status }}\n           slack_token: ${{ secrets.SLACK_CIFEEDBACK_BOT_TOKEN }}\n \n@@ -294,19 +226,6 @@ jobs:\n             REF=main\n           push: true\n           tags: huggingface/transformers-pytorch-deepspeed-amd-gpu${{ inputs.image_postfix }}\n-      # Push CI images still need to be re-built daily\n-      -\n-        name: Build and push (for Push CI) in a daily basis\n-        # This condition allows `schedule` events, or `push` events that trigger this workflow NOT via `workflow_call`.\n-        # The later case is useful for manual image building for debugging purpose. Use another tag in this case!\n-        if: inputs.image_postfix != '-push-ci'\n-        uses: docker/build-push-action@v5\n-        with:\n-          context: ./docker/transformers-pytorch-deepspeed-amd-gpu\n-          build-args: |\n-            REF=main\n-          push: true\n-          tags: huggingface/transformers-pytorch-deepspeed-amd-gpu-push-ci\n \n       - name: Post to Slack\n         if: always()\n@@ -319,8 +238,6 @@ jobs:\n \n   latest-quantization-torch-docker:\n     name: \"Latest Pytorch + Quantization [dev]\"\n-     # Push CI doesn't need this image\n-    if: inputs.image_postfix != '-push-ci'\n     runs-on:\n       group: aws-general-8-plus\n     steps:"
      },
      {
        "filename": ".github/workflows/push-important-models.yml",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -149,7 +149,7 @@ jobs:\n     with:\n       job: run_models_gpu\n       slack_report_channel: \"#transformers-ci-push\"\n-      docker: huggingface/transformers-all-latest-gpu\n+      docker: huggingface/transformers-all-latest-gpu:flash-attn\n       ci_event: push\n       report_repo_id: hf-internal-testing/transformers_ci_push\n       commit_sha: ${{ github.sha }}"
      },
      {
        "filename": ".github/workflows/self-push-amd-mi210-caller.yml",
        "status": "removed",
        "additions": 0,
        "deletions": 25,
        "changes": 25,
        "patch": "@@ -1,25 +0,0 @@\n-name: Self-hosted runner (AMD mi210 CI caller)\n-\n-on:\n-  #workflow_run:\n-  #  workflows: [\"Self-hosted runner (push-caller)\"]\n-  #  branches: [\"main\"]\n-  #  types: [completed]\n-  push:\n-    branches:\n-      - run_amd_push_ci_caller*\n-    paths:\n-      - \"src/**\"\n-      - \"tests/**\"\n-      - \".github/**\"\n-      - \"templates/**\"\n-      - \"utils/**\"\n-\n-jobs:\n-  run_amd_ci:\n-    name: AMD mi210\n-    if: (cancelled() != true) && ((github.event_name == 'workflow_run') || ((github.event_name == 'push') && startsWith(github.ref_name, 'run_amd_push_ci_caller')))\n-    uses: ./.github/workflows/self-push-amd.yml\n-    with:\n-      gpu_flavor: mi210\n-    secrets: inherit"
      },
      {
        "filename": ".github/workflows/self-push-amd-mi250-caller.yml",
        "status": "removed",
        "additions": 0,
        "deletions": 25,
        "changes": 25,
        "patch": "@@ -1,25 +0,0 @@\n-name: Self-hosted runner (AMD mi250 CI caller)\n-\n-on:\n-  #workflow_run:\n-  #  workflows: [\"Self-hosted runner (push-caller)\"]\n-  #  branches: [\"main\"]\n-  #  types: [completed]\n-  push:\n-    branches:\n-      - run_amd_push_ci_caller*\n-    paths:\n-      - \"src/**\"\n-      - \"tests/**\"\n-      - \".github/**\"\n-      - \"templates/**\"\n-      - \"utils/**\"\n-\n-jobs:\n-  run_amd_ci:\n-    name: AMD mi250\n-    if: (cancelled() != true) && ((github.event_name == 'workflow_run') || ((github.event_name == 'push') && startsWith(github.ref_name, 'run_amd_push_ci_caller')))\n-    uses: ./.github/workflows/self-push-amd.yml\n-    with:\n-      gpu_flavor: mi250\n-    secrets: inherit"
      },
      {
        "filename": ".github/workflows/self-push-amd.yml",
        "status": "removed",
        "additions": 0,
        "deletions": 334,
        "changes": 334,
        "patch": "@@ -1,334 +0,0 @@\n-name: Self-hosted runner AMD GPU (push)\n-\n-on:\n-  workflow_call:\n-    inputs:\n-      gpu_flavor:\n-        required: true\n-        type: string\n-\n-env:\n-  HF_HOME: /mnt/cache\n-  TRANSFORMERS_IS_CI: yes\n-  OMP_NUM_THREADS: 8\n-  MKL_NUM_THREADS: 8\n-  PYTEST_TIMEOUT: 60\n-  TF_FORCE_GPU_ALLOW_GROWTH: true\n-  HF_HUB_READ_TOKEN: ${{ secrets.HF_HUB_READ_TOKEN }}\n-\n-jobs:\n-  check_runner_status:\n-    name: Check Runner Status\n-    runs-on: ubuntu-22.04\n-    steps:\n-      - name: Checkout transformers\n-        uses: actions/checkout@v4\n-        with:\n-          fetch-depth: 2\n-\n-      - name: Check Runner Status\n-        run: python utils/check_self_hosted_runner.py --target_runners amd-mi210-single-gpu-ci-runner-docker --token ${{ secrets.ACCESS_REPO_INFO_TOKEN }}\n-\n-  check_runners:\n-    name: Check Runners\n-    needs: check_runner_status\n-    strategy:\n-      matrix:\n-        machine_type: [single-gpu, multi-gpu]\n-    runs-on: [self-hosted, amd-gpu, '${{ matrix.machine_type }}', '${{ inputs.gpu_flavor }}']\n-    container:\n-      image: huggingface/transformers-pytorch-amd-gpu-push-ci  # <--- We test only for PyTorch for now\n-      options: --device /dev/kfd --device /dev/dri --env ROCR_VISIBLE_DEVICES --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n-    steps:\n-      - name: ROCM-SMI\n-        run: |\n-          rocm-smi\n-      - name: ROCM-INFO\n-        run: |\n-          rocminfo  | grep \"Agent\" -A 14\n-      - name: Show ROCR environment\n-        run: |\n-          echo \"ROCR: $ROCR_VISIBLE_DEVICES\"\n-\n-  setup_gpu:\n-    name: Setup\n-    needs: check_runners\n-    strategy:\n-      matrix:\n-        machine_type: [single-gpu, multi-gpu]\n-    runs-on: [self-hosted, amd-gpu, '${{ matrix.machine_type }}', '${{ inputs.gpu_flavor }}']\n-    container:\n-      image: huggingface/transformers-pytorch-amd-gpu-push-ci  # <--- We test only for PyTorch for now\n-      options: --device /dev/kfd --device /dev/dri --env ROCR_VISIBLE_DEVICES --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n-    outputs:\n-      matrix: ${{ steps.set-matrix.outputs.matrix }}\n-      test_map: ${{ steps.set-matrix.outputs.test_map }}\n-    env:\n-      # `CI_BRANCH_PUSH`: The branch name from the push event\n-      # `CI_BRANCH_WORKFLOW_RUN`: The name of the branch on which this workflow is triggered by `workflow_run` event\n-      # `CI_SHA_PUSH`: The commit SHA from the push event\n-      # `CI_SHA_WORKFLOW_RUN`: The commit SHA that triggers this workflow by `workflow_run` event\n-      CI_BRANCH_PUSH: ${{ github.event.ref }}\n-      CI_BRANCH_WORKFLOW_RUN: ${{ github.event.workflow_run.head_branch }}\n-      CI_SHA_PUSH: ${{ github.event.head_commit.id }}\n-      CI_SHA_WORKFLOW_RUN: ${{ github.event.workflow_run.head_sha }}\n-    steps:\n-      # Necessary to get the correct branch name and commit SHA for `workflow_run` event\n-      # We also take into account the `push` event (we might want to test some changes in a branch)\n-      - name: Prepare custom environment variables\n-        shell: bash\n-        # `CI_BRANCH`: The non-empty branch name from the above two (one and only one of them is empty)\n-        # `CI_SHA`: The non-empty commit SHA from the above two (one and only one of them is empty)\n-        run: |\n-          CI_BRANCH_PUSH=${CI_BRANCH_PUSH/'refs/heads/'/''}\n-          echo $CI_BRANCH_PUSH\n-          echo $CI_BRANCH_WORKFLOW_RUN\n-          echo $CI_SHA_PUSH\n-          echo $CI_SHA_WORKFLOW_RUN\n-          [[ ! -z \"$CI_BRANCH_PUSH\" ]] && echo \"CI_BRANCH=$CI_BRANCH_PUSH\" >> $GITHUB_ENV || echo \"CI_BRANCH=$CI_BRANCH_WORKFLOW_RUN\" >> $GITHUB_ENV\n-          [[ ! -z \"$CI_SHA_PUSH\" ]] && echo \"CI_SHA=$CI_SHA_PUSH\" >> $GITHUB_ENV || echo \"CI_SHA=$CI_SHA_WORKFLOW_RUN\" >> $GITHUB_ENV\n-\n-      - name: print environment variables\n-        run: |\n-          echo \"env.CI_BRANCH = ${{ env.CI_BRANCH }}\"\n-          echo \"env.CI_SHA = ${{ env.CI_SHA }}\"\n-\n-      - name: Update clone using environment variables\n-        working-directory: /transformers\n-        run: |\n-          echo \"original branch = $(git branch --show-current)\"\n-          git fetch && git checkout ${{ env.CI_BRANCH }}\n-          echo \"updated branch = $(git branch --show-current)\"\n-          git checkout ${{ env.CI_SHA }}\n-          echo \"log = $(git log -n 1)\"\n-\n-      - name: Cleanup\n-        working-directory: /transformers\n-        run: |\n-          rm -rf tests/__pycache__\n-          rm -rf tests/models/__pycache__\n-          rm -rf reports\n-\n-      - name: Show installed libraries and their versions\n-        working-directory: /transformers\n-        run: pip freeze\n-\n-      - name: Fetch the tests to run\n-        working-directory: /transformers\n-        # TODO: add `git-python` in the docker images\n-        run: |\n-          pip install --upgrade git-python\n-          python3 utils/tests_fetcher.py --diff_with_last_commit | tee test_preparation.txt\n-\n-      - name: Report fetched tests\n-        uses: actions/upload-artifact@v4\n-        with:\n-          name: test_fetched\n-          path: /transformers/test_preparation.txt\n-\n-      - id: set-matrix\n-        name: Organize tests into models\n-        working-directory: /transformers\n-        # The `keys` is used as GitHub actions matrix for jobs, i.e. `models/bert`, `tokenization`, `pipeline`, etc.\n-        # The `test_map` is used to get the actual identified test files under each key.\n-        # If no test to run (so no `test_map.json` file), create a dummy map (empty matrix will fail)\n-        run: |\n-          if [ -f test_map.json ]; then\n-              keys=$(python3 -c 'import json; fp = open(\"test_map.json\"); test_map = json.load(fp); fp.close(); d = list(test_map.keys()); print(d)')\n-              test_map=$(python3 -c 'import json; fp = open(\"test_map.json\"); test_map = json.load(fp); fp.close(); print(test_map)')\n-          else\n-              keys=$(python3 -c 'keys = [\"dummy\"]; print(keys)')\n-              test_map=$(python3 -c 'test_map = {\"dummy\": []}; print(test_map)')\n-          fi\n-          echo $keys\n-          echo $test_map\n-          echo \"matrix=$keys\" >> $GITHUB_OUTPUT\n-          echo \"test_map=$test_map\" >> $GITHUB_OUTPUT\n-\n-  run_models_gpu:\n-    name: Model tests\n-    needs: setup_gpu\n-    # `dummy` means there is no test to run\n-    if: contains(fromJson(needs.setup_gpu.outputs.matrix), 'dummy') != true\n-    strategy:\n-      fail-fast: false\n-      matrix:\n-        folders: ${{ fromJson(needs.setup_gpu.outputs.matrix) }}\n-        machine_type: [single-gpu, multi-gpu]\n-    runs-on: [self-hosted, amd-gpu, '${{ matrix.machine_type }}', '${{ inputs.gpu_flavor }}']\n-    container:\n-      image: huggingface/transformers-pytorch-amd-gpu-push-ci  # <--- We test only for PyTorch for now\n-      options: --device /dev/kfd --device /dev/dri --env ROCR_VISIBLE_DEVICES --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n-    env:\n-      # For the meaning of these environment variables, see the job `Setup`\n-      CI_BRANCH_PUSH: ${{ github.event.ref }}\n-      CI_BRANCH_WORKFLOW_RUN: ${{ github.event.workflow_run.head_branch }}\n-      CI_SHA_PUSH: ${{ github.event.head_commit.id }}\n-      CI_SHA_WORKFLOW_RUN: ${{ github.event.workflow_run.head_sha }}\n-    steps:\n-      # Necessary to get the correct branch name and commit SHA for `workflow_run` event\n-      # We also take into account the `push` event (we might want to test some changes in a branch)\n-      - name: Prepare custom environment variables\n-        shell: bash\n-        # For the meaning of these environment variables, see the job `Setup`\n-        run: |\n-          CI_BRANCH_PUSH=${CI_BRANCH_PUSH/'refs/heads/'/''}\n-          echo $CI_BRANCH_PUSH\n-          echo $CI_BRANCH_WORKFLOW_RUN\n-          echo $CI_SHA_PUSH\n-          echo $CI_SHA_WORKFLOW_RUN\n-          [[ ! -z \"$CI_BRANCH_PUSH\" ]] && echo \"CI_BRANCH=$CI_BRANCH_PUSH\" >> $GITHUB_ENV || echo \"CI_BRANCH=$CI_BRANCH_WORKFLOW_RUN\" >> $GITHUB_ENV\n-          [[ ! -z \"$CI_SHA_PUSH\" ]] && echo \"CI_SHA=$CI_SHA_PUSH\" >> $GITHUB_ENV || echo \"CI_SHA=$CI_SHA_WORKFLOW_RUN\" >> $GITHUB_ENV\n-\n-      - name: print environment variables\n-        run: |\n-          echo \"env.CI_BRANCH = ${{ env.CI_BRANCH }}\"\n-          echo \"env.CI_SHA = ${{ env.CI_SHA }}\"\n-\n-      - name: Update clone using environment variables\n-        working-directory: /transformers\n-        run: |\n-          echo \"original branch = $(git branch --show-current)\"\n-          git fetch && git checkout ${{ env.CI_BRANCH }}\n-          echo \"updated branch = $(git branch --show-current)\"\n-          git checkout ${{ env.CI_SHA }}\n-          echo \"log = $(git log -n 1)\"\n-\n-      - name: Reinstall transformers in edit mode (remove the one installed during docker image build)\n-        working-directory: /transformers\n-        run: python3 -m pip uninstall -y transformers && python3 -m pip install -e .\n-\n-      - name: Echo folder ${{ matrix.folders }}\n-        shell: bash\n-        # For folders like `models/bert`, set an env. var. (`matrix_folders`) to `models_bert`, which will be used to\n-        # set the artifact folder names (because the character `/` is not allowed).\n-        run: |\n-          echo \"${{ matrix.folders }}\"\n-          echo \"${{ fromJson(needs.setup_gpu.outputs.test_map)[matrix.folders] }}\"\n-          matrix_folders=${{ matrix.folders }}\n-          matrix_folders=${matrix_folders/'models/'/'models_'}\n-          echo \"$matrix_folders\"\n-          echo \"matrix_folders=$matrix_folders\" >> $GITHUB_ENV\n-\n-      - name: ROCM-SMI\n-        run: |\n-          rocm-smi\n-      - name: ROCM-INFO\n-        run: |\n-          rocminfo  | grep \"Agent\" -A 14\n-      - name: Show ROCR environment\n-        run: |\n-          echo \"ROCR: $ROCR_VISIBLE_DEVICES\"\n-\n-      - name: Environment\n-        working-directory: /transformers\n-        run: |\n-          python3 utils/print_env.py\n-\n-      - name: Show installed libraries and their versions\n-        working-directory: /transformers\n-        run: pip freeze\n-\n-      - name: Run all non-slow selected tests on GPU\n-        working-directory: /transformers\n-        run: |\n-          python3 -m pytest -n 2 --dist=loadfile -v --make-reports=${{ matrix.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports ${{ fromJson(needs.setup_gpu.outputs.test_map)[matrix.folders] }} -m \"not not_device_test\"\n-\n-      - name: Failure short reports\n-        if: ${{ failure() }}\n-        continue-on-error: true\n-        run: cat /transformers/reports/${{ matrix.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports/failures_short.txt\n-\n-      - name: \"Test suite reports artifacts: ${{ matrix.machine_type }}_run_models_gpu_${{ env.matrix_folders }}_test_reports\"\n-        if: ${{ always() }}\n-        uses: actions/upload-artifact@v4\n-        with:\n-          name: ${{ matrix.machine_type }}_run_models_gpu_${{ env.matrix_folders }}_test_reports\n-          path: /transformers/reports/${{ matrix.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports\n-\n-  send_results:\n-    name: Send results to webhook\n-    runs-on: ubuntu-22.04\n-    if: always()\n-    needs: [\n-        check_runner_status,\n-        check_runners,\n-        setup_gpu,\n-        run_models_gpu,\n-#        run_tests_torch_cuda_extensions_single_gpu,\n-#        run_tests_torch_cuda_extensions_multi_gpu\n-    ]\n-    env:\n-      # For the meaning of these environment variables, see the job `Setup`\n-      CI_BRANCH_PUSH: ${{ github.event.ref }}\n-      CI_BRANCH_WORKFLOW_RUN: ${{ github.event.workflow_run.head_branch }}\n-      CI_SHA_PUSH: ${{ github.event.head_commit.id }}\n-      CI_SHA_WORKFLOW_RUN: ${{ github.event.workflow_run.head_sha }}\n-    steps:\n-      - name: Preliminary job status\n-        shell: bash\n-        # For the meaning of these environment variables, see the job `Setup`\n-        run: |\n-          echo \"Runner availability: ${{ needs.check_runner_status.result }}\"\n-          echo \"Setup status: ${{ needs.setup_gpu.result }}\"\n-          echo \"Runner status: ${{ needs.check_runners.result }}\"\n-\n-      # Necessary to get the correct branch name and commit SHA for `workflow_run` event\n-      # We also take into account the `push` event (we might want to test some changes in a branch)\n-      - name: Prepare custom environment variables\n-        shell: bash\n-        # For the meaning of these environment variables, see the job `Setup`\n-        run: |\n-          CI_BRANCH_PUSH=${CI_BRANCH_PUSH/'refs/heads/'/''}\n-          echo $CI_BRANCH_PUSH\n-          echo $CI_BRANCH_WORKFLOW_RUN\n-          echo $CI_SHA_PUSH\n-          echo $CI_SHA_WORKFLOW_RUN\n-          [[ ! -z \"$CI_BRANCH_PUSH\" ]] && echo \"CI_BRANCH=$CI_BRANCH_PUSH\" >> $GITHUB_ENV || echo \"CI_BRANCH=$CI_BRANCH_WORKFLOW_RUN\" >> $GITHUB_ENV\n-          [[ ! -z \"$CI_SHA_PUSH\" ]] && echo \"CI_SHA=$CI_SHA_PUSH\" >> $GITHUB_ENV || echo \"CI_SHA=$CI_SHA_WORKFLOW_RUN\" >> $GITHUB_ENV\n-\n-      - name: print environment variables\n-        run: |\n-          echo \"env.CI_BRANCH = ${{ env.CI_BRANCH }}\"\n-          echo \"env.CI_SHA = ${{ env.CI_SHA }}\"\n-\n-      - uses: actions/checkout@v4\n-        # To avoid failure when multiple commits are merged into `main` in a short period of time.\n-        # Checking out to an old commit beyond the fetch depth will get an error `fatal: reference is not a tree: ...\n-        # (Only required for `workflow_run` event, where we get the latest HEAD on `main` instead of the event commit)\n-        with:\n-          fetch-depth: 20\n-\n-      - name: Update clone using environment variables\n-        run: |\n-          echo \"original branch = $(git branch --show-current)\"\n-          git fetch && git checkout ${{ env.CI_BRANCH }}\n-          echo \"updated branch = $(git branch --show-current)\"\n-          git checkout ${{ env.CI_SHA }}\n-          echo \"log = $(git log -n 1)\"\n-\n-      - uses: actions/download-artifact@v4\n-      - name: Send message to Slack\n-        env:\n-          CI_SLACK_BOT_TOKEN: ${{ secrets.CI_SLACK_BOT_TOKEN }}\n-          CI_SLACK_CHANNEL_ID: ${{ secrets.CI_SLACK_CHANNEL_ID }}\n-          CI_SLACK_CHANNEL_ID_DAILY: ${{ secrets.CI_SLACK_CHANNEL_ID_DAILY }}\n-          CI_SLACK_CHANNEL_ID_AMD: ${{ secrets.CI_SLACK_CHANNEL_ID_AMD }}\n-          CI_SLACK_CHANNEL_DUMMY_TESTS: ${{ secrets.CI_SLACK_CHANNEL_DUMMY_TESTS }}\n-          CI_SLACK_REPORT_CHANNEL_ID: ${{ secrets.CI_SLACK_CHANNEL_ID_AMD }}\n-          ACCESS_REPO_INFO_TOKEN: ${{ secrets.ACCESS_REPO_INFO_TOKEN }}\n-          CI_EVENT: Push CI (AMD) - ${{ inputs.gpu_flavor }}\n-          CI_TITLE_PUSH: ${{ github.event.head_commit.message }}\n-          CI_TITLE_WORKFLOW_RUN: ${{ github.event.workflow_run.head_commit.message }}\n-          CI_SHA: ${{ env.CI_SHA }}\n-          RUNNER_STATUS: ${{ needs.check_runner_status.result }}\n-          RUNNER_ENV_STATUS: ${{ needs.check_runners.result }}\n-          SETUP_STATUS: ${{ needs.setup_gpu.result }}\n-\n-        # We pass `needs.setup_gpu.outputs.matrix` as the argument. A processing in `notification_service.py` to change\n-        # `models/bert` to `models_bert` is required, as the artifact names use `_` instead of `/`.\n-        run: |\n-          pip install huggingface_hub\n-          pip install slack_sdk\n-          pip show slack_sdk\n-          python utils/notification_service.py \"${{ needs.setup_gpu.outputs.matrix }}\""
      },
      {
        "filename": ".github/workflows/self-push-caller.yml",
        "status": "removed",
        "additions": 0,
        "deletions": 54,
        "changes": 54,
        "patch": "@@ -1,54 +0,0 @@\n-# Used to trigger self-push CI\n-name: Self-hosted runner (push-caller)\n-\n-on:\n-  push:\n-    branches:\n-      - main\n-    paths:\n-      - \"src/**\"\n-      - \"tests/**\"\n-      - \".github/**\"\n-      - \"templates/**\"\n-      - \"utils/**\"\n-\n-jobs:\n-  check-for-setup:\n-      runs-on: ubuntu-22.04\n-      name: Check if setup was changed\n-      outputs:\n-        changed: ${{ steps.was_changed.outputs.changed }}\n-      steps:\n-        - uses: actions/checkout@v4\n-          with: \n-            fetch-depth: \"2\"\n-        \n-        - name: Get changed files\n-          id: changed-files\n-          uses: tj-actions/changed-files@1c8e6069583811afb28f97afeaf8e7da80c6be5c\n-        \n-        - name: Was setup changed \n-          id: was_changed\n-          run: |\n-            for file in ${{ steps.changed-files.outputs.all_changed_files }}; do\n-              if [ `basename \"${file}\"` = \"setup.py\" ]; then\n-                echo \"changed=1\" >> $GITHUB_OUTPUT\n-              fi\n-            done\n-\n-  build-docker-containers:\n-    needs: check-for-setup\n-    if: (github.event_name == 'push') && (needs.check-for-setup.outputs.changed == '1')\n-    uses: ./.github/workflows/build-docker-images.yml\n-    with:\n-      image_postfix: \"-push-ci\"\n-    secrets: inherit\n-\n-  run_push_ci:\n-    name: Trigger Push CI\n-    runs-on: ubuntu-22.04\n-    if: ${{ always() }}\n-    needs: build-docker-containers\n-    steps:\n-      - name: Trigger push CI via workflow_run\n-        run: echo \"Trigger push CI via workflow_run\""
      },
      {
        "filename": ".github/workflows/self-push.yml",
        "status": "removed",
        "additions": 0,
        "deletions": 652,
        "changes": 652,
        "patch": "@@ -1,652 +0,0 @@\n-name: Self-hosted runner (push)\n-\n-on:\n-  workflow_run:\n-    workflows: [\"Self-hosted runner (push-caller)\"]\n-    branches: [\"main\"]\n-    types: [completed]\n-  push:\n-    branches:\n-      - ci_*\n-      - ci-*\n-    paths:\n-      - \"src/**\"\n-      - \"tests/**\"\n-      - \".github/**\"\n-      - \"templates/**\"\n-      - \"utils/**\"\n-  repository_dispatch:\n-\n-env:\n-  HF_HOME: /mnt/cache\n-  TRANSFORMERS_IS_CI: yes\n-  OMP_NUM_THREADS: 8\n-  MKL_NUM_THREADS: 8\n-  PYTEST_TIMEOUT: 60\n-  TF_FORCE_GPU_ALLOW_GROWTH: true\n-  CUDA_VISIBLE_DEVICES: 0,1\n-\n-jobs:\n-  setup:\n-    name: Setup\n-    strategy:\n-      matrix:\n-        machine_type: [aws-g5-4xlarge-cache, aws-g5-12xlarge-cache]\n-    runs-on:\n-      group: '${{ matrix.machine_type }}'\n-    container:\n-      image: huggingface/transformers-all-latest-gpu-push-ci\n-      options: --gpus all --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n-    outputs:\n-      matrix: ${{ steps.set-matrix.outputs.matrix }}\n-      test_map: ${{ steps.set-matrix.outputs.test_map }}\n-    env:\n-      # `CI_BRANCH_PUSH`: The branch name from the push event\n-      # `CI_BRANCH_WORKFLOW_RUN`: The name of the branch on which this workflow is triggered by `workflow_run` event\n-      # `CI_SHA_PUSH`: The commit SHA from the push event\n-      # `CI_SHA_WORKFLOW_RUN`: The commit SHA that triggers this workflow by `workflow_run` event\n-      CI_BRANCH_PUSH: ${{ github.event.ref }}\n-      CI_BRANCH_WORKFLOW_RUN: ${{ github.event.workflow_run.head_branch }}\n-      CI_SHA_PUSH: ${{ github.event.head_commit.id }}\n-      CI_SHA_WORKFLOW_RUN: ${{ github.event.workflow_run.head_sha }}\n-    steps:\n-      # Necessary to get the correct branch name and commit SHA for `workflow_run` event\n-      # We also take into account the `push` event (we might want to test some changes in a branch)\n-      - name: Prepare custom environment variables\n-        shell: bash\n-        # `CI_BRANCH`: The non-empty branch name from the above two (one and only one of them is empty)\n-        # `CI_SHA`: The non-empty commit SHA from the above two (one and only one of them is empty)\n-        run: |\n-          CI_BRANCH_PUSH=${CI_BRANCH_PUSH/'refs/heads/'/''}\n-          echo $CI_BRANCH_PUSH\n-          echo $CI_BRANCH_WORKFLOW_RUN\n-          echo $CI_SHA_PUSH\n-          echo $CI_SHA_WORKFLOW_RUN\n-          [[ ! -z \"$CI_BRANCH_PUSH\" ]] && echo \"CI_BRANCH=$CI_BRANCH_PUSH\" >> $GITHUB_ENV || echo \"CI_BRANCH=$CI_BRANCH_WORKFLOW_RUN\" >> $GITHUB_ENV\n-          [[ ! -z \"$CI_SHA_PUSH\" ]] && echo \"CI_SHA=$CI_SHA_PUSH\" >> $GITHUB_ENV || echo \"CI_SHA=$CI_SHA_WORKFLOW_RUN\" >> $GITHUB_ENV\n-\n-      - name: print environment variables\n-        run: |\n-          echo \"env.CI_BRANCH = ${{ env.CI_BRANCH }}\"\n-          echo \"env.CI_SHA = ${{ env.CI_SHA }}\"\n-\n-      - name: Update clone using environment variables\n-        working-directory: /transformers\n-        run: |\n-          echo \"original branch = $(git branch --show-current)\"\n-          git fetch && git checkout ${{ env.CI_BRANCH }}\n-          echo \"updated branch = $(git branch --show-current)\"\n-          git checkout ${{ env.CI_SHA }}\n-          echo \"log = $(git log -n 1)\"\n-\n-      - name: Cleanup\n-        working-directory: /transformers\n-        run: |\n-          rm -rf tests/__pycache__\n-          rm -rf tests/models/__pycache__\n-          rm -rf reports\n-\n-      - name: Show installed libraries and their versions\n-        working-directory: /transformers\n-        run: pip freeze\n-\n-      - name: Fetch the tests to run\n-        working-directory: /transformers\n-        # TODO: add `git-python` in the docker images\n-        run: |\n-          pip install --upgrade git-python\n-          python3 utils/tests_fetcher.py --diff_with_last_commit | tee test_preparation.txt\n-\n-      - name: Report fetched tests\n-        uses: actions/upload-artifact@v4\n-        with:\n-          name: test_fetched\n-          path: /transformers/test_preparation.txt\n-\n-      - id: set-matrix\n-        name: Organize tests into models\n-        working-directory: /transformers\n-        # The `keys` is used as GitHub actions matrix for jobs, i.e. `models/bert`, `tokenization`, `pipeline`, etc.\n-        # The `test_map` is used to get the actual identified test files under each key.\n-        # If no test to run (so no `test_map.json` file), create a dummy map (empty matrix will fail)\n-        run: |\n-          if [ -f test_map.json ]; then\n-              keys=$(python3 -c 'import json; fp = open(\"test_map.json\"); test_map = json.load(fp); fp.close(); d = list(test_map.keys()); print(d)')\n-              test_map=$(python3 -c 'import json; fp = open(\"test_map.json\"); test_map = json.load(fp); fp.close(); print(test_map)')\n-          else\n-              keys=$(python3 -c 'keys = [\"dummy\"]; print(keys)')\n-              test_map=$(python3 -c 'test_map = {\"dummy\": []}; print(test_map)')\n-          fi\n-          echo $keys\n-          echo $test_map\n-          echo \"matrix=$keys\" >> $GITHUB_OUTPUT\n-          echo \"test_map=$test_map\" >> $GITHUB_OUTPUT\n-\n-  run_tests_single_gpu:\n-    name: Model tests\n-    needs: setup\n-    # `dummy` means there is no test to run\n-    if: contains(fromJson(needs.setup.outputs.matrix), 'dummy') != true\n-    strategy:\n-      fail-fast: false\n-      matrix:\n-        folders: ${{ fromJson(needs.setup.outputs.matrix) }}\n-        machine_type: [aws-g5-4xlarge-cache]\n-    runs-on:\n-      group: '${{ matrix.machine_type }}'\n-    container:\n-      image: huggingface/transformers-all-latest-gpu-push-ci\n-      options: --gpus all --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n-    env:\n-      # For the meaning of these environment variables, see the job `Setup`\n-      CI_BRANCH_PUSH: ${{ github.event.ref }}\n-      CI_BRANCH_WORKFLOW_RUN: ${{ github.event.workflow_run.head_branch }}\n-      CI_SHA_PUSH: ${{ github.event.head_commit.id }}\n-      CI_SHA_WORKFLOW_RUN: ${{ github.event.workflow_run.head_sha }}\n-    steps:\n-      # Necessary to get the correct branch name and commit SHA for `workflow_run` event\n-      # We also take into account the `push` event (we might want to test some changes in a branch)\n-      - name: Prepare custom environment variables\n-        shell: bash\n-        # For the meaning of these environment variables, see the job `Setup`\n-        run: |\n-          CI_BRANCH_PUSH=${CI_BRANCH_PUSH/'refs/heads/'/''}\n-          echo $CI_BRANCH_PUSH\n-          echo $CI_BRANCH_WORKFLOW_RUN\n-          echo $CI_SHA_PUSH\n-          echo $CI_SHA_WORKFLOW_RUN\n-          [[ ! -z \"$CI_BRANCH_PUSH\" ]] && echo \"CI_BRANCH=$CI_BRANCH_PUSH\" >> $GITHUB_ENV || echo \"CI_BRANCH=$CI_BRANCH_WORKFLOW_RUN\" >> $GITHUB_ENV\n-          [[ ! -z \"$CI_SHA_PUSH\" ]] && echo \"CI_SHA=$CI_SHA_PUSH\" >> $GITHUB_ENV || echo \"CI_SHA=$CI_SHA_WORKFLOW_RUN\" >> $GITHUB_ENV\n-\n-      - name: print environment variables\n-        run: |\n-          echo \"env.CI_BRANCH = ${{ env.CI_BRANCH }}\"\n-          echo \"env.CI_SHA = ${{ env.CI_SHA }}\"\n-\n-      - name: Set `machine_type` for report and artifact names\n-        working-directory: /transformers\n-        shell: bash\n-        run: |\n-          echo \"${{ matrix.machine_type }}\"\n-\n-          if [ \"${{ matrix.machine_type }}\" = \"aws-g5-4xlarge-cache\" ]; then\n-            machine_type=single-gpu\n-          elif [ \"${{ matrix.machine_type }}\" = \"aws-g5-12xlarge-cache\" ]; then\n-            machine_type=multi-gpu\n-          else\n-            machine_type=${{ matrix.machine_type }}\n-          fi\n-\n-          echo \"$machine_type\"\n-          echo \"machine_type=$machine_type\" >> $GITHUB_ENV\n-\n-      - name: Update clone using environment variables\n-        working-directory: /transformers\n-        run: |\n-          echo \"original branch = $(git branch --show-current)\"\n-          git fetch && git checkout ${{ env.CI_BRANCH }}\n-          echo \"updated branch = $(git branch --show-current)\"\n-          git checkout ${{ env.CI_SHA }}\n-          echo \"log = $(git log -n 1)\"\n-\n-      - name: Reinstall transformers in edit mode (remove the one installed during docker image build)\n-        working-directory: /transformers\n-        run: python3 -m pip uninstall -y transformers && python3 -m pip install -e .\n-\n-      - name: Echo folder ${{ matrix.folders }}\n-        shell: bash\n-        # For folders like `models/bert`, set an env. var. (`matrix_folders`) to `models_bert`, which will be used to\n-        # set the artifact folder names (because the character `/` is not allowed).\n-        run: |\n-          echo \"${{ matrix.folders }}\"\n-          echo \"${{ fromJson(needs.setup.outputs.test_map)[matrix.folders] }}\"\n-          matrix_folders=${{ matrix.folders }}\n-          matrix_folders=${matrix_folders/'models/'/'models_'}\n-          echo \"$matrix_folders\"\n-          echo \"matrix_folders=$matrix_folders\" >> $GITHUB_ENV\n-\n-      - name: NVIDIA-SMI\n-        run: |\n-          nvidia-smi\n-\n-      - name: Environment\n-        working-directory: /transformers\n-        run: |\n-          python3 utils/print_env.py\n-\n-      - name: Show installed libraries and their versions\n-        working-directory: /transformers\n-        run: pip freeze\n-\n-      - name: Run all non-slow selected tests on GPU\n-        working-directory: /transformers\n-        run: |\n-          python3 -m pytest -n 2 --dist=loadfile -v --make-reports=${{ env.machine_type }}_tests_gpu_${{ matrix.folders }} ${{ fromJson(needs.setup.outputs.test_map)[matrix.folders] }}\n-\n-      - name: Failure short reports\n-        if: ${{ failure() }}\n-        continue-on-error: true\n-        run: cat /transformers/reports/${{ env.machine_type }}_tests_gpu_${{ matrix.folders }}/failures_short.txt\n-\n-      - name: \"Test suite reports artifacts: ${{ env.machine_type }}_run_all_tests_gpu_${{ env.matrix_folders }}_test_reports\"\n-        if: ${{ always() }}\n-        uses: actions/upload-artifact@v4\n-        with:\n-          name: ${{ env.machine_type }}_run_all_tests_gpu_${{ env.matrix_folders }}_test_reports\n-          path: /transformers/reports/${{ env.machine_type }}_tests_gpu_${{ matrix.folders }}\n-\n-  run_tests_multi_gpu:\n-    name: Model tests\n-    needs: setup\n-    # `dummy` means there is no test to run\n-    if: contains(fromJson(needs.setup.outputs.matrix), 'dummy') != true\n-    strategy:\n-      fail-fast: false\n-      matrix:\n-        folders: ${{ fromJson(needs.setup.outputs.matrix) }}\n-        machine_type: [aws-g5-12xlarge-cache]\n-    runs-on:\n-      group: '${{ matrix.machine_type }}'\n-    container:\n-      image: huggingface/transformers-all-latest-gpu-push-ci\n-      options: --gpus all --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n-    env:\n-      # For the meaning of these environment variables, see the job `Setup`\n-      CI_BRANCH_PUSH: ${{ github.event.ref }}\n-      CI_BRANCH_WORKFLOW_RUN: ${{ github.event.workflow_run.head_branch }}\n-      CI_SHA_PUSH: ${{ github.event.head_commit.id }}\n-      CI_SHA_WORKFLOW_RUN: ${{ github.event.workflow_run.head_sha }}\n-    steps:\n-      # Necessary to get the correct branch name and commit SHA for `workflow_run` event\n-      # We also take into account the `push` event (we might want to test some changes in a branch)\n-      - name: Prepare custom environment variables\n-        shell: bash\n-        # For the meaning of these environment variables, see the job `Setup`\n-        run: |\n-          CI_BRANCH_PUSH=${CI_BRANCH_PUSH/'refs/heads/'/''}\n-          echo $CI_BRANCH_PUSH\n-          echo $CI_BRANCH_WORKFLOW_RUN\n-          echo $CI_SHA_PUSH\n-          echo $CI_SHA_WORKFLOW_RUN\n-          [[ ! -z \"$CI_BRANCH_PUSH\" ]] && echo \"CI_BRANCH=$CI_BRANCH_PUSH\" >> $GITHUB_ENV || echo \"CI_BRANCH=$CI_BRANCH_WORKFLOW_RUN\" >> $GITHUB_ENV\n-          [[ ! -z \"$CI_SHA_PUSH\" ]] && echo \"CI_SHA=$CI_SHA_PUSH\" >> $GITHUB_ENV || echo \"CI_SHA=$CI_SHA_WORKFLOW_RUN\" >> $GITHUB_ENV\n-\n-      - name: print environment variables\n-        run: |\n-          echo \"env.CI_BRANCH = ${{ env.CI_BRANCH }}\"\n-          echo \"env.CI_SHA = ${{ env.CI_SHA }}\"\n-\n-      - name: Set `machine_type` for report and artifact names\n-        working-directory: /transformers\n-        shell: bash\n-        run: |\n-          echo \"${{ matrix.machine_type }}\"\n-\n-          if [ \"${{ matrix.machine_type }}\" = \"aws-g5-4xlarge-cache\" ]; then\n-            machine_type=single-gpu\n-          elif [ \"${{ matrix.machine_type }}\" = \"aws-g5-12xlarge-cache\" ]; then\n-            machine_type=multi-gpu\n-          else\n-            machine_type=${{ matrix.machine_type }}\n-          fi\n-\n-          echo \"$machine_type\"\n-          echo \"machine_type=$machine_type\" >> $GITHUB_ENV\n-\n-      - name: Update clone using environment variables\n-        working-directory: /transformers\n-        run: |\n-          echo \"original branch = $(git branch --show-current)\"\n-          git fetch && git checkout ${{ env.CI_BRANCH }}\n-          echo \"updated branch = $(git branch --show-current)\"\n-          git checkout ${{ env.CI_SHA }}\n-          echo \"log = $(git log -n 1)\"\n-\n-      - name: Reinstall transformers in edit mode (remove the one installed during docker image build)\n-        working-directory: /transformers\n-        run: python3 -m pip uninstall -y transformers && python3 -m pip install -e .\n-\n-      - name: Echo folder ${{ matrix.folders }}\n-        shell: bash\n-        # For folders like `models/bert`, set an env. var. (`matrix_folders`) to `models_bert`, which will be used to\n-        # set the artifact folder names (because the character `/` is not allowed).\n-        run: |\n-          echo \"${{ matrix.folders }}\"\n-          echo \"${{ fromJson(needs.setup.outputs.test_map)[matrix.folders] }}\"\n-          matrix_folders=${{ matrix.folders }}\n-          matrix_folders=${matrix_folders/'models/'/'models_'}\n-          echo \"$matrix_folders\"\n-          echo \"matrix_folders=$matrix_folders\" >> $GITHUB_ENV\n-\n-      - name: NVIDIA-SMI\n-        run: |\n-          nvidia-smi\n-\n-      - name: Environment\n-        working-directory: /transformers\n-        run: |\n-          python3 utils/print_env.py\n-\n-      - name: Show installed libraries and their versions\n-        working-directory: /transformers\n-        run: pip freeze\n-\n-      - name: Run all non-slow selected tests on GPU\n-        env:\n-          MKL_SERVICE_FORCE_INTEL: 1\n-        working-directory: /transformers\n-        run: |\n-          python3 -m pytest -n 2 --dist=loadfile -v --make-reports=${{ env.machine_type }}_tests_gpu_${{ matrix.folders }} ${{ fromJson(needs.setup.outputs.test_map)[matrix.folders] }}\n-\n-      - name: Failure short reports\n-        if: ${{ failure() }}\n-        continue-on-error: true\n-        run: cat /transformers/reports/${{ env.machine_type }}_tests_gpu_${{ matrix.folders }}/failures_short.txt\n-\n-      - name: \"Test suite reports artifacts: ${{ env.machine_type }}_run_all_tests_gpu_${{ env.matrix_folders }}_test_reports\"\n-        if: ${{ always() }}\n-        uses: actions/upload-artifact@v4\n-        with:\n-          name: ${{ env.machine_type }}_run_all_tests_gpu_${{ env.matrix_folders }}_test_reports\n-          path: /transformers/reports/${{ env.machine_type }}_tests_gpu_${{ matrix.folders }}\n-\n-  run_tests_torch_cuda_extensions_single_gpu:\n-    name: Torch CUDA extension tests\n-    needs: setup\n-    if: contains(fromJson(needs.setup.outputs.matrix), 'deepspeed') || contains(fromJson(needs.setup.outputs.matrix), 'extended')\n-    strategy:\n-      fail-fast: false\n-      matrix:\n-        machine_type: [aws-g5-4xlarge-cache]\n-    runs-on:\n-      group: '${{ matrix.machine_type }}'\n-    container:\n-      image: huggingface/transformers-pytorch-deepspeed-latest-gpu-push-ci\n-      options: --gpus all --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n-    env:\n-      # For the meaning of these environment variables, see the job `Setup`\n-      CI_BRANCH_PUSH: ${{ github.event.ref }}\n-      CI_BRANCH_WORKFLOW_RUN: ${{ github.event.workflow_run.head_branch }}\n-      CI_SHA_PUSH: ${{ github.event.head_commit.id }}\n-      CI_SHA_WORKFLOW_RUN: ${{ github.event.workflow_run.head_sha }}\n-    steps:\n-      # Necessary to get the correct branch name and commit SHA for `workflow_run` event\n-      # We also take into account the `push` event (we might want to test some changes in a branch)\n-      - name: Prepare custom environment variables\n-        shell: bash\n-        # For the meaning of these environment variables, see the job `Setup`\n-        run: |\n-          CI_BRANCH_PUSH=${CI_BRANCH_PUSH/'refs/heads/'/''}\n-          echo $CI_BRANCH_PUSH\n-          echo $CI_BRANCH_WORKFLOW_RUN\n-          echo $CI_SHA_PUSH\n-          echo $CI_SHA_WORKFLOW_RUN\n-          [[ ! -z \"$CI_BRANCH_PUSH\" ]] && echo \"CI_BRANCH=$CI_BRANCH_PUSH\" >> $GITHUB_ENV || echo \"CI_BRANCH=$CI_BRANCH_WORKFLOW_RUN\" >> $GITHUB_ENV\n-          [[ ! -z \"$CI_SHA_PUSH\" ]] && echo \"CI_SHA=$CI_SHA_PUSH\" >> $GITHUB_ENV || echo \"CI_SHA=$CI_SHA_WORKFLOW_RUN\" >> $GITHUB_ENV\n-\n-      - name: print environment variables\n-        run: |\n-          echo \"env.CI_BRANCH = ${{ env.CI_BRANCH }}\"\n-          echo \"env.CI_SHA = ${{ env.CI_SHA }}\"\n-\n-      - name: Set `machine_type` for report and artifact names\n-        working-directory: /workspace/transformers\n-        shell: bash\n-        run: |\n-          echo \"${{ matrix.machine_type }}\"\n-\n-          if [ \"${{ matrix.machine_type }}\" = \"aws-g5-4xlarge-cache\" ]; then\n-            machine_type=single-gpu\n-          elif [ \"${{ matrix.machine_type }}\" = \"aws-g5-12xlarge-cache\" ]; then\n-            machine_type=multi-gpu\n-          else\n-            machine_type=${{ matrix.machine_type }}\n-          fi\n-\n-          echo \"$machine_type\"\n-          echo \"machine_type=$machine_type\" >> $GITHUB_ENV\n-\n-      - name: Update clone using environment variables\n-        working-directory: /workspace/transformers\n-        run: |\n-          echo \"original branch = $(git branch --show-current)\"\n-          git fetch && git checkout ${{ env.CI_BRANCH }}\n-          echo \"updated branch = $(git branch --show-current)\"\n-          git checkout ${{ env.CI_SHA }}\n-          echo \"log = $(git log -n 1)\"\n-\n-      - name: Reinstall transformers in edit mode (remove the one installed during docker image build)\n-        working-directory: /workspace/transformers\n-        run: python3 -m pip uninstall -y transformers && python3 -m pip install -e .\n-\n-      - name: Remove cached torch extensions\n-        run: rm -rf /github/home/.cache/torch_extensions/\n-\n-      # To avoid unknown test failures\n-      - name: Pre build DeepSpeed *again*\n-        working-directory: /workspace\n-        run: |\n-          python3 -m pip uninstall -y deepspeed\n-          DS_BUILD_CPU_ADAM=1 DS_BUILD_FUSED_ADAM=1 python3 -m pip install deepspeed --global-option=\"build_ext\" --global-option=\"-j8\" --no-cache -v --disable-pip-version-check\n-\n-      - name: NVIDIA-SMI\n-        run: |\n-          nvidia-smi\n-\n-      - name: Environment\n-        working-directory: /workspace/transformers\n-        run: |\n-          python utils/print_env.py\n-\n-      - name: Show installed libraries and their versions\n-        working-directory: /workspace/transformers\n-        run: pip freeze\n-\n-      - name: Run all non-slow selected tests on GPU\n-        working-directory: /workspace/transformers\n-        # TODO: Here we pass all tests in the 2 folders for simplicity. It's better to pass only the identified tests.\n-        run: |\n-          python -m pytest -n 1 --dist=loadfile -v --make-reports=${{ env.machine_type }}_run_torch_cuda_extensions_gpu_test_reports tests/deepspeed tests/extended\n-\n-      - name: Failure short reports\n-        if: ${{ failure() }}\n-        continue-on-error: true\n-        run: cat /workspace/transformers/reports/${{ env.machine_type }}_run_torch_cuda_extensions_gpu_test_reports/failures_short.txt\n-\n-      - name: \"Test suite reports artifacts: ${{ env.machine_type }}_run_torch_cuda_extensions_gpu_test_reports\"\n-        if: ${{ always() }}\n-        uses: actions/upload-artifact@v4\n-        with:\n-          name: ${{ env.machine_type }}_run_torch_cuda_extensions_gpu_test_reports\n-          path: /workspace/transformers/reports/${{ env.machine_type }}_run_torch_cuda_extensions_gpu_test_reports\n-\n-  run_tests_torch_cuda_extensions_multi_gpu:\n-    name: Torch CUDA extension tests\n-    needs: setup\n-    if: contains(fromJson(needs.setup.outputs.matrix), 'deepspeed') || contains(fromJson(needs.setup.outputs.matrix), 'extended')\n-    strategy:\n-      fail-fast: false\n-      matrix:\n-        machine_type: [aws-g5-12xlarge-cache]\n-    runs-on:\n-      group: '${{ matrix.machine_type }}'\n-    container:\n-      image: huggingface/transformers-pytorch-deepspeed-latest-gpu-push-ci\n-      options: --gpus all --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n-    env:\n-      # For the meaning of these environment variables, see the job `Setup`\n-      CI_BRANCH_PUSH: ${{ github.event.ref }}\n-      CI_BRANCH_WORKFLOW_RUN: ${{ github.event.workflow_run.head_branch }}\n-      CI_SHA_PUSH: ${{ github.event.head_commit.id }}\n-      CI_SHA_WORKFLOW_RUN: ${{ github.event.workflow_run.head_sha }}\n-    steps:\n-      # Necessary to get the correct branch name and commit SHA for `workflow_run` event\n-      # We also take into account the `push` event (we might want to test some changes in a branch)\n-      - name: Prepare custom environment variables\n-        shell: bash\n-        # For the meaning of these environment variables, see the job `Setup`\n-        run: |\n-          CI_BRANCH_PUSH=${CI_BRANCH_PUSH/'refs/heads/'/''}\n-          echo $CI_BRANCH_PUSH\n-          echo $CI_BRANCH_WORKFLOW_RUN\n-          echo $CI_SHA_PUSH\n-          echo $CI_SHA_WORKFLOW_RUN\n-          [[ ! -z \"$CI_BRANCH_PUSH\" ]] && echo \"CI_BRANCH=$CI_BRANCH_PUSH\" >> $GITHUB_ENV || echo \"CI_BRANCH=$CI_BRANCH_WORKFLOW_RUN\" >> $GITHUB_ENV\n-          [[ ! -z \"$CI_SHA_PUSH\" ]] && echo \"CI_SHA=$CI_SHA_PUSH\" >> $GITHUB_ENV || echo \"CI_SHA=$CI_SHA_WORKFLOW_RUN\" >> $GITHUB_ENV\n-\n-      - name: print environment variables\n-        run: |\n-          echo \"env.CI_BRANCH = ${{ env.CI_BRANCH }}\"\n-          echo \"env.CI_SHA = ${{ env.CI_SHA }}\"\n-\n-      - name: Set `machine_type` for report and artifact names\n-        working-directory: /workspace/transformers\n-        shell: bash\n-        run: |\n-          echo \"${{ matrix.machine_type }}\"\n-\n-          if [ \"${{ matrix.machine_type }}\" = \"aws-g5-4xlarge-cache\" ]; then\n-            machine_type=single-gpu\n-          elif [ \"${{ matrix.machine_type }}\" = \"aws-g5-12xlarge-cache\" ]; then\n-            machine_type=multi-gpu\n-          else\n-            machine_type=${{ matrix.machine_type }}\n-          fi\n-\n-          echo \"$machine_type\"\n-          echo \"machine_type=$machine_type\" >> $GITHUB_ENV\n-\n-      - name: Update clone using environment variables\n-        working-directory: /workspace/transformers\n-        run: |\n-          echo \"original branch = $(git branch --show-current)\"\n-          git fetch && git checkout ${{ env.CI_BRANCH }}\n-          echo \"updated branch = $(git branch --show-current)\"\n-          git checkout ${{ env.CI_SHA }}\n-          echo \"log = $(git log -n 1)\"\n-\n-      - name: Reinstall transformers in edit mode (remove the one installed during docker image build)\n-        working-directory: /workspace/transformers\n-        run: python3 -m pip uninstall -y transformers && python3 -m pip install -e .\n-\n-      - name: Remove cached torch extensions\n-        run: rm -rf /github/home/.cache/torch_extensions/\n-\n-      # To avoid unknown test failures\n-      - name: Pre build DeepSpeed *again*\n-        working-directory: /workspace\n-        run: |\n-          python3 -m pip uninstall -y deepspeed\n-          DS_BUILD_CPU_ADAM=1 DS_BUILD_FUSED_ADAM=1 python3 -m pip install deepspeed --global-option=\"build_ext\" --global-option=\"-j8\" --no-cache -v --disable-pip-version-check\n-\n-      - name: NVIDIA-SMI\n-        run: |\n-          nvidia-smi\n-\n-      - name: Environment\n-        working-directory: /workspace/transformers\n-        run: |\n-          python utils/print_env.py\n-\n-      - name: Show installed libraries and their versions\n-        working-directory: /workspace/transformers\n-        run: pip freeze\n-\n-      - name: Run all non-slow selected tests on GPU\n-        working-directory: /workspace/transformers\n-        # TODO: Here we pass all tests in the 2 folders for simplicity. It's better to pass only the identified tests.\n-        run: |\n-          python -m pytest -n 1 --dist=loadfile -v --make-reports=${{ env.machine_type }}_run_torch_cuda_extensions_gpu_test_reports tests/deepspeed tests/extended\n-\n-      - name: Failure short reports\n-        if: ${{ failure() }}\n-        continue-on-error: true\n-        run: cat /workspace/transformers/reports/${{ env.machine_type }}_run_torch_cuda_extensions_gpu_test_reports/failures_short.txt\n-\n-      - name: \"Test suite reports artifacts: ${{ env.machine_type }}_run_torch_cuda_extensions_gpu_test_reports\"\n-        if: ${{ always() }}\n-        uses: actions/upload-artifact@v4\n-        with:\n-          name: ${{ env.machine_type }}_run_torch_cuda_extensions_gpu_test_reports\n-          path: /workspace/transformers/reports/${{ env.machine_type }}_run_torch_cuda_extensions_gpu_test_reports\n-\n-  send_results:\n-    name: Send results to webhook\n-    runs-on: ubuntu-22.04\n-    if: always()\n-    needs: [\n-        setup,\n-        run_tests_single_gpu,\n-        run_tests_multi_gpu,\n-        run_tests_torch_cuda_extensions_single_gpu,\n-        run_tests_torch_cuda_extensions_multi_gpu\n-    ]\n-    env:\n-      # For the meaning of these environment variables, see the job `Setup`\n-      CI_BRANCH_PUSH: ${{ github.event.ref }}\n-      CI_BRANCH_WORKFLOW_RUN: ${{ github.event.workflow_run.head_branch }}\n-      CI_SHA_PUSH: ${{ github.event.head_commit.id }}\n-      CI_SHA_WORKFLOW_RUN: ${{ github.event.workflow_run.head_sha }}\n-    steps:\n-      - name: Preliminary job status\n-        shell: bash\n-        # For the meaning of these environment variables, see the job `Setup`\n-        run: |\n-          echo \"Setup status: ${{ needs.setup.result }}\"\n-\n-      # Necessary to get the correct branch name and commit SHA for `workflow_run` event\n-      # We also take into account the `push` event (we might want to test some changes in a branch)\n-      - name: Prepare custom environment variables\n-        shell: bash\n-        # For the meaning of these environment variables, see the job `Setup`\n-        run: |\n-          CI_BRANCH_PUSH=${CI_BRANCH_PUSH/'refs/heads/'/''}\n-          echo $CI_BRANCH_PUSH\n-          echo $CI_BRANCH_WORKFLOW_RUN\n-          echo $CI_SHA_PUSH\n-          echo $CI_SHA_WORKFLOW_RUN\n-          [[ ! -z \"$CI_BRANCH_PUSH\" ]] && echo \"CI_BRANCH=$CI_BRANCH_PUSH\" >> $GITHUB_ENV || echo \"CI_BRANCH=$CI_BRANCH_WORKFLOW_RUN\" >> $GITHUB_ENV\n-          [[ ! -z \"$CI_SHA_PUSH\" ]] && echo \"CI_SHA=$CI_SHA_PUSH\" >> $GITHUB_ENV || echo \"CI_SHA=$CI_SHA_WORKFLOW_RUN\" >> $GITHUB_ENV\n-\n-      - name: print environment variables\n-        run: |\n-          echo \"env.CI_BRANCH = ${{ env.CI_BRANCH }}\"\n-          echo \"env.CI_SHA = ${{ env.CI_SHA }}\"\n-\n-      - uses: actions/checkout@v4\n-        # To avoid failure when multiple commits are merged into `main` in a short period of time.\n-        # Checking out to an old commit beyond the fetch depth will get an error `fatal: reference is not a tree: ...\n-        # (Only required for `workflow_run` event, where we get the latest HEAD on `main` instead of the event commit)\n-        with:\n-          fetch-depth: 20\n-\n-      - name: Update clone using environment variables\n-        run: |\n-          echo \"original branch = $(git branch --show-current)\"\n-          git fetch && git checkout ${{ env.CI_BRANCH }}\n-          echo \"updated branch = $(git branch --show-current)\"\n-          git checkout ${{ env.CI_SHA }}\n-          echo \"log = $(git log -n 1)\"\n-\n-      - uses: actions/download-artifact@v4\n-      - name: Send message to Slack\n-        env:\n-          CI_SLACK_BOT_TOKEN: ${{ secrets.CI_SLACK_BOT_TOKEN }}\n-          CI_SLACK_CHANNEL_ID: ${{ secrets.CI_SLACK_CHANNEL_ID }}\n-          CI_SLACK_CHANNEL_ID_DAILY: ${{ secrets.CI_SLACK_CHANNEL_ID_DAILY }}\n-          CI_SLACK_CHANNEL_DUMMY_TESTS: ${{ secrets.CI_SLACK_CHANNEL_DUMMY_TESTS }}\n-          CI_SLACK_REPORT_CHANNEL_ID: ${{ secrets.CI_SLACK_CHANNEL_ID }}\n-          ACCESS_REPO_INFO_TOKEN: ${{ secrets.ACCESS_REPO_INFO_TOKEN }}\n-          CI_EVENT: push\n-          CI_TITLE_PUSH: ${{ github.event.head_commit.message }}\n-          CI_TITLE_WORKFLOW_RUN: ${{ github.event.workflow_run.head_commit.message }}\n-          CI_SHA: ${{ env.CI_SHA }}\n-          SETUP_STATUS: ${{ needs.setup.result }}\n-\n-        # We pass `needs.setup.outputs.matrix` as the argument. A processing in `notification_service.py` to change\n-        # `models/bert` to `models_bert` is required, as the artifact names use `_` instead of `/`.\n-        run: |\n-          pip install huggingface_hub\n-          pip install slack_sdk\n-          pip show slack_sdk\n-          python utils/notification_service.py \"${{ needs.setup.outputs.matrix }}\""
      },
      {
        "filename": ".github/workflows/self-scheduled-caller.yml",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -63,7 +63,7 @@ jobs:\n     with:\n       job: run_pipelines_torch_gpu\n       slack_report_channel: \"#transformers-ci-daily-pipeline-torch\"\n-      docker: huggingface/transformers-pytorch-gpu\n+      docker: huggingface/transformers-all-latest-gpu\n       ci_event: Daily CI\n       report_repo_id: hf-internal-testing/transformers_daily_ci\n       commit_sha: ${{ github.sha }}"
      },
      {
        "filename": ".github/workflows/self-scheduled-flash-attn-caller.yml",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -51,7 +51,7 @@ jobs:\n     with:\n       job: run_models_gpu\n       slack_report_channel: \"#transformers-ci-flash-attn\"\n-      docker: huggingface/transformers-all-latest-gpu\n+      docker: huggingface/transformers-all-latest-gpu:flash-attn\n       ci_event: Daily CI\n       runner_type: \"a10\"\n       report_repo_id: hf-internal-testing/transformers_flash_attn_ci"
      },
      {
        "filename": ".github/workflows/self-scheduled.yml",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -165,7 +165,7 @@ jobs:\n     runs-on:\n       group: '${{ matrix.machine_type }}'\n     container:\n-      image: huggingface/transformers-pytorch-gpu\n+      image: huggingface/transformers-all-latest-gpu\n       options: --gpus all --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n     steps:\n       - name: Update clone"
      },
      {
        "filename": "docker/transformers-all-latest-gpu/Dockerfile",
        "status": "modified",
        "additions": 44,
        "deletions": 6,
        "changes": 50,
        "patch": "@@ -9,10 +9,15 @@ SHELL [\"sh\", \"-lc\"]\n # The following `ARG` are mainly used to specify the versions explicitly & directly in this docker file, and not meant\n # to be used as arguments for docker build (so far).\n \n-ARG PYTORCH='2.8.0'\n+ARG PYTORCH='2.9.0'\n # Example: `cu102`, `cu113`, etc.\n ARG CUDA='cu126'\n \n+# This needs to be compatible with the above `PYTORCH`.\n+ARG TORCHCODEC='0.8.0'\n+\n+ARG FLASH_ATTN='false'\n+\n RUN apt update\n RUN apt install -y git libsndfile1-dev tesseract-ocr espeak-ng python3 python3-pip ffmpeg git-lfs\n RUN git lfs install\n@@ -21,11 +26,44 @@ RUN python3 -m pip install --no-cache-dir --upgrade pip\n ARG REF=main\n RUN git clone https://github.com/huggingface/transformers && cd transformers && git checkout $REF\n \n+RUN python3 -m pip install --no-cache-dir -e ./transformers[dev]\n+\n # 1. Put several commands in a single `RUN` to avoid image/layer exporting issue. Could be revised in the future.\n-# 2. Regarding `torch` part, We might need to specify proper versions for `torchvision` and `torchaudio`.\n-#    Currently, let's not bother to specify their versions explicitly (so installed with their latest release versions).\n-# 3. For `torchcodec<0.8`: this is quickly added as torch 2.9.0 + torchcodec 0.8.0 fails on our CI env. Need to remove later once they work.\n-RUN python3 -m pip install --no-cache-dir -e ./transformers[dev,onnxruntime] && [ ${#PYTORCH} -gt 0 -a \"$PYTORCH\" != \"pre\" ] && VERSION='torch=='$PYTORCH'.*' ||  VERSION='torch'; echo \"export VERSION='$VERSION'\" >> ~/.profile && echo torch=$VERSION && [ \"$PYTORCH\" != \"pre\" ] && python3 -m pip install --no-cache-dir -U $VERSION torchvision torchaudio \"torchcodec<0.8\" --extra-index-url https://download.pytorch.org/whl/$CUDA || python3 -m pip install --no-cache-dir -U --pre torch torchvision torchaudio torchcodec --extra-index-url https://download.pytorch.org/whl/nightly/$CUDA\n+# 2. For `torchcodec`, use `cpu` as we don't have `libnvcuvid.so` on the host runner. See https://github.com/meta-pytorch/torchcodec/issues/912\n+#    **Important**: We need to specify `torchcodec` version if the torch version is not the latest stable one.\n+# 3. `set -e` means \"exit immediately if any command fails\".\n+RUN set -e; \\\n+    # Determine torch version\n+    if [ ${#PYTORCH} -gt 0 ] && [ \"$PYTORCH\" != \"pre\" ]; then \\\n+        VERSION=\"torch==${PYTORCH}.*\"; \\\n+        TORCHCODEC_VERSION=\"torchcodec==${TORCHCODEC}.*\"; \\\n+    else \\\n+        VERSION=\"torch\"; \\\n+        TORCHCODEC_VERSION=\"torchcodec\"; \\\n+    fi; \\\n+    \\\n+    # Log the version being installed\n+    echo \"Installing torch version: $VERSION\"; \\\n+    \\\n+    # Install PyTorch packages\n+    if [ \"$PYTORCH\" != \"pre\" ]; then \\\n+        python3 -m pip install --no-cache-dir -U \\\n+            $VERSION \\\n+            torchvision \\\n+            torchaudio \\\n+            --extra-index-url https://download.pytorch.org/whl/$CUDA; \\\n+        # We need to specify the version if the torch version is not the latest stable one.\n+        python3 -m pip install --no-cache-dir -U \\\n+            $TORCHCODEC_VERSION --extra-index-url https://download.pytorch.org/whl/cpu; \\\n+    else \\\n+        python3 -m pip install --no-cache-dir -U --pre \\\n+            torch \\\n+            torchvision \\\n+            torchaudio \\\n+            --extra-index-url https://download.pytorch.org/whl/nightly/$CUDA; \\\n+        python3 -m pip install --no-cache-dir -U --pre \\\n+            torchcodec --extra-index-url https://download.pytorch.org/whl/nightly/cpu; \\\n+    fi\n \n RUN python3 -m pip install --no-cache-dir -U timm\n \n@@ -54,7 +92,7 @@ RUN python3 -m pip install --no-cache-dir bitsandbytes\n RUN python3 -m pip install --no-cache-dir quanto\n \n # After using A10 as CI runner, let's run FA2 tests\n-RUN [ \"$PYTORCH\" != \"pre\" ] && python3 -m pip uninstall -y ninja && python3 -m pip install --no-cache-dir ninja && python3 -m pip install flash-attn --no-cache-dir --no-build-isolation || echo \"Don't install FA2 with nightly torch\"\n+RUN [ \"$FLASH_ATTN\" != \"false\" ] && python3 -m pip uninstall -y ninja && python3 -m pip install --no-cache-dir ninja && python3 -m pip install flash-attn --no-cache-dir --no-build-isolation || echo \"Don't install FA2 with nightly torch\"\n \n # TODO (ydshieh): check this again\n # `quanto` will install `ninja` which leads to many `CUDA error: an illegal memory access ...` in some model tests"
      }
    ],
    "num_files": 13,
    "scraped_at": "2025-11-16T21:16:36.540517",
    "filter_decision": {
      "accept": false,
      "reasoning": "This PR primarily consists of configuration and CI/CD workflow updates: renaming Docker image tags, removing deprecated push-CI infrastructure, and reorganizing workflow files. While the PR description provides context, the actual code changes are mostly find-and-replace updates to image references and deletion of obsolete workflow files with no new logic or architectural decisions that would warrant substantive technical questions.",
      "substance_level": "low"
    }
  },
  {
    "pr_number": 41864,
    "title": "Fix AutoImageProcessor.register and documentation in auto processing modules",
    "body": "# What does this PR do?\r\n\r\nThis PR fixes several issues in the auto processing modules:\r\n\r\n1. **Fixes `AutoImageProcessor.register` bug**: Removes incorrect validation logic that was copied from tokenizers. The validation checked for `slow_image_processor_class` attribute consistency, but fast image processors don't have this attribute (unlike fast tokenizers), causing the `register` method to fail when registering custom image processors.\r\n\r\n2. **Fixes documentation errors**: Corrects copy-paste errors in docstrings where \"tokenizer\" was incorrectly used instead of the appropriate processor type (feature extractor, image processor, video processor).\r\n\r\n3. **Fixes typos**: Corrects \"fine\" \u2192 \"find\" in comments across multiple auto modules.\r\n\r\n4. **Improves `AutoVideoProcessor` trust handling**: Adds proper upstream repository extraction from `video_processor_auto_map` when resolving trust_remote_code.\r\n\r\n## Changes by file:\r\n\r\n- `feature_extraction_auto.py`: Fixed typo and corrected docstring to reference feature extractors instead of tokenizers\r\n- `image_processing_auto.py`: Removed incorrect `slow_image_processor_class` validation and fixed import in docstring example\r\n- `processing_auto.py`: Fixed typo in comment\r\n- `tokenization_auto.py`: Fixed typo in comment  \r\n- `video_processing_auto.py`: Fixed docstring reference and added upstream repo handling for trust_remote_code\r\n\r\n## Before submitting\r\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [x] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\n@ArthurZucker @Rocketknight1 (auto modules and processing)\r\n\r\n",
    "html_url": "https://github.com/huggingface/transformers/pull/41864",
    "created_at": "2025-10-25T19:33:20Z",
    "merged_at": "2025-11-06T07:43:07Z",
    "merge_commit_sha": "32e49f2884cdc23c172513d1a2cafe0f03255591",
    "base_ref": "main",
    "head_sha": "17312eb3bb331a16b28737c9075dcbe65c545d0a",
    "user": "MilkClouds",
    "files": [
      {
        "filename": "src/transformers/models/auto/feature_extraction_auto.py",
        "status": "modified",
        "additions": 13,
        "deletions": 13,
        "changes": 26,
        "patch": "@@ -93,7 +93,7 @@ def feature_extractor_class_from_name(class_name: str):\n         if getattr(extractor, \"__name__\", None) == class_name:\n             return extractor\n \n-    # We did not fine the class, but maybe it's because a dep is missing. In that case, the class will be in the main\n+    # We did not find the class, but maybe it's because a dep is missing. In that case, the class will be in the main\n     # init and we return the proper dummy to get an appropriate error message.\n     main_module = importlib.import_module(\"transformers\")\n     if hasattr(main_module, class_name):\n@@ -113,7 +113,7 @@ def get_feature_extractor_config(\n     **kwargs,\n ):\n     \"\"\"\n-    Loads the tokenizer configuration from a pretrained model tokenizer configuration.\n+    Loads the feature extractor configuration from a pretrained model feature extractor configuration.\n \n     Args:\n         pretrained_model_name_or_path (`str` or `os.PathLike`):\n@@ -122,7 +122,7 @@ def get_feature_extractor_config(\n             - a string, the *model id* of a pretrained model configuration hosted inside a model repo on\n               huggingface.co.\n             - a path to a *directory* containing a configuration file saved using the\n-              [`~PreTrainedTokenizer.save_pretrained`] method, e.g., `./my_model_directory/`.\n+              [`~FeatureExtractionMixin.save_pretrained`] method, e.g., `./my_model_directory/`.\n \n         cache_dir (`str` or `os.PathLike`, *optional*):\n             Path to a directory in which a downloaded pretrained model configuration should be cached if the standard\n@@ -141,7 +141,7 @@ def get_feature_extractor_config(\n             git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\n             identifier allowed by git.\n         local_files_only (`bool`, *optional*, defaults to `False`):\n-            If `True`, will only try to load the tokenizer configuration from local files.\n+            If `True`, will only try to load the feature extractor configuration from local files.\n \n     <Tip>\n \n@@ -150,22 +150,22 @@ def get_feature_extractor_config(\n     </Tip>\n \n     Returns:\n-        `Dict`: The configuration of the tokenizer.\n+        `Dict`: The configuration of the feature extractor.\n \n     Examples:\n \n     ```python\n     # Download configuration from huggingface.co and cache.\n-    tokenizer_config = get_tokenizer_config(\"google-bert/bert-base-uncased\")\n-    # This model does not have a tokenizer config so the result will be an empty dict.\n-    tokenizer_config = get_tokenizer_config(\"FacebookAI/xlm-roberta-base\")\n+    feature_extractor_config = get_feature_extractor_config(\"facebook/wav2vec2-base-960h\")\n+    # This model does not have a feature extractor config so the result will be an empty dict.\n+    feature_extractor_config = get_feature_extractor_config(\"FacebookAI/xlm-roberta-base\")\n \n-    # Save a pretrained tokenizer locally and you can reload its config\n-    from transformers import AutoTokenizer\n+    # Save a pretrained feature extractor locally and you can reload its config\n+    from transformers import AutoFeatureExtractor\n \n-    tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n-    tokenizer.save_pretrained(\"tokenizer-test\")\n-    tokenizer_config = get_tokenizer_config(\"tokenizer-test\")\n+    feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n+    feature_extractor.save_pretrained(\"feature-extractor-test\")\n+    feature_extractor_config = get_feature_extractor_config(\"feature-extractor-test\")\n     ```\"\"\"\n     resolved_config_file = cached_file(\n         pretrained_model_name_or_path,"
      },
      {
        "filename": "src/transformers/models/auto/image_processing_auto.py",
        "status": "modified",
        "additions": 2,
        "deletions": 15,
        "changes": 17,
        "patch": "@@ -260,7 +260,7 @@ def get_image_processor_config(\n             - a string, the *model id* of a pretrained model configuration hosted inside a model repo on\n               huggingface.co.\n             - a path to a *directory* containing a configuration file saved using the\n-              [`~PreTrainedTokenizer.save_pretrained`] method, e.g., `./my_model_directory/`.\n+              [`~ProcessorMixin.save_pretrained`] method, e.g., `./my_model_directory/`.\n \n         cache_dir (`str` or `os.PathLike`, *optional*):\n             Path to a directory in which a downloaded pretrained model configuration should be cached if the standard\n@@ -299,7 +299,7 @@ def get_image_processor_config(\n     image_processor_config = get_image_processor_config(\"FacebookAI/xlm-roberta-base\")\n \n     # Save a pretrained image processor locally and you can reload its config\n-    from transformers import AutoTokenizer\n+    from transformers import AutoImageProcessor\n \n     image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n     image_processor.save_pretrained(\"image-processor-test\")\n@@ -629,19 +629,6 @@ def register(\n         ):\n             raise ValueError(\"The `fast_image_processor_class` should inherit from `BaseImageProcessorFast`.\")\n \n-        if (\n-            slow_image_processor_class is not None\n-            and fast_image_processor_class is not None\n-            and issubclass(fast_image_processor_class, BaseImageProcessorFast)\n-            and fast_image_processor_class.slow_image_processor_class != slow_image_processor_class\n-        ):\n-            raise ValueError(\n-                \"The fast processor class you are passing has a `slow_image_processor_class` attribute that is not \"\n-                \"consistent with the slow processor class you passed (fast tokenizer has \"\n-                f\"{fast_image_processor_class.slow_image_processor_class} and you passed {slow_image_processor_class}. Fix one of those \"\n-                \"so they match!\"\n-            )\n-\n         # Avoid resetting a set slow/fast image processor if we are passing just the other ones.\n         if config_class in IMAGE_PROCESSOR_MAPPING._extra_content:\n             existing_slow, existing_fast = IMAGE_PROCESSOR_MAPPING[config_class]"
      },
      {
        "filename": "src/transformers/models/auto/processing_auto.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -175,7 +175,7 @@ def processor_class_from_name(class_name: str):\n         if getattr(processor, \"__name__\", None) == class_name:\n             return processor\n \n-    # We did not fine the class, but maybe it's because a dep is missing. In that case, the class will be in the main\n+    # We did not find the class, but maybe it's because a dep is missing. In that case, the class will be in the main\n     # init and we return the proper dummy to get an appropriate error message.\n     main_module = importlib.import_module(\"transformers\")\n     if hasattr(main_module, class_name):"
      },
      {
        "filename": "src/transformers/models/auto/tokenization_auto.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -815,7 +815,7 @@ def tokenizer_class_from_name(class_name: str) -> Union[type[Any], None]:\n             if getattr(tokenizer, \"__name__\", None) == class_name:\n                 return tokenizer\n \n-    # We did not fine the class, but maybe it's because a dep is missing. In that case, the class will be in the main\n+    # We did not find the class, but maybe it's because a dep is missing. In that case, the class will be in the main\n     # init and we return the proper dummy to get an appropriate error message.\n     main_module = importlib.import_module(\"transformers\")\n     if hasattr(main_module, class_name):"
      },
      {
        "filename": "src/transformers/models/auto/video_processing_auto.py",
        "status": "modified",
        "additions": 9,
        "deletions": 4,
        "changes": 13,
        "patch": "@@ -122,7 +122,7 @@ def get_video_processor_config(\n             - a string, the *model id* of a pretrained model configuration hosted inside a model repo on\n               huggingface.co.\n             - a path to a *directory* containing a configuration file saved using the\n-              [`~PreTrainedTokenizer.save_pretrained`] method, e.g., `./my_model_directory/`.\n+              [`~BaseVideoProcessor.save_pretrained`] method, e.g., `./my_model_directory/`.\n \n         cache_dir (`str` or `os.PathLike`, *optional*):\n             Path to a directory in which a downloaded pretrained model configuration should be cached if the standard\n@@ -313,9 +313,14 @@ def from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs):\n \n         has_remote_code = video_processor_auto_map is not None\n         has_local_code = video_processor_class is not None or type(config) in VIDEO_PROCESSOR_MAPPING\n-        trust_remote_code = resolve_trust_remote_code(\n-            trust_remote_code, pretrained_model_name_or_path, has_local_code, has_remote_code\n-        )\n+        if has_remote_code:\n+            if \"--\" in video_processor_auto_map:\n+                upstream_repo = video_processor_auto_map.split(\"--\")[0]\n+            else:\n+                upstream_repo = None\n+            trust_remote_code = resolve_trust_remote_code(\n+                trust_remote_code, pretrained_model_name_or_path, has_local_code, has_remote_code, upstream_repo\n+            )\n \n         if has_remote_code and trust_remote_code:\n             class_ref = video_processor_auto_map"
      }
    ],
    "num_files": 5,
    "scraped_at": "2025-11-16T21:16:41.495801",
    "filter_decision": {
      "accept": false,
      "reasoning": "While the PR does fix a real bug in `AutoImageProcessor.register`, the majority of changes are trivial: typo fixes ('fine' \u2192 'find'), docstring corrections (copy-paste errors with wrong class names), and documentation updates. The only meaningful code change is removing 13 lines of incorrect validation logic, which is insufficient substance for generating multiple high-quality technical questions about codebase understanding.",
      "substance_level": "low"
    }
  },
  {
    "pr_number": 41857,
    "title": "CI workflow for Flash Attn",
    "body": "# What does this PR do?\r\n\r\nAs discussed with @vasqu ",
    "html_url": "https://github.com/huggingface/transformers/pull/41857",
    "created_at": "2025-10-25T07:39:45Z",
    "merged_at": "2025-10-25T07:45:47Z",
    "merge_commit_sha": "e2e8dbed13c6a8455fd85c15c9fa91c99d609010",
    "base_ref": "main",
    "head_sha": "25872bd20caadf6157a34d0b6694a0e4244de601",
    "user": "ydshieh",
    "files": [
      {
        "filename": ".github/workflows/model_jobs.yml",
        "status": "modified",
        "additions": 4,
        "deletions": 1,
        "changes": 5,
        "patch": "@@ -28,6 +28,9 @@ on:\n       report_repo_id:\n         required: false\n         type: string\n+      pytest_marker:\n+        required: false\n+        type: string\n \n env:\n   HF_HOME: /mnt/cache\n@@ -137,7 +140,7 @@ jobs:\n       - name: Run all tests on GPU\n         working-directory: /transformers\n         run: |\n-          script -q -c \"PATCH_TESTING_METHODS_TO_COLLECT_OUTPUTS=yes _PATCHED_TESTING_METHODS_OUTPUT_DIR=/transformers/reports/${{ env.machine_type }}_${{ inputs.report_name_prefix }}_${{ env.matrix_folders }}_test_reports python3 -m pytest -rsfE -v --make-reports=${{ env.machine_type }}_${{ inputs.report_name_prefix }}_${{ env.matrix_folders }}_test_reports tests/${{ matrix.folders }}\" test_outputs.txt\n+          script -q -c \"PATCH_TESTING_METHODS_TO_COLLECT_OUTPUTS=yes _PATCHED_TESTING_METHODS_OUTPUT_DIR=/transformers/reports/${{ env.machine_type }}_${{ inputs.report_name_prefix }}_${{ env.matrix_folders }}_test_reports python3 -m pytest -rsfE -v -m '${{ inputs.pytest_marker }}' --make-reports=${{ env.machine_type }}_${{ inputs.report_name_prefix }}_${{ env.matrix_folders }}_test_reports tests/${{ matrix.folders }}\" test_outputs.txt\n           ls -la\n           # Extract the exit code from the output file\n           EXIT_CODE=$(tail -1 test_outputs.txt | grep -o 'COMMAND_EXIT_CODE=\"[0-9]*\"' | cut -d'\"' -f2)"
      },
      {
        "filename": ".github/workflows/self-scheduled-flash-attn-caller.yml",
        "status": "added",
        "additions": 60,
        "deletions": 0,
        "changes": 60,
        "patch": "@@ -0,0 +1,60 @@\n+name: Nvidia CI - Flash Attn\n+\n+on:\n+  repository_dispatch:\n+  schedule:\n+    - cron: \"17 2 * * *\"\n+  push:\n+    branches:\n+      - run_nvidia_ci_flash_attn*\n+  workflow_dispatch:\n+    inputs:\n+      prev_workflow_run_id:\n+        description: 'previous workflow run id to compare'\n+        type: string\n+        required: false\n+        default: \"\"\n+      other_workflow_run_id:\n+        description: 'other workflow run id to compare'\n+        type: string\n+        required: false\n+        default: \"\"\n+\n+\n+# Used for `push` to easily modify the target workflow runs to compare against\n+env:\n+    prev_workflow_run_id: \"\"\n+    other_workflow_run_id: \"\"\n+\n+\n+jobs:\n+  setup:\n+    name: Setup\n+    runs-on: ubuntu-22.04\n+    steps:\n+      - name: Setup\n+        run: |\n+          mkdir \"setup_values\"\n+          echo \"${{ inputs.prev_workflow_run_id || env.prev_workflow_run_id }}\" > \"setup_values/prev_workflow_run_id.txt\"\n+          echo \"${{ inputs.other_workflow_run_id || env.other_workflow_run_id }}\" > \"setup_values/other_workflow_run_id.txt\"\n+\n+      - name: Upload artifacts\n+        uses: actions/upload-artifact@v4\n+        with:\n+          name: setup_values\n+          path: setup_values\n+\n+\n+  model-ci:\n+    name: Model CI\n+    uses: ./.github/workflows/self-scheduled.yml\n+    with:\n+      job: run_models_gpu\n+      slack_report_channel: \"#transformers-ci-flash-attn\"\n+      docker: huggingface/transformers-all-latest-gpu\n+      ci_event: Daily CI\n+      runner_type: \"a10\"\n+      report_repo_id: hf-internal-testing/transformers_flash_attn_ci\n+      commit_sha: ${{ github.sha }}\n+      pytest_marker: \"flash_attn_test or flash_attn_3_test\"\n+    secrets: inherit"
      },
      {
        "filename": ".github/workflows/self-scheduled.yml",
        "status": "modified",
        "additions": 5,
        "deletions": 0,
        "changes": 5,
        "patch": "@@ -38,6 +38,10 @@ on:\n         default: \"\"\n         required: false\n         type: string\n+      pytest_marker:\n+        required: false\n+        type: string\n+\n \n env:\n   HF_HOME: /mnt/cache\n@@ -127,6 +131,7 @@ jobs:\n       commit_sha: ${{ inputs.commit_sha || github.sha }}\n       runner_type: ${{ inputs.runner_type }}\n       report_repo_id: ${{ inputs.report_repo_id }}\n+      pytest_marker: ${{ inputs.pytest_marker }}\n     secrets: inherit\n \n   run_trainer_and_fsdp_gpu:"
      },
      {
        "filename": "utils/notification_service.py",
        "status": "modified",
        "additions": 4,
        "deletions": 1,
        "changes": 5,
        "patch": "@@ -1407,7 +1407,10 @@ def pop_default(l: list[Any], i: int, default: Any) -> Any:\n     if not os.path.isdir(os.path.join(os.getcwd(), f\"ci_results_{job_name}\")):\n         os.makedirs(os.path.join(os.getcwd(), f\"ci_results_{job_name}\"))\n \n-    nvidia_daily_ci_workflow = \"huggingface/transformers/.github/workflows/self-scheduled-caller.yml\"\n+    nvidia_daily_ci_workflow = (\n+        \"huggingface/transformers/.github/workflows/self-scheduled-caller.yml\",\n+        \"huggingface/transformers/.github/workflows/self-scheduled-flash-attn-caller.yml\",\n+    )\n     amd_daily_ci_workflows = (\n         \"huggingface/transformers/.github/workflows/self-scheduled-amd-mi325-caller.yml\",\n         \"huggingface/transformers/.github/workflows/self-scheduled-amd-mi355-caller.yml\","
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T21:16:42.096834",
    "filter_decision": {
      "accept": false,
      "reasoning": "This PR is primarily a configuration change to CI/CD workflows that adds a new pytest marker parameter and creates a new workflow file for Flash Attention testing. While it involves multiple files, the changes are largely mechanical parameter passing and workflow configuration without substantive logic changes, algorithmic decisions, or architectural patterns that would generate meaningful technical questions about codebase understanding.",
      "substance_level": "low"
    }
  },
  {
    "pr_number": 41812,
    "title": "Fix invalid examples in QwenVL model docstrings and add Qwen3VL example",
    "body": "# What does this PR do?\r\nThis PR fixes the non-functional examples for Qwen2-VL and Qwen2.5-VL models, and adds a runnable example for Qwen3VL.\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\n## Before submitting\r\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\n@zucchini-nlp, @Rocketknight1\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker @Cyrilvallez\r\n- vision models: @yonigozlan @molbap\r\n- audio models: @eustlb @ebezzam @vasqu\r\n- multimodal models: @zucchini-nlp\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- generate: @zucchini-nlp (visual-language models) or @gante (all others)\r\n- continuous batching: @remi-or @ArthurZucker @McPatate\r\n- pipelines: @Rocketknight1\r\n- tokenizers: @ArthurZucker and @itazap\r\n- trainer: @SunMarc\r\n- attention: @vasqu @ArthurZucker @CyrilVallez\r\n- model loading (from pretrained, etc): @CyrilVallez\r\n- distributed: @3outeille @ArthurZucker\r\n- CIs: @ydshieh\r\n\r\nIntegrations:\r\n\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization: @SunMarc @MekkCyber\r\n- kernels: @MekkCyber @drbh\r\n- peft: @BenjaminBossan @githubnemo\r\n\r\nDevices/Backends:\r\n\r\n- AMD ROCm: @ivarflakstad\r\n- Intel XPU: @IlyasMoutawwakil\r\n- Ascend NPU: @ivarflakstad \r\n\r\nDocumentation: @stevhliu\r\n\r\nResearch projects are not maintained and should be taken as is.\r\n\r\n -->\r\n",
    "html_url": "https://github.com/huggingface/transformers/pull/41812",
    "created_at": "2025-10-23T12:28:17Z",
    "merged_at": "2025-10-29T12:34:13Z",
    "merge_commit_sha": "5462376a5c9d33963c5249668e1061ccc98dcbce",
    "base_ref": "main",
    "head_sha": "b0b1e5511017beefbc7a96eba8f739ba1cca110d",
    "user": "Xqle",
    "files": [
      {
        "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
        "status": "modified",
        "additions": 19,
        "deletions": 13,
        "changes": 32,
        "patch": "@@ -1453,8 +1453,6 @@ def forward(\n         Example:\n \n         ```python\n-        >>> from PIL import Image\n-        >>> import requests\n         >>> from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration\n \n         >>> model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\")\n@@ -1464,22 +1462,30 @@ def forward(\n             {\n                 \"role\": \"user\",\n                 \"content\": [\n-                    {\"type\": \"image\"},\n-                    {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n+                    {\n+                        \"type\": \"image\",\n+                        \"image\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\",\n+                    },\n+                    {\"type\": \"text\", \"text\": \"Describe the image.\"},\n                 ],\n-            },\n+            }\n         ]\n-        >>> url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n-        >>> image = Image.open(requests.get(url, stream=True).raw)\n \n-        >>> text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n-        >>> inputs = processor(text=[text], images=[image], vision_infos=[vision_infos])\n+        >>> inputs = processor.apply_chat_template(\n+            messages,\n+            tokenize=True,\n+            add_generation_prompt=True,\n+            return_dict=True,\n+            return_tensors=\"pt\"\n+        )\n \n         >>> # Generate\n-        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n-        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n-        \"The image shows a street scene with a red stop sign in the foreground. In the background, there is a large red gate with Chinese characters ...\"\n-        ```\"\"\"\n+        >>> generated_ids = model.generate(**inputs, max_new_tokens=1024)\n+        >>> generated_ids_trimmed = [out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]\n+        >>> output_text = processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+        >>> print(output_text)\n+        ```\n+        \"\"\"\n \n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = ("
      },
      {
        "filename": "src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py",
        "status": "modified",
        "additions": 19,
        "deletions": 13,
        "changes": 32,
        "patch": "@@ -684,8 +684,6 @@ def forward(\n         Example:\n \n         ```python\n-        >>> from PIL import Image\n-        >>> import requests\n         >>> from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration\n \n         >>> model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\")\n@@ -695,22 +693,30 @@ def forward(\n             {\n                 \"role\": \"user\",\n                 \"content\": [\n-                    {\"type\": \"image\"},\n-                    {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n+                    {\n+                        \"type\": \"image\",\n+                        \"image\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\",\n+                    },\n+                    {\"type\": \"text\", \"text\": \"Describe the image.\"},\n                 ],\n-            },\n+            }\n         ]\n-        >>> url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n-        >>> image = Image.open(requests.get(url, stream=True).raw)\n \n-        >>> text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n-        >>> inputs = processor(text=[text], images=[image], vision_infos=[vision_infos])\n+        >>> inputs = processor.apply_chat_template(\n+            messages,\n+            tokenize=True,\n+            add_generation_prompt=True,\n+            return_dict=True,\n+            return_tensors=\"pt\"\n+        )\n \n         >>> # Generate\n-        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n-        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n-        \"The image shows a street scene with a red stop sign in the foreground. In the background, there is a large red gate with Chinese characters ...\"\n-        ```\"\"\"\n+        >>> generated_ids = model.generate(**inputs, max_new_tokens=1024)\n+        >>> generated_ids_trimmed = [out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]\n+        >>> output_text = processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+        >>> print(output_text)\n+        ```\n+        \"\"\"\n \n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = ("
      },
      {
        "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
        "status": "modified",
        "additions": 19,
        "deletions": 13,
        "changes": 32,
        "patch": "@@ -1348,8 +1348,6 @@ def forward(\n         Example:\n \n         ```python\n-        >>> from PIL import Image\n-        >>> import requests\n         >>> from transformers import AutoProcessor, Qwen2VLForConditionalGeneration\n \n         >>> model = Qwen2VLForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n@@ -1359,22 +1357,30 @@ def forward(\n             {\n                 \"role\": \"user\",\n                 \"content\": [\n-                    {\"type\": \"image\"},\n-                    {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n+                    {\n+                        \"type\": \"image\",\n+                        \"image\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\",\n+                    },\n+                    {\"type\": \"text\", \"text\": \"Describe the image.\"},\n                 ],\n-            },\n+            }\n         ]\n-        >>> url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n-        >>> image = Image.open(requests.get(url, stream=True).raw)\n \n-        >>> text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n-        >>> inputs = processor(text=[text], images=[image], vision_infos=[vision_infos])\n+        >>> inputs = processor.apply_chat_template(\n+            messages,\n+            tokenize=True,\n+            add_generation_prompt=True,\n+            return_dict=True,\n+            return_tensors=\"pt\"\n+        )\n \n         >>> # Generate\n-        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n-        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n-        \"The image shows a street scene with a red stop sign in the foreground. In the background, there is a large red gate with Chinese characters ...\"\n-        ```\"\"\"\n+        >>> generated_ids = model.generate(**inputs, max_new_tokens=1024)\n+        >>> generated_ids_trimmed = [out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]\n+        >>> output_text = processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+        >>> print(output_text)\n+        ```\n+        \"\"\"\n \n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = ("
      },
      {
        "filename": "src/transformers/models/qwen3_vl/modeling_qwen3_vl.py",
        "status": "modified",
        "additions": 35,
        "deletions": 1,
        "changes": 36,
        "patch": "@@ -1369,8 +1369,42 @@ def forward(\n             The temporal, height and width of feature shape of each video in LLM.\n \n         Example:\n-            TODO: Add example\n+\n+        ```python\n+        >>> from transformers import AutoProcessor, Qwen3VLForConditionalGeneration\n+\n+        >>> model = Qwen3VLForConditionalGeneration.from_pretrained(\"Qwen/Qwen3-VL-8B-Instruct\")\n+        >>> processor = AutoProcessor.from_pretrained(\"Qwen/Qwen3-VL-8B-Instruct\")\n+\n+        >>> messages = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\n+                        \"type\": \"image\",\n+                        \"image\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\",\n+                    },\n+                    {\"type\": \"text\", \"text\": \"Describe the image.\"},\n+                ],\n+            }\n+        ]\n+\n+        >>> inputs = processor.apply_chat_template(\n+            messages,\n+            tokenize=True,\n+            add_generation_prompt=True,\n+            return_dict=True,\n+            return_tensors=\"pt\"\n+        )\n+\n+        >>> # Generate\n+        >>> generated_ids = model.generate(**inputs, max_new_tokens=1024)\n+        >>> generated_ids_trimmed = [out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]\n+        >>> output_text = processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+        >>> print(output_text)\n+        ```\n         \"\"\"\n+\n         outputs = self.model(\n             input_ids=input_ids,\n             pixel_values=pixel_values,"
      },
      {
        "filename": "src/transformers/models/qwen3_vl/modular_qwen3_vl.py",
        "status": "modified",
        "additions": 35,
        "deletions": 1,
        "changes": 36,
        "patch": "@@ -1134,8 +1134,42 @@ def forward(\n             The temporal, height and width of feature shape of each video in LLM.\n \n         Example:\n-            TODO: Add example\n+\n+        ```python\n+        >>> from transformers import AutoProcessor, Qwen3VLForConditionalGeneration\n+\n+        >>> model = Qwen3VLForConditionalGeneration.from_pretrained(\"Qwen/Qwen3-VL-8B-Instruct\")\n+        >>> processor = AutoProcessor.from_pretrained(\"Qwen/Qwen3-VL-8B-Instruct\")\n+\n+        >>> messages = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\n+                        \"type\": \"image\",\n+                        \"image\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\",\n+                    },\n+                    {\"type\": \"text\", \"text\": \"Describe the image.\"},\n+                ],\n+            }\n+        ]\n+\n+        >>> inputs = processor.apply_chat_template(\n+            messages,\n+            tokenize=True,\n+            add_generation_prompt=True,\n+            return_dict=True,\n+            return_tensors=\"pt\"\n+        )\n+\n+        >>> # Generate\n+        >>> generated_ids = model.generate(**inputs, max_new_tokens=1024)\n+        >>> generated_ids_trimmed = [out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]\n+        >>> output_text = processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+        >>> print(output_text)\n+        ```\n         \"\"\"\n+\n         outputs = self.model(\n             input_ids=input_ids,\n             pixel_values=pixel_values,"
      }
    ],
    "num_files": 5,
    "scraped_at": "2025-11-16T21:16:50.937248",
    "filter_decision": {
      "accept": false,
      "reasoning": "This PR is purely documentation/example fixing with no code logic changes. It updates docstring examples to be runnable by fixing API calls and removing external dependencies, which is a documentation improvement but lacks substantive technical content that would support meaningful codebase questions.",
      "substance_level": "low"
    }
  },
  {
    "pr_number": 41765,
    "title": "[kernels]\u00a0Add Tests & CI for kernels",
    "body": "# What does this PR do?\r\n\r\nAdds tests for kernels, and proper daily CI, and slack notifications\r\n\r\nrun example : https://github.com/huggingface/transformers/actions/runs/18688016017/job/53285883834",
    "html_url": "https://github.com/huggingface/transformers/pull/41765",
    "created_at": "2025-10-21T11:00:27Z",
    "merged_at": "2025-11-03T15:36:52Z",
    "merge_commit_sha": "a623cda4271c739bd53b874b713d4479a19ff907",
    "base_ref": "main",
    "head_sha": "24ae78111b98586cd7af30fa6961969488a98dd7",
    "user": "MekkCyber",
    "files": [
      {
        "filename": ".github/workflows/self-scheduled-caller.yml",
        "status": "modified",
        "additions": 12,
        "deletions": 0,
        "changes": 12,
        "patch": "@@ -118,3 +118,15 @@ jobs:\n       report_repo_id: hf-internal-testing/transformers_daily_ci\n       commit_sha: ${{ github.sha }}\n     secrets: inherit\n+\n+  kernels-ci:\n+    name: Kernels CI\n+    uses: ./.github/workflows/self-scheduled.yml\n+    with:\n+      job: run_kernels_gpu\n+      slack_report_channel: \"#transformers-ci-daily-kernels\"\n+      docker: huggingface/transformers-all-latest-gpu\n+      ci_event: Daily CI\n+      report_repo_id: hf-internal-testing/transformers_daily_ci\n+      commit_sha: ${{ github.sha }}\n+    secrets: inherit\n\\ No newline at end of file"
      },
      {
        "filename": ".github/workflows/self-scheduled.yml",
        "status": "modified",
        "additions": 65,
        "deletions": 0,
        "changes": 65,
        "patch": "@@ -463,6 +463,70 @@ jobs:\n           name: ${{ env.machine_type }}_run_quantization_torch_gpu_${{ env.matrix_folders }}_test_reports\n           path: /transformers/reports/${{ env.machine_type }}_run_quantization_torch_gpu_${{ matrix.folders }}_test_reports\n \n+  run_kernels_gpu:\n+    if: ${{ inputs.job == 'run_kernels_gpu' }}\n+    name: Kernel tests\n+    strategy:\n+      fail-fast: false\n+      matrix:\n+        machine_type: [aws-g5-4xlarge-cache]\n+    runs-on:\n+      group: '${{ matrix.machine_type }}'\n+    container:\n+      image: ${{ inputs.docker }}\n+      options: --gpus all --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n+    steps:\n+      - name: Update clone\n+        working-directory: /transformers\n+        run: git fetch && git checkout ${{ inputs.commit_sha || github.sha }}\n+\n+      - name: Reinstall transformers in edit mode\n+        working-directory: /transformers\n+        run: python3 -m pip uninstall -y transformers && python3 -m pip install -e .[testing]\n+  \n+      - name: Install kernels\n+        working-directory: /transformers\n+        run: python3 -m pip install -U kernels\n+  \n+      - name: NVIDIA-SMI\n+        run: nvidia-smi\n+\n+      - name: Environment\n+        working-directory: /transformers\n+        run: python3 utils/print_env.py\n+\n+      - name: Show installed libraries and their versions\n+        working-directory: /transformers\n+        run: pip freeze\n+\n+      - name: Set `machine_type` for report and artifact names\n+        working-directory: /transformers\n+        shell: bash\n+        run: |\n+          if [ \"${{ matrix.machine_type }}\" = \"aws-g5-4xlarge-cache\" ]; then\n+            machine_type=single-gpu\n+          else\n+            machine_type=${{ matrix.machine_type }}\n+          fi\n+          echo \"machine_type=$machine_type\" >> $GITHUB_ENV\n+    \n+      - name: Run kernel tests on GPU\n+        working-directory: /transformers\n+        run: |\n+          python3 -m pytest -v --make-reports=${{ env.machine_type }}_run_kernels_gpu_test_reports tests/kernels/test_kernels.py\n+\n+      - name: Failure short reports\n+        if: ${{ failure() }}\n+        continue-on-error: true\n+        run: cat /transformers/reports/${{ env.machine_type }}_run_kernels_gpu_test_reports/failures_short.txt\n+\n+      - name: \"Test suite reports artifacts: ${{ env.machine_type }}_run_kernels_gpu_test_reports\"\n+        if: ${{ always() }}\n+        uses: actions/upload-artifact@v4\n+        with:\n+          name: ${{ env.machine_type }}_run_kernels_gpu_test_reports\n+          path: /transformers/reports/${{ env.machine_type }}_run_kernels_gpu_test_reports\n+\n   run_extract_warnings:\n     # Let's only do this for the job `run_models_gpu` to simplify the (already complex) logic.\n     if: ${{ always() && inputs.job == 'run_models_gpu' }}\n@@ -515,6 +579,7 @@ jobs:\n       run_examples_gpu,\n       run_torch_cuda_extensions_gpu,\n       run_quantization_torch_gpu,\n+      run_kernels_gpu,\n       run_extract_warnings\n     ]\n     if: always() && !cancelled()"
      },
      {
        "filename": "src/transformers/integrations/hub_kernels.py",
        "status": "modified",
        "additions": 7,
        "deletions": 4,
        "changes": 11,
        "patch": "@@ -51,10 +51,13 @@\n             )\n         },\n         \"RMSNorm\": {\n-            \"cuda\": LayerRepository(\n-                repo_id=\"kernels-community/liger_kernels\",\n-                layer_name=\"LigerRMSNorm\",\n-            ),\n+            \"cuda\": {\n+                Mode.INFERENCE: LayerRepository(\n+                    repo_id=\"kernels-community/liger_kernels\",\n+                    layer_name=\"LigerRMSNorm\",\n+                    # revision=\"pure-layer-test\",\n+                ),\n+            },\n             \"rocm\": {\n                 Mode.INFERENCE: LayerRepository(\n                     repo_id=\"kernels-community/liger_kernels\","
      },
      {
        "filename": "tests/kernels/test_kernels.py",
        "status": "added",
        "additions": 403,
        "deletions": 0,
        "changes": 403,
        "patch": "@@ -0,0 +1,403 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+# Run the test: CUDA_VISIBLE_DEVICES=0 RUN_SLOW=1 pytest -sv tests/kernels/test_kernels.py\n+\n+\n+import copy\n+import types\n+from unittest.mock import patch\n+\n+from transformers import AutoModelForCausalLM, AutoTokenizer, KernelConfig\n+from transformers.integrations.hub_kernels import (\n+    _HUB_KERNEL_MAPPING,\n+    _KERNEL_MODULE_MAPPING,\n+    is_kernel,\n+    lazy_load_kernel,\n+    load_and_register_attn_kernel,\n+)\n+from transformers.masking_utils import ALL_MASK_ATTENTION_FUNCTIONS\n+from transformers.modeling_utils import ALL_ATTENTION_FUNCTIONS\n+from transformers.testing_utils import (\n+    TestCasePlus,\n+    cleanup,\n+    require_kernels,\n+    require_torch_accelerator,\n+    slow,\n+    torch_device,\n+)\n+from transformers.utils.import_utils import is_kernels_available\n+\n+\n+if is_kernels_available():\n+    import kernels as kernels_pkg\n+    from kernels import Device, Mode, kernelize\n+\n+\n+@require_kernels\n+@slow\n+class TestHubKernels(TestCasePlus):\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.model_id = \"unsloth/Llama-3.2-1B-Instruct\"\n+        cls.tokenizer = AutoTokenizer.from_pretrained(cls.model_id)\n+        cls.model_kernelized = AutoModelForCausalLM.from_pretrained(\n+            cls.model_id, use_kernels=True, device_map=torch_device\n+        )\n+        cls.model_not_kernelized = AutoModelForCausalLM.from_pretrained(\n+            cls.model_id, use_kernels=False, device_map=torch_device\n+        )\n+        cls.input = \"Hello\"\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        for attr in [\n+            \"model_kernelized\",\n+            \"model_not_kernelized\",\n+            \"tokenizer\",\n+        ]:\n+            if hasattr(cls, attr):\n+                try:\n+                    delattr(cls, attr)\n+                except Exception:\n+                    pass\n+\n+        # Clear any temporary kernel module cache entries populated by tests\n+        try:\n+            keys_to_remove = [\n+                k for k, v in list(_KERNEL_MODULE_MAPPING.items()) if v is None or isinstance(v, types.ModuleType)\n+            ]\n+            for k in keys_to_remove:\n+                _KERNEL_MODULE_MAPPING.pop(k, None)\n+        except Exception:\n+            pass\n+\n+    def tearDown(self):\n+        # Free accelerator memory/cache and trigger GC\n+        cleanup(torch_device, gc_collect=True)\n+\n+    @require_torch_accelerator\n+    def test_forward(self):\n+        tokenized_input = self.tokenizer(self.input, return_tensors=\"pt\").input_ids.to(self.model_kernelized.device)\n+        output_ = self.model_kernelized.generate(tokenized_input, max_new_tokens=10, do_sample=False)\n+        output = self.tokenizer.decode(output_[0], skip_special_tokens=True)\n+\n+        self.EXPECTED_OUTPUT = set()\n+        self.EXPECTED_OUTPUT.add(\"Hello, I'm looking for a reliable and trustworthy online\")\n+\n+        self.assertTrue(output in self.EXPECTED_OUTPUT)\n+\n+    def test_getter_use_kernels(self):\n+        self.assertTrue(self.model_kernelized.use_kernels)\n+        self.assertFalse(self.model_not_kernelized.use_kernels)\n+\n+    def assert_kernelized_forward_is_different(self, kernelized_model, not_kernelized_model):\n+        \"\"\"\n+        Iterate over modules and check if the forward method is different between\n+        the kernelized and not kernelized models. Break on first difference, else continue.\n+        Finally, assert that at least one forward is different.\n+        \"\"\"\n+        found_difference = False\n+        for (name1, module1), (name2, module2) in zip(\n+            kernelized_model.named_modules(), not_kernelized_model.named_modules()\n+        ):\n+            # Only compare modules with the same name\n+            if name1 != name2:\n+                continue\n+            # Check if both modules have a 'forward' attribute\n+            if hasattr(module1, \"forward\") and hasattr(module2, \"forward\"):\n+                # Compare the code objects of the forward methods\n+                code1 = getattr(module1.forward, \"__code__\", None)\n+                code2 = getattr(module2.forward, \"__code__\", None)\n+                if code1 is not None and code2 is not None:\n+                    if code1 is not code2:\n+                        found_difference = True\n+                        break\n+        self.assertTrue(\n+            found_difference,\n+            \"No module's forward method was different between kernelized and not kernelized models.\",\n+        )\n+\n+    def assert_kernelized_forward_is_the_same(self, model_1, model_2):\n+        \"\"\"\n+        Iterate over modules and check if the forward method is the same between\n+        the kernelized and not kernelized models. Break on first difference, else continue.\n+        Finally, assert that at least one forward is the same.\n+        \"\"\"\n+        no_difference = True\n+        for (name1, module1), (name2, module2) in zip(model_1.named_modules(), model_2.named_modules()):\n+            # Only compare modules with the same name\n+            if name1 != name2:\n+                continue\n+            # Check if both modules have a 'forward' attribute\n+            if hasattr(module1, \"forward\") and hasattr(module2, \"forward\"):\n+                # Compare the code objects of the forward methods\n+                code1 = getattr(module1.forward, \"__code__\", None)\n+                code2 = getattr(module2.forward, \"__code__\", None)\n+                if code1 is not None and code2 is not None:\n+                    if code1 != code2:\n+                        no_difference = False\n+                        break\n+        self.assertTrue(\n+            no_difference,\n+            \"All module's forward methods were the same between the two models\",\n+        )\n+\n+    def test_kernelize(self):\n+        model = copy.deepcopy(self.model_not_kernelized)\n+        kernelize(model, mode=Mode.INFERENCE, device=Device(type=model.device.type))  # type: ignore[arg-type]\n+        self.assert_kernelized_forward_is_different(model, self.model_not_kernelized)\n+        self.assert_kernelized_forward_is_the_same(model, self.model_kernelized)\n+        del model\n+\n+    def test_setter_use_kernels(self):\n+        model = copy.deepcopy(self.model_not_kernelized)\n+        model.use_kernels = True\n+        self.assertTrue(model.use_kernels)\n+        self.assert_kernelized_forward_is_different(model, self.model_not_kernelized)\n+        self.assert_kernelized_forward_is_the_same(model, self.model_kernelized)\n+        del model\n+\n+    def test_unkernelize(self):\n+        model = copy.deepcopy(self.model_kernelized)\n+\n+        with self.assertLogs(\"transformers.modeling_utils\", level=\"WARNING\") as cm:\n+            model.use_kernels = False\n+\n+        self.assertTrue(\n+            any(\n+                \"Disabling kernels at runtime is a no-op as there is no 'unkernelize' routine; keeping current kernels active.\"\n+                in msg\n+                for msg in cm.output\n+            )\n+        )\n+\n+        self.assertFalse(model.use_kernels)\n+        del model\n+\n+    def test_kernels_mapping(self):\n+        kernel_config = KernelConfig(kernel_mapping={\"RMSNorm\": \"kernels-community/layer_norm:LlamaRMSNorm\"})\n+        model = AutoModelForCausalLM.from_pretrained(\n+            \"unsloth/Llama-3.2-1B-Instruct\", use_kernels=True, device_map=torch_device, kernel_config=kernel_config\n+        )\n+\n+        EXPECTED_OUTPUT = set()\n+        EXPECTED_OUTPUT.add(\"Hello, I'm looking for a reliable and trustworthy online\")\n+\n+        tokenized_input = self.tokenizer(self.input, return_tensors=\"pt\").input_ids.to(model.device)\n+        output = model.generate(tokenized_input, max_new_tokens=10, do_sample=False)\n+        output = self.tokenizer.decode(output[0], skip_special_tokens=True)\n+        self.assertTrue(output in EXPECTED_OUTPUT)\n+\n+        del model\n+\n+    def test_faulty_kernel_mapping_layer_name(self):\n+        kernel_config = KernelConfig(kernel_mapping={\"RMSNorm1\": \"kernels-community/layer_norm:LlamaRMSNorm\"})\n+        with self.assertRaises(ValueError):\n+            _ = AutoModelForCausalLM.from_pretrained(\n+                \"unsloth/Llama-3.2-1B-Instruct\", use_kernels=True, device_map=torch_device, kernel_config=kernel_config\n+            )\n+\n+    def test_faulty_kernel_mapping_type(self):\n+        kernel_config = KernelConfig(kernel_mapping={\"RMSNorm\": 1})\n+        with self.assertRaises(ValueError):\n+            _ = AutoModelForCausalLM.from_pretrained(\n+                \"unsloth/Llama-3.2-1B-Instruct\", use_kernels=True, device_map=torch_device, kernel_config=kernel_config\n+            )\n+\n+\n+@require_kernels\n+class TestKernelUtilities(TestCasePlus):\n+    def test_is_kernel_regex(self):\n+        valid = [\n+            \"org/model\",\n+            \"org/model@main\",\n+            \"org/model:my_func\",\n+            \"org/model@v1.2.3:my_func\",\n+            \"flash|org/model@rev:fn\",\n+        ]\n+        invalid = [\n+            \"org//model\",\n+            \"org/model:too:many\",\n+            \"org/model@rev:fn:extra\",\n+            \"/org/model\",\n+            \"org:model\",\n+        ]\n+        for s in valid:\n+            self.assertTrue(is_kernel(s.split(\"|\")[-1]))\n+        for s in invalid:\n+            self.assertFalse(is_kernel(s))\n+\n+    def test_lazy_load_kernel_success_and_cache(self):\n+        sentinel = types.SimpleNamespace(name=\"sentinel\")\n+\n+        original_get_kernel = getattr(kernels_pkg, \"get_kernel\")\n+        try:\n+\n+            def fake_get_kernel(repo_id, revision=None, version=None):\n+                self.assertIn(repo_id, {\"kernels-community/causal-conv1d\"})\n+                return sentinel\n+\n+            setattr(kernels_pkg, \"get_kernel\", fake_get_kernel)\n+            _KERNEL_MODULE_MAPPING.pop(\"causal-conv1d\", None)\n+\n+            mod1 = lazy_load_kernel(\"causal-conv1d\")\n+            self.assertIs(mod1, sentinel)\n+            mod2 = lazy_load_kernel(\"causal-conv1d\")\n+            self.assertIs(mod2, sentinel)\n+        finally:\n+            setattr(kernels_pkg, \"get_kernel\", original_get_kernel)\n+            # Ensure cache is cleared to avoid holding onto module references across tests\n+            _KERNEL_MODULE_MAPPING.pop(\"causal-conv1d\", None)\n+\n+    def test_lazy_load_kernel_unknown(self):\n+        name = \"unknown-kernel-name\"\n+        _KERNEL_MODULE_MAPPING.pop(name, None)\n+        mod = lazy_load_kernel(name)\n+        self.assertIsNone(mod)\n+        self.assertIn(name, _KERNEL_MODULE_MAPPING)\n+        # Cleanup cache entry to avoid growth across tests\n+        _KERNEL_MODULE_MAPPING.pop(name, None)\n+\n+    def test_lazy_load_kernel_version(self):\n+        HUB = _HUB_KERNEL_MAPPING\n+        name = \"causal-conv1d\"\n+        version_spec = \">=0.0.4,<0.1.0\"\n+        original_get_kernel = getattr(kernels_pkg, \"get_kernel\")\n+        original_entry = HUB.get(name, None)\n+\n+        # Use a real ModuleType so caching short-circuits on the second call\n+        sentinel_mod = types.ModuleType(\"sentinel_kernel_module\")\n+        call_count = {\"n\": 0}\n+\n+        try:\n+            # Inject dict-style mapping with repo_id and version\n+            HUB[name] = {\"repo_id\": \"kernels-community/causal-conv1d\", \"version\": version_spec}  # type: ignore[assignment]\n+            _KERNEL_MODULE_MAPPING.pop(name, None)\n+\n+            def fake_get_kernel(repo_id, revision=None, version=None, user_agent=None):\n+                call_count[\"n\"] += 1\n+                self.assertEqual(repo_id, \"kernels-community/causal-conv1d\")\n+                self.assertIsNone(revision, \"revision must not be set when version is provided\")\n+                self.assertEqual(version, version_spec)\n+                return sentinel_mod\n+\n+            # Patch kernels.get_kernel so lazy_load_kernel picks it up on import\n+            setattr(kernels_pkg, \"get_kernel\", fake_get_kernel)\n+\n+            # Act\n+            mod1 = lazy_load_kernel(name)\n+            mod2 = lazy_load_kernel(name)\n+\n+            # Assert\n+            self.assertIs(mod1, sentinel_mod)\n+            self.assertIs(mod2, sentinel_mod)\n+            self.assertEqual(call_count[\"n\"], 1, \"second call should hit the cache\")\n+        finally:\n+            # Restore patched function and mapping to avoid side effects\n+            setattr(kernels_pkg, \"get_kernel\", original_get_kernel)\n+            if original_entry is None:\n+                HUB.pop(name, None)\n+            else:\n+                HUB[name] = original_entry\n+            _KERNEL_MODULE_MAPPING.pop(name, None)\n+\n+\n+@require_kernels\n+class TestAttentionKernelRegistration(TestCasePlus):\n+    def test_load_and_register_flash_attn_like_kernel(self):\n+        kernel_obj = types.SimpleNamespace(flash_attn_varlen_func=lambda *a, **k: None)\n+\n+        with (\n+            patch(\"transformers.integrations.hub_kernels.get_kernel\", return_value=kernel_obj),\n+            patch(\"transformers.integrations.hub_kernels.lazy_import_flash_attention\", return_value=None),\n+        ):\n+            attn_impl = \"org/model\"\n+            load_and_register_attn_kernel(attn_impl)\n+            self.assertIn(attn_impl, ALL_ATTENTION_FUNCTIONS.valid_keys())\n+            # Cleanup registration to avoid leaking functions across tests\n+            try:\n+                ALL_ATTENTION_FUNCTIONS.pop(attn_impl, None)\n+            except Exception:\n+                pass\n+            try:\n+                ALL_MASK_ATTENTION_FUNCTIONS.pop(attn_impl, None)\n+            except Exception:\n+                pass\n+\n+    def test_load_and_register_named_function_kernel(self):\n+        def my_attention(*args, **kwargs):\n+            return None\n+\n+        kernel_obj = types.SimpleNamespace(my_func=my_attention)\n+        with patch(\"transformers.integrations.hub_kernels.get_kernel\", return_value=kernel_obj):\n+            attn_impl = \"org/model:my_func\"\n+            load_and_register_attn_kernel(attn_impl)\n+            self.assertIn(attn_impl, ALL_ATTENTION_FUNCTIONS.valid_keys())\n+            # Cleanup registration to avoid leaking functions across tests\n+            try:\n+                ALL_ATTENTION_FUNCTIONS.pop(attn_impl, None)\n+            except Exception:\n+                pass\n+            try:\n+                ALL_MASK_ATTENTION_FUNCTIONS.pop(attn_impl, None)\n+            except Exception:\n+                pass\n+\n+\n+@require_kernels\n+class TestUseKernelsLifecycle(TestCasePlus):\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.model_id = \"unsloth/Llama-3.2-1B-Instruct\"\n+        cls.model = AutoModelForCausalLM.from_pretrained(cls.model_id, use_kernels=False, device_map=torch_device)\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        # Delete large objects to drop references early\n+        if hasattr(cls, \"model\"):\n+            try:\n+                del cls.model\n+            except Exception:\n+                pass\n+\n+    def tearDown(self):\n+        # Free accelerator memory/cache and trigger GC\n+        cleanup(torch_device, gc_collect=True)\n+\n+    def test_setting_use_kernels_twice_does_not_rekernelize(self):\n+        call_count = {\"n\": 0}\n+\n+        def spy_kernelize(*args, **kwargs):\n+            call_count[\"n\"] += 1\n+\n+        with patch.object(kernels_pkg, \"kernelize\", side_effect=spy_kernelize):\n+            self.model.use_kernels = True\n+            self.assertTrue(self.model.use_kernels)\n+            self.assertEqual(call_count[\"n\"], 1)\n+            self.model.use_kernels = True\n+            self.assertEqual(call_count[\"n\"], 1)\n+\n+    def test_train_eval_calls_kernelize_with_correct_mode(self):\n+        last_modes = []\n+\n+        def spy_kernelize(model, device=None, mode=None):\n+            last_modes.append(mode)\n+\n+        with patch.object(kernels_pkg, \"kernelize\", side_effect=spy_kernelize):\n+            self.model.use_kernels = True\n+            self.model.train(True)\n+            self.assertTrue(any(m == Mode.TRAINING for m in last_modes))\n+            self.model.eval()\n+            self.assertTrue(any(m == Mode.INFERENCE for m in last_modes))"
      },
      {
        "filename": "utils/notification_service.py",
        "status": "modified",
        "additions": 4,
        "deletions": 0,
        "changes": 4,
        "patch": "@@ -40,6 +40,7 @@\n     \"run_examples_gpu\": \"Examples directory\",\n     \"run_torch_cuda_extensions_gpu\": \"DeepSpeed\",\n     \"run_quantization_torch_gpu\": \"Quantization\",\n+    \"run_kernels_gpu\": \"Kernels\",\n }\n \n # The values are used as the file names where to save the corresponding CI job results.\n@@ -50,6 +51,7 @@\n     \"Examples directory\": \"example\",\n     \"DeepSpeed\": \"deepspeed\",\n     \"Quantization\": \"quantization\",\n+    \"Kernels\": \"kernels\",\n }\n \n NON_MODEL_TEST_MODULES = [\n@@ -65,6 +67,7 @@\n     \"utils\",\n     \"fsdp\",\n     \"quantization\",\n+    \"kernels\",\n ]\n \n \n@@ -1301,6 +1304,7 @@ def pop_default(l: list[Any], i: int, default: Any) -> Any:\n         \"PyTorch pipelines\": \"run_pipelines_torch_gpu_test_reports\",\n         \"Examples directory\": \"run_examples_gpu_test_reports\",\n         \"DeepSpeed\": \"run_torch_cuda_extensions_gpu_test_reports\",\n+        \"Kernels\": \"run_kernels_gpu_test_reports\",\n     }\n \n     if ci_event in [\"push\", \"Nightly CI\"] or ci_event.startswith(\"Past CI\"):"
      }
    ],
    "num_files": 5,
    "scraped_at": "2025-11-16T21:16:59.715865",
    "filter_decision": {
      "accept": false,
      "reasoning": "This PR is primarily infrastructure and configuration work: adding CI/CD workflows, test registration, and notification mappings. While it includes a new test file, the changes are largely non-substantive configuration additions (workflow YAML, notification service mappings) with minimal logic changes. The actual kernel integration logic modification is trivial (restructuring a data structure to add a Mode key).",
      "substance_level": "low"
    }
  },
  {
    "pr_number": 41691,
    "title": "Remove skipped tests without parents",
    "body": "# What does this PR do?\r\n\r\nTensorflow tests that were still present! And a function not used anymore after https://github.com/huggingface/transformers/pull/41683 and https://github.com/huggingface/transformers/pull/41688\r\n",
    "html_url": "https://github.com/huggingface/transformers/pull/41691",
    "created_at": "2025-10-17T14:18:30Z",
    "merged_at": "2025-10-17T14:25:40Z",
    "merge_commit_sha": "39b6d3bf7e30b92b2de50a31ee991557d41ab568",
    "base_ref": "main",
    "head_sha": "bede5c6954f1d7777b9b1e9ccc1d6513fc49bcce",
    "user": "Cyrilvallez",
    "files": [
      {
        "filename": "tests/models/mpnet/test_modeling_mpnet.py",
        "status": "modified",
        "additions": 0,
        "deletions": 4,
        "changes": 4,
        "patch": "@@ -242,10 +242,6 @@ def test_for_question_answering(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_mpnet_for_question_answering(*config_and_inputs)\n \n-    @unittest.skip(reason=\"TFMPNet adds poolers to all models, unlike the PT model class.\")\n-    def test_tf_from_pt_safetensors(self):\n-        return\n-\n \n @require_torch\n class MPNetModelIntegrationTest(unittest.TestCase):"
      },
      {
        "filename": "tests/models/speech_to_text/test_modeling_speech_to_text.py",
        "status": "modified",
        "additions": 0,
        "deletions": 4,
        "changes": 4,
        "patch": "@@ -600,10 +600,6 @@ def test_resize_embeddings_untied(self):\n     def test_generate_without_input_ids(self):\n         pass\n \n-    @unittest.skip(reason=\"Test failing,  @RocketNight is looking into it\")\n-    def test_tf_from_pt_safetensors(self):\n-        pass\n-\n \n @require_torch\n @require_torchaudio"
      },
      {
        "filename": "tests/models/tapas/test_modeling_tapas.py",
        "status": "modified",
        "additions": 0,
        "deletions": 4,
        "changes": 4,
        "patch": "@@ -520,10 +520,6 @@ def test_for_sequence_classification(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_for_sequence_classification(*config_and_inputs)\n \n-    @unittest.skip(reason=\"tfp is not defined even if installed. FIXME @Arthur in a followup PR!\")\n-    def test_tf_from_pt_safetensors(self):\n-        pass\n-\n \n def prepare_tapas_single_inputs_for_inference():\n     # Here we prepare a single table-question pair to test TAPAS inference on:"
      },
      {
        "filename": "tests/test_modeling_common.py",
        "status": "modified",
        "additions": 0,
        "deletions": 8,
        "changes": 8,
        "patch": "@@ -1344,14 +1344,6 @@ def test_attention_outputs(self):\n                     [self.model_tester.num_attention_heads, encoder_seq_length, encoder_key_length],\n                 )\n \n-    # This is copied from `torch/testing/_internal/jit_utils.py::clear_class_registry`\n-    def clear_torch_jit_class_registry(self):\n-        torch._C._jit_clear_class_registry()\n-        torch.jit._recursive.concrete_type_store = torch.jit._recursive.ConcreteTypeStore()\n-        # torch 1.8 has no `_clear_class_state` in `torch.jit._state`\n-        if hasattr(torch.jit._state, \"_clear_class_state\"):\n-            torch.jit._state._clear_class_state()\n-\n     def test_hidden_states_output(self):\n         def check_hidden_states_output(inputs_dict, config, model_class):\n             model = model_class(copy.deepcopy(config))"
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T21:17:10.991250",
    "filter_decision": {
      "accept": false,
      "reasoning": "This PR only removes skipped tests and an unused utility function without adding any new logic, features, or architectural changes. It's purely cleanup/deletion work that provides no substantive technical content for generating meaningful codebase questions.",
      "substance_level": "low"
    }
  },
  {
    "pr_number": 41662,
    "title": "Small changes to benchmarking script",
    "body": "This PR:\r\n- adds throughput to the final pretty print at the end of the benchmark runs\r\n- adds the information of the output shape to the decoded text of the output (usefull to double check if throughput is right)\r\n- changes the behavior of the `run_benchmarks.py` script so that it will only run 3 configs unless instructed to do otherwise\r\n\r\nThis way, anyone can run a \"quick\"\r\n```\r\npython benchmark_v2/run_benchmarks.py --model-id \"meta-llama/Meta-Llama-3-8B\" -b 32 -s 128 -n 256\r\n```\r\nafter changing the `generate` code or some model specific code to check perf did not take a big hit. \r\nThanks @SunMarc for the suggestion!",
    "html_url": "https://github.com/huggingface/transformers/pull/41662",
    "created_at": "2025-10-16T14:19:13Z",
    "merged_at": "2025-10-16T15:25:49Z",
    "merge_commit_sha": "f7c33abab3a6233a51d7d4fd116625be14df68ff",
    "base_ref": "main",
    "head_sha": "ed5f5fef324b63eb093fb551fb482a1afd54afcb",
    "user": "remi-or",
    "files": [
      {
        "filename": "benchmark_v2/framework/benchmark_config.py",
        "status": "modified",
        "additions": 15,
        "deletions": 18,
        "changes": 33,
        "patch": "@@ -104,7 +104,7 @@ def to_dict(self) -> dict[str, Any]:\n             \"attn_implementation\": self.attn_implementation,\n             \"sdpa_backend\": self.sdpa_backend,\n             \"compile_mode\": self.compile_mode,\n-            \"compile_options\": self.compile_options,\n+            \"compile_options\": self.compile_options | {},  # to avoid inplace modification of the original dict\n             \"kernelize\": self.kernelize,\n         }\n \n@@ -191,28 +191,25 @@ def generate_all_configs(\n     )\n \n \n-def generate_default_configs(\n+def generate_main_configs(\n     warmup_iterations: int = 5,\n     measurement_iterations: int = 20,\n     batch_size: int = 1,\n     sequence_length: int = 128,\n     num_tokens_to_generate: int = 128,\n     gpu_monitoring: bool = False,\n ) -> list[BenchmarkConfig]:\n-    all_attn_implementations = [\n-        (\"flash_attention_2\", None),\n-        (\"eager\", None),\n-        (\"sdpa\", \"math\"),\n-        (\"sdpa\", \"flash_attention\"),  # note: this one can fail with compile because of attn mask\n+    # Create kwargs common to all configs\n+    kwargs = {\n+        \"warmup_iterations\": warmup_iterations,\n+        \"measurement_iterations\": measurement_iterations,\n+        \"batch_size\": batch_size,\n+        \"sequence_length\": sequence_length,\n+        \"num_tokens_to_generate\": num_tokens_to_generate,\n+        \"gpu_monitoring\": gpu_monitoring,\n+    }\n+    return [  # TODO: test max-autotune instead of default\n+        BenchmarkConfig(attn_implementation=\"flex_attention\", compile_mode=\"default\", **kwargs),\n+        BenchmarkConfig(attn_implementation=\"eager\", compile_mode=\"default\", **kwargs),\n+        BenchmarkConfig(attn_implementation=\"flash_attention_2\", **kwargs),\n     ]\n-    return cross_generate_configs(\n-        attn_impl_and_sdpa_backend=all_attn_implementations,\n-        compiled_mode=[None, \"max-autotune\"],\n-        kernelized=[False, KERNELIZATION_AVAILABLE],\n-        warmup_iterations=warmup_iterations,\n-        measurement_iterations=measurement_iterations,\n-        batch_size=batch_size,\n-        sequence_length=sequence_length,\n-        num_tokens_to_generate=num_tokens_to_generate,\n-        gpu_monitoring=gpu_monitoring,\n-    )"
      },
      {
        "filename": "benchmark_v2/framework/benchmark_runner.py",
        "status": "modified",
        "additions": 11,
        "deletions": 10,
        "changes": 21,
        "patch": "@@ -144,11 +144,11 @@ def __next__(self):\n class BenchmarkRunner:\n     \"\"\"Main benchmark runner that coordinates benchmark execution.\"\"\"\n \n-    def __init__(\n-        self, logger: logging.Logger, output_dir: str = \"benchmark_results\", commit_id: str | None = None\n-    ) -> None:\n+    def __init__(self, logger: logging.Logger, output_dir: str | None = None, commit_id: str | None = None) -> None:\n         # Those stay constant for the whole run\n         self.logger = logger\n+        if output_dir is None:\n+            output_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), \"benchmark_results\")\n         self.output_dir = output_dir\n         self.commit_id = get_git_revision() if commit_id is None else commit_id\n         os.makedirs(self.output_dir, exist_ok=True)\n@@ -214,7 +214,7 @@ def run_one_benchmark(self, model_id: str, config: BenchmarkConfig, num_tokens_t\n \n             # Quick validation: try one measurement first to see if this scenario works\n             flush_memory()\n-            e2e_latency, token_generation_times, decoded_output, gpu_metrics = self.time_generate(\n+            e2e_latency, token_generation_times, shape_and_decoded_output, gpu_metrics = self.time_generate(\n                 max_new_tokens=1, gpu_monitor=None\n             )\n             if e2e_latency < 0:\n@@ -231,11 +231,11 @@ def run_one_benchmark(self, model_id: str, config: BenchmarkConfig, num_tokens_t\n             result = BenchmarkResult()\n             self.logger.info(f\"Benchmarking with {config.measurement_iterations} iterations.\")\n             for _ in trange(config.measurement_iterations):\n-                e2e_latency, token_generation_times, decoded_output, gpu_metrics = self.time_generate(\n+                e2e_latency, token_generation_times, shape_and_decoded_output, gpu_metrics = self.time_generate(\n                     max_new_tokens=config.num_tokens_to_generate,\n                     gpu_monitor=(GPUMonitor(logger=self.logger) if config.gpu_monitoring else None),\n                 )\n-                result.accumulate(e2e_latency, token_generation_times, decoded_output, gpu_metrics)\n+                result.accumulate(e2e_latency, token_generation_times, shape_and_decoded_output, gpu_metrics)\n             self.logger.info(\"Benchmarking done. Cleaning up.\")\n \n             # Profile if needed\n@@ -277,10 +277,11 @@ def time_generate(\n             raise RuntimeError(f\"Generated {new_tokens} tokens, expected {max_new_tokens}\")\n         # Decode outputs\n         decoded_output = self.tokenizer.decode(outputs[0, input_tokens:], skip_special_tokens=True)\n+        shape_and_decoded_output = f\"{tuple(outputs.shape)} | {decoded_output}\"\n         # Compute intermediate quantities\n         e2e_latency = wall_time_1 - wall_time_0\n         token_generation_times = [t - wall_time_0 for t in streamer.timestamps[1:]]\n-        return e2e_latency, token_generation_times, decoded_output, gpu_metrics\n+        return e2e_latency, token_generation_times, shape_and_decoded_output, gpu_metrics\n \n     def profile_generate(self, num_tokens_to_profile: int, config_name: str) -> None:\n         \"\"\"Profile the latency of a call to model.generate() with the given (inputs) and (max_new_tokens).\"\"\"\n@@ -351,10 +352,10 @@ def run_benchmarks(\n                 first_metadata = all_results[first_key][\"metadata\"].to_dict()\n                 hardware_info = first_metadata.pop(\"hardware_info\")\n                 pretty_print_dict(first_metadata | hardware_info, tabs=1)\n-            for value in all_results.values():\n+            for result in all_results.values():\n                 print(\"=\" * 100)\n-                print(f\"Config: {value['config'].infer_name(compact=False)}\\n\")\n-                value[\"measurements\"].pprint(tabs=1)\n+                print(f\"Config: {result['config'].infer_name(compact=False)}\\n\")\n+                result[\"measurements\"].pprint(batch_size=result[\"config\"].batch_size, tabs=1)\n             print(\"=\" * 100)\n \n         return all_results"
      },
      {
        "filename": "benchmark_v2/framework/data_classes.py",
        "status": "modified",
        "additions": 29,
        "deletions": 21,
        "changes": 50,
        "patch": "@@ -82,19 +82,19 @@ class BenchmarkResult:\n     def __init__(self) -> None:\n         self.e2e_latency = []\n         self.token_generation_times = []  # time at which each token was generated (relative to start of the generation)\n-        self.decoded_outputs = []\n+        self.shape_and_decoded_outputs = []\n         self.gpu_metrics = []\n \n     def accumulate(\n         self,\n         e2e_latency: float,\n         token_generation_times: list[float],\n-        decoded_output: str,\n+        shape_and_decoded_output: str,\n         gpu_metrics: GPURawMetrics | None,\n     ) -> None:\n         self.e2e_latency.append(e2e_latency)\n         self.token_generation_times.append(token_generation_times)\n-        self.decoded_outputs.append(decoded_output)\n+        self.shape_and_decoded_outputs.append(shape_and_decoded_output)\n         self.gpu_metrics.append(gpu_metrics)\n \n     def to_dict(self) -> dict[str, None | int | float]:\n@@ -106,7 +106,7 @@ def to_dict(self) -> dict[str, None | int | float]:\n         return {\n             \"e2e_latency\": self.e2e_latency,\n             \"token_generation_times\": self.token_generation_times,\n-            \"decoded_outputs\": self.decoded_outputs,\n+            \"shape_and_decoded_outputs\": self.shape_and_decoded_outputs,\n             \"gpu_metrics\": gpu_metrics,\n         }\n \n@@ -123,7 +123,7 @@ def from_dict(cls, data: dict[str, None | int | float]) -> \"BenchmarkResult\":\n             new_instance.accumulate(\n                 e2e_latency=data[\"e2e_latency\"][i],\n                 token_generation_times=data[\"token_generation_times\"][i],\n-                decoded_output=data[\"decoded_output\"][i],\n+                shape_and_decoded_output=data[\"shape_and_decoded_outputs\"][i],\n                 gpu_metrics=gpu_metrics[i],\n             )\n         return new_instance\n@@ -134,19 +134,27 @@ def get_measured_ttft(self) -> list[float]:\n     def get_measured_itl(self) -> list[float]:\n         return [(dt[-1] - dt[0]) / (len(dt) - 1) for dt in self.token_generation_times if len(dt) > 1]\n \n-    def pprint(self, tabs: int = 0) -> None:\n-        collated_stats = equalize_lengths_and_collate(\n-            [\n-                add_unit_to_duration(compute_basic_statistics(self.e2e_latency)),\n-                add_unit_to_duration(compute_basic_statistics(self.get_measured_ttft())),\n-                add_unit_to_duration(compute_basic_statistics(self.get_measured_itl())),\n-            ]\n-        )\n-        pretty_print_dict(\n-            {\n-                \"E2E Latency\": collated_stats[0],\n-                \"Time to First Token\": collated_stats[1],\n-                \"Inter-Token Latency\": collated_stats[2],\n-            },\n-            tabs=tabs,\n-        )\n+    def get_throughput(self, batch_size: int) -> float:\n+        return [\n+            batch_size * len(dt) / e2e_latency\n+            for e2e_latency, dt in zip(self.e2e_latency, self.token_generation_times)\n+        ]\n+\n+    def pprint(self, batch_size: int = 0, tabs: int = 0) -> None:\n+        stats_to_collate = [\n+            add_unit_to_duration(compute_basic_statistics(self.e2e_latency)),\n+            add_unit_to_duration(compute_basic_statistics(self.get_measured_ttft())),\n+            add_unit_to_duration(compute_basic_statistics(self.get_measured_itl())),\n+        ]\n+        if batch_size > 0:\n+            throughput_stats = compute_basic_statistics(self.get_throughput(batch_size))\n+            stats_to_collate.append({key: f\"{value:.2f}tok/s\" for key, value in throughput_stats.items()})\n+        collated_stats = equalize_lengths_and_collate(stats_to_collate)\n+        dict_to_pprint = {\n+            \"E2E Latency\": collated_stats[0],\n+            \"Time to First Token\": collated_stats[1],\n+            \"Inter-Token Latency\": collated_stats[2],\n+        }\n+        if batch_size > 0:\n+            dict_to_pprint[\"Throughput\"] = collated_stats[3]\n+        pretty_print_dict(dict_to_pprint, tabs=tabs)"
      },
      {
        "filename": "benchmark_v2/run_benchmarks.py",
        "status": "modified",
        "additions": 33,
        "deletions": 28,
        "changes": 61,
        "patch": "@@ -20,28 +20,28 @@\n \n import argparse\n import logging\n-import random\n import sys\n import uuid\n \n-from framework.benchmark_config import BenchmarkConfig, generate_all_configs\n+from framework.benchmark_config import BenchmarkConfig, generate_all_configs, generate_main_configs\n from framework.benchmark_runner import BenchmarkRunner\n \n \n if __name__ == \"__main__\":\n     # Parse arguments\n     parser = argparse.ArgumentParser()\n-    parser.add_argument(\"--output-dir\", type=str, default=\"benchmark_results\", help=\"Output dir for benchmark results\")\n+    parser.add_argument(\"--output-dir\", type=str, default=None, help=\"Output dir for benchmark results\")\n     parser.add_argument(\"--log-level\", type=str, choices=[\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\"], default=\"INFO\")\n     parser.add_argument(\"--model-id\", type=str, help=\"Specific model ID to benchmark (if supported by benchmarks)\")\n \n-    parser.add_argument(\"--warmup\", type=int, default=5, help=\"Number of warmup iterations\")\n-    parser.add_argument(\"--iterations\", type=int, default=20, help=\"Number of measurement iterations\")\n+    parser.add_argument(\"--warmup\", type=int, default=3, help=\"Number of warmup iterations\")\n+    parser.add_argument(\"--iterations\", type=int, default=10, help=\"Number of measurement iterations\")\n \n     parser.add_argument(\"--batch-size\", \"-b\", type=int, nargs=\"+\", help=\"Batch size\")\n     parser.add_argument(\"--sequence-length\", \"-s\", type=int, nargs=\"+\", help=\"Sequence length\")\n     parser.add_argument(\"--num-tokens-to-generate\", \"-n\", type=int, nargs=\"+\", help=\"Number of tokens to generate\")\n \n+    parser.add_argument(\"--cross-generate\", action=\"store_true\", help=\"Cross-generate all combinations of configs\")\n     parser.add_argument(\"--num-tokens-to-profile\", \"-p\", type=int, default=0, help=\"Number of tokens to profile\")\n \n     parser.add_argument(\"--commit-id\", type=str, help=\"Git commit ID (if not provided, will auto-detect from git)\")\n@@ -69,42 +69,47 @@\n \n     # If there is only one (batch_size, sequence_length, num_tokens_to_generate), we benchmark across configs\n     elif len(args.batch_size) * len(args.sequence_length) * len(args.num_tokens_to_generate) == 1:\n-        benchmark_configs = generate_all_configs(\n+        if args.cross_generate:\n+            benchmark_configs = generate_all_configs(\n+                warmup_iterations=args.warmup,\n+                measurement_iterations=args.iterations,\n+                batch_size=args.batch_size[0],\n+                sequence_length=args.sequence_length[0],\n+                num_tokens_to_generate=args.num_tokens_to_generate[0],\n+            )\n+        else:\n+            benchmark_configs = generate_main_configs(\n+                warmup_iterations=args.warmup,\n+                measurement_iterations=args.iterations,\n+                batch_size=args.batch_size[0],\n+                sequence_length=args.sequence_length[0],\n+                num_tokens_to_generate=args.num_tokens_to_generate[0],\n+            )\n+\n+    # Otherwise, we benchmark across all combinations of dimensions\n+    else:\n+        main_config = generate_main_configs(\n             warmup_iterations=args.warmup,\n             measurement_iterations=args.iterations,\n             batch_size=args.batch_size[0],\n             sequence_length=args.sequence_length[0],\n             num_tokens_to_generate=args.num_tokens_to_generate[0],\n-        )\n-        random.shuffle(benchmark_configs)\n-\n-    # Otherwise, we benchmark across all combinations of dimensions\n-    else:\n-        kwargs = {\n-            \"warmup_iterations\": args.warmup,\n-            \"measurement_iterations\": args.iterations,\n-            \"gpu_monitoring\": False,\n-            \"batch_size\": args.batch_size[0],\n-            \"sequence_length\": args.sequence_length[0],\n-            \"num_tokens_to_generate\": args.num_tokens_to_generate[0],\n-            \"attn_implementation\": \"flex_attention\",\n-            \"sdpa_backend\": None,\n-            \"compile_mode\": \"default\",\n-            \"kernelize\": False,\n-        }\n+        )[0]\n         benchmark_configs = []\n         for num_tokens_to_generate in args.num_tokens_to_generate:\n             for sequence_length in args.sequence_length:\n                 for batch_size in args.batch_size:\n-                    kwargs[\"batch_size\"] = batch_size\n-                    kwargs[\"sequence_length\"] = sequence_length\n-                    kwargs[\"num_tokens_to_generate\"] = num_tokens_to_generate\n-                    benchmark_configs.append(BenchmarkConfig(**kwargs))\n+                    cfg_dict = main_config.to_dict()\n+                    cfg_dict[\"batch_size\"] = batch_size\n+                    cfg_dict[\"sequence_length\"] = sequence_length\n+                    cfg_dict[\"num_tokens_to_generate\"] = num_tokens_to_generate\n+                    cfg_dict.pop(\"name\")\n+                    benchmark_configs.append(BenchmarkConfig.from_dict(cfg_dict))\n \n     runner = BenchmarkRunner(logger, args.output_dir, args.commit_id)\n     results = runner.run_benchmarks(\n         args.model_id,\n-        benchmark_configs[:3],\n+        benchmark_configs,\n         args.num_tokens_to_profile,\n         pretty_print_summary=True,\n     )"
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T21:17:18.771857",
    "filter_decision": {
      "accept": false,
      "reasoning": "This PR consists primarily of refactoring and configuration changes to a benchmarking script. While the description explains the intent (adding throughput info, output shape details, and quick-run mode), the actual code changes are mostly variable renaming (decoded_output \u2192 shape_and_decoded_output), function renaming (generate_default_configs \u2192 generate_main_configs), adjusting default parameters, and reducing benchmark configurations. There is minimal algorithmic or architectural complexity to generate substantive technical questions about.",
      "substance_level": "low"
    }
  },
  {
    "pr_number": 41627,
    "title": "[`Executorch`] Simplify for encoder models",
    "body": "Now that we include a fast path using no vmapping, we can revert the extra treatment. Followup to #41586",
    "html_url": "https://github.com/huggingface/transformers/pull/41627",
    "created_at": "2025-10-15T15:49:13Z",
    "merged_at": "2025-10-16T11:57:52Z",
    "merge_commit_sha": "44539827d55254546dff5249d976419c798d2f63",
    "base_ref": "main",
    "head_sha": "a621bc1f50b5a35d9a47f50635d78f0ba0f07cbc",
    "user": "vasqu",
    "files": [
      {
        "filename": "src/transformers/integrations/executorch.py",
        "status": "modified",
        "additions": 0,
        "deletions": 144,
        "changes": 144,
        "patch": "@@ -26,7 +26,6 @@\n from ..generation.configuration_utils import GenerationConfig\n from ..masking_utils import (\n     ALL_MASK_ATTENTION_FUNCTIONS,\n-    _ignore_bidirectional_mask_sdpa,\n     _ignore_causal_mask_sdpa,\n     _is_torch_greater_or_equal_than_2_5,\n     prepare_padding_mask,\n@@ -193,101 +192,6 @@ def generate(\n         pass\n \n \n-class TorchExportableModuleForEncoderOnlyLM(torch.nn.Module):\n-    \"\"\"\n-    A recipe module designed to make a `PreTrainedModel` exportable with `torch.export`,\n-    specifically for encoder-only LM. This module ensures that the exported model is compatible\n-    with further lowering and execution in `ExecuTorch`.\n-    \"\"\"\n-\n-    def __init__(self, model: PreTrainedModel) -> None:\n-        \"\"\"\n-        Initializes the exportable module.\n-\n-        Args:\n-            model (`PreTrainedModel`): The pretrained model to wrap.\n-        \"\"\"\n-        super().__init__()\n-\n-        self.model = model\n-        # This is the same as sdpa, but mask creation does not use `vmap` which is not exportable\n-        ALL_MASK_ATTENTION_FUNCTIONS.register(\n-            \"sdpa_bidirectional_mask_without_vmap\", sdpa_bidirectional_mask_without_vmap\n-        )\n-        ALL_ATTENTION_FUNCTIONS.register(\"sdpa_bidirectional_mask_without_vmap\", ALL_ATTENTION_FUNCTIONS[\"sdpa\"])\n-        self.model.config._attn_implementation = \"sdpa_bidirectional_mask_without_vmap\"\n-\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.Tensor] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-    ) -> torch.Tensor:\n-        \"\"\"\n-        Forward pass of the module, which is compatible with the ExecuTorch llm runner.\n-\n-        Args:\n-            input_ids (`torch.Tensor`): Tensor representing current input token id to the module.\n-            inputs_embeds (`torch.Tensor`): Tensor representing current input embeddings to the module.\n-            cache_position (`torch.Tensor`): Tensor representing current input position in the cache.\n-\n-        Returns:\n-            torch.Tensor: Logits output from the model.\n-        \"\"\"\n-        return self.model.forward(\n-            input_ids=input_ids,\n-            inputs_embeds=inputs_embeds,\n-            attention_mask=attention_mask,\n-        )\n-\n-    def export(\n-        self,\n-        input_ids: Optional[torch.Tensor] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        strict: Optional[bool] = None,\n-    ) -> torch.export.ExportedProgram:\n-        \"\"\"\n-        Export the wrapped module using `torch.export`.\n-\n-        Args:\n-            input_ids (`Optional[torch.Tensor]`):\n-                Tensor representing current input token id to the module. Must specify either this or inputs_embeds.\n-            inputs_embeds (`Optional[torch.Tensor]`):\n-                Tensor representing current input embeddings to the module. Must specify either this or input_ids.\n-            strict(`Optional[bool]`):\n-                Flag to instruct `torch.export` to use `torchdynamo`.\n-\n-        Returns:\n-            torch.export.ExportedProgram: The exported program that can be used for inference.\n-\n-        \"\"\"\n-        if not (input_ids is None) ^ (inputs_embeds is None):\n-            raise ValueError(\"Need to specify either input_ids or inputs_embeds.\")\n-\n-        if input_ids is not None:\n-            input_kwargs = {\n-                \"input_ids\": input_ids,\n-                \"attention_mask\": attention_mask if attention_mask is not None else torch.ones_like(input_ids),\n-            }\n-        else:\n-            input_kwargs = {\n-                \"inputs_embeds\": inputs_embeds,\n-                \"attention_mask\": attention_mask\n-                if attention_mask is not None\n-                else torch.ones_like(inputs_embeds)[..., 0],\n-            }\n-\n-        exported_program = torch.export.export(\n-            self.model,\n-            args=(),\n-            kwargs=input_kwargs,\n-            strict=strict if strict is not None else True,\n-        )\n-\n-        return exported_program\n-\n-\n class TorchExportableModuleForDecoderOnlyLM(torch.nn.Module):\n     \"\"\"\n     A recipe module designed to make a `PreTrainedModel` exportable with `torch.export`,\n@@ -1296,51 +1200,3 @@ def sdpa_mask_without_vmap(\n     if not _is_torch_greater_or_equal_than_2_5 and allow_torch_fix:\n         causal_mask |= torch.all(~causal_mask, dim=-1, keepdim=True)\n     return causal_mask\n-\n-\n-def sdpa_bidirectional_mask_without_vmap(\n-    kv_length: int,\n-    kv_offset: int = 0,\n-    attention_mask: Optional[torch.Tensor] = None,\n-    allow_torch_fix: bool = True,\n-    allow_is_bidirectional_skip: bool = True,\n-    **kwargs,\n-) -> Optional[torch.Tensor]:\n-    \"\"\"\n-    Create a 4D boolean mask of shape `(batch_size, 1, query_length, kv_length)` where a value of True indicates that\n-    the element should take part in the attention computation, and False that it should not.\n-\n-    This is similar to `masking_utils.sdpa_mask` but does not use `vmap` which is incompatible with export.\n-    Additionally, surrounding logic for causal masks is omitted for simplicity.\n-\n-    Args:\n-        kv_length (`int`):\n-            The size that the key and value states will have during the attention computation.\n-        kv_offset (`int`, optional):\n-            An optional offset to indicate at which first position the key and values states will refer to.\n-        attention_mask (`torch.Tensor`, optional):\n-            The 2D attention mask corresponding to padded tokens of shape (batch_size, number_of_seen_tokens+q_length)\n-        allow_torch_fix (`bool`, optional):\n-            Whether to update the mask in case a query is not attending to any tokens, to solve a bug in torch's older\n-            versions. We need an arg to skip it when using eager. By default `True`.\n-        allow_is_bidirectional_skip (`bool`, optional):\n-            Whether to allow to return `None` for the mask under conditions where we do not have to add any bias,\n-            i.e. full attention without any padding. Default to `True`.\n-    \"\"\"\n-    # Potentially pad the 2D mask, and slice it correctly\n-    padding_mask = prepare_padding_mask(attention_mask, kv_length, kv_offset, _slice=False)\n-\n-    # Under specific conditions, we can avoid materializing the mask\n-    if allow_is_bidirectional_skip and _ignore_bidirectional_mask_sdpa(padding_mask):\n-        return None\n-\n-    bidirectional_mask = None\n-    if padding_mask is not None:\n-        bidirectional_mask = padding_mask[:, None, None, :]\n-\n-    # Due to a bug in some older torch version, we need to update the mask in case a query is not attending to any\n-    # tokens (due to padding). See details in https://github.com/pytorch/pytorch/issues/110213\n-    if not _is_torch_greater_or_equal_than_2_5 and allow_torch_fix and bidirectional_mask is not None:\n-        bidirectional_mask |= torch.all(~bidirectional_mask, dim=-1, keepdim=True)\n-\n-    return bidirectional_mask"
      },
      {
        "filename": "tests/models/albert/test_modeling_albert.py",
        "status": "modified",
        "additions": 5,
        "deletions": 9,
        "changes": 14,
        "patch": "@@ -337,8 +337,6 @@ def test_export(self):\n         if version.parse(torch.__version__) < version.parse(\"2.4.0\"):\n             self.skipTest(reason=\"This test requires torch >= 2.4 to run.\")\n \n-        from transformers.integrations.executorch import TorchExportableModuleForEncoderOnlyLM\n-\n         distilbert_model = \"albert/albert-base-v2\"\n         device = \"cpu\"\n         attn_implementation = \"sdpa\"\n@@ -365,15 +363,13 @@ def test_export(self):\n             [\"capital\", \"capitol\", \"comune\", \"arrondissement\", \"bastille\"],\n         )\n \n-        exportable_module = TorchExportableModuleForEncoderOnlyLM(model)\n-        exported_program = exportable_module.export(\n-            input_ids=inputs[\"input_ids\"],\n-            attention_mask=inputs[\"attention_mask\"],\n+        exported_program = torch.export.export(\n+            model,\n+            args=(inputs[\"input_ids\"],),\n+            kwargs={\"attention_mask\": inputs[\"attention_mask\"]},\n             strict=True,\n         )\n \n-        result = exported_program.module().forward(\n-            input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"]\n-        )\n+        result = exported_program.module().forward(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n         ep_predicted_mask = tokenizer.decode(result.logits[0, 4].topk(5).indices)\n         self.assertEqual(eg_predicted_mask, ep_predicted_mask)"
      },
      {
        "filename": "tests/models/bert/test_modeling_bert.py",
        "status": "modified",
        "additions": 5,
        "deletions": 9,
        "changes": 14,
        "patch": "@@ -709,8 +709,6 @@ def test_export(self):\n         if version.parse(torch.__version__) < version.parse(\"2.4.0\"):\n             self.skipTest(reason=\"This test requires torch >= 2.4 to run.\")\n \n-        from transformers.integrations.executorch import TorchExportableModuleForEncoderOnlyLM\n-\n         bert_model = \"google-bert/bert-base-uncased\"\n         device = \"cpu\"\n         attn_implementation = \"sdpa\"\n@@ -735,15 +733,13 @@ def test_export(self):\n         eg_predicted_mask = tokenizer.decode(logits[0, 6].topk(5).indices)\n         self.assertEqual(eg_predicted_mask.split(), [\"carpenter\", \"waiter\", \"barber\", \"mechanic\", \"salesman\"])\n \n-        exportable_module = TorchExportableModuleForEncoderOnlyLM(model)\n-        exported_program = exportable_module.export(\n-            input_ids=inputs[\"input_ids\"],\n-            attention_mask=inputs[\"attention_mask\"],\n+        exported_program = torch.export.export(\n+            model,\n+            args=(inputs[\"input_ids\"],),\n+            kwargs={\"attention_mask\": inputs[\"attention_mask\"]},\n             strict=True,\n         )\n \n-        result = exported_program.module().forward(\n-            input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"]\n-        )\n+        result = exported_program.module().forward(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n         ep_predicted_mask = tokenizer.decode(result.logits[0, 6].topk(5).indices)\n         self.assertEqual(eg_predicted_mask, ep_predicted_mask)"
      },
      {
        "filename": "tests/models/distilbert/test_modeling_distilbert.py",
        "status": "modified",
        "additions": 5,
        "deletions": 9,
        "changes": 14,
        "patch": "@@ -404,8 +404,6 @@ def test_export(self):\n         if not is_torch_greater_or_equal_than_2_4:\n             self.skipTest(reason=\"This test requires torch >= 2.4 to run.\")\n \n-        from transformers.integrations.executorch import TorchExportableModuleForEncoderOnlyLM\n-\n         distilbert_model = \"distilbert-base-uncased\"\n         device = \"cpu\"\n         attn_implementation = \"sdpa\"\n@@ -432,15 +430,13 @@ def test_export(self):\n             [\"capital\", \"birthplace\", \"northernmost\", \"centre\", \"southernmost\"],\n         )\n \n-        exportable_module = TorchExportableModuleForEncoderOnlyLM(model)\n-        exported_program = exportable_module.export(\n-            input_ids=inputs[\"input_ids\"],\n-            attention_mask=inputs[\"attention_mask\"],\n+        exported_program = torch.export.export(\n+            model,\n+            args=(inputs[\"input_ids\"],),\n+            kwargs={\"attention_mask\": inputs[\"attention_mask\"]},\n             strict=True,\n         )\n \n-        result = exported_program.module().forward(\n-            input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"]\n-        )\n+        result = exported_program.module().forward(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n         exported_predicted_mask = tokenizer.decode(result.logits[0, 4].topk(5).indices)\n         self.assertEqual(eager_predicted_mask, exported_predicted_mask)"
      },
      {
        "filename": "tests/models/mobilebert/test_modeling_mobilebert.py",
        "status": "modified",
        "additions": 5,
        "deletions": 9,
        "changes": 14,
        "patch": "@@ -395,8 +395,6 @@ def test_export(self):\n         if version.parse(torch.__version__) < version.parse(\"2.4.0\"):\n             self.skipTest(reason=\"This test requires torch >= 2.4 to run.\")\n \n-        from transformers.integrations.executorch import TorchExportableModuleForEncoderOnlyLM\n-\n         mobilebert_model = \"google/mobilebert-uncased\"\n         device = \"cpu\"\n         attn_implementation = \"eager\"\n@@ -420,15 +418,13 @@ def test_export(self):\n         eg_predicted_mask = tokenizer.decode(logits[0, 6].topk(5).indices)\n         self.assertEqual(eg_predicted_mask.split(), [\"carpenter\", \"waiter\", \"mechanic\", \"teacher\", \"clerk\"])\n \n-        exportable_module = TorchExportableModuleForEncoderOnlyLM(model)\n-        exported_program = exportable_module.export(\n-            input_ids=inputs[\"input_ids\"],\n-            attention_mask=inputs[\"attention_mask\"],\n+        exported_program = torch.export.export(\n+            model,\n+            args=(inputs[\"input_ids\"],),\n+            kwargs={\"attention_mask\": inputs[\"attention_mask\"]},\n             strict=True,\n         )\n \n-        result = exported_program.module().forward(\n-            input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"]\n-        )\n+        result = exported_program.module().forward(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n         ep_predicted_mask = tokenizer.decode(result.logits[0, 6].topk(5).indices)\n         self.assertEqual(eg_predicted_mask, ep_predicted_mask)"
      },
      {
        "filename": "tests/models/roberta/test_modeling_roberta.py",
        "status": "modified",
        "additions": 5,
        "deletions": 9,
        "changes": 14,
        "patch": "@@ -691,8 +691,6 @@ def test_export(self):\n         if not is_torch_greater_or_equal_than_2_4:\n             self.skipTest(reason=\"This test requires torch >= 2.4 to run.\")\n \n-        from transformers.integrations.executorch import TorchExportableModuleForEncoderOnlyLM\n-\n         roberta_model = \"FacebookAI/roberta-base\"\n         device = \"cpu\"\n         attn_implementation = \"sdpa\"\n@@ -717,15 +715,13 @@ def test_export(self):\n         eager_predicted_mask = tokenizer.decode(logits[0, 6].topk(5).indices)\n         self.assertEqual(eager_predicted_mask.split(), [\"happiness\", \"love\", \"peace\", \"freedom\", \"simplicity\"])\n \n-        exportable_module = TorchExportableModuleForEncoderOnlyLM(model)\n-        exported_program = exportable_module.export(\n-            input_ids=inputs[\"input_ids\"],\n-            attention_mask=inputs[\"attention_mask\"],\n+        exported_program = torch.export.export(\n+            model,\n+            args=(inputs[\"input_ids\"],),\n+            kwargs={\"attention_mask\": inputs[\"attention_mask\"]},\n             strict=True,\n         )\n \n-        result = exported_program.module().forward(\n-            input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"]\n-        )\n+        result = exported_program.module().forward(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n         exported_predicted_mask = tokenizer.decode(result.logits[0, 6].topk(5).indices)\n         self.assertEqual(eager_predicted_mask, exported_predicted_mask)"
      }
    ],
    "num_files": 6,
    "scraped_at": "2025-11-16T21:17:25.221784",
    "filter_decision": {
      "accept": false,
      "reasoning": "This PR is primarily a code deletion/simplification that removes a wrapper class (`TorchExportableModuleForEncoderOnlyLM`) and updates test cases to use the native `torch.export.export` API instead. While the PR description references a followup (#41586) suggesting architectural context, the actual changes are straightforward removals without introducing new logic, algorithms, or complex interactions that would generate substantive technical questions.",
      "substance_level": "low"
    }
  },
  {
    "pr_number": 41624,
    "title": "Fix serving continuous batching",
    "body": "# What does this PR do?\r\n\r\nServing is broken with continuous batching to due recent PRs. This PR fixes it ",
    "html_url": "https://github.com/huggingface/transformers/pull/41624",
    "created_at": "2025-10-15T14:49:47Z",
    "merged_at": "2025-10-16T15:24:22Z",
    "merge_commit_sha": "9839d57a0244f86125c89981f6301b48309b4913",
    "base_ref": "main",
    "head_sha": "5c12811e5a3cb4de1f5edc72fc70a97c1b14a70f",
    "user": "SunMarc",
    "files": [
      {
        "filename": "docs/source/en/serving.md",
        "status": "modified",
        "additions": 2,
        "deletions": 3,
        "changes": 5,
        "patch": "@@ -380,7 +380,7 @@ CB is opt-in and currently applies to chat completions.\n ```sh\n transformers serve \\\n   --continuous-batching\n-  --attn_implementation sdpa_paged\n+  --attn_implementation \"sdpa\"\n ```\n \n ### Performance tips\n@@ -390,11 +390,10 @@ transformers serve \\\n ```sh\n transformers serve \\\n   --continuous_batching \\\n-  --attn_implementation paged_attention\n+  --attn_implementation \"flash_attention_2\"\n ```\n \n > [!TIP]\n-> If you choose `paged_attention`, you must install `flash-attn` separately: `pip install flash-attn --no-build-isolation`\n \n - `--dtype {bfloat16|float16}` typically improve throughput and memory use vs. `float32`\n "
      },
      {
        "filename": "src/transformers/generation/continuous_batching/continuous_api.py",
        "status": "modified",
        "additions": 0,
        "deletions": 8,
        "changes": 8,
        "patch": "@@ -929,14 +929,6 @@ def request_id_iter(self, request_id: str) -> Generator[GenerationOutput]:\n             if self.batch_processor is not None:\n                 request_cancelled = self.batch_processor.scheduler.request_is_cancelled(request_id)\n \n-    @staticmethod\n-    def supported_attention_implementations() -> set[str]:\n-        return {\"eager_paged\", \"sdpa_paged\", \"flash_attention_2\"}\n-\n-    @staticmethod\n-    def default_attention_implementation() -> str:\n-        return \"sdpa_paged\"\n-\n     @traced\n     def warmup(self, batch_processor: ContinuousBatchProcessor) -> None:\n         stream = torch.cuda.Stream(device=self.model.device)"
      },
      {
        "filename": "src/transformers/modeling_utils.py",
        "status": "modified",
        "additions": 8,
        "deletions": 8,
        "changes": 16,
        "patch": "@@ -2426,30 +2426,30 @@ def get_correct_attn_implementation(self, requested_attention: Optional[str], is\n         if applicable_attention not in [\"eager\"] + ALL_ATTENTION_FUNCTIONS.valid_keys():\n             message = (\n                 f'Specified `attn_implementation=\"{applicable_attention}\"` is not supported. The only possible arguments are '\n-                '`attn_implementation=\"eager\"`'\n+                '`attn_implementation=\"eager\"`, `\"paged|eager\"`'\n             )\n             # check `supports_flash_attn_2` for BC with custom code. TODO: remove after a few releases\n             if self._supports_flash_attn or getattr(self, \"_supports_flash_attn_2\", False):\n-                message += ', `\"attn_implementation=flash_attention_3\"`, `\"attn_implementation=flash_attention_2\"`'\n+                message += ', `\"attn_implementation=flash_attention_3\"`, `\"attn_implementation=flash_attention_2\"`, `\"attn_implementation=paged|flash_attention_2\"`'\n             if self._supports_sdpa:\n-                message += ', `\"attn_implementation=sdpa\"'\n+                message += ', `\"attn_implementation=sdpa\"`, `\"attn_implementation=paged|spda\"`'\n             if self._supports_flex_attn:\n                 message += ', `\"attn_implementation=flex_attention\"`'\n             raise ValueError(message + \".\")\n \n         # Perform relevant checks\n-        if applicable_attention == \"flash_attention_2\":\n+        if \"flash_attention_2\" in applicable_attention:\n             self._flash_attn_2_can_dispatch(is_init_check)\n-        elif applicable_attention == \"flash_attention_3\":\n+        elif \"flash_attention_3\" in applicable_attention:\n             self._flash_attn_3_can_dispatch(is_init_check)\n-        elif applicable_attention == \"flex_attention\":\n+        elif \"flex_attention\" in applicable_attention:\n             self._flex_attn_can_dispatch(is_init_check)\n-        elif applicable_attention == \"sdpa\":\n+        elif \"sdpa\" in applicable_attention:\n             # Sdpa is the default, so we try it and fallback to eager otherwise when not possible\n             try:\n                 self._sdpa_can_dispatch(is_init_check)\n             except (ValueError, ImportError) as e:\n-                if requested_attention == \"sdpa\":\n+                if requested_attention is not None and \"sdpa\" in requested_attention:\n                     raise e\n                 applicable_attention = \"eager\"\n "
      },
      {
        "filename": "src/transformers/utils/import_utils.py",
        "status": "modified",
        "additions": 7,
        "deletions": 0,
        "changes": 7,
        "patch": "@@ -1167,6 +1167,13 @@ def is_mistral_common_available() -> bool:\n     return _is_package_available(\"mistral_common\")\n \n \n+@lru_cache\n+def is_opentelemetry_available() -> bool:\n+    return _is_package_available(\"opentelemetry\") and version.parse(\n+        importlib.metadata.version(\"opentelemetry-api\")\n+    ) >= version.parse(\"1.30.0\")\n+\n+\n def check_torch_load_is_safe() -> None:\n     if not is_torch_greater_or_equal(\"2.6\"):\n         raise ValueError("
      },
      {
        "filename": "src/transformers/utils/metrics.py",
        "status": "modified",
        "additions": 8,
        "deletions": 3,
        "changes": 11,
        "patch": "@@ -5,6 +5,8 @@\n from enum import Enum\n from typing import Any, Optional, Union\n \n+from .import_utils import is_opentelemetry_available\n+\n \n class RequestStatus(Enum):\n     \"\"\"Status of a generation request through its lifecycle.\"\"\"\n@@ -18,12 +20,12 @@ class RequestStatus(Enum):\n     FAILED = \"failed\"\n \n \n-try:\n+if is_opentelemetry_available():\n     from opentelemetry import metrics\n     from opentelemetry.trace import Status, StatusCode, get_tracer\n \n     _has_opentelemetry = True\n-except ImportError:\n+else:\n     _has_opentelemetry = False\n \n \n@@ -183,7 +185,10 @@ def _setup_metrics(self):\n         \"\"\"Initialize OpenTelemetry metrics and tracing if the library is available.\"\"\"\n \n         if not _has_opentelemetry:\n-            logger.info(\"OpenTelemetry is not installed. Metrics and tracing will not be recorded.\")\n+            logger.info(\n+                \"OpenTelemetry is not installed. Metrics and tracing will not be recorded.\"\n+                \"You can install it with `pip install opentelemetry-api>=1.30.0`\"\n+            )\n             return\n \n         self.meter = metrics.get_meter(\"transformers.generation.continuous_batch_processor\")"
      }
    ],
    "num_files": 5,
    "scraped_at": "2025-11-16T21:17:26.111555",
    "filter_decision": {
      "accept": false,
      "reasoning": "This PR is primarily a bug fix that corrects broken documentation examples and removes outdated/incorrect attention implementation references. While it touches multiple files, the changes are mostly configuration/documentation updates (changing 'sdpa_paged' to 'sdpa', removing methods that returned hardcoded strings) rather than implementing new logic or fixing algorithmic issues. The code changes lack substantive complexity\u2014they're essentially removing dead code and correcting parameter values.",
      "substance_level": "low"
    }
  },
  {
    "pr_number": 41607,
    "title": "[v5] Delete `videos` from image processing classes ",
    "body": "# What does this PR do?\r\n\r\nAs per title, it was deprecated for v5",
    "html_url": "https://github.com/huggingface/transformers/pull/41607",
    "created_at": "2025-10-15T09:56:46Z",
    "merged_at": "2025-10-21T10:03:31Z",
    "merge_commit_sha": "ee3a1002e2a3a4b60b66a1f9b034fbbe00c60e87",
    "base_ref": "main",
    "head_sha": "8cc6bbb30a00966e6245a6b41386f892895ab701",
    "user": "zucchini-nlp",
    "files": [
      {
        "filename": "docs/source/en/model_doc/instructblipvideo.md",
        "status": "modified",
        "additions": 0,
        "deletions": 5,
        "changes": 5,
        "patch": "@@ -63,11 +63,6 @@ The attributes can be obtained from model config, as `model.config.num_query_tok\n [[autodoc]] InstructBlipVideoVideoProcessor\n     - preprocess\n \n-## InstructBlipVideoImageProcessor\n-\n-[[autodoc]] InstructBlipVideoImageProcessor\n-    - preprocess\n-\n ## InstructBlipVideoVisionModel\n \n [[autodoc]] InstructBlipVideoVisionModel"
      },
      {
        "filename": "docs/source/en/model_doc/llava_next_video.md",
        "status": "modified",
        "additions": 0,
        "deletions": 4,
        "changes": 4,
        "patch": "@@ -247,10 +247,6 @@ model = LlavaNextVideoForConditionalGeneration.from_pretrained(\n \n [[autodoc]] LlavaNextVideoProcessor\n \n-## LlavaNextVideoImageProcessor\n-\n-[[autodoc]] LlavaNextVideoImageProcessor\n-\n ## LlavaNextVideoVideoProcessor\n \n [[autodoc]] LlavaNextVideoVideoProcessor"
      },
      {
        "filename": "src/transformers/models/auto/image_processing_auto.py",
        "status": "modified",
        "additions": 1,
        "deletions": 2,
        "changes": 3,
        "patch": "@@ -114,7 +114,6 @@\n             (\"ijepa\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n             (\"imagegpt\", (\"ImageGPTImageProcessor\", \"ImageGPTImageProcessorFast\")),\n             (\"instructblip\", (\"BlipImageProcessor\", \"BlipImageProcessorFast\")),\n-            (\"instructblipvideo\", (\"InstructBlipVideoImageProcessor\", None)),\n             (\"janus\", (\"JanusImageProcessor\", \"JanusImageProcessorFast\")),\n             (\"kosmos-2\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),\n             (\"kosmos-2.5\", (\"Kosmos2_5ImageProcessor\", \"Kosmos2_5ImageProcessorFast\")),\n@@ -126,7 +125,7 @@\n             (\"llama4\", (\"Llama4ImageProcessor\", \"Llama4ImageProcessorFast\")),\n             (\"llava\", (\"LlavaImageProcessor\", \"LlavaImageProcessorFast\")),\n             (\"llava_next\", (\"LlavaNextImageProcessor\", \"LlavaNextImageProcessorFast\")),\n-            (\"llava_next_video\", (\"LlavaNextVideoImageProcessor\", None)),\n+            (\"llava_next_video\", (\"LlavaNextImageProcessor\", \"LlavaNextImageProcessorFast\")),\n             (\"llava_onevision\", (\"LlavaOnevisionImageProcessor\", \"LlavaOnevisionImageProcessorFast\")),\n             (\"mask2former\", (\"Mask2FormerImageProcessor\", \"Mask2FormerImageProcessorFast\")),\n             (\"maskformer\", (\"MaskFormerImageProcessor\", \"MaskFormerImageProcessorFast\")),"
      },
      {
        "filename": "src/transformers/models/glm4v/image_processing_glm4v.py",
        "status": "modified",
        "additions": 0,
        "deletions": 4,
        "changes": 4,
        "patch": "@@ -313,7 +313,6 @@ def _preprocess(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        videos: Optional[VideoInput] = None,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n         resample: Optional[PILImageResampling] = None,\n@@ -335,9 +334,6 @@ def preprocess(\n             images (`ImageInput`):\n                 Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If\n                 passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n-            videos (`VideoInput`):\n-                Video to preprocess. Expects a single or batch of videos with pixel values ranging from 0 to 255. If\n-                passing in videos with pixel values between 0 and 1, set `do_rescale=False`.\n             do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n                 Whether to resize the image.\n             size (`Dict[str, int]`, *optional*, defaults to `self.size`):"
      },
      {
        "filename": "src/transformers/models/instructblipvideo/image_processing_instructblipvideo.py",
        "status": "removed",
        "additions": 0,
        "deletions": 327,
        "changes": 327,
        "patch": "@@ -1,327 +0,0 @@\n-# coding=utf-8\n-# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-\"\"\"\n-Image processor class for InstructBLIPVideo. Largely copy of Blip2Processor with addition of a video processing abilities\n-\"\"\"\n-\n-from typing import Optional, Union\n-\n-import numpy as np\n-\n-from ...image_processing_utils import BaseImageProcessor, BatchFeature, get_size_dict\n-from ...image_transforms import convert_to_rgb, resize, to_channel_dimension_format\n-from ...image_utils import (\n-    OPENAI_CLIP_MEAN,\n-    OPENAI_CLIP_STD,\n-    ChannelDimension,\n-    ImageInput,\n-    PILImageResampling,\n-    infer_channel_dimension_format,\n-    is_scaled_image,\n-    to_numpy_array,\n-    valid_images,\n-    validate_preprocess_arguments,\n-)\n-from ...utils import TensorType, filter_out_non_signature_kwargs, logging\n-from ...video_utils import VideoInput, make_batched_videos\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-\n-# TODO (raushan): processor can be removed after v5 release. Kept for backwards compatibility\n-# Copied from transformers.models.blip.image_processing_blip.BlipImageProcessor with Blip->InstructBlipVideo, BLIP->InstructBLIPVideo\n-class InstructBlipVideoImageProcessor(BaseImageProcessor):\n-    r\"\"\"\n-    Constructs a InstructBLIPVideo image processor.\n-\n-    Args:\n-        do_resize (`bool`, *optional*, defaults to `True`):\n-            Whether to resize the image's (height, width) dimensions to the specified `size`. Can be overridden by the\n-            `do_resize` parameter in the `preprocess` method.\n-        size (`dict`, *optional*, defaults to `{\"height\": 384, \"width\": 384}`):\n-            Size of the output image after resizing. Can be overridden by the `size` parameter in the `preprocess`\n-            method.\n-        resample (`PILImageResampling`, *optional*, defaults to `Resampling.BICUBIC`):\n-            Resampling filter to use if resizing the image. Only has an effect if `do_resize` is set to `True`. Can be\n-            overridden by the `resample` parameter in the `preprocess` method.\n-        do_rescale (`bool`, *optional*, defaults to `True`):\n-            Whether to rescale the image by the specified scale `rescale_factor`. Can be overridden by the\n-            `do_rescale` parameter in the `preprocess` method.\n-        rescale_factor (`int` or `float`, *optional*, defaults to `1/255`):\n-            Scale factor to use if rescaling the image. Only has an effect if `do_rescale` is set to `True`. Can be\n-            overridden by the `rescale_factor` parameter in the `preprocess` method.\n-        do_normalize (`bool`, *optional*, defaults to `True`):\n-            Whether to normalize the image. Can be overridden by the `do_normalize` parameter in the `preprocess`\n-            method. Can be overridden by the `do_normalize` parameter in the `preprocess` method.\n-        image_mean (`float` or `list[float]`, *optional*, defaults to `IMAGENET_STANDARD_MEAN`):\n-            Mean to use if normalizing the image. This is a float or list of floats the length of the number of\n-            channels in the image. Can be overridden by the `image_mean` parameter in the `preprocess` method. Can be\n-            overridden by the `image_mean` parameter in the `preprocess` method.\n-        image_std (`float` or `list[float]`, *optional*, defaults to `IMAGENET_STANDARD_STD`):\n-            Standard deviation to use if normalizing the image. This is a float or list of floats the length of the\n-            number of channels in the image. Can be overridden by the `image_std` parameter in the `preprocess` method.\n-            Can be overridden by the `image_std` parameter in the `preprocess` method.\n-        do_convert_rgb (`bool`, *optional*, defaults to `True`):\n-            Whether to convert the image to RGB.\n-    \"\"\"\n-\n-    model_input_names = [\"pixel_values\"]\n-\n-    def __init__(\n-        self,\n-        do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = PILImageResampling.BICUBIC,\n-        do_rescale: bool = True,\n-        rescale_factor: Union[int, float] = 1 / 255,\n-        do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_convert_rgb: bool = True,\n-        **kwargs,\n-    ) -> None:\n-        super().__init__(**kwargs)\n-        size = size if size is not None else {\"height\": 384, \"width\": 384}\n-        size = get_size_dict(size, default_to_square=True)\n-\n-        self.do_resize = do_resize\n-        self.size = size\n-        self.resample = resample\n-        self.do_rescale = do_rescale\n-        self.rescale_factor = rescale_factor\n-        self.do_normalize = do_normalize\n-        self.image_mean = image_mean if image_mean is not None else OPENAI_CLIP_MEAN\n-        self.image_std = image_std if image_std is not None else OPENAI_CLIP_STD\n-        self.do_convert_rgb = do_convert_rgb\n-\n-    # Copied from transformers.models.vit.image_processing_vit.ViTImageProcessor.resize with PILImageResampling.BILINEAR->PILImageResampling.BICUBIC\n-    def resize(\n-        self,\n-        image: np.ndarray,\n-        size: dict[str, int],\n-        resample: PILImageResampling = PILImageResampling.BICUBIC,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-        **kwargs,\n-    ) -> np.ndarray:\n-        \"\"\"\n-        Resize an image to `(size[\"height\"], size[\"width\"])`.\n-\n-        Args:\n-            image (`np.ndarray`):\n-                Image to resize.\n-            size (`dict[str, int]`):\n-                Dictionary in the format `{\"height\": int, \"width\": int}` specifying the size of the output image.\n-            resample (`PILImageResampling`, *optional*, defaults to `PILImageResampling.BICUBIC`):\n-                `PILImageResampling` filter to use when resizing the image e.g. `PILImageResampling.BICUBIC`.\n-            data_format (`ChannelDimension` or `str`, *optional*):\n-                The channel dimension format for the output image. If unset, the channel dimension format of the input\n-                image is used. Can be one of:\n-                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n-                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n-                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n-            input_data_format (`ChannelDimension` or `str`, *optional*):\n-                The channel dimension format for the input image. If unset, the channel dimension format is inferred\n-                from the input image. Can be one of:\n-                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n-                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n-                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n-\n-        Returns:\n-            `np.ndarray`: The resized image.\n-        \"\"\"\n-        size = get_size_dict(size)\n-        if \"height\" not in size or \"width\" not in size:\n-            raise ValueError(f\"The `size` dictionary must contain the keys `height` and `width`. Got {size.keys()}\")\n-\n-        output_size = (size[\"height\"], size[\"width\"])\n-        return resize(\n-            image,\n-            size=output_size,\n-            resample=resample,\n-            data_format=data_format,\n-            input_data_format=input_data_format,\n-            **kwargs,\n-        )\n-\n-    # Ignore copy\n-    @filter_out_non_signature_kwargs()\n-    def preprocess(\n-        self,\n-        images: Optional[VideoInput] = None,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        do_convert_rgb: Optional[bool] = None,\n-        data_format: ChannelDimension = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-    ) -> BatchFeature:\n-        \"\"\"\n-        Preprocess a video or batch of images/videos.\n-\n-        Args:\n-            videos (`VideoInput`):\n-                Video frames to preprocess. Expects a single or batch of videos as a list of frames with pixel values\n-                ranging from 0 to 255. If passing in video with pixel values between 0 and 1, set `do_rescale=False`.\n-            do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n-                Whether to resize the video.\n-            size (`dict[str, int]`, *optional*, defaults to `self.size`):\n-                Controls the size of the video after `resize`. The shortest edge of the image is resized to\n-                `size[\"shortest_edge\"]` whilst preserving the aspect ratio. If the longest edge of this resized image\n-                is > `int(size[\"shortest_edge\"] * (1333 / 800))`, then the image is resized again to make the longest\n-                edge equal to `int(size[\"shortest_edge\"] * (1333 / 800))`.\n-            resample (`PILImageResampling`, *optional*, defaults to `self.resample`):\n-                Resampling filter to use if resizing the video. Only has an effect if `do_resize` is set to `True`.\n-            do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n-                Whether to rescale the video values between [0 - 1].\n-            rescale_factor (`float`, *optional*, defaults to `self.rescale_factor`):\n-                Rescale factor to rescale the video by if `do_rescale` is set to `True`.\n-            do_normalize (`bool`, *optional*, defaults to `self.do_normalize`):\n-                Whether to normalize the video.\n-            image_mean (`float` or `list[float]`, *optional*, defaults to `self.image_mean`):\n-                Image mean to normalize the video by if `do_normalize` is set to `True`.\n-            image_std (`float` or `list[float]`, *optional*, defaults to `self.image_std`):\n-                Image standard deviation to normalize the video by if `do_normalize` is set to `True`.\n-            do_convert_rgb (`bool`, *optional*, defaults to `self.do_convert_rgb`):\n-                Whether to convert the image to RGB.\n-            return_tensors (`str` or `TensorType`, *optional*):\n-                The type of tensors to return. Can be one of:\n-                    - Unset: Return a list of `np.ndarray`.\n-                    - `TensorType.PYTORCH` or `'pt'`: Return a batch of type `torch.Tensor`.\n-                    - `TensorType.NUMPY` or `'np'`: Return a batch of type `np.ndarray`.\n-            data_format (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`):\n-                The channel dimension format for the output image. Can be one of:\n-                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n-                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n-                - Unset: Use the channel dimension format of the input image.\n-            input_data_format (`ChannelDimension` or `str`, *optional*):\n-                The channel dimension format for the input image. If unset, the channel dimension format is inferred\n-                from the input image. Can be one of:\n-                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n-                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n-                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n-        \"\"\"\n-        do_resize = do_resize if do_resize is not None else self.do_resize\n-        resample = resample if resample is not None else self.resample\n-        do_rescale = do_rescale if do_rescale is not None else self.do_rescale\n-        rescale_factor = rescale_factor if rescale_factor is not None else self.rescale_factor\n-        do_normalize = do_normalize if do_normalize is not None else self.do_normalize\n-        image_mean = image_mean if image_mean is not None else self.image_mean\n-        image_std = image_std if image_std is not None else self.image_std\n-        do_convert_rgb = do_convert_rgb if do_convert_rgb is not None else self.do_convert_rgb\n-\n-        size = size if size is not None else self.size\n-        size = get_size_dict(size, default_to_square=False)\n-\n-        videos = make_batched_videos(images)\n-        logger.warning(\n-            \"`InstructBlipVideoImageProcessor` is deprecated and will be removed in v5.0. \"\n-            \"We recommend to load an instance of `InstructBlipVideoVideoProcessor` to process videos for the model. \"\n-        )\n-\n-        validate_preprocess_arguments(\n-            do_rescale=do_rescale,\n-            rescale_factor=rescale_factor,\n-            do_normalize=do_normalize,\n-            image_mean=image_mean,\n-            image_std=image_std,\n-            do_resize=do_resize,\n-            size=size,\n-            resample=resample,\n-        )\n-\n-        if not valid_images(videos):\n-            raise ValueError(\"Invalid input type. Must be of type PIL.Image.Image, numpy.ndarray, or torch.Tensor\")\n-\n-        pixel_values = [\n-            [\n-                self._preprocess_image(\n-                    image=frame,\n-                    do_resize=do_resize,\n-                    size=size,\n-                    resample=resample,\n-                    do_rescale=do_rescale,\n-                    rescale_factor=rescale_factor,\n-                    do_normalize=do_normalize,\n-                    image_mean=image_mean,\n-                    image_std=image_std,\n-                    do_convert_rgb=do_convert_rgb,\n-                    data_format=data_format,\n-                    input_data_format=input_data_format,\n-                )\n-                for frame in video\n-            ]\n-            for video in videos\n-        ]\n-\n-        encoded_outputs = BatchFeature(data={\"pixel_values\": pixel_values}, tensor_type=return_tensors)\n-        return encoded_outputs\n-\n-    # Ignore copy\n-    def _preprocess_image(\n-        self,\n-        image: Optional[ImageInput] = None,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_convert_rgb: Optional[bool] = None,\n-        data_format: ChannelDimension = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-    ) -> np.ndarray:\n-        # PIL RGBA images are converted to RGB\n-        if do_convert_rgb:\n-            image = convert_to_rgb(image)\n-\n-        # All transformations expect numpy arrays.\n-        image = to_numpy_array(image)\n-\n-        if do_rescale and is_scaled_image(image):\n-            logger.warning_once(\n-                \"It looks like you are trying to rescale already rescaled video frames. If the input\"\n-                \" images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\"\n-            )\n-\n-        if input_data_format is None:\n-            # We assume that all images have the same channel dimension format.\n-            input_data_format = infer_channel_dimension_format(image)\n-\n-        if do_resize:\n-            image = self.resize(image=image, size=size, resample=resample, input_data_format=input_data_format)\n-\n-        if do_rescale:\n-            image = self.rescale(image=image, scale=rescale_factor, input_data_format=input_data_format)\n-\n-        if do_normalize:\n-            image = self.normalize(image=image, mean=image_mean, std=image_std, input_data_format=input_data_format)\n-\n-        image = to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format)\n-\n-        return image\n-\n-\n-__all__ = [\"InstructBlipVideoImageProcessor\"]"
      },
      {
        "filename": "src/transformers/models/instructblipvideo/processing_instructblipvideo.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -41,7 +41,7 @@ class InstructBlipVideoProcessor(ProcessorMixin):\n     Constructs an InstructBLIPVideo processor which wraps a InstructBLIP image processor and a LLaMa/T5 tokenizer into a single\n     processor.\n \n-    [`InstructBlipVideoProcessor`] offers all the functionalities of [`InstructBlipVideoImageProcessor`] and [`AutoTokenizer`]. See the\n+    [`InstructBlipVideoProcessor`] offers all the functionalities of [`InstructBlipVideoVideoProcessor`] and [`AutoTokenizer`]. See the\n     docstring of [`~InstructBlipVideoProcessor.__call__`] and [`~InstructBlipVideoProcessor.decode`] for more information.\n \n     Args:"
      },
      {
        "filename": "src/transformers/models/llava_next_video/convert_llava_next_video_weights_to_hf.py",
        "status": "modified",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "patch": "@@ -34,8 +34,8 @@\n     LlavaNextImageProcessor,\n     LlavaNextVideoConfig,\n     LlavaNextVideoForConditionalGeneration,\n-    LlavaNextVideoImageProcessor,\n     LlavaNextVideoProcessor,\n+    LlavaNextVideoVideoProcessor,\n )\n \n \n@@ -187,7 +187,7 @@ def convert_llava_to_hf(model_id, pytorch_dump_folder_path, push_to_hub=False):\n     tokenizer.add_tokens(AddedToken(\"<image>\", special=True, normalized=False), special_tokens=True)\n \n     image_processor = LlavaNextImageProcessor.from_pretrained(vision_model_id)\n-    video_processor = LlavaNextVideoImageProcessor.from_pretrained(vision_model_id)\n+    video_processor = LlavaNextVideoVideoProcessor.from_pretrained(vision_model_id)\n     processor = LlavaNextVideoProcessor(\n         tokenizer=tokenizer,\n         video_processor=video_processor,"
      },
      {
        "filename": "src/transformers/models/llava_next_video/image_processing_llava_next_video.py",
        "status": "removed",
        "additions": 0,
        "deletions": 401,
        "changes": 401,
        "patch": "@@ -1,401 +0,0 @@\n-# coding=utf-8\n-# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Image processor class for LLaVa-NeXT-Video.\"\"\"\n-\n-from typing import Optional, Union\n-\n-import numpy as np\n-\n-from ...image_processing_utils import BaseImageProcessor, BatchFeature, get_size_dict\n-from ...image_transforms import (\n-    convert_to_rgb,\n-    get_resize_output_image_size,\n-    resize,\n-    to_channel_dimension_format,\n-)\n-from ...image_utils import (\n-    OPENAI_CLIP_MEAN,\n-    OPENAI_CLIP_STD,\n-    ChannelDimension,\n-    ImageInput,\n-    PILImageResampling,\n-    infer_channel_dimension_format,\n-    is_scaled_image,\n-    make_flat_list_of_images,\n-    to_numpy_array,\n-    validate_preprocess_arguments,\n-)\n-from ...utils import TensorType, logging\n-from ...video_utils import VideoInput, make_batched_videos\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-\n-class LlavaNextVideoImageProcessor(BaseImageProcessor):\n-    r\"\"\"\n-    Constructs a LLaVa-NeXT-Video video processor. Based on [`CLIPImageProcessor`] with incorporation of processing each video frame.\n-\n-    Args:\n-        do_resize (`bool`, *optional*, defaults to `True`):\n-            Whether to resize the image's (height, width) dimensions to the specified `size`. Can be overridden by\n-            `do_resize` in the `preprocess` method.\n-        size (`dict[str, int]` *optional*, defaults to `{\"shortest_edge\": 224}`):\n-            Size of the image after resizing. The shortest edge of the image is resized to size[\"shortest_edge\"], with\n-            the longest edge resized to keep the input aspect ratio. Can be overridden by `size` in the `preprocess`\n-            method.\n-        image_grid_pinpoints (`List` *optional*, defaults to `[[672, 336], [336, 672], [672, 672], [336, 1008], [1008, 336]]`):\n-            A list of possible resolutions to use for processing high resolution images. The best resolution is selected\n-            based on the original size of the image. Can be overridden by `image_grid_pinpoints` in the `preprocess`\n-            method. Not used for processing videos.\n-        resample (`PILImageResampling`, *optional*, defaults to `Resampling.BICUBIC`):\n-            Resampling filter to use if resizing the image. Can be overridden by `resample` in the `preprocess` method.\n-        do_center_crop (`bool`, *optional*, defaults to `True`):\n-            Whether to center crop the image to the specified `crop_size`. Can be overridden by `do_center_crop` in the\n-            `preprocess` method.\n-        crop_size (`dict[str, int]` *optional*, defaults to 224):\n-            Size of the output image after applying `center_crop`. Can be overridden by `crop_size` in the `preprocess`\n-            method.\n-        do_rescale (`bool`, *optional*, defaults to `True`):\n-            Whether to rescale the image by the specified scale `rescale_factor`. Can be overridden by `do_rescale` in\n-            the `preprocess` method.\n-        rescale_factor (`int` or `float`, *optional*, defaults to `1/255`):\n-            Scale factor to use if rescaling the image. Can be overridden by `rescale_factor` in the `preprocess`\n-            method.\n-        do_normalize (`bool`, *optional*, defaults to `True`):\n-            Whether to normalize the image. Can be overridden by `do_normalize` in the `preprocess` method.\n-        image_mean (`float` or `list[float]`, *optional*, defaults to `[0.48145466, 0.4578275, 0.40821073]`):\n-            Mean to use if normalizing the image. This is a float or list of floats the length of the number of\n-            channels in the image. Can be overridden by the `image_mean` parameter in the `preprocess` method.\n-        image_std (`float` or `list[float]`, *optional*, defaults to `[0.26862954, 0.26130258, 0.27577711]`):\n-            Standard deviation to use if normalizing the image. This is a float or list of floats the length of the\n-            number of channels in the image. Can be overridden by the `image_std` parameter in the `preprocess` method.\n-            Can be overridden by the `image_std` parameter in the `preprocess` method.\n-        do_convert_rgb (`bool`, *optional*, defaults to `True`):\n-            Whether to convert the image to RGB.\n-    \"\"\"\n-\n-    model_input_names = [\"pixel_values_videos\"]\n-\n-    def __init__(\n-        self,\n-        do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n-        image_grid_pinpoints: Optional[list] = None,\n-        resample: PILImageResampling = PILImageResampling.BICUBIC,\n-        do_center_crop: bool = True,\n-        crop_size: Optional[dict[str, int]] = None,\n-        do_rescale: bool = True,\n-        rescale_factor: Union[int, float] = 1 / 255,\n-        do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_convert_rgb: bool = True,\n-        **kwargs,\n-    ) -> None:\n-        super().__init__(**kwargs)\n-        size = size if size is not None else {\"shortest_edge\": 224}\n-        size = get_size_dict(size, default_to_square=False)\n-        crop_size = crop_size if crop_size is not None else {\"height\": 224, \"width\": 224}\n-        crop_size = get_size_dict(crop_size, default_to_square=True, param_name=\"crop_size\")\n-\n-        self.do_resize = do_resize\n-        self.size = size\n-        self.image_grid_pinpoints = image_grid_pinpoints\n-        self.resample = resample\n-        self.do_center_crop = do_center_crop\n-        self.crop_size = crop_size\n-        self.do_rescale = do_rescale\n-        self.rescale_factor = rescale_factor\n-        self.do_normalize = do_normalize\n-        self.image_mean = image_mean if image_mean is not None else OPENAI_CLIP_MEAN\n-        self.image_std = image_std if image_std is not None else OPENAI_CLIP_STD\n-        self.do_convert_rgb = do_convert_rgb\n-\n-    # Copied from transformers.models.clip.image_processing_clip.CLIPImageProcessor.resize with CLIP->LLaVa\n-    def resize(\n-        self,\n-        image: np.ndarray,\n-        size: dict[str, int],\n-        resample: PILImageResampling = PILImageResampling.BICUBIC,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-        **kwargs,\n-    ) -> np.ndarray:\n-        \"\"\"\n-        Resize an image. The shortest edge of the image is resized to size[\"shortest_edge\"], with the longest edge\n-        resized to keep the input aspect ratio.\n-\n-        Args:\n-            image (`np.ndarray`):\n-                Image to resize.\n-            size (`dict[str, int]`):\n-                Size of the output image.\n-            resample (`PILImageResampling`, *optional*, defaults to `PILImageResampling.BICUBIC`):\n-                Resampling filter to use when resiizing the image.\n-            data_format (`str` or `ChannelDimension`, *optional*):\n-                The channel dimension format of the image. If not provided, it will be the same as the input image.\n-            input_data_format (`ChannelDimension` or `str`, *optional*):\n-                The channel dimension format of the input image. If not provided, it will be inferred.\n-        \"\"\"\n-        default_to_square = True\n-        if \"shortest_edge\" in size:\n-            size = size[\"shortest_edge\"]\n-            default_to_square = False\n-        elif \"height\" in size and \"width\" in size:\n-            size = (size[\"height\"], size[\"width\"])\n-        else:\n-            raise ValueError(\"Size must contain either 'shortest_edge' or 'height' and 'width'.\")\n-\n-        output_size = get_resize_output_image_size(\n-            image,\n-            size=size,\n-            default_to_square=default_to_square,\n-            input_data_format=input_data_format,\n-        )\n-\n-        return resize(\n-            image,\n-            size=output_size,\n-            resample=resample,\n-            data_format=data_format,\n-            input_data_format=input_data_format,\n-            **kwargs,\n-        )\n-\n-    def _preprocess(\n-        self,\n-        images: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_center_crop: Optional[bool] = None,\n-        crop_size: Optional[int] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_convert_rgb: Optional[bool] = None,\n-        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-    ) -> list[np.ndarray]:\n-        \"\"\"\n-        Preprocess an image or batch of images. Copy of the `preprocess` method from `CLIPImageProcessor`.\n-\n-        Args:\n-            images (`ImageInput`):\n-                Batch of frames (one video) to preprocess. Expects a batch of frames with pixel values ranging from 0 to 255. If\n-                passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n-            do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n-                Whether to resize the image.\n-            size (`dict[str, int]`, *optional*, defaults to `self.size`):\n-                Size of the image after resizing. Shortest edge of the image is resized to size[\"shortest_edge\"], with\n-                the longest edge resized to keep the input aspect ratio.\n-            resample (`int`, *optional*, defaults to `self.resample`):\n-                Resampling filter to use if resizing the image. This can be one of the enum `PILImageResampling`. Only\n-                has an effect if `do_resize` is set to `True`.\n-            do_center_crop (`bool`, *optional*, defaults to `self.do_center_crop`):\n-                Whether to center crop the image.\n-            crop_size (`dict[str, int]`, *optional*, defaults to `self.crop_size`):\n-                Size of the center crop. Only has an effect if `do_center_crop` is set to `True`.\n-            do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n-                Whether to rescale the image.\n-            rescale_factor (`float`, *optional*, defaults to `self.rescale_factor`):\n-                Rescale factor to rescale the image by if `do_rescale` is set to `True`.\n-            do_normalize (`bool`, *optional*, defaults to `self.do_normalize`):\n-                Whether to normalize the image.\n-            image_mean (`float` or `list[float]`, *optional*, defaults to `self.image_mean`):\n-                Image mean to use for normalization. Only has an effect if `do_normalize` is set to `True`.\n-            image_std (`float` or `list[float]`, *optional*, defaults to `self.image_std`):\n-                Image standard deviation to use for normalization. Only has an effect if `do_normalize` is set to\n-                `True`.\n-            data_format (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`):\n-                The channel dimension format for the output image. Can be one of:\n-                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n-                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n-                - Unset: Use the channel dimension format of the input image.\n-            input_data_format (`ChannelDimension` or `str`, *optional*):\n-                The channel dimension format for the input image. If unset, the channel dimension format is inferred\n-                from the input image. Can be one of:\n-                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n-                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n-                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n-        \"\"\"\n-        images = make_flat_list_of_images(images)\n-\n-        if do_convert_rgb:\n-            images = [convert_to_rgb(image) for image in images]\n-\n-        # All transformations expect numpy arrays.\n-        images = [to_numpy_array(image) for image in images]\n-\n-        if do_rescale and is_scaled_image(images[0]):\n-            logger.warning_once(\n-                \"It looks like you are trying to rescale already rescaled images. If the input\"\n-                \" images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\"\n-            )\n-        if input_data_format is None:\n-            # We assume that all images have the same channel dimension format.\n-            input_data_format = infer_channel_dimension_format(images[0])\n-\n-        all_images = []\n-        for image in images:\n-            if do_resize:\n-                image = self.resize(image=image, size=size, resample=resample, input_data_format=input_data_format)\n-\n-            if do_center_crop:\n-                image = self.center_crop(image=image, size=crop_size, input_data_format=input_data_format)\n-\n-            if do_rescale:\n-                image = self.rescale(image=image, scale=rescale_factor, input_data_format=input_data_format)\n-\n-            if do_normalize:\n-                image = self.normalize(\n-                    image=image, mean=image_mean, std=image_std, input_data_format=input_data_format\n-                )\n-\n-            all_images.append(image)\n-        images = [\n-            to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format)\n-            for image in all_images\n-        ]\n-\n-        return images\n-\n-    def preprocess(\n-        self,\n-        images: VideoInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_center_crop: Optional[bool] = None,\n-        crop_size: Optional[int] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_convert_rgb: Optional[bool] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-    ):\n-        \"\"\"\n-        Args:\n-            images (`VideoInput`):\n-                Videos to preprocess. Expects a single or batch of videos with pixel values ranging from 0 to 255. If\n-                passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n-            do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n-                Whether to resize the video.\n-            size (`dict[str, int]`, *optional*, defaults to `self.size`):\n-                Size of the video after resizing. Shortest edge of the video is resized to size[\"shortest_edge\"], with\n-                the longest edge resized to keep the input aspect ratio.\n-            resample (`int`, *optional*, defaults to `self.resample`):\n-                Resampling filter to use if resizing the video. This can be one of the enum `PILImageResampling`. Only\n-                has an effect if `do_resize` is set to `True`.\n-            do_center_crop (`bool`, *optional*, defaults to `self.do_center_crop`):\n-                Whether to center crop the video.\n-            crop_size (`dict[str, int]`, *optional*, defaults to `self.crop_size`):\n-                Size of the center crop. Only has an effect if `do_center_crop` is set to `True`.\n-            do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n-                Whether to rescale the video.\n-            rescale_factor (`float`, *optional*, defaults to `self.rescale_factor`):\n-                Rescale factor to rescale the video by if `do_rescale` is set to `True`.\n-            do_normalize (`bool`, *optional*, defaults to `self.do_normalize`):\n-                Whether to normalize the video.\n-            image_mean (`float` or `list[float]`, *optional*, defaults to `self.image_mean`):\n-                Frame mean to use for normalization. Only has an effect if `do_normalize` is set to `True`.\n-            image_std (`float` or `list[float]`, *optional*, defaults to `self.image_std`):\n-                Frame standard deviation to use for normalization. Only has an effect if `do_normalize` is set to\n-                `True`.\n-            do_convert_rgb (`bool`, *optional*, defaults to `self.do_convert_rgb`):\n-                Whether to convert the video to RGB.\n-            return_tensors (`str` or `TensorType`, *optional*):\n-                The type of tensors to return. Can be one of:\n-                - Unset: Return a list of `np.ndarray`.\n-                - `TensorType.PYTORCH` or `'pt'`: Return a batch of type `torch.Tensor`.\n-                - `TensorType.NUMPY` or `'np'`: Return a batch of type `np.ndarray`.\n-            data_format (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`):\n-                The channel dimension format for the output image. Can be one of:\n-                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n-                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n-                - Unset: Use the channel dimension format of the input image.\n-            input_data_format (`ChannelDimension` or `str`, *optional*):\n-                The channel dimension format for the input image. If unset, the channel dimension format is inferred\n-                from the input image. Can be one of:\n-                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n-                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n-                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n-        \"\"\"\n-        do_resize = do_resize if do_resize is not None else self.do_resize\n-        size = size if size is not None else self.size\n-        size = get_size_dict(size, param_name=\"size\", default_to_square=False)\n-        resample = resample if resample is not None else self.resample\n-        do_center_crop = do_center_crop if do_center_crop is not None else self.do_center_crop\n-        crop_size = crop_size if crop_size is not None else self.crop_size\n-        crop_size = get_size_dict(crop_size, param_name=\"crop_size\", default_to_square=True)\n-        do_rescale = do_rescale if do_rescale is not None else self.do_rescale\n-        rescale_factor = rescale_factor if rescale_factor is not None else self.rescale_factor\n-        do_normalize = do_normalize if do_normalize is not None else self.do_normalize\n-        image_mean = image_mean if image_mean is not None else self.image_mean\n-        image_std = image_std if image_std is not None else self.image_std\n-        do_convert_rgb = do_convert_rgb if do_convert_rgb is not None else self.do_convert_rgb\n-\n-        images = self.fetch_images(images)\n-        images = make_batched_videos(images)\n-        logger.warning(\n-            \"`LlavaNextVideoImageProcessor` is deprecated and will be removed in v5.0. \"\n-            \"We recommend to load an instance of `LlavaNextVideoVideoProcessor` to process videos for the model. \"\n-        )\n-\n-        validate_preprocess_arguments(\n-            do_rescale=do_rescale,\n-            rescale_factor=rescale_factor,\n-            do_normalize=do_normalize,\n-            image_mean=image_mean,\n-            image_std=image_std,\n-            do_center_crop=do_center_crop,\n-            crop_size=crop_size,\n-            do_resize=do_resize,\n-            size=size,\n-            resample=resample,\n-        )\n-\n-        # preprocess each video frame by frame\n-        pixel_values = [\n-            self._preprocess(\n-                frames,\n-                do_resize=do_resize,\n-                size=size,\n-                resample=resample,\n-                do_center_crop=do_center_crop,\n-                crop_size=crop_size,\n-                do_rescale=do_rescale,\n-                rescale_factor=rescale_factor,\n-                do_normalize=do_normalize,\n-                image_mean=image_mean,\n-                image_std=image_std,\n-                data_format=data_format,\n-                input_data_format=input_data_format,\n-            )\n-            for frames in images\n-        ]\n-\n-        data = {\"pixel_values_videos\": pixel_values}\n-        return BatchFeature(data=data, tensor_type=return_tensors)\n-\n-\n-__all__ = [\"LlavaNextVideoImageProcessor\"]"
      },
      {
        "filename": "src/transformers/models/llava_next_video/processing_llava_next_video.py",
        "status": "modified",
        "additions": 3,
        "deletions": 3,
        "changes": 6,
        "patch": "@@ -49,7 +49,7 @@ class LlavaNextVideoProcessor(ProcessorMixin):\n     Constructs a LLaVa-NeXT-Video processor which wraps a LLaVa-NeXT image processor, LLaVa-NeXT-Video video processor and\n     a LLaMa tokenizer into a single processor.\n \n-    [`LlavaNextVideoProcessor`] offers all the functionalities of [`LlavaNextImageProcessor`], [`LlavaNextVideoImageProcessor`] and\n+    [`LlavaNextVideoProcessor`] offers all the functionalities of [`LlavaNextImageProcessor`], [`LlavaNextVideoVideoProcessor`] and\n     [`LlamaTokenizerFast`]. See the [`~LlavaNextVideoProcessor.__call__`] and [`~LlavaNextVideoProcessor.decode`] for more information.\n \n     Args:\n@@ -124,8 +124,8 @@ def __call__(\n         and `kwargs` arguments to LlamaTokenizerFast's [`~LlamaTokenizerFast.__call__`] if `text` is not `None` to encode\n         the text. To prepare the image(s), this method forwards the `images` and `kwargs` arguments to\n         LlavaNextImageProcessor's [`~LlavaNextImageProcessor.__call__`] if `images` is not `None`. To prepare the video(s),\n-        this method forwards the `videos` and `kwargs` arguments to LlavaNextVideoImageProcessor's\n-        [`~LlavaNextVideoImageProcessor.__call__`] if `videos` is not `None`. Please refer to the docstring\n+        this method forwards the `videos` and `kwargs` arguments to LlavaNextVideoVideoProcessor's\n+        [`~LlavaNextVideoVideoProcessor.__call__`] if `videos` is not `None`. Please refer to the docstring\n         of the above two methods for more information.\n \n         Args:"
      },
      {
        "filename": "src/transformers/models/qwen2_5_omni/processing_qwen2_5_omni.py",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -341,11 +341,13 @@ def model_input_names(self):\n         tokenizer_input_names = self.tokenizer.model_input_names\n         feature_extractor_input_names = self.feature_extractor.model_input_names\n         image_processor_input_names = self.image_processor.model_input_names\n+        video_processor_input_names = self.video_processor.model_input_names\n         return list(\n             dict.fromkeys(\n                 tokenizer_input_names\n                 + feature_extractor_input_names\n                 + image_processor_input_names\n+                + video_processor_input_names\n                 + [\"feature_attention_mask\"]\n                 + [\"video_second_per_grid\"]\n             )"
      },
      {
        "filename": "src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py",
        "status": "modified",
        "additions": 4,
        "deletions": 1,
        "changes": 5,
        "patch": "@@ -858,7 +858,10 @@ class Qwen2_5_VLProcessor(Qwen2VLProcessor):\n     def model_input_names(self):\n         tokenizer_input_names = self.tokenizer.model_input_names\n         image_processor_input_names = self.image_processor.model_input_names\n-        names_from_processor = list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n+        video_processor_input_names = self.video_processor.model_input_names\n+        names_from_processor = list(\n+            dict.fromkeys(tokenizer_input_names + image_processor_input_names + video_processor_input_names)\n+        )\n         return names_from_processor + [\"second_per_grid_ts\"]\n \n     def __call__("
      },
      {
        "filename": "src/transformers/models/qwen2_5_vl/processing_qwen2_5_vl.py",
        "status": "modified",
        "additions": 4,
        "deletions": 1,
        "changes": 5,
        "patch": "@@ -255,7 +255,10 @@ def post_process_image_text_to_text(\n     def model_input_names(self):\n         tokenizer_input_names = self.tokenizer.model_input_names\n         image_processor_input_names = self.image_processor.model_input_names\n-        names_from_processor = list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n+        video_processor_input_names = self.video_processor.model_input_names\n+        names_from_processor = list(\n+            dict.fromkeys(tokenizer_input_names + image_processor_input_names + video_processor_input_names)\n+        )\n         return names_from_processor + [\"second_per_grid_ts\"]\n \n "
      },
      {
        "filename": "src/transformers/models/qwen2_vl/image_processing_qwen2_vl.py",
        "status": "modified",
        "additions": 25,
        "deletions": 66,
        "changes": 91,
        "patch": "@@ -46,7 +46,7 @@\n )\n from ...processing_utils import ImagesKwargs\n from ...utils import TensorType, logging\n-from ...video_utils import VideoInput, make_batched_videos\n+from ...video_utils import VideoInput\n \n \n logger = logging.get_logger(__name__)\n@@ -137,7 +137,7 @@ class Qwen2VLImageProcessor(BaseImageProcessor):\n             The merge size of the vision encoder to llm encoder.\n     \"\"\"\n \n-    model_input_names = [\"pixel_values\", \"image_grid_thw\", \"pixel_values_videos\", \"video_grid_thw\"]\n+    model_input_names = [\"pixel_values\", \"image_grid_thw\"]\n     valid_kwargs = Qwen2VLImageProcessorKwargs\n \n     def __init__(\n@@ -322,7 +322,6 @@ def _preprocess(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        videos: Optional[VideoInput] = None,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n         min_pixels: Optional[int] = None,\n@@ -346,9 +345,6 @@ def preprocess(\n             images (`ImageInput`):\n                 Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If\n                 passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n-            videos (`VideoInput`):\n-                Video to preprocess. Expects a single or batch of videos with pixel values ranging from 0 to 255. If\n-                passing in videos with pixel values between 0 and 1, set `do_rescale=False`.\n             do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n                 Whether to resize the image.\n             size (`dict[str, int]`, *optional*, defaults to `self.size`):\n@@ -442,67 +438,30 @@ def preprocess(\n         )\n \n         data = {}\n-        if images is not None:\n-            pixel_values, vision_grid_thws = [], []\n-            for image in images:\n-                patches, image_grid_thw = self._preprocess(\n-                    image,\n-                    do_resize=do_resize,\n-                    size=size,\n-                    resample=resample,\n-                    do_rescale=do_rescale,\n-                    rescale_factor=rescale_factor,\n-                    do_normalize=do_normalize,\n-                    image_mean=image_mean,\n-                    image_std=image_std,\n-                    patch_size=patch_size,\n-                    temporal_patch_size=temporal_patch_size,\n-                    merge_size=merge_size,\n-                    data_format=data_format,\n-                    do_convert_rgb=do_convert_rgb,\n-                    input_data_format=input_data_format,\n-                )\n-                pixel_values.extend(patches)\n-                vision_grid_thws.append(image_grid_thw)\n-            pixel_values = np.array(pixel_values)\n-            vision_grid_thws = np.array(vision_grid_thws)\n-            data.update({\"pixel_values\": pixel_values, \"image_grid_thw\": vision_grid_thws})\n-\n-        # kept for BC only and should be removed after v5.0\n-        if videos is not None:\n-            logger.warning(\n-                \"`Qwen2VLImageProcessor` works only with image inputs and doesn't process videos anymore. \"\n-                \"This is a deprecated behavior and will be removed in v5.0. \"\n-                \"Your videos should be forwarded to `Qwen2VLVideoProcessor`. \"\n-            )\n-            videos = make_batched_videos(videos)\n-            pixel_values_videos, vision_grid_thws_videos = [], []\n-            for images in videos:\n-                patches, video_grid_thw = self._preprocess(\n-                    images,\n-                    do_resize=do_resize,\n-                    size=size,\n-                    resample=resample,\n-                    do_rescale=do_rescale,\n-                    rescale_factor=rescale_factor,\n-                    do_normalize=do_normalize,\n-                    image_mean=image_mean,\n-                    image_std=image_std,\n-                    patch_size=patch_size,\n-                    temporal_patch_size=temporal_patch_size,\n-                    merge_size=merge_size,\n-                    data_format=data_format,\n-                    do_convert_rgb=do_convert_rgb,\n-                    input_data_format=input_data_format,\n-                )\n-                pixel_values_videos.extend(patches)\n-                vision_grid_thws_videos.append(video_grid_thw)\n-            data.update(\n-                {\n-                    \"pixel_values_videos\": np.array(pixel_values_videos),\n-                    \"video_grid_thw\": np.array(vision_grid_thws_videos),\n-                }\n+        pixel_values, vision_grid_thws = [], []\n+        for image in images:\n+            patches, image_grid_thw = self._preprocess(\n+                image,\n+                do_resize=do_resize,\n+                size=size,\n+                resample=resample,\n+                do_rescale=do_rescale,\n+                rescale_factor=rescale_factor,\n+                do_normalize=do_normalize,\n+                image_mean=image_mean,\n+                image_std=image_std,\n+                patch_size=patch_size,\n+                temporal_patch_size=temporal_patch_size,\n+                merge_size=merge_size,\n+                data_format=data_format,\n+                do_convert_rgb=do_convert_rgb,\n+                input_data_format=input_data_format,\n             )\n+            pixel_values.extend(patches)\n+            vision_grid_thws.append(image_grid_thw)\n+        pixel_values = np.array(pixel_values)\n+        vision_grid_thws = np.array(vision_grid_thws)\n+        data.update({\"pixel_values\": pixel_values, \"image_grid_thw\": vision_grid_thws})\n \n         return BatchFeature(data=data, tensor_type=return_tensors)\n "
      },
      {
        "filename": "src/transformers/models/qwen2_vl/image_processing_qwen2_vl_fast.py",
        "status": "modified",
        "additions": 6,
        "deletions": 26,
        "changes": 32,
        "patch": "@@ -44,7 +44,6 @@\n     auto_docstring,\n     logging,\n )\n-from ...video_utils import VideoInput, make_batched_videos\n from .image_processing_qwen2_vl import Qwen2VLImageProcessorKwargs, smart_resize\n \n \n@@ -67,7 +66,7 @@ class Qwen2VLImageProcessorFast(BaseImageProcessorFast):\n     min_pixels = None\n     max_pixels = None\n     valid_kwargs = Qwen2VLImageProcessorKwargs\n-    model_input_names = [\"pixel_values\", \"image_grid_thw\", \"pixel_values_videos\", \"video_grid_thw\"]\n+    model_input_names = [\"pixel_values\", \"image_grid_thw\"]\n \n     def __init__(self, **kwargs: Unpack[Qwen2VLImageProcessorKwargs]):\n         size = kwargs.pop(\"size\", None)\n@@ -113,15 +112,13 @@ def _further_process_kwargs(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        videos: Optional[VideoInput] = None,\n         **kwargs: Unpack[Qwen2VLImageProcessorKwargs],\n     ) -> BatchFeature:\n-        return super().preprocess(images, videos, **kwargs)\n+        return super().preprocess(images, **kwargs)\n \n     def _preprocess_image_like_inputs(\n         self,\n         images: ImageInput,\n-        videos: VideoInput,\n         do_convert_rgb: bool,\n         input_data_format: ChannelDimension,\n         device: Optional[Union[str, \"torch.device\"]] = None,\n@@ -134,27 +131,10 @@ def _preprocess_image_like_inputs(\n         \"\"\"\n         # Prepare input images\n         batch_feature = BatchFeature()\n-        if images is not None:\n-            images = self._prepare_image_like_inputs(\n-                images=images, do_convert_rgb=do_convert_rgb, input_data_format=input_data_format, device=device\n-            )\n-            batch_feature = self._preprocess(images, **kwargs)\n-        if videos is not None:\n-            logger.warning(\n-                \"`Qwen2VLImageProcessorFast` works only with image inputs and doesn't process videos anymore. \"\n-                \"This is a deprecated behavior and will be removed in v5.0. \"\n-                \"Your videos should be forwarded to `Qwen2VLVideoProcessor`. \"\n-            )\n-            # Can't change _prepare_images_structure to work with videos because it also needs to work with images.\n-            videos = make_batched_videos(videos)\n-            videos = [\n-                torch.stack(self._prepare_image_like_inputs(video, do_convert_rgb, input_data_format, device))\n-                for video in videos\n-            ]\n-            video_outputs = self._preprocess(videos, **kwargs)\n-            batch_feature.update(\n-                {\"pixel_values_videos\": video_outputs.pixel_values, \"video_grid_thw\": video_outputs.image_grid_thw}\n-            )\n+        images = self._prepare_image_like_inputs(\n+            images=images, do_convert_rgb=do_convert_rgb, input_data_format=input_data_format, device=device\n+        )\n+        batch_feature = self._preprocess(images, **kwargs)\n         return batch_feature\n \n     def _preprocess("
      },
      {
        "filename": "src/transformers/models/qwen3_omni_moe/processing_qwen3_omni_moe.py",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -340,11 +340,13 @@ def model_input_names(self):\n         tokenizer_input_names = self.tokenizer.model_input_names\n         feature_extractor_input_names = self.feature_extractor.model_input_names\n         image_processor_input_names = self.image_processor.model_input_names\n+        video_processor_input_names = self.video_processor.model_input_names\n         return list(\n             dict.fromkeys(\n                 tokenizer_input_names\n                 + feature_extractor_input_names\n                 + image_processor_input_names\n+                + video_processor_input_names\n                 + [\"feature_attention_mask\"]\n                 + [\"video_second_per_grid\"]\n             )"
      },
      {
        "filename": "src/transformers/models/video_llama_3/image_processing_video_llama_3_fast.py",
        "status": "modified",
        "additions": 13,
        "deletions": 44,
        "changes": 57,
        "patch": "@@ -34,14 +34,10 @@\n     SizeDict,\n )\n from ...processing_utils import Unpack\n-from ...utils import TensorType, auto_docstring, logging\n-from ...video_utils import VideoInput, make_batched_videos\n+from ...utils import TensorType, auto_docstring\n from .image_processing_video_llama_3 import VideoLlama3ImageProcessorKwargs\n \n \n-logger = logging.get_logger(__name__)\n-\n-\n def smart_resize(\n     height: int, width: int, factor: int = 28, min_pixels: int = 56 * 56, max_pixels: int = 14 * 14 * 4 * 1280\n ):\n@@ -91,9 +87,6 @@ class VideoLlama3ImageProcessorFast(BaseImageProcessorFast):\n         \"pixel_values\",\n         \"image_grid_thw\",\n         \"image_merge_sizes\",\n-        \"pixel_values_videos\",\n-        \"video_grid_thw\",\n-        \"video_merge_sizes\",\n     ]\n \n     def __init__(self, **kwargs: Unpack[VideoLlama3ImageProcessorKwargs]):\n@@ -140,15 +133,13 @@ def _further_process_kwargs(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        videos: Optional[VideoInput] = None,\n         **kwargs: Unpack[VideoLlama3ImageProcessorKwargs],\n     ) -> BatchFeature:\n-        return super().preprocess(images, videos, **kwargs)\n+        return super().preprocess(images, **kwargs)\n \n     def _preprocess_image_like_inputs(\n         self,\n         images: ImageInput,\n-        videos: VideoInput,\n         do_convert_rgb: bool,\n         input_data_format: ChannelDimension,\n         device: Optional[Union[str, \"torch.device\"]] = None,\n@@ -161,39 +152,17 @@ def _preprocess_image_like_inputs(\n         \"\"\"\n         # Prepare input images\n         batch_feature = BatchFeature()\n-        if images is not None:\n-            if kwargs[\"temporal_patch_size\"] != 1:\n-                raise ValueError(\"`temporal_patch_size` must be 1 for VideoLLaMA3\")\n-            images = self._prepare_image_like_inputs(\n-                images=images, do_convert_rgb=do_convert_rgb, input_data_format=input_data_format, device=device\n-            )\n-            batch_feature = self._preprocess(images, **kwargs)\n-            batch_feature[\"image_merge_sizes\"] = torch.tensor(\n-                [kwargs[\"merge_size\"]] * batch_feature.image_grid_thw.size(0),\n-                dtype=batch_feature.image_grid_thw.dtype,\n-                device=batch_feature.image_grid_thw.device,\n-            )\n-        if videos is not None:\n-            logger.warning(\n-                \"`VideoLlama3ImageProcessorFast` works only with image inputs and doesn't process videos anymore. \"\n-                \"This is a deprecated behavior and will be removed in v5.0. \"\n-                \"Your videos should be forwarded to `VideoLlama3VideoProcessor`. \"\n-            )\n-            # Can't change _prepare_images_structure to work with videos because it also needs to work with images.\n-            videos = make_batched_videos(videos)\n-            videos = [\n-                torch.stack(self._prepare_image_like_inputs(video, do_convert_rgb, input_data_format, device))\n-                for video in videos\n-            ]\n-            video_outputs = self._preprocess(videos, **kwargs)\n-            batch_feature.update(\n-                {\"pixel_values_videos\": video_outputs.pixel_values, \"video_grid_thw\": video_outputs.image_grid_thw}\n-            )\n-            batch_feature[\"video_merge_sizes\"] = torch.tensor(\n-                [kwargs[\"merge_size\"]] * video_outputs.image_grid_thw.size(0),\n-                dtype=video_outputs.image_grid_thw.dtype,\n-                device=video_outputs.image_grid_thw.device,\n-            )\n+        if kwargs[\"temporal_patch_size\"] != 1:\n+            raise ValueError(\"`temporal_patch_size` must be 1 for VideoLLaMA3\")\n+        images = self._prepare_image_like_inputs(\n+            images=images, do_convert_rgb=do_convert_rgb, input_data_format=input_data_format, device=device\n+        )\n+        batch_feature = self._preprocess(images, **kwargs)\n+        batch_feature[\"image_merge_sizes\"] = torch.tensor(\n+            [kwargs[\"merge_size\"]] * batch_feature.image_grid_thw.size(0),\n+            dtype=batch_feature.image_grid_thw.dtype,\n+            device=batch_feature.image_grid_thw.device,\n+        )\n         return batch_feature\n \n     def _preprocess("
      },
      {
        "filename": "src/transformers/models/video_llama_3/modular_video_llama_3.py",
        "status": "modified",
        "additions": 11,
        "deletions": 38,
        "changes": 49,
        "patch": "@@ -50,7 +50,6 @@\n from ...video_utils import (\n     VideoInput,\n     group_videos_by_shape,\n-    make_batched_videos,\n     reorder_videos,\n )\n from ..auto import CONFIG_MAPPING, AutoConfig\n@@ -1446,55 +1445,29 @@ class VideoLlama3ImageProcessorFast(Qwen2VLImageProcessorFast):\n         \"pixel_values\",\n         \"image_grid_thw\",\n         \"image_merge_sizes\",\n-        \"pixel_values_videos\",\n-        \"video_grid_thw\",\n-        \"video_merge_sizes\",\n     ]\n \n     def _preprocess_image_like_inputs(\n         self,\n         images: ImageInput,\n-        videos: VideoInput,\n         do_convert_rgb: bool,\n         input_data_format: ChannelDimension,\n         device: Optional[Union[str, \"torch.device\"]] = None,\n         **kwargs: Unpack[VideoLlama3ImageProcessorKwargs],\n     ) -> BatchFeature:\n         # Prepare input images\n         batch_feature = BatchFeature()\n-        if images is not None:\n-            if kwargs[\"temporal_patch_size\"] != 1:\n-                raise ValueError(\"`temporal_patch_size` must be 1 for VideoLLaMA3\")\n-            images = self._prepare_image_like_inputs(\n-                images=images, do_convert_rgb=do_convert_rgb, input_data_format=input_data_format, device=device\n-            )\n-            batch_feature = self._preprocess(images, **kwargs)\n-            batch_feature[\"image_merge_sizes\"] = torch.tensor(\n-                [kwargs[\"merge_size\"]] * batch_feature.image_grid_thw.size(0),\n-                dtype=batch_feature.image_grid_thw.dtype,\n-                device=batch_feature.image_grid_thw.device,\n-            )\n-        if videos is not None:\n-            logger.warning(\n-                \"`VideoLlama3ImageProcessorFast` works only with image inputs and doesn't process videos anymore. \"\n-                \"This is a deprecated behavior and will be removed in v5.0. \"\n-                \"Your videos should be forwarded to `VideoLlama3VideoProcessor`. \"\n-            )\n-            # Can't change _prepare_images_structure to work with videos because it also needs to work with images.\n-            videos = make_batched_videos(videos)\n-            videos = [\n-                torch.stack(self._prepare_image_like_inputs(video, do_convert_rgb, input_data_format, device))\n-                for video in videos\n-            ]\n-            video_outputs = self._preprocess(videos, **kwargs)\n-            batch_feature.update(\n-                {\"pixel_values_videos\": video_outputs.pixel_values, \"video_grid_thw\": video_outputs.image_grid_thw}\n-            )\n-            batch_feature[\"video_merge_sizes\"] = torch.tensor(\n-                [kwargs[\"merge_size\"]] * video_outputs.image_grid_thw.size(0),\n-                dtype=video_outputs.image_grid_thw.dtype,\n-                device=video_outputs.image_grid_thw.device,\n-            )\n+        if kwargs[\"temporal_patch_size\"] != 1:\n+            raise ValueError(\"`temporal_patch_size` must be 1 for VideoLLaMA3\")\n+        images = self._prepare_image_like_inputs(\n+            images=images, do_convert_rgb=do_convert_rgb, input_data_format=input_data_format, device=device\n+        )\n+        batch_feature = self._preprocess(images, **kwargs)\n+        batch_feature[\"image_merge_sizes\"] = torch.tensor(\n+            [kwargs[\"merge_size\"]] * batch_feature.image_grid_thw.size(0),\n+            dtype=batch_feature.image_grid_thw.dtype,\n+            device=batch_feature.image_grid_thw.device,\n+        )\n         return batch_feature\n \n "
      },
      {
        "filename": "src/transformers/models/video_llava/image_processing_video_llava.py",
        "status": "modified",
        "additions": 19,
        "deletions": 58,
        "changes": 77,
        "patch": "@@ -39,7 +39,6 @@\n     validate_preprocess_arguments,\n )\n from ...utils import TensorType, filter_out_non_signature_kwargs, logging\n-from ...video_utils import VideoInput, make_batched_videos\n \n \n logger = logging.get_logger(__name__)\n@@ -172,7 +171,6 @@ def resize(\n     def preprocess(\n         self,\n         images: Optional[list[ImageInput]] = None,\n-        videos: Optional[list[VideoInput]] = None,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n         resample: Optional[PILImageResampling] = None,\n@@ -195,9 +193,6 @@ def preprocess(\n             images (`ImageInput`, *optional*):\n                 List of images to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If\n                 passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n-            videos (`VideoInput`, *optional*):\n-                List of videos to preprocess. Expects a single or batch of videos with pixel values ranging from 0 to 255. If\n-                passing in videos with pixel values between 0 and 1, set `do_rescale=False`.\n             do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n                 Whether to resize the image.\n             size (`dict[str, int]`, *optional*, defaults to `self.size`):\n@@ -261,60 +256,26 @@ def preprocess(\n         if images is not None and not valid_images(images):\n             raise ValueError(\"Invalid input type. Must be of type PIL.Image.Image, numpy.ndarray, or torch.Tensor\")\n \n-        data = {}\n-        if videos is not None:\n-            logger.warning(\n-                \"`VideoLlavaImageProcessor` works only with image inputs and doesn't process videos anymore. \"\n-                \"This is a deprecated behavior and will be removed in v5.0. \"\n-                \"Your videos should be forwarded to `VideoLlavaVideoProcessor`. \"\n+        pixel_values_images = [\n+            self._preprocess_image(\n+                image=image,\n+                do_resize=do_resize,\n+                size=size,\n+                resample=resample,\n+                do_rescale=do_rescale,\n+                rescale_factor=rescale_factor,\n+                do_normalize=do_normalize,\n+                image_mean=image_mean,\n+                image_std=image_std,\n+                do_center_crop=do_center_crop,\n+                crop_size=crop_size,\n+                do_convert_rgb=do_convert_rgb,\n+                data_format=data_format,\n+                input_data_format=input_data_format,\n             )\n-            videos = make_batched_videos(videos)\n-            pixel_values_videos = [\n-                [\n-                    self._preprocess_image(\n-                        image=frame,\n-                        do_resize=do_resize,\n-                        size=size,\n-                        resample=resample,\n-                        do_rescale=do_rescale,\n-                        rescale_factor=rescale_factor,\n-                        do_normalize=do_normalize,\n-                        image_mean=image_mean,\n-                        image_std=image_std,\n-                        do_center_crop=do_center_crop,\n-                        crop_size=crop_size,\n-                        do_convert_rgb=do_convert_rgb,\n-                        data_format=data_format,\n-                        input_data_format=input_data_format,\n-                    )\n-                    for frame in video\n-                ]\n-                for video in videos\n-            ]\n-            data[\"pixel_values_videos\"] = pixel_values_videos\n-\n-        if images is not None:\n-            pixel_values_images = [\n-                self._preprocess_image(\n-                    image=image,\n-                    do_resize=do_resize,\n-                    size=size,\n-                    resample=resample,\n-                    do_rescale=do_rescale,\n-                    rescale_factor=rescale_factor,\n-                    do_normalize=do_normalize,\n-                    image_mean=image_mean,\n-                    image_std=image_std,\n-                    do_center_crop=do_center_crop,\n-                    crop_size=crop_size,\n-                    do_convert_rgb=do_convert_rgb,\n-                    data_format=data_format,\n-                    input_data_format=input_data_format,\n-                )\n-                for image in images\n-            ]\n-            data[\"pixel_values_images\"] = pixel_values_images\n-\n+            for image in images\n+        ]\n+        data = {\"pixel_values_images\": pixel_values_images}\n         encoded_outputs = BatchFeature(data, tensor_type=return_tensors)\n \n         return encoded_outputs"
      },
      {
        "filename": "tests/models/qwen2_vl/test_image_processing_qwen2_vl.py",
        "status": "modified",
        "additions": 0,
        "deletions": 43,
        "changes": 43,
        "patch": "@@ -274,31 +274,6 @@ def test_nested_input(self):\n             self.assertTrue((encoded_images_nested == encoded_images).all())\n             self.assertTrue((image_grid_thws_nested == expected_image_grid_thws).all())\n \n-    def test_video_inputs(self):\n-        for image_processing_class in self.image_processor_list:\n-            image_processing = image_processing_class(**self.image_processor_dict)\n-            expected_dims_by_frames = {1: 34300, 2: 34300, 3: 68600, 4: 68600, 5: 102900, 6: 102900}\n-\n-            for num_frames, expected_dims in expected_dims_by_frames.items():\n-                image_processor_tester = Qwen2VLImageProcessingTester(self, num_frames=num_frames)\n-                video_inputs = image_processor_tester.prepare_video_inputs(equal_resolution=True)\n-                process_out = image_processing(None, videos=video_inputs, return_tensors=\"pt\")\n-                encoded_video = process_out.pixel_values_videos\n-                expected_output_video_shape = (expected_dims, 1176)\n-                self.assertEqual(tuple(encoded_video.shape), expected_output_video_shape)\n-\n-    def test_custom_patch_size(self):\n-        for image_processing_class in self.image_processor_list:\n-            image_processing = image_processing_class(**self.image_processor_dict)\n-\n-            for patch_size in (1, 3, 5, 7):\n-                image_processor_tester = Qwen2VLImageProcessingTester(self, patch_size=patch_size)\n-                video_inputs = image_processor_tester.prepare_video_inputs(equal_resolution=True)\n-                process_out = image_processing(None, videos=video_inputs, return_tensors=\"pt\")\n-                encoded_video = process_out.pixel_values_videos\n-                expected_output_video_shape = (171500, 1176)\n-                self.assertEqual(tuple(encoded_video.shape), expected_output_video_shape)\n-\n     def test_custom_image_size(self):\n         for image_processing_class in self.image_processor_list:\n             image_processing = image_processing_class(**self.image_processor_dict)\n@@ -325,24 +300,6 @@ def test_custom_pixels(self):\n                 # Just checking that it doesn't raise an error\n                 image_processor(image_inputs, return_tensors=\"pt\")\n \n-    def test_temporal_padding(self):\n-        for image_processing_class in self.image_processor_list:\n-            # Initialize image_processing\n-            image_processing = image_processing_class(**self.image_processor_dict)\n-            # Create random video inputs with a number of frames not divisible by temporal_patch_size\n-            image_processor_tester = Qwen2VLImageProcessingTester(self, num_frames=5, temporal_patch_size=4)\n-            video_inputs = image_processor_tester.prepare_video_inputs(equal_resolution=True)\n-\n-            # Process the video inputs\n-            process_out = image_processing(None, videos=video_inputs, return_tensors=\"pt\")\n-            encoded_video = process_out.pixel_values_videos\n-\n-            # Check the shape after padding\n-            expected_output_video_shape = (102900, 1176)  # Adjusted based on padding\n-            self.assertEqual(tuple(encoded_video.shape), expected_output_video_shape)\n-            # Check divisibility by temporal_patch_size\n-            self.assertEqual(encoded_video.shape[0] % 4, 0)\n-\n     @require_vision\n     @require_torch\n     def test_slow_fast_equivalence(self):"
      }
    ],
    "num_files": 19,
    "scraped_at": "2025-11-16T21:17:29.105375",
    "filter_decision": {
      "accept": false,
      "reasoning": "This PR is primarily a cleanup/deletion task that removes deprecated video-related image processor classes as part of a v5 release. While it involves multiple files, the changes are largely mechanical deletions, documentation updates, and simple find-and-replace refactoring (renaming class references from `*ImageProcessor` to `*VideoProcessor`). There is no new logic, algorithms, or architectural decisions to generate meaningful technical questions about.",
      "substance_level": "low"
    }
  },
  {
    "pr_number": 41514,
    "title": "delete some tokenizer tests using pickle",
    "body": "# What does this PR do?\r\n\r\nThere is no room for `pickle` within `transformers`!",
    "html_url": "https://github.com/huggingface/transformers/pull/41514",
    "created_at": "2025-10-10T13:54:34Z",
    "merged_at": "2025-10-14T12:50:52Z",
    "merge_commit_sha": "abf5b57a684e665f03514535a53a668ddcc72303",
    "base_ref": "main",
    "head_sha": "15d109ff9376bca531ed89a004e3801a0f4126a2",
    "user": "ydshieh",
    "files": [
      {
        "filename": "tests/models/bert_japanese/test_tokenization_bert_japanese.py",
        "status": "modified",
        "additions": 0,
        "deletions": 63,
        "changes": 63,
        "patch": "@@ -14,7 +14,6 @@\n \n \n import os\n-import pickle\n import unittest\n \n from transformers import AutoTokenizer\n@@ -103,26 +102,6 @@ def test_full_tokenizer(self):\n         self.assertListEqual(tokens, [\"\u3053\u3093\u306b\u3061\u306f\", \"\u3001\", \"\u4e16\u754c\", \"\u3002\", \"\u3053\u3093\", \"##\u3070\u3093\u306f\", \"\u3001\", \"\u4e16\u754c\", \"\u3002\"])\n         self.assertListEqual(tokenizer.convert_tokens_to_ids(tokens), [3, 12, 10, 14, 4, 9, 12, 10, 14])\n \n-    def test_pickle_mecab_tokenizer(self):\n-        tokenizer = self.tokenizer_class(self.vocab_file, word_tokenizer_type=\"mecab\")\n-        self.assertIsNotNone(tokenizer)\n-\n-        text = \"\u3053\u3093\u306b\u3061\u306f\u3001\u4e16\u754c\u3002\\n\u3053\u3093\u3070\u3093\u306f\u3001\u4e16\u754c\u3002\"\n-        tokens = tokenizer.tokenize(text)\n-        self.assertListEqual(tokens, [\"\u3053\u3093\u306b\u3061\u306f\", \"\u3001\", \"\u4e16\u754c\", \"\u3002\", \"\u3053\u3093\", \"##\u3070\u3093\u306f\", \"\u3001\", \"\u4e16\u754c\", \"\u3002\"])\n-        self.assertListEqual(tokenizer.convert_tokens_to_ids(tokens), [3, 12, 10, 14, 4, 9, 12, 10, 14])\n-\n-        filename = os.path.join(self.tmpdirname, \"tokenizer.bin\")\n-        with open(filename, \"wb\") as handle:\n-            pickle.dump(tokenizer, handle)\n-\n-        with open(filename, \"rb\") as handle:\n-            tokenizer_new = pickle.load(handle)\n-\n-        tokens_loaded = tokenizer_new.tokenize(text)\n-\n-        self.assertListEqual(tokens, tokens_loaded)\n-\n     def test_mecab_full_tokenizer_with_mecab_kwargs(self):\n         tokenizer = self.tokenizer_class(\n             self.vocab_file, word_tokenizer_type=\"mecab\", mecab_kwargs={\"mecab_dic\": \"ipadic\"}\n@@ -198,27 +177,6 @@ def test_mecab_tokenizer_no_normalize(self):\n             [\"\uff71\uff6f\uff8c\uff9f\uff99\u30b9\u30c8\u30a2\", \"\u3067\", \"iPhone\", \"\uff18\", \"\u304c\", \"\u767a\u58f2\", \"\u3055\", \"\u308c\", \"\u305f\", \"\u3000\", \"\u3002\"],\n         )\n \n-    @require_sudachi_projection\n-    def test_pickle_sudachi_tokenizer(self):\n-        tokenizer = self.tokenizer_class(self.vocab_file, word_tokenizer_type=\"sudachi\")\n-        self.assertIsNotNone(tokenizer)\n-\n-        text = \"\u3053\u3093\u306b\u3061\u306f\u3001\u4e16\u754c\u3002\\n\u3053\u3093\u3070\u3093\u306f\u3001\u4e16\u754c\u3002\"\n-        tokens = tokenizer.tokenize(text)\n-        self.assertListEqual(tokens, [\"\u3053\u3093\u306b\u3061\u306f\", \"\u3001\", \"\u4e16\u754c\", \"\u3002\", \"\u3053\u3093\", \"##\u3070\u3093\u306f\", \"\u3001\", \"\u4e16\u754c\", \"\u3002\"])\n-        self.assertListEqual(tokenizer.convert_tokens_to_ids(tokens), [3, 12, 10, 14, 4, 9, 12, 10, 14])\n-\n-        filename = os.path.join(self.tmpdirname, \"tokenizer.bin\")\n-        with open(filename, \"wb\") as handle:\n-            pickle.dump(tokenizer, handle)\n-\n-        with open(filename, \"rb\") as handle:\n-            tokenizer_new = pickle.load(handle)\n-\n-        tokens_loaded = tokenizer_new.tokenize(text)\n-\n-        self.assertListEqual(tokens, tokens_loaded)\n-\n     @require_sudachi_projection\n     def test_sudachi_tokenizer_core(self):\n         tokenizer = SudachiTokenizer(sudachi_dict_type=\"core\")\n@@ -293,27 +251,6 @@ def test_sudachi_tokenizer_trim_whitespace(self):\n             [\"\u30a2\u30c3\u30d7\u30eb\", \"\u30b9\u30c8\u30a2\", \"\u3067\", \"iPhone\", \"8\", \"\u304c\", \"\u767a\u58f2\", \"\u3055\", \"\u308c\", \"\u305f\", \"\u3002\"],\n         )\n \n-    @require_jumanpp\n-    def test_pickle_jumanpp_tokenizer(self):\n-        tokenizer = self.tokenizer_class(self.vocab_file, word_tokenizer_type=\"jumanpp\")\n-        self.assertIsNotNone(tokenizer)\n-\n-        text = \"\u3053\u3093\u306b\u3061\u306f\u3001\u4e16\u754c\u3002\\n\u3053\u3093\u3070\u3093\u306f\u3001\u4e16\u754c\u3002\"\n-        tokens = tokenizer.tokenize(text)\n-        self.assertListEqual(tokens, [\"\u3053\u3093\u306b\u3061\u306f\", \"\u3001\", \"\u4e16\u754c\", \"\u3002\", \"\u3053\u3093\", \"##\u3070\u3093\u306f\", \"\u3001\", \"\u4e16\u754c\", \"\u3002\"])\n-        self.assertListEqual(tokenizer.convert_tokens_to_ids(tokens), [3, 12, 10, 14, 4, 9, 12, 10, 14])\n-\n-        filename = os.path.join(self.tmpdirname, \"tokenizer.bin\")\n-        with open(filename, \"wb\") as handle:\n-            pickle.dump(tokenizer, handle)\n-\n-        with open(filename, \"rb\") as handle:\n-            tokenizer_new = pickle.load(handle)\n-\n-        tokens_loaded = tokenizer_new.tokenize(text)\n-\n-        self.assertListEqual(tokens, tokens_loaded)\n-\n     @require_jumanpp\n     def test_jumanpp_tokenizer(self):\n         tokenizer = JumanppTokenizer()"
      },
      {
        "filename": "tests/models/code_llama/test_tokenization_code_llama.py",
        "status": "modified",
        "additions": 0,
        "deletions": 12,
        "changes": 12,
        "patch": "@@ -13,7 +13,6 @@\n # limitations under the License.\n \n import os\n-import pickle\n import shutil\n import tempfile\n import unittest\n@@ -293,17 +292,6 @@ def test_tokenizer_integration(self):\n             padding=False,\n         )\n \n-    def test_picklable(self):\n-        with tempfile.NamedTemporaryFile() as f:\n-            shutil.copyfile(SAMPLE_VOCAB, f.name)\n-            tokenizer = CodeLlamaTokenizer(f.name, keep_accents=True)\n-            pickled_tokenizer = pickle.dumps(tokenizer)\n-        pickle.loads(pickled_tokenizer)\n-\n-    @unittest.skip(reason=\"worker 'gw4' crashed on CI, passing locally.\")\n-    def test_pickle_subword_regularization_tokenizer(self):\n-        pass\n-\n     @unittest.skip(reason=\"worker 'gw4' crashed on CI, passing locally.\")\n     def test_subword_regularization_tokenizer(self):\n         pass"
      },
      {
        "filename": "tests/models/gemma/test_tokenization_gemma.py",
        "status": "modified",
        "additions": 0,
        "deletions": 4,
        "changes": 4,
        "patch": "@@ -140,10 +140,6 @@ def test_tokenizer_integration(self):\n             padding=False,\n         )\n \n-    @unittest.skip(reason=\"worker 'gw4' crashed on CI, passing locally.\")\n-    def test_pickle_subword_regularization_tokenizer(self):\n-        pass\n-\n     @unittest.skip(reason=\"worker 'gw4' crashed on CI, passing locally.\")\n     def test_subword_regularization_tokenizer(self):\n         pass"
      },
      {
        "filename": "tests/models/llama/test_tokenization_llama.py",
        "status": "modified",
        "additions": 0,
        "deletions": 12,
        "changes": 12,
        "patch": "@@ -13,7 +13,6 @@\n # limitations under the License.\n \n import os\n-import pickle\n import shutil\n import tempfile\n import unittest\n@@ -291,17 +290,6 @@ def test_tokenizer_integration(self):\n             padding=False,\n         )\n \n-    def test_picklable(self):\n-        with tempfile.NamedTemporaryFile() as f:\n-            shutil.copyfile(SAMPLE_VOCAB, f.name)\n-            tokenizer = LlamaTokenizer(f.name, keep_accents=True)\n-            pickled_tokenizer = pickle.dumps(tokenizer)\n-        pickle.loads(pickled_tokenizer)\n-\n-    @unittest.skip(reason=\"worker 'gw4' crashed on CI, passing locally.\")\n-    def test_pickle_subword_regularization_tokenizer(self):\n-        pass\n-\n     @unittest.skip(reason=\"worker 'gw4' crashed on CI, passing locally.\")\n     def test_subword_regularization_tokenizer(self):\n         pass"
      },
      {
        "filename": "tests/models/moshi/test_tokenization_moshi.py",
        "status": "modified",
        "additions": 0,
        "deletions": 15,
        "changes": 15,
        "patch": "@@ -13,9 +13,6 @@\n # limitations under the License.\n \n import inspect\n-import pickle\n-import shutil\n-import tempfile\n import unittest\n \n from transformers import (\n@@ -171,18 +168,6 @@ def test_special_tokens_initialization(self):\n \n                 self.assertTrue(special_token_id in r_output)\n \n-    def test_picklable(self):\n-        with tempfile.NamedTemporaryFile() as f:\n-            shutil.copyfile(SAMPLE_VOCAB, f.name)\n-            tokenizer = PreTrainedTokenizerFast(\n-                tokenizer_object=MoshiConverter(vocab_file=f.name).converted(),\n-                bos_token=\"<s>\",\n-                unk_token=\"<unk>\",\n-                eos_token=\"</s>\",\n-            )\n-            pickled_tokenizer = pickle.dumps(tokenizer)\n-        pickle.loads(pickled_tokenizer)\n-\n     def test_training_new_tokenizer(self):\n         # This feature only exists for fast tokenizers\n         if not self.test_rust_tokenizer:"
      },
      {
        "filename": "tests/models/pop2piano/test_tokenization_pop2piano.py",
        "status": "modified",
        "additions": 0,
        "deletions": 19,
        "changes": 19,
        "patch": "@@ -15,8 +15,6 @@\n Please note that Pop2PianoTokenizer is too far from our usual tokenizers and thus cannot use the TokenizerTesterMixin class.\n \"\"\"\n \n-import os\n-import pickle\n import shutil\n import tempfile\n import unittest\n@@ -224,23 +222,6 @@ def test_save_and_load_tokenizer(self):\n \n         shutil.rmtree(tmpdirname)\n \n-    def test_pickle_tokenizer(self):\n-        tmpdirname = tempfile.mkdtemp()\n-\n-        notes = self.get_input_notes()\n-        subwords = self.tokenizer(notes)[\"token_ids\"]\n-\n-        filename = os.path.join(tmpdirname, \"tokenizer.bin\")\n-        with open(filename, \"wb\") as handle:\n-            pickle.dump(self.tokenizer, handle)\n-\n-        with open(filename, \"rb\") as handle:\n-            tokenizer_new = pickle.load(handle)\n-\n-        subwords_loaded = tokenizer_new(notes)[\"token_ids\"]\n-\n-        self.assertListEqual(subwords, subwords_loaded)\n-\n     def test_padding_side_in_kwargs(self):\n         tokenizer_p = Pop2PianoTokenizer.from_pretrained(\"sweetcocoa/pop2piano\", padding_side=\"left\")\n         self.assertEqual(tokenizer_p.padding_side, \"left\")"
      },
      {
        "filename": "tests/models/seamless_m4t/test_tokenization_seamless_m4t.py",
        "status": "modified",
        "additions": 0,
        "deletions": 4,
        "changes": 4,
        "patch": "@@ -426,10 +426,6 @@ def test_training_new_tokenizer(self):\n \n         self.assertDictEqual(tokenizer.special_tokens_map, new_tokenizer.special_tokens_map)\n \n-    @unittest.skip(reason=\"Fails because of the hack of adding <unk> in _tokenize\")\n-    def test_pickle_subword_regularization_tokenizer(self):\n-        pass\n-\n     @unittest.skip(reason=\"Fails because of the hack of adding <unk> in _tokenize\")\n     def test_subword_regularization_tokenizer(self):\n         pass"
      },
      {
        "filename": "tests/models/siglip/test_tokenization_siglip.py",
        "status": "modified",
        "additions": 0,
        "deletions": 4,
        "changes": 4,
        "patch": "@@ -207,10 +207,6 @@ def test_eos_in_input(self):\n     def test_subword_regularization_tokenizer(self):\n         pass\n \n-    @unittest.skip(reason=\"SiglipTokenizer strips the punctuation\")\n-    def test_pickle_subword_regularization_tokenizer(self):\n-        pass\n-\n     # Copied from tests.models.t5.test_tokenization_t5.T5TokenizationTest.test_special_tokens_initialization with T5->Siglip\n     def test_special_tokens_initialization(self):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:"
      },
      {
        "filename": "tests/models/speecht5/test_tokenization_speecht5.py",
        "status": "modified",
        "additions": 0,
        "deletions": 4,
        "changes": 4,
        "patch": "@@ -143,10 +143,6 @@ def test_add_tokens_tokenizer(self):\n                 self.assertEqual(tokens[0], tokenizer.eos_token_id)\n                 self.assertEqual(tokens[-3], tokenizer.pad_token_id)\n \n-    @unittest.skip\n-    def test_pickle_subword_regularization_tokenizer(self):\n-        pass\n-\n     @unittest.skip\n     def test_subword_regularization_tokenizer(self):\n         pass"
      },
      {
        "filename": "tests/models/xglm/test_tokenization_xglm.py",
        "status": "modified",
        "additions": 0,
        "deletions": 10,
        "changes": 10,
        "patch": "@@ -12,9 +12,6 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-import pickle\n-import shutil\n-import tempfile\n import unittest\n from functools import cached_property\n \n@@ -141,13 +138,6 @@ def test_full_tokenizer(self):\n     def big_tokenizer(self):\n         return XGLMTokenizer.from_pretrained(\"facebook/xglm-564M\")\n \n-    def test_picklable_without_disk(self):\n-        with tempfile.NamedTemporaryFile() as f:\n-            shutil.copyfile(SAMPLE_VOCAB, f.name)\n-            tokenizer = XGLMTokenizer(f.name, keep_accents=True)\n-            pickled_tokenizer = pickle.dumps(tokenizer)\n-        pickle.loads(pickled_tokenizer)\n-\n     def test_rust_and_python_full_tokenizers(self):\n         if not self.test_rust_tokenizer:\n             self.skipTest(reason=\"test_rust_tokenizer is set to False\")"
      },
      {
        "filename": "tests/models/xlm_roberta/test_tokenization_xlm_roberta.py",
        "status": "modified",
        "additions": 0,
        "deletions": 8,
        "changes": 8,
        "patch": "@@ -12,7 +12,6 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-import pickle\n import shutil\n import tempfile\n import unittest\n@@ -215,13 +214,6 @@ def test_save_pretrained(self):\n     def big_tokenizer(self):\n         return XLMRobertaTokenizer.from_pretrained(\"FacebookAI/xlm-roberta-base\")\n \n-    def test_picklable_without_disk(self):\n-        with tempfile.NamedTemporaryFile() as f:\n-            shutil.copyfile(SAMPLE_VOCAB, f.name)\n-            tokenizer = XLMRobertaTokenizer(f.name, keep_accents=True)\n-            pickled_tokenizer = pickle.dumps(tokenizer)\n-        pickle.loads(pickled_tokenizer)\n-\n     def test_rust_and_python_full_tokenizers(self):\n         if not self.test_rust_tokenizer:\n             self.skipTest(reason=\"test_rust_tokenizer is set to False\")"
      },
      {
        "filename": "tests/test_tokenization_common.py",
        "status": "modified",
        "additions": 0,
        "deletions": 51,
        "changes": 51,
        "patch": "@@ -18,7 +18,6 @@\n import itertools\n import json\n import os\n-import pickle\n import re\n import shutil\n import tempfile\n@@ -520,28 +519,6 @@ def test_subword_regularization_tokenizer(self) -> None:\n             },\n         )\n \n-    def test_pickle_subword_regularization_tokenizer(self) -> None:\n-        if not self.test_sentencepiece:\n-            self.skipTest(reason=\"test_sentencepiece is set to False\")\n-\n-        \"\"\"Google pickle __getstate__ __setstate__ if you are struggling with this.\"\"\"\n-        # Subword regularization is only available for the slow tokenizer.\n-        sp_model_kwargs = {\"enable_sampling\": True, \"alpha\": 0.1, \"nbest_size\": -1}\n-        tokenizer = self.get_tokenizer(sp_model_kwargs=sp_model_kwargs)\n-        tokenizer_bin = pickle.dumps(tokenizer)\n-        del tokenizer\n-        tokenizer_new = pickle.loads(tokenizer_bin)\n-\n-        run_test_in_subprocess(\n-            test_case=self,\n-            target_func=_test_subword_regularization_tokenizer,\n-            inputs={\n-                \"tokenizer\": tokenizer_new,\n-                \"sp_model_kwargs\": sp_model_kwargs,\n-                \"test_sentencepiece_ignore_case\": self.test_sentencepiece_ignore_case,\n-            },\n-        )\n-\n     def test_save_sentencepiece_tokenizer(self) -> None:\n         if not self.test_sentencepiece or not self.test_slow_tokenizer:\n             self.skipTest(reason=\"test_sentencepiece or test_slow_tokenizer is set to False\")\n@@ -827,34 +804,6 @@ def test_save_and_load_tokenizer(self):\n \n                 shutil.rmtree(tmpdirname)\n \n-    def test_pickle_tokenizer(self):\n-        \"\"\"Google pickle __getstate__ __setstate__ if you are struggling with this.\"\"\"\n-        tokenizers = self.get_tokenizers()\n-        for tokenizer in tokenizers:\n-            with self.subTest(f\"{tokenizer.__class__.__name__}\"):\n-                self.assertIsNotNone(tokenizer)\n-\n-                text = \"Munich and Berlin are nice cities\"\n-                subwords = tokenizer.tokenize(text)\n-\n-                filename = os.path.join(self.tmpdirname, \"tokenizer.bin\")\n-                with open(filename, \"wb\") as handle:\n-                    pickle.dump(tokenizer, handle)\n-\n-                with open(filename, \"rb\") as handle:\n-                    tokenizer_new = pickle.load(handle)\n-\n-                subwords_loaded = tokenizer_new.tokenize(text)\n-\n-                self.assertListEqual(subwords, subwords_loaded)\n-\n-    @require_tokenizers\n-    def test_pickle_added_tokens(self):\n-        tok1 = AddedToken(\"<s>\", rstrip=True, lstrip=True, normalized=False, single_word=True)\n-        tok2 = pickle.loads(pickle.dumps(tok1))\n-\n-        self.assertEqual(tok1.__getstate__(), tok2.__getstate__())\n-\n     def test_added_tokens_do_lower_case(self):\n         tokenizers = self.get_tokenizers(do_lower_case=True)\n         for tokenizer in tokenizers:"
      },
      {
        "filename": "tests/tokenization/test_tokenization_utils.py",
        "status": "modified",
        "additions": 0,
        "deletions": 65,
        "changes": 65,
        "patch": "@@ -16,11 +16,8 @@\n \"\"\"\n \n import os\n-import pickle\n import tempfile\n import unittest\n-from collections.abc import Callable\n-from typing import Optional\n \n import numpy as np\n \n@@ -66,28 +63,6 @@ def check_tokenizer_from_pretrained(self, tokenizer_class):\n                 special_tok_id = tokenizer.convert_tokens_to_ids(special_tok)\n                 self.assertIsInstance(special_tok_id, int)\n \n-    def assert_dump_and_restore(self, be_original: BatchEncoding, equal_op: Optional[Callable] = None):\n-        batch_encoding_str = pickle.dumps(be_original)\n-        self.assertIsNotNone(batch_encoding_str)\n-\n-        be_restored = pickle.loads(batch_encoding_str)\n-\n-        # Ensure is_fast is correctly restored\n-        self.assertEqual(be_restored.is_fast, be_original.is_fast)\n-\n-        # Ensure encodings are potentially correctly restored\n-        if be_original.is_fast:\n-            self.assertIsNotNone(be_restored.encodings)\n-        else:\n-            self.assertIsNone(be_restored.encodings)\n-\n-        # Ensure the keys are the same\n-        for original_v, restored_v in zip(be_original.values(), be_restored.values()):\n-            if equal_op:\n-                self.assertTrue(equal_op(restored_v, original_v))\n-            else:\n-                self.assertEqual(restored_v, original_v)\n-\n     @slow\n     def test_pretrained_tokenizers(self):\n         self.check_tokenizer_from_pretrained(GPT2Tokenizer)\n@@ -96,46 +71,6 @@ def test_tensor_type_from_str(self):\n         self.assertEqual(TensorType(\"pt\"), TensorType.PYTORCH)\n         self.assertEqual(TensorType(\"np\"), TensorType.NUMPY)\n \n-    @require_tokenizers\n-    def test_batch_encoding_pickle(self):\n-        tokenizer_p = BertTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n-        tokenizer_r = BertTokenizerFast.from_pretrained(\"google-bert/bert-base-cased\")\n-\n-        # Python no tensor\n-        with self.subTest(\"BatchEncoding (Python, return_tensors=None)\"):\n-            self.assert_dump_and_restore(tokenizer_p(\"Small example to encode\"))\n-\n-        with self.subTest(\"BatchEncoding (Python, return_tensors=NUMPY)\"):\n-            self.assert_dump_and_restore(\n-                tokenizer_p(\"Small example to encode\", return_tensors=TensorType.NUMPY), np.array_equal\n-            )\n-\n-        with self.subTest(\"BatchEncoding (Rust, return_tensors=None)\"):\n-            self.assert_dump_and_restore(tokenizer_r(\"Small example to encode\"))\n-\n-        with self.subTest(\"BatchEncoding (Rust, return_tensors=NUMPY)\"):\n-            self.assert_dump_and_restore(\n-                tokenizer_r(\"Small example to encode\", return_tensors=TensorType.NUMPY), np.array_equal\n-            )\n-\n-    @require_torch\n-    @require_tokenizers\n-    def test_batch_encoding_pickle_pt(self):\n-        import torch\n-\n-        tokenizer_p = BertTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n-        tokenizer_r = BertTokenizerFast.from_pretrained(\"google-bert/bert-base-cased\")\n-\n-        with self.subTest(\"BatchEncoding (Python, return_tensors=PYTORCH)\"):\n-            self.assert_dump_and_restore(\n-                tokenizer_p(\"Small example to encode\", return_tensors=TensorType.PYTORCH), torch.equal\n-            )\n-\n-        with self.subTest(\"BatchEncoding (Rust, return_tensors=PYTORCH)\"):\n-            self.assert_dump_and_restore(\n-                tokenizer_r(\"Small example to encode\", return_tensors=TensorType.PYTORCH), torch.equal\n-            )\n-\n     @require_tokenizers\n     def test_batch_encoding_is_fast(self):\n         tokenizer_p = BertTokenizer.from_pretrained(\"google-bert/bert-base-cased\")"
      }
    ],
    "num_files": 13,
    "scraped_at": "2025-11-16T21:17:46.160624",
    "filter_decision": {
      "accept": false,
      "reasoning": "This PR is purely a deletion of test code with no new logic, algorithmic changes, or architectural decisions. It removes pickle-based tests across multiple tokenizer test files based on a policy decision (no room for pickle in transformers), which is cleanup/maintenance work rather than substantive code changes that would help developers understand the codebase.",
      "substance_level": "low"
    }
  },
  {
    "pr_number": 41507,
    "title": "[kernels] rm mra kernels",
    "body": "# What does this PR do?\r\n\r\nRemoves the mra kernels, and uses kernels from the hub : https://huggingface.co/kernels-community/mra instead",
    "html_url": "https://github.com/huggingface/transformers/pull/41507",
    "created_at": "2025-10-10T09:49:55Z",
    "merged_at": "2025-10-14T11:34:04Z",
    "merge_commit_sha": "8fe4db53994cdccf7629284add65655e6ce73af4",
    "base_ref": "main",
    "head_sha": "3d5f6a4aa53573b3bceae2d486dd36cd46495f46",
    "user": "MekkCyber",
    "files": [
      {
        "filename": "src/transformers/kernels/mra/cuda_kernel.cu",
        "status": "removed",
        "additions": 0,
        "deletions": 383,
        "changes": 383,
        "patch": "@@ -1,383 +0,0 @@\n-#include \"cuda_kernel.h\"\n-\n-//////////////////////////////////////////////////////////////////////////////////////////////////\n-//////////////////////////////////////////////////////////////////////////////////////////////////\n-\n-__global__ void index_max_cuda_kernel(\n-  float *index_vals,       // [batch_size, 32, num_block]\n-  int   *indices,        // [batch_size, num_block]\n-  float *max_vals,        // [batch_size, A_num_block * 32]\n-  float *max_vals_scatter,   // [batch_size, 32, num_block]\n-  long batch_size,\n-  long A_num_block,\n-  long B_num_block,\n-  long num_block\n-) {\n-\n-  long batch_idx = blockIdx.x;\n-\n-  long thread_idx = threadIdx.x;\n-  long num_thread = blockDim.x;\n-\n-  extern __shared__ float buffer[];\n-  int *max_buffer = (int*)buffer;\n-\n-  for (int i = 0; i < A_num_block * 32; i = i + num_thread) {\n-    int idx = i + thread_idx;\n-    if (idx < A_num_block * 32) {\n-      max_buffer[idx] = -1e8;\n-    }\n-  }\n-  __syncthreads();\n-\n-  int *indices_pt = &indices[batch_idx * num_block];\n-  float *index_vals_pt = &index_vals[batch_idx * num_block * 32];\n-\n-  for (int idx_start = 0; idx_start < 32 * num_block; idx_start = idx_start + num_thread) {\n-    int idx = idx_start + thread_idx;\n-    int A_block_idx = indices_pt[idx % num_block] / B_num_block;\n-    atomicMax(&max_buffer[A_block_idx * 32 + idx / num_block], (int)(index_vals_pt[idx] * 1000));\n-  }\n-  __syncthreads();\n-  \n-  float *max_vals_pt = &max_vals[batch_idx * A_num_block * 32];\n-  for (int i = 0; i < A_num_block * 32; i = i + num_thread) {\n-    int idx = i + thread_idx;\n-    if (idx < A_num_block * 32) {\n-      max_vals_pt[idx] = (float)max_buffer[idx] / 1000.;\n-    }\n-  }\n-  \n-  float *max_vals_scatter_pt = &max_vals_scatter[batch_idx * num_block * 32];\n-  for (int idx_start = 0; idx_start < 32 * num_block; idx_start = idx_start + num_thread) {\n-    int idx = idx_start + thread_idx;\n-    int A_block_idx = indices_pt[idx % num_block] / B_num_block;\n-    max_vals_scatter_pt[idx] = (float)max_buffer[A_block_idx * 32 + idx / num_block] / 1000.;\n-  }\n-\n-}\n-\n-__global__ void mm_to_sparse_cuda_kernel(\n-  float *dense_A,   // [batch_size, A_num_block, dim, 32]\n-  float *dense_B,   // [batch_size, B_num_block, dim, 32]\n-  int   *indices,   // [batch_size, num_block]\n-  float *sparse_C,  // [batch_size, num_block, 32, 32]\n-  long batch_size,\n-  long A_num_block,\n-  long B_num_block,\n-  long dim,\n-  long num_block\n-) {\n-\n-  long batch_idx = blockIdx.y;\n-  long block_idx = blockIdx.x * blockDim.y + threadIdx.y;\n-\n-  long thread_idx = threadIdx.x;\n-\n-  __shared__ float buffer[4096];\n-  float *A_buffer = &buffer[threadIdx.y * 1024]; // [2, 8, 32]\n-  float *B_buffer = &buffer[threadIdx.y * 1024 + 512]; // [2, 8, 32]\n-\n-  long batch_idx__block_idx = batch_idx * num_block + block_idx;\n-\n-  long AB_block_idx = indices[batch_idx__block_idx];\n-  float *dense_A_pt = &dense_A[(batch_idx * A_num_block + AB_block_idx / B_num_block) * dim * 32];\n-  float *dense_B_pt = &dense_B[(batch_idx * B_num_block + AB_block_idx % B_num_block) * dim * 32];\n-\n-  int reg_1_idx = thread_idx / 8;    // [0000000011111111222222223333333344444444555555556666666677777777]\n-  int reg_2_idx = thread_idx % 8;    // [0123456701234567012345670123456701234567012345670123456701234567]\n-\n-  float reg_1[8];\n-  float reg_2[8];\n-\n-  float reg_array[16] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};\n-\n-  #pragma unroll\n-  for (int i = 0; i < 4; i++) {\n-    A_buffer[i * 64 + thread_idx] = dense_A_pt[i * 64 + thread_idx];\n-    B_buffer[i * 64 + thread_idx] = dense_B_pt[i * 64 + thread_idx];\n-  }\n-\n-  __syncthreads();\n-\n-  #pragma unroll\n-  for (int i = 0; i < 4; i++) {\n-    reg_1[i] = A_buffer[reg_1_idx * 4 + i];\n-    reg_2[i] = B_buffer[reg_2_idx * 4 + i];\n-  }\n-\n-  for (int dim_stride = 1; dim_stride < (dim / 8); dim_stride++) {\n-\n-    #pragma unroll\n-    for (int i = 0; i < 4; i++) {\n-      A_buffer[(dim_stride % 2) * 256 + i * 64 + thread_idx] = dense_A_pt[dim_stride * 256 + i * 64 + thread_idx];\n-      B_buffer[(dim_stride % 2) * 256 + i * 64 + thread_idx] = dense_B_pt[dim_stride * 256 + i * 64 + thread_idx];\n-    }\n-\n-    #pragma unroll\n-    for (int mini_dim_idx = 1; mini_dim_idx < 8; mini_dim_idx++) {\n-      #pragma unroll\n-      for (int i = 0; i < 4; i++) {\n-        reg_1[(mini_dim_idx % 2) * 4 + i] = A_buffer[((dim_stride - 1) % 2) * 256 + mini_dim_idx * 32 + reg_1_idx * 4 + i];\n-        reg_2[(mini_dim_idx % 2) * 4 + i] = B_buffer[((dim_stride - 1) % 2) * 256 + mini_dim_idx * 32 + reg_2_idx * 4 + i];\n-      }\n-      #pragma unroll\n-      for (int i = 0; i < 4; i++) {\n-        #pragma unroll\n-        for (int j = 0; j < 4; j++) {\n-          reg_array[i * 4 + j] += reg_1[((mini_dim_idx - 1) % 2) * 4 + i] * reg_2[((mini_dim_idx - 1) % 2) * 4 + j];\n-        }\n-      }\n-    }\n-\n-    __syncthreads();\n-\n-    #pragma unroll\n-    for (int i = 0; i < 4; i++) {\n-      reg_1[i] = A_buffer[(dim_stride % 2) * 256 + reg_1_idx * 4 + i];\n-      reg_2[i] = B_buffer[(dim_stride % 2) * 256 + reg_2_idx * 4 + i];\n-    }\n-\n-    #pragma unroll\n-    for (int i = 0; i < 4; i++) {\n-      #pragma unroll\n-      for (int j = 0; j < 4; j++) {\n-        reg_array[i * 4 + j] += reg_1[4 + i] * reg_2[4 + j];\n-      }\n-    }\n-\n-  }\n-\n-  #pragma unroll\n-  for (int mini_dim_idx = 1; mini_dim_idx < 8; mini_dim_idx++) {\n-    #pragma unroll\n-    for (int i = 0; i < 4; i++) {\n-      reg_1[(mini_dim_idx % 2) * 4 + i] = A_buffer[256 + mini_dim_idx * 32 + reg_1_idx * 4 + i];\n-      reg_2[(mini_dim_idx % 2) * 4 + i] = B_buffer[256 + mini_dim_idx * 32 + reg_2_idx * 4 + i];\n-    }\n-    #pragma unroll\n-    for (int i = 0; i < 4; i++) {\n-      #pragma unroll\n-      for (int j = 0; j < 4; j++) {\n-        reg_array[i * 4 + j] += reg_1[((mini_dim_idx - 1) % 2) * 4 + i] * reg_2[((mini_dim_idx - 1) % 2) * 4 + j];\n-      }\n-    }\n-  }\n-  #pragma unroll\n-  for (int i = 0; i < 4; i++) {\n-    #pragma unroll\n-    for (int j = 0; j < 4; j++) {\n-      reg_array[i * 4 + j] += reg_1[4 + i] * reg_2[4 + j];\n-    }\n-  }\n-  __syncthreads();\n-\n-  float *C_buffer = &buffer[threadIdx.y * 1024]; // [32, 32]\n-\n-  #pragma unroll\n-  for (int i = 0; i < 4; i++) {\n-    #pragma unroll\n-    for (int j = 0; j < 4; j++) {\n-      C_buffer[(reg_2_idx * 4 + j) * 32 + reg_1_idx * 4 + i] = reg_array[i * 4 + j];\n-    }\n-  }\n-  __syncthreads();\n-\n-  float *sparse_C_pt = &sparse_C[batch_idx__block_idx * 1024];\n-\n-  #pragma unroll\n-  for (int i = 0; i < 16; i++) {\n-    sparse_C_pt[i * 64 + thread_idx] = C_buffer[i * 64 + thread_idx];\n-  }\n-\n-}\n-\n-__global__ void sparse_dense_mm_cuda_kernel(\n-  float *sparse_A,  // [batch_size, num_block, 32, 32]\n-  int   *indices,   // [batch_size, num_block]\n-  float *dense_B,   // [batch_size, B_num_block, dim, 32]\n-  float *dense_C,   // [batch_size, A_num_block, dim, 32]\n-  long batch_size,\n-  long A_num_block,\n-  long B_num_block,\n-  long dim,\n-  long num_block\n-) {\n-\n-  long batch_idx = blockIdx.y;\n-  long block_idx = blockIdx.x * blockDim.y + threadIdx.y;\n-\n-  long thread_idx = threadIdx.x;\n-\n-  __shared__ float buffer[6144];\n-  float *A_buffer = &buffer[threadIdx.y * 3072]; // [32, 32]\n-  float *B_buffer = &buffer[threadIdx.y * 3072 + 1024]; // [32, 64]\n-\n-  long batch_idx__block_idx = batch_idx * num_block + block_idx;\n-\n-  float *sparse_A_pt = &sparse_A[batch_idx__block_idx * 1024];\n-  #pragma unroll\n-  for (int i = 0; i < 8; i++) {\n-    A_buffer[i * 128 + thread_idx] = sparse_A_pt[i * 128 + thread_idx];\n-  }\n-\n-  long AB_block_idx = indices[batch_idx__block_idx];\n-  float *dense_B_pt = &dense_B[(batch_idx * B_num_block + AB_block_idx % B_num_block) * 32 * dim];\n-  float *dense_C_pt = &dense_C[(batch_idx * A_num_block + AB_block_idx / B_num_block) * 32 * dim];\n-\n-  // [0000000011111111222222223333333344444444555555556666666677777777]\n-  // [0123456701234567012345670123456701234567012345670123456701234567]\n-  int reg_1_idx = thread_idx / 8;\n-  int reg_2_idx = thread_idx % 8;\n-\n-  float reg_1[8];\n-  float reg_2[8];\n-\n-  float reg_array[16];\n-\n-  for (int dim_stride = 0; dim_stride < dim; dim_stride = dim_stride + 64) {\n-\n-    #pragma unroll\n-    for (int i = 0; i < 16; i++) {\n-      B_buffer[i * 128 + thread_idx] = dense_B_pt[dim_stride * 32 + i * 128 + thread_idx];\n-    }\n-\n-    #pragma unroll\n-    for (int i = 0; i < 16; i++) {\n-      reg_array[i] = 0;\n-    }\n-\n-    __syncthreads();\n-\n-    #pragma unroll\n-    for (int i = 0; i < 4; i++) {\n-      reg_1[i] = B_buffer[(reg_1_idx * 4 + i) * 32];\n-      reg_2[i] = A_buffer[reg_2_idx * 4 + i];\n-    }\n-\n-    #pragma unroll\n-    for (int mini_dim_idx = 1; mini_dim_idx < 32; mini_dim_idx++) {\n-      #pragma unroll\n-      for (int i = 0; i < 4; i++) {\n-        reg_1[(mini_dim_idx % 2) * 4 + i] = B_buffer[(reg_1_idx * 4 + i) * 32 + mini_dim_idx];\n-        reg_2[(mini_dim_idx % 2) * 4 + i] = A_buffer[mini_dim_idx * 32 + reg_2_idx * 4 + i];\n-      }\n-      #pragma unroll\n-      for (int i = 0; i < 4; i++) {\n-        #pragma unroll\n-        for (int j = 0; j < 4; j++) {\n-          reg_array[i * 4 + j] += reg_1[((mini_dim_idx - 1) % 2) * 4 + i] * reg_2[((mini_dim_idx - 1) % 2) * 4 + j];\n-        }\n-      }\n-    }\n-\n-    #pragma unroll\n-    for (int i = 0; i < 4; i++) {\n-      #pragma unroll\n-      for (int j = 0; j < 4; j++) {\n-        reg_array[i * 4 + j] += reg_1[4 + i] * reg_2[4 + j];\n-      }\n-    }\n-\n-    __syncthreads();\n-\n-    float *C_buffer = &buffer[threadIdx.y * 3072 + 1024]; // [64, 32]\n-\n-    #pragma unroll\n-    for (int i = 0; i < 4; i++) {\n-      #pragma unroll\n-      for (int j = 0; j < 4; j++) {\n-        C_buffer[(reg_1_idx * 4 + i) * 32 + reg_2_idx * 4 + j] = reg_array[i * 4 + j];\n-      }\n-    }\n-    __syncthreads();\n-\n-    #pragma unroll\n-    for (int i = 0; i < 16; i++) {\n-      atomicAdd(&dense_C_pt[dim_stride * 32 + i * 128 + thread_idx], C_buffer[i * 128 + thread_idx]);\n-    }\n-    __syncthreads();\n-\n-  }\n-\n-}\n-\n-\n-__global__ void reduce_sum_cuda_kernel(\n-  float *sparse_A,  // [batch_size, num_block, 32, 32]\n-  int   *indices,   // [batch_size, num_block]\n-  float *dense_C,   // [batch_size, A_num_block, 32]\n-  long batch_size,\n-  long A_num_block,\n-  long B_num_block,\n-  long num_block\n-) {\n-\n-  long batch_idx = blockIdx.y;\n-  long block_idx = blockIdx.x * blockDim.y + threadIdx.y;\n-\n-  long thread_idx = threadIdx.x;\n-\n-  long batch_idx__block_idx = batch_idx * num_block + block_idx;\n-\n-  long AB_block_idx = indices[batch_idx__block_idx];\n-  float *sparse_A_pt = &sparse_A[batch_idx__block_idx * 1024];\n-\n-  float reg_array[16];\n-  float value = 0;\n-\n-  #pragma unroll\n-  for (int i = 0; i < 8; i++) {\n-    reg_array[i] = sparse_A_pt[i * 32 + thread_idx];\n-  }\n-  #pragma unroll\n-  for (int stride = 8; stride < 32; stride = stride + 8) {\n-    #pragma unroll\n-    for (int i = 0; i < 8; i++) {\n-      reg_array[(stride + i) % 16] = sparse_A_pt[(stride + i) * 32 + thread_idx];\n-    }\n-    #pragma unroll\n-    for (int i = 0; i < 8; i++) {\n-      value = value + reg_array[(stride - 8 + i) % 16];\n-    }\n-  }\n-  #pragma unroll\n-  for (int i = 0; i < 8; i++) {\n-    value = value + reg_array[8 + i];\n-  }\n-\n-  float *dense_C_pt = &dense_C[(batch_idx * A_num_block + AB_block_idx / B_num_block) * 32];\n-\n-  atomicAdd(&dense_C_pt[thread_idx], value);\n-\n-}\n-\n-__global__ void scatter_cuda_kernel(\n-  float *dense_A,   // [batch_size, A_num_block, 32]\n-  int   *indices,   // [batch_size, num_block]\n-  float *sparse_C,  // [batch_size, num_block, 32, 32]\n-  long batch_size,\n-  long A_num_block,\n-  long B_num_block,\n-  long num_block\n-) {\n-\n-  long batch_idx = blockIdx.y;\n-  long block_idx = blockIdx.x * blockDim.y + threadIdx.y;\n-\n-  long thread_idx = threadIdx.x;\n-\n-  long batch_idx__block_idx = batch_idx * num_block + block_idx;\n-\n-  long AB_block_idx = indices[batch_idx__block_idx];\n-  float *dense_A_pt = &dense_A[(batch_idx * A_num_block + AB_block_idx / B_num_block) * 32];\n-  float *sparse_C_pt = &sparse_C[(batch_idx * num_block + block_idx) * 1024];\n-\n-  float value = dense_A_pt[thread_idx];\n-\n-  #pragma unroll\n-  for (int i = 0; i < 32; i++) {\n-    sparse_C_pt[i * 32 + thread_idx] = value;\n-  }\n-\n-}"
      },
      {
        "filename": "src/transformers/kernels/mra/cuda_kernel.h",
        "status": "removed",
        "additions": 0,
        "deletions": 59,
        "changes": 59,
        "patch": "@@ -1,59 +0,0 @@\n-\n-#define WARP_SIZE 32\n-#define FULL_MASK 0xffffffff\n-#define OPTIMAL_THREADS 256\n-\n-__global__ void index_max_cuda_kernel(\n-  float *index_vals,       // [batch_size, 32, num_block]\n-  int   *indices,        // [batch_size, num_block]\n-  float *max_vals,        // [batch_size, A_num_block * 32]\n-  float *max_vals_scatter,   // [batch_size, 32, num_block]\n-  long batch_size,\n-  long A_num_block,\n-  long B_num_block,\n-  long num_block\n-);\n-\n-__global__ void mm_to_sparse_cuda_kernel(\n-  float *dense_A,   // [batch_size, A_num_block, dim, 32]\n-  float *dense_B,   // [batch_size, B_num_block, dim, 32]\n-  int   *indices,   // [batch_size, num_block]\n-  float *sparse_C,  // [batch_size, num_block, 32, 32]\n-  long batch_size,\n-  long A_num_block,\n-  long B_num_block,\n-  long dim,\n-  long num_block\n-);\n-\n-__global__ void sparse_dense_mm_cuda_kernel(\n-  float *sparse_A,  // [batch_size, num_block, 32, 32]\n-  int   *indices,   // [batch_size, num_block]\n-  float *dense_B,   // [batch_size, B_num_block, dim, 32]\n-  float *dense_C,   // [batch_size, A_num_block, dim, 32]\n-  long batch_size,\n-  long A_num_block,\n-  long B_num_block,\n-  long dim,\n-  long num_block\n-);\n-\n-__global__ void reduce_sum_cuda_kernel(\n-  float *sparse_A,  // [batch_size, num_block, 32, 32]\n-  int   *indices,   // [batch_size, num_block]\n-  float *dense_C,   // [batch_size, A_num_block, 32]\n-  long batch_size,\n-  long A_num_block,\n-  long B_num_block,\n-  long num_block\n-);\n-\n-__global__ void scatter_cuda_kernel(\n-  float *dense_A,   // [batch_size, A_num_block, 32]\n-  int   *indices,   // [batch_size, num_block]\n-  float *sparse_C,  // [batch_size, num_block, 32, 32]\n-  long batch_size,\n-  long A_num_block,\n-  long B_num_block,\n-  long num_block\n-);"
      },
      {
        "filename": "src/transformers/kernels/mra/cuda_launch.cu",
        "status": "removed",
        "additions": 0,
        "deletions": 154,
        "changes": 154,
        "patch": "@@ -1,154 +0,0 @@\n-#include <torch/extension.h>\n-#include <ATen/ATen.h>\n-#include \"cuda_launch.h\"\n-#include \"cuda_kernel.h\"\n-#include <vector>\n-\n-//////////////////////////////////////////////////////////////////////////////////////////////////\n-//////////////////////////////////////////////////////////////////////////////////////////////////\n-\n-std::vector<at::Tensor> index_max_kernel(\n-  at::Tensor index_vals,  // [batch_size, 32, num_block]\n-  at::Tensor indices,     // [batch_size, num_block],\n-  int A_num_block,\n-  int B_num_block\n-) {\n-  int batch_size = indices.size(0);\n-  int num_block = indices.size(1);\n-\n-  at::Tensor max_vals = at::zeros({batch_size, A_num_block * 32}, index_vals.options());\n-  at::Tensor max_vals_scatter = at::zeros({batch_size, 32, num_block}, index_vals.options());\n-\n-  dim3 threads(256);\n-  dim3 blocks(batch_size);\n-  int shared_mem = A_num_block * 32 * sizeof(float);\n-\n-  index_max_cuda_kernel<<<blocks, threads, shared_mem>>>(\n-    index_vals.data_ptr<float>(),\n-    indices.data_ptr<int>(),\n-    max_vals.data_ptr<float>(),\n-    max_vals_scatter.data_ptr<float>(),\n-    batch_size,\n-    A_num_block,\n-    B_num_block,\n-    num_block\n-  );\n-\n-  return {max_vals, max_vals_scatter};\n-}\n-\n-at::Tensor mm_to_sparse_kernel(\n-  at::Tensor dense_A,  // [batch_size, A_num_block, dim, 32]\n-  at::Tensor dense_B,  // [batch_size, B_num_block, dim, 32]\n-  at::Tensor indices   // [batch_size, num_block]\n-) {\n-  int batch_size = dense_A.size(0);\n-  int A_num_block = dense_A.size(1);\n-  int B_num_block = dense_B.size(1);\n-  int dim = dense_A.size(2);\n-  int num_block = indices.size(1);\n-\n-  at::Tensor sparse_C = at::zeros({batch_size, num_block, 32, 32}, dense_A.options());\n-\n-  dim3 threads(64, 4);\n-  dim3 blocks(num_block / 4, batch_size);\n-\n-  mm_to_sparse_cuda_kernel<<<blocks, threads>>>(\n-    dense_A.data_ptr<float>(),\n-    dense_B.data_ptr<float>(),\n-    indices.data_ptr<int>(),\n-    sparse_C.data_ptr<float>(),\n-    batch_size,\n-    A_num_block,\n-    B_num_block,\n-    dim,\n-    num_block\n-  );\n-\n-  return sparse_C;\n-}\n-\n-at::Tensor sparse_dense_mm_kernel(\n-  at::Tensor sparse_A,  // [batch_size, num_block, 32, 32]\n-  at::Tensor indices,   // [batch_size, num_block]\n-  at::Tensor dense_B,   // [batch_size, B_num_block, dim, 32]\n-  int A_num_block\n-) {\n-  int batch_size = sparse_A.size(0);\n-  int num_block = sparse_A.size(1);\n-  int B_num_block = dense_B.size(1);\n-  int dim = dense_B.size(2);\n-\n-  at::Tensor dense_C = at::zeros({batch_size, A_num_block, dim, 32}, dense_B.options());\n-\n-  dim3 threads(128, 2);\n-  dim3 blocks(num_block / 2, batch_size);\n-\n-  sparse_dense_mm_cuda_kernel<<<blocks, threads>>>(\n-    sparse_A.data_ptr<float>(),\n-    indices.data_ptr<int>(),\n-    dense_B.data_ptr<float>(),\n-    dense_C.data_ptr<float>(),\n-    batch_size,\n-    A_num_block,\n-    B_num_block,\n-    dim,\n-    num_block\n-  );\n-\n-  return dense_C;\n-}\n-\n-at::Tensor reduce_sum_kernel(\n-  at::Tensor sparse_A,  // [batch_size, num_block, 32, 32]\n-  at::Tensor indices,   // [batch_size, num_block]\n-  int A_num_block,\n-  int B_num_block\n-) {\n-  int batch_size = sparse_A.size(0);\n-  int num_block = sparse_A.size(1);\n-\n-  at::Tensor dense_C = at::zeros({batch_size, A_num_block, 32}, sparse_A.options());\n-\n-  dim3 threads(32, 4);\n-  dim3 blocks(num_block / 4, batch_size);\n-\n-  reduce_sum_cuda_kernel<<<blocks, threads>>>(\n-    sparse_A.data_ptr<float>(),\n-    indices.data_ptr<int>(),\n-    dense_C.data_ptr<float>(),\n-    batch_size,\n-    A_num_block,\n-    B_num_block,\n-    num_block\n-  );\n-\n-  return dense_C;\n-}\n-\n-at::Tensor scatter_kernel(\n-  at::Tensor dense_A,   // [batch_size, A_num_block, 32]\n-  at::Tensor indices,   // [batch_size, num_block]\n-  int B_num_block\n-) {\n-  int batch_size = dense_A.size(0);\n-  int A_num_block = dense_A.size(1);\n-  int num_block = indices.size(1);\n-\n-  at::Tensor sparse_C = at::zeros({batch_size, num_block, 32, 32}, dense_A.options());\n-\n-  dim3 threads(32, 4);\n-  dim3 blocks(num_block / 4, batch_size);\n-\n-  scatter_cuda_kernel<<<blocks, threads>>>(\n-    dense_A.data_ptr<float>(),\n-    indices.data_ptr<int>(),\n-    sparse_C.data_ptr<float>(),\n-    batch_size,\n-    A_num_block,\n-    B_num_block,\n-    num_block\n-  );\n-\n-  return sparse_C;\n-}"
      },
      {
        "filename": "src/transformers/kernels/mra/cuda_launch.h",
        "status": "removed",
        "additions": 0,
        "deletions": 39,
        "changes": 39,
        "patch": "@@ -1,39 +0,0 @@\n-#include <torch/extension.h>\n-#include <ATen/ATen.h>\n-#include <vector>\n-\n-#define min(a, b) ((a)<(b)?(a):(b))\n-#define max(a, b) ((a)>(b)?(a):(b))\n-\n-std::vector<at::Tensor> index_max_kernel(\n-  at::Tensor index_vals,\n-  at::Tensor indices,\n-  int A_num_block,\n-  int B_num_block\n-);\n-\n-at::Tensor mm_to_sparse_kernel(\n-  at::Tensor dense_A,\n-  at::Tensor dense_B,\n-  at::Tensor indices\n-);\n-\n-at::Tensor sparse_dense_mm_kernel(\n-  at::Tensor sparse_A,\n-  at::Tensor indices,\n-  at::Tensor dense_B,\n-  int A_num_block\n-);\n-\n-at::Tensor reduce_sum_kernel(\n-  at::Tensor sparse_A,\n-  at::Tensor indices,\n-  int A_num_block,\n-  int B_num_block\n-);\n-\n-at::Tensor scatter_kernel(\n-  at::Tensor dense_A,\n-  at::Tensor indices,\n-  int B_num_block\n-);"
      },
      {
        "filename": "src/transformers/kernels/mra/torch_extension.cpp",
        "status": "removed",
        "additions": 0,
        "deletions": 78,
        "changes": 78,
        "patch": "@@ -1,78 +0,0 @@\n-#include <torch/extension.h>\n-#include <ATen/ATen.h>\n-#include \"cuda_launch.h\"\n-#include <vector>\n-\n-std::vector<at::Tensor> index_max(\n-  at::Tensor index_vals,\n-  at::Tensor indices,\n-  int A_num_block,\n-  int B_num_block\n-) {\n-  return index_max_kernel(\n-    index_vals,\n-    indices,\n-    A_num_block,\n-    B_num_block\n-  );\n-}\n-\n-at::Tensor mm_to_sparse(\n-  at::Tensor dense_A,\n-  at::Tensor dense_B,\n-  at::Tensor indices\n-) {\n-  return mm_to_sparse_kernel(\n-    dense_A,\n-    dense_B,\n-    indices\n-  );\n-}\n-\n-at::Tensor sparse_dense_mm(\n-  at::Tensor sparse_A,\n-  at::Tensor indices,\n-  at::Tensor dense_B,\n-  int A_num_block\n-) {\n-  return sparse_dense_mm_kernel(\n-    sparse_A,\n-    indices,\n-    dense_B,\n-    A_num_block\n-  );\n-}\n-\n-at::Tensor reduce_sum(\n-  at::Tensor sparse_A,\n-  at::Tensor indices,\n-  int A_num_block,\n-  int B_num_block\n-) {\n-  return reduce_sum_kernel(\n-    sparse_A,\n-    indices,\n-    A_num_block,\n-    B_num_block\n-  );\n-}\n-\n-at::Tensor scatter(\n-  at::Tensor dense_A,\n-  at::Tensor indices,\n-  int B_num_block\n-) {\n-  return scatter_kernel(\n-    dense_A,\n-    indices,\n-    B_num_block\n-  );\n-}\n-\n-PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n-  m.def(\"index_max\", &index_max, \"index_max (CUDA)\");\n-  m.def(\"mm_to_sparse\", &mm_to_sparse, \"mm_to_sparse (CUDA)\");\n-  m.def(\"sparse_dense_mm\", &sparse_dense_mm, \"sparse_dense_mm (CUDA)\");\n-  m.def(\"reduce_sum\", &reduce_sum, \"reduce_sum (CUDA)\");\n-  m.def(\"scatter\", &scatter, \"scatter (CUDA)\");\n-}"
      },
      {
        "filename": "src/transformers/models/mra/modeling_mra.py",
        "status": "modified",
        "additions": 12,
        "deletions": 10,
        "changes": 22,
        "patch": "@@ -15,13 +15,11 @@\n \"\"\"PyTorch MRA model.\"\"\"\n \n import math\n-from pathlib import Path\n from typing import Optional, Union\n \n import torch\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n-from torch.utils.cpp_extension import load\n \n from ...activations import ACT2FN\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -35,7 +33,14 @@\n )\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward\n-from ...utils import auto_docstring, is_cuda_platform, is_ninja_available, is_torch_cuda_available, logging\n+from ...utils import (\n+    auto_docstring,\n+    is_cuda_platform,\n+    is_kernels_available,\n+    is_ninja_available,\n+    is_torch_cuda_available,\n+    logging,\n+)\n from .configuration_mra import MraConfig\n \n \n@@ -46,14 +51,11 @@\n \n def load_cuda_kernels():\n     global mra_cuda_kernel\n-    src_folder = Path(__file__).resolve().parent.parent.parent / \"kernels\" / \"mra\"\n-\n-    def append_root(files):\n-        return [src_folder / file for file in files]\n-\n-    src_files = append_root([\"cuda_kernel.cu\", \"cuda_launch.cu\", \"torch_extension.cpp\"])\n+    if not is_kernels_available():\n+        raise ImportError(\"kernels is not installed, please install it with `pip install kernels`\")\n+    from kernels import get_kernel\n \n-    mra_cuda_kernel = load(\"cuda_kernel\", src_files, verbose=True)\n+    mra_cuda_kernel = get_kernel(\"kernels-community/mra\")\n \n \n def sparse_max(sparse_qk_prod, indices, query_num_block, key_num_block):"
      }
    ],
    "num_files": 6,
    "scraped_at": "2025-11-16T21:17:47.833456",
    "filter_decision": {
      "accept": false,
      "reasoning": "This PR is primarily a code deletion/removal with minimal logic changes. While it removes ~654 lines of CUDA kernel code, the actual substantive change is just updating the import mechanism in one file to use an external package instead of local kernels. This is a simple find-and-replace style refactoring (changing from locally-compiled kernels to a hub-based import), which doesn't provide enough technical depth for meaningful codebase questions.",
      "substance_level": "low"
    }
  },
  {
    "pr_number": 41504,
    "title": "Revert `local_rank` deletion and some cleaning",
    "body": "# What does this PR do?\r\n\r\nThis PR removes some bits that I forgot when removing `logging_dir`. Also, we need to keep `local_rank` as torch.distributed.launch inject `local_rank` in the script. I will deprecate at once torch removes it from their codebase + we don't support this version of pytorch which is in a super long time ",
    "html_url": "https://github.com/huggingface/transformers/pull/41504",
    "created_at": "2025-10-10T09:05:21Z",
    "merged_at": "2025-10-10T10:23:04Z",
    "merge_commit_sha": "f9f8bf5a1062ce0293cbd42be0126e17a15446e9",
    "base_ref": "main",
    "head_sha": "e0bcb483ef84c7b8301e1208df9c32b01f23bb59",
    "user": "SunMarc",
    "files": [
      {
        "filename": "examples/legacy/seq2seq/seq2seq_trainer.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -144,7 +144,7 @@ def _get_train_sampler(self) -> Optional[torch.utils.data.Sampler]:\n \n             return (\n                 RandomSampler(self.train_dataset)\n-                if self.args.local_rank == -1\n+                if self.args.local_process_index == -1\n                 else DistributedSampler(self.train_dataset)\n             )\n "
      },
      {
        "filename": "src/transformers/training_args.py",
        "status": "modified",
        "additions": 6,
        "deletions": 0,
        "changes": 6,
        "patch": "@@ -996,6 +996,12 @@ class TrainingArguments:\n             )\n         },\n     )\n+    local_rank: int = field(\n+        default=-1,\n+        metadata={\n+            \"help\": \"When using torch.distributed.launch (Deprecated), it will pass `local_rank` in the script, so we need this for the parser. To get the local rank, prefer using the property `local_process_index`\"\n+        },\n+    )\n     ddp_backend: Optional[str] = field(\n         default=None,\n         metadata={"
      },
      {
        "filename": "tests/deepspeed/test_deepspeed.py",
        "status": "modified",
        "additions": 5,
        "deletions": 10,
        "changes": 15,
        "patch": "@@ -518,7 +518,6 @@ def test_hf_ds_config_mismatch(self):\n \n         with mockenv_context(**self.dist_env_1_gpu):\n             trainer = get_regression_trainer(\n-                local_rank=0,\n                 fp16=fp16,\n                 deepspeed=ds_config,\n                 per_device_train_batch_size=per_device_train_batch_size,\n@@ -552,7 +551,7 @@ def test_hf_scheduler_hf_optimizer(self):\n             ds_config_zero2_dict[\"zero_optimization\"][\"offload_optimizer\"][\"device\"] = \"none\"\n             ds_config_zero2_dict[\"fp16\"][\"initial_scale_power\"] = 1  # force optimizer on the first step\n             trainer = get_regression_trainer(\n-                a=a, local_rank=0, fp16=True, deepspeed=ds_config_zero2_dict, output_dir=self.get_auto_remove_tmp_dir()\n+                a=a, fp16=True, deepspeed=ds_config_zero2_dict, output_dir=self.get_auto_remove_tmp_dir()\n             )\n             trainer.train()\n         new_a = trainer.model.a.item()\n@@ -566,7 +565,7 @@ def test_ds_scheduler_hf_optimizer(self):\n             ds_config_zero2_dict[\"zero_optimization\"][\"offload_optimizer\"][\"device\"] = \"none\"\n             ds_config_zero2_dict[\"fp16\"][\"initial_scale_power\"] = 1  # force optimizer on the first step\n             trainer = get_regression_trainer(\n-                a=a, local_rank=0, fp16=True, deepspeed=ds_config_zero2_dict, output_dir=self.get_auto_remove_tmp_dir()\n+                a=a, fp16=True, deepspeed=ds_config_zero2_dict, output_dir=self.get_auto_remove_tmp_dir()\n             )\n             trainer.train()\n         new_a = trainer.model.a.item()\n@@ -580,7 +579,7 @@ def test_hf_scheduler_ds_optimizer(self):\n             ds_config_zero2_dict[\"zero_optimization\"][\"offload_optimizer\"][\"device\"] = \"none\"\n             ds_config_zero2_dict[\"fp16\"][\"initial_scale_power\"] = 1  # force optimizer on the first step\n             trainer = get_regression_trainer(\n-                a=a, local_rank=0, fp16=True, deepspeed=ds_config_zero2_dict, output_dir=self.get_auto_remove_tmp_dir()\n+                a=a, fp16=True, deepspeed=ds_config_zero2_dict, output_dir=self.get_auto_remove_tmp_dir()\n             )\n             trainer.train()\n         new_a = trainer.model.a.item()\n@@ -598,7 +597,7 @@ def test_stage3_nvme_offload(self):\n             ds_config_zero3_dict[\"zero_optimization\"][\"offload_param\"] = nvme_config\n             ds_config_zero3_dict[\"zero_optimization\"][\"stage3_gather_16bit_weights_on_model_save\"] = True\n             trainer = get_regression_trainer(\n-                local_rank=0, fp16=True, deepspeed=ds_config_zero3_dict, output_dir=self.get_auto_remove_tmp_dir()\n+                fp16=True, deepspeed=ds_config_zero3_dict, output_dir=self.get_auto_remove_tmp_dir()\n             )\n             with CaptureLogger(deepspeed_logger) as cl:\n                 trainer.train()\n@@ -616,7 +615,6 @@ def model_init():\n                 return model\n \n             trainer = get_regression_trainer(\n-                local_rank=0,\n                 fp16=True,\n                 model_init=model_init,\n                 deepspeed=ds_config_zero3_dict,\n@@ -642,7 +640,7 @@ def test_hf_optimizer_with_offload(self, stage, dtype):\n         ds_config_dict[\"zero_optimization\"][\"offload_optimizer\"][\"device\"] = \"cpu\"\n         ds_config_dict[\"zero_force_ds_cpu_optimizer\"] = False  # offload is not efficient w/o CPUAdam\n         with mockenv_context(**self.dist_env_1_gpu):\n-            kwargs = {\"local_rank\": 0, \"deepspeed\": ds_config_dict, \"output_dir\": self.get_auto_remove_tmp_dir()}\n+            kwargs = {\"deepspeed\": ds_config_dict, \"output_dir\": self.get_auto_remove_tmp_dir()}\n             kwargs[dtype] = True\n             trainer = get_regression_trainer(**kwargs)\n             with CaptureLogger(deepspeed_logger) as cl:\n@@ -659,7 +657,6 @@ def test_fake_notebook_no_launcher(self, stage, dtype):\n         # to reset `deepspeed_logger.handlers[0].setStream(sys.stdout)` or directly capture from the deepspeed_logger.\n         with mockenv_context(**self.dist_env_1_gpu):\n             kwargs = {\n-                \"local_rank\": 0,\n                 \"deepspeed\": self.get_config_dict(stage),\n                 \"output_dir\": self.get_auto_remove_tmp_dir(),\n             }\n@@ -683,7 +680,6 @@ def test_early_get_last_lr(self, stage, dtype):\n             kwargs = {\n                 \"a\": a,\n                 \"b\": b,\n-                \"local_rank\": 0,\n                 \"train_len\": 8,\n                 \"deepspeed\": self.get_config_dict(stage),\n                 \"per_device_train_batch_size\": 8,\n@@ -729,7 +725,6 @@ def test_gradient_accumulation(self, stage, dtype):\n         kwargs = {\n             \"a\": a,\n             \"b\": b,\n-            \"local_rank\": 0,\n             \"train_len\": train_len,\n             \"deepspeed\": self.get_config_dict(stage),\n             \"output_dir\": self.get_auto_remove_tmp_dir(),"
      },
      {
        "filename": "tests/trainer/test_trainer.py",
        "status": "modified",
        "additions": 1,
        "deletions": 7,
        "changes": 8,
        "patch": "@@ -1437,9 +1437,7 @@ def test_training_arguments_are_left_untouched(self):\n         args = TrainingArguments(tmp_dir, report_to=[])\n         dict1, dict2 = args.to_dict(), trainer.args.to_dict()\n         for key in dict1:\n-            # Logging dir can be slightly different as they default to something with the time.\n-            if key != \"logging_dir\":\n-                self.assertEqual(dict1[key], dict2[key])\n+            self.assertEqual(dict1[key], dict2[key])\n \n     def test_number_of_steps_in_training(self):\n         # Regular training has n_epochs * len(train_dl) steps\n@@ -5433,7 +5431,6 @@ def hp_name(trial):\n                 num_train_epochs=4,\n                 disable_tqdm=True,\n                 load_best_model_at_end=True,\n-                logging_dir=\"runs\",\n                 run_name=\"test\",\n                 model_init=model_init,\n             )\n@@ -5482,7 +5479,6 @@ def compute_objective(metrics: dict[str, float]) -> list[float]:\n                 num_train_epochs=10,\n                 disable_tqdm=True,\n                 load_best_model_at_end=True,\n-                logging_dir=\"runs\",\n                 run_name=\"test\",\n                 model_init=model_init,\n                 compute_metrics=AlmostAccuracy(),\n@@ -5572,7 +5568,6 @@ def hp_name(params):\n                 num_train_epochs=4,\n                 disable_tqdm=True,\n                 load_best_model_at_end=True,\n-                logging_dir=\"runs\",\n                 run_name=\"test\",\n                 model_init=model_init,\n             )\n@@ -6170,7 +6165,6 @@ def model_init(config):\n                 num_train_epochs=4,\n                 disable_tqdm=True,\n                 load_best_model_at_end=True,\n-                logging_dir=\"runs\",\n                 run_name=\"test\",\n                 model_init=model_init,\n             )"
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T21:17:49.871168",
    "filter_decision": {
      "accept": false,
      "reasoning": "This PR is primarily a cleanup/revert operation with minimal substantive logic changes. The main changes are: (1) reverting a deletion of the `local_rank` parameter and adding it back with a deprecation note, (2) removing test parameter arguments (`local_rank=0`), and (3) removing conditional checks for `logging_dir`. These are largely mechanical changes driven by dependency constraints rather than new algorithmic or architectural decisions.",
      "substance_level": "low"
    }
  },
  {
    "pr_number": 41503,
    "title": "Fix some tests",
    "body": "# What does this PR do?\r\n\r\nThis PR removes some last remnants of the old cache format, and fixes related tests. I also updated contrastive search on the hub https://huggingface.co/transformers-community/contrastive-search/discussions/3 to fix the related tests with the new format",
    "html_url": "https://github.com/huggingface/transformers/pull/41503",
    "created_at": "2025-10-10T08:41:49Z",
    "merged_at": "2025-10-10T09:05:09Z",
    "merge_commit_sha": "e8194fe84f6622ea06593a2a371382bda43749c1",
    "base_ref": "main",
    "head_sha": "3bfeaef9901e6909b31d799a16ef5cdb527d0ee8",
    "user": "Cyrilvallez",
    "files": [
      {
        "filename": "docs/source/ar/llm_tutorial_optimization.md",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -472,7 +472,7 @@ for _ in range(5):\n   next_token_id = torch.argmax(next_logits, dim=-1)\n \n   print(\"shape of input_ids\", next_token_id.shape)\n-  print(\"length of key-value cache\", len(past_key_values[0][0]))  # past_key_values are of shape [num_layers, 0 for k, 1 for v, batch_size, length, hidden_dim]\n+  print(\"length of key-value cache\", past_key_values.get_seq_length())  # past_key_values are of shape [num_layers, 0 for k, 1 for v, batch_size, length, hidden_dim]\n   generated_tokens.append(next_token_id.item())\n \n generated_text = tokenizer.batch_decode(generated_tokens)"
      },
      {
        "filename": "docs/source/en/llm_tutorial_optimization.md",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -484,7 +484,7 @@ for _ in range(5):\n   next_token_id = torch.argmax(next_logits, dim=-1)\n \n   print(\"shape of input_ids\", next_token_id.shape)\n-  print(\"length of key-value cache\", len(past_key_values[0][0]))  # past_key_values are of shape [num_layers, 0 for k, 1 for v, batch_size, length, hidden_dim]\n+  print(\"length of key-value cache\", past_key_values.get_seq_length())  # past_key_values are of shape [num_layers, 0 for k, 1 for v, batch_size, length, hidden_dim]\n   generated_tokens.append(next_token_id.item())\n \n generated_text = tokenizer.batch_decode(generated_tokens)"
      },
      {
        "filename": "docs/source/ko/llm_tutorial_optimization.md",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -457,7 +457,7 @@ for _ in range(5):\n   next_token_id = torch.argmax(next_logits, dim=-1)\n \n   print(\"shape of input_ids\", next_token_id.shape)\n-  print(\"length of key-value cache\", len(past_key_values[0][0]))  # past_key_values \ud615\ud0dc: [num_layers, 0 for k, 1 for v, batch_size, length, hidden_dim]\n+  print(\"length of key-value cache\", past_key_values.get_seq_length())  # past_key_values \ud615\ud0dc: [num_layers, 0 for k, 1 for v, batch_size, length, hidden_dim]\n   generated_tokens.append(next_token_id.item())\n \n generated_text = tokenizer.batch_decode(generated_tokens)"
      },
      {
        "filename": "src/transformers/models/big_bird/modeling_big_bird.py",
        "status": "modified",
        "additions": 1,
        "deletions": 7,
        "changes": 8,
        "patch": "@@ -1689,13 +1689,7 @@ def forward(\n         batch_size, seq_length = input_shape\n         device = input_ids.device if input_ids is not None else inputs_embeds.device\n \n-        past_key_values_length = 0\n-        if past_key_values is not None:\n-            past_key_values_length = (\n-                past_key_values[0][0].shape[-2]\n-                if not isinstance(past_key_values, Cache)\n-                else past_key_values.get_seq_length()\n-            )\n+        past_key_values_length = 0 if past_key_values is None else past_key_values.get_seq_length()\n \n         if attention_mask is None:\n             attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)"
      },
      {
        "filename": "src/transformers/models/blip/modeling_blip_text.py",
        "status": "modified",
        "additions": 1,
        "deletions": 7,
        "changes": 8,
        "patch": "@@ -674,13 +674,7 @@ def forward(\n         else:\n             raise ValueError(\"You have to specify either input_ids or inputs_embeds or encoder_embeds\")\n \n-        past_key_values_length = 0\n-        if past_key_values is not None:\n-            past_key_values_length = (\n-                past_key_values[0][0].shape[-2]\n-                if not isinstance(past_key_values, Cache)\n-                else past_key_values.get_seq_length()\n-            )\n+        past_key_values_length = 0 if past_key_values is None else past_key_values.get_seq_length()\n \n         if attention_mask is None:\n             attention_mask = torch.ones((batch_size, seq_length + past_key_values_length)).to(device)"
      },
      {
        "filename": "src/transformers/models/ctrl/modeling_ctrl.py",
        "status": "modified",
        "additions": 0,
        "deletions": 34,
        "changes": 34,
        "patch": "@@ -250,18 +250,6 @@ def forward(\n         **kwargs,  # NOOP kwargs, for now\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPast]:\n         r\"\"\"\n-        input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n-            `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values[0].shape[-2]`\n-            (`sequence_length` of input past key value states). Indices of input sequence tokens in the vocabulary.\n-\n-            If `past_key_values` is used, only input IDs that do not have their past calculated should be passed as\n-            `input_ids`.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.__call__`] and\n-            [`PreTrainedTokenizer.encode`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-\n         Example:\n \n         ```python\n@@ -424,17 +412,6 @@ def forward(\n         **kwargs,\n     ) -> Union[tuple[torch.Tensor], CausalLMOutputWithPast]:\n         r\"\"\"\n-        input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n-            `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values[0].shape[-2]`\n-            (`sequence_length` of input past key value states). Indices of input sequence tokens in the vocabulary.\n-\n-            If `past_key_values` is used, only input IDs that do not have their past calculated should be passed as\n-            `input_ids`.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.__call__`] and\n-            [`PreTrainedTokenizer.encode`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\n             `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\n@@ -572,17 +549,6 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[tuple[torch.Tensor], SequenceClassifierOutput]:\n         r\"\"\"\n-        input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n-            `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values[0].shape[-2]`\n-            (`sequence_length` of input past key value states). Indices of input sequence tokens in the vocabulary.\n-\n-            If `past_key_values` is used, only input IDs that do not have their past calculated should be passed as\n-            `input_ids`.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.__call__`] and\n-            [`PreTrainedTokenizer.encode`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If"
      },
      {
        "filename": "src/transformers/models/megatron_bert/modeling_megatron_bert.py",
        "status": "modified",
        "additions": 1,
        "deletions": 7,
        "changes": 8,
        "patch": "@@ -644,13 +644,7 @@ def forward(\n         batch_size, seq_length = input_shape\n         device = input_ids.device if input_ids is not None else inputs_embeds.device\n \n-        past_key_values_length = 0\n-        if past_key_values is not None:\n-            past_key_values_length = (\n-                past_key_values[0][0].shape[-2]\n-                if not isinstance(past_key_values, Cache)\n-                else past_key_values.get_seq_length()\n-            )\n+        past_key_values_length = 0 if past_key_values is None else past_key_values.get_seq_length()\n \n         if attention_mask is None:\n             attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)"
      },
      {
        "filename": "src/transformers/models/pix2struct/modeling_pix2struct.py",
        "status": "modified",
        "additions": 0,
        "deletions": 1,
        "changes": 1,
        "patch": "@@ -160,7 +160,6 @@ def forward(\n         \"\"\"\n         # Input is (batch_size, seq_length, dim)\n         # Mask is (batch_size, key_length) (non-causal) or (batch_size, key_length, key_length)\n-        # past_key_values[0] is (batch_size, n_heads, q_len - 1, dim_per_head)\n         batch_size, seq_length = hidden_states.shape[:2]\n \n         def to_projection_shape(states):"
      },
      {
        "filename": "src/transformers/models/rembert/modeling_rembert.py",
        "status": "modified",
        "additions": 1,
        "deletions": 7,
        "changes": 8,
        "patch": "@@ -579,13 +579,7 @@ def forward(\n         batch_size, seq_length = input_shape\n         device = input_ids.device if input_ids is not None else inputs_embeds.device\n \n-        past_key_values_length = 0\n-        if past_key_values is not None:\n-            past_key_values_length = (\n-                past_key_values[0][0].shape[-2]\n-                if not isinstance(past_key_values, Cache)\n-                else past_key_values.get_seq_length()\n-            )\n+        past_key_values_length = 0 if past_key_values is None else past_key_values.get_seq_length()\n \n         if attention_mask is None:\n             attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)"
      },
      {
        "filename": "src/transformers/models/roformer/modeling_roformer.py",
        "status": "modified",
        "additions": 1,
        "deletions": 7,
        "changes": 8,
        "patch": "@@ -736,13 +736,7 @@ def forward(\n         batch_size, seq_length = input_shape\n         device = input_ids.device if input_ids is not None else inputs_embeds.device\n \n-        past_key_values_length = 0\n-        if past_key_values is not None:\n-            past_key_values_length = (\n-                past_key_values[0][0].shape[-2]\n-                if not isinstance(past_key_values, Cache)\n-                else past_key_values.get_seq_length()\n-            )\n+        past_key_values_length = 0 if past_key_values is None else past_key_values.get_seq_length()\n \n         if attention_mask is None:\n             attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)"
      },
      {
        "filename": "src/transformers/models/speecht5/modeling_speecht5.py",
        "status": "modified",
        "additions": 1,
        "deletions": 8,
        "changes": 9,
        "patch": "@@ -805,14 +805,7 @@ def forward(\n         else:\n             raise ValueError(\"You have to specify `decoder_input_ids`\")\n \n-        past_key_values_length = 0\n-        if past_key_values is not None:\n-            past_key_values_length = (\n-                past_key_values[0][0].shape[-2]\n-                if not isinstance(past_key_values, Cache)\n-                else past_key_values.get_seq_length()\n-            )\n-\n+        past_key_values_length = 0 if past_key_values is None else past_key_values.get_seq_length()\n         positions = self.embed_positions(input_ids, past_key_values_length)\n \n         inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale"
      },
      {
        "filename": "tests/generation/test_utils.py",
        "status": "modified",
        "additions": 13,
        "deletions": 8,
        "changes": 21,
        "patch": "@@ -4665,7 +4665,7 @@ def test_generate_custom_cache_position(self):\n             value=1,\n         )\n         inputs_2b[\"past_key_values\"] = outputs_1b.past_key_values\n-        cache_length_1b = outputs_1b.past_key_values[0][0].shape[-2]\n+        cache_length_1b = outputs_1b.past_key_values.get_seq_length()\n         inputs_2b[\"cache_position\"] = torch.arange(\n             cache_length_1b,\n             cache_length_1b + inputs_2b[\"input_ids\"].shape[1],\n@@ -4677,14 +4677,19 @@ def test_generate_custom_cache_position(self):\n \n         # The two sets of generated text and past kv should be equal to each other\n         self.assertTrue(has_similar_generate_outputs(traditional_outputs, incremental_outputs))\n-        for layer_idx in range(len(traditional_outputs.past_key_values)):\n-            for kv_idx in range(len(traditional_outputs.past_key_values[layer_idx])):\n-                self.assertTrue(\n-                    torch.allclose(\n-                        traditional_outputs.past_key_values[layer_idx][kv_idx],\n-                        incremental_outputs.past_key_values[layer_idx][kv_idx],\n+        cache1, cache2 = traditional_outputs.past_key_values, incremental_outputs.past_key_values\n+        for idx in range(len(cache1)):\n+            if isinstance(cache1, EncoderDecoderCache):\n+                for subcache in [\"self_attention_cache\", \"cross_attention_cache\"]:\n+                    torch.testing.assert_close(\n+                        getattr(cache1, subcache).layers[idx].keys, getattr(cache2, subcache).layers[idx].keys\n                     )\n-                )\n+                    torch.testing.assert_close(\n+                        getattr(cache1, subcache).layers[idx].values, getattr(cache2, subcache).layers[idx].values\n+                    )\n+            else:\n+                torch.testing.assert_close(cache1.layers[idx].keys, cache2.layers[idx].keys)\n+                torch.testing.assert_close(cache1.layers[idx].values, cache2.layers[idx].values)\n \n     @pytest.mark.generate\n     @parameterized.expand("
      }
    ],
    "num_files": 12,
    "scraped_at": "2025-11-16T21:17:50.141351",
    "filter_decision": {
      "accept": false,
      "reasoning": "This PR is primarily a cleanup task that removes old cache format remnants and updates code to use a new unified API (`get_seq_length()`). The changes are systematic find-and-replace refactoring across multiple files (replacing `past_key_values[0][0].shape[-2]` with `past_key_values.get_seq_length()`), combined with documentation updates and comment removals. While there is context about migrating from an old cache format, the actual code changes lack substantive logic\u2014they're replacing direct cache access with a method call that encapsulates the same operation.",
      "substance_level": "low"
    }
  },
  {
    "pr_number": 41495,
    "title": "Rm yoso kernel",
    "body": "# What does this PR do?\r\n\r\nRemoving the yoso kernels, and using https://huggingface.co/kernels-community/yoso instead",
    "html_url": "https://github.com/huggingface/transformers/pull/41495",
    "created_at": "2025-10-09T23:41:30Z",
    "merged_at": "2025-10-10T08:50:13Z",
    "merge_commit_sha": "3585737746e5c73a37b6d43f429ca6f56f1e3da5",
    "base_ref": "main",
    "head_sha": "73b687e06cb2c3a0397685f6bb86875e66443147",
    "user": "MekkCyber",
    "files": [
      {
        "filename": "src/transformers/kernels/yoso/common.h",
        "status": "removed",
        "additions": 0,
        "deletions": 10,
        "changes": 10,
        "patch": "@@ -1,10 +0,0 @@\n-\n-#define min(a, b) ((a)<(b)?(a):(b))\n-#define max(a, b) ((a)>(b)?(a):(b))\n-#define ceil_divide(a, b) ((a)/(b)+((a)%(b)!=0))\n-#define select(cond, a, b) ((cond)?(a):(b))\n-#define PI 3.141592\n-#define EPSILON 1e-8\n-#define MAX_VAL 1e12\n-#define MIN_VAL -1e12\n-#define EMPTY_VALUE -1"
      },
      {
        "filename": "src/transformers/kernels/yoso/common_cuda.h",
        "status": "removed",
        "additions": 0,
        "deletions": 9,
        "changes": 9,
        "patch": "@@ -1,9 +0,0 @@\n-\n-#define MAX_THREADS_PER_BLOCK 1024\n-#define OPTIMAL_THREADS_PER_BLOCK 256\n-#define WARP_SIZE 32\n-#define MAX_NUM_BLOCK_X 2147483647\n-#define MAX_NUM_BLOCK_Y 65535\n-#define MAX_NUM_BLOCK_Z 65535\n-#define MAX_SHARED_MEM_PER_BLOCK 48000\n-#define FULL_MASK 0xffffffff"
      },
      {
        "filename": "src/transformers/kernels/yoso/common_cuda_device.h",
        "status": "removed",
        "additions": 0,
        "deletions": 79,
        "changes": 79,
        "patch": "@@ -1,79 +0,0 @@\n-\n-#include \"common.h\"\n-\n-template<typename T>\n-__device__ int set_insert(T *set, int set_size, T value) {\n-  int slot = value % set_size;\n-  int start_slot = slot;\n-  while (true) {\n-    T prev = atomicCAS(&set[slot], EMPTY_VALUE, value);\n-    if (prev == EMPTY_VALUE || prev == value) {\n-      return slot;\n-    }\n-    slot = (slot + 1) % set_size;\n-    if (slot == start_slot) {\n-      return -1;\n-    }\n-  }\n-  return -1;\n-}\n-\n-template<typename T>\n-__device__ int set_lookup(T *set, int set_size, T value) {\n-  int slot = value % set_size;\n-  int start_slot = slot;\n-  while (true) {\n-    if (set[slot] == value) {\n-      return slot;\n-    }\n-    slot = (slot + 1) % set_size;\n-    if (slot == start_slot) {\n-      return -1;\n-    }\n-  }\n-  return -1;\n-}\n-\n-template<typename T>\n-__device__ void init_buffer(T init_value, T *buffer, int buffer_size, int num_threads, int thread_id) {\n-  __syncthreads();\n-  for (int i = 0; i < buffer_size; i = i + num_threads) {\n-    int offset_idx = i + thread_id;\n-    if (offset_idx < buffer_size) {\n-      buffer[offset_idx] = init_value;\n-    }\n-  }\n-  __syncthreads();\n-}\n-\n-template<typename T>\n-__device__ void copy_data(T *src_pt, T *dist_pt, int data_length, int num_threads, int thread_id) {\n-  __syncthreads();\n-  for (int i = 0; i < data_length; i = i + num_threads) {\n-    int offset_idx = i + thread_id;\n-    if (offset_idx < data_length) {\n-      dist_pt[offset_idx] = src_pt[offset_idx];\n-    }\n-  }\n-  __syncthreads();\n-}\n-\n-template<typename T>\n-__device__ void init_buffer_nonblocking(T init_value, T *buffer, int buffer_size, int num_threads, int thread_id) {\n-  for (int i = 0; i < buffer_size; i = i + num_threads) {\n-    int offset_idx = i + thread_id;\n-    if (offset_idx < buffer_size) {\n-      buffer[offset_idx] = init_value;\n-    }\n-  }\n-}\n-\n-template<typename T>\n-__device__ void copy_data_nonblocking(T *src_pt, T *dist_pt, int data_length, int num_threads, int thread_id) {\n-  for (int i = 0; i < data_length; i = i + num_threads) {\n-    int offset_idx = i + thread_id;\n-    if (offset_idx < data_length) {\n-      dist_pt[offset_idx] = src_pt[offset_idx];\n-    }\n-  }\n-}"
      },
      {
        "filename": "src/transformers/kernels/yoso/fast_lsh_cumulation.cu",
        "status": "removed",
        "additions": 0,
        "deletions": 588,
        "changes": 588,
        "patch": "@@ -1,588 +0,0 @@\n-// File from https://github.com/mlpen/YOSO/blob/main/encoders/backbones/efficient_attentions/yoso/yoso_v1/cuda/fast_lsh_cumulation.cu\n-\n-#include <torch/extension.h>\n-#include <ATen/ATen.h>\n-#include \"fast_lsh_cumulation.h\"\n-#include \"fast_lsh_cumulation_cuda.h\"\n-#include \"common_cuda.h\"\n-#include \"common.h\"\n-#include <vector>\n-//////////////////////////////////////////////////////////////////////////////////////////////////\n-//////////////////////////////////////////////////////////////////////////////////////////////////\n-\n-std::vector<at::Tensor> fast_hash_ver1_kernel(\n-  at::Tensor query_mask,\n-  at::Tensor query_vector,\n-  at::Tensor key_mask,\n-  at::Tensor key_vector,\n-  int num_hash_f,\n-  int hash_code_len,\n-  bool use_cuda\n-) {\n-\n-  int batch_size = query_vector.size(0);\n-  int num_query = query_vector.size(1);\n-  int num_key = key_vector.size(1);\n-  int vector_dim = query_vector.size(2);\n-\n-  int num_hash_per_part = vector_dim / hash_code_len;\n-  int num_part = max(1, ceil_divide(num_hash_f, num_hash_per_part));\n-\n-  at::Tensor Dmat = 2 * at::randint(0, 2, {batch_size, 3, num_part, vector_dim}, query_mask.options()) - 1;\n-  at::Tensor query_hash_code = at::zeros({batch_size, num_query, num_hash_f}, query_mask.options());\n-  at::Tensor key_hash_code = at::zeros({batch_size, num_key, num_hash_f}, key_mask.options());\n-\n-  int *query_mask_ptr = query_mask.data_ptr<int>();\n-  float *query_vector_ptr = query_vector.data_ptr<float>();\n-  int *key_mask_ptr = key_mask.data_ptr<int>();\n-  float *key_vector_ptr = key_vector.data_ptr<float>();\n-\n-  int *Dmat_ptr = Dmat.data_ptr<int>();\n-\n-  int *query_hash_code_ptr = query_hash_code.data_ptr<int>();\n-  int *key_hash_code_ptr = key_hash_code.data_ptr<int>();\n-\n-  if (use_cuda) {\n-    {\n-      dim3 threads(vector_dim);\n-      dim3 blocks(num_part, num_query, batch_size);\n-      int shared_mem = vector_dim * sizeof(float);\n-      fast_hash_ver1_cuda_kernel<<<blocks, threads, shared_mem>>>(\n-        query_mask_ptr,\n-        query_vector_ptr,\n-        Dmat_ptr,\n-        query_hash_code_ptr,\n-        batch_size,\n-        num_query,\n-        vector_dim,\n-        num_part,\n-        num_hash_f,\n-        hash_code_len\n-      );\n-    }\n-    {\n-      dim3 threads(vector_dim);\n-      dim3 blocks(num_part, num_key, batch_size);\n-      int shared_mem = vector_dim * sizeof(float);\n-      fast_hash_ver1_cuda_kernel<<<blocks, threads, shared_mem>>>(\n-        key_mask_ptr,\n-        key_vector_ptr,\n-        Dmat_ptr,\n-        key_hash_code_ptr,\n-        batch_size,\n-        num_key,\n-        vector_dim,\n-        num_part,\n-        num_hash_f,\n-        hash_code_len\n-      );\n-    }\n-  }\n-\n-  return {query_hash_code, key_hash_code};\n-\n-}\n-\n-at::Tensor lsh_cumulation_ver1_kernel(\n-  at::Tensor query_mask,\n-  at::Tensor query_hash_code,\n-  at::Tensor key_mask,\n-  at::Tensor key_hash_code,\n-  at::Tensor value,\n-  int hashtable_capacity,\n-  bool use_cuda\n-) {\n-\n-  int batch_size = query_hash_code.size(0);\n-  int num_hash_f = query_hash_code.size(2);\n-\n-  int num_query = query_hash_code.size(1);\n-  int num_key = key_hash_code.size(1);\n-  int value_dim = value.size(2);\n-\n-  at::Tensor hashtable_value = at::empty({batch_size, num_hash_f, hashtable_capacity, WARP_SIZE}, value.options());\n-  at::Tensor cumulation_value = at::zeros({batch_size, num_query, value_dim}, value.options());\n-\n-  if (use_cuda) {\n-    int threads_x = WARP_SIZE;\n-    int threads_y = OPTIMAL_THREADS_PER_BLOCK / WARP_SIZE;\n-    int block_x_step1 = num_key / threads_y;\n-    int block_x_step2 = num_query / threads_y;\n-    int block_y = batch_size;\n-\n-    dim3 threads(threads_x, threads_y);\n-    dim3 blocks_step1(block_x_step1, block_y);\n-    dim3 blocks_step2(block_x_step2, block_y);\n-\n-    int *query_mask_ptr = query_mask.data_ptr<int>();\n-    int *query_hash_code_ptr = query_hash_code.data_ptr<int>();\n-    int *key_mask_ptr = key_mask.data_ptr<int>();\n-    int *key_hash_code_ptr = key_hash_code.data_ptr<int>();\n-    float *value_ptr = value.data_ptr<float>();\n-    float *hashtable_value_ptr = hashtable_value.data_ptr<float>();\n-    float *cumulation_value_ptr = cumulation_value.data_ptr<float>();\n-\n-    for (int value_offset = 0; value_offset < value_dim; value_offset = value_offset + WARP_SIZE) {\n-\n-      cudaMemset(hashtable_value_ptr, 0, (batch_size * num_hash_f * hashtable_capacity * WARP_SIZE) * sizeof(float));\n-\n-      lsh_cumulation_ver1_step1_cuda_kernel<<<blocks_step1, threads>>>(\n-        key_mask_ptr,\n-        key_hash_code_ptr,\n-        value_ptr,\n-        hashtable_value_ptr,\n-        batch_size,\n-        num_hash_f,\n-        hashtable_capacity,\n-        num_key,\n-        value_dim,\n-        value_offset\n-      );\n-\n-      lsh_cumulation_ver1_step2_cuda_kernel<<<blocks_step2, threads>>>(\n-        query_mask_ptr,\n-        query_hash_code_ptr,\n-        hashtable_value_ptr,\n-        cumulation_value_ptr,\n-        batch_size,\n-        num_hash_f,\n-        hashtable_capacity,\n-        num_query,\n-        value_dim,\n-        value_offset\n-      );\n-    }\n-\n-  }\n-\n-  return cumulation_value;\n-\n-}\n-\n-at::Tensor lsh_weighted_cumulation_ver1_kernel(\n-  at::Tensor query_mask,\n-  at::Tensor query_hash_code,\n-  at::Tensor query_weight,\n-  at::Tensor key_mask,\n-  at::Tensor key_hash_code,\n-  at::Tensor key_weight,\n-  at::Tensor value,\n-  int hashtable_capacity,\n-  bool use_cuda\n-) {\n-\n-  int batch_size = query_hash_code.size(0);\n-  int num_hash_f = query_hash_code.size(2);\n-\n-  int num_query = query_hash_code.size(1);\n-  int num_key = key_hash_code.size(1);\n-  int value_dim = value.size(2);\n-  int weight_dim = query_weight.size(2);\n-\n-  at::Tensor hashtable_value = at::zeros({batch_size, num_hash_f, hashtable_capacity, WARP_SIZE}, value.options());\n-  at::Tensor cumulation_value = at::zeros({batch_size, num_query, value_dim}, value.options());\n-\n-  if (use_cuda) {\n-    int threads_x = WARP_SIZE;\n-    int threads_y = OPTIMAL_THREADS_PER_BLOCK / WARP_SIZE;\n-    int block_x_step1 = num_key / threads_y;\n-    int block_x_step2 = num_query / threads_y;\n-    int block_y = batch_size;\n-\n-    dim3 threads(threads_x, threads_y);\n-    dim3 blocks_step1(block_x_step1, block_y);\n-    dim3 blocks_step2(block_x_step2, block_y);\n-\n-    int *query_mask_ptr = query_mask.data_ptr<int>();\n-    int *query_hash_code_ptr = query_hash_code.data_ptr<int>();\n-    float *query_weight_ptr = query_weight.data_ptr<float>();\n-    int *key_mask_ptr = key_mask.data_ptr<int>();\n-    int *key_hash_code_ptr = key_hash_code.data_ptr<int>();\n-    float *key_weight_ptr = key_weight.data_ptr<float>();\n-    float *value_ptr = value.data_ptr<float>();\n-    float *hashtable_value_ptr = hashtable_value.data_ptr<float>();\n-    float *cumulation_value_ptr = cumulation_value.data_ptr<float>();\n-\n-    for (int value_offset = 0; value_offset < value_dim; value_offset = value_offset + WARP_SIZE) {\n-      for (int weight_idx = 0; weight_idx < weight_dim; weight_idx++) {\n-\n-        cudaMemset(hashtable_value_ptr, 0, (batch_size * num_hash_f * hashtable_capacity * WARP_SIZE) * sizeof(float));\n-\n-        lsh_weighted_cumulation_ver1_step1_cuda_kernel<<<blocks_step1, threads>>>(\n-          key_mask_ptr,\n-          key_hash_code_ptr,\n-          key_weight_ptr,\n-          value_ptr,\n-          hashtable_value_ptr,\n-          batch_size,\n-          num_hash_f,\n-          hashtable_capacity,\n-          num_key,\n-          value_dim,\n-          weight_dim,\n-          value_offset,\n-          weight_idx\n-        );\n-\n-        lsh_weighted_cumulation_ver1_step2_cuda_kernel<<<blocks_step2, threads>>>(\n-          query_mask_ptr,\n-          query_hash_code_ptr,\n-          query_weight_ptr,\n-          hashtable_value_ptr,\n-          cumulation_value_ptr,\n-          batch_size,\n-          num_hash_f,\n-          hashtable_capacity,\n-          num_query,\n-          value_dim,\n-          weight_dim,\n-          value_offset,\n-          weight_idx\n-        );\n-      }\n-    }\n-\n-  }\n-\n-  return cumulation_value;\n-\n-}\n-\n-at::Tensor lsh_weighted_cumulation_ver2_kernel(\n-  at::Tensor query_mask,\n-  at::Tensor query_hash_code,\n-  at::Tensor query_weight,\n-  at::Tensor key_mask,\n-  at::Tensor key_hash_code,\n-  at::Tensor key_weight,\n-  at::Tensor value,\n-  int hashtable_capacity,\n-  bool use_cuda\n-) {\n-\n-  int batch_size = query_hash_code.size(0);\n-  int num_hash_f = query_hash_code.size(2);\n-\n-  int num_query = query_hash_code.size(1);\n-  int num_key = key_hash_code.size(1);\n-  int value_dim = value.size(2);\n-  int weight_dim = query_weight.size(2);\n-\n-  at::Tensor count_sort_table = at::zeros({batch_size, num_hash_f, hashtable_capacity}, query_hash_code.options());\n-  at::Tensor key_sorted_idxes = at::zeros({batch_size, num_hash_f, num_key}, query_hash_code.options());\n-  at::Tensor query_info = at::zeros({batch_size, num_query, 2, num_hash_f}, query_hash_code.options());\n-  at::Tensor cumulation_value = at::zeros({batch_size, num_query, value_dim}, value.options());\n-\n-  if (use_cuda) {\n-\n-    int *query_mask_ptr = query_mask.data_ptr<int>();\n-    int *query_hash_code_ptr = query_hash_code.data_ptr<int>();\n-    float *query_weight_ptr = query_weight.data_ptr<float>();\n-    int *key_mask_ptr = key_mask.data_ptr<int>();\n-    int *key_hash_code_ptr = key_hash_code.data_ptr<int>();\n-    float *key_weight_ptr = key_weight.data_ptr<float>();\n-    float *value_ptr = value.data_ptr<float>();\n-\n-    int *count_sort_table_ptr = count_sort_table.data_ptr<int>();\n-    int *key_sorted_idxes_ptr = key_sorted_idxes.data_ptr<int>();\n-    int *query_info_ptr = query_info.data_ptr<int>();\n-\n-    float *cumulation_value_ptr = cumulation_value.data_ptr<float>();\n-\n-    {\n-      dim3 threads_step13(num_hash_f, max(1, OPTIMAL_THREADS_PER_BLOCK / num_hash_f));\n-      dim3 blocks_step13(num_key / max(1, OPTIMAL_THREADS_PER_BLOCK / num_hash_f), batch_size);\n-      dim3 threads_step2(min(hashtable_capacity, OPTIMAL_THREADS_PER_BLOCK));\n-      dim3 blocks_step2(num_hash_f, batch_size);\n-      int shared_mem = hashtable_capacity * sizeof(float);\n-      count_sort_step1_cuda_kernel<<<blocks_step13, threads_step13>>>(\n-        key_mask_ptr,\n-        key_hash_code_ptr,\n-        count_sort_table_ptr,\n-        batch_size,\n-        num_hash_f,\n-        hashtable_capacity,\n-        num_key\n-      );\n-      count_sort_step2_cuda_kernel<<<blocks_step2, threads_step2, shared_mem>>>(\n-        count_sort_table_ptr,\n-        batch_size,\n-        num_hash_f,\n-        hashtable_capacity\n-      );\n-      count_sort_step3_cuda_kernel<<<blocks_step13, threads_step13>>>(\n-        key_mask_ptr,\n-        key_hash_code_ptr,\n-        count_sort_table_ptr,\n-        key_sorted_idxes_ptr,\n-        batch_size,\n-        num_hash_f,\n-        hashtable_capacity,\n-        num_key\n-      );\n-    }\n-    {\n-      dim3 threads(num_hash_f, max(1, OPTIMAL_THREADS_PER_BLOCK / num_hash_f));\n-      dim3 blocks(num_query / max(1, OPTIMAL_THREADS_PER_BLOCK / num_hash_f), batch_size);\n-      extract_query_info_cuda_kernel<<<blocks, threads>>>(\n-        query_mask_ptr,\n-        query_hash_code_ptr,\n-        count_sort_table_ptr,\n-        query_info_ptr,\n-        batch_size,\n-        num_hash_f,\n-        hashtable_capacity,\n-        num_query\n-      );\n-    }\n-    {\n-      dim3 threads(WARP_SIZE, OPTIMAL_THREADS_PER_BLOCK / WARP_SIZE);\n-      dim3 blocks(num_query, num_hash_f, batch_size);\n-      int shared_mem = (weight_dim + WARP_SIZE) * sizeof(float);\n-      lsh_weighted_cumulation_ver2_step2_cuda_kernel<<<blocks, threads, shared_mem>>>(\n-        query_mask_ptr,\n-        query_info_ptr,\n-        key_sorted_idxes_ptr,\n-        query_weight_ptr,\n-        key_weight_ptr,\n-        value_ptr,\n-        cumulation_value_ptr,\n-        batch_size,\n-        num_hash_f,\n-        num_query,\n-        num_key,\n-        value_dim,\n-        weight_dim\n-      );\n-    }\n-  }\n-\n-  return cumulation_value;\n-\n-}\n-\n-at::Tensor lsh_weighted_cumulation_ver3_kernel(\n-  at::Tensor query_mask,\n-  at::Tensor query_hash_code,\n-  at::Tensor query_weight,\n-  at::Tensor key_mask,\n-  at::Tensor key_hash_code,\n-  at::Tensor key_weight,\n-  at::Tensor value,\n-  int hashtable_capacity,\n-  bool use_cuda\n-) {\n-\n-  int batch_size = query_hash_code.size(0);\n-  int num_hash_f = query_hash_code.size(2);\n-\n-  int num_query = query_hash_code.size(1);\n-  int num_key = key_hash_code.size(1);\n-  int value_dim = value.size(2);\n-  int weight_dim = query_weight.size(2);\n-\n-  at::Tensor count_sort_table = at::zeros({batch_size, num_hash_f, hashtable_capacity}, query_hash_code.options());\n-  at::Tensor query_sorted_idxes = at::zeros({batch_size, num_hash_f, num_query}, query_hash_code.options());\n-  at::Tensor key_info = at::zeros({batch_size, num_key, 2, num_hash_f}, query_hash_code.options());\n-  at::Tensor cumulation_value = at::zeros({batch_size, num_query, value_dim}, value.options());\n-\n-  if (use_cuda) {\n-\n-    int *query_mask_ptr = query_mask.data_ptr<int>();\n-    int *query_hash_code_ptr = query_hash_code.data_ptr<int>();\n-    float *query_weight_ptr = query_weight.data_ptr<float>();\n-    int *key_mask_ptr = key_mask.data_ptr<int>();\n-    int *key_hash_code_ptr = key_hash_code.data_ptr<int>();\n-    float *key_weight_ptr = key_weight.data_ptr<float>();\n-    float *value_ptr = value.data_ptr<float>();\n-\n-    int *count_sort_table_ptr = count_sort_table.data_ptr<int>();\n-    int *query_sorted_idxes_ptr = query_sorted_idxes.data_ptr<int>();\n-    int *key_info_ptr = key_info.data_ptr<int>();\n-\n-    float *cumulation_value_ptr = cumulation_value.data_ptr<float>();\n-\n-    {\n-      dim3 threads_step13(num_hash_f, max(1, OPTIMAL_THREADS_PER_BLOCK / num_hash_f));\n-      dim3 blocks_step13(num_query / max(1, OPTIMAL_THREADS_PER_BLOCK / num_hash_f), batch_size);\n-      dim3 threads_step2(min(hashtable_capacity, OPTIMAL_THREADS_PER_BLOCK));\n-      dim3 blocks_step2(num_hash_f, batch_size);\n-      int shared_mem = hashtable_capacity * sizeof(float);\n-      count_sort_step1_cuda_kernel<<<blocks_step13, threads_step13>>>(\n-        query_mask_ptr,\n-        query_hash_code_ptr,\n-        count_sort_table_ptr,\n-        batch_size,\n-        num_hash_f,\n-        hashtable_capacity,\n-        num_query\n-      );\n-      count_sort_step2_cuda_kernel<<<blocks_step2, threads_step2, shared_mem>>>(\n-        count_sort_table_ptr,\n-        batch_size,\n-        num_hash_f,\n-        hashtable_capacity\n-      );\n-      count_sort_step3_cuda_kernel<<<blocks_step13, threads_step13>>>(\n-        query_mask_ptr,\n-        query_hash_code_ptr,\n-        count_sort_table_ptr,\n-        query_sorted_idxes_ptr,\n-        batch_size,\n-        num_hash_f,\n-        hashtable_capacity,\n-        num_query\n-      );\n-    }\n-    {\n-      dim3 threads(num_hash_f, max(1, OPTIMAL_THREADS_PER_BLOCK / num_hash_f));\n-      dim3 blocks(num_key / max(1, OPTIMAL_THREADS_PER_BLOCK / num_hash_f), batch_size);\n-      extract_query_info_cuda_kernel<<<blocks, threads>>>(\n-        key_mask_ptr,\n-        key_hash_code_ptr,\n-        count_sort_table_ptr,\n-        key_info_ptr,\n-        batch_size,\n-        num_hash_f,\n-        hashtable_capacity,\n-        num_key\n-      );\n-    }\n-    {\n-      dim3 threads(WARP_SIZE, OPTIMAL_THREADS_PER_BLOCK / WARP_SIZE);\n-      dim3 blocks(num_key, num_hash_f, batch_size);\n-      int shared_mem = (weight_dim + value_dim + WARP_SIZE) * sizeof(float);\n-      lsh_weighted_cumulation_ver3_step2_cuda_kernel<<<blocks, threads, shared_mem>>>(\n-        query_sorted_idxes_ptr,\n-        key_mask_ptr,\n-        key_info_ptr,\n-        query_weight_ptr,\n-        key_weight_ptr,\n-        value_ptr,\n-        cumulation_value_ptr,\n-        batch_size,\n-        num_hash_f,\n-        num_query,\n-        num_key,\n-        value_dim,\n-        weight_dim\n-      );\n-    }\n-  }\n-\n-  return cumulation_value;\n-\n-}\n-\n-at::Tensor lsh_weighted_cumulation_ver4_kernel(\n-  at::Tensor query_mask,\n-  at::Tensor query_hash_code,\n-  at::Tensor query_weight,\n-  at::Tensor key_mask,\n-  at::Tensor key_hash_code,\n-  at::Tensor key_weight,\n-  at::Tensor value,\n-  int hashtable_capacity,\n-  bool use_cuda\n-) {\n-\n-  int batch_size = query_hash_code.size(0);\n-  int num_hash_f = query_hash_code.size(2);\n-\n-  int num_query = query_hash_code.size(1);\n-  int num_key = key_hash_code.size(1);\n-  int value_dim = value.size(2);\n-  int weight_dim = query_weight.size(2);\n-\n-  at::Tensor count_sort_table = at::zeros({batch_size, num_hash_f, hashtable_capacity}, query_hash_code.options());\n-  at::Tensor query_sorted_idxes = at::zeros({batch_size, num_hash_f, num_query}, query_hash_code.options());\n-  at::Tensor key_info = at::zeros({batch_size, num_key, 2, num_hash_f}, query_hash_code.options());\n-  at::Tensor cumulation_value = at::zeros({batch_size, num_query, value_dim}, value.options());\n-\n-  if (use_cuda) {\n-\n-    int *query_mask_ptr = query_mask.data_ptr<int>();\n-    int *query_hash_code_ptr = query_hash_code.data_ptr<int>();\n-    float *query_weight_ptr = query_weight.data_ptr<float>();\n-    int *key_mask_ptr = key_mask.data_ptr<int>();\n-    int *key_hash_code_ptr = key_hash_code.data_ptr<int>();\n-    float *key_weight_ptr = key_weight.data_ptr<float>();\n-    float *value_ptr = value.data_ptr<float>();\n-\n-    int *count_sort_table_ptr = count_sort_table.data_ptr<int>();\n-    int *query_sorted_idxes_ptr = query_sorted_idxes.data_ptr<int>();\n-    int *key_info_ptr = key_info.data_ptr<int>();\n-\n-    float *cumulation_value_ptr = cumulation_value.data_ptr<float>();\n-\n-    {\n-      dim3 threads_step13(num_hash_f, max(1, OPTIMAL_THREADS_PER_BLOCK / num_hash_f));\n-      dim3 blocks_step13(num_query / max(1, OPTIMAL_THREADS_PER_BLOCK / num_hash_f), batch_size);\n-      dim3 threads_step2(min(hashtable_capacity, OPTIMAL_THREADS_PER_BLOCK));\n-      dim3 blocks_step2(num_hash_f, batch_size);\n-      int shared_mem = hashtable_capacity * sizeof(float);\n-      count_sort_step1_cuda_kernel<<<blocks_step13, threads_step13>>>(\n-        query_mask_ptr,\n-        query_hash_code_ptr,\n-        count_sort_table_ptr,\n-        batch_size,\n-        num_hash_f,\n-        hashtable_capacity,\n-        num_query\n-      );\n-      count_sort_step2_cuda_kernel<<<blocks_step2, threads_step2, shared_mem>>>(\n-        count_sort_table_ptr,\n-        batch_size,\n-        num_hash_f,\n-        hashtable_capacity\n-      );\n-      count_sort_step3_cuda_kernel<<<blocks_step13, threads_step13>>>(\n-        query_mask_ptr,\n-        query_hash_code_ptr,\n-        count_sort_table_ptr,\n-        query_sorted_idxes_ptr,\n-        batch_size,\n-        num_hash_f,\n-        hashtable_capacity,\n-        num_query\n-      );\n-    }\n-    {\n-      dim3 threads(num_hash_f, max(1, OPTIMAL_THREADS_PER_BLOCK / num_hash_f));\n-      dim3 blocks(num_key / max(1, OPTIMAL_THREADS_PER_BLOCK / num_hash_f), batch_size);\n-      extract_query_info_cuda_kernel<<<blocks, threads>>>(\n-        key_mask_ptr,\n-        key_hash_code_ptr,\n-        count_sort_table_ptr,\n-        key_info_ptr,\n-        batch_size,\n-        num_hash_f,\n-        hashtable_capacity,\n-        num_key\n-      );\n-    }\n-    {\n-      dim3 threads(WARP_SIZE, OPTIMAL_THREADS_PER_BLOCK / WARP_SIZE);\n-      dim3 blocks(num_key, batch_size);\n-      int shared_mem = (weight_dim + value_dim + 2 * num_hash_f) * sizeof(float);\n-      lsh_weighted_cumulation_ver4_step2_cuda_kernel<<<blocks, threads, shared_mem>>>(\n-        query_sorted_idxes_ptr,\n-        key_mask_ptr,\n-        key_info_ptr,\n-        query_weight_ptr,\n-        key_weight_ptr,\n-        value_ptr,\n-        cumulation_value_ptr,\n-        batch_size,\n-        num_hash_f,\n-        num_query,\n-        num_key,\n-        value_dim,\n-        weight_dim\n-      );\n-    }\n-  }\n-\n-  return cumulation_value;\n-\n-}"
      },
      {
        "filename": "src/transformers/kernels/yoso/fast_lsh_cumulation.h",
        "status": "removed",
        "additions": 0,
        "deletions": 71,
        "changes": 71,
        "patch": "@@ -1,71 +0,0 @@\n-#include <torch/extension.h>\n-#include <ATen/ATen.h>\n-#include <vector>\n-\n-std::vector<at::Tensor> fast_hash_ver1_kernel(\n-  at::Tensor query_mask,\n-  at::Tensor query_vector,\n-  at::Tensor key_mask,\n-  at::Tensor key_vector,\n-  int num_hash_f,\n-  int hash_code_len,\n-  bool use_cuda\n-);\n-\n-at::Tensor lsh_cumulation_ver1_kernel(\n-  at::Tensor query_mask,\n-  at::Tensor query_hash_code,\n-  at::Tensor key_mask,\n-  at::Tensor key_hash_code,\n-  at::Tensor value,\n-  int hashtable_capacity,\n-  bool use_cuda\n-);\n-\n-at::Tensor lsh_weighted_cumulation_ver1_kernel(\n-  at::Tensor query_mask,\n-  at::Tensor query_hash_code,\n-  at::Tensor query_weight,\n-  at::Tensor key_mask,\n-  at::Tensor key_hash_code,\n-  at::Tensor key_weight,\n-  at::Tensor value,\n-  int hashtable_capacity,\n-  bool use_cuda\n-);\n-\n-at::Tensor lsh_weighted_cumulation_ver2_kernel(\n-  at::Tensor query_mask,\n-  at::Tensor query_hash_code,\n-  at::Tensor query_weight,\n-  at::Tensor key_mask,\n-  at::Tensor key_hash_code,\n-  at::Tensor key_weight,\n-  at::Tensor value,\n-  int hashtable_capacity,\n-  bool use_cuda\n-);\n-\n-at::Tensor lsh_weighted_cumulation_ver3_kernel(\n-  at::Tensor query_mask,\n-  at::Tensor query_hash_code,\n-  at::Tensor query_weight,\n-  at::Tensor key_mask,\n-  at::Tensor key_hash_code,\n-  at::Tensor key_weight,\n-  at::Tensor value,\n-  int hashtable_capacity,\n-  bool use_cuda\n-);\n-\n-at::Tensor lsh_weighted_cumulation_ver4_kernel(\n-  at::Tensor query_mask,\n-  at::Tensor query_hash_code,\n-  at::Tensor query_weight,\n-  at::Tensor key_mask,\n-  at::Tensor key_hash_code,\n-  at::Tensor key_weight,\n-  at::Tensor value,\n-  int hashtable_capacity,\n-  bool use_cuda\n-);"
      },
      {
        "filename": "src/transformers/kernels/yoso/fast_lsh_cumulation_cuda.cu",
        "status": "removed",
        "additions": 0,
        "deletions": 825,
        "changes": 825,
        "patch": "@@ -1,825 +0,0 @@\n-// File from https://github.com/mlpen/YOSO/blob/main/encoders/backbones/efficient_attentions/yoso/yoso_v1/cuda/fast_lsh_cumulation_cuda.cu\n-\n-#include \"fast_lsh_cumulation_cuda.h\"\n-#include \"common_cuda_device.h\"\n-#include \"common_cuda.h\"\n-#include \"common.h\"\n-#include <stdio.h>\n-//////////////////////////////////////////////////////////////////////////////////////////////////\n-//////////////////////////////////////////////////////////////////////////////////////////////////\n-\n-inline __device__ void fast_hadamard_transform(float *vector_buffer, int vector_dim, int dim_idx) {\n-  int stride = vector_dim / 2;\n-  while (stride > (WARP_SIZE / 2)) {\n-    __syncthreads();\n-    int sign = 1 - ((dim_idx / stride) % 2) * 2;\n-    float val1 = vector_buffer[dim_idx];\n-    float val2 = vector_buffer[dim_idx + sign * stride];\n-    __syncthreads();\n-    vector_buffer[dim_idx] = float(sign) * val1 + val2;\n-    stride = stride / 2;\n-  }\n-\n-  float val = vector_buffer[dim_idx];\n-  #pragma unroll\n-  for (stride = (WARP_SIZE / 2); stride > 0; stride = stride / 2) {\n-    int sign = 1 - ((dim_idx / stride) % 2) * 2;\n-    val = float(sign) * val + __shfl_xor_sync(FULL_MASK, val, stride);\n-  }\n-  vector_buffer[dim_idx] = val;\n-}\n-\n-__global__ void fast_hash_ver1_cuda_kernel(\n-  int *mask,        // [batch_size, num_vector]\n-  float *vector,    // [batch_size, num_vector, vector_dim]\n-  int *Dmat,        // [batch_size, 3, num_part, vector_dim]\n-  int *hash_code,   // [batch_size, num_vector, num_hash_f]\n-  int batch_size,\n-  int num_vector,\n-  int vector_dim,\n-  int num_part,\n-  int num_hash_f,\n-  int hash_code_len\n-) {\n-\n-  int batch_idx = blockIdx.z;\n-  int vector_idx = blockIdx.y;\n-  int part_idx = blockIdx.x;\n-\n-  int dim_idx = threadIdx.x;\n-\n-  int batch_idx__vector_idx = batch_idx * num_vector + vector_idx;\n-  if (mask[batch_idx__vector_idx] == 0) {\n-    return;\n-  }\n-\n-  extern __shared__ float buffer[];\n-  float *vector_buffer = buffer;\n-\n-  vector_buffer[dim_idx] = vector[batch_idx__vector_idx * vector_dim + dim_idx];\n-\n-  vector_buffer[dim_idx] = vector_buffer[dim_idx] * (float)Dmat[((batch_idx * 3 + 0) * num_part + part_idx) * vector_dim + dim_idx];\n-  fast_hadamard_transform(vector_buffer, vector_dim, dim_idx);\n-  vector_buffer[dim_idx] = vector_buffer[dim_idx] * (float)Dmat[((batch_idx * 3 + 1) * num_part + part_idx) * vector_dim + dim_idx];\n-  fast_hadamard_transform(vector_buffer, vector_dim, dim_idx);\n-  vector_buffer[dim_idx] = vector_buffer[dim_idx] * (float)Dmat[((batch_idx * 3 + 2) * num_part + part_idx) * vector_dim + dim_idx];\n-  fast_hadamard_transform(vector_buffer, vector_dim, dim_idx);\n-\n-  int num_hash_per_part = vector_dim / hash_code_len;\n-  if (hash_code_len == 8 || hash_code_len == 16) {\n-    int code = select(vector_buffer[dim_idx] > 0, 1 << (dim_idx % hash_code_len), 0);\n-    for (int offset = 1; offset < hash_code_len; offset = offset * 2) {\n-      code += __shfl_xor_sync(FULL_MASK, code, offset);\n-    }\n-    if (dim_idx % hash_code_len == 0) {\n-      int hash_f_idx = part_idx * num_hash_per_part + dim_idx / hash_code_len;\n-      if (hash_f_idx < num_hash_f) {\n-        hash_code[batch_idx__vector_idx * num_hash_f + hash_f_idx] = code;\n-      }\n-    }\n-  } else {\n-    vector_buffer[dim_idx] = select(vector_buffer[dim_idx] > 0, 1 << (dim_idx % hash_code_len), 0);\n-    __syncthreads();\n-    if (dim_idx < num_hash_per_part) {\n-      int code = 0;\n-      for (int i = 0; i < hash_code_len; i++) {\n-        code += vector_buffer[dim_idx * hash_code_len + i];\n-      }\n-      int hash_f_idx = part_idx * num_hash_per_part + dim_idx;\n-      if (hash_f_idx < num_hash_f) {\n-        hash_code[batch_idx__vector_idx * num_hash_f + hash_f_idx] = code;\n-      }\n-    }\n-  }\n-}\n-\n-__global__ void lsh_cumulation_ver1_step1_cuda_kernel(\n-  int *key_mask,           // [batch_size, num_key]\n-  int *key_hash_code,      // [batch_size, num_key, num_hash_f]\n-  float *value,            // [batch_size, num_key, value_dim]\n-  float *hashtable_value,  // [batch_size, num_hash_f, hashtable_capacity, WARP_SIZE]\n-  int batch_size,\n-  int num_hash_f,\n-  int hashtable_capacity,\n-  int num_key,\n-  int value_dim,\n-  int offset_warp\n-) {\n-\n-  int warp_thread_idx = threadIdx.x;\n-\n-  int batch_idx = blockIdx.y;\n-  int key_idx = blockIdx.x * blockDim.y + threadIdx.y;\n-\n-  int batch_idx__key_idx = batch_idx * num_key + key_idx;\n-  if (key_mask[batch_idx__key_idx] == 0) {\n-    return;\n-  }\n-\n-  if (num_hash_f > WARP_SIZE) {\n-    float warp_value = value[batch_idx__key_idx * value_dim + offset_warp + warp_thread_idx];\n-    for (int hash_f_start = 0; hash_f_start < num_hash_f; hash_f_start = hash_f_start + WARP_SIZE) {\n-      int warp_hashcode = key_hash_code[batch_idx__key_idx * num_hash_f + hash_f_start + warp_thread_idx];\n-      #pragma unroll\n-      for (int hash_f_offset = 0; hash_f_offset < WARP_SIZE; hash_f_offset++) {\n-        int current_hashcode = warp_hashcode;\n-        current_hashcode = __shfl_sync(FULL_MASK, current_hashcode, hash_f_offset);\n-        int hashtable_idx = (batch_idx * num_hash_f + (hash_f_start + hash_f_offset)) * hashtable_capacity + current_hashcode;\n-        atomicAdd(&hashtable_value[hashtable_idx * WARP_SIZE + warp_thread_idx], warp_value);\n-      }\n-    }\n-  } else {\n-    float warp_value = value[batch_idx__key_idx * value_dim + offset_warp + warp_thread_idx];\n-    int warp_hashcode = 0;\n-    if (warp_thread_idx < num_hash_f) {\n-      warp_hashcode = key_hash_code[batch_idx__key_idx * num_hash_f + warp_thread_idx];\n-    }\n-    for (int hash_f_idx = 0; hash_f_idx < num_hash_f; hash_f_idx++) {\n-      int current_hashcode = warp_hashcode;\n-      current_hashcode = __shfl_sync(FULL_MASK, current_hashcode, hash_f_idx);\n-      int hashtable_idx = (batch_idx * num_hash_f + hash_f_idx) * hashtable_capacity + current_hashcode;\n-      atomicAdd(&hashtable_value[hashtable_idx * WARP_SIZE + warp_thread_idx], warp_value);\n-    }\n-  }\n-\n-}\n-\n-__global__ void lsh_cumulation_ver1_step2_cuda_kernel(\n-  int *query_mask,         // [batch_size, num_query]\n-  int *query_hash_code,    // [batch_size, num_query, num_hash_f]\n-  float *hashtable_value,  // [batch_size, num_hash_f, hashtable_capacity, WARP_SIZE]\n-  float *cumulation_value, // [batch_size, num_query, value_dim]\n-  int batch_size,\n-  int num_hash_f,\n-  int hashtable_capacity,\n-  int num_query,\n-  int value_dim,\n-  int offset_warp\n-) {\n-\n-  int warp_thread_idx = threadIdx.x;\n-\n-  int batch_idx = blockIdx.y;\n-  int query_idx = blockIdx.x * blockDim.y + threadIdx.y;\n-\n-  int batch_idx__query_idx = batch_idx * num_query + query_idx;\n-  if (query_mask[batch_idx__query_idx] == 0) {\n-    return;\n-  }\n-\n-  if (num_hash_f > WARP_SIZE) {\n-    float warp_value = 0;\n-    for (int hash_f_start = 0; hash_f_start < num_hash_f; hash_f_start = hash_f_start + WARP_SIZE) {\n-      int warp_hashcode = query_hash_code[batch_idx__query_idx * num_hash_f + hash_f_start + warp_thread_idx];\n-      #pragma unroll\n-      for (int hash_f_offset = 0; hash_f_offset < WARP_SIZE; hash_f_offset++) {\n-        int current_hashcode = warp_hashcode;\n-        current_hashcode = __shfl_sync(FULL_MASK, current_hashcode, hash_f_offset);\n-        int hashtable_idx = (batch_idx * num_hash_f + (hash_f_start + hash_f_offset)) * hashtable_capacity + current_hashcode;\n-        warp_value = warp_value + hashtable_value[hashtable_idx * WARP_SIZE + warp_thread_idx];\n-      }\n-    }\n-    cumulation_value[batch_idx__query_idx * value_dim + offset_warp + warp_thread_idx] = warp_value / float(num_hash_f);\n-  } else {\n-    float warp_value = 0;\n-    int warp_hashcode = 0;\n-    if (warp_thread_idx < num_hash_f) {\n-      warp_hashcode = query_hash_code[batch_idx__query_idx * num_hash_f + warp_thread_idx];\n-    }\n-    for (int hash_f_idx = 0; hash_f_idx < num_hash_f; hash_f_idx++) {\n-      int current_hashcode = warp_hashcode;\n-      current_hashcode = __shfl_sync(FULL_MASK, current_hashcode, hash_f_idx);\n-      int hashtable_idx = (batch_idx * num_hash_f + hash_f_idx) * hashtable_capacity + current_hashcode;\n-      warp_value = warp_value + hashtable_value[hashtable_idx * WARP_SIZE + warp_thread_idx];\n-    }\n-    cumulation_value[batch_idx__query_idx * value_dim + offset_warp + warp_thread_idx] = warp_value / float(num_hash_f);\n-  }\n-\n-}\n-\n-__global__ void lsh_weighted_cumulation_ver1_step1_cuda_kernel(\n-  int *key_mask,            // [batch_size, num_key]\n-  int *key_hash_code,       // [batch_size, num_key, num_hash_f]\n-  float *key_weight,        // [batch_size, num_key, weight_dim]\n-  float *value,             // [batch_size, num_key, value_dim]\n-  float *hashtable_value,   // [batch_size, num_hash_f, hashtable_capacity, WARP_SIZE]\n-  int batch_size,\n-  int num_hash_f,\n-  int hashtable_capacity,\n-  int num_key,\n-  int value_dim,\n-  int weight_dim,\n-  int offset_warp,\n-  int weight_idx\n-) {\n-\n-  int warp_thread_idx = threadIdx.x;\n-\n-  int batch_idx = blockIdx.y;\n-  int key_idx = blockIdx.x * blockDim.y + threadIdx.y;\n-\n-  int batch_idx__key_idx = batch_idx * num_key + key_idx;\n-  if (key_mask[batch_idx__key_idx] == 0) {\n-    return;\n-  }\n-\n-  if (num_hash_f > WARP_SIZE) {\n-    float warp_value = key_weight[batch_idx__key_idx * weight_dim + weight_idx] * value[batch_idx__key_idx * value_dim + offset_warp + warp_thread_idx];\n-    for (int hash_f_start = 0; hash_f_start < num_hash_f; hash_f_start = hash_f_start + WARP_SIZE) {\n-      int warp_hashcode = key_hash_code[batch_idx__key_idx * num_hash_f + hash_f_start + warp_thread_idx];\n-      #pragma unroll\n-      for (int hash_f_offset = 0; hash_f_offset < WARP_SIZE; hash_f_offset++) {\n-        int current_hashcode = warp_hashcode;\n-        current_hashcode = __shfl_sync(FULL_MASK, current_hashcode, hash_f_offset);\n-        int hashtable_idx = (batch_idx * num_hash_f + (hash_f_start + hash_f_offset)) * hashtable_capacity + current_hashcode;\n-        atomicAdd(&hashtable_value[hashtable_idx * WARP_SIZE + warp_thread_idx], warp_value);\n-      }\n-    }\n-  } else {\n-    float warp_value = key_weight[batch_idx__key_idx * weight_dim + weight_idx] * value[batch_idx__key_idx * value_dim + offset_warp + warp_thread_idx];\n-    int warp_hashcode = 0;\n-    if (warp_thread_idx < num_hash_f) {\n-      warp_hashcode = key_hash_code[batch_idx__key_idx * num_hash_f + warp_thread_idx];\n-    }\n-    for (int hash_f_idx = 0; hash_f_idx < num_hash_f; hash_f_idx++) {\n-      int current_hashcode = warp_hashcode;\n-      current_hashcode = __shfl_sync(FULL_MASK, current_hashcode, hash_f_idx);\n-      int hashtable_idx = (batch_idx * num_hash_f + hash_f_idx) * hashtable_capacity + current_hashcode;\n-      atomicAdd(&hashtable_value[hashtable_idx * WARP_SIZE + warp_thread_idx], warp_value);\n-    }\n-  }\n-\n-}\n-\n-__global__ void lsh_weighted_cumulation_ver1_step2_cuda_kernel(\n-  int *query_mask,          // [batch_size, num_query]\n-  int *query_hash_code,     // [batch_size, num_query, num_hash_f]\n-  float *query_weight,      // [batch_size, num_query, weight_dim]\n-  float *hashtable_value,   // [batch_size, num_hash_f, hashtable_capacity, WARP_SIZE]\n-  float *cumulation_value,  // [batch_size, num_query, value_dim]\n-  int batch_size,\n-  int num_hash_f,\n-  int hashtable_capacity,\n-  int num_query,\n-  int value_dim,\n-  int weight_dim,\n-  int offset_warp,\n-  int weight_idx\n-) {\n-\n-  int warp_thread_idx = threadIdx.x;\n-\n-  int batch_idx = blockIdx.y;\n-  int query_idx = blockIdx.x * blockDim.y + threadIdx.y;\n-\n-  int batch_idx__query_idx = batch_idx * num_query + query_idx;\n-  if (query_mask[batch_idx__query_idx] == 0) {\n-    return;\n-  }\n-\n-  if (num_hash_f > WARP_SIZE) {\n-    float warp_value = 0;\n-    for (int hash_f_start = 0; hash_f_start < num_hash_f; hash_f_start = hash_f_start + WARP_SIZE) {\n-      int warp_hashcode = query_hash_code[batch_idx__query_idx * num_hash_f + hash_f_start + warp_thread_idx];\n-      #pragma unroll\n-      for (int hash_f_offset = 0; hash_f_offset < WARP_SIZE; hash_f_offset++) {\n-        int current_hashcode = warp_hashcode;\n-        current_hashcode = __shfl_sync(FULL_MASK, current_hashcode, hash_f_offset);\n-        int hashtable_idx = (batch_idx * num_hash_f + (hash_f_start + hash_f_offset)) * hashtable_capacity + current_hashcode;\n-        warp_value = warp_value + hashtable_value[hashtable_idx * WARP_SIZE + warp_thread_idx];\n-      }\n-    }\n-    float warp_weight = query_weight[batch_idx__query_idx * weight_dim + weight_idx];\n-    cumulation_value[batch_idx__query_idx * value_dim + offset_warp + warp_thread_idx] += warp_weight * warp_value / float(num_hash_f);\n-  } else {\n-    float warp_value = 0;\n-    int warp_hashcode = 0;\n-    if (warp_thread_idx < num_hash_f) {\n-      warp_hashcode = query_hash_code[batch_idx__query_idx * num_hash_f + warp_thread_idx];\n-    }\n-    for (int hash_f_idx = 0; hash_f_idx < num_hash_f; hash_f_idx++) {\n-      int current_hashcode = warp_hashcode;\n-      current_hashcode = __shfl_sync(FULL_MASK, current_hashcode, hash_f_idx);\n-      int hashtable_idx = (batch_idx * num_hash_f + hash_f_idx) * hashtable_capacity + current_hashcode;\n-      warp_value = warp_value + hashtable_value[hashtable_idx * WARP_SIZE + warp_thread_idx];\n-    }\n-    float warp_weight = query_weight[batch_idx__query_idx * weight_dim + weight_idx];\n-    cumulation_value[batch_idx__query_idx * value_dim + offset_warp + warp_thread_idx] += warp_weight * warp_value / float(num_hash_f);\n-  }\n-\n-}\n-\n-__global__ void count_sort_step1_cuda_kernel(\n-  int *key_mask,         // [batch_size, num_key]\n-  int *key_hash_code,    // [batch_size, num_key, num_hash_f]\n-  int *count_sort_table, // [batch_size, num_hash_f, hashtable_capacity]\n-  int batch_size,\n-  int num_hash_f,\n-  int hashtable_capacity,\n-  int num_key\n-) {\n-\n-  int batch_idx = blockIdx.y;\n-  int key_idx = blockIdx.x * blockDim.y + threadIdx.y;\n-  int hash_f_idx = threadIdx.x;\n-\n-  int batch_idx__key_idx = batch_idx * num_key + key_idx;\n-  if (key_mask[batch_idx__key_idx] == 0) {\n-    return;\n-  }\n-\n-  int hash_code = key_hash_code[batch_idx__key_idx * num_hash_f + hash_f_idx];\n-  atomicAdd(&count_sort_table[(batch_idx * num_hash_f + hash_f_idx) * hashtable_capacity + hash_code], 1);\n-\n-}\n-\n-__global__ void count_sort_step2_cuda_kernel(\n-  int *count_sort_table,  // [batch_size, num_hash_f, hashtable_capacity]\n-  int batch_size,\n-  int num_hash_f,\n-  int hashtable_capacity\n-) {\n-\n-  int batch_idx = blockIdx.y;\n-  int hash_f_idx = blockIdx.x;\n-\n-  int num_threads = blockDim.x;\n-  int thread_id = threadIdx.x;\n-\n-  int batch_idx__hash_f_idx = batch_idx * num_hash_f + hash_f_idx;\n-\n-  extern __shared__ float buffer[];\n-  int *table_buffer = (int*)buffer;\n-\n-  if (thread_id == 0) {\n-    table_buffer[0] = 0;\n-  }\n-  copy_data<int>(&count_sort_table[batch_idx__hash_f_idx * hashtable_capacity], &table_buffer[1], hashtable_capacity - 1, num_threads, thread_id);\n-\n-  for (int table_idx_start = 0; table_idx_start < hashtable_capacity; table_idx_start = table_idx_start + num_threads) {\n-    int thread_value = table_buffer[table_idx_start + thread_id];\n-    int next_thread_value = 0;\n-    for (int offset = 1; offset < WARP_SIZE; offset = offset << 1) {\n-      next_thread_value = __shfl_up_sync(FULL_MASK, thread_value, offset);\n-      if (thread_id % WARP_SIZE >= offset) {\n-        thread_value = thread_value + next_thread_value;\n-      }\n-    }\n-    table_buffer[table_idx_start + thread_id] = thread_value;\n-  }\n-  __syncthreads();\n-\n-  if (hashtable_capacity > WARP_SIZE) {\n-    if (thread_id < WARP_SIZE) {\n-      for (int table_idx_start = WARP_SIZE; table_idx_start < hashtable_capacity; table_idx_start = table_idx_start + WARP_SIZE) {\n-        table_buffer[table_idx_start + thread_id] += table_buffer[table_idx_start - 1];\n-      }\n-    }\n-  }\n-\n-  copy_data<int>(table_buffer, &count_sort_table[batch_idx__hash_f_idx * hashtable_capacity], hashtable_capacity, num_threads, thread_id);\n-\n-}\n-\n-\n-__global__ void count_sort_step3_cuda_kernel(\n-  int *key_mask,          // [batch_size, num_key]\n-  int *key_hash_code,     // [batch_size, num_key, num_hash_f]\n-  int *count_sort_table,  // [batch_size, num_hash_f, hashtable_capacity]\n-  int *key_sorted_idxes,  // [batch_size, num_hash_f, num_key]\n-  int batch_size,\n-  int num_hash_f,\n-  int hashtable_capacity,\n-  int num_key\n-) {\n-\n-  int batch_idx = blockIdx.y;\n-  int key_idx = blockIdx.x * blockDim.y + threadIdx.y;\n-  int hash_f_idx = threadIdx.x;\n-\n-  int batch_idx__key_idx = batch_idx * num_key + key_idx;\n-  if (key_mask[batch_idx__key_idx] == 0) {\n-    return;\n-  }\n-\n-  int batch_idx__hash_f_idx = batch_idx * num_hash_f + hash_f_idx;\n-\n-  int hash_code = key_hash_code[batch_idx__key_idx * num_hash_f + hash_f_idx];\n-  int sort_idx = atomicAdd(&count_sort_table[batch_idx__hash_f_idx * hashtable_capacity + hash_code], 1);\n-  key_sorted_idxes[batch_idx__hash_f_idx * num_key + sort_idx] = key_idx;\n-\n-}\n-\n-__global__ void extract_query_info_cuda_kernel(\n-  int *query_mask,       // [batch_size, num_query]\n-  int *query_hash_code,  // [batch_size, num_query, num_hash_f]\n-  int *count_sort_table, // [batch_size, num_hash_f, hashtable_capacity]\n-  int *query_info,       // [batch_size, num_query, 2, num_hash_f]\n-  int batch_size,\n-  int num_hash_f,\n-  int hashtable_capacity,\n-  int num_query\n-) {\n-\n-  int batch_idx = blockIdx.y;\n-  int query_idx = blockIdx.x * blockDim.y + threadIdx.y;\n-  int hash_f_idx = threadIdx.x;\n-\n-  int batch_idx__query_idx = batch_idx * num_query + query_idx;\n-  if (query_mask[batch_idx__query_idx] == 0) {\n-    return;\n-  }\n-\n-  int hash_code = query_hash_code[batch_idx__query_idx * num_hash_f + hash_f_idx];\n-  int batch_idx__hash_f_idx__hash_code = (batch_idx * num_hash_f + hash_f_idx) * hashtable_capacity + hash_code;\n-\n-  int key_offset = select(hash_code == 0, 0, count_sort_table[batch_idx__hash_f_idx__hash_code - 1]);\n-  int key_count = count_sort_table[batch_idx__hash_f_idx__hash_code] - key_offset;\n-\n-  query_info[batch_idx__query_idx * 2 * num_hash_f + hash_f_idx] = key_offset;\n-  query_info[(batch_idx__query_idx * 2 + 1) * num_hash_f + hash_f_idx] = key_count;\n-\n-}\n-\n-__global__ void lsh_weighted_cumulation_ver2_step2_cuda_kernel(\n-  int *query_mask,         // [batch_size, num_query]\n-  int *query_info,         // [batch_size, num_query, 2, num_hash_f]\n-  int *key_sorted_idxes,   // [batch_size, num_hash_f, num_key]\n-  float *query_weight,     // [batch_size, num_query, weight_dim]\n-  float *key_weight,       // [batch_size, num_key, weight_dim]\n-  float *value,            // [batch_size, num_key, value_dim]\n-  float *cumulation_value, // [batch_size, num_query, value_dim]\n-  int batch_size,\n-  int num_hash_f,\n-  int num_query,\n-  int num_key,\n-  int value_dim,\n-  int weight_dim\n-) {\n-\n-  int batch_idx = blockIdx.z;\n-  int hash_f_idx = blockIdx.y;\n-  int query_idx = blockIdx.x;\n-\n-  int num_threads = blockDim.y * blockDim.x;\n-  int thread_id = threadIdx.y * blockDim.x + threadIdx.x;\n-\n-  int num_warps = blockDim.y;\n-  int warp_idx = threadIdx.y;\n-  int warp_thread_idx = threadIdx.x;\n-\n-  int batch_idx__query_idx = batch_idx * num_query + query_idx;\n-  if (query_mask[batch_idx__query_idx] == 0) {\n-    return;\n-  }\n-\n-  int key_offset = query_info[batch_idx__query_idx * 2 * num_hash_f + hash_f_idx];\n-  int key_count = query_info[(batch_idx__query_idx * 2 + 1) * num_hash_f + hash_f_idx];\n-\n-  if (key_count == 0) {\n-    return;\n-  }\n-\n-  extern __shared__ float buffer[];\n-\n-  if (key_count == 1) {\n-    if (warp_idx == 0) {\n-      int key_idx = key_sorted_idxes[(batch_idx * num_hash_f + hash_f_idx) * num_key + key_offset];\n-      int batch_idx__key_idx = batch_idx * num_key + key_idx;\n-      float weight = 0;\n-      for (int weight_offset = 0; weight_offset < weight_dim; weight_offset = weight_offset + WARP_SIZE) {\n-        int weight_dim_idx = weight_offset + warp_thread_idx;\n-        float val = query_weight[batch_idx__query_idx * weight_dim + weight_dim_idx] * key_weight[batch_idx__key_idx * weight_dim + weight_dim_idx];\n-        #pragma unroll\n-        for (int offset = 1; offset < WARP_SIZE; offset = offset << 1) {\n-          val += __shfl_xor_sync(FULL_MASK, val, offset);\n-        }\n-        weight = weight + val;\n-      }\n-      weight = weight / float(num_hash_f);\n-      for (int value_offset = 0; value_offset < value_dim; value_offset = value_offset + WARP_SIZE) {\n-        int value_dim_idx = value_offset + warp_thread_idx;\n-        float val = value[batch_idx__key_idx * value_dim + value_dim_idx];\n-        atomicAdd(&cumulation_value[batch_idx__query_idx * value_dim + value_dim_idx], weight * val);\n-      }\n-    }\n-  } else {\n-    float *weight_buffer = buffer;\n-    int *key_idxes_buffer = (int*)&buffer[weight_dim];\n-\n-    copy_data_nonblocking<float>(&query_weight[batch_idx__query_idx * weight_dim], weight_buffer, weight_dim, num_threads, thread_id);\n-\n-    while (key_count > 0) {\n-      int work_size = min(WARP_SIZE, key_count);\n-      copy_data_nonblocking<int>(&key_sorted_idxes[(batch_idx * num_hash_f + hash_f_idx) * num_key + key_offset], key_idxes_buffer, work_size, num_threads, thread_id);\n-      __syncthreads();\n-      for (int work_offset = 0; work_offset < WARP_SIZE; work_offset = work_offset + num_warps) {\n-        int work_idx = work_offset + warp_idx;\n-        if (work_idx < key_count) {\n-          int key_idx = key_idxes_buffer[work_idx];\n-          int batch_idx__key_idx = batch_idx * num_key + key_idx;\n-          float weight = 0;\n-          for (int weight_offset = 0; weight_offset < weight_dim; weight_offset = weight_offset + WARP_SIZE) {\n-            int weight_dim_idx = weight_offset + warp_thread_idx;\n-            float val = weight_buffer[weight_dim_idx] * key_weight[batch_idx__key_idx * weight_dim + weight_dim_idx];\n-            #pragma unroll\n-            for (int offset = 1; offset < WARP_SIZE; offset = offset << 1) {\n-              val += __shfl_xor_sync(FULL_MASK, val, offset);\n-            }\n-            weight = weight + val;\n-          }\n-          weight = weight / float(num_hash_f);\n-          for (int value_offset = 0; value_offset < value_dim; value_offset = value_offset + WARP_SIZE) {\n-            int value_dim_idx = value_offset + warp_thread_idx;\n-            float val = value[batch_idx__key_idx * value_dim + value_dim_idx];\n-            atomicAdd(&cumulation_value[batch_idx__query_idx * value_dim + value_dim_idx], weight * val);\n-          }\n-        }\n-      }\n-      key_count = key_count - work_size;\n-      key_offset = key_offset + work_size;\n-    }\n-  }\n-\n-}\n-\n-__global__ void lsh_weighted_cumulation_ver3_step2_cuda_kernel(\n-  int *query_sorted_idxes,   // [batch_size, num_hash_f, num_query]\n-  int *key_mask,             // [batch_size, num_key]\n-  int *key_info,             // [batch_size, num_key, 2, num_hash_f]\n-  float *query_weight,       // [batch_size, num_query, weight_dim]\n-  float *key_weight,         // [batch_size, num_key, weight_dim]\n-  float *value,              // [batch_size, num_key, value_dim]\n-  float *cumulation_value,   // [batch_size, num_query, value_dim]\n-  int batch_size,\n-  int num_hash_f,\n-  int num_query,\n-  int num_key,\n-  int value_dim,\n-  int weight_dim\n-) {\n-\n-  int batch_idx = blockIdx.z;\n-  int hash_f_idx = blockIdx.y;\n-  int key_idx = blockIdx.x;\n-\n-  int num_threads = blockDim.y * blockDim.x;\n-  int thread_id = threadIdx.y * blockDim.x + threadIdx.x;\n-\n-  int num_warps = blockDim.y;\n-  int warp_idx = threadIdx.y;\n-  int warp_thread_idx = threadIdx.x;\n-\n-  int batch_idx__key_idx = batch_idx * num_key + key_idx;\n-  if (key_mask[batch_idx__key_idx] == 0) {\n-    return;\n-  }\n-\n-  int query_offset = key_info[batch_idx__key_idx * 2 * num_hash_f + hash_f_idx];\n-  int query_count = key_info[(batch_idx__key_idx * 2 + 1) * num_hash_f + hash_f_idx];\n-\n-  if (query_count == 0) {\n-    return;\n-  }\n-\n-  extern __shared__ float buffer[];\n-\n-  if (query_count == 1) {\n-    if (warp_idx == 0) {\n-      int query_idx = query_sorted_idxes[(batch_idx * num_hash_f + hash_f_idx) * num_query + query_offset];\n-      int batch_idx__query_idx = batch_idx * num_query + query_idx;\n-      float weight = 0;\n-      for (int weight_offset = 0; weight_offset < weight_dim; weight_offset = weight_offset + WARP_SIZE) {\n-        int weight_dim_idx = weight_offset + warp_thread_idx;\n-        float val = key_weight[batch_idx__key_idx * weight_dim + weight_dim_idx] * query_weight[batch_idx__query_idx * weight_dim + weight_dim_idx];\n-        #pragma unroll\n-        for (int offset = 1; offset < WARP_SIZE; offset = offset << 1) {\n-          val += __shfl_xor_sync(FULL_MASK, val, offset);\n-        }\n-        weight = weight + val;\n-      }\n-      weight = weight / float(num_hash_f);\n-      for (int value_offset = 0; value_offset < value_dim; value_offset = value_offset + WARP_SIZE) {\n-        int value_dim_idx = value_offset + warp_thread_idx;\n-        float val = value[batch_idx__key_idx * value_dim + value_dim_idx];\n-        atomicAdd(&cumulation_value[batch_idx__query_idx * value_dim + value_dim_idx], weight * val);\n-      }\n-    }\n-  } else {\n-    float *weight_buffer = buffer;\n-    float *value_buffer = &buffer[weight_dim];\n-    int *query_idxes_buffer = (int*)&buffer[weight_dim + value_dim];\n-\n-    copy_data_nonblocking<float>(&key_weight[batch_idx__key_idx * weight_dim], weight_buffer, weight_dim, num_threads, thread_id);\n-    copy_data_nonblocking<float>(&value[batch_idx__key_idx * value_dim], value_buffer, value_dim, num_threads, thread_id);\n-\n-    while (query_count > 0) {\n-      int work_size = min(WARP_SIZE, query_count);\n-      copy_data_nonblocking<int>(&query_sorted_idxes[(batch_idx * num_hash_f + hash_f_idx) * num_query + query_offset], query_idxes_buffer, work_size, num_threads, thread_id);\n-      __syncthreads();\n-      for (int work_offset = 0; work_offset < WARP_SIZE; work_offset = work_offset + num_warps) {\n-        int work_idx = work_offset + warp_idx;\n-        if (work_idx < query_count) {\n-          int query_idx = query_idxes_buffer[work_idx];\n-          int batch_idx__query_idx = batch_idx * num_query + query_idx;\n-          float weight = 0;\n-          for (int weight_offset = 0; weight_offset < weight_dim; weight_offset = weight_offset + WARP_SIZE) {\n-            int weight_dim_idx = weight_offset + warp_thread_idx;\n-            float val = weight_buffer[weight_dim_idx] * query_weight[batch_idx__query_idx * weight_dim + weight_dim_idx];\n-            #pragma unroll\n-            for (int offset = 1; offset < WARP_SIZE; offset = offset << 1) {\n-              val += __shfl_xor_sync(FULL_MASK, val, offset);\n-            }\n-            weight = weight + val;\n-          }\n-          weight = weight / float(num_hash_f);\n-          for (int value_offset = 0; value_offset < value_dim; value_offset = value_offset + WARP_SIZE) {\n-            int value_dim_idx = value_offset + warp_thread_idx;\n-            float val = value_buffer[value_dim_idx];\n-            atomicAdd(&cumulation_value[batch_idx__query_idx * value_dim + value_dim_idx], weight * val);\n-          }\n-        }\n-      }\n-      query_count = query_count - work_size;\n-      query_offset = query_offset + work_size;\n-    }\n-  }\n-\n-}\n-\n-__global__ void lsh_weighted_cumulation_ver4_step2_cuda_kernel(\n-  int *query_sorted_idxes,   // [batch_size, num_hash_f, num_query]\n-  int *key_mask,             // [batch_size, num_key]\n-  int *key_info,             // [batch_size, num_key, 2, num_hash_f]\n-  float *query_weight,       // [batch_size, num_query, weight_dim]\n-  float *key_weight,         // [batch_size, num_key, weight_dim]\n-  float *value,              // [batch_size, num_key, value_dim]\n-  float *cumulation_value,   // [batch_size, num_query, value_dim]\n-  int batch_size,\n-  int num_hash_f,\n-  int num_query,\n-  int num_key,\n-  int value_dim,\n-  int weight_dim\n-) {\n-\n-  int batch_idx = blockIdx.y;\n-  int key_idx = blockIdx.x;\n-\n-  int num_threads = blockDim.y * blockDim.x;\n-  int thread_id = threadIdx.y * blockDim.x + threadIdx.x;\n-\n-  int num_warps = blockDim.y;\n-  int warp_idx = threadIdx.y;\n-  int warp_thread_idx = threadIdx.x;\n-\n-  int batch_idx__key_idx = batch_idx * num_key + key_idx;\n-  if (key_mask[batch_idx__key_idx] == 0) {\n-    return;\n-  }\n-\n-  extern __shared__ float buffer[];\n-  float *weight_buffer = buffer;\n-  float *value_buffer = &buffer[weight_dim];\n-  int *key_info_buffer = (int*)&buffer[weight_dim + value_dim];\n-\n-  copy_data_nonblocking<float>(&key_weight[batch_idx__key_idx * weight_dim], weight_buffer, weight_dim, num_threads, thread_id);\n-  copy_data_nonblocking<float>(&value[batch_idx__key_idx * value_dim], value_buffer, value_dim, num_threads, thread_id);\n-  copy_data_nonblocking<int>(&key_info[batch_idx__key_idx * 2 * num_hash_f], key_info_buffer, 2 * num_hash_f, num_threads, thread_id);\n-\n-  int *query_offset_buffer = key_info_buffer;\n-  int *query_count_buffer = &key_info_buffer[num_hash_f];\n-\n-  const int hashtable_size = 1024 + OPTIMAL_THREADS_PER_BLOCK;\n-  __shared__ int hashtable_query[hashtable_size];\n-  __shared__ int hashtable_count[hashtable_size];\n-  __shared__ int inserted_query[hashtable_size];\n-  __shared__ int query_counter[1];\n-\n-  int hash_f_idx_base = 0;\n-\n-  while (true) {\n-\n-    init_buffer_nonblocking<int>(EMPTY_VALUE, hashtable_query, hashtable_size, num_threads, thread_id);\n-    init_buffer_nonblocking<int>(0, hashtable_count, hashtable_size, num_threads, thread_id);\n-    init_buffer_nonblocking<int>(EMPTY_VALUE, inserted_query, hashtable_size, num_threads, thread_id);\n-    init_buffer_nonblocking<int>(0, query_counter, 1, num_threads, thread_id);\n-    __syncthreads();\n-\n-    while (hash_f_idx_base < num_hash_f) {\n-\n-      int hash_f_idx = hash_f_idx_base + warp_idx;\n-      int batch_idx__hash_f_idx = batch_idx * num_hash_f + hash_f_idx;\n-\n-      int stop_flag = 0;\n-\n-      int query_offset = query_offset_buffer[hash_f_idx];\n-      int query_count = query_count_buffer[hash_f_idx];\n-\n-      while (query_count > 0) {\n-\n-        int work_size = min(query_count, WARP_SIZE);\n-\n-        // try inserting query to set and check whether the query is new\n-        int found_new_query = 0;\n-        int query_idx = -1;\n-        if (warp_thread_idx < work_size) {\n-          query_idx = query_sorted_idxes[batch_idx__hash_f_idx * num_query + query_offset + warp_thread_idx];\n-          int slot = set_insert<int>(hashtable_query, hashtable_size, query_idx);\n-          if (slot >= 0) {\n-            found_new_query = atomicAdd(&hashtable_count[slot], 1) == 0;\n-          }\n-        }\n-\n-        // compute cumulative offset\n-        int position_offset = found_new_query;\n-        int next_position_offset = 0;\n-        #pragma unroll\n-        for (int offset = 1; offset < WARP_SIZE; offset = offset << 1) {\n-          next_position_offset = __shfl_up_sync(FULL_MASK, position_offset, offset);\n-          if (thread_id % WARP_SIZE >= offset) {\n-            position_offset = position_offset + next_position_offset;\n-          }\n-        }\n-\n-        // get the inserted query list end index\n-        int inserted_query_base = 0;\n-        if (thread_id % WARP_SIZE == WARP_SIZE - 1) {\n-          inserted_query_base = atomicAdd(query_counter, position_offset);\n-        }\n-        inserted_query_base = __shfl_sync(FULL_MASK, inserted_query_base, WARP_SIZE - 1);\n-\n-        // insert new queries to list\n-        int insert_idx = inserted_query_base + position_offset - 1;\n-        if (found_new_query) {\n-          inserted_query[insert_idx] = query_idx;\n-        }\n-\n-        // remove inserted queries from list\n-        query_offset_buffer[hash_f_idx] += work_size;\n-        query_count_buffer[hash_f_idx] -= work_size;\n-        query_offset += work_size;\n-        query_count -= work_size;\n-\n-        // if list is almost full, stop inserting\n-        if (inserted_query_base + OPTIMAL_THREADS_PER_BLOCK > hashtable_size) {\n-          stop_flag = 1;\n-          break;\n-        }\n-\n-      }\n-\n-      if (stop_flag) {\n-        break;\n-      }\n-\n-      hash_f_idx_base = hash_f_idx_base + num_warps;\n-\n-    }\n-\n-    __syncthreads();\n-\n-    int num_distinct_query = query_counter[0];\n-\n-    if (num_distinct_query > 0) {\n-      for (int idx_base = 0; idx_base < num_distinct_query; idx_base = idx_base + num_warps) {\n-        int idx = idx_base + warp_idx;\n-        if (idx < num_distinct_query) {\n-          int query_idx = inserted_query[idx];\n-          int batch_idx__query_idx = batch_idx * num_query + query_idx;\n-\n-          int slot = set_lookup<int>(hashtable_query, hashtable_size, query_idx);\n-          int duplicate_count = hashtable_count[slot];\n-\n-          float weight = 0;\n-          for (int weight_idx_base = 0; weight_idx_base < weight_dim; weight_idx_base = weight_idx_base + WARP_SIZE) {\n-            int weight_dim_idx = weight_idx_base + warp_thread_idx;\n-            float val = weight_buffer[weight_dim_idx] * query_weight[batch_idx__query_idx * weight_dim + weight_dim_idx];\n-            #pragma unroll\n-            for (int offset = 1; offset < WARP_SIZE; offset = offset << 1) {\n-              val += __shfl_xor_sync(FULL_MASK, val, offset);\n-            }\n-            weight = weight + val;\n-          }\n-\n-          weight = (float)duplicate_count * weight / float(num_hash_f);\n-\n-          for (int value_idx_base = 0; value_idx_base < value_dim; value_idx_base = value_idx_base + WARP_SIZE) {\n-            int value_dim_idx = value_idx_base + warp_thread_idx;\n-            float val = value_buffer[value_dim_idx];\n-            atomicAdd(&cumulation_value[batch_idx__query_idx * value_dim + value_dim_idx], weight * val);\n-          }\n-        }\n-      }\n-    } else {\n-\n-      // all computation is completed if num_distinct_query == 0\n-      break;\n-\n-    }\n-\n-    __syncthreads();\n-\n-  }\n-\n-}"
      },
      {
        "filename": "src/transformers/kernels/yoso/fast_lsh_cumulation_cuda.h",
        "status": "removed",
        "additions": 0,
        "deletions": 157,
        "changes": 157,
        "patch": "@@ -1,157 +0,0 @@\n-__global__ void fast_hash_ver1_cuda_kernel(\n-  int *mask,        // [batch_size, num_vector]\n-  float *vector,    // [batch_size, num_vector, vector_dim]\n-  int *Dmat,        // [3, num_part, vector_dim]\n-  int *hash_code,   // [batch_size, num_vector, num_hash_f]\n-  int batch_size,\n-  int num_vector,\n-  int vector_dim,\n-  int num_part,\n-  int num_hash_f,\n-  int hash_code_len\n-);\n-\n-__global__ void lsh_cumulation_ver1_step1_cuda_kernel(\n-  int *key_mask,           // [batch_size, num_key]\n-  int *key_hash_code,      // [batch_size, num_key, num_hash_f]\n-  float *value,            // [batch_size, num_key, value_dim]\n-  float *hashtable_value,  // [batch_size, num_hash_f, hashtable_capacity, value_dim]\n-  int batch_size,\n-  int num_hash_f,\n-  int hashtable_capacity,\n-  int num_key,\n-  int value_dim,\n-  int offset_warp\n-);\n-\n-__global__ void lsh_cumulation_ver1_step2_cuda_kernel(\n-  int *query_mask,         // [batch_size, num_query]\n-  int *query_hash_code,    // [batch_size, num_query, num_hash_f]\n-  float *hashtable_value,  // [batch_size, num_hash_f, hashtable_capacity, value_dim]\n-  float *cumulation_value, // [batch_size, num_query, value_dim]\n-  int batch_size,\n-  int num_hash_f,\n-  int hashtable_capacity,\n-  int num_query,\n-  int value_dim,\n-  int offset_warp\n-);\n-\n-__global__ void lsh_weighted_cumulation_ver1_step1_cuda_kernel(\n-  int *key_mask,            // [batch_size, num_key]\n-  int *key_hash_code,       // [batch_size, num_key, num_hash_f]\n-  float *key_weight,        // [batch_size, num_key, weight_dim]\n-  float *value,             // [batch_size, num_key, value_dim]\n-  float *hashtable_value,   // [batch_size, num_hash_f, hashtable_capacity, WARP_SIZE]\n-  int batch_size,\n-  int num_hash_f,\n-  int hashtable_capacity,\n-  int num_key,\n-  int value_dim,\n-  int weight_dim,\n-  int offset_warp,\n-  int weight_idx\n-);\n-\n-__global__ void lsh_weighted_cumulation_ver1_step2_cuda_kernel(\n-  int *query_mask,          // [batch_size, num_query]\n-  int *query_hash_code,     // [batch_size, num_query, num_hash_f]\n-  float *query_weight,      // [batch_size, num_query, weight_dim]\n-  float *hashtable_value,   // [batch_size, num_hash_f, hashtable_capacity, WARP_SIZE]\n-  float *cumulation_value,  // [batch_size, num_query, value_dim]\n-  int batch_size,\n-  int num_hash_f,\n-  int hashtable_capacity,\n-  int num_query,\n-  int value_dim,\n-  int weight_dim,\n-  int offset_warp,\n-  int weight_idx\n-);\n-\n-__global__ void count_sort_step1_cuda_kernel(\n-  int *key_mask,         // [batch_size, num_key]\n-  int *key_hash_code,    // [batch_size, num_key, num_hash_f]\n-  int *count_sort_table, // [batch_size, num_hash_f, hashtable_capacity]\n-  int batch_size,\n-  int num_hash_f,\n-  int hashtable_capacity,\n-  int num_key\n-);\n-\n-__global__ void count_sort_step2_cuda_kernel(\n-  int *count_sort_table,  // [batch_size, num_hash_f, hashtable_capacity]\n-  int batch_size,\n-  int num_hash_f,\n-  int hashtable_capacity\n-);\n-\n-__global__ void count_sort_step3_cuda_kernel(\n-  int *key_mask,          // [batch_size, num_key]\n-  int *key_hash_code,     // [batch_size, num_key, num_hash_f]\n-  int *count_sort_table,  // [batch_size, num_hash_f, hashtable_capacity]\n-  int *key_sorted_idxes,  // [batch_size, num_hash_f, num_key]\n-  int batch_size,\n-  int num_hash_f,\n-  int hashtable_capacity,\n-  int num_key\n-);\n-\n-__global__ void extract_query_info_cuda_kernel(\n-  int *query_mask,       // [batch_size, num_query]\n-  int *query_hash_code,  // [batch_size, num_query, num_hash_f]\n-  int *count_sort_table, // [batch_size, num_hash_f, hashtable_capacity]\n-  int *query_info,       // [batch_size, num_query, 2, num_hash_f]\n-  int batch_size,\n-  int num_hash_f,\n-  int hashtable_capacity,\n-  int num_query\n-);\n-\n-__global__ void lsh_weighted_cumulation_ver2_step2_cuda_kernel(\n-  int *query_mask,         // [batch_size, num_query]\n-  int *query_info,         // [batch_size, num_query, 2, num_hash_f]\n-  int *key_sorted_idxes,   // [batch_size, num_hash_f, num_key]\n-  float *query_weight,     // [batch_size, num_query, weight_dim]\n-  float *key_weight,       // [batch_size, num_key, weight_dim]\n-  float *value,            // [batch_size, num_key, value_dim]\n-  float *cumulation_value, // [batch_size, num_query, value_dim]\n-  int batch_size,\n-  int num_hash_f,\n-  int num_query,\n-  int num_key,\n-  int value_dim,\n-  int weight_dim\n-);\n-\n-__global__ void lsh_weighted_cumulation_ver3_step2_cuda_kernel(\n-  int *query_sorted_idxes,   // [batch_size, num_hash_f, num_query]\n-  int *key_mask,             // [batch_size, num_key]\n-  int *key_info,             // [batch_size, num_key, 2, num_hash_f]\n-  float *query_weight,       // [batch_size, num_query, weight_dim]\n-  float *key_weight,         // [batch_size, num_key, weight_dim]\n-  float *value,              // [batch_size, num_key, value_dim]\n-  float *cumulation_value,   // [batch_size, num_query, value_dim]\n-  int batch_size,\n-  int num_hash_f,\n-  int num_query,\n-  int num_key,\n-  int value_dim,\n-  int weight_dim\n-);\n-\n-__global__ void lsh_weighted_cumulation_ver4_step2_cuda_kernel(\n-  int *query_sorted_idxes,   // [batch_size, num_hash_f, num_query]\n-  int *key_mask,             // [batch_size, num_key]\n-  int *key_info,             // [batch_size, num_key, 2, num_hash_f]\n-  float *query_weight,       // [batch_size, num_query, weight_dim]\n-  float *key_weight,         // [batch_size, num_key, weight_dim]\n-  float *value,              // [batch_size, num_key, value_dim]\n-  float *cumulation_value,   // [batch_size, num_query, value_dim]\n-  int batch_size,\n-  int num_hash_f,\n-  int num_query,\n-  int num_key,\n-  int value_dim,\n-  int weight_dim\n-);"
      },
      {
        "filename": "src/transformers/kernels/yoso/fast_lsh_cumulation_torch.cpp",
        "status": "removed",
        "additions": 0,
        "deletions": 128,
        "changes": 128,
        "patch": "@@ -1,128 +0,0 @@\n-#include <torch/extension.h>\n-#include <ATen/ATen.h>\n-#include \"fast_lsh_cumulation.h\"\n-#include \"common_cuda.h\"\n-#include <vector>\n-\n-std::vector<at::Tensor> fast_hash(\n-  at::Tensor query_mask,\n-  at::Tensor query_vector,\n-  at::Tensor key_mask,\n-  at::Tensor key_vector,\n-  int num_hash_f,\n-  int hash_code_len,\n-  bool use_cuda,\n-  int version\n-) {\n-  return fast_hash_ver1_kernel(\n-    query_mask,\n-    query_vector,\n-    key_mask,\n-    key_vector,\n-    num_hash_f,\n-    hash_code_len,\n-    use_cuda\n-  );\n-}\n-\n-at::Tensor lsh_cumulation(\n-  at::Tensor query_mask,         // [batch_size, num_query]\n-  at::Tensor query_hash_code,    // [batch_size, num_query, num_hash_f]\n-  at::Tensor key_mask,           // [batch_size, num_key]\n-  at::Tensor key_hash_code,      // [batch_size, num_key, num_hash_f]\n-  at::Tensor value,              // [batch_size, num_key, value_dim]\n-  int hashtable_capacity,\n-  bool use_cuda,\n-  int version\n-) {\n-  return lsh_cumulation_ver1_kernel(\n-    query_mask,\n-    query_hash_code,\n-    key_mask,\n-    key_hash_code,\n-    value,\n-    hashtable_capacity,\n-    use_cuda\n-  );\n-}\n-\n-at::Tensor lsh_weighted_cumulation(\n-  at::Tensor query_mask,         // [batch_size, num_query]\n-  at::Tensor query_hash_code,    // [batch_size, num_query, num_hash_f]\n-  at::Tensor query_weight,       // [batch_size, num_query, weight_dim]\n-  at::Tensor key_mask,           // [batch_size, num_key]\n-  at::Tensor key_hash_code,      // [batch_size, num_key, num_hash_f]\n-  at::Tensor key_weight,         // [batch_size, num_key, weight_dim]\n-  at::Tensor value,              // [batch_size, num_key, value_dim]\n-  int hashtable_capacity,\n-  bool use_cuda,\n-  int version\n-) {\n-  if (version == 1) {\n-    return lsh_weighted_cumulation_ver1_kernel(\n-      query_mask,\n-      query_hash_code,\n-      query_weight,\n-      key_mask,\n-      key_hash_code,\n-      key_weight,\n-      value,\n-      hashtable_capacity,\n-      use_cuda\n-    );\n-  } else if (version == 2) {\n-    return lsh_weighted_cumulation_ver2_kernel(\n-      query_mask,\n-      query_hash_code,\n-      query_weight,\n-      key_mask,\n-      key_hash_code,\n-      key_weight,\n-      value,\n-      hashtable_capacity,\n-      use_cuda\n-    );\n-  } else if (version == 3) {\n-    return lsh_weighted_cumulation_ver3_kernel(\n-      query_mask,\n-      query_hash_code,\n-      query_weight,\n-      key_mask,\n-      key_hash_code,\n-      key_weight,\n-      value,\n-      hashtable_capacity,\n-      use_cuda\n-    );\n-  } else if (version == 4) {\n-    return lsh_weighted_cumulation_ver4_kernel(\n-      query_mask,\n-      query_hash_code,\n-      query_weight,\n-      key_mask,\n-      key_hash_code,\n-      key_weight,\n-      value,\n-      hashtable_capacity,\n-      use_cuda\n-    );\n-  } else {\n-    return lsh_weighted_cumulation_ver3_kernel(\n-      query_mask,\n-      query_hash_code,\n-      query_weight,\n-      key_mask,\n-      key_hash_code,\n-      key_weight,\n-      value,\n-      hashtable_capacity,\n-      use_cuda\n-    );\n-  }\n-}\n-\n-PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n-  m.def(\"fast_hash\", &fast_hash, \"Fast Hash (CUDA)\");\n-  m.def(\"lsh_cumulation\", &lsh_cumulation, \"LSH Cumulation (CUDA)\");\n-  m.def(\"lsh_weighted_cumulation\", &lsh_weighted_cumulation, \"LSH Weighted Cumulation (CUDA)\");\n-}"
      },
      {
        "filename": "src/transformers/models/yoso/modeling_yoso.py",
        "status": "modified",
        "additions": 6,
        "deletions": 11,
        "changes": 17,
        "patch": "@@ -15,7 +15,6 @@\n \"\"\"PyTorch YOSO model.\"\"\"\n \n import math\n-from pathlib import Path\n from typing import Optional, Union\n \n import torch\n@@ -36,6 +35,7 @@\n from ...pytorch_utils import apply_chunking_to_forward\n from ...utils import (\n     auto_docstring,\n+    is_kernels_available,\n     is_ninja_available,\n     is_torch_cuda_available,\n     logging,\n@@ -51,17 +51,12 @@\n \n def load_cuda_kernels():\n     global lsh_cumulation\n-    from torch.utils.cpp_extension import load\n+    if not is_kernels_available():\n+        raise ImportError(\"kernels is not installed, please install it with `pip install kernels`\")\n+    from kernels import get_kernel\n \n-    def append_root(files):\n-        src_folder = Path(__file__).resolve().parent.parent.parent / \"kernels\" / \"yoso\"\n-        return [src_folder / file for file in files]\n-\n-    src_files = append_root([\"fast_lsh_cumulation_torch.cpp\", \"fast_lsh_cumulation.cu\", \"fast_lsh_cumulation_cuda.cu\"])\n-\n-    load(\"fast_lsh_cumulation\", src_files, verbose=True)\n-\n-    import fast_lsh_cumulation as lsh_cumulation\n+    yoso = get_kernel(\"kernels-community/yoso\")\n+    lsh_cumulation = yoso.lsh_cumulation\n \n \n def to_contiguous(input_tensors):"
      }
    ],
    "num_files": 9,
    "scraped_at": "2025-11-16T21:17:51.771600",
    "filter_decision": {
      "accept": false,
      "reasoning": "This PR is primarily a code deletion/removal that replaces internal kernel implementations with an external dependency. While there is a small amount of logic change in the Python file (switching from local torch.utils.cpp_extension.load to importing from an external kernels package), the substantial majority of the PR is removal of CUDA kernel code without corresponding new logic being added. This is essentially a refactoring/cleanup operation that externalizes functionality rather than implementing new features or fixing bugs with meaningful architectural decisions.",
      "substance_level": "low"
    }
  },
  {
    "pr_number": 41493,
    "title": "[kernels] Remove RWKV kernel finally !",
    "body": "# What does this PR do?\r\n\r\nCleans the rwkv kernel, after adding the kernel to `kernels-community` : https://huggingface.co/kernels-community/rwkv",
    "html_url": "https://github.com/huggingface/transformers/pull/41493",
    "created_at": "2025-10-09T22:19:42Z",
    "merged_at": "2025-10-10T08:32:05Z",
    "merge_commit_sha": "b543679d0ec057cb51a1d0be7b86df0e78556763",
    "base_ref": "main",
    "head_sha": "8791d4e62c7d7330952e59e32e4857ff30f7897d",
    "user": "MekkCyber",
    "files": [
      {
        "filename": "src/transformers/kernels/rwkv/wkv_cuda.cu",
        "status": "removed",
        "additions": 0,
        "deletions": 187,
        "changes": 187,
        "patch": "@@ -1,187 +0,0 @@\n-#include <stdio.h>\n-#include <assert.h>\n-\n-#define MIN_VALUE (-1e38)\n-\n-template <typename F>\n-__global__ void kernel_forward(\n-    const int B, const int T, const int C, const F *__restrict__ const _w, const F *__restrict__ const _u,\n-    const F *__restrict__ const _k, const F *__restrict__ const _v, F *__restrict__ const _y\n-) {\n-    const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n-    const int _b = idx / C;\n-    const int _c = idx % C;\n-    const int _offset = _b * T * C + _c;\n-\n-    F u = _u[_c];\n-    F w = _w[_c];\n-    const F *__restrict__ const k = _k + _offset;\n-    const F *__restrict__ const v = _v + _offset;\n-    F *__restrict__ const y = _y + _offset;\n-\n-    // aa and bb are running sums divided by exp(pp) (to avoid overflow)\n-    F aa = 0, bb = 0, pp = MIN_VALUE;\n-    for (int i = 0; i < T; i++) {\n-        const int ii = i * C;\n-        const F kk = k[ii];\n-        const F vv = v[ii];\n-\n-        F ww = u + kk;\n-        F p = max(pp, ww);\n-        F e1 = exp(pp - p);\n-        F e2 = exp(ww - p);\n-        y[ii] = (e1 * aa + e2 * vv) / (e1 * bb + e2);\n-        \n-        ww = w + pp;\n-        p = max(ww, kk);\n-        e1 = exp(ww - p);\n-        e2 = exp(kk - p);\n-        aa = e1 * aa + e2 * vv;\n-        bb = e1 * bb + e2;\n-        pp = p;\n-    }\n-}\n-\n-template <typename F>\n-__global__ void kernel_forward_with_state(\n-    const int B, const int T, const int C, const F *__restrict__ const _w, const F *__restrict__ const _u,\n-    const F *__restrict__ const _k, const F *__restrict__ const _v, F *__restrict__ const _y, F *__restrict__ const _s\n-) {\n-    const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n-    const int _b = idx / C;\n-    const int _c = idx % C;\n-    const int _offset_s = _b * C * 3 + _c * 3;\n-    const int _offset = _b * T * C + _c;\n-\n-    F u = _u[_c];\n-    F w = _w[_c];\n-    const F *__restrict__ const k = _k + _offset;\n-    const F *__restrict__ const v = _v + _offset;\n-    F *__restrict__ const y = _y + _offset;\n-    F *__restrict__ const s = _s + _offset_s;\n-\n-    // aa and bb are running sums divided by exp(pp) (to avoid overflow)\n-    F aa = s[0], bb = s[1], pp = s[2];\n-    for (int i = 0; i < T; i++) {\n-        const int ii = i * C;\n-        const F kk = k[ii];\n-        const F vv = v[ii];\n-\n-        F ww = u + kk;\n-        F p = max(pp, ww);\n-        F e1 = exp(pp - p);\n-        F e2 = exp(ww - p);\n-        y[ii] = (e1 * aa + e2 * vv) / (e1 * bb + e2);\n-        \n-        ww = w + pp;\n-        p = max(ww, kk);\n-        e1 = exp(ww - p);\n-        e2 = exp(kk - p);\n-        aa = e1 * aa + e2 * vv;\n-        bb = e1 * bb + e2;\n-        pp = p;\n-    }\n-    s[0] = aa;\n-    s[1] = bb;\n-    s[2] = pp;\n-}\n-\n-template <typename F>\n-__global__ void kernel_backward(\n-    const int B, const int T, const int C, const F *__restrict__ const _w, const F *__restrict__ const _u,\n-    const F *__restrict__ const _k, const F *__restrict__ const _v, const F *__restrict__ const _y,\n-    const F *__restrict__ const _gy, F *__restrict__ const _gw, F *__restrict__ const _gu, F *__restrict__ const _gk,\n-    F *__restrict__ const _gv\n-) {\n-    const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n-    const int _b = idx / C;\n-    const int _c = idx % C;\n-    const int _offset = _b * T * C + _c;\n-\n-    F u = _u[_c];\n-    F w = _w[_c];\n-    const F *__restrict__ const k = _k + _offset;\n-    const F *__restrict__ const v = _v + _offset;\n-    const F *__restrict__ const y = _y + _offset;\n-    const F *__restrict__ const gy = _gy + _offset;\n-    F *__restrict__ const gk = _gk + _offset;\n-    F *__restrict__ const gv = _gv + _offset;\n-\n-    F q[Tmax], r[Tmax];\n-\n-    F gw = 0, gu = 0, aa = 0, bb = 0, ga = 0, gb = 0, pp = MIN_VALUE;\n-    for (int i = 0; i < T; i++) {\n-        const int ii = i * C;\n-        const F kk = k[ii];\n-        const F vv = v[ii];\n-        const F yy = y[ii];\n-\n-        F ww = u + kk;\n-        F p = max(pp, ww);\n-        F e1 = exp(pp - p);\n-        F e2 = exp(ww - p);\n-        const F qq = gy[ii] / (e1 * bb + e2);\n-        gw += (ga - gb * yy) * e1 * qq;\n-        gu += (vv - yy) * e2 * qq;\n-        q[i] = qq;\n-        r[i] = ww - p;\n-\n-        ww = w + pp;\n-        p = max(ww, kk);\n-        e1 = exp(ww - p);\n-        e2 = exp(kk - p);\n-        ga = e1 * (aa + ga);\n-        gb = e1 * (bb + gb);\n-        aa = e1 * aa + e2 * vv;\n-        bb = e1 * bb + e2;\n-        pp = p;\n-    }\n-    const int _offsetBC = _b * C + _c;\n-    _gw[_offsetBC] = gw * _w[_c]; // multiply by w because of w -> -exp(w) in python forward()\n-    _gu[_offsetBC] = gu;\n-\n-    aa = 0, bb = 0, pp = MIN_VALUE;\n-    for (int i = T - 1; i >= 0; i--) {\n-        const int ii = i * C;\n-        const F kk = k[ii];\n-        const F vv = v[ii];\n-        const F yy = y[ii];\n-        const F qq = q[i];\n-        const F rr = r[i];\n-\n-        F e1 = qq * exp(rr);\n-        F e2 = exp(kk + pp);\n-        gk[ii] = e1 * (vv - yy) + e2 * (aa * vv + bb);\n-        gv[ii] = e1 + e2 * aa;\n-\n-        const F ww = w + pp;\n-        const F www = rr - u - kk;\n-        const F p = max(ww, www);\n-        e1 = exp(ww - p);\n-        e2 = qq * exp(www - p);\n-        aa = e1 * aa + e2;\n-        bb = e1 * bb - e2 * yy;\n-        pp = p;\n-    }\n-}\n-\n-void cuda_forward(int B, int T, int C, float *w, float *u, float *k, float *v, float *y) {\n-    dim3 threadsPerBlock( min(C, 32) ); // requires --maxrregcount 60 for optimal performance\n-    assert(B * C % threadsPerBlock.x == 0);\n-    dim3 numBlocks(B * C / threadsPerBlock.x);\n-    kernel_forward<<<numBlocks, threadsPerBlock>>>(B, T, C, w, u, k, v, y);\n-}\n-\n-void cuda_forward_with_state(int B, int T, int C, float *w, float *u, float *k, float *v, float *y, float *s) {\n-    dim3 threadsPerBlock( min(C, 32) ); // requires --maxrregcount 60 for optimal performance\n-    assert(B * C % threadsPerBlock.x == 0);\n-    dim3 numBlocks(B * C / threadsPerBlock.x);\n-    kernel_forward_with_state<<<numBlocks, threadsPerBlock>>>(B, T, C, w, u, k, v, y, s);\n-}\n-\n-void cuda_backward(int B, int T, int C, float *w, float *u, float *k, float *v, float *y, float *gy, float *gw, float *gu, float *gk, float *gv) {\n-    dim3 threadsPerBlock( min(C, 32) ); // requires --maxrregcount 60 for optimal performance\n-    assert(B * C % threadsPerBlock.x == 0);\n-    dim3 numBlocks(B * C / threadsPerBlock.x);\n-    kernel_backward<<<numBlocks, threadsPerBlock>>>(B, T, C, w, u, k, v, y, gy, gw, gu, gk, gv);\n-}"
      },
      {
        "filename": "src/transformers/kernels/rwkv/wkv_cuda_bf16.cu",
        "status": "removed",
        "additions": 0,
        "deletions": 186,
        "changes": 186,
        "patch": "@@ -1,186 +0,0 @@\n-#include <stdio.h>\n-#include <assert.h>\n-#include \"ATen/ATen.h\"\n-#define MIN_VALUE (-1e38)\n-typedef at::BFloat16 bf16;\n-\n-__global__ void kernel_forward_bf16(\n-    const int B, const int T, const int C, const float *__restrict__ const _w, const bf16 *__restrict__ const _u,\n-    const bf16 *__restrict__ const _k, const bf16 *__restrict__ const _v, bf16 *__restrict__ const _y\n-) {\n-    const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n-    const int _b = idx / C;\n-    const int _c = idx % C;\n-    const int _offset = _b * T * C + _c;\n-\n-    float u = float(_u[_c]);\n-    float w = _w[_c];\n-    const bf16 *__restrict__ const k = _k + _offset;\n-    const bf16 *__restrict__ const v = _v + _offset;\n-    bf16 *__restrict__ const y = _y + _offset;\n-\n-    // aa and bb are running sums divided by exp(pp) (to avoid overflow)\n-    float aa = 0, bb = 0, pp = MIN_VALUE;\n-    for (int i = 0; i < T; i++) {\n-        const int ii = i * C;\n-        const float kk = float(k[ii]);\n-        const float vv = float(v[ii]);\n-\n-        float ww = u + kk;\n-        float p = max(pp, ww);\n-        float e1 = exp(pp - p);\n-        float e2 = exp(ww - p);\n-        y[ii] = bf16((e1 * aa + e2 * vv) / (e1 * bb + e2));\n-        \n-        ww = w + pp;\n-        p = max(ww, kk);\n-        e1 = exp(ww - p);\n-        e2 = exp(kk - p);\n-        aa = e1 * aa + e2 * vv;\n-        bb = e1 * bb + e2;\n-        pp = p;\n-    }\n-}\n-\n-__global__ void kernel_forward_with_state_bf16(\n-    const int B, const int T, const int C, const float *__restrict__ const _w, const bf16 *__restrict__ const _u,\n-    const bf16 *__restrict__ const _k, const bf16 *__restrict__ const _v, bf16 *__restrict__ const _y,\n-    float *__restrict__ const _s\n-) {\n-    const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n-    const int _b = idx / C;\n-    const int _c = idx % C;\n-    const int _offset_s = _b * C * 3 + _c * 3;\n-    const int _offset = _b * T * C + _c;\n-\n-    float u = float(_u[_c]);\n-    float w = _w[_c];\n-    const bf16 *__restrict__ const k = _k + _offset;\n-    const bf16 *__restrict__ const v = _v + _offset;\n-    bf16 *__restrict__ const y = _y + _offset;\n-    float *__restrict__ const s = _s + _offset_s;\n-\n-    // aa and bb are running sums divided by exp(pp) (to avoid overflow)\n-    float aa = s[0], bb = s[1], pp = s[2];\n-    for (int i = 0; i < T; i++) {\n-        const int ii = i * C;\n-        const float kk = float(k[ii]);\n-        const float vv = float(v[ii]);\n-\n-        float ww = u + kk;\n-        float p = max(pp, ww);\n-        float e1 = exp(pp - p);\n-        float e2 = exp(ww - p);\n-        y[ii] = bf16(e1 * aa + e2 * vv) / (e1 * bb + e2);\n-        \n-        ww = w + pp;\n-        p = max(ww, kk);\n-        e1 = exp(ww - p);\n-        e2 = exp(kk - p);\n-        aa = e1 * aa + e2 * vv;\n-        bb = e1 * bb + e2;\n-        pp = p;\n-    }\n-    s[0] = aa;\n-    s[1] = bb;\n-    s[2] = pp;\n-}\n-\n-__global__ void kernel_backward_bf16(\n-    const int B, const int T, const int C, const float *__restrict__ const _w, const bf16 *__restrict__ const _u,\n-    const bf16 *__restrict__ const _k, const bf16 *__restrict__ const _v, const bf16 *__restrict__ const _y,\n-    const bf16 *__restrict__ const _gy, bf16 *__restrict__ const _gw, bf16 *__restrict__ const _gu,\n-    bf16 *__restrict__ const _gk, bf16 *__restrict__ const _gv\n-) {\n-    const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n-    const int _b = idx / C;\n-    const int _c = idx % C;\n-    const int _offset = _b * T * C + _c;\n-\n-    float u = float(_u[_c]);\n-    float w = _w[_c];\n-    const bf16 *__restrict__ const k = _k + _offset;\n-    const bf16 *__restrict__ const v = _v + _offset;\n-    const bf16 *__restrict__ const y = _y + _offset;\n-    const bf16 *__restrict__ const gy = _gy + _offset;\n-    bf16 *__restrict__ const gk = _gk + _offset;\n-    bf16 *__restrict__ const gv = _gv + _offset;\n-\n-    float q[Tmax], r[Tmax];\n-\n-    float gw = 0, gu = 0, aa = 0, bb = 0, ga = 0, gb = 0, pp = MIN_VALUE;\n-    for (int i = 0; i < T; i++) {\n-        const int ii = i * C;\n-        const float kk = float(k[ii]);\n-        const float vv = float(v[ii]);\n-        const float yy = float(y[ii]);\n-\n-        float ww = u + kk;\n-        float p = max(pp, ww);\n-        float e1 = exp(pp - p);\n-        float e2 = exp(ww - p);\n-        const float qq = float(gy[ii]) / (e1 * bb + e2);\n-        gw += (ga - gb * yy) * e1 * qq;\n-        gu += (vv - yy) * e2 * qq;\n-        q[i] = qq;\n-        r[i] = ww - p;\n-\n-        ww = w + pp;\n-        p = max(ww, kk);\n-        e1 = exp(ww - p);\n-        e2 = exp(kk - p);\n-        ga = e1 * (aa + ga);\n-        gb = e1 * (bb + gb);\n-        aa = e1 * aa + e2 * vv;\n-        bb = e1 * bb + e2;\n-        pp = p;\n-    }\n-    const int _offsetBC = _b * C + _c;\n-    _gw[_offsetBC] = bf16(gw * _w[_c]); // multiply by w because of w -> -exp(w) in python forward()\n-    _gu[_offsetBC] = bf16(gu);\n-\n-    aa = 0, bb = 0, pp = MIN_VALUE;\n-    for (int i = T - 1; i >= 0; i--) {\n-        const int ii = i * C;\n-        const float kk = float(k[ii]);\n-        const float vv = float(v[ii]);\n-        const float yy = float(y[ii]);\n-        const float qq = q[i];\n-        const float rr = r[i];\n-\n-        float e1 = qq * exp(rr);\n-        float e2 = exp(kk + pp);\n-        gk[ii] = bf16(e1 * (vv - yy) + e2 * (aa * vv + bb));\n-        gv[ii] = bf16(e1 + e2 * aa);\n-\n-        const float ww = w + pp;\n-        const float www = rr - u - kk;\n-        const float p = max(ww, www);\n-        e1 = exp(ww - p);\n-        e2 = qq * exp(www - p);\n-        aa = e1 * aa + e2;\n-        bb = e1 * bb - e2 * yy;\n-        pp = p;\n-    }\n-}\n-\n-void cuda_forward_bf16(int B, int T, int C, float *w, bf16 *u, bf16 *k, bf16 *v, bf16 *y) {\n-    dim3 threadsPerBlock( min(C, 32) ); // requires --maxrregcount 60 for optimal performance\n-    assert(B * C % threadsPerBlock.x == 0);\n-    dim3 numBlocks(B * C / threadsPerBlock.x);\n-    kernel_forward_bf16<<<numBlocks, threadsPerBlock>>>(B, T, C, w, u, k, v, y);\n-}\n-\n-void cuda_forward_with_state_bf16(int B, int T, int C, float *w, bf16 *u, bf16 *k, bf16 *v, bf16 *y, float *s) {\n-    dim3 threadsPerBlock( min(C, 32) ); // requires --maxrregcount 60 for optimal performance\n-    assert(B * C % threadsPerBlock.x == 0);\n-    dim3 numBlocks(B * C / threadsPerBlock.x);\n-    kernel_forward_with_state_bf16<<<numBlocks, threadsPerBlock>>>(B, T, C, w, u, k, v, y, s);\n-}\n-\n-void cuda_backward_bf16(int B, int T, int C, float *w, bf16 *u, bf16 *k, bf16 *v, bf16 *y, bf16 *gy, bf16 *gw, bf16 *gu, bf16 *gk, bf16 *gv) {\n-    dim3 threadsPerBlock( min(C, 32) ); // requires --maxrregcount 60 for optimal performance\n-    assert(B * C % threadsPerBlock.x == 0);\n-    dim3 numBlocks(B * C / threadsPerBlock.x);\n-    kernel_backward_bf16<<<numBlocks, threadsPerBlock>>>(B, T, C, w, u, k, v, y, gy, gw, gu, gk, gv);\n-}"
      },
      {
        "filename": "src/transformers/kernels/rwkv/wkv_op.cpp",
        "status": "removed",
        "additions": 0,
        "deletions": 66,
        "changes": 66,
        "patch": "@@ -1,66 +0,0 @@\n-#include <torch/extension.h>\n-#include \"ATen/ATen.h\"\n-typedef at::BFloat16 bf16;\n-\n-void cuda_forward(int B, int T, int C, float *w, float *u, float *k, float *v, float *y);\n-void cuda_forward_bf16(int B, int T, int C, float *w, bf16 *u, bf16 *k, bf16 *v, bf16 *y);\n-void cuda_forward_with_state(int B, int T, int C, float *w, float *u, float *k, float *v, float *y, float *s);\n-void cuda_forward_with_state_bf16(int B, int T, int C, float *w, bf16 *u, bf16 *k, bf16 *v, bf16 *y, float *s);\n-void cuda_backward(int B, int T, int C, float *w, float *u, float *k, float *v, float *y, float *gy, float *gw, float *gu, float *gk, float *gv);\n-void cuda_backward_bf16(int B, int T, int C, float *w, bf16 *u, bf16 *k, bf16 *v, bf16 *y, bf16 *gy, bf16 *gw, bf16 *gu, bf16 *gk, bf16 *gv);\n-\n-void forward(torch::Tensor &w, torch::Tensor &u, torch::Tensor &k, torch::Tensor &v, torch::Tensor &y) {\n-    const int B = k.size(0);\n-    const int T = k.size(1);\n-    const int C = k.size(2);\n-    cuda_forward(B, T, C, w.data_ptr<float>(), u.data_ptr<float>(), k.data_ptr<float>(), v.data_ptr<float>(), y.data_ptr<float>());\n-}\n-void forward_bf16(torch::Tensor &w, torch::Tensor &u, torch::Tensor &k, torch::Tensor &v, torch::Tensor &y) {\n-    const int B = k.size(0);\n-    const int T = k.size(1);\n-    const int C = k.size(2);\n-    cuda_forward_bf16(B, T, C, w.data_ptr<float>(), u.data_ptr<bf16>(), k.data_ptr<bf16>(), v.data_ptr<bf16>(), y.data_ptr<bf16>());\n-}\n-void forward_with_state(torch::Tensor &w, torch::Tensor &u, torch::Tensor &k, torch::Tensor &v, torch::Tensor &y, torch::Tensor &s) {\n-    const int B = k.size(0);\n-    const int T = k.size(1);\n-    const int C = k.size(2);\n-    cuda_forward_with_state(B, T, C, w.data_ptr<float>(), u.data_ptr<float>(), k.data_ptr<float>(), v.data_ptr<float>(), y.data_ptr<float>(), s.data_ptr<float>());\n-}\n-void forward_with_state_bf16(torch::Tensor &w, torch::Tensor &u, torch::Tensor &k, torch::Tensor &v, torch::Tensor &y, torch::Tensor &s) {\n-    const int B = k.size(0);\n-    const int T = k.size(1);\n-    const int C = k.size(2);\n-    cuda_forward_with_state_bf16(B, T, C, w.data_ptr<float>(), u.data_ptr<bf16>(), k.data_ptr<bf16>(), v.data_ptr<bf16>(), y.data_ptr<bf16>(), s.data_ptr<float>());\n-}\n-void backward(torch::Tensor &w, torch::Tensor &u, torch::Tensor &k, torch::Tensor &v, torch::Tensor &y, torch::Tensor &gy, torch::Tensor &gw, torch::Tensor &gu, torch::Tensor &gk, torch::Tensor &gv) {\n-    const int B = k.size(0);\n-    const int T = k.size(1);\n-    const int C = k.size(2);\n-    cuda_backward(B, T, C, w.data_ptr<float>(), u.data_ptr<float>(), k.data_ptr<float>(), v.data_ptr<float>(), y.data_ptr<float>(), gy.data_ptr<float>(), gw.data_ptr<float>(), gu.data_ptr<float>(), gk.data_ptr<float>(), gv.data_ptr<float>());\n-}\n-void backward_bf16(torch::Tensor &w, torch::Tensor &u, torch::Tensor &k, torch::Tensor &v, torch::Tensor &y, torch::Tensor &gy, torch::Tensor &gw, torch::Tensor &gu, torch::Tensor &gk, torch::Tensor &gv) {\n-    const int B = k.size(0);\n-    const int T = k.size(1);\n-    const int C = k.size(2);\n-    cuda_backward_bf16(B, T, C, w.data_ptr<float>(), u.data_ptr<bf16>(), k.data_ptr<bf16>(), v.data_ptr<bf16>(), y.data_ptr<bf16>(),\n-        gy.data_ptr<bf16>(), gw.data_ptr<bf16>(), gu.data_ptr<bf16>(), gk.data_ptr<bf16>(), gv.data_ptr<bf16>());\n-}\n-\n-PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n-    m.def(\"forward\", &forward, \"wkv forward\");\n-    m.def(\"forward_bf16\", &forward_bf16, \"wkv forward bf16\");\n-    m.def(\"forward_with_state\", &forward_with_state, \"wkv forward with state\");\n-    m.def(\"forward_with_state_bf16\", &forward_with_state_bf16, \"wkv forward with state bf16\");\n-    m.def(\"backward\", &backward, \"wkv backward\");\n-    m.def(\"backward_bf16\", &backward_bf16, \"wkv backward bf16\");\n-}\n-\n-TORCH_LIBRARY(wkv, m) {\n-    m.def(\"forward\", forward);\n-    m.def(\"forward_bf16\", forward_bf16);\n-    m.def(\"forward_with_state\", forward_with_state);\n-    m.def(\"forward_with_state_bf16\", forward_with_state_bf16);\n-    m.def(\"backward\", backward);\n-    m.def(\"backward_bf16\", backward_bf16);\n-}"
      },
      {
        "filename": "src/transformers/models/rwkv/modeling_rwkv.py",
        "status": "modified",
        "additions": 6,
        "deletions": 27,
        "changes": 33,
        "patch": "@@ -17,7 +17,6 @@\n \n import math\n from dataclasses import dataclass\n-from pathlib import Path\n from typing import Optional, Union\n \n import torch\n@@ -30,6 +29,7 @@\n     ModelOutput,\n     auto_docstring,\n     is_bitsandbytes_available,\n+    is_kernels_available,\n     is_ninja_available,\n     is_torch_cuda_available,\n     logging,\n@@ -44,34 +44,13 @@\n \n \n def load_wkv_cuda_kernel(context_length):\n-    from torch.utils.cpp_extension import load as load_kernel\n-\n     global rwkv_cuda_kernel\n+    if not is_kernels_available():\n+        raise ImportError(\"kernels is not installed, please install it with `pip install kernels`\")\n+\n+    from kernels import get_kernel\n \n-    kernel_folder = Path(__file__).resolve().parent.parent.parent / \"kernels\" / \"rwkv\"\n-    cuda_kernel_files = [kernel_folder / f for f in [\"wkv_op.cpp\", \"wkv_cuda.cu\", \"wkv_cuda_bf16.cu\"]]\n-\n-    # Only load the kernel if it's not been loaded yet or if we changed the context length\n-    if rwkv_cuda_kernel is not None and rwkv_cuda_kernel.max_seq_length == context_length:\n-        return\n-\n-    logger.info(f\"Loading CUDA kernel for RWKV at context length of {context_length}.\")\n-\n-    flags = [\n-        \"-res-usage\",\n-        \"--maxrregcount 60\",\n-        \"--use_fast_math\",\n-        \"-O3\",\n-        \"-Xptxas -O3\",\n-        \"--extra-device-vectorization\",\n-        f\"-DTmax={context_length}\",\n-    ]\n-    rwkv_cuda_kernel = load_kernel(\n-        name=f\"wkv_{context_length}\",\n-        sources=cuda_kernel_files,\n-        verbose=(logging.get_verbosity() == logging.DEBUG),\n-        extra_cuda_cflags=flags,\n-    )\n+    rwkv_cuda_kernel = get_kernel(\"kernels-community/rwkv\")\n     rwkv_cuda_kernel.max_seq_length = context_length\n \n "
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T21:17:52.075473",
    "filter_decision": {
      "accept": false,
      "reasoning": "This PR is primarily a code deletion/removal with minimal new logic. While it involves refactoring how RWKV kernels are loaded (moving from local files to an external package), the actual substance is limited to replacing inline kernel loading code with an import from an external dependency. The changes do not involve algorithmic complexity, architectural decisions, or logic that would support substantive technical questions about how the codebase works.",
      "substance_level": "low"
    }
  },
  {
    "pr_number": 41476,
    "title": "Pickle - part 2",
    "body": "# What does this PR do?\r\n\r\nRequire \r\n```\r\n    if not strtobool(os.environ.get(\"TRUST_REMOTE_CODE\", \"False\")):\r\n        raise ValueError(\r\n            \"This part uses `pickle.load` which is insecure and will execute arbitrary code that is potentially \"\r\n            \"malicious. It's recommended to never unpickle data that could have come from an untrusted source, or \"\r\n            \"that could have been tampered with. If you already verified the pickle data and decided to use it, \"\r\n            \"you can set the environment variable `TRUST_REMOTE_CODE` to `True` to allow it.\"\r\n        )\r\n```\r\n\r\nI limit this PR to conversion scripts only. For other usage of pickle in our codebase, I will try to see if there is any alternative.\r\n",
    "html_url": "https://github.com/huggingface/transformers/pull/41476",
    "created_at": "2025-10-09T13:16:11Z",
    "merged_at": "2025-10-09T13:46:54Z",
    "merge_commit_sha": "9ef804472b25c4f69c1eb213dea6f791615538a0",
    "base_ref": "main",
    "head_sha": "67f8f0e7d16a7c297a1cbdeefb62ab19e11cd940",
    "user": "ydshieh",
    "files": [
      {
        "filename": "src/transformers/models/deprecated/mega/convert_mega_original_pytorch_checkpoint_to_pytorch.py",
        "status": "modified",
        "additions": 11,
        "deletions": 2,
        "changes": 13,
        "patch": "@@ -29,14 +29,16 @@\n \n # utilities to import the model weights and config file\n import os\n-import pickle as pkl\n+import pickle\n \n # PyTorch + new model classes\n import torch\n from torch import nn\n \n from transformers import AutoTokenizer, MegaConfig, MegaForMaskedLM\n \n+from ....utils import strtobool\n+\n \n # import the EncoderLayer class used to pretrain\n # !! NOTE !! this requires the version of fairseq that is built when you install the Mega source\n@@ -122,8 +124,15 @@ def forward(self, input_ids, attention_mask, batch_first=True, ignore_mask_value\n \n # code to convert the checkpoint located in the user-specified location\n def convert_checkpoint_to_huggingface(pretrained_checkpoint_path, output_path, includes_tokenizer):\n+    if not strtobool(os.environ.get(\"TRUST_REMOTE_CODE\", \"False\")):\n+        raise ValueError(\n+            \"This part uses `pickle.load` which is insecure and will execute arbitrary code that is potentially \"\n+            \"malicious. It's recommended to never unpickle data that could have come from an untrusted source, or \"\n+            \"that could have been tampered with. If you already verified the pickle data and decided to use it, \"\n+            \"you can set the environment variable `TRUST_REMOTE_CODE` to `True` to allow it.\"\n+        )\n     with open(os.path.join(pretrained_checkpoint_path, \"model_args.pkl\"), \"rb\") as f:\n-        mega_original_args = pkl.load(f)\n+        mega_original_args = pickle.load(f)\n \n     # load the original encoder\n     original_mlm = OriginalMegaForMaskedLM(**mega_original_args).eval()"
      },
      {
        "filename": "src/transformers/models/glm4v/convert_glm4v_mgt_weights_to_hf.py",
        "status": "modified",
        "additions": 10,
        "deletions": 0,
        "changes": 10,
        "patch": "@@ -24,6 +24,8 @@\n import torch\n from safetensors.torch import save_file\n \n+from ...utils import strtobool\n+\n \n # Avoid Using Megatron Lib\n class UnpicklerWrapper(pickle.Unpickler):\n@@ -248,6 +250,14 @@ def save_sharded_model(state_dict, output_path, max_shard_size_gb=5, num_layers=\n \n \n def merge_tp_weights(model_path, output_path, vllm_config_path=None):\n+    if not strtobool(os.environ.get(\"TRUST_REMOTE_CODE\", \"False\")):\n+        raise ValueError(\n+            \"This part uses `pickle.load` which is insecure and will execute arbitrary code that is potentially \"\n+            \"malicious. It's recommended to never unpickle data that could have come from an untrusted source, or \"\n+            \"that could have been tampered with. If you already verified the pickle data and decided to use it, \"\n+            \"you can set the environment variable `TRUST_REMOTE_CODE` to `True` to allow it.\"\n+        )\n+\n     tp_size = 0\n     for item in Path(model_path).iterdir():\n         if item.is_dir():"
      },
      {
        "filename": "src/transformers/models/maskformer/convert_maskformer_resnet_to_pytorch.py",
        "status": "modified",
        "additions": 10,
        "deletions": 0,
        "changes": 10,
        "patch": "@@ -17,6 +17,7 @@\n \n import argparse\n import json\n+import os\n import pickle\n from pathlib import Path\n \n@@ -28,6 +29,8 @@\n from transformers import MaskFormerConfig, MaskFormerForInstanceSegmentation, MaskFormerImageProcessor, ResNetConfig\n from transformers.utils import logging\n \n+from ...utils import strtobool\n+\n \n logging.set_verbosity_info()\n logger = logging.get_logger(__name__)\n@@ -266,6 +269,13 @@ def convert_maskformer_checkpoint(\n     \"\"\"\n     config = get_maskformer_config(model_name)\n \n+    if not strtobool(os.environ.get(\"TRUST_REMOTE_CODE\", \"False\")):\n+        raise ValueError(\n+            \"This part uses `pickle.load` which is insecure and will execute arbitrary code that is potentially \"\n+            \"malicious. It's recommended to never unpickle data that could have come from an untrusted source, or \"\n+            \"that could have been tampered with. If you already verified the pickle data and decided to use it, \"\n+            \"you can set the environment variable `TRUST_REMOTE_CODE` to `True` to allow it.\"\n+        )\n     # load original state_dict\n     with open(checkpoint_path, \"rb\") as f:\n         data = pickle.load(f)"
      },
      {
        "filename": "src/transformers/models/maskformer/convert_maskformer_swin_to_pytorch.py",
        "status": "modified",
        "additions": 10,
        "deletions": 0,
        "changes": 10,
        "patch": "@@ -17,6 +17,7 @@\n \n import argparse\n import json\n+import os\n import pickle\n from pathlib import Path\n \n@@ -28,6 +29,8 @@\n from transformers import MaskFormerConfig, MaskFormerForInstanceSegmentation, MaskFormerImageProcessor, SwinConfig\n from transformers.utils import logging\n \n+from ...utils import strtobool\n+\n \n logging.set_verbosity_info()\n logger = logging.get_logger(__name__)\n@@ -235,6 +238,13 @@ def convert_maskformer_checkpoint(\n     \"\"\"\n     config = get_maskformer_config(model_name)\n \n+    if not strtobool(os.environ.get(\"TRUST_REMOTE_CODE\", \"False\")):\n+        raise ValueError(\n+            \"This part uses `pickle.load` which is insecure and will execute arbitrary code that is potentially \"\n+            \"malicious. It's recommended to never unpickle data that could have come from an untrusted source, or \"\n+            \"that could have been tampered with. If you already verified the pickle data and decided to use it, \"\n+            \"you can set the environment variable `TRUST_REMOTE_CODE` to `True` to allow it.\"\n+        )\n     # load original state_dict\n     with open(checkpoint_path, \"rb\") as f:\n         data = pickle.load(f)"
      },
      {
        "filename": "src/transformers/models/olmo3/convert_olmo3_weights_to_hf.py",
        "status": "modified",
        "additions": 16,
        "deletions": 0,
        "changes": 16,
        "patch": "@@ -39,6 +39,8 @@\n \n from transformers import AutoTokenizer, Olmo3Config, Olmo3ForCausalLM\n \n+from ...utils import strtobool\n+\n \n \"\"\"\n Sample usage:\n@@ -198,6 +200,13 @@ def read_data(self, plan: dist_cp.LoadPlan, planner: dist_cp.LoadPlanner) -> Fut\n     def read_metadata(self) -> Metadata:\n         if self._metadata is None:\n             try:\n+                if not strtobool(os.environ.get(\"TRUST_REMOTE_CODE\", \"False\")):\n+                    raise ValueError(\n+                        \"This part uses `pickle.load` which is insecure and will execute arbitrary code that is potentially \"\n+                        \"malicious. It's recommended to never unpickle data that could have come from an untrusted source, or \"\n+                        \"that could have been tampered with. If you already verified the pickle data and decided to use it, \"\n+                        \"you can set the environment variable `TRUST_REMOTE_CODE` to `True` to allow it.\"\n+                    )\n                 with (Path(self.path) / \".metadata\").open(\"rb\") as metadata_file:\n                     metadata = pickle.load(metadata_file)\n             except FileNotFoundError as exc:\n@@ -256,6 +265,13 @@ def _load_unsharded_keys(\n         )\n         return state_dict\n \n+    if not strtobool(os.environ.get(\"TRUST_REMOTE_CODE\", \"False\")):\n+        raise ValueError(\n+            \"This part uses `pickle.load` which is insecure and will execute arbitrary code that is potentially \"\n+            \"malicious. It's recommended to never unpickle data that could have come from an untrusted source, or \"\n+            \"that could have been tampered with. If you already verified the pickle data and decided to use it, \"\n+            \"you can set the environment variable `TRUST_REMOTE_CODE` to `True` to allow it.\"\n+        )\n     with (Path(model_path) / \".metadata\").open(\"rb\") as metadata_file:\n         metadata = pickle.load(metadata_file)\n         keys = [key for key in metadata.state_dict_metadata.keys() if key.startswith(\"model.\")]"
      },
      {
        "filename": "src/transformers/models/perceiver/convert_perceiver_haiku_to_pytorch.py",
        "status": "modified",
        "additions": 10,
        "deletions": 0,
        "changes": 10,
        "patch": "@@ -16,6 +16,7 @@\n \n import argparse\n import json\n+import os\n import pickle\n from pathlib import Path\n \n@@ -39,6 +40,8 @@\n )\n from transformers.utils import logging\n \n+from ...utils import strtobool\n+\n \n logging.set_verbosity_info()\n logger = logging.get_logger(__name__)\n@@ -264,6 +267,13 @@ def convert_perceiver_checkpoint(pickle_file, pytorch_dump_folder_path, architec\n     \"\"\"\n     Copy/paste/tweak model's weights to our Perceiver structure.\n     \"\"\"\n+    if not strtobool(os.environ.get(\"TRUST_REMOTE_CODE\", \"False\")):\n+        raise ValueError(\n+            \"This part uses `pickle.load` which is insecure and will execute arbitrary code that is potentially \"\n+            \"malicious. It's recommended to never unpickle data that could have come from an untrusted source, or \"\n+            \"that could have been tampered with. If you already verified the pickle data and decided to use it, \"\n+            \"you can set the environment variable `TRUST_REMOTE_CODE` to `True` to allow it.\"\n+        )\n \n     # load parameters as FlatMapping data structure\n     with open(pickle_file, \"rb\") as f:"
      },
      {
        "filename": "src/transformers/models/reformer/convert_reformer_trax_checkpoint_to_pytorch.py",
        "status": "modified",
        "additions": 10,
        "deletions": 0,
        "changes": 10,
        "patch": "@@ -15,6 +15,7 @@\n \"\"\"Convert Reformer checkpoint.\"\"\"\n \n import argparse\n+import os\n import pickle\n \n import numpy as np\n@@ -24,6 +25,8 @@\n from transformers import ReformerConfig, ReformerModelWithLMHead\n from transformers.utils import logging\n \n+from ...utils import strtobool\n+\n \n logging.set_verbosity_info()\n \n@@ -188,6 +191,13 @@ def convert_trax_checkpoint_to_pytorch(trax_model_pkl_path, config_file, pytorch\n     print(f\"Building PyTorch model from configuration: {config}\")\n     model = ReformerModelWithLMHead(config)\n \n+    if not strtobool(os.environ.get(\"TRUST_REMOTE_CODE\", \"False\")):\n+        raise ValueError(\n+            \"This part uses `pickle.load` which is insecure and will execute arbitrary code that is potentially \"\n+            \"malicious. It's recommended to never unpickle data that could have come from an untrusted source, or \"\n+            \"that could have been tampered with. If you already verified the pickle data and decided to use it, \"\n+            \"you can set the environment variable `TRUST_REMOTE_CODE` to `True` to allow it.\"\n+        )\n     with open(trax_model_pkl_path, \"rb\") as f:\n         model_weights = pickle.load(f)[\"weights\"]\n "
      }
    ],
    "num_files": 7,
    "scraped_at": "2025-11-16T21:17:55.325378",
    "filter_decision": {
      "accept": false,
      "reasoning": "This PR is a repetitive find-and-replace pattern across multiple conversion scripts that adds the same security check before pickle.load calls. While the security concern is valid, the changes are mechanically identical across 8 files with no new logic, algorithms, or architectural decisions\u2014making it more of a systematic cleanup/hardening task than substantive code that would generate meaningful technical questions.",
      "substance_level": "low"
    }
  },
  {
    "pr_number": 41470,
    "title": "[kernels] Cleanup deta kernel",
    "body": "# What does this PR do?\r\n\r\nCleanup the `deta` kernel since it's the same as `deformable_detr` kernel cleaned here : https://github.com/huggingface/transformers/pull/36853\r\nAnd do the necessary modeling changes (even though the model is deprecated)",
    "html_url": "https://github.com/huggingface/transformers/pull/41470",
    "created_at": "2025-10-09T09:38:55Z",
    "merged_at": "2025-10-09T11:17:42Z",
    "merge_commit_sha": "927aa8bef2f29296a34840b3562f9c03cc45ef81",
    "base_ref": "main",
    "head_sha": "58d47f521610e06f00e246fa269b9e437edb0136",
    "user": "MekkCyber",
    "files": [
      {
        "filename": "src/transformers/kernels/deta/cpu/ms_deform_attn_cpu.cpp",
        "status": "removed",
        "additions": 0,
        "deletions": 40,
        "changes": 40,
        "patch": "@@ -1,40 +0,0 @@\n-/*!\n-**************************************************************************************************\n-* Deformable DETR\n-* Copyright (c) 2020 SenseTime. All Rights Reserved.\n-* Licensed under the Apache License, Version 2.0 [see LICENSE for details]\n-**************************************************************************************************\n-* Modified from https://github.com/chengdazhi/Deformable-Convolution-V2-PyTorch/tree/pytorch_1.0.0\n-**************************************************************************************************\n-*/\n-\n-#include <vector>\n-\n-#include <ATen/ATen.h>\n-#include <ATen/cuda/CUDAContext.h>\n-\n-\n-at::Tensor\n-ms_deform_attn_cpu_forward(\n-    const at::Tensor &value, \n-    const at::Tensor &spatial_shapes,\n-    const at::Tensor &level_start_index,\n-    const at::Tensor &sampling_loc,\n-    const at::Tensor &attn_weight,\n-    const int im2col_step)\n-{\n-    AT_ERROR(\"Not implement on cpu\");\n-}\n-\n-std::vector<at::Tensor>\n-ms_deform_attn_cpu_backward(\n-    const at::Tensor &value, \n-    const at::Tensor &spatial_shapes,\n-    const at::Tensor &level_start_index,\n-    const at::Tensor &sampling_loc,\n-    const at::Tensor &attn_weight,\n-    const at::Tensor &grad_output,\n-    const int im2col_step)\n-{\n-    AT_ERROR(\"Not implement on cpu\");\n-}"
      },
      {
        "filename": "src/transformers/kernels/deta/cpu/ms_deform_attn_cpu.h",
        "status": "removed",
        "additions": 0,
        "deletions": 32,
        "changes": 32,
        "patch": "@@ -1,32 +0,0 @@\n-/*!\n-**************************************************************************************************\n-* Deformable DETR\n-* Copyright (c) 2020 SenseTime. All Rights Reserved.\n-* Licensed under the Apache License, Version 2.0 [see LICENSE for details]\n-**************************************************************************************************\n-* Modified from https://github.com/chengdazhi/Deformable-Convolution-V2-PyTorch/tree/pytorch_1.0.0\n-**************************************************************************************************\n-*/\n-\n-#pragma once\n-#include <torch/extension.h>\n-\n-at::Tensor\n-ms_deform_attn_cpu_forward(\n-    const at::Tensor &value, \n-    const at::Tensor &spatial_shapes,\n-    const at::Tensor &level_start_index,\n-    const at::Tensor &sampling_loc,\n-    const at::Tensor &attn_weight,\n-    const int im2col_step);\n-\n-std::vector<at::Tensor>\n-ms_deform_attn_cpu_backward(\n-    const at::Tensor &value, \n-    const at::Tensor &spatial_shapes,\n-    const at::Tensor &level_start_index,\n-    const at::Tensor &sampling_loc,\n-    const at::Tensor &attn_weight,\n-    const at::Tensor &grad_output,\n-    const int im2col_step);\n-"
      },
      {
        "filename": "src/transformers/kernels/deta/cuda/ms_deform_attn_cuda.cu",
        "status": "removed",
        "additions": 0,
        "deletions": 156,
        "changes": 156,
        "patch": "@@ -1,156 +0,0 @@\n-/*!\n-**************************************************************************************************\n-* Deformable DETR\n-* Copyright (c) 2020 SenseTime. All Rights Reserved.\n-* Licensed under the Apache License, Version 2.0 [see LICENSE for details]\n-**************************************************************************************************\n-* Modified from https://github.com/chengdazhi/Deformable-Convolution-V2-PyTorch/tree/pytorch_1.0.0\n-**************************************************************************************************\n-*/\n-\n-#include <vector>\n-#include \"cuda/ms_deform_im2col_cuda.cuh\"\n-\n-#include <ATen/ATen.h>\n-#include <ATen/cuda/CUDAContext.h>\n-#include <cuda.h>\n-#include <cuda_runtime.h>\n-\n-#pragma once\n-#include <torch/extension.h>\n-\n-\n-at::Tensor ms_deform_attn_cuda_forward(\n-    const at::Tensor &value, \n-    const at::Tensor &spatial_shapes,\n-    const at::Tensor &level_start_index,\n-    const at::Tensor &sampling_loc,\n-    const at::Tensor &attn_weight,\n-    const int im2col_step)\n-{\n-    AT_ASSERTM(value.is_contiguous(), \"value tensor has to be contiguous\");\n-    AT_ASSERTM(spatial_shapes.is_contiguous(), \"spatial_shapes tensor has to be contiguous\");\n-    AT_ASSERTM(level_start_index.is_contiguous(), \"level_start_index tensor has to be contiguous\");\n-    AT_ASSERTM(sampling_loc.is_contiguous(), \"sampling_loc tensor has to be contiguous\");\n-    AT_ASSERTM(attn_weight.is_contiguous(), \"attn_weight tensor has to be contiguous\");\n-\n-    AT_ASSERTM(value.type().is_cuda(), \"value must be a CUDA tensor\");\n-    AT_ASSERTM(spatial_shapes.type().is_cuda(), \"spatial_shapes must be a CUDA tensor\");\n-    AT_ASSERTM(level_start_index.type().is_cuda(), \"level_start_index must be a CUDA tensor\");\n-    AT_ASSERTM(sampling_loc.type().is_cuda(), \"sampling_loc must be a CUDA tensor\");\n-    AT_ASSERTM(attn_weight.type().is_cuda(), \"attn_weight must be a CUDA tensor\");\n-\n-    const int batch = value.size(0);\n-    const int spatial_size = value.size(1);\n-    const int num_heads = value.size(2);\n-    const int channels = value.size(3);\n-\n-    const int num_levels = spatial_shapes.size(0);\n-\n-    const int num_query = sampling_loc.size(1);\n-    const int num_point = sampling_loc.size(4);\n-\n-    const int im2col_step_ = std::min(batch, im2col_step);\n-\n-    AT_ASSERTM(batch % im2col_step_ == 0, \"batch(%d) must divide im2col_step(%d)\", batch, im2col_step_);\n-    \n-    auto output = at::zeros({batch, num_query, num_heads, channels}, value.options());\n-\n-    const int batch_n = im2col_step_;\n-    auto output_n = output.view({batch/im2col_step_, batch_n, num_query, num_heads, channels});\n-    auto per_value_size = spatial_size * num_heads * channels;\n-    auto per_sample_loc_size = num_query * num_heads * num_levels * num_point * 2;\n-    auto per_attn_weight_size = num_query * num_heads * num_levels * num_point;\n-    for (int n = 0; n < batch/im2col_step_; ++n)\n-    {\n-        auto columns = output_n.select(0, n);\n-        AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n-            ms_deformable_im2col_cuda(at::cuda::getCurrentCUDAStream(),\n-                value.data<scalar_t>() + n * im2col_step_ * per_value_size,\n-                spatial_shapes.data<int64_t>(),\n-                level_start_index.data<int64_t>(),\n-                sampling_loc.data<scalar_t>() + n * im2col_step_ * per_sample_loc_size,\n-                attn_weight.data<scalar_t>() + n * im2col_step_ * per_attn_weight_size,\n-                batch_n, spatial_size, num_heads, channels, num_levels, num_query, num_point,\n-                columns.data<scalar_t>());\n-\n-        }));\n-    }\n-\n-    output = output.view({batch, num_query, num_heads*channels});\n-\n-    return output;\n-}\n-\n-\n-std::vector<at::Tensor> ms_deform_attn_cuda_backward(\n-    const at::Tensor &value, \n-    const at::Tensor &spatial_shapes,\n-    const at::Tensor &level_start_index,\n-    const at::Tensor &sampling_loc,\n-    const at::Tensor &attn_weight,\n-    const at::Tensor &grad_output,\n-    const int im2col_step)\n-{\n-\n-    AT_ASSERTM(value.is_contiguous(), \"value tensor has to be contiguous\");\n-    AT_ASSERTM(spatial_shapes.is_contiguous(), \"spatial_shapes tensor has to be contiguous\");\n-    AT_ASSERTM(level_start_index.is_contiguous(), \"level_start_index tensor has to be contiguous\");\n-    AT_ASSERTM(sampling_loc.is_contiguous(), \"sampling_loc tensor has to be contiguous\");\n-    AT_ASSERTM(attn_weight.is_contiguous(), \"attn_weight tensor has to be contiguous\");\n-    AT_ASSERTM(grad_output.is_contiguous(), \"grad_output tensor has to be contiguous\");\n-\n-    AT_ASSERTM(value.type().is_cuda(), \"value must be a CUDA tensor\");\n-    AT_ASSERTM(spatial_shapes.type().is_cuda(), \"spatial_shapes must be a CUDA tensor\");\n-    AT_ASSERTM(level_start_index.type().is_cuda(), \"level_start_index must be a CUDA tensor\");\n-    AT_ASSERTM(sampling_loc.type().is_cuda(), \"sampling_loc must be a CUDA tensor\");\n-    AT_ASSERTM(attn_weight.type().is_cuda(), \"attn_weight must be a CUDA tensor\");\n-    AT_ASSERTM(grad_output.type().is_cuda(), \"grad_output must be a CUDA tensor\");\n-\n-    const int batch = value.size(0);\n-    const int spatial_size = value.size(1);\n-    const int num_heads = value.size(2);\n-    const int channels = value.size(3);\n-\n-    const int num_levels = spatial_shapes.size(0);\n-\n-    const int num_query = sampling_loc.size(1);\n-    const int num_point = sampling_loc.size(4);\n-\n-    const int im2col_step_ = std::min(batch, im2col_step);\n-\n-    AT_ASSERTM(batch % im2col_step_ == 0, \"batch(%d) must divide im2col_step(%d)\", batch, im2col_step_);\n-\n-    auto grad_value = at::zeros_like(value);\n-    auto grad_sampling_loc = at::zeros_like(sampling_loc);\n-    auto grad_attn_weight = at::zeros_like(attn_weight);\n-\n-    const int batch_n = im2col_step_;\n-    auto per_value_size = spatial_size * num_heads * channels;\n-    auto per_sample_loc_size = num_query * num_heads * num_levels * num_point * 2;\n-    auto per_attn_weight_size = num_query * num_heads * num_levels * num_point;\n-    auto grad_output_n = grad_output.view({batch/im2col_step_, batch_n, num_query, num_heads, channels});\n-    \n-    for (int n = 0; n < batch/im2col_step_; ++n)\n-    {\n-        auto grad_output_g = grad_output_n.select(0, n);\n-        AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n-            ms_deformable_col2im_cuda(at::cuda::getCurrentCUDAStream(),\n-                                    grad_output_g.data<scalar_t>(),\n-                                    value.data<scalar_t>() + n * im2col_step_ * per_value_size,\n-                                    spatial_shapes.data<int64_t>(),\n-                                    level_start_index.data<int64_t>(),\n-                                    sampling_loc.data<scalar_t>() + n * im2col_step_ * per_sample_loc_size,\n-                                    attn_weight.data<scalar_t>() + n * im2col_step_ * per_attn_weight_size,\n-                                    batch_n, spatial_size, num_heads, channels, num_levels, num_query, num_point,\n-                                    grad_value.data<scalar_t>() +  n * im2col_step_ * per_value_size,\n-                                    grad_sampling_loc.data<scalar_t>() + n * im2col_step_ * per_sample_loc_size,\n-                                    grad_attn_weight.data<scalar_t>() + n * im2col_step_ * per_attn_weight_size);\n-\n-        }));\n-    }\n-\n-    return {\n-        grad_value, grad_sampling_loc, grad_attn_weight\n-    };\n-}"
      },
      {
        "filename": "src/transformers/kernels/deta/cuda/ms_deform_attn_cuda.cuh",
        "status": "removed",
        "additions": 0,
        "deletions": 1467,
        "changes": 1467,
        "patch": "@@ -1,1467 +0,0 @@\n-/*!\n-**************************************************************************************************\n-* Deformable DETR\n-* Copyright (c) 2020 SenseTime. All Rights Reserved.\n-* Licensed under the Apache License, Version 2.0 [see LICENSE for details]\n-**************************************************************************************************\n-* Modified from https://github.com/chengdazhi/Deformable-Convolution-V2-PyTorch/tree/pytorch_1.0.0\n-**************************************************************************************************\n-*/\n-\n-#include <vector>\n-\n-#include <cuda.h>\n-#include <cuda_runtime.h>\n-\n-#include <cstdio>\n-#include <algorithm>\n-#include <cstring>\n-\n-#include <ATen/ATen.h>\n-#include <ATen/cuda/CUDAContext.h>\n-\n-#include <THC/THCAtomics.cuh>\n-\n-#define CUDA_KERNEL_LOOP(i, n)                          \\\n-  for (int i = blockIdx.x * blockDim.x + threadIdx.x;   \\\n-      i < (n);                                          \\\n-      i += blockDim.x * gridDim.x)\n-\n-\n-at::Tensor ms_deform_attn_cuda_forward(\n-    const at::Tensor &value, \n-    const at::Tensor &spatial_shapes,\n-    const at::Tensor &level_start_index,\n-    const at::Tensor &sampling_loc,\n-    const at::Tensor &attn_weight,\n-    const int im2col_step)\n-{\n-    AT_ASSERTM(value.is_contiguous(), \"value tensor has to be contiguous\");\n-    AT_ASSERTM(spatial_shapes.is_contiguous(), \"spatial_shapes tensor has to be contiguous\");\n-    AT_ASSERTM(level_start_index.is_contiguous(), \"level_start_index tensor has to be contiguous\");\n-    AT_ASSERTM(sampling_loc.is_contiguous(), \"sampling_loc tensor has to be contiguous\");\n-    AT_ASSERTM(attn_weight.is_contiguous(), \"attn_weight tensor has to be contiguous\");\n-\n-    AT_ASSERTM(value.type().is_cuda(), \"value must be a CUDA tensor\");\n-    AT_ASSERTM(spatial_shapes.type().is_cuda(), \"spatial_shapes must be a CUDA tensor\");\n-    AT_ASSERTM(level_start_index.type().is_cuda(), \"level_start_index must be a CUDA tensor\");\n-    AT_ASSERTM(sampling_loc.type().is_cuda(), \"sampling_loc must be a CUDA tensor\");\n-    AT_ASSERTM(attn_weight.type().is_cuda(), \"attn_weight must be a CUDA tensor\");\n-\n-    const int batch = value.size(0);\n-    const int spatial_size = value.size(1);\n-    const int num_heads = value.size(2);\n-    const int channels = value.size(3);\n-\n-    const int num_levels = spatial_shapes.size(0);\n-\n-    const int num_query = sampling_loc.size(1);\n-    const int num_point = sampling_loc.size(4);\n-\n-    const int im2col_step_ = std::min(batch, im2col_step);\n-\n-    AT_ASSERTM(batch % im2col_step_ == 0, \"batch(%d) must divide im2col_step(%d)\", batch, im2col_step_);\n-    \n-    auto output = at::zeros({batch, num_query, num_heads, channels}, value.options());\n-\n-    const int batch_n = im2col_step_;\n-    auto output_n = output.view({batch/im2col_step_, batch_n, num_query, num_heads, channels});\n-    auto per_value_size = spatial_size * num_heads * channels;\n-    auto per_sample_loc_size = num_query * num_heads * num_levels * num_point * 2;\n-    auto per_attn_weight_size = num_query * num_heads * num_levels * num_point;\n-    for (int n = 0; n < batch/im2col_step_; ++n)\n-    {\n-        auto columns = output_n.select(0, n);\n-        AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n-            ms_deformable_im2col_cuda(at::cuda::getCurrentCUDAStream(),\n-                value.data<scalar_t>() + n * im2col_step_ * per_value_size,\n-                spatial_shapes.data<int64_t>(),\n-                level_start_index.data<int64_t>(),\n-                sampling_loc.data<scalar_t>() + n * im2col_step_ * per_sample_loc_size,\n-                attn_weight.data<scalar_t>() + n * im2col_step_ * per_attn_weight_size,\n-                batch_n, spatial_size, num_heads, channels, num_levels, num_query, num_point,\n-                columns.data<scalar_t>());\n-\n-        }));\n-    }\n-\n-    output = output.view({batch, num_query, num_heads*channels});\n-\n-    return output;\n-}\n-\n-\n-std::vector<at::Tensor> ms_deform_attn_cuda_backward(\n-    const at::Tensor &value, \n-    const at::Tensor &spatial_shapes,\n-    const at::Tensor &level_start_index,\n-    const at::Tensor &sampling_loc,\n-    const at::Tensor &attn_weight,\n-    const at::Tensor &grad_output,\n-    const int im2col_step)\n-{\n-\n-    AT_ASSERTM(value.is_contiguous(), \"value tensor has to be contiguous\");\n-    AT_ASSERTM(spatial_shapes.is_contiguous(), \"spatial_shapes tensor has to be contiguous\");\n-    AT_ASSERTM(level_start_index.is_contiguous(), \"level_start_index tensor has to be contiguous\");\n-    AT_ASSERTM(sampling_loc.is_contiguous(), \"sampling_loc tensor has to be contiguous\");\n-    AT_ASSERTM(attn_weight.is_contiguous(), \"attn_weight tensor has to be contiguous\");\n-    AT_ASSERTM(grad_output.is_contiguous(), \"grad_output tensor has to be contiguous\");\n-\n-    AT_ASSERTM(value.type().is_cuda(), \"value must be a CUDA tensor\");\n-    AT_ASSERTM(spatial_shapes.type().is_cuda(), \"spatial_shapes must be a CUDA tensor\");\n-    AT_ASSERTM(level_start_index.type().is_cuda(), \"level_start_index must be a CUDA tensor\");\n-    AT_ASSERTM(sampling_loc.type().is_cuda(), \"sampling_loc must be a CUDA tensor\");\n-    AT_ASSERTM(attn_weight.type().is_cuda(), \"attn_weight must be a CUDA tensor\");\n-    AT_ASSERTM(grad_output.type().is_cuda(), \"grad_output must be a CUDA tensor\");\n-\n-    const int batch = value.size(0);\n-    const int spatial_size = value.size(1);\n-    const int num_heads = value.size(2);\n-    const int channels = value.size(3);\n-\n-    const int num_levels = spatial_shapes.size(0);\n-\n-    const int num_query = sampling_loc.size(1);\n-    const int num_point = sampling_loc.size(4);\n-\n-    const int im2col_step_ = std::min(batch, im2col_step);\n-\n-    AT_ASSERTM(batch % im2col_step_ == 0, \"batch(%d) must divide im2col_step(%d)\", batch, im2col_step_);\n-\n-    auto grad_value = at::zeros_like(value);\n-    auto grad_sampling_loc = at::zeros_like(sampling_loc);\n-    auto grad_attn_weight = at::zeros_like(attn_weight);\n-\n-    const int batch_n = im2col_step_;\n-    auto per_value_size = spatial_size * num_heads * channels;\n-    auto per_sample_loc_size = num_query * num_heads * num_levels * num_point * 2;\n-    auto per_attn_weight_size = num_query * num_heads * num_levels * num_point;\n-    auto grad_output_n = grad_output.view({batch/im2col_step_, batch_n, num_query, num_heads, channels});\n-    \n-    for (int n = 0; n < batch/im2col_step_; ++n)\n-    {\n-        auto grad_output_g = grad_output_n.select(0, n);\n-        AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n-            ms_deformable_col2im_cuda(at::cuda::getCurrentCUDAStream(),\n-                                    grad_output_g.data<scalar_t>(),\n-                                    value.data<scalar_t>() + n * im2col_step_ * per_value_size,\n-                                    spatial_shapes.data<int64_t>(),\n-                                    level_start_index.data<int64_t>(),\n-                                    sampling_loc.data<scalar_t>() + n * im2col_step_ * per_sample_loc_size,\n-                                    attn_weight.data<scalar_t>() + n * im2col_step_ * per_attn_weight_size,\n-                                    batch_n, spatial_size, num_heads, channels, num_levels, num_query, num_point,\n-                                    grad_value.data<scalar_t>() +  n * im2col_step_ * per_value_size,\n-                                    grad_sampling_loc.data<scalar_t>() + n * im2col_step_ * per_sample_loc_size,\n-                                    grad_attn_weight.data<scalar_t>() + n * im2col_step_ * per_attn_weight_size);\n-\n-        }));\n-    }\n-\n-    return {\n-        grad_value, grad_sampling_loc, grad_attn_weight\n-    };\n-}\n-\n-const int CUDA_NUM_THREADS = 1024;\n-inline int GET_BLOCKS(const int N, const int num_threads)\n-{\n-  return (N + num_threads - 1) / num_threads;\n-}\n-\n-\n-template <typename scalar_t>\n-__device__ scalar_t ms_deform_attn_im2col_bilinear(const scalar_t* &bottom_data, \n-                                                   const int &height, const int &width, const int &nheads, const int &channels,\n-                                                   const scalar_t &h, const scalar_t &w, const int &m, const int &c)\n-{\n-  const int h_low = floor(h);\n-  const int w_low = floor(w);\n-  const int h_high = h_low + 1;\n-  const int w_high = w_low + 1;\n-\n-  const scalar_t lh = h - h_low;\n-  const scalar_t lw = w - w_low;\n-  const scalar_t hh = 1 - lh, hw = 1 - lw;\n-\n-  const int w_stride = nheads * channels;\n-  const int h_stride = width * w_stride;\n-  const int h_low_ptr_offset = h_low * h_stride;\n-  const int h_high_ptr_offset = h_low_ptr_offset + h_stride;\n-  const int w_low_ptr_offset = w_low * w_stride;\n-  const int w_high_ptr_offset = w_low_ptr_offset + w_stride;\n-  const int base_ptr = m * channels + c;\n-\n-  scalar_t v1 = 0;\n-  if (h_low >= 0 && w_low >= 0)\n-  {\n-    const int ptr1 = h_low_ptr_offset + w_low_ptr_offset + base_ptr;\n-    v1 = bottom_data[ptr1];\n-  }\n-  scalar_t v2 = 0;\n-  if (h_low >= 0 && w_high <= width - 1)\n-  {\n-    const int ptr2 = h_low_ptr_offset + w_high_ptr_offset + base_ptr;\n-    v2 = bottom_data[ptr2];\n-  }\n-  scalar_t v3 = 0;\n-  if (h_high <= height - 1 && w_low >= 0)\n-  {\n-    const int ptr3 = h_high_ptr_offset + w_low_ptr_offset + base_ptr;\n-    v3 = bottom_data[ptr3];\n-  }\n-  scalar_t v4 = 0;\n-  if (h_high <= height - 1 && w_high <= width - 1)\n-  {\n-    const int ptr4 = h_high_ptr_offset + w_high_ptr_offset + base_ptr;\n-    v4 = bottom_data[ptr4];\n-  }\n-\n-  const scalar_t w1 = hh * hw, w2 = hh * lw, w3 = lh * hw, w4 = lh * lw;\n-\n-  const scalar_t val = (w1 * v1 + w2 * v2 + w3 * v3 + w4 * v4);\n-  return val;\n-}\n-\n-\n-template <typename scalar_t>\n-__device__ void ms_deform_attn_col2im_bilinear(const scalar_t* &bottom_data, \n-                                                   const int &height, const int &width, const int &nheads, const int &channels,\n-                                                   const scalar_t &h, const scalar_t &w, const int &m, const int &c,\n-                                                   const scalar_t &top_grad,\n-                                                   const scalar_t &attn_weight,\n-                                                   scalar_t* &grad_value, \n-                                                   scalar_t* grad_sampling_loc,\n-                                                   scalar_t* grad_attn_weight)\n-{\n-  const int h_low = floor(h);\n-  const int w_low = floor(w);\n-  const int h_high = h_low + 1;\n-  const int w_high = w_low + 1;\n-\n-  const scalar_t lh = h - h_low;\n-  const scalar_t lw = w - w_low;\n-  const scalar_t hh = 1 - lh, hw = 1 - lw;\n-\n-  const int w_stride = nheads * channels;\n-  const int h_stride = width * w_stride;\n-  const int h_low_ptr_offset = h_low * h_stride;\n-  const int h_high_ptr_offset = h_low_ptr_offset + h_stride;\n-  const int w_low_ptr_offset = w_low * w_stride;\n-  const int w_high_ptr_offset = w_low_ptr_offset + w_stride;\n-  const int base_ptr = m * channels + c;\n-\n-  const scalar_t w1 = hh * hw, w2 = hh * lw, w3 = lh * hw, w4 = lh * lw;\n-  const scalar_t top_grad_value = top_grad * attn_weight;\n-  scalar_t grad_h_weight = 0, grad_w_weight = 0;\n-\n-  scalar_t v1 = 0;\n-  if (h_low >= 0 && w_low >= 0)\n-  {\n-    const int ptr1 = h_low_ptr_offset + w_low_ptr_offset + base_ptr;\n-    v1 = bottom_data[ptr1];\n-    grad_h_weight -= hw * v1;\n-    grad_w_weight -= hh * v1;\n-    atomicAdd(grad_value+ptr1, w1*top_grad_value);\n-  }\n-  scalar_t v2 = 0;\n-  if (h_low >= 0 && w_high <= width - 1)\n-  {\n-    const int ptr2 = h_low_ptr_offset + w_high_ptr_offset + base_ptr;\n-    v2 = bottom_data[ptr2];\n-    grad_h_weight -= lw * v2;\n-    grad_w_weight += hh * v2;\n-    atomicAdd(grad_value+ptr2, w2*top_grad_value);\n-  }\n-  scalar_t v3 = 0;\n-  if (h_high <= height - 1 && w_low >= 0)\n-  {\n-    const int ptr3 = h_high_ptr_offset + w_low_ptr_offset + base_ptr;\n-    v3 = bottom_data[ptr3];\n-    grad_h_weight += hw * v3;\n-    grad_w_weight -= lh * v3;\n-    atomicAdd(grad_value+ptr3, w3*top_grad_value); \n-  }\n-  scalar_t v4 = 0;\n-  if (h_high <= height - 1 && w_high <= width - 1)\n-  {\n-    const int ptr4 = h_high_ptr_offset + w_high_ptr_offset + base_ptr;\n-    v4 = bottom_data[ptr4];\n-    grad_h_weight += lw * v4;\n-    grad_w_weight += lh * v4;\n-    atomicAdd(grad_value+ptr4, w4*top_grad_value);\n-  }\n-\n-  const scalar_t val = (w1 * v1 + w2 * v2 + w3 * v3 + w4 * v4);\n-  *grad_attn_weight = top_grad * val;\n-  *grad_sampling_loc = width * grad_w_weight * top_grad_value;\n-  *(grad_sampling_loc + 1) = height * grad_h_weight * top_grad_value;\n-}\n-\n-\n-template <typename scalar_t>\n-__device__ void ms_deform_attn_col2im_bilinear_gm(const scalar_t* &bottom_data, \n-                                                   const int &height, const int &width, const int &nheads, const int &channels,\n-                                                   const scalar_t &h, const scalar_t &w, const int &m, const int &c,\n-                                                   const scalar_t &top_grad,\n-                                                   const scalar_t &attn_weight,\n-                                                   scalar_t* &grad_value, \n-                                                   scalar_t* grad_sampling_loc,\n-                                                   scalar_t* grad_attn_weight)\n-{\n-  const int h_low = floor(h);\n-  const int w_low = floor(w);\n-  const int h_high = h_low + 1;\n-  const int w_high = w_low + 1;\n-\n-  const scalar_t lh = h - h_low;\n-  const scalar_t lw = w - w_low;\n-  const scalar_t hh = 1 - lh, hw = 1 - lw;\n-\n-  const int w_stride = nheads * channels;\n-  const int h_stride = width * w_stride;\n-  const int h_low_ptr_offset = h_low * h_stride;\n-  const int h_high_ptr_offset = h_low_ptr_offset + h_stride;\n-  const int w_low_ptr_offset = w_low * w_stride;\n-  const int w_high_ptr_offset = w_low_ptr_offset + w_stride;\n-  const int base_ptr = m * channels + c;\n-\n-  const scalar_t w1 = hh * hw, w2 = hh * lw, w3 = lh * hw, w4 = lh * lw;\n-  const scalar_t top_grad_value = top_grad * attn_weight;\n-  scalar_t grad_h_weight = 0, grad_w_weight = 0;\n-\n-  scalar_t v1 = 0;\n-  if (h_low >= 0 && w_low >= 0)\n-  {\n-    const int ptr1 = h_low_ptr_offset + w_low_ptr_offset + base_ptr;\n-    v1 = bottom_data[ptr1];\n-    grad_h_weight -= hw * v1;\n-    grad_w_weight -= hh * v1;\n-    atomicAdd(grad_value+ptr1, w1*top_grad_value);\n-  }\n-  scalar_t v2 = 0;\n-  if (h_low >= 0 && w_high <= width - 1)\n-  {\n-    const int ptr2 = h_low_ptr_offset + w_high_ptr_offset + base_ptr;\n-    v2 = bottom_data[ptr2];\n-    grad_h_weight -= lw * v2;\n-    grad_w_weight += hh * v2;\n-    atomicAdd(grad_value+ptr2, w2*top_grad_value);\n-  }\n-  scalar_t v3 = 0;\n-  if (h_high <= height - 1 && w_low >= 0)\n-  {\n-    const int ptr3 = h_high_ptr_offset + w_low_ptr_offset + base_ptr;\n-    v3 = bottom_data[ptr3];\n-    grad_h_weight += hw * v3;\n-    grad_w_weight -= lh * v3;\n-    atomicAdd(grad_value+ptr3, w3*top_grad_value); \n-  }\n-  scalar_t v4 = 0;\n-  if (h_high <= height - 1 && w_high <= width - 1)\n-  {\n-    const int ptr4 = h_high_ptr_offset + w_high_ptr_offset + base_ptr;\n-    v4 = bottom_data[ptr4];\n-    grad_h_weight += lw * v4;\n-    grad_w_weight += lh * v4;\n-    atomicAdd(grad_value+ptr4, w4*top_grad_value);\n-  }\n-\n-  const scalar_t val = (w1 * v1 + w2 * v2 + w3 * v3 + w4 * v4);\n-  atomicAdd(grad_attn_weight, top_grad * val); \n-  atomicAdd(grad_sampling_loc, width * grad_w_weight * top_grad_value);\n-  atomicAdd(grad_sampling_loc + 1, height * grad_h_weight * top_grad_value);\n-}\n-\n-\n-template <typename scalar_t>\n-__global__ void ms_deformable_im2col_gpu_kernel(const int n,\n-                                                const scalar_t *data_value, \n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *data_col)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    scalar_t *data_col_ptr = data_col + index;\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-    scalar_t col = 0;\n-    \n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const scalar_t *data_value_ptr = data_value + (data_value_ptr_init_offset + level_start_id * qid_stride);\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          col += ms_deform_attn_im2col_bilinear(data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col) * weight;\n-        }\n-\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-      }\n-    }\n-    *data_col_ptr = col;\n-  }\n-}\n-\n-template <typename scalar_t, unsigned int blockSize>\n-__global__ void ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1(const int n,\n-                                                const scalar_t *grad_col,\n-                                                const scalar_t *data_value,\n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *grad_value,\n-                                                scalar_t *grad_sampling_loc,\n-                                                scalar_t *grad_attn_weight)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    __shared__ scalar_t cache_grad_sampling_loc[blockSize * 2];\n-    __shared__ scalar_t cache_grad_attn_weight[blockSize];\n-    unsigned int tid = threadIdx.x;\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    const scalar_t top_grad = grad_col[index];\n-\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int grad_sampling_ptr = data_weight_ptr;\n-    grad_sampling_loc += grad_sampling_ptr << 1;\n-    grad_attn_weight += grad_sampling_ptr;\n-    const int grad_weight_stride = 1;\n-    const int grad_loc_stride = 2;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-\n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const int value_ptr_offset = data_value_ptr_init_offset + level_start_id * qid_stride;\n-      const scalar_t *data_value_ptr = data_value + value_ptr_offset;\n-      scalar_t *grad_value_ptr = grad_value + value_ptr_offset;\n-\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-        *(cache_grad_sampling_loc+(threadIdx.x << 1)) = 0;\n-        *(cache_grad_sampling_loc+((threadIdx.x << 1) + 1)) = 0;\n-        *(cache_grad_attn_weight+threadIdx.x)=0;\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          ms_deform_attn_col2im_bilinear(\n-            data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col,\n-            top_grad, weight, grad_value_ptr, \n-            cache_grad_sampling_loc+(threadIdx.x << 1), cache_grad_attn_weight+threadIdx.x);\n-        }\n-        \n-        __syncthreads();\n-        if (tid == 0)\n-        {\n-          scalar_t _grad_w=cache_grad_sampling_loc[0], _grad_h=cache_grad_sampling_loc[1], _grad_a=cache_grad_attn_weight[0];\n-          int sid=2;\n-          for (unsigned int tid = 1; tid < blockSize; ++tid)\n-          {\n-            _grad_w += cache_grad_sampling_loc[sid];\n-            _grad_h += cache_grad_sampling_loc[sid + 1];\n-            _grad_a += cache_grad_attn_weight[tid];\n-            sid += 2;\n-          }\n-          \n-          \n-          *grad_sampling_loc = _grad_w;\n-          *(grad_sampling_loc + 1) = _grad_h;\n-          *grad_attn_weight = _grad_a;\n-        }\n-        __syncthreads();\n-\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-        grad_attn_weight += grad_weight_stride;\n-        grad_sampling_loc += grad_loc_stride;\n-      }\n-    }\n-  }\n-}\n-\n-\n-template <typename scalar_t, unsigned int blockSize>\n-__global__ void ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v2(const int n,\n-                                                const scalar_t *grad_col,\n-                                                const scalar_t *data_value,\n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *grad_value,\n-                                                scalar_t *grad_sampling_loc,\n-                                                scalar_t *grad_attn_weight)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    __shared__ scalar_t cache_grad_sampling_loc[blockSize * 2];\n-    __shared__ scalar_t cache_grad_attn_weight[blockSize];\n-    unsigned int tid = threadIdx.x;\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    const scalar_t top_grad = grad_col[index];\n-\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int grad_sampling_ptr = data_weight_ptr;\n-    grad_sampling_loc += grad_sampling_ptr << 1;\n-    grad_attn_weight += grad_sampling_ptr;\n-    const int grad_weight_stride = 1;\n-    const int grad_loc_stride = 2;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-\n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const int value_ptr_offset = data_value_ptr_init_offset + level_start_id * qid_stride;\n-      const scalar_t *data_value_ptr = data_value + value_ptr_offset;\n-      scalar_t *grad_value_ptr = grad_value + value_ptr_offset;\n-\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-        *(cache_grad_sampling_loc+(threadIdx.x << 1)) = 0;\n-        *(cache_grad_sampling_loc+((threadIdx.x << 1) + 1)) = 0;\n-        *(cache_grad_attn_weight+threadIdx.x)=0;\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          ms_deform_attn_col2im_bilinear(\n-            data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col,\n-            top_grad, weight, grad_value_ptr, \n-            cache_grad_sampling_loc+(threadIdx.x << 1), cache_grad_attn_weight+threadIdx.x);\n-        }\n-        \n-        __syncthreads();\n-\n-        for (unsigned int s=blockSize/2; s>0; s>>=1)\n-        {\n-          if (tid < s) {\n-            const unsigned int xid1 = tid << 1;\n-            const unsigned int xid2 = (tid + s) << 1;\n-            cache_grad_attn_weight[tid] += cache_grad_attn_weight[tid + s];\n-            cache_grad_sampling_loc[xid1] += cache_grad_sampling_loc[xid2];\n-            cache_grad_sampling_loc[xid1 + 1] += cache_grad_sampling_loc[xid2 + 1];\n-          }\n-          __syncthreads();\n-        }\n-\n-        if (tid == 0)\n-        { \n-          *grad_sampling_loc = cache_grad_sampling_loc[0];\n-          *(grad_sampling_loc + 1) = cache_grad_sampling_loc[1];\n-          *grad_attn_weight = cache_grad_attn_weight[0];\n-        }\n-        __syncthreads();\n-\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-        grad_attn_weight += grad_weight_stride;\n-        grad_sampling_loc += grad_loc_stride;\n-      }\n-    }\n-  }\n-}\n-\n-\n-template <typename scalar_t>\n-__global__ void ms_deformable_col2im_gpu_kernel_shm_reduce_v1(const int n,\n-                                                const scalar_t *grad_col,\n-                                                const scalar_t *data_value,\n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *grad_value,\n-                                                scalar_t *grad_sampling_loc,\n-                                                scalar_t *grad_attn_weight)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    extern __shared__ int _s[];\n-    scalar_t* cache_grad_sampling_loc = (scalar_t*)_s;\n-    scalar_t* cache_grad_attn_weight = cache_grad_sampling_loc + 2 * blockDim.x;\n-    unsigned int tid = threadIdx.x;\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    const scalar_t top_grad = grad_col[index];\n-\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int grad_sampling_ptr = data_weight_ptr;\n-    grad_sampling_loc += grad_sampling_ptr << 1;\n-    grad_attn_weight += grad_sampling_ptr;\n-    const int grad_weight_stride = 1;\n-    const int grad_loc_stride = 2;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-\n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const int value_ptr_offset = data_value_ptr_init_offset + level_start_id * qid_stride;\n-      const scalar_t *data_value_ptr = data_value + value_ptr_offset;\n-      scalar_t *grad_value_ptr = grad_value + value_ptr_offset;\n-\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-        *(cache_grad_sampling_loc+(threadIdx.x << 1)) = 0;\n-        *(cache_grad_sampling_loc+((threadIdx.x << 1) + 1)) = 0;\n-        *(cache_grad_attn_weight+threadIdx.x)=0;\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          ms_deform_attn_col2im_bilinear(\n-            data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col,\n-            top_grad, weight, grad_value_ptr, \n-            cache_grad_sampling_loc+(threadIdx.x << 1), cache_grad_attn_weight+threadIdx.x);\n-        }\n-        \n-        __syncthreads();\n-        if (tid == 0)\n-        {\n-          scalar_t _grad_w=cache_grad_sampling_loc[0], _grad_h=cache_grad_sampling_loc[1], _grad_a=cache_grad_attn_weight[0];\n-          int sid=2;\n-          for (unsigned int tid = 1; tid < blockDim.x; ++tid)\n-          {\n-            _grad_w += cache_grad_sampling_loc[sid];\n-            _grad_h += cache_grad_sampling_loc[sid + 1];\n-            _grad_a += cache_grad_attn_weight[tid];\n-            sid += 2;\n-          }\n-          \n-          \n-          *grad_sampling_loc = _grad_w;\n-          *(grad_sampling_loc + 1) = _grad_h;\n-          *grad_attn_weight = _grad_a;\n-        }\n-        __syncthreads();\n-\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-        grad_attn_weight += grad_weight_stride;\n-        grad_sampling_loc += grad_loc_stride;\n-      }\n-    }\n-  }\n-}\n-\n-template <typename scalar_t>\n-__global__ void ms_deformable_col2im_gpu_kernel_shm_reduce_v2(const int n,\n-                                                const scalar_t *grad_col,\n-                                                const scalar_t *data_value,\n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *grad_value,\n-                                                scalar_t *grad_sampling_loc,\n-                                                scalar_t *grad_attn_weight)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    extern __shared__ int _s[];\n-    scalar_t* cache_grad_sampling_loc = (scalar_t*)_s;\n-    scalar_t* cache_grad_attn_weight = cache_grad_sampling_loc + 2 * blockDim.x;\n-    unsigned int tid = threadIdx.x;\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    const scalar_t top_grad = grad_col[index];\n-\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int grad_sampling_ptr = data_weight_ptr;\n-    grad_sampling_loc += grad_sampling_ptr << 1;\n-    grad_attn_weight += grad_sampling_ptr;\n-    const int grad_weight_stride = 1;\n-    const int grad_loc_stride = 2;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-\n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const int value_ptr_offset = data_value_ptr_init_offset + level_start_id * qid_stride;\n-      const scalar_t *data_value_ptr = data_value + value_ptr_offset;\n-      scalar_t *grad_value_ptr = grad_value + value_ptr_offset;\n-\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-        *(cache_grad_sampling_loc+(threadIdx.x << 1)) = 0;\n-        *(cache_grad_sampling_loc+((threadIdx.x << 1) + 1)) = 0;\n-        *(cache_grad_attn_weight+threadIdx.x)=0;\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          ms_deform_attn_col2im_bilinear(\n-            data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col,\n-            top_grad, weight, grad_value_ptr, \n-            cache_grad_sampling_loc+(threadIdx.x << 1), cache_grad_attn_weight+threadIdx.x);\n-        }\n-        \n-        __syncthreads();\n-\n-        for (unsigned int s=blockDim.x/2, spre=blockDim.x; s>0; s>>=1, spre>>=1)\n-        {\n-          if (tid < s) {\n-            const unsigned int xid1 = tid << 1;\n-            const unsigned int xid2 = (tid + s) << 1;\n-            cache_grad_attn_weight[tid] += cache_grad_attn_weight[tid + s];\n-            cache_grad_sampling_loc[xid1] += cache_grad_sampling_loc[xid2];\n-            cache_grad_sampling_loc[xid1 + 1] += cache_grad_sampling_loc[xid2 + 1];\n-            if (tid + (s << 1) < spre)\n-            {\n-              cache_grad_attn_weight[tid] += cache_grad_attn_weight[tid + (s << 1)];\n-              cache_grad_sampling_loc[xid1] += cache_grad_sampling_loc[xid2 + (s << 1)];\n-              cache_grad_sampling_loc[xid1 + 1] += cache_grad_sampling_loc[xid2 + 1 + (s << 1)];\n-            } \n-          }\n-          __syncthreads();\n-        }\n-\n-        if (tid == 0)\n-        {\n-          *grad_sampling_loc = cache_grad_sampling_loc[0];\n-          *(grad_sampling_loc + 1) = cache_grad_sampling_loc[1];\n-          *grad_attn_weight = cache_grad_attn_weight[0];\n-        }\n-        __syncthreads();\n-\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-        grad_attn_weight += grad_weight_stride;\n-        grad_sampling_loc += grad_loc_stride;\n-      }\n-    }\n-  }\n-}\n-\n-template <typename scalar_t>\n-__global__ void ms_deformable_col2im_gpu_kernel_shm_reduce_v2_multi_blocks(const int n,\n-                                                const scalar_t *grad_col,\n-                                                const scalar_t *data_value,\n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *grad_value,\n-                                                scalar_t *grad_sampling_loc,\n-                                                scalar_t *grad_attn_weight)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    extern __shared__ int _s[];\n-    scalar_t* cache_grad_sampling_loc = (scalar_t*)_s;\n-    scalar_t* cache_grad_attn_weight = cache_grad_sampling_loc + 2 * blockDim.x;\n-    unsigned int tid = threadIdx.x;\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    const scalar_t top_grad = grad_col[index];\n-\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int grad_sampling_ptr = data_weight_ptr;\n-    grad_sampling_loc += grad_sampling_ptr << 1;\n-    grad_attn_weight += grad_sampling_ptr;\n-    const int grad_weight_stride = 1;\n-    const int grad_loc_stride = 2;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-\n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const int value_ptr_offset = data_value_ptr_init_offset + level_start_id * qid_stride;\n-      const scalar_t *data_value_ptr = data_value + value_ptr_offset;\n-      scalar_t *grad_value_ptr = grad_value + value_ptr_offset;\n-\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-        *(cache_grad_sampling_loc+(threadIdx.x << 1)) = 0;\n-        *(cache_grad_sampling_loc+((threadIdx.x << 1) + 1)) = 0;\n-        *(cache_grad_attn_weight+threadIdx.x)=0;\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          ms_deform_attn_col2im_bilinear(\n-            data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col,\n-            top_grad, weight, grad_value_ptr, \n-            cache_grad_sampling_loc+(threadIdx.x << 1), cache_grad_attn_weight+threadIdx.x);\n-        }\n-        \n-        __syncthreads();\n-\n-        for (unsigned int s=blockDim.x/2, spre=blockDim.x; s>0; s>>=1, spre>>=1)\n-        {\n-          if (tid < s) {\n-            const unsigned int xid1 = tid << 1;\n-            const unsigned int xid2 = (tid + s) << 1;\n-            cache_grad_attn_weight[tid] += cache_grad_attn_weight[tid + s];\n-            cache_grad_sampling_loc[xid1] += cache_grad_sampling_loc[xid2];\n-            cache_grad_sampling_loc[xid1 + 1] += cache_grad_sampling_loc[xid2 + 1];\n-            if (tid + (s << 1) < spre)\n-            {\n-              cache_grad_attn_weight[tid] += cache_grad_attn_weight[tid + (s << 1)];\n-              cache_grad_sampling_loc[xid1] += cache_grad_sampling_loc[xid2 + (s << 1)];\n-              cache_grad_sampling_loc[xid1 + 1] += cache_grad_sampling_loc[xid2 + 1 + (s << 1)];\n-            }\n-          }\n-          __syncthreads();\n-        }\n-\n-        if (tid == 0)\n-        {\n-          atomicAdd(grad_sampling_loc, cache_grad_sampling_loc[0]);\n-          atomicAdd(grad_sampling_loc + 1, cache_grad_sampling_loc[1]);\n-          atomicAdd(grad_attn_weight, cache_grad_attn_weight[0]);\n-        }\n-        __syncthreads();\n-\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-        grad_attn_weight += grad_weight_stride;\n-        grad_sampling_loc += grad_loc_stride;\n-      }\n-    }\n-  }\n-}\n-\n-\n-template <typename scalar_t>\n-__global__ void ms_deformable_col2im_gpu_kernel_gm(const int n,\n-                                                const scalar_t *grad_col,\n-                                                const scalar_t *data_value,\n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *grad_value,\n-                                                scalar_t *grad_sampling_loc,\n-                                                scalar_t *grad_attn_weight)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    const scalar_t top_grad = grad_col[index];\n-\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int grad_sampling_ptr = data_weight_ptr;\n-    grad_sampling_loc += grad_sampling_ptr << 1;\n-    grad_attn_weight += grad_sampling_ptr;\n-    const int grad_weight_stride = 1;\n-    const int grad_loc_stride = 2;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-\n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const int value_ptr_offset = data_value_ptr_init_offset + level_start_id * qid_stride;\n-      const scalar_t *data_value_ptr = data_value + value_ptr_offset;\n-      scalar_t *grad_value_ptr = grad_value + value_ptr_offset;\n-\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          ms_deform_attn_col2im_bilinear_gm(\n-            data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col,\n-            top_grad, weight, grad_value_ptr, \n-            grad_sampling_loc, grad_attn_weight);\n-        }\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-        grad_attn_weight += grad_weight_stride;\n-        grad_sampling_loc += grad_loc_stride;\n-      }\n-    }\n-  }\n-}\n-\n-\n-template <typename scalar_t>\n-void ms_deformable_im2col_cuda(cudaStream_t stream,\n-                              const scalar_t* data_value,\n-                              const int64_t* data_spatial_shapes, \n-                              const int64_t* data_level_start_index, \n-                              const scalar_t* data_sampling_loc,\n-                              const scalar_t* data_attn_weight,\n-                              const int batch_size,\n-                              const int spatial_size, \n-                              const int num_heads, \n-                              const int channels, \n-                              const int num_levels, \n-                              const int num_query,\n-                              const int num_point,\n-                              scalar_t* data_col)\n-{\n-  const int num_kernels = batch_size * num_query * num_heads * channels;\n-  const int num_actual_kernels = batch_size * num_query * num_heads * channels;\n-  const int num_threads = CUDA_NUM_THREADS;\n-  ms_deformable_im2col_gpu_kernel<scalar_t>\n-      <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-          0, stream>>>(\n-      num_kernels, data_value, data_spatial_shapes, data_level_start_index, data_sampling_loc, data_attn_weight, \n-      batch_size, spatial_size, num_heads, channels, num_levels, num_query, num_point, data_col);\n-  \n-  cudaError_t err = cudaGetLastError();\n-  if (err != cudaSuccess)\n-  {\n-    printf(\"error in ms_deformable_im2col_cuda: %s\\n\", cudaGetErrorString(err));\n-  }\n-\n-}\n-\n-template <typename scalar_t>\n-void ms_deformable_col2im_cuda(cudaStream_t stream,\n-                              const scalar_t* grad_col,\n-                              const scalar_t* data_value,\n-                              const int64_t * data_spatial_shapes,\n-                              const int64_t * data_level_start_index,\n-                              const scalar_t * data_sampling_loc,\n-                              const scalar_t * data_attn_weight,\n-                              const int batch_size, \n-                              const int spatial_size, \n-                              const int num_heads,\n-                              const int channels, \n-                              const int num_levels,\n-                              const int num_query,\n-                              const int num_point, \n-                              scalar_t* grad_value,\n-                              scalar_t* grad_sampling_loc,\n-                              scalar_t* grad_attn_weight)\n-{\n-  const int num_threads = (channels > CUDA_NUM_THREADS)?CUDA_NUM_THREADS:channels;\n-  const int num_kernels = batch_size * num_query * num_heads * channels;\n-  const int num_actual_kernels = batch_size * num_query * num_heads * channels;\n-  if (channels > 1024)\n-  {\n-    if ((channels & 1023) == 0)\n-    {\n-      ms_deformable_col2im_gpu_kernel_shm_reduce_v2_multi_blocks<scalar_t>\n-          <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-              num_threads*3*sizeof(scalar_t), stream>>>(\n-                        num_kernels, \n-                        grad_col,\n-                        data_value,\n-                        data_spatial_shapes,\n-                        data_level_start_index, \n-                        data_sampling_loc,\n-                        data_attn_weight,\n-                        batch_size, \n-                        spatial_size, \n-                        num_heads,\n-                        channels, \n-                        num_levels,\n-                        num_query,\n-                        num_point,\n-                        grad_value,\n-                        grad_sampling_loc,\n-                        grad_attn_weight);\n-    }\n-    else\n-    {\n-      ms_deformable_col2im_gpu_kernel_gm<scalar_t>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-    }\n-  }\n-  else{\n-    switch(channels)\n-    {\n-      case 1:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1<scalar_t, 1>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 2:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1<scalar_t, 2>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 4:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1<scalar_t, 4>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 8:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1<scalar_t, 8>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 16:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1<scalar_t, 16>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 32:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1<scalar_t, 32>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 64:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v2<scalar_t, 64>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 128:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v2<scalar_t, 128>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 256:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v2<scalar_t, 256>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 512:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v2<scalar_t, 512>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 1024:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v2<scalar_t, 1024>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      default:\n-        if (channels < 64)\n-        {\n-          ms_deformable_col2im_gpu_kernel_shm_reduce_v1<scalar_t>\n-          <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-              num_threads*3*sizeof(scalar_t), stream>>>(\n-                        num_kernels, \n-                        grad_col,\n-                        data_value,\n-                        data_spatial_shapes,\n-                        data_level_start_index, \n-                        data_sampling_loc,\n-                        data_attn_weight,\n-                        batch_size, \n-                        spatial_size, \n-                        num_heads,\n-                        channels, \n-                        num_levels,\n-                        num_query,\n-                        num_point,\n-                        grad_value,\n-                        grad_sampling_loc,\n-                        grad_attn_weight);\n-        }\n-        else\n-        {\n-          ms_deformable_col2im_gpu_kernel_shm_reduce_v2<scalar_t>\n-          <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-              num_threads*3*sizeof(scalar_t), stream>>>(\n-                        num_kernels, \n-                        grad_col,\n-                        data_value,\n-                        data_spatial_shapes,\n-                        data_level_start_index, \n-                        data_sampling_loc,\n-                        data_attn_weight,\n-                        batch_size, \n-                        spatial_size, \n-                        num_heads,\n-                        channels, \n-                        num_levels,\n-                        num_query,\n-                        num_point,\n-                        grad_value,\n-                        grad_sampling_loc,\n-                        grad_attn_weight);\n-        }\n-    }\n-  }\n-  cudaError_t err = cudaGetLastError();\n-  if (err != cudaSuccess)\n-  {\n-    printf(\"error in ms_deformable_col2im_cuda: %s\\n\", cudaGetErrorString(err));\n-  }\n-\n-}"
      },
      {
        "filename": "src/transformers/kernels/deta/cuda/ms_deform_attn_cuda.h",
        "status": "removed",
        "additions": 0,
        "deletions": 29,
        "changes": 29,
        "patch": "@@ -1,29 +0,0 @@\n-/*!\n-**************************************************************************************************\n-* Deformable DETR\n-* Copyright (c) 2020 SenseTime. All Rights Reserved.\n-* Licensed under the Apache License, Version 2.0 [see LICENSE for details]\n-**************************************************************************************************\n-* Modified from https://github.com/chengdazhi/Deformable-Convolution-V2-PyTorch/tree/pytorch_1.0.0\n-**************************************************************************************************\n-*/\n-\n-#pragma once\n-#include <torch/extension.h>\n-\n-at::Tensor ms_deform_attn_cuda_forward(\n-    const at::Tensor &value, \n-    const at::Tensor &spatial_shapes,\n-    const at::Tensor &level_start_index,\n-    const at::Tensor &sampling_loc,\n-    const at::Tensor &attn_weight,\n-    const int im2col_step);\n-\n-std::vector<at::Tensor> ms_deform_attn_cuda_backward(\n-    const at::Tensor &value, \n-    const at::Tensor &spatial_shapes,\n-    const at::Tensor &level_start_index,\n-    const at::Tensor &sampling_loc,\n-    const at::Tensor &attn_weight,\n-    const at::Tensor &grad_output,\n-    const int im2col_step);"
      },
      {
        "filename": "src/transformers/kernels/deta/cuda/ms_deform_im2col_cuda.cuh",
        "status": "removed",
        "additions": 0,
        "deletions": 1327,
        "changes": 1327,
        "patch": "@@ -1,1327 +0,0 @@\n-/*!\n-**************************************************************************\n-* Deformable DETR\n-* Copyright (c) 2020 SenseTime. All Rights Reserved.\n-* Licensed under the Apache License, Version 2.0 [see LICENSE for details]\n-**************************************************************************\n-* Modified from DCN (https://github.com/msracver/Deformable-ConvNets)\n-* Copyright (c) 2018 Microsoft\n-**************************************************************************\n-*/\n-\n-#include <cstdio>\n-#include <algorithm>\n-#include <cstring>\n-\n-#include <ATen/ATen.h>\n-#include <ATen/cuda/CUDAContext.h>\n-\n-#include <THC/THCAtomics.cuh>\n-\n-#define CUDA_KERNEL_LOOP(i, n)                          \\\n-  for (int i = blockIdx.x * blockDim.x + threadIdx.x;   \\\n-      i < (n);                                          \\\n-      i += blockDim.x * gridDim.x)\n-\n-const int CUDA_NUM_THREADS = 1024;\n-inline int GET_BLOCKS(const int N, const int num_threads)\n-{\n-  return (N + num_threads - 1) / num_threads;\n-}\n-\n-\n-template <typename scalar_t>\n-__device__ scalar_t ms_deform_attn_im2col_bilinear(const scalar_t* &bottom_data, \n-                                                   const int &height, const int &width, const int &nheads, const int &channels,\n-                                                   const scalar_t &h, const scalar_t &w, const int &m, const int &c)\n-{\n-  const int h_low = floor(h);\n-  const int w_low = floor(w);\n-  const int h_high = h_low + 1;\n-  const int w_high = w_low + 1;\n-\n-  const scalar_t lh = h - h_low;\n-  const scalar_t lw = w - w_low;\n-  const scalar_t hh = 1 - lh, hw = 1 - lw;\n-\n-  const int w_stride = nheads * channels;\n-  const int h_stride = width * w_stride;\n-  const int h_low_ptr_offset = h_low * h_stride;\n-  const int h_high_ptr_offset = h_low_ptr_offset + h_stride;\n-  const int w_low_ptr_offset = w_low * w_stride;\n-  const int w_high_ptr_offset = w_low_ptr_offset + w_stride;\n-  const int base_ptr = m * channels + c;\n-\n-  scalar_t v1 = 0;\n-  if (h_low >= 0 && w_low >= 0)\n-  {\n-    const int ptr1 = h_low_ptr_offset + w_low_ptr_offset + base_ptr;\n-    v1 = bottom_data[ptr1];\n-  }\n-  scalar_t v2 = 0;\n-  if (h_low >= 0 && w_high <= width - 1)\n-  {\n-    const int ptr2 = h_low_ptr_offset + w_high_ptr_offset + base_ptr;\n-    v2 = bottom_data[ptr2];\n-  }\n-  scalar_t v3 = 0;\n-  if (h_high <= height - 1 && w_low >= 0)\n-  {\n-    const int ptr3 = h_high_ptr_offset + w_low_ptr_offset + base_ptr;\n-    v3 = bottom_data[ptr3];\n-  }\n-  scalar_t v4 = 0;\n-  if (h_high <= height - 1 && w_high <= width - 1)\n-  {\n-    const int ptr4 = h_high_ptr_offset + w_high_ptr_offset + base_ptr;\n-    v4 = bottom_data[ptr4];\n-  }\n-\n-  const scalar_t w1 = hh * hw, w2 = hh * lw, w3 = lh * hw, w4 = lh * lw;\n-\n-  const scalar_t val = (w1 * v1 + w2 * v2 + w3 * v3 + w4 * v4);\n-  return val;\n-}\n-\n-\n-template <typename scalar_t>\n-__device__ void ms_deform_attn_col2im_bilinear(const scalar_t* &bottom_data, \n-                                                   const int &height, const int &width, const int &nheads, const int &channels,\n-                                                   const scalar_t &h, const scalar_t &w, const int &m, const int &c,\n-                                                   const scalar_t &top_grad,\n-                                                   const scalar_t &attn_weight,\n-                                                   scalar_t* &grad_value, \n-                                                   scalar_t* grad_sampling_loc,\n-                                                   scalar_t* grad_attn_weight)\n-{\n-  const int h_low = floor(h);\n-  const int w_low = floor(w);\n-  const int h_high = h_low + 1;\n-  const int w_high = w_low + 1;\n-\n-  const scalar_t lh = h - h_low;\n-  const scalar_t lw = w - w_low;\n-  const scalar_t hh = 1 - lh, hw = 1 - lw;\n-\n-  const int w_stride = nheads * channels;\n-  const int h_stride = width * w_stride;\n-  const int h_low_ptr_offset = h_low * h_stride;\n-  const int h_high_ptr_offset = h_low_ptr_offset + h_stride;\n-  const int w_low_ptr_offset = w_low * w_stride;\n-  const int w_high_ptr_offset = w_low_ptr_offset + w_stride;\n-  const int base_ptr = m * channels + c;\n-\n-  const scalar_t w1 = hh * hw, w2 = hh * lw, w3 = lh * hw, w4 = lh * lw;\n-  const scalar_t top_grad_value = top_grad * attn_weight;\n-  scalar_t grad_h_weight = 0, grad_w_weight = 0;\n-\n-  scalar_t v1 = 0;\n-  if (h_low >= 0 && w_low >= 0)\n-  {\n-    const int ptr1 = h_low_ptr_offset + w_low_ptr_offset + base_ptr;\n-    v1 = bottom_data[ptr1];\n-    grad_h_weight -= hw * v1;\n-    grad_w_weight -= hh * v1;\n-    atomicAdd(grad_value+ptr1, w1*top_grad_value);\n-  }\n-  scalar_t v2 = 0;\n-  if (h_low >= 0 && w_high <= width - 1)\n-  {\n-    const int ptr2 = h_low_ptr_offset + w_high_ptr_offset + base_ptr;\n-    v2 = bottom_data[ptr2];\n-    grad_h_weight -= lw * v2;\n-    grad_w_weight += hh * v2;\n-    atomicAdd(grad_value+ptr2, w2*top_grad_value);\n-  }\n-  scalar_t v3 = 0;\n-  if (h_high <= height - 1 && w_low >= 0)\n-  {\n-    const int ptr3 = h_high_ptr_offset + w_low_ptr_offset + base_ptr;\n-    v3 = bottom_data[ptr3];\n-    grad_h_weight += hw * v3;\n-    grad_w_weight -= lh * v3;\n-    atomicAdd(grad_value+ptr3, w3*top_grad_value); \n-  }\n-  scalar_t v4 = 0;\n-  if (h_high <= height - 1 && w_high <= width - 1)\n-  {\n-    const int ptr4 = h_high_ptr_offset + w_high_ptr_offset + base_ptr;\n-    v4 = bottom_data[ptr4];\n-    grad_h_weight += lw * v4;\n-    grad_w_weight += lh * v4;\n-    atomicAdd(grad_value+ptr4, w4*top_grad_value);\n-  }\n-\n-  const scalar_t val = (w1 * v1 + w2 * v2 + w3 * v3 + w4 * v4);\n-  *grad_attn_weight = top_grad * val;\n-  *grad_sampling_loc = width * grad_w_weight * top_grad_value;\n-  *(grad_sampling_loc + 1) = height * grad_h_weight * top_grad_value;\n-}\n-\n-\n-template <typename scalar_t>\n-__device__ void ms_deform_attn_col2im_bilinear_gm(const scalar_t* &bottom_data, \n-                                                   const int &height, const int &width, const int &nheads, const int &channels,\n-                                                   const scalar_t &h, const scalar_t &w, const int &m, const int &c,\n-                                                   const scalar_t &top_grad,\n-                                                   const scalar_t &attn_weight,\n-                                                   scalar_t* &grad_value, \n-                                                   scalar_t* grad_sampling_loc,\n-                                                   scalar_t* grad_attn_weight)\n-{\n-  const int h_low = floor(h);\n-  const int w_low = floor(w);\n-  const int h_high = h_low + 1;\n-  const int w_high = w_low + 1;\n-\n-  const scalar_t lh = h - h_low;\n-  const scalar_t lw = w - w_low;\n-  const scalar_t hh = 1 - lh, hw = 1 - lw;\n-\n-  const int w_stride = nheads * channels;\n-  const int h_stride = width * w_stride;\n-  const int h_low_ptr_offset = h_low * h_stride;\n-  const int h_high_ptr_offset = h_low_ptr_offset + h_stride;\n-  const int w_low_ptr_offset = w_low * w_stride;\n-  const int w_high_ptr_offset = w_low_ptr_offset + w_stride;\n-  const int base_ptr = m * channels + c;\n-\n-  const scalar_t w1 = hh * hw, w2 = hh * lw, w3 = lh * hw, w4 = lh * lw;\n-  const scalar_t top_grad_value = top_grad * attn_weight;\n-  scalar_t grad_h_weight = 0, grad_w_weight = 0;\n-\n-  scalar_t v1 = 0;\n-  if (h_low >= 0 && w_low >= 0)\n-  {\n-    const int ptr1 = h_low_ptr_offset + w_low_ptr_offset + base_ptr;\n-    v1 = bottom_data[ptr1];\n-    grad_h_weight -= hw * v1;\n-    grad_w_weight -= hh * v1;\n-    atomicAdd(grad_value+ptr1, w1*top_grad_value);\n-  }\n-  scalar_t v2 = 0;\n-  if (h_low >= 0 && w_high <= width - 1)\n-  {\n-    const int ptr2 = h_low_ptr_offset + w_high_ptr_offset + base_ptr;\n-    v2 = bottom_data[ptr2];\n-    grad_h_weight -= lw * v2;\n-    grad_w_weight += hh * v2;\n-    atomicAdd(grad_value+ptr2, w2*top_grad_value);\n-  }\n-  scalar_t v3 = 0;\n-  if (h_high <= height - 1 && w_low >= 0)\n-  {\n-    const int ptr3 = h_high_ptr_offset + w_low_ptr_offset + base_ptr;\n-    v3 = bottom_data[ptr3];\n-    grad_h_weight += hw * v3;\n-    grad_w_weight -= lh * v3;\n-    atomicAdd(grad_value+ptr3, w3*top_grad_value); \n-  }\n-  scalar_t v4 = 0;\n-  if (h_high <= height - 1 && w_high <= width - 1)\n-  {\n-    const int ptr4 = h_high_ptr_offset + w_high_ptr_offset + base_ptr;\n-    v4 = bottom_data[ptr4];\n-    grad_h_weight += lw * v4;\n-    grad_w_weight += lh * v4;\n-    atomicAdd(grad_value+ptr4, w4*top_grad_value);\n-  }\n-\n-  const scalar_t val = (w1 * v1 + w2 * v2 + w3 * v3 + w4 * v4);\n-  atomicAdd(grad_attn_weight, top_grad * val); \n-  atomicAdd(grad_sampling_loc, width * grad_w_weight * top_grad_value);\n-  atomicAdd(grad_sampling_loc + 1, height * grad_h_weight * top_grad_value);\n-}\n-\n-\n-template <typename scalar_t>\n-__global__ void ms_deformable_im2col_gpu_kernel(const int n,\n-                                                const scalar_t *data_value, \n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *data_col)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    scalar_t *data_col_ptr = data_col + index;\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-    scalar_t col = 0;\n-    \n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const scalar_t *data_value_ptr = data_value + (data_value_ptr_init_offset + level_start_id * qid_stride);\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          col += ms_deform_attn_im2col_bilinear(data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col) * weight;\n-        }\n-\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-      }\n-    }\n-    *data_col_ptr = col;\n-  }\n-}\n-\n-template <typename scalar_t, unsigned int blockSize>\n-__global__ void ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1(const int n,\n-                                                const scalar_t *grad_col,\n-                                                const scalar_t *data_value,\n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *grad_value,\n-                                                scalar_t *grad_sampling_loc,\n-                                                scalar_t *grad_attn_weight)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    __shared__ scalar_t cache_grad_sampling_loc[blockSize * 2];\n-    __shared__ scalar_t cache_grad_attn_weight[blockSize];\n-    unsigned int tid = threadIdx.x;\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    const scalar_t top_grad = grad_col[index];\n-\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int grad_sampling_ptr = data_weight_ptr;\n-    grad_sampling_loc += grad_sampling_ptr << 1;\n-    grad_attn_weight += grad_sampling_ptr;\n-    const int grad_weight_stride = 1;\n-    const int grad_loc_stride = 2;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-\n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const int value_ptr_offset = data_value_ptr_init_offset + level_start_id * qid_stride;\n-      const scalar_t *data_value_ptr = data_value + value_ptr_offset;\n-      scalar_t *grad_value_ptr = grad_value + value_ptr_offset;\n-\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-        *(cache_grad_sampling_loc+(threadIdx.x << 1)) = 0;\n-        *(cache_grad_sampling_loc+((threadIdx.x << 1) + 1)) = 0;\n-        *(cache_grad_attn_weight+threadIdx.x)=0;\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          ms_deform_attn_col2im_bilinear(\n-            data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col,\n-            top_grad, weight, grad_value_ptr, \n-            cache_grad_sampling_loc+(threadIdx.x << 1), cache_grad_attn_weight+threadIdx.x);\n-        }\n-        \n-        __syncthreads();\n-        if (tid == 0)\n-        {\n-          scalar_t _grad_w=cache_grad_sampling_loc[0], _grad_h=cache_grad_sampling_loc[1], _grad_a=cache_grad_attn_weight[0];\n-          int sid=2;\n-          for (unsigned int tid = 1; tid < blockSize; ++tid)\n-          {\n-            _grad_w += cache_grad_sampling_loc[sid];\n-            _grad_h += cache_grad_sampling_loc[sid + 1];\n-            _grad_a += cache_grad_attn_weight[tid];\n-            sid += 2;\n-          }\n-          \n-          \n-          *grad_sampling_loc = _grad_w;\n-          *(grad_sampling_loc + 1) = _grad_h;\n-          *grad_attn_weight = _grad_a;\n-        }\n-        __syncthreads();\n-\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-        grad_attn_weight += grad_weight_stride;\n-        grad_sampling_loc += grad_loc_stride;\n-      }\n-    }\n-  }\n-}\n-\n-\n-template <typename scalar_t, unsigned int blockSize>\n-__global__ void ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v2(const int n,\n-                                                const scalar_t *grad_col,\n-                                                const scalar_t *data_value,\n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *grad_value,\n-                                                scalar_t *grad_sampling_loc,\n-                                                scalar_t *grad_attn_weight)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    __shared__ scalar_t cache_grad_sampling_loc[blockSize * 2];\n-    __shared__ scalar_t cache_grad_attn_weight[blockSize];\n-    unsigned int tid = threadIdx.x;\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    const scalar_t top_grad = grad_col[index];\n-\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int grad_sampling_ptr = data_weight_ptr;\n-    grad_sampling_loc += grad_sampling_ptr << 1;\n-    grad_attn_weight += grad_sampling_ptr;\n-    const int grad_weight_stride = 1;\n-    const int grad_loc_stride = 2;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-\n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const int value_ptr_offset = data_value_ptr_init_offset + level_start_id * qid_stride;\n-      const scalar_t *data_value_ptr = data_value + value_ptr_offset;\n-      scalar_t *grad_value_ptr = grad_value + value_ptr_offset;\n-\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-        *(cache_grad_sampling_loc+(threadIdx.x << 1)) = 0;\n-        *(cache_grad_sampling_loc+((threadIdx.x << 1) + 1)) = 0;\n-        *(cache_grad_attn_weight+threadIdx.x)=0;\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          ms_deform_attn_col2im_bilinear(\n-            data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col,\n-            top_grad, weight, grad_value_ptr, \n-            cache_grad_sampling_loc+(threadIdx.x << 1), cache_grad_attn_weight+threadIdx.x);\n-        }\n-        \n-        __syncthreads();\n-\n-        for (unsigned int s=blockSize/2; s>0; s>>=1)\n-        {\n-          if (tid < s) {\n-            const unsigned int xid1 = tid << 1;\n-            const unsigned int xid2 = (tid + s) << 1;\n-            cache_grad_attn_weight[tid] += cache_grad_attn_weight[tid + s];\n-            cache_grad_sampling_loc[xid1] += cache_grad_sampling_loc[xid2];\n-            cache_grad_sampling_loc[xid1 + 1] += cache_grad_sampling_loc[xid2 + 1];\n-          }\n-          __syncthreads();\n-        }\n-\n-        if (tid == 0)\n-        { \n-          *grad_sampling_loc = cache_grad_sampling_loc[0];\n-          *(grad_sampling_loc + 1) = cache_grad_sampling_loc[1];\n-          *grad_attn_weight = cache_grad_attn_weight[0];\n-        }\n-        __syncthreads();\n-\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-        grad_attn_weight += grad_weight_stride;\n-        grad_sampling_loc += grad_loc_stride;\n-      }\n-    }\n-  }\n-}\n-\n-\n-template <typename scalar_t>\n-__global__ void ms_deformable_col2im_gpu_kernel_shm_reduce_v1(const int n,\n-                                                const scalar_t *grad_col,\n-                                                const scalar_t *data_value,\n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *grad_value,\n-                                                scalar_t *grad_sampling_loc,\n-                                                scalar_t *grad_attn_weight)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    extern __shared__ int _s[];\n-    scalar_t* cache_grad_sampling_loc = (scalar_t*)_s;\n-    scalar_t* cache_grad_attn_weight = cache_grad_sampling_loc + 2 * blockDim.x;\n-    unsigned int tid = threadIdx.x;\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    const scalar_t top_grad = grad_col[index];\n-\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int grad_sampling_ptr = data_weight_ptr;\n-    grad_sampling_loc += grad_sampling_ptr << 1;\n-    grad_attn_weight += grad_sampling_ptr;\n-    const int grad_weight_stride = 1;\n-    const int grad_loc_stride = 2;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-\n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const int value_ptr_offset = data_value_ptr_init_offset + level_start_id * qid_stride;\n-      const scalar_t *data_value_ptr = data_value + value_ptr_offset;\n-      scalar_t *grad_value_ptr = grad_value + value_ptr_offset;\n-\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-        *(cache_grad_sampling_loc+(threadIdx.x << 1)) = 0;\n-        *(cache_grad_sampling_loc+((threadIdx.x << 1) + 1)) = 0;\n-        *(cache_grad_attn_weight+threadIdx.x)=0;\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          ms_deform_attn_col2im_bilinear(\n-            data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col,\n-            top_grad, weight, grad_value_ptr, \n-            cache_grad_sampling_loc+(threadIdx.x << 1), cache_grad_attn_weight+threadIdx.x);\n-        }\n-        \n-        __syncthreads();\n-        if (tid == 0)\n-        {\n-          scalar_t _grad_w=cache_grad_sampling_loc[0], _grad_h=cache_grad_sampling_loc[1], _grad_a=cache_grad_attn_weight[0];\n-          int sid=2;\n-          for (unsigned int tid = 1; tid < blockDim.x; ++tid)\n-          {\n-            _grad_w += cache_grad_sampling_loc[sid];\n-            _grad_h += cache_grad_sampling_loc[sid + 1];\n-            _grad_a += cache_grad_attn_weight[tid];\n-            sid += 2;\n-          }\n-          \n-          \n-          *grad_sampling_loc = _grad_w;\n-          *(grad_sampling_loc + 1) = _grad_h;\n-          *grad_attn_weight = _grad_a;\n-        }\n-        __syncthreads();\n-\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-        grad_attn_weight += grad_weight_stride;\n-        grad_sampling_loc += grad_loc_stride;\n-      }\n-    }\n-  }\n-}\n-\n-template <typename scalar_t>\n-__global__ void ms_deformable_col2im_gpu_kernel_shm_reduce_v2(const int n,\n-                                                const scalar_t *grad_col,\n-                                                const scalar_t *data_value,\n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *grad_value,\n-                                                scalar_t *grad_sampling_loc,\n-                                                scalar_t *grad_attn_weight)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    extern __shared__ int _s[];\n-    scalar_t* cache_grad_sampling_loc = (scalar_t*)_s;\n-    scalar_t* cache_grad_attn_weight = cache_grad_sampling_loc + 2 * blockDim.x;\n-    unsigned int tid = threadIdx.x;\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    const scalar_t top_grad = grad_col[index];\n-\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int grad_sampling_ptr = data_weight_ptr;\n-    grad_sampling_loc += grad_sampling_ptr << 1;\n-    grad_attn_weight += grad_sampling_ptr;\n-    const int grad_weight_stride = 1;\n-    const int grad_loc_stride = 2;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-\n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const int value_ptr_offset = data_value_ptr_init_offset + level_start_id * qid_stride;\n-      const scalar_t *data_value_ptr = data_value + value_ptr_offset;\n-      scalar_t *grad_value_ptr = grad_value + value_ptr_offset;\n-\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-        *(cache_grad_sampling_loc+(threadIdx.x << 1)) = 0;\n-        *(cache_grad_sampling_loc+((threadIdx.x << 1) + 1)) = 0;\n-        *(cache_grad_attn_weight+threadIdx.x)=0;\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          ms_deform_attn_col2im_bilinear(\n-            data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col,\n-            top_grad, weight, grad_value_ptr, \n-            cache_grad_sampling_loc+(threadIdx.x << 1), cache_grad_attn_weight+threadIdx.x);\n-        }\n-        \n-        __syncthreads();\n-\n-        for (unsigned int s=blockDim.x/2, spre=blockDim.x; s>0; s>>=1, spre>>=1)\n-        {\n-          if (tid < s) {\n-            const unsigned int xid1 = tid << 1;\n-            const unsigned int xid2 = (tid + s) << 1;\n-            cache_grad_attn_weight[tid] += cache_grad_attn_weight[tid + s];\n-            cache_grad_sampling_loc[xid1] += cache_grad_sampling_loc[xid2];\n-            cache_grad_sampling_loc[xid1 + 1] += cache_grad_sampling_loc[xid2 + 1];\n-            if (tid + (s << 1) < spre)\n-            {\n-              cache_grad_attn_weight[tid] += cache_grad_attn_weight[tid + (s << 1)];\n-              cache_grad_sampling_loc[xid1] += cache_grad_sampling_loc[xid2 + (s << 1)];\n-              cache_grad_sampling_loc[xid1 + 1] += cache_grad_sampling_loc[xid2 + 1 + (s << 1)];\n-            } \n-          }\n-          __syncthreads();\n-        }\n-\n-        if (tid == 0)\n-        {\n-          *grad_sampling_loc = cache_grad_sampling_loc[0];\n-          *(grad_sampling_loc + 1) = cache_grad_sampling_loc[1];\n-          *grad_attn_weight = cache_grad_attn_weight[0];\n-        }\n-        __syncthreads();\n-\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-        grad_attn_weight += grad_weight_stride;\n-        grad_sampling_loc += grad_loc_stride;\n-      }\n-    }\n-  }\n-}\n-\n-template <typename scalar_t>\n-__global__ void ms_deformable_col2im_gpu_kernel_shm_reduce_v2_multi_blocks(const int n,\n-                                                const scalar_t *grad_col,\n-                                                const scalar_t *data_value,\n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *grad_value,\n-                                                scalar_t *grad_sampling_loc,\n-                                                scalar_t *grad_attn_weight)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    extern __shared__ int _s[];\n-    scalar_t* cache_grad_sampling_loc = (scalar_t*)_s;\n-    scalar_t* cache_grad_attn_weight = cache_grad_sampling_loc + 2 * blockDim.x;\n-    unsigned int tid = threadIdx.x;\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    const scalar_t top_grad = grad_col[index];\n-\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int grad_sampling_ptr = data_weight_ptr;\n-    grad_sampling_loc += grad_sampling_ptr << 1;\n-    grad_attn_weight += grad_sampling_ptr;\n-    const int grad_weight_stride = 1;\n-    const int grad_loc_stride = 2;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-\n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const int value_ptr_offset = data_value_ptr_init_offset + level_start_id * qid_stride;\n-      const scalar_t *data_value_ptr = data_value + value_ptr_offset;\n-      scalar_t *grad_value_ptr = grad_value + value_ptr_offset;\n-\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-        *(cache_grad_sampling_loc+(threadIdx.x << 1)) = 0;\n-        *(cache_grad_sampling_loc+((threadIdx.x << 1) + 1)) = 0;\n-        *(cache_grad_attn_weight+threadIdx.x)=0;\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          ms_deform_attn_col2im_bilinear(\n-            data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col,\n-            top_grad, weight, grad_value_ptr, \n-            cache_grad_sampling_loc+(threadIdx.x << 1), cache_grad_attn_weight+threadIdx.x);\n-        }\n-        \n-        __syncthreads();\n-\n-        for (unsigned int s=blockDim.x/2, spre=blockDim.x; s>0; s>>=1, spre>>=1)\n-        {\n-          if (tid < s) {\n-            const unsigned int xid1 = tid << 1;\n-            const unsigned int xid2 = (tid + s) << 1;\n-            cache_grad_attn_weight[tid] += cache_grad_attn_weight[tid + s];\n-            cache_grad_sampling_loc[xid1] += cache_grad_sampling_loc[xid2];\n-            cache_grad_sampling_loc[xid1 + 1] += cache_grad_sampling_loc[xid2 + 1];\n-            if (tid + (s << 1) < spre)\n-            {\n-              cache_grad_attn_weight[tid] += cache_grad_attn_weight[tid + (s << 1)];\n-              cache_grad_sampling_loc[xid1] += cache_grad_sampling_loc[xid2 + (s << 1)];\n-              cache_grad_sampling_loc[xid1 + 1] += cache_grad_sampling_loc[xid2 + 1 + (s << 1)];\n-            }\n-          }\n-          __syncthreads();\n-        }\n-\n-        if (tid == 0)\n-        {\n-          atomicAdd(grad_sampling_loc, cache_grad_sampling_loc[0]);\n-          atomicAdd(grad_sampling_loc + 1, cache_grad_sampling_loc[1]);\n-          atomicAdd(grad_attn_weight, cache_grad_attn_weight[0]);\n-        }\n-        __syncthreads();\n-\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-        grad_attn_weight += grad_weight_stride;\n-        grad_sampling_loc += grad_loc_stride;\n-      }\n-    }\n-  }\n-}\n-\n-\n-template <typename scalar_t>\n-__global__ void ms_deformable_col2im_gpu_kernel_gm(const int n,\n-                                                const scalar_t *grad_col,\n-                                                const scalar_t *data_value,\n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *grad_value,\n-                                                scalar_t *grad_sampling_loc,\n-                                                scalar_t *grad_attn_weight)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    const scalar_t top_grad = grad_col[index];\n-\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int grad_sampling_ptr = data_weight_ptr;\n-    grad_sampling_loc += grad_sampling_ptr << 1;\n-    grad_attn_weight += grad_sampling_ptr;\n-    const int grad_weight_stride = 1;\n-    const int grad_loc_stride = 2;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-\n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const int value_ptr_offset = data_value_ptr_init_offset + level_start_id * qid_stride;\n-      const scalar_t *data_value_ptr = data_value + value_ptr_offset;\n-      scalar_t *grad_value_ptr = grad_value + value_ptr_offset;\n-\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          ms_deform_attn_col2im_bilinear_gm(\n-            data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col,\n-            top_grad, weight, grad_value_ptr, \n-            grad_sampling_loc, grad_attn_weight);\n-        }\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-        grad_attn_weight += grad_weight_stride;\n-        grad_sampling_loc += grad_loc_stride;\n-      }\n-    }\n-  }\n-}\n-\n-\n-template <typename scalar_t>\n-void ms_deformable_im2col_cuda(cudaStream_t stream,\n-                              const scalar_t* data_value,\n-                              const int64_t* data_spatial_shapes, \n-                              const int64_t* data_level_start_index, \n-                              const scalar_t* data_sampling_loc,\n-                              const scalar_t* data_attn_weight,\n-                              const int batch_size,\n-                              const int spatial_size, \n-                              const int num_heads, \n-                              const int channels, \n-                              const int num_levels, \n-                              const int num_query,\n-                              const int num_point,\n-                              scalar_t* data_col)\n-{\n-  const int num_kernels = batch_size * num_query * num_heads * channels;\n-  const int num_actual_kernels = batch_size * num_query * num_heads * channels;\n-  const int num_threads = CUDA_NUM_THREADS;\n-  ms_deformable_im2col_gpu_kernel<scalar_t>\n-      <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-          0, stream>>>(\n-      num_kernels, data_value, data_spatial_shapes, data_level_start_index, data_sampling_loc, data_attn_weight, \n-      batch_size, spatial_size, num_heads, channels, num_levels, num_query, num_point, data_col);\n-  \n-  cudaError_t err = cudaGetLastError();\n-  if (err != cudaSuccess)\n-  {\n-    printf(\"error in ms_deformable_im2col_cuda: %s\\n\", cudaGetErrorString(err));\n-  }\n-\n-}\n-\n-template <typename scalar_t>\n-void ms_deformable_col2im_cuda(cudaStream_t stream,\n-                              const scalar_t* grad_col,\n-                              const scalar_t* data_value,\n-                              const int64_t * data_spatial_shapes,\n-                              const int64_t * data_level_start_index,\n-                              const scalar_t * data_sampling_loc,\n-                              const scalar_t * data_attn_weight,\n-                              const int batch_size, \n-                              const int spatial_size, \n-                              const int num_heads,\n-                              const int channels, \n-                              const int num_levels,\n-                              const int num_query,\n-                              const int num_point, \n-                              scalar_t* grad_value,\n-                              scalar_t* grad_sampling_loc,\n-                              scalar_t* grad_attn_weight)\n-{\n-  const int num_threads = (channels > CUDA_NUM_THREADS)?CUDA_NUM_THREADS:channels;\n-  const int num_kernels = batch_size * num_query * num_heads * channels;\n-  const int num_actual_kernels = batch_size * num_query * num_heads * channels;\n-  if (channels > 1024)\n-  {\n-    if ((channels & 1023) == 0)\n-    {\n-      ms_deformable_col2im_gpu_kernel_shm_reduce_v2_multi_blocks<scalar_t>\n-          <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-              num_threads*3*sizeof(scalar_t), stream>>>(\n-                        num_kernels, \n-                        grad_col,\n-                        data_value,\n-                        data_spatial_shapes,\n-                        data_level_start_index, \n-                        data_sampling_loc,\n-                        data_attn_weight,\n-                        batch_size, \n-                        spatial_size, \n-                        num_heads,\n-                        channels, \n-                        num_levels,\n-                        num_query,\n-                        num_point,\n-                        grad_value,\n-                        grad_sampling_loc,\n-                        grad_attn_weight);\n-    }\n-    else\n-    {\n-      ms_deformable_col2im_gpu_kernel_gm<scalar_t>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-    }\n-  }\n-  else{\n-    switch(channels)\n-    {\n-      case 1:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1<scalar_t, 1>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 2:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1<scalar_t, 2>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 4:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1<scalar_t, 4>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 8:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1<scalar_t, 8>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 16:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1<scalar_t, 16>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 32:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1<scalar_t, 32>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 64:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v2<scalar_t, 64>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 128:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v2<scalar_t, 128>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 256:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v2<scalar_t, 256>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 512:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v2<scalar_t, 512>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 1024:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v2<scalar_t, 1024>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      default:\n-        if (channels < 64)\n-        {\n-          ms_deformable_col2im_gpu_kernel_shm_reduce_v1<scalar_t>\n-          <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-              num_threads*3*sizeof(scalar_t), stream>>>(\n-                        num_kernels, \n-                        grad_col,\n-                        data_value,\n-                        data_spatial_shapes,\n-                        data_level_start_index, \n-                        data_sampling_loc,\n-                        data_attn_weight,\n-                        batch_size, \n-                        spatial_size, \n-                        num_heads,\n-                        channels, \n-                        num_levels,\n-                        num_query,\n-                        num_point,\n-                        grad_value,\n-                        grad_sampling_loc,\n-                        grad_attn_weight);\n-        }\n-        else\n-        {\n-          ms_deformable_col2im_gpu_kernel_shm_reduce_v2<scalar_t>\n-          <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-              num_threads*3*sizeof(scalar_t), stream>>>(\n-                        num_kernels, \n-                        grad_col,\n-                        data_value,\n-                        data_spatial_shapes,\n-                        data_level_start_index, \n-                        data_sampling_loc,\n-                        data_attn_weight,\n-                        batch_size, \n-                        spatial_size, \n-                        num_heads,\n-                        channels, \n-                        num_levels,\n-                        num_query,\n-                        num_point,\n-                        grad_value,\n-                        grad_sampling_loc,\n-                        grad_attn_weight);\n-        }\n-    }\n-  }\n-  cudaError_t err = cudaGetLastError();\n-  if (err != cudaSuccess)\n-  {\n-    printf(\"error in ms_deformable_col2im_cuda: %s\\n\", cudaGetErrorString(err));\n-  }\n-\n-}"
      },
      {
        "filename": "src/transformers/kernels/deta/ms_deform_attn.h",
        "status": "removed",
        "additions": 0,
        "deletions": 61,
        "changes": 61,
        "patch": "@@ -1,61 +0,0 @@\n-/*!\n-**************************************************************************************************\n-* Deformable DETR\n-* Copyright (c) 2020 SenseTime. All Rights Reserved.\n-* Licensed under the Apache License, Version 2.0 [see LICENSE for details]\n-**************************************************************************************************\n-* Modified from https://github.com/chengdazhi/Deformable-Convolution-V2-PyTorch/tree/pytorch_1.0.0\n-**************************************************************************************************\n-*/\n-\n-#pragma once\n-\n-#include \"cpu/ms_deform_attn_cpu.h\"\n-\n-#ifdef WITH_CUDA\n-#include \"cuda/ms_deform_attn_cuda.h\"\n-#endif\n-\n-\n-at::Tensor\n-ms_deform_attn_forward(\n-    const at::Tensor &value, \n-    const at::Tensor &spatial_shapes,\n-    const at::Tensor &level_start_index,\n-    const at::Tensor &sampling_loc,\n-    const at::Tensor &attn_weight,\n-    const int im2col_step)\n-{\n-    if (value.type().is_cuda())\n-    {\n-#ifdef WITH_CUDA\n-        return ms_deform_attn_cuda_forward(\n-            value, spatial_shapes, level_start_index, sampling_loc, attn_weight, im2col_step);\n-#else\n-        AT_ERROR(\"Not compiled with GPU support\");\n-#endif\n-    }\n-    AT_ERROR(\"Not implemented on the CPU\");\n-}\n-\n-std::vector<at::Tensor>\n-ms_deform_attn_backward(\n-    const at::Tensor &value, \n-    const at::Tensor &spatial_shapes,\n-    const at::Tensor &level_start_index,\n-    const at::Tensor &sampling_loc,\n-    const at::Tensor &attn_weight,\n-    const at::Tensor &grad_output,\n-    const int im2col_step)\n-{\n-    if (value.type().is_cuda())\n-    {\n-#ifdef WITH_CUDA\n-        return ms_deform_attn_cuda_backward(\n-            value, spatial_shapes, level_start_index, sampling_loc, attn_weight, grad_output, im2col_step);\n-#else\n-        AT_ERROR(\"Not compiled with GPU support\");\n-#endif\n-    }\n-    AT_ERROR(\"Not implemented on the CPU\");\n-}"
      },
      {
        "filename": "src/transformers/kernels/deta/vision.cpp",
        "status": "removed",
        "additions": 0,
        "deletions": 16,
        "changes": 16,
        "patch": "@@ -1,16 +0,0 @@\n-/*!\n-**************************************************************************************************\n-* Deformable DETR\n-* Copyright (c) 2020 SenseTime. All Rights Reserved.\n-* Licensed under the Apache License, Version 2.0 [see LICENSE for details]\n-**************************************************************************************************\n-* Modified from https://github.com/chengdazhi/Deformable-Convolution-V2-PyTorch/tree/pytorch_1.0.0\n-**************************************************************************************************\n-*/\n-\n-#include \"ms_deform_attn.h\"\n-\n-PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n-  m.def(\"ms_deform_attn_forward\", &ms_deform_attn_forward, \"ms_deform_attn_forward\");\n-  m.def(\"ms_deform_attn_backward\", &ms_deform_attn_backward, \"ms_deform_attn_backward\");\n-}\n\\ No newline at end of file"
      },
      {
        "filename": "src/transformers/models/deprecated/deta/modeling_deta.py",
        "status": "modified",
        "additions": 59,
        "deletions": 103,
        "changes": 162,
        "patch": "@@ -16,119 +16,89 @@\n \n import copy\n import math\n-import os\n import warnings\n from dataclasses import dataclass\n-from pathlib import Path\n from typing import Optional, Union\n \n import torch\n import torch.nn.functional as F\n from torch import Tensor, nn\n-from torch.autograd import Function\n-from torch.autograd.function import once_differentiable\n \n from ....activations import ACT2FN\n from ....file_utils import (\n     ModelOutput,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     is_scipy_available,\n-    is_torch_cuda_available,\n     is_vision_available,\n     replace_return_docstrings,\n )\n+from ....integrations.hub_kernels import use_kernel_forward_from_hub\n from ....modeling_attn_mask_utils import _prepare_4d_attention_mask\n from ....modeling_layers import GradientCheckpointingLayer\n from ....modeling_outputs import BaseModelOutput\n from ....modeling_utils import PreTrainedModel\n from ....pytorch_utils import meshgrid\n-from ....utils import is_accelerate_available, is_ninja_available, is_torchvision_available, logging, requires_backends\n+from ....utils import is_accelerate_available, is_torchvision_available, logging, requires_backends\n from ....utils.backbone_utils import load_backbone\n from .configuration_deta import DetaConfig\n \n \n logger = logging.get_logger(__name__)\n \n-MultiScaleDeformableAttention = None\n \n-\n-def load_cuda_kernels():\n-    from torch.utils.cpp_extension import load\n-\n-    global MultiScaleDeformableAttention\n-\n-    root = Path(__file__).resolve().parent.parent.parent.parent / \"kernels\" / \"deta\"\n-    src_files = [\n-        root / filename\n-        for filename in [\n-            \"vision.cpp\",\n-            os.path.join(\"cpu\", \"ms_deform_attn_cpu.cpp\"),\n-            os.path.join(\"cuda\", \"ms_deform_attn_cuda.cu\"),\n-        ]\n-    ]\n-\n-    MultiScaleDeformableAttention = load(\n-        \"MultiScaleDeformableAttention\",\n-        src_files,\n-        with_cuda=True,\n-        extra_include_paths=[str(root)],\n-        extra_cflags=[\"-DWITH_CUDA=1\"],\n-        extra_cuda_cflags=[\n-            \"-DCUDA_HAS_FP16=1\",\n-            \"-D__CUDA_NO_HALF_OPERATORS__\",\n-            \"-D__CUDA_NO_HALF_CONVERSIONS__\",\n-            \"-D__CUDA_NO_HALF2_OPERATORS__\",\n-        ],\n-    )\n-\n-\n-class MultiScaleDeformableAttentionFunction(Function):\n-    @staticmethod\n+@use_kernel_forward_from_hub(\"MultiScaleDeformableAttention\")\n+class MultiScaleDeformableAttention(nn.Module):\n     def forward(\n-        context,\n-        value,\n-        value_spatial_shapes,\n-        value_level_start_index,\n-        sampling_locations,\n-        attention_weights,\n-        im2col_step,\n+        self,\n+        value: Tensor,\n+        value_spatial_shapes: Tensor,\n+        level_start_index: Tensor,\n+        sampling_locations: Tensor,\n+        attention_weights: Tensor,\n+        im2col_step: int,\n     ):\n-        context.im2col_step = im2col_step\n-        output = MultiScaleDeformableAttention.ms_deform_attn_forward(\n-            value,\n-            value_spatial_shapes,\n-            value_level_start_index,\n-            sampling_locations,\n-            attention_weights,\n-            context.im2col_step,\n-        )\n-        context.save_for_backward(\n-            value, value_spatial_shapes, value_level_start_index, sampling_locations, attention_weights\n+        batch_size, _, num_heads, hidden_dim = value.shape\n+        _, num_queries, num_heads, num_levels, num_points, _ = sampling_locations.shape\n+        value_list = value.split([height * width for height, width in value_spatial_shapes], dim=1)\n+        sampling_grids = 2 * sampling_locations - 1\n+        sampling_value_list = []\n+        for level_id, (height, width) in enumerate(value_spatial_shapes):\n+            # batch_size, height*width, num_heads, hidden_dim\n+            # -> batch_size, height*width, num_heads*hidden_dim\n+            # -> batch_size, num_heads*hidden_dim, height*width\n+            # -> batch_size*num_heads, hidden_dim, height, width\n+            value_l_ = (\n+                value_list[level_id]\n+                .flatten(2)\n+                .transpose(1, 2)\n+                .reshape(batch_size * num_heads, hidden_dim, height, width)\n+            )\n+            # batch_size, num_queries, num_heads, num_points, 2\n+            # -> batch_size, num_heads, num_queries, num_points, 2\n+            # -> batch_size*num_heads, num_queries, num_points, 2\n+            sampling_grid_l_ = sampling_grids[:, :, :, level_id].transpose(1, 2).flatten(0, 1)\n+            # batch_size*num_heads, hidden_dim, num_queries, num_points\n+            sampling_value_l_ = nn.functional.grid_sample(\n+                value_l_,\n+                sampling_grid_l_,\n+                mode=\"bilinear\",\n+                padding_mode=\"zeros\",\n+                align_corners=False,\n+            )\n+            sampling_value_list.append(sampling_value_l_)\n+        # (batch_size, num_queries, num_heads, num_levels, num_points)\n+        # -> (batch_size, num_heads, num_queries, num_levels, num_points)\n+        # -> (batch_size, num_heads, 1, num_queries, num_levels*num_points)\n+        attention_weights = attention_weights.transpose(1, 2).reshape(\n+            batch_size * num_heads, 1, num_queries, num_levels * num_points\n         )\n-        return output\n-\n-    @staticmethod\n-    @once_differentiable\n-    def backward(context, grad_output):\n-        (\n-            value,\n-            value_spatial_shapes,\n-            value_level_start_index,\n-            sampling_locations,\n-            attention_weights,\n-        ) = context.saved_tensors\n-        grad_value, grad_sampling_loc, grad_attn_weight = MultiScaleDeformableAttention.ms_deform_attn_backward(\n-            value,\n-            value_spatial_shapes,\n-            value_level_start_index,\n-            sampling_locations,\n-            attention_weights,\n-            grad_output,\n-            context.im2col_step,\n+        output = (\n+            (torch.stack(sampling_value_list, dim=-2).flatten(-2) * attention_weights)\n+            .sum(-1)\n+            .view(batch_size, num_heads * hidden_dim, num_queries)\n         )\n-\n-        return grad_value, None, None, grad_sampling_loc, grad_attn_weight, None\n+        return output.transpose(1, 2).contiguous()\n \n \n if is_accelerate_available():\n@@ -571,12 +541,7 @@ class DetaMultiscaleDeformableAttention(nn.Module):\n     def __init__(self, config: DetaConfig, num_heads: int, n_points: int):\n         super().__init__()\n \n-        kernel_loaded = MultiScaleDeformableAttention is not None\n-        if is_torch_cuda_available() and is_ninja_available() and not kernel_loaded:\n-            try:\n-                load_cuda_kernels()\n-            except Exception as e:\n-                logger.warning(f\"Could not load the custom kernel for multi-scale deformable attention: {e}\")\n+        self.attn = MultiScaleDeformableAttention()\n \n         if config.d_model % num_heads != 0:\n             raise ValueError(\n@@ -684,23 +649,14 @@ def forward(\n         else:\n             raise ValueError(f\"Last dim of reference_points must be 2 or 4, but got {reference_points.shape[-1]}\")\n \n-        if self.disable_custom_kernels:\n-            # PyTorch implementation\n-            output = multi_scale_deformable_attention(value, spatial_shapes, sampling_locations, attention_weights)\n-        else:\n-            try:\n-                # custom kernel\n-                output = MultiScaleDeformableAttentionFunction.apply(\n-                    value,\n-                    spatial_shapes,\n-                    level_start_index,\n-                    sampling_locations,\n-                    attention_weights,\n-                    self.im2col_step,\n-                )\n-            except Exception:\n-                # PyTorch implementation\n-                output = multi_scale_deformable_attention(value, spatial_shapes, sampling_locations, attention_weights)\n+        output = self.attn(\n+            value,\n+            spatial_shapes,\n+            level_start_index,\n+            sampling_locations,\n+            attention_weights,\n+            self.im2col_step,\n+        )\n         output = self.output_proj(output)\n \n         return output, attention_weights"
      }
    ],
    "num_files": 9,
    "scraped_at": "2025-11-16T21:17:56.707387",
    "filter_decision": {
      "accept": false,
      "reasoning": "This PR is primarily a cleanup/deletion of deprecated kernel code that is being removed because it duplicates functionality from another kernel. While there are modeling changes mentioned, the overwhelming majority of the diff consists of removing old C++/CUDA code files without replacing them with new logic. The changes don't introduce new functionality, algorithms, or architectural decisions\u2014they simplify by removing redundant code and updating imports.",
      "substance_level": "low"
    }
  }
]