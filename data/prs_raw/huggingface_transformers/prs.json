[
  {
    "pr_number": 41997,
    "title": "Fix issue with from pretrained and kwargs in image processors",
    "body": "# What does this PR do?\r\nFixes https://github.com/huggingface/transformers/issues/41955.\r\nFixes an issue raised in https://github.com/huggingface/transformers/pull/41954. Instead of setting attributes from kwargs after instantiating the image processor in `from_pretrained`, we update the image processor dict with the kwargs before instantiating the object. This allows custom logic in the init to take into account the custom kwargs passed to `from_pretrained`.\r\n\r\nIn the PR linked, the issue was that `max_pixels` is supposed to overwrite `size[\"longest_edge\"] `when passed to the init, but in `from_pretrained`, `max_pixels` was never passed to the init and only set as an attribute after instantiating the image processor.",
    "html_url": "https://github.com/huggingface/transformers/pull/41997",
    "created_at": "2025-11-03T15:57:28Z",
    "merged_at": "2025-11-04T15:35:39Z",
    "merge_commit_sha": "900cf9d33bc091f3e47f8e598cba464f8b93bdd7",
    "base_ref": "main",
    "head_sha": "96a2a7085096b928c72a8d07652180c1c913fef3",
    "user": "yonigozlan",
    "files": [
      {
        "filename": "src/transformers/image_processing_base.py",
        "status": "modified",
        "additions": 4,
        "deletions": 16,
        "changes": 20,
        "patch": "@@ -362,25 +362,13 @@ def from_dict(cls, image_processor_dict: dict[str, Any], **kwargs):\n         \"\"\"\n         image_processor_dict = image_processor_dict.copy()\n         return_unused_kwargs = kwargs.pop(\"return_unused_kwargs\", False)\n-\n-        # The `size` parameter is a dict and was previously an int or tuple in feature extractors.\n-        # We set `size` here directly to the `image_processor_dict` so that it is converted to the appropriate\n-        # dict within the image processor and isn't overwritten if `size` is passed in as a kwarg.\n-        if \"size\" in kwargs and \"size\" in image_processor_dict:\n-            image_processor_dict[\"size\"] = kwargs.pop(\"size\")\n-        if \"crop_size\" in kwargs and \"crop_size\" in image_processor_dict:\n-            image_processor_dict[\"crop_size\"] = kwargs.pop(\"crop_size\")\n-\n+        image_processor_dict.update({k: v for k, v in kwargs.items() if k in cls.valid_kwargs.__annotations__})\n         image_processor = cls(**image_processor_dict)\n \n-        # Update image_processor with kwargs if needed\n-        to_remove = []\n-        for key, value in kwargs.items():\n+        # Remove kwargs that are used to initialize the image processor attributes\n+        for key in list(kwargs):\n             if hasattr(image_processor, key):\n-                setattr(image_processor, key, value)\n-                to_remove.append(key)\n-        for key in to_remove:\n-            kwargs.pop(key, None)\n+                kwargs.pop(key)\n \n         logger.info(f\"Image processor {image_processor}\")\n         if return_unused_kwargs:"
      },
      {
        "filename": "src/transformers/image_processing_utils_fast.py",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -185,6 +185,7 @@ class BaseImageProcessorFast(BaseImageProcessor):\n     input_data_format = None\n     device = None\n     model_input_names = [\"pixel_values\"]\n+    image_seq_length = None\n     valid_kwargs = ImagesKwargs\n     unused_kwargs = None\n "
      },
      {
        "filename": "src/transformers/models/pix2struct/image_processing_pix2struct.py",
        "status": "modified",
        "additions": 7,
        "deletions": 0,
        "changes": 7,
        "patch": "@@ -53,11 +53,18 @@ class Pix2StructImageProcessorKwargs(ImagesKwargs, total=False):\n     \"\"\"\n     max_patches (`int`, *optional*):\n         Maximum number of patches to extract.\n+    patch_size (`dict[str, int]`, *optional*, defaults to `{\"height\": 16, \"width\": 16}`):\n+        The patch size to use for the image. According to Pix2Struct paper and code, the patch size is 16x16.\n+    is_vqa (`bool`, *optional*, defaults to `False`):\n+        Whether or not the image processor is for the VQA task. If `True` and `header_text` is passed in, text is\n+        rendered onto the input images.\n     header_text (`Union[list[str], str]`, *optional*):\n         Text to render as a header. Only has an effect if `image_processor.is_vqa` is `True`.\n     \"\"\"\n \n     max_patches: int\n+    patch_size: dict[str, int]\n+    is_vqa: bool\n     header_text: Optional[Union[list[str], str]]\n \n "
      },
      {
        "filename": "src/transformers/processing_utils.py",
        "status": "modified",
        "additions": 4,
        "deletions": 0,
        "changes": 4,
        "patch": "@@ -219,6 +219,9 @@ class methods and docstrings.\n             - `'np'`: Return NumPy `np.ndarray` objects.\n         disable_grouping (`bool`, *optional*):\n             Whether to group images by shapes when processing or not, only relevant for fast image processing.\n+        image_seq_length (`int`, *optional*):\n+            The number of image tokens to be used for each image in the input.\n+            Added for backward compatibility but this should be set as a processor attribute in future models.\n     \"\"\"\n \n     do_convert_rgb: Optional[bool]\n@@ -239,6 +242,7 @@ class methods and docstrings.\n     device: Annotated[Optional[str], device_validator()]\n     return_tensors: Annotated[Optional[Union[str, TensorType]], tensor_type_validator()]\n     disable_grouping: Optional[bool]\n+    image_seq_length: Optional[int]\n \n \n class VideosKwargs(TypedDict, total=False):"
      },
      {
        "filename": "tests/models/pix2struct/test_processing_pix2struct.py",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -172,6 +172,7 @@ def test_image_processor_defaults_preserved_by_image_kwargs(self):\n         if \"image_processor\" not in self.processor_class.attributes:\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         image_processor = self.get_component(\"image_processor\", max_patches=1024, patch_size={\"height\": 8, \"width\": 8})\n+        print(\"image_processor\", image_processor)\n         tokenizer = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n \n         processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)"
      }
    ],
    "num_files": 5,
    "scraped_at": "2025-11-16T21:16:18.178797"
  },
  {
    "pr_number": 41969,
    "title": "add support for saving encoder only so any parakeet model can be loaded for inference",
    "body": "# What does this PR do?\r\n\r\n\r\n\r\nAdds support for conversion of any parakeet model encoder for both ctc and tdt decoders. This will enable researchers to use encoder only for foundation model training experiments. \r\n\r\n\r\n\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes # (issue)\r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker @Cyrilvallez\r\n- vision models: @yonigozlan @molbap\r\n- audio models: @eustlb @ebezzam @vasqu\r\n- multimodal models: @zucchini-nlp\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- generate: @zucchini-nlp (visual-language models) or @gante (all others)\r\n- continuous batching: @remi-or @ArthurZucker @McPatate\r\n- pipelines: @Rocketknight1\r\n- tokenizers: @ArthurZucker and @itazap\r\n- trainer: @SunMarc\r\n- attention: @vasqu @ArthurZucker @CyrilVallez\r\n- model loading (from pretrained, etc): @CyrilVallez\r\n- distributed: @3outeille @ArthurZucker\r\n- CIs: @ydshieh\r\n\r\nIntegrations:\r\n\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization: @SunMarc @MekkCyber\r\n- kernels: @MekkCyber @drbh\r\n- peft: @BenjaminBossan @githubnemo\r\n\r\nDevices/Backends:\r\n\r\n- AMD ROCm: @ivarflakstad\r\n- Intel XPU: @IlyasMoutawwakil\r\n- Ascend NPU: @ivarflakstad \r\n\r\nDocumentation: @stevhliu\r\n\r\nResearch projects are not maintained and should be taken as is.\r\n\r\n -->\r\n",
    "html_url": "https://github.com/huggingface/transformers/pull/41969",
    "created_at": "2025-10-31T19:28:32Z",
    "merged_at": "2025-11-02T18:21:41Z",
    "merge_commit_sha": "b9f90dc388fd415a2ba2a6a31a372f451d4a4eed",
    "base_ref": "main",
    "head_sha": "959de987484cd7a686c3c92a42bce511fc9e3f78",
    "user": "nithinraok",
    "files": [
      {
        "filename": "src/transformers/models/fastspeech2_conformer/configuration_fastspeech2_conformer.py",
        "status": "modified",
        "additions": 4,
        "deletions": 0,
        "changes": 4,
        "patch": "@@ -147,6 +147,8 @@ class FastSpeech2ConformerConfig(PreTrainedConfig):\n             Speaker embedding dimension. If set to > 0, assume that speaker_embedding will be provided as the input.\n         is_encoder_decoder (`bool`, *optional*, defaults to `True`):\n             Specifies whether the model is an encoder-decoder.\n+        convolution_bias (`bool`, *optional*, defaults to `True`):\n+            Specifies whether to use bias in convolutions of the conformer's convolution module.\n \n     Example:\n \n@@ -224,6 +226,7 @@ def __init__(\n         num_languages=None,\n         speaker_embed_dim=None,\n         is_encoder_decoder=True,\n+        convolution_bias=True,\n         **kwargs,\n     ):\n         if positionwise_conv_kernel_size % 2 == 0:\n@@ -318,6 +321,7 @@ def __init__(\n         self.speaker_embed_dim = speaker_embed_dim\n         self.duration_predictor_dropout_rate = duration_predictor_dropout_rate\n         self.is_encoder_decoder = is_encoder_decoder\n+        self.convolution_bias = convolution_bias\n \n         super().__init__(\n             is_encoder_decoder=is_encoder_decoder,"
      },
      {
        "filename": "src/transformers/models/fastspeech2_conformer/modeling_fastspeech2_conformer.py",
        "status": "modified",
        "additions": 13,
        "deletions": 3,
        "changes": 16,
        "patch": "@@ -490,12 +490,22 @@ def __init__(self, config: FastSpeech2ConformerConfig, module_config=None):\n             kernel_size = module_config[\"kernel_size\"]\n             self.activation = ACT2FN[module_config.get(\"activation\", \"silu\")]\n         self.padding = (kernel_size - 1) // 2\n-        self.pointwise_conv1 = nn.Conv1d(channels, 2 * channels, kernel_size=1, stride=1, padding=0, bias=True)\n+        self.pointwise_conv1 = nn.Conv1d(\n+            channels, 2 * channels, kernel_size=1, stride=1, padding=0, bias=config.convolution_bias\n+        )\n         self.depthwise_conv = nn.Conv1d(\n-            channels, channels, kernel_size, stride=1, padding=self.padding, groups=channels, bias=True\n+            channels,\n+            channels,\n+            kernel_size,\n+            stride=1,\n+            padding=self.padding,\n+            groups=channels,\n+            bias=config.convolution_bias,\n         )\n         self.norm = nn.BatchNorm1d(channels)\n-        self.pointwise_conv2 = nn.Conv1d(channels, channels, kernel_size=1, stride=1, padding=0, bias=True)\n+        self.pointwise_conv2 = nn.Conv1d(\n+            channels, channels, kernel_size=1, stride=1, padding=0, bias=config.convolution_bias\n+        )\n \n     def forward(self, hidden_states, attention_mask=None):\n         \"\"\""
      },
      {
        "filename": "src/transformers/models/parakeet/configuration_parakeet.py",
        "status": "modified",
        "additions": 4,
        "deletions": 0,
        "changes": 4,
        "patch": "@@ -44,6 +44,8 @@ class ParakeetEncoderConfig(PreTrainedConfig):\n             The non-linear activation function (function or string) in the encoder and pooler.\n         attention_bias (`bool`, *optional*, defaults to `True`):\n             Whether to use bias in the attention layers.\n+        convolution_bias (`bool`, *optional*, defaults to `True`):\n+            Whether to use bias in convolutions of the conformer's convolution module.\n         conv_kernel_size (`int`, *optional*, defaults to 9):\n             The kernel size of the convolution layers in the Conformer block.\n         subsampling_factor (`int`, *optional*, defaults to 8):\n@@ -102,6 +104,7 @@ def __init__(\n         intermediate_size=4096,\n         hidden_act=\"silu\",\n         attention_bias=True,\n+        convolution_bias=True,\n         conv_kernel_size=9,\n         subsampling_factor=8,\n         subsampling_conv_channels=256,\n@@ -128,6 +131,7 @@ def __init__(\n         self.intermediate_size = intermediate_size\n         self.hidden_act = hidden_act\n         self.attention_bias = attention_bias\n+        self.convolution_bias = convolution_bias\n \n         if (conv_kernel_size - 1) % 2 != 0:\n             raise ValueError(f\"conv_kernel_size must be odd, got {conv_kernel_size}\")"
      },
      {
        "filename": "src/transformers/models/parakeet/convert_nemo_to_hf.py",
        "status": "modified",
        "additions": 92,
        "deletions": 26,
        "changes": 118,
        "patch": "@@ -25,6 +25,8 @@\n \n from transformers import (\n     ParakeetCTCConfig,\n+    ParakeetEncoder,\n+    ParakeetEncoderConfig,\n     ParakeetFeatureExtractor,\n     ParakeetForCTC,\n     ParakeetProcessor,\n@@ -203,7 +205,8 @@ def write_processor(nemo_config: dict, model_files, output_dir, push_to_repo_id=\n         processor.push_to_hub(push_to_repo_id)\n \n \n-def write_model(nemo_config, model_files, model_type, output_dir, push_to_repo_id=None):\n+def convert_encoder_config(nemo_config):\n+    \"\"\"Convert NeMo encoder config to HF encoder config.\"\"\"\n     encoder_keys_to_ignore = [\n         \"att_context_size\",\n         \"causal_downsampling\",\n@@ -220,8 +223,11 @@ def write_model(nemo_config, model_files, model_type, output_dir, push_to_repo_i\n         \"stochastic_depth_mode\",\n         \"conv_context_size\",\n         \"dropout_pre_encoder\",\n+        \"reduction\",\n+        \"reduction_factor\",\n+        \"reduction_position\",\n     ]\n-    enocder_config_keys_mapping = {\n+    encoder_config_keys_mapping = {\n         \"d_model\": \"hidden_size\",\n         \"n_heads\": \"num_attention_heads\",\n         \"n_layers\": \"num_hidden_layers\",\n@@ -234,17 +240,26 @@ def write_model(nemo_config, model_files, model_type, output_dir, push_to_repo_i\n         \"dropout_emb\": \"dropout_positions\",\n         \"dropout_att\": \"attention_dropout\",\n         \"xscaling\": \"scale_input\",\n+        \"use_bias\": \"attention_bias\",\n     }\n     converted_encoder_config = {}\n \n     for key, value in nemo_config[\"encoder\"].items():\n         if key in encoder_keys_to_ignore:\n             continue\n-        if key in enocder_config_keys_mapping:\n-            converted_encoder_config[enocder_config_keys_mapping[key]] = value\n+        if key in encoder_config_keys_mapping:\n+            converted_encoder_config[encoder_config_keys_mapping[key]] = value\n+            # NeMo uses 'use_bias' for both attention and convolution bias, but HF separates them\n+            if key == \"use_bias\":\n+                converted_encoder_config[\"convolution_bias\"] = value\n         else:\n-            raise ValueError(f\"Key {key} not found in enocder_config_keys_mapping\")\n+            raise ValueError(f\"Key {key} not found in encoder_config_keys_mapping\")\n+\n+    return ParakeetEncoderConfig(**converted_encoder_config)\n+\n \n+def load_and_convert_state_dict(model_files):\n+    \"\"\"Load NeMo state dict and convert keys to HF format.\"\"\"\n     state_dict = torch.load(model_files[\"model_weights\"], map_location=\"cpu\", weights_only=True)\n     converted_state_dict = {}\n     for key, value in state_dict.items():\n@@ -255,31 +270,80 @@ def write_model(nemo_config, model_files, model_type, output_dir, push_to_repo_i\n         converted_key = convert_key(key, NEMO_TO_HF_WEIGHT_MAPPING)\n         converted_state_dict[converted_key] = value\n \n-    if model_type == \"ctc\":\n-        model_config = ParakeetCTCConfig(\n-            encoder_config=converted_encoder_config,\n-        )\n-        print(\"Loading the checkpoint in a Parakeet CTC model.\")\n-        with torch.device(\"meta\"):\n-            model = ParakeetForCTC(model_config)\n-        model.load_state_dict(converted_state_dict, strict=True, assign=True)\n-        print(\"Checkpoint loaded successfully.\")\n-        del model.config._name_or_path\n+    return converted_state_dict\n+\n+\n+def write_ctc_model(encoder_config, converted_state_dict, output_dir, push_to_repo_id=None):\n+    \"\"\"Write CTC model using encoder config and converted state dict.\"\"\"\n+    model_config = ParakeetCTCConfig.from_encoder_config(encoder_config)\n+\n+    print(\"Loading the checkpoint in a Parakeet CTC model.\")\n+    with torch.device(\"meta\"):\n+        model = ParakeetForCTC(model_config)\n+    model.load_state_dict(converted_state_dict, strict=True, assign=True)\n+    print(\"Checkpoint loaded successfully.\")\n+    del model.config._name_or_path\n+\n+    print(\"Saving the model.\")\n+    model.save_pretrained(output_dir)\n+\n+    if push_to_repo_id:\n+        model.push_to_hub(push_to_repo_id)\n \n-        print(\"Saving the model.\")\n-        model.save_pretrained(output_dir)\n+    del model\n \n-        if push_to_repo_id:\n-            model.push_to_hub(push_to_repo_id)\n+    # Safety check: reload the converted model\n+    gc.collect()\n+    print(\"Reloading the model to check if it's saved correctly.\")\n+    ParakeetForCTC.from_pretrained(output_dir, dtype=torch.bfloat16, device_map=\"auto\")\n+    print(\"Model reloaded successfully.\")\n \n-        del converted_state_dict, model\n \n-        # Safety check: reload the converted model\n-        gc.collect()\n-        print(\"Reloading the model to check if it's saved correctly.\")\n-        ParakeetForCTC.from_pretrained(output_dir, dtype=torch.bfloat16, device_map=\"auto\")\n-        print(\"Model reloaded successfully.\")\n+def write_encoder_model(encoder_config, converted_state_dict, output_dir, push_to_repo_id=None):\n+    \"\"\"Write encoder model using encoder config and converted state dict.\"\"\"\n+    # Filter to only encoder weights (exclude CTC head if present)\n+    encoder_state_dict = {\n+        k.replace(\"encoder.\", \"\", 1) if k.startswith(\"encoder.\") else k: v\n+        for k, v in converted_state_dict.items()\n+        if k.startswith(\"encoder.\")\n+    }\n+\n+    print(\"Loading the checkpoint in a Parakeet Encoder model (for TDT).\")\n+    with torch.device(\"meta\"):\n+        model = ParakeetEncoder(encoder_config)\n+\n+    model.load_state_dict(encoder_state_dict, strict=True, assign=True)\n+    print(\"Checkpoint loaded successfully.\")\n+    del model.config._name_or_path\n+\n+    print(\"Saving the model.\")\n+    model.save_pretrained(output_dir)\n+\n+    if push_to_repo_id:\n+        model.push_to_hub(push_to_repo_id)\n+    del model\n+\n+    # Safety check: reload the converted model\n+    gc.collect()\n+    print(\"Reloading the model to check if it's saved correctly.\")\n+    ParakeetEncoder.from_pretrained(output_dir, dtype=torch.bfloat16, device_map=\"auto\")\n+    print(\"Model reloaded successfully.\")\n+\n \n+def write_model(nemo_config, model_files, model_type, output_dir, push_to_repo_id=None):\n+    \"\"\"Main model conversion function.\"\"\"\n+    # Step 1: Convert encoder config (shared across all model types)\n+    encoder_config = convert_encoder_config(nemo_config)\n+    print(f\"Converted encoder config: {encoder_config}\")\n+\n+    # Step 2: Load and convert state dict (shared across all model types)\n+    converted_state_dict = load_and_convert_state_dict(model_files)\n+\n+    # Step 3: Write model based on type\n+    if model_type == \"encoder\":\n+        write_encoder_model(encoder_config, converted_state_dict, output_dir, push_to_repo_id)\n+    elif model_type == \"ctc\":\n+        write_ctc_model(encoder_config, converted_state_dict, output_dir, push_to_repo_id)\n     else:\n         raise ValueError(f\"Model type {model_type} not supported.\")\n \n@@ -303,7 +367,9 @@ def main(\n if __name__ == \"__main__\":\n     parser = argparse.ArgumentParser()\n     parser.add_argument(\"--hf_repo_id\", required=True, help=\"Model repo on huggingface.co\")\n-    parser.add_argument(\"--model_type\", required=True, choices=[\"ctc\"], help=\"Model type (`ctc`, `tdt`)\")\n+    parser.add_argument(\n+        \"--model_type\", required=True, choices=[\"encoder\", \"ctc\"], help=\"Model type (`encoder`, `ctc`)\"\n+    )\n     parser.add_argument(\"--output_dir\", required=True, help=\"Output directory for HuggingFace model\")\n     parser.add_argument(\"--push_to_repo_id\", help=\"Repository ID to push the model to on the Hub\")\n     args = parser.parse_args()"
      },
      {
        "filename": "src/transformers/models/parakeet/modeling_parakeet.py",
        "status": "modified",
        "additions": 13,
        "deletions": 3,
        "changes": 16,
        "patch": "@@ -130,12 +130,22 @@ def __init__(self, config: ParakeetEncoderConfig, module_config=None):\n             kernel_size = module_config[\"kernel_size\"]\n             self.activation = ACT2FN[module_config.get(\"activation\", \"silu\")]\n         self.padding = (kernel_size - 1) // 2\n-        self.pointwise_conv1 = nn.Conv1d(channels, 2 * channels, kernel_size=1, stride=1, padding=0, bias=True)\n+        self.pointwise_conv1 = nn.Conv1d(\n+            channels, 2 * channels, kernel_size=1, stride=1, padding=0, bias=config.convolution_bias\n+        )\n         self.depthwise_conv = nn.Conv1d(\n-            channels, channels, kernel_size, stride=1, padding=self.padding, groups=channels, bias=True\n+            channels,\n+            channels,\n+            kernel_size,\n+            stride=1,\n+            padding=self.padding,\n+            groups=channels,\n+            bias=config.convolution_bias,\n         )\n         self.norm = nn.BatchNorm1d(channels)\n-        self.pointwise_conv2 = nn.Conv1d(channels, channels, kernel_size=1, stride=1, padding=0, bias=True)\n+        self.pointwise_conv2 = nn.Conv1d(\n+            channels, channels, kernel_size=1, stride=1, padding=0, bias=config.convolution_bias\n+        )\n \n     def forward(self, hidden_states, attention_mask=None):\n         \"\"\""
      }
    ],
    "num_files": 5,
    "scraped_at": "2025-11-16T21:16:23.965275"
  },
  {
    "pr_number": 41930,
    "title": "handle inputs from Siglip/Siglip2 non-automapped encoder layers",
    "body": "# What does this PR do?\r\n\r\nShould fix #41929 . The `check_model_inputs` / `can_record_outputs` interaction is not always trivial and models with several entrypoints such as `VisionModel` vs `VisionTransformer` are missing some, adding it here. Also added a modification in `generic` to make sure the flag was captured, not 100% sure it's needed. ",
    "html_url": "https://github.com/huggingface/transformers/pull/41930",
    "created_at": "2025-10-29T09:51:32Z",
    "merged_at": "2025-11-12T13:58:44Z",
    "merge_commit_sha": "fd36275be2f3e56bc20da01f1f320b623b413957",
    "base_ref": "main",
    "head_sha": "f74fde9c3aef78d41ec7cf83b72980d920d7b27d",
    "user": "molbap",
    "files": [
      {
        "filename": "src/transformers/models/siglip/modeling_siglip.py",
        "status": "modified",
        "additions": 8,
        "deletions": 2,
        "changes": 10,
        "patch": "@@ -678,9 +678,14 @@ def forward(\n         )\n \n \n-class SiglipVisionTransformer(nn.Module):\n+class SiglipVisionTransformer(SiglipPreTrainedModel):\n+    _can_record_outputs = {\n+        \"hidden_states\": SiglipEncoderLayer,\n+        \"attentions\": SiglipAttention,\n+    }\n+\n     def __init__(self, config: SiglipVisionConfig):\n-        super().__init__()\n+        super().__init__(config)\n         self.config = config\n         embed_dim = config.hidden_size\n \n@@ -691,6 +696,7 @@ def __init__(self, config: SiglipVisionConfig):\n         if self.use_head:\n             self.head = SiglipMultiheadAttentionPoolingHead(config)\n \n+    @check_model_inputs(tie_last_hidden_states=False)\n     @auto_docstring\n     def forward(\n         self,"
      },
      {
        "filename": "src/transformers/models/siglip2/modeling_siglip2.py",
        "status": "modified",
        "additions": 99,
        "deletions": 93,
        "changes": 192,
        "patch": "@@ -349,99 +349,6 @@ def forward(\n         return hidden_states\n \n \n-class Siglip2Encoder(nn.Module):\n-    \"\"\"\n-    Transformer encoder consisting of `config.num_hidden_layers` self attention layers. Each layer is a\n-    [`Siglip2EncoderLayer`].\n-\n-    Args:\n-        config: Siglip2Config\n-    \"\"\"\n-\n-    def __init__(self, config: Siglip2Config):\n-        super().__init__()\n-        self.config = config\n-        self.layers = nn.ModuleList([Siglip2EncoderLayer(config) for _ in range(config.num_hidden_layers)])\n-        self.gradient_checkpointing = False\n-\n-    # Ignore copy\n-    @auto_docstring\n-    def forward(\n-        self,\n-        inputs_embeds,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        **kwargs: Unpack[TransformersKwargs],\n-    ) -> BaseModelOutput:\n-        hidden_states = inputs_embeds\n-        for encoder_layer in self.layers:\n-            hidden_states = encoder_layer(\n-                hidden_states,\n-                attention_mask,\n-                **kwargs,\n-            )\n-\n-        return BaseModelOutput(last_hidden_state=hidden_states)\n-\n-\n-class Siglip2VisionTransformer(nn.Module):\n-    def __init__(self, config: Siglip2VisionConfig):\n-        super().__init__()\n-        self.config = config\n-        embed_dim = config.hidden_size\n-\n-        self.embeddings = Siglip2VisionEmbeddings(config)\n-        self.encoder = Siglip2Encoder(config)\n-        self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n-        self.use_head = True if not hasattr(config, \"vision_use_head\") else config.vision_use_head\n-        if self.use_head:\n-            self.head = Siglip2MultiheadAttentionPoolingHead(config)\n-\n-    @auto_docstring\n-    def forward(\n-        self,\n-        pixel_values: torch.FloatTensor,\n-        attention_mask: torch.Tensor,\n-        spatial_shapes: torch.LongTensor,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-    ) -> BaseModelOutputWithPooling:\n-        r\"\"\"\n-        spatial_shapes (`torch.LongTensor` of shape `(batch_size, 2)`):\n-            Tensor containing the spatial dimensions (height, width) of the input images.\n-        \"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-\n-        hidden_states = self.embeddings(pixel_values, spatial_shapes)\n-\n-        if attention_mask is not None and self.config._attn_implementation != \"flash_attention_2\":\n-            # [batch_size, seq_len] -> [batch_size, 1, tgt_seq_len, src_seq_len]\n-            encoder_attention_mask = _prepare_4d_attention_mask(attention_mask, hidden_states.dtype)\n-        else:\n-            encoder_attention_mask = attention_mask\n-\n-        encoder_outputs: BaseModelOutput = self.encoder(\n-            inputs_embeds=hidden_states,\n-            attention_mask=encoder_attention_mask,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-        )\n-\n-        last_hidden_state = encoder_outputs.last_hidden_state\n-        last_hidden_state = self.post_layernorm(last_hidden_state)\n-\n-        pooler_output = self.head(last_hidden_state, attention_mask) if self.use_head else None\n-\n-        return BaseModelOutputWithPooling(\n-            last_hidden_state=last_hidden_state,\n-            pooler_output=pooler_output,\n-            hidden_states=encoder_outputs.hidden_states,\n-            attentions=encoder_outputs.attentions,\n-        )\n-\n-\n def _trunc_normal_(tensor, mean, std, a, b):\n     # Cut & paste from PyTorch official master until it's in a few official releases - RW\n     # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n@@ -607,6 +514,105 @@ def _init_weights(self, module):\n             module.weight.data.fill_(1.0)\n \n \n+class Siglip2Encoder(nn.Module):\n+    \"\"\"\n+    Transformer encoder consisting of `config.num_hidden_layers` self attention layers. Each layer is a\n+    [`Siglip2EncoderLayer`].\n+\n+    Args:\n+        config: Siglip2Config\n+    \"\"\"\n+\n+    def __init__(self, config: Siglip2Config):\n+        super().__init__()\n+        self.config = config\n+        self.layers = nn.ModuleList([Siglip2EncoderLayer(config) for _ in range(config.num_hidden_layers)])\n+        self.gradient_checkpointing = False\n+\n+    # Ignore copy\n+    @auto_docstring\n+    def forward(\n+        self,\n+        inputs_embeds,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> BaseModelOutput:\n+        hidden_states = inputs_embeds\n+        for encoder_layer in self.layers:\n+            hidden_states = encoder_layer(\n+                hidden_states,\n+                attention_mask,\n+                **kwargs,\n+            )\n+\n+        return BaseModelOutput(last_hidden_state=hidden_states)\n+\n+\n+class Siglip2VisionTransformer(Siglip2PreTrainedModel):\n+    _can_record_outputs = {\n+        \"hidden_states\": Siglip2EncoderLayer,\n+        \"attentions\": Siglip2Attention,\n+    }\n+\n+    def __init__(self, config: Siglip2VisionConfig):\n+        super().__init__(config)\n+        self.config = config\n+        embed_dim = config.hidden_size\n+\n+        self.embeddings = Siglip2VisionEmbeddings(config)\n+        self.encoder = Siglip2Encoder(config)\n+        self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n+        self.use_head = True if not hasattr(config, \"vision_use_head\") else config.vision_use_head\n+        if self.use_head:\n+            self.head = Siglip2MultiheadAttentionPoolingHead(config)\n+\n+    @check_model_inputs(tie_last_hidden_states=False)\n+    @auto_docstring\n+    def forward(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        attention_mask: torch.Tensor,\n+        spatial_shapes: torch.LongTensor,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+    ) -> BaseModelOutputWithPooling:\n+        r\"\"\"\n+        spatial_shapes (`torch.LongTensor` of shape `(batch_size, 2)`):\n+            Tensor containing the spatial dimensions (height, width) of the input images.\n+        \"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+\n+        hidden_states = self.embeddings(pixel_values, spatial_shapes)\n+\n+        if attention_mask is not None and self.config._attn_implementation != \"flash_attention_2\":\n+            # [batch_size, seq_len] -> [batch_size, 1, tgt_seq_len, src_seq_len]\n+            encoder_attention_mask = _prepare_4d_attention_mask(attention_mask, hidden_states.dtype)\n+        else:\n+            encoder_attention_mask = attention_mask\n+\n+        encoder_outputs: BaseModelOutput = self.encoder(\n+            inputs_embeds=hidden_states,\n+            attention_mask=encoder_attention_mask,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+        )\n+\n+        last_hidden_state = encoder_outputs.last_hidden_state\n+        last_hidden_state = self.post_layernorm(last_hidden_state)\n+\n+        pooler_output = self.head(last_hidden_state, attention_mask) if self.use_head else None\n+\n+        return BaseModelOutputWithPooling(\n+            last_hidden_state=last_hidden_state,\n+            pooler_output=pooler_output,\n+            hidden_states=encoder_outputs.hidden_states,\n+            attentions=encoder_outputs.attentions,\n+        )\n+\n+\n class Siglip2TextEmbeddings(nn.Module):\n     def __init__(self, config: Siglip2TextConfig):\n         super().__init__()"
      },
      {
        "filename": "src/transformers/models/siglip2/modular_siglip2.py",
        "status": "modified",
        "additions": 7,
        "deletions": 4,
        "changes": 11,
        "patch": "@@ -37,6 +37,7 @@\n \n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n from ...utils import auto_docstring, filter_out_non_signature_kwargs\n+from ...utils.generic import check_model_inputs\n \n \n class Siglip2TextConfig(SiglipTextConfig):\n@@ -230,6 +231,10 @@ def forward(self, pixel_values: torch.FloatTensor, spatial_shapes: torch.LongTen\n         return embeddings\n \n \n+class Siglip2PreTrainedModel(SiglipPreTrainedModel):\n+    pass\n+\n+\n class Siglip2VisionTransformer(SiglipVisionTransformer):\n     def __init__(self, config: Siglip2VisionConfig):\n         super().__init__(config)\n@@ -280,10 +285,6 @@ def forward(\n         )\n \n \n-class Siglip2PreTrainedModel(SiglipPreTrainedModel):\n-    pass\n-\n-\n class Siglip2TextModel(SiglipTextModel):\n     pass\n \n@@ -314,6 +315,8 @@ def forward(self, hidden_state: torch.Tensor, attention_mask: Optional[torch.Ten\n \n class Siglip2VisionModel(SiglipVisionModel):\n     # Update: add `spatial_shapes` and `pixel_attention_mask`\n+    @check_model_inputs(tie_last_hidden_states=False)\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: torch.FloatTensor,"
      },
      {
        "filename": "utils/check_repo.py",
        "status": "modified",
        "additions": 4,
        "deletions": 0,
        "changes": 4,
        "patch": "@@ -90,6 +90,8 @@\n     \"Kosmos2_5TextForCausalLM\",\n     \"Kosmos2_5VisionModel\",\n     \"SmolVLMVisionTransformer\",\n+    \"SiglipVisionTransformer\",\n+    \"Siglip2VisionTransformer\",\n     \"AriaTextForCausalLM\",\n     \"AriaTextModel\",\n     \"Phi4MultimodalAudioModel\",\n@@ -358,7 +360,9 @@\n     \"SegGptForImageSegmentation\",\n     \"SiglipVisionModel\",\n     \"SiglipTextModel\",\n+    \"SiglipVisionTransformer\",\n     \"Siglip2VisionModel\",\n+    \"Siglip2VisionTransformer\",\n     \"Siglip2TextModel\",\n     \"ChameleonVQVAE\",  # no autoclass for VQ-VAE models\n     \"VitPoseForPoseEstimation\","
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T21:16:31.379977"
  },
  {
    "pr_number": 41923,
    "title": "fix some ut failures on XPU w/ torch 2.9",
    "body": "cases are below, all passed. @ydshieh , pls help review, thx very much.\r\n\r\n> tests/models/aya_vision/test_modeling_aya_vision.py::AyaVisionIntegrationTest::test_small_model_integration_generate_text_only\r\n> tests/models/aya_vision/test_modeling_aya_vision.py::AyaVisionIntegrationTest::test_small_model_integration_forward\r\n> tests/models/aya_vision/test_modeling_aya_vision.py::AyaVisionIntegrationTest::test_small_model_integration_batched_generate_multi_image\r\n> tests/pipelines/test_pipelines_automatic_speech_recognition.py::AutomaticSpeechRecognitionPipelineTests::test_whisper_longform\r\n> tests/test_pipeline_mixin.py::AutomaticSpeechRecognitionPipelineTests::test_whisper_longform\r\n> tests/models/aria/test_modeling_aria.py::AriaForConditionalGenerationIntegrationTest::test_generation_no_images\r\n\r\n> tests/models/gemma3/test_modeling_gemma3.py::Gemma3IntegrationTest::test_model_4b_bf16\r\n> tests/models/gemma3/test_modeling_gemma3.py::Gemma3IntegrationTest::test_model_4b_crops\r\n> tests/models/glm4v/test_modeling_glm4v.py::Glm4vIntegrationTest::test_small_model_integration_test_expand\r\n> tests/models/mistral3/test_modeling_mistral3.py::Mistral3IntegrationTest::test_mistral3_integration_generate\r\n> tests/models/mllama/test_modeling_mllama.py::MllamaForConditionalGenerationIntegrationTest::test_11b_model_integration_generate_text_only",
    "html_url": "https://github.com/huggingface/transformers/pull/41923",
    "created_at": "2025-10-28T21:37:42Z",
    "merged_at": "2025-10-29T15:15:34Z",
    "merge_commit_sha": "a43b36cf802f00616800e0bd4d748679236123ee",
    "base_ref": "main",
    "head_sha": "ced44924dde51f50cc71b6c771ab311818217658",
    "user": "yao-matrix",
    "files": [
      {
        "filename": "tests/models/aria/test_modeling_aria.py",
        "status": "modified",
        "additions": 0,
        "deletions": 1,
        "changes": 1,
        "patch": "@@ -520,7 +520,6 @@ def test_generation_no_images(self):\n             quantization_config=BitsAndBytesConfig(load_in_4bit=True, llm_int8_skip_modules=[\"multihead_attn\"]),\n         )\n         processor = AutoProcessor.from_pretrained(model_id)\n-        assert model.device.type == \"cuda\", \"This test is only supported on CUDA\"  # TODO: remove this\n         # Prepare inputs with no images\n         inputs = processor(text=\"Hello, I am\", return_tensors=\"pt\").to(torch_device)\n "
      },
      {
        "filename": "tests/models/aya_vision/test_modeling_aya_vision.py",
        "status": "modified",
        "additions": 3,
        "deletions": 3,
        "changes": 6,
        "patch": "@@ -267,7 +267,7 @@ def test_small_model_integration_forward(self):\n \n         EXPECTED_LOGITS = Expectations(\n             {\n-                (\"xpu\", 3): [0.4109, 0.1532, 0.8018, 2.1328, 0.5483],\n+                (\"xpu\", 3): [1.6699, 0.6260, 3.2266, 8.5547, 2.209],\n                 # 4-bit\n                 (\"cuda\", 7): [0.1097, 0.3481, 3.8340, 9.7969, 2.0488],\n                 (\"cuda\", 8): [1.6396, 0.6094, 3.1992, 8.5234, 2.1875],\n@@ -308,7 +308,7 @@ def test_small_model_integration_generate_text_only(self):\n \n         expected_outputs = Expectations(\n             {\n-                (\"xpu\", 3): \"Whispers on the breeze,\\nLeaves dance under moonlit skies,\\nNature's quiet song.\",\n+                (\"xpu\", 3): \"Whispers on the breeze,\\nLeaves dance under moonlit sky,\\nNature's quiet song.\",\n                 # 4-bit\n                 (\"cuda\", 7): \"Sure, here's a haiku for you:\\n\\nMorning dew sparkles,\\nPetals unfold in sunlight,\\n\",\n                 (\"cuda\", 8): \"Whispers on the breeze,\\nLeaves dance under moonlit skies,\\nNature's quiet song.\",\n@@ -474,7 +474,7 @@ def test_small_model_integration_batched_generate_multi_image(self):\n         # Batching seems to alter the output slightly, but it is also the case in the original implementation. This seems to be expected: https://github.com/huggingface/transformers/issues/23017#issuecomment-1649630232\n         expected_outputs = Expectations(\n             {\n-                (\"xpu\", 3): \"Wooden path to water,\\nMountains echo in stillness,\\nPeaceful forest lake.\",\n+                (\"xpu\", 3): \"Wooden path to water,\\nMountains echo in stillness,\\nPeaceful forest scene.\",\n                 (\"cuda\", 7): 'Wooden bridge stretches\\nMirrored lake below, mountains rise\\nPeaceful, serene',\n                 (\"cuda\", 8): 'Wooden path to water,\\nMountains echo in stillness,\\nPeaceful forest scene.',\n             }"
      },
      {
        "filename": "tests/models/gemma3/test_modeling_gemma3.py",
        "status": "modified",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "patch": "@@ -499,7 +499,7 @@ def test_model_4b_bf16(self):\n \n         EXPECTED_TEXTS = Expectations(\n             {\n-                (\"xpu\", 3): ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nCertainly! \\n\\nThe image shows a brown and white cow standing on a sandy beach with turquoise water in the background. It looks like a lovely,'],\n+                (\"xpu\", 3): ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nCertainly! \\n\\nThe image shows a brown cow standing on a sandy beach with turquoise water and a blue sky in the background. It looks like a'],\n                 (\"cuda\", (8, 0)): ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nCertainly! \\n\\nThe image shows a brown cow standing on a sandy beach with clear turquoise water and a blue sky in the background. It looks like'],\n                 (\"cuda\", (8, 6)): ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nCertainly! \\n\\nThe image shows a brown cow standing on a sandy beach with clear blue water and a blue sky in the background. It looks like'],\n                 (\"rocm\", (9, 4)): ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nCertainly! \\n\\nThe image shows a brown cow standing on a sandy beach with turquoise water and a blue sky in the background. It looks like a'],\n@@ -610,7 +610,7 @@ def test_model_4b_crops(self):\n         EXPECTED_NUM_IMAGES = 3  # one for the origin image and two crops of images\n         EXPECTED_TEXTS = Expectations(\n             {\n-                (\"xpu\", 3): ['user\\nYou are a helpful assistant.\\n\\nHere is the original image \\n\\n\\n\\n and here are some crops to help you see better \\n\\n\\n\\n \\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nThe image shows a brown cow standing on a sandy beach next to a turquoise ocean. There are clouds in the blue sky above.'],\n+                (\"xpu\", 3): [\"user\\nYou are a helpful assistant.\\n\\nHere is the original image \\n\\n\\n\\n and here are some crops to help you see better \\n\\n\\n\\n \\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nThe image shows a brown cow standing on a sandy beach next to a turquoise ocean. There's a bright blue sky with some white clouds in the\"],\n                 (\"cuda\", 7): [],\n                 (\"cuda\", (8, 6)): [\"user\\nYou are a helpful assistant.\\n\\nHere is the original image \\n\\n\\n\\n and here are some crops to help you see better \\n\\n\\n\\n \\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nThe image shows a brown cow standing on a sandy beach next to a turquoise ocean. There's a clear blue sky with some white clouds above.\"],\n                 (\"cuda\", (8, 0)): [\"user\\nYou are a helpful assistant.\\n\\nHere is the original image \\n\\n\\n\\n and here are some crops to help you see better \\n\\n\\n\\n \\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nThe image shows a brown cow standing on a sandy beach next to a turquoise ocean. There's a blue sky with some white clouds in the background\"],"
      },
      {
        "filename": "tests/models/glm4v/test_modeling_glm4v.py",
        "status": "modified",
        "additions": 19,
        "deletions": 7,
        "changes": 26,
        "patch": "@@ -24,7 +24,9 @@\n     is_torch_available,\n )\n from transformers.testing_utils import (\n+    Expectations,\n     cleanup,\n+    require_deterministic_for_xpu,\n     require_flash_attn,\n     require_torch,\n     require_torch_gpu,\n@@ -413,6 +415,7 @@ def test_small_model_integration_test_with_video(self):\n         )\n \n     @slow\n+    @require_deterministic_for_xpu\n     def test_small_model_integration_test_expand(self):\n         model = Glm4vForConditionalGeneration.from_pretrained(\n             \"THUDM/GLM-4.1V-9B-Thinking\", dtype=\"auto\", device_map=\"auto\"\n@@ -426,14 +429,23 @@ def test_small_model_integration_test_expand(self):\n \n         output = model.generate(**inputs, max_new_tokens=30, do_sample=False, num_beams=2, num_return_sequences=2)\n \n-        EXPECTED_DECODED_TEXT = [\n-            \"\\nWhat kind of dog is this?\\n<think>Got it, let's look at the image. The animal in the picture doesn't look like a dog; it's actually a cat. Specifically\",\n-            \"\\nWhat kind of dog is this?\\n<think>Got it, let's look at the image. The animal in the picture doesn't look like a dog; it's actually a cat, specifically\"\n-        ]  # fmt: skip\n-        self.assertEqual(\n-            self.processor.batch_decode(output, skip_special_tokens=True),\n-            EXPECTED_DECODED_TEXT,\n+        # fmt: off\n+        EXPECTED_DECODED_TEXTS = Expectations(\n+            {\n+\n+                (None, None): [\"\\nWhat kind of dog is this?\\n<think>Got it, let's look at the image. The animal in the picture doesn't look like a dog; it's actually a cat. Specifically\",\n+                               \"\\nWhat kind of dog is this?\\n<think>Got it, let's look at the image. The animal in the picture doesn't look like a dog; it's actually a cat, specifically\"\n+                              ],\n+                (\"xpu\", None): [\"\\nWhat kind of dog is this?\\n<think>Got it, let's look at the image. The animal in the picture is not a dog; it's a cat. Specifically, it looks\",\n+                                \"\\nWhat kind of dog is this?\\n<think>Got it, let's look at the image. The animal in the picture is not a dog; it's a cat, specifically a Pallas\"\n+                               ],\n+            }\n         )\n+        # fmt: on\n+        EXPECTED_DECODED_TEXT = EXPECTED_DECODED_TEXTS.get_expectation()\n+\n+        decoded_text = self.processor.batch_decode(output, skip_special_tokens=True)\n+        self.assertEqual(decoded_text, EXPECTED_DECODED_TEXT)\n \n     @slow\n     def test_small_model_integration_test_batch_wo_image(self):"
      },
      {
        "filename": "tests/models/mistral3/test_modeling_mistral3.py",
        "status": "modified",
        "additions": 2,
        "deletions": 1,
        "changes": 3,
        "patch": "@@ -275,6 +275,7 @@ def test_mistral3_integration_generate_text_only(self):\n         self.assertEqual(decoded_output, expected_output)\n \n     @require_read_token\n+    @require_deterministic_for_xpu\n     def test_mistral3_integration_generate(self):\n         processor = AutoProcessor.from_pretrained(self.model_checkpoint)\n         processor.chat_template = processor.chat_template.replace('strftime_now(\"%Y-%m-%d\")', '\"2025-06-20\"')\n@@ -299,7 +300,7 @@ def test_mistral3_integration_generate(self):\n \n         expected_outputs = Expectations(\n             {\n-                (\"xpu\", 3): \"The image features two cats resting on a pink blanket. The cat on the left is a kitten\",\n+                (\"xpu\", 3): \"The image features two tabby cats lying on a pink surface, which appears to be a cushion or\",\n                 (\"cuda\", 8): 'The image features two cats lying on a pink surface, which appears to be a couch or a bed',\n                 (\"rocm\", (9, 4)): \"The image features two cats lying on a pink surface, which appears to be a couch or a bed\",\n                 (\"rocm\", (9, 5)): \"The image features two tabby cats lying on a pink surface, which appears to be a cushion or\""
      },
      {
        "filename": "tests/models/mllama/test_modeling_mllama.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -547,7 +547,7 @@ def test_11b_model_integration_generate_text_only(self):\n         decoded_output = processor.decode(output[0], skip_special_tokens=True)\n         expected_outputs = Expectations(\n                 {\n-                    (\"xpu\", 3): \"If I had to write a haiku about my life, I would write:\\nLife is a messy tapestry\\n Threads of joy and sorrow\\nWeft of memories\",\n+                    (\"xpu\", 3): \"If I had to write a haiku about my life, I would write:\\nLife is a messy stream\\nRipples of joy and pain\\nFlowing, ever\",\n                     (\"cuda\", 7): \"If I had to write a haiku about my life, I would write:\\nLife is a messy stream\\nRipples of joy and pain\\nFlowing, ever\",\n                     (\"cuda\", 8): \"If I had to write a haiku about my life, I would write:\\nLife is a messy stream\\nRipples of joy and pain\\nFlowing, ever\",\n                 }"
      },
      {
        "filename": "tests/pipelines/test_pipelines_automatic_speech_recognition.py",
        "status": "modified",
        "additions": 8,
        "deletions": 1,
        "changes": 9,
        "patch": "@@ -35,6 +35,7 @@\n from transformers.pipelines.audio_utils import chunk_bytes_iter, ffmpeg_microphone_live\n from transformers.pipelines.automatic_speech_recognition import chunk_iter\n from transformers.testing_utils import (\n+    Expectations,\n     compare_pipeline_output_to_hub_spec,\n     is_pipeline_test,\n     is_torch_available,\n@@ -1443,8 +1444,14 @@ def test_whisper_prompted(self):\n     @slow\n     def test_whisper_longform(self):\n         # fmt: off\n-        EXPECTED_RESULT = \" Folks, if you watch the show, you know, I spent a lot of time right over there. Patiently and astutely scrutinizing the boxwood and mahogany chest set of the day's biggest stories developing the central headline pawns, definitely maneuvering an oso topical night to F6, fainting a classic Sicilian, nade door variation on the news, all the while seeing eight moves deep and patiently marshalling the latest press releases into a fisher's shows in Lip Nitsky attack that culminates in the elegant lethal slow-played, all-passant checkmate that is my nightly monologue. But sometimes, sometimes, folks, I. CHEERING AND APPLAUSE Sometimes I startle away, cubside down in the monkey bars of a condemned playground on a super fun site. Get all hept up on goofballs. Rummage that were discarded tag bag of defective toys. Yank out a fist bowl of disembodied doll limbs, toss them on Saturday, Rusty Cargo, container down by the Wharf, and challenge toothless drifters to the godless bughouse lets of tournament that is my segment. MUSIC Meanwhile!\"\n+        EXPECTED_RESULTS = Expectations(\n+            {\n+                (None, None): \" Folks, if you watch the show, you know, I spent a lot of time right over there. Patiently and astutely scrutinizing the boxwood and mahogany chest set of the day's biggest stories developing the central headline pawns, definitely maneuvering an oso topical night to F6, fainting a classic Sicilian, nade door variation on the news, all the while seeing eight moves deep and patiently marshalling the latest press releases into a fisher's shows in Lip Nitsky attack that culminates in the elegant lethal slow-played, all-passant checkmate that is my nightly monologue. But sometimes, sometimes, folks, I. CHEERING AND APPLAUSE Sometimes I startle away, cubside down in the monkey bars of a condemned playground on a super fun site. Get all hept up on goofballs. Rummage that were discarded tag bag of defective toys. Yank out a fist bowl of disembodied doll limbs, toss them on Saturday, Rusty Cargo, container down by the Wharf, and challenge toothless drifters to the godless bughouse lets of tournament that is my segment. MUSIC Meanwhile!\",\n+                (\"xpu\", None): \" Folks, if you watch the show, you know, I spent a lot of time right over there. Patiently and astutely scrutinizing the boxwood and mahogany chest set of the day's biggest stories developing the central headline pawns, definitely maneuvering an oso topical night to F6, fainting of classics, Sicilian, nade door variation on the news, all the while seeing eight moves deep and patiently marshalling the latest press releases into a Fisher shows in Lip Nitsky attack that culminates in the elegant lethal slow-played, all-passant checkmate that is my nightly monologue. But sometimes, sometimes, folks, I... APPLAUSE Sometimes I... Startle away, upside down on the monkey bars of a condemned playground on a superfund site. Get all heaped up on goofballs, rummaged that would discard a tag bag of defective toys, yank out a fist bowl of disembodied doll limbs, toss them on a stain kid's place mat from a defunct denys, set up a table inside a rusty cargo container down by the Wharf and challenge toothless drifters to the godless bug house blitz of tournament that is my segment.\",\n+            }\n+        )\n         # fmt: on\n+        EXPECTED_RESULT = EXPECTED_RESULTS.get_expectation()\n \n         processor = AutoProcessor.from_pretrained(\"openai/whisper-tiny.en\")\n         model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny.en\")"
      }
    ],
    "num_files": 7,
    "scraped_at": "2025-11-16T21:16:32.226620"
  },
  {
    "pr_number": 41914,
    "title": "Run slow v2",
    "body": "# What does this PR do?\r\n\r\nRun slow v2!\r\n\r\n- Send feedback as a comment.\r\n- Include verified failed tests specific to a PR.\r\n\r\nDiscussed offline in the office. merge now \ud83d\udd25 ",
    "html_url": "https://github.com/huggingface/transformers/pull/41914",
    "created_at": "2025-10-28T13:13:51Z",
    "merged_at": "2025-11-01T18:40:40Z",
    "merge_commit_sha": "8fb854cac869b42c87a7bd15d9298985c5aea96e",
    "base_ref": "main",
    "head_sha": "cfea0a5035c6995d3cded16684734a0483ee5c2f",
    "user": "ydshieh",
    "files": [
      {
        "filename": ".github/workflows/check_failed_tests.yml",
        "status": "modified",
        "additions": 75,
        "deletions": 28,
        "changes": 103,
        "patch": "@@ -6,9 +6,6 @@ on:\n       docker:\n         required: true\n         type: string\n-      start_sha:\n-        required: true\n-        type: string\n       job:\n         required: true\n         type: string\n@@ -24,7 +21,13 @@ on:\n       commit_sha:\n         required: false\n         type: string\n-\n+      pr_number:\n+        required: false\n+        type: string\n+    outputs:\n+      report:\n+        description: \"Content of the report of new failures\"\n+        value: ${{ jobs.process_new_failures_with_commit_info.outputs.report }}\n \n env:\n   HF_HOME: /mnt/cache\n@@ -88,27 +91,55 @@ jobs:\n             echo \"PREV_WORKFLOW_RUN_ID=\" >> $GITHUB_ENV\n           fi\n \n-          if [ -f setup_values/other_workflow_run_id.txt ]; then\n-            echo \"OTHER_WORKFLOW_RUN_ID=$(cat setup_values/other_workflow_run_id.txt)\" >> $GITHUB_ENV\n-          else\n-            echo \"OTHER_WORKFLOW_RUN_ID=\" >> $GITHUB_ENV\n-          fi\n-\n       - name: Update clone\n         working-directory: /transformers\n         if: ${{ env.process == 'true' }}\n-        run: git fetch && git checkout ${{ inputs.commit_sha || github.sha }}\n+        run: |\n+          git fetch origin ${{ inputs.commit_sha || github.sha }}\n+          git fetch && git checkout ${{ inputs.commit_sha || github.sha }}\n \n-      - name: Get target commit\n+      - name: Get `START_SHA`\n         working-directory: /transformers/utils\n         if: ${{ env.process == 'true' }}\n+        run: |\n+          echo \"START_SHA=${{ inputs.commit_sha || github.sha }}\" >> $GITHUB_ENV\n+\n+      # This is used if the CI is triggered from a pull request `self-comment-ci.yml` (after security check is verified)\n+      - name: Extract the base commit on `main` (of the merge commit created by Github) if it is a PR\n+        id: pr_info\n+        if: ${{ env.process == 'true' && inputs.pr_number != '' }}\n+        uses: actions/github-script@v6\n+        with:\n+          script: |            \n+            const { data: pr } = await github.rest.pulls.get({\n+              owner: context.repo.owner,\n+              repo: context.repo.repo,\n+              pull_number: ${{ inputs.pr_number }}\n+            });\n+\n+            const { data: merge_commit }  = await github.rest.repos.getCommit({\n+              owner: pr.base.repo.owner.login,\n+              repo: pr.base.repo.name,\n+              ref: pr.merge_commit_sha,\n+            });\n+\n+            core.setOutput('merge_commit_base_sha', merge_commit.parents[0].sha);\n+\n+      # Usually, `END_SHA` should be the commit of the last previous workflow run of the **SAME** (scheduled) workflow.\n+      # (This is why we don't need to specify `workflow_id` which would be fetched automatically in the python script.)\n+      - name: Get `END_SHA` from previous CI runs of the same workflow\n+        working-directory: /transformers/utils\n+        if: ${{ env.process == 'true' && inputs.pr_number == '' }}\n         run: |\n           echo \"END_SHA=$(TOKEN=${{ secrets.ACCESS_REPO_INFO_TOKEN }} python3 -c 'import os; from get_previous_daily_ci import get_last_daily_ci_run_commit; commit=get_last_daily_ci_run_commit(token=os.environ[\"TOKEN\"], workflow_run_id=os.environ[\"PREV_WORKFLOW_RUN_ID\"]); print(commit)')\" >> $GITHUB_ENV\n \n-      - name: Checkout to `start_sha`\n-        working-directory: /transformers\n-        if: ${{ env.process == 'true' }}\n-        run: git fetch && git checkout ${{ inputs.start_sha }}\n+      # However, for workflow runs triggered by `issue_comment` (for pull requests), we want to check against the\n+      # parent commit (on `main`) of the `merge_commit` (dynamically created by GitHub). In this case, the goal is to\n+      # see if a reported failing test is actually ONLY failing on the `merge_commit`.\n+      - name: Set `END_SHA`\n+        if: ${{ env.process == 'true' && inputs.pr_number != '' }}\n+        run: |\n+          echo \"END_SHA=${{ steps.pr_info.outputs.merge_commit_base_sha }}\" >> $GITHUB_ENV\n \n       - name: Reinstall transformers in edit mode (remove the one installed during docker image build)\n         working-directory: /transformers\n@@ -138,7 +169,7 @@ jobs:\n       - name: Check failed tests\n         working-directory: /transformers\n         if: ${{ env.process == 'true' }}\n-        run: python3 utils/check_bad_commit.py --start_commit ${{ inputs.start_sha }} --end_commit ${{ env.END_SHA }} --file ci_results_${{ inputs.job }}/new_failures.json --output_file new_failures_with_bad_commit_${{ inputs.job }}_${{ matrix.run_idx }}.json\n+        run: python3 utils/check_bad_commit.py --start_commit ${{ env.START_SHA }} --end_commit ${{ env.END_SHA }} --file ci_results_${{ inputs.job }}/new_failures.json --output_file new_failures_with_bad_commit_${{ inputs.job }}_${{ matrix.run_idx }}.json\n \n       - name: Show results\n         working-directory: /transformers\n@@ -159,6 +190,8 @@ jobs:\n     if: needs.check_new_failures.outputs.process == 'true'\n     runs-on:\n       group: aws-g5-4xlarge-cache\n+    outputs:\n+      report: ${{ steps.set_output.outputs.report }}\n     container:\n       image: ${{ inputs.docker }}\n       options: --gpus all --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n@@ -190,18 +223,9 @@ jobs:\n \n       - name: Update clone\n         working-directory: /transformers\n-        run: git fetch && git checkout ${{ inputs.commit_sha || github.sha }}\n-\n-      - name: Process report\n-        shell: bash\n-        working-directory: /transformers\n-        env:\n-          ACCESS_REPO_INFO_TOKEN: ${{ secrets.ACCESS_REPO_INFO_TOKEN }}\n-          TRANSFORMERS_CI_RESULTS_UPLOAD_TOKEN: ${{ secrets.TRANSFORMERS_CI_RESULTS_UPLOAD_TOKEN }}\n-          JOB_NAME: ${{ inputs.job }}\n-          REPORT_REPO_ID: ${{ inputs.report_repo_id }}\n         run: |\n-          python3 utils/process_bad_commit_report.py\n+          git fetch origin ${{ inputs.commit_sha || github.sha }}\n+          git fetch && git checkout ${{ inputs.commit_sha || github.sha }}\n \n       - name: Process report\n         shell: bash\n@@ -218,6 +242,29 @@ jobs:\n             echo EOF\n           } >> \"$GITHUB_ENV\"\n \n+      # The output is useful if a caller needs more processing, for example, we have a chain\n+      # self-comment-ci.yml -> self-scheduled.yml -> this one (check_failed_tests.yml),\n+      # and `self-comment-ci.yml` needs further processing before sending a GitHub comment to the pull request page.\n+      - name: Show results & Set outputs\n+        id: set_output\n+        working-directory: /transformers\n+        run: |\n+          ls -l new_failures_with_bad_commit.json\n+          cat new_failures_with_bad_commit.json\n+\n+          {\n+            echo 'report<<EOF'\n+            cat new_failures_with_bad_commit.json\n+            echo ''  # Force a newline\n+            echo EOF\n+          } >> \"$GITHUB_OUTPUT\"\n+\n+      - name: Upload artifacts\n+        uses: actions/upload-artifact@v4\n+        with:\n+          name: new_failures_with_bad_commit_${{ inputs.job }}\n+          path: /transformers/new_failures_with_bad_commit.json\n+\n       - name: Prepare Slack report title\n         working-directory: /transformers\n         run: |"
      },
      {
        "filename": ".github/workflows/get-pr-info.yml",
        "status": "modified",
        "additions": 9,
        "deletions": 0,
        "changes": 9,
        "patch": "@@ -39,6 +39,9 @@ on:\n       PR_MERGE_COMMIT_SHA:\n         description: \"The sha of the merge commit for the pull request (created by GitHub) in the base repository\"\n         value: ${{ jobs.get-pr-info.outputs.PR_MERGE_COMMIT_SHA }}\n+      PR_MERGE_COMMIT_BASE_SHA:\n+        description: \"The sha of the parent commit of the the merge commit on the target branch in the base repository\"\n+        value: ${{ jobs.get-pr-info.outputs.PR_MERGE_COMMIT_BASE_SHA }}\n       PR_HEAD_COMMIT_DATE:\n         description: \"The date of the head sha of the pull request branch in the head repository\"\n         value: ${{ jobs.get-pr-info.outputs.PR_HEAD_COMMIT_DATE }}\n@@ -74,6 +77,7 @@ jobs:\n       PR_BASE_REF: ${{ steps.pr_info.outputs.base_ref }}\n       PR_HEAD_SHA: ${{ steps.pr_info.outputs.head_sha }}\n       PR_BASE_SHA: ${{ steps.pr_info.outputs.base_sha }}\n+      PR_MERGE_COMMIT_BASE_SHA: ${{ steps.pr_info.outputs.merge_commit_base_sha }}\n       PR_MERGE_COMMIT_SHA: ${{ steps.pr_info.outputs.merge_commit_sha }}\n       PR_HEAD_COMMIT_DATE: ${{ steps.pr_info.outputs.head_commit_date }}\n       PR_MERGE_COMMIT_DATE: ${{ steps.pr_info.outputs.merge_commit_date }}\n@@ -122,6 +126,7 @@ jobs:\n             core.setOutput('base_ref', pr.base.ref);\n             core.setOutput('head_sha', pr.head.sha);\n             core.setOutput('base_sha', pr.base.sha);\n+            core.setOutput('merge_commit_base_sha', merge_commit.parents[0].sha);\n             core.setOutput('merge_commit_sha', pr.merge_commit_sha);\n             core.setOutput('pr', pr);\n \n@@ -142,6 +147,10 @@ jobs:\n               date: merge_commit.commit.committer.date\n             });\n \n+            console.log('PR Info:', {\n+              pr_info: pr\n+            });\n+\n       - name: Convert dates to timestamps\n         id: get_timestamps\n         run: |"
      },
      {
        "filename": ".github/workflows/model_jobs.yml",
        "status": "modified",
        "additions": 4,
        "deletions": 2,
        "changes": 6,
        "patch": "@@ -80,7 +80,9 @@ jobs:\n \n       - name: Update clone\n         working-directory: /transformers\n-        run: git fetch && git checkout ${{ inputs.commit_sha || github.sha }}\n+        run: |\n+          git fetch origin ${{ inputs.commit_sha || github.sha }}\n+          git fetch && git checkout ${{ inputs.commit_sha || github.sha }}\n \n       - name: Reinstall transformers in edit mode (remove the one installed during docker image build)\n         working-directory: /transformers\n@@ -174,7 +176,7 @@ jobs:\n \n   collated_reports:\n     name: Collated Reports\n-    if: ${{ always() }}\n+    if: ${{ always() && inputs.runner_type != '' }}\n     needs: run_models_gpu\n     uses: huggingface/transformers/.github/workflows/collated-reports.yml@main\n     with:"
      },
      {
        "filename": ".github/workflows/push-important-models.yml",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -153,5 +153,5 @@ jobs:\n       ci_event: push\n       report_repo_id: hf-internal-testing/transformers_ci_push\n       commit_sha: ${{ github.sha }}\n-      models: ${{ needs.get_modified_models.outputs.matrix }}\n+      subdirs: ${{ needs.get_modified_models.outputs.matrix }}\n     secrets: inherit"
      },
      {
        "filename": ".github/workflows/self-comment-ci.yml",
        "status": "modified",
        "additions": 203,
        "deletions": 280,
        "changes": 483,
        "patch": "@@ -23,62 +23,34 @@ env:\n   TF_FORCE_GPU_ALLOW_GROWTH: true\n   CUDA_VISIBLE_DEVICES: 0,1\n \n+\n jobs:\n   get-pr-number:\n-    runs-on: ubuntu-22.04\n     name: Get PR number\n-    # For security: only allow team members to run\n     if: ${{ github.event.issue.state == 'open' && contains(fromJSON('[\"ydshieh\", \"ArthurZucker\", \"zucchini-nlp\", \"molbap\", \"gante\", \"LysandreJik\", \"Cyrilvallez\", \"Rocketknight1\", \"SunMarc\", \"eustlb\", \"MekkCyber\", \"vasqu\", \"ivarflakstad\", \"stevhliu\", \"ebezzam\", \"remi-or\", \"itazap\"]'), github.actor) && (startsWith(github.event.comment.body, 'run-slow') || startsWith(github.event.comment.body, 'run slow') || startsWith(github.event.comment.body, 'run_slow')) }}\n-    outputs:\n-      PR_NUMBER: ${{ steps.set_pr_number.outputs.PR_NUMBER }}\n-    steps:\n-      - name: Get PR number\n-        shell: bash\n-        run: |\n-          if [[ \"${{ github.event.issue.number }}\" != \"\" && \"${{ github.event.issue.pull_request }}\" != \"\" ]]; then\n-            echo \"PR_NUMBER=${{ github.event.issue.number }}\" >> $GITHUB_ENV\n-          else\n-            echo \"PR_NUMBER=\" >> $GITHUB_ENV\n-          fi\n-\n-      - name: Check PR number\n-        shell: bash\n-        run: |\n-          echo \"${{ env.PR_NUMBER }}\"\n-\n-      - name: Set PR number\n-        id: set_pr_number\n-        run: echo \"PR_NUMBER=${{ env.PR_NUMBER }}\" >> \"$GITHUB_OUTPUT\"\n+    uses: ./.github/workflows/get-pr-number.yml\n \n-  get-sha:\n-    runs-on: ubuntu-22.04\n+  get-pr-info:\n+    name: Get PR commit SHA\n     needs: get-pr-number\n     if: ${{ needs.get-pr-number.outputs.PR_NUMBER != ''}}\n+    uses: ./.github/workflows/get-pr-info.yml\n+    with:\n+      pr_number: ${{ needs.get-pr-number.outputs.PR_NUMBER }}\n+\n+  check-timestamps:\n+    name: Check timestamps (security check)\n+    runs-on: ubuntu-22.04\n+    needs: get-pr-info\n     outputs:\n-      PR_HEAD_SHA: ${{ steps.get_sha.outputs.PR_HEAD_SHA }}\n-      PR_MERGE_SHA: ${{ steps.get_sha.outputs.PR_MERGE_SHA }}\n+      PR_HEAD_SHA: ${{ needs.get-pr-info.outputs.PR_HEAD_SHA }}\n+      PR_MERGE_SHA: ${{ needs.get-pr-info.outputs.PR_MERGE_COMMIT_SHA }}\n     steps:\n-      - uses: actions/checkout@v4\n-        with:\n-          fetch-depth: \"0\"\n-          ref: \"refs/pull/${{needs.get-pr-number.outputs.PR_NUMBER}}/merge\"\n-\n-      - name: Get SHA (and verify timestamps against the issue comment date)\n-        id: get_sha\n+      - name: Verify `merge_commit` timestamp is older than the issue comment timestamp\n         env:\n-          PR_NUMBER: ${{ needs.get-pr-number.outputs.PR_NUMBER }}\n           COMMENT_DATE: ${{ github.event.comment.created_at }}\n+          PR_MERGE_COMMIT_TIMESTAMP: ${{ needs.get-pr-info.outputs.PR_MERGE_COMMIT_TIMESTAMP }}\n         run: |\n-            git fetch origin refs/pull/$PR_NUMBER/head:refs/remotes/pull/$PR_NUMBER/head\n-            git checkout refs/remotes/pull/$PR_NUMBER/head\n-            echo \"PR_HEAD_SHA: $(git log -1 --format=%H)\"\n-            echo \"PR_HEAD_SHA=$(git log -1 --format=%H)\" >> \"$GITHUB_OUTPUT\"\n-            git fetch origin refs/pull/$PR_NUMBER/merge:refs/remotes/pull/$PR_NUMBER/merge\n-            git checkout refs/remotes/pull/$PR_NUMBER/merge\n-            echo \"PR_MERGE_SHA: $(git log -1 --format=%H)\"\n-            echo \"PR_MERGE_SHA=$(git log -1 --format=%H)\" >> \"$GITHUB_OUTPUT\"\n-            PR_MERGE_COMMIT_TIMESTAMP=$(git log -1 --date=unix --format=%cd)\n-            echo \"PR_MERGE_COMMIT_TIMESTAMP: $PR_MERGE_COMMIT_TIMESTAMP\"\n             COMMENT_TIMESTAMP=$(date -d \"${COMMENT_DATE}\" +\"%s\")\n             echo \"COMMENT_DATE: $COMMENT_DATE\"\n             echo \"COMMENT_TIMESTAMP: $COMMENT_TIMESTAMP\"\n@@ -87,25 +59,22 @@ jobs:\n               exit -1;\n             fi\n \n-  # use a python script to handle this complex logic\n-  # case 1: `run-slow` (auto. infer with limited number of models, but in particular, new model)\n-  # case 2: `run-slow model_1, model_2`\n+  # use a python script to handle this complex logic.\n   get-tests:\n     runs-on: ubuntu-22.04\n-    needs: [get-pr-number, get-sha]\n-    if: ${{ needs.get-pr-number.outputs.PR_NUMBER != ''}}\n+    needs: [get-pr-number, check-timestamps]\n     outputs:\n       models: ${{ steps.models_to_run.outputs.models }}\n       quantizations: ${{ steps.models_to_run.outputs.quantizations }}\n     steps:\n       - uses: actions/checkout@v4\n         with:\n           fetch-depth: \"0\"\n-          ref: \"refs/pull/${{needs.get-pr-number.outputs.PR_NUMBER}}/merge\"\n+          ref: \"refs/pull/${{ needs.get-pr-number.outputs.PR_NUMBER }}/merge\"\n \n       - name: Verify merge commit SHA\n         env:\n-          VERIFIED_PR_MERGE_SHA: ${{ needs.get-sha.outputs.PR_MERGE_SHA }}\n+          VERIFIED_PR_MERGE_SHA: ${{ needs.check-timestamps.outputs.PR_MERGE_SHA }}\n         run: |\n             PR_MERGE_SHA=$(git log -1 --format=%H)\n             if [ $PR_MERGE_SHA != $VERIFIED_PR_MERGE_SHA ]; then\n@@ -119,19 +88,39 @@ jobs:\n         run: |\n           python -m pip install GitPython\n           python utils/pr_slow_ci_models.py --message \"$PR_COMMENT\" | tee output.txt\n-          echo \"models=$(tail -n 1 output.txt)\" >> $GITHUB_ENV\n+          echo 'models=$(tail -n 1 output.txt)' >> $GITHUB_ENV\n           python utils/pr_slow_ci_models.py --message \"$PR_COMMENT\" --quantization | tee output2.txt\n-          echo \"quantizations=$(tail -n 1 output2.txt)\" >> $GITHUB_ENV\n+          echo 'quantizations=$(tail -n 1 output2.txt)' >> $GITHUB_ENV\n \n       - name: Show models to test\n         id: models_to_run\n         run: |\n           echo \"${{ env.models }}\"\n-          echo \"models=${{ env.models }}\" >> $GITHUB_ENV\n           echo \"models=${{ env.models }}\" >> $GITHUB_OUTPUT\n           echo \"${{ env.quantizations }}\"\n           echo \"quantizations=${{ env.quantizations }}\" >> $GITHUB_OUTPUT\n \n+  # Report back if we are not able to get the tests (for example, security check is failing)\n+  report_error_earlier:\n+    name: Report error earlier\n+    if: ${{ always() && needs.get-pr-info.result == 'success' && needs.get-tests.result != 'success' }}\n+    needs: [get-pr-number, get-pr-info, get-tests]\n+    permissions:\n+      pull-requests: write\n+    runs-on: ubuntu-22.04\n+    steps:\n+      - name: Reply to the comment\n+        env:\n+          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n+          GITHUB_RUN_URL: https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}\n+        run: |\n+          gh api \\\n+            --method POST \\\n+            -H \"Accept: application/vnd.github+json\" \\\n+            -H \"X-GitHub-Api-Version: 2022-11-28\" \\\n+            repos/${{ github.repository }}/issues/${{ needs.get-pr-number.outputs.PR_NUMBER }}/comments \\\n+            -f body=\"\ud83d\udc94 This comment contains \\`run-slow\\`, but unknown error occurred and [the workflow run]($GITHUB_RUN_URL) aborted!\"\n+\n   reply_to_comment:\n     name: Reply to the comment\n     if: ${{ needs.get-tests.outputs.models != '[]'  || needs.get-tests.outputs.quantizations != '[]' }}\n@@ -143,20 +132,18 @@ jobs:\n       - name: Reply to the comment\n         env:\n           GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n-          MODELS: ${{ needs.get-tests.outputs.models }}\n-          BODY: \"\\n\\nmodels: ${{ needs.get-tests.outputs.models }}\\nquantizations: ${{ needs.get-tests.outputs.quantizations }}\"\n+          BODY: '\\n\\nmodels: ${{ needs.get-tests.outputs.models }}\\nquantizations: ${{ needs.get-tests.outputs.quantizations }}'\n         run: |\n           gh api \\\n             --method POST \\\n             -H \"Accept: application/vnd.github+json\" \\\n             -H \"X-GitHub-Api-Version: 2022-11-28\" \\\n             repos/${{ github.repository }}/issues/${{ needs.get-pr-number.outputs.PR_NUMBER }}/comments \\\n-            -f \"body=This comment contains run-slow, running the specified jobs: ${{ env.BODY }} ...\"\n+            -f body=\"This comment contains \\`run-slow\\`, running the specified jobs: $(echo -e '${{ env.BODY }}')\"\n \n   create_run:\n     name: Create run\n-    if: ${{ needs.get-tests.outputs.models != '[]' || needs.get-tests.outputs.quantizations != '[]' }}\n-    needs: [get-sha, get-tests, reply_to_comment]\n+    needs: [check-timestamps, reply_to_comment]\n     permissions:\n       statuses: write\n     runs-on: ubuntu-22.04\n@@ -173,243 +160,179 @@ jobs:\n             --method POST \\\n             -H \"Accept: application/vnd.github+json\" \\\n             -H \"X-GitHub-Api-Version: 2022-11-28\" \\\n-            repos/${{ github.repository }}/statuses/${{ needs.get-sha.outputs.PR_HEAD_SHA }} \\\n+            repos/${{ github.repository }}/statuses/${{ needs.check-timestamps.outputs.PR_HEAD_SHA }} \\\n             -f \"target_url=$GITHUB_RUN_URL\" -f \"state=pending\" -f \"description=Slow CI job\" -f \"context=pytest/custom-tests\"\n \n-  run_models_gpu:\n-    name: Run all tests for the model\n+  model-ci:\n+    name: Model CI\n     if: ${{ needs.get-tests.outputs.models != '[]' }}\n-    needs: [get-pr-number, get-sha, get-tests, create_run]\n-    strategy:\n-      fail-fast: false\n-      matrix:\n-        folders: ${{ fromJson(needs.get-tests.outputs.models) }}\n-        machine_type: [aws-g5-4xlarge-cache, aws-g5-12xlarge-cache]\n-    runs-on:\n-       group: '${{ matrix.machine_type }}'\n-    container:\n-      image: huggingface/transformers-all-latest-gpu\n-      options: --gpus all --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n-    steps:\n-      - name: Echo input and matrix info\n-        shell: bash\n-        run: |\n-          echo \"${{ matrix.folders }}\"\n-\n-      - name: Echo folder ${{ matrix.folders }}\n-        shell: bash\n-        # For folders like `models/bert`, set an env. var. (`matrix_folders`) to `models_bert`, which will be used to\n-        # set the artifact folder names (because the character `/` is not allowed).\n-        run: |\n-          echo \"${{ matrix.folders }}\"\n-          matrix_folders=${{ matrix.folders }}\n-          matrix_folders=${matrix_folders/'models/'/'models_'}\n-          echo \"$matrix_folders\"\n-          echo \"matrix_folders=$matrix_folders\" >> $GITHUB_ENV\n-\n-      - name: Checkout to PR merge commit\n-        working-directory: /transformers\n-        run: |\n-          git fetch origin refs/pull/${{ needs.get-pr-number.outputs.PR_NUMBER }}/merge:refs/remotes/pull/${{ needs.get-pr-number.outputs.PR_NUMBER }}/merge\n-          git checkout refs/remotes/pull/${{ needs.get-pr-number.outputs.PR_NUMBER }}/merge\n-          git log -1 --format=%H\n-\n-      - name: Verify merge commit SHA\n-        env:\n-          VERIFIED_PR_MERGE_SHA: ${{ needs.get-sha.outputs.PR_MERGE_SHA }}\n-        working-directory: /transformers\n-        run: |\n-          PR_MERGE_SHA=$(git log -1 --format=%H)\n-          if [ $PR_MERGE_SHA != $VERIFIED_PR_MERGE_SHA ]; then\n-            echo \"The merged commit SHA is not the same as the verified one! Security issue detected, abort the workflow!\";\n-            exit -1;\n-          fi\n-\n-      - name: Reinstall transformers in edit mode (remove the one installed during docker image build)\n-        working-directory: /transformers\n-        run: python3 -m pip uninstall -y transformers && python3 -m pip install -e .\n-\n-      - name: NVIDIA-SMI\n-        run: |\n-          nvidia-smi\n-\n-      - name: Set `machine_type` for report and artifact names\n-        working-directory: /transformers\n-        shell: bash\n-        run: |\n-          echo \"${{ matrix.machine_type }}\"\n-          if [ \"${{ matrix.machine_type }}\" = \"aws-g5-4xlarge-cache\" ]; then\n-            machine_type=single-gpu\n-          elif [ \"${{ matrix.machine_type }}\" = \"aws-g5-12xlarge-cache\" ]; then\n-            machine_type=multi-gpu\n-          else\n-            machine_type=${{ matrix.machine_type }}\n-          fi\n-          echo \"$machine_type\"\n-          echo \"machine_type=$machine_type\" >> $GITHUB_ENV\n-\n-      - name: Environment\n-        working-directory: /transformers\n-        run: |\n-          python3 utils/print_env.py\n-\n-      - name: Show installed libraries and their versions\n-        working-directory: /transformers\n-        run: pip freeze\n-\n-      - name: Run all tests on GPU\n-        working-directory: /transformers\n-        run: |\n-          export CUDA_VISIBLE_DEVICES=\"$(python3 utils/set_cuda_devices_for_ci.py --test_folder ${{ matrix.folders }})\"\n-          echo $CUDA_VISIBLE_DEVICES\n-          python3 -m pytest -v -rsfE --make-reports=${{ env.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports tests/${{ matrix.folders }}\n-\n-      - name: Failure short reports\n-        if: ${{ failure() }}\n-        continue-on-error: true\n-        run: cat /transformers/reports/${{ env.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports/failures_short.txt\n-\n-      - name: Make sure report directory exists\n-        shell: bash\n-        run: |\n-          mkdir -p /transformers/reports/${{ env.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports\n-          echo \"hello\" > /transformers/reports/${{ env.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports/hello.txt\n-          echo \"${{ env.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports\"\n-\n-      - name: \"Test suite reports artifacts: ${{ env.machine_type }}_run_models_gpu_${{ env.matrix_folders }}_test_reports\"\n-        if: ${{ always() }}\n-        uses: actions/upload-artifact@v4\n-        with:\n-          name: ${{ env.machine_type }}_run_models_gpu_${{ env.matrix_folders }}_test_reports\n-          path: /transformers/reports/${{ env.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports\n-\n-  run_quantization_torch_gpu:\n-    name: Run all tests for a quantization\n+    uses: ./.github/workflows/self-scheduled.yml\n+    needs: [get-pr-number, check-timestamps, get-tests, create_run]\n+    with:\n+      job: run_models_gpu\n+      slack_report_channel: \"#transformers-ci-pr\"\n+      docker: huggingface/transformers-all-latest-gpu\n+      ci_event: PR Comment CI\n+      report_repo_id: hf-internal-testing/transformers_pr_ci\n+      commit_sha: ${{ needs.check-timestamps.outputs.PR_MERGE_SHA }}\n+      subdirs: ${{ needs.get-tests.outputs.models }}\n+      pr_number: ${{ needs.get-pr-number.outputs.PR_NUMBER }}\n+    secrets: inherit\n+\n+  quantization-ci:\n+    name: Quantization CI\n     if: ${{ needs.get-tests.outputs.quantizations != '[]' }}\n-    needs: [get-pr-number, get-sha, get-tests, create_run]\n-    strategy:\n-      fail-fast: false\n-      matrix:\n-        folders: ${{ fromJson(needs.get-tests.outputs.quantizations) }}\n-        machine_type: [aws-g5-4xlarge-cache, aws-g5-12xlarge-cache]\n-    runs-on:\n-      group: '${{ matrix.machine_type }}'\n-    container:\n-      image: huggingface/transformers-quantization-latest-gpu\n-      options: --gpus all --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n+    uses: ./.github/workflows/self-scheduled.yml\n+    needs: [get-pr-number, check-timestamps, get-tests, create_run]\n+    with:\n+      job: run_quantization_torch_gpu\n+      slack_report_channel: \"#transformers-ci-pr\"\n+      docker: huggingface/transformers-quantization-latest-gpu\n+      ci_event: PR Comment CI\n+      report_repo_id: hf-internal-testing/transformers_pr_ci\n+      commit_sha: ${{ needs.check-timestamps.outputs.PR_MERGE_SHA }}\n+      subdirs: ${{ needs.get-tests.outputs.quantizations }}\n+      pr_number: ${{ needs.get-pr-number.outputs.PR_NUMBER }}\n+    secrets: inherit\n+\n+  report:\n+    name: Check & Report\n+    needs: [get-pr-number, check-timestamps, create_run, model-ci, quantization-ci]\n+    permissions:\n+      pull-requests: write\n+      statuses: write\n+    if: ${{ always() && needs.create_run.result == 'success' }}\n+    runs-on: ubuntu-22.04\n     steps:\n-      - name: Echo folder ${{ matrix.folders }}\n-        shell: bash\n-        run: |\n-          echo \"${{ matrix.folders }}\"\n-          matrix_folders=${{ matrix.folders }}\n-          matrix_folders=${matrix_folders/'quantization/'/'quantization_'}\n-          echo \"$matrix_folders\"\n-          echo \"matrix_folders=$matrix_folders\" >> $GITHUB_ENV\n-\n-      - name: Checkout to PR merge commit\n-        working-directory: /transformers\n+      - name: Show reports from jobs\n         run: |\n-          git fetch origin refs/pull/${{ needs.get-pr-number.outputs.PR_NUMBER }}/merge:refs/remotes/pull/${{ needs.get-pr-number.outputs.PR_NUMBER }}/merge\n-          git checkout refs/remotes/pull/${{ needs.get-pr-number.outputs.PR_NUMBER }}/merge\n-          git log -1 --format=%H\n+          echo \"${{ needs.model-ci.outputs.report }}\"\n+          echo \"${{ needs.quantization-ci.outputs.report }}\"\n \n-      - name: Verify merge commit SHA\n+      - name: Process and filter reports\n         env:\n-          VERIFIED_PR_MERGE_SHA: ${{ needs.get-sha.outputs.PR_MERGE_SHA }}\n-        working-directory: /transformers\n-        run: |\n-          PR_MERGE_SHA=$(git log -1 --format=%H)\n-          if [ $PR_MERGE_SHA != $VERIFIED_PR_MERGE_SHA ]; then\n-            echo \"The merged commit SHA is not the same as the verified one! Security issue detected, abort the workflow!\";\n-            exit -1;\n-          fi\n-\n-      - name: Reinstall transformers in edit mode (remove the one installed during docker image build)\n-        working-directory: /transformers\n-        run: python3 -m pip uninstall -y transformers && python3 -m pip install -e .\n-      - name: NVIDIA-SMI\n-        run: |\n-          nvidia-smi\n-\n-      - name: Set `machine_type` for report and artifact names\n-        working-directory: /transformers\n-        shell: bash\n-        run: |\n-          echo \"${{ matrix.machine_type }}\"\n-          if [ \"${{ matrix.machine_type }}\" = \"aws-g5-4xlarge-cache\" ]; then\n-            machine_type=single-gpu\n-          elif [ \"${{ matrix.machine_type }}\" = \"aws-g5-12xlarge-cache\" ]; then\n-            machine_type=multi-gpu\n-          else\n-            machine_type=${{ matrix.machine_type }}\n-          fi\n-          echo \"$machine_type\"\n-          echo \"machine_type=$machine_type\" >> $GITHUB_ENV\n-\n-      - name: Environment\n-        working-directory: /transformers\n-        run: |\n-          python3 utils/print_env.py\n-\n-      - name: Show installed libraries and their versions\n-        working-directory: /transformers\n-        run: pip freeze\n-\n-      - name: Run quantization tests on GPU\n-        working-directory: /transformers\n+          MODEL_REPORT: ${{ needs.model-ci.outputs.report }}\n+          QUANT_REPORT: ${{ needs.quantization-ci.outputs.report }}\n         run: |\n-          python3 -m pytest -v --make-reports=${{ env.machine_type }}_run_quantization_torch_gpu_${{ matrix.folders }}_test_reports tests/${{ matrix.folders }}\n-\n-      - name: Failure short reports\n-        if: ${{ failure() }}\n-        continue-on-error: true\n-        run: cat /transformers/reports/${{ env.machine_type }}_run_quantization_torch_gpu_${{ matrix.folders }}_test_reports/failures_short.txt\n-\n-      - name: Make sure report directory exists\n-        shell: bash\n+          # Preprocess with Python\n+          python3 << 'PYTHON_SCRIPT'\n+          import json\n+          import os\n+          \n+          def filter_and_format_report(data):\n+            \"\"\"\n+            Filter out entries where commit is `None` (failing tests who status is not certain) and format as text\n+            \"\"\"\n+            lines = []\n+            \n+            for model, model_result in data.items():\n+                model_lines = []\n+                for device, failures in model_result.items():\n+                    \n+                    # Filter out None commits and extract just the test names\n+                    test_names = [\n+                        failure['test'] \n+                        for failure in failures \n+                        if isinstance(failure, dict) and failure.get('commit') is not None\n+                    ]\n+\n+                    # Add tests to model lines\n+                    for idx, test_name in enumerate(test_names):\n+                        if idx == 0:\n+                            job_link = failures[idx]['job_link']\n+                            model_lines.append(f\"- [{model}]({job_link}):\")\n+          \n+                        model_lines.append(f\"    {test_name}\")\n+\n+                # Only add model section if it has tests\n+                if len(model_lines) > 0:\n+                    lines.extend(model_lines)\n+                    lines.append(\"\")  # Empty line between models\n+            \n+            return \"\\n\".join(lines).strip()\n+          \n+          # Load and filter reports\n+          model_report_str = os.environ.get('MODEL_REPORT', '{}')\n+          quant_report_str = os.environ.get('QUANT_REPORT', '{}')\n+          \n+          model_report = json.loads(model_report_str) if model_report_str else {}\n+          quant_report = json.loads(quant_report_str) if quant_report_str else {}\n+          \n+          formatted_model = filter_and_format_report(model_report)\n+          formatted_quant = filter_and_format_report(quant_report)\n+          \n+          # Write to files\n+          with open('model_ci.txt', 'w') as f:\n+              f.write(formatted_model)\n+              if formatted_model:\n+                  f.write('\\n')\n+          \n+          with open('quantization_ci.txt', 'w') as f:\n+              f.write(formatted_quant)\n+              if formatted_quant:\n+                  f.write('\\n')\n+          PYTHON_SCRIPT\n+\n+      - name: Post results as PR comment\n+        env:\n+          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n+          GITHUB_RUN_URL: https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}\n         run: |\n-          mkdir -p /transformers/reports/${{ env.machine_type }}_run_quantization_gpu_${{ matrix.folders }}_test_reports\n-          echo \"hello\" > /transformers/reports/${{ env.machine_type }}_run_quantization_gpu_${{ matrix.folders }}_test_reports/hello.txt\n-          echo \"${{ env.machine_type }}_run_quantization_gpu_${{ matrix.folders }}_test_reports\"\n-\n-      - name: \"Test suite reports artifacts: ${{ env.machine_type }}_run_quantization_torch_gpu_${{ env.matrix_folders }}_test_reports\"\n-        if: ${{ always() }}\n-        uses: actions/upload-artifact@v4\n-        with:\n-          name: ${{ env.machine_type }}_run_quantization_torch_gpu_${{ env.matrix_folders }}_test_reports\n-          path: /transformers/reports/${{ env.machine_type }}_run_quantization_torch_gpu_${{ matrix.folders }}_test_reports\n+          {\n+            echo '## CI Results'\n+            echo \"[Workflow Run \u2699\ufe0f]($GITHUB_RUN_URL)\"\n+            echo ''\n+\n+            # Check if both jobs were skipped or cancelled\n+            if [[ \"${{ needs.model-ci.result }}\" == \"skipped\" || \"${{ needs.model-ci.result }}\" == \"cancelled\" ]] && \\\n+               [[ \"${{ needs.quantization-ci.result }}\" == \"skipped\" || \"${{ needs.quantization-ci.result }}\" == \"cancelled\" ]]; then\n+              echo '\u26a0\ufe0f No test being reported (jobs are skipped or cancelled)!'\n+              echo \"STATUS=error\" >> $GITHUB_ENV\n+\n+            # Check if either file has content\n+            elif [ -s model_ci.txt ] || [ -s quantization_ci.txt ]; then\n+              echo \"STATUS=failure\" >> $GITHUB_ENV\n+\n+              # Check if model_ci.txt has content\n+              if [ -s model_ci.txt ]; then\n+                echo '### Model CI Report'\n+                echo ''\n+                echo '#### \u274c Failed tests'\n+                echo ''\n+                cat model_ci.txt\n+                echo ''\n+              fi\n+              \n+              # Check if quantization_ci.txt has content\n+              if [ -s quantization_ci.txt ]; then\n+                echo '### Quantization CI Report'\n+                echo ''\n+                echo '#### \u274c Failed tests'\n+                echo ''\n+                cat quantization_ci.txt\n+                echo ''\n+              fi\n+            else\n+              echo \"STATUS=success\" >> $GITHUB_ENV\n+              echo '\u2705 No failing test specific to this PR \ud83c\udf89 !'\n+            fi\n+          } > comment_body.txt\n \n-  update_run_status:\n-    name: Update Check Run Status\n-    needs: [get-sha, create_run, run_models_gpu, run_quantization_torch_gpu]\n-    permissions:\n-      statuses: write\n-    if: ${{ always() && needs.create_run.result == 'success' }}\n-    runs-on: ubuntu-22.04\n-    env:\n-      GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n-      GITHUB_RUN_URL: https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}\n-      STATUS_OK: ${{ contains(fromJSON('[\"skipped\", \"success\"]'), needs.run_models_gpu.result) && contains(fromJSON('[\"skipped\", \"success\"]'), needs.run_quantization_torch_gpu.result) }}\n-    steps:\n-      - name: Get `run_models_gpu` job status\n-        run: |\n-          echo \"${{ needs.run_models_gpu.result }}\"\n-          echo \"${{ needs.run_quantization_torch_gpu.result }}\"\n-          echo $STATUS_OK\n-          if [ \"$STATUS_OK\" = \"true\" ]; then\n-            echo \"STATUS=success\" >> $GITHUB_ENV\n-          else\n-            echo \"STATUS=failure\" >> $GITHUB_ENV\n-          fi\n+          gh api \\\n+            --method POST \\\n+            -H \"Accept: application/vnd.github+json\" \\\n+            -H \"X-GitHub-Api-Version: 2022-11-28\" \\\n+            repos/${{ github.repository }}/issues/${{ needs.get-pr-number.outputs.PR_NUMBER }}/comments \\\n+            -F body=@comment_body.txt\n \n       - name: Update PR commit statuses\n+        env:\n+          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n+          GITHUB_RUN_URL: https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}\n         run: |\n-          echo \"${{ needs.run_models_gpu.result }}\"\n-          echo \"${{ env.STATUS }}\"\n           gh api \\\n             --method POST \\\n             -H \"Accept: application/vnd.github+json\" \\\n             -H \"X-GitHub-Api-Version: 2022-11-28\" \\\n-            repos/${{ github.repository }}/statuses/${{ needs.get-sha.outputs.PR_HEAD_SHA }} \\\n+            repos/${{ github.repository }}/statuses/${{ needs.check-timestamps.outputs.PR_HEAD_SHA }} \\\n             -f \"target_url=$GITHUB_RUN_URL\" -f \"state=${{ env.STATUS }}\" -f \"description=Slow CI job\" -f \"context=pytest/custom-tests\""
      },
      {
        "filename": ".github/workflows/self-nightly-caller.yml",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -51,6 +51,7 @@ jobs:\n       slack_report_channel: \"#transformers-ci-past-future\"\n       docker: huggingface/transformers-all-latest-torch-nightly-gpu\n       ci_event: Nightly CI\n+      runner_type: \"a10\"\n       report_repo_id: hf-internal-testing/transformers_daily_ci_with_torch_nightly\n       commit_sha: ${{ github.event.workflow_run.head_sha || github.sha }}\n     secrets: inherit"
      },
      {
        "filename": ".github/workflows/self-scheduled.yml",
        "status": "modified",
        "additions": 14,
        "deletions": 6,
        "changes": 20,
        "patch": "@@ -34,14 +34,20 @@ on:\n       runner_type:\n         required: false\n         type: string\n-      models:\n+      subdirs:\n         default: \"\"\n         required: false\n         type: string\n       pytest_marker:\n         required: false\n         type: string\n-\n+      pr_number:\n+        required: false\n+        type: string\n+    outputs:\n+      report:\n+        description: \"Content of the report of new failures\"\n+        value: ${{ jobs.check_new_failures.outputs.report }}\n \n env:\n   HF_HOME: /mnt/cache\n@@ -76,6 +82,7 @@ jobs:\n       - name: Update clone\n         working-directory: /transformers\n         run: |\n+          git fetch origin ${{ inputs.commit_sha || github.sha }}\n           git fetch && git checkout ${{ inputs.commit_sha || github.sha }}\n \n       - name: Cleanup\n@@ -95,7 +102,7 @@ jobs:\n         working-directory: /transformers/tests\n         run: |\n           if [ \"${{ inputs.job }}\" = \"run_models_gpu\" ]; then\n-            echo \"folder_slices=$(python3 ../utils/split_model_tests.py --models '${{ inputs.models }}' --num_splits ${{ env.NUM_SLICES }})\" >> $GITHUB_OUTPUT\n+            echo \"folder_slices=$(python3 ../utils/split_model_tests.py --subdirs '${{ inputs.subdirs }}' --num_splits ${{ env.NUM_SLICES }})\" >> $GITHUB_OUTPUT\n             echo \"slice_ids=$(python3 -c 'd = list(range(${{ env.NUM_SLICES }})); print(d)')\" >> $GITHUB_OUTPUT\n           elif [ \"${{ inputs.job }}\" = \"run_trainer_and_fsdp_gpu\" ]; then\n             echo \"folder_slices=[['trainer'], ['fsdp']]\" >> $GITHUB_OUTPUT\n@@ -107,7 +114,7 @@ jobs:\n         name: Identify quantization method to test\n         working-directory: /transformers/tests\n         run: |\n-          echo \"quantization_matrix=$(python3 -c 'import os; tests = os.getcwd(); quantization_tests = os.listdir(os.path.join(tests, \"quantization\")); d = sorted(list(filter(os.path.isdir, [f\"quantization/{x}\" for x in quantization_tests]))) ;  print(d)')\" >> $GITHUB_OUTPUT\n+          echo \"quantization_matrix=$(python3 -c 'import ast; import os; tests = os.getcwd(); quantization_tests = os.listdir(os.path.join(tests, \"quantization\")); subdirs = ast.literal_eval(${{ inputs.subdirs || '\"None\"' }}); quantization_tests = [x.removeprefix(\"quantization/\") for x in subdirs] if subdirs is not None else quantization_tests; d = sorted(list(filter(os.path.isdir, [f\"quantization/{x}\" for x in quantization_tests]))) ;  print(d)')\" >> $GITHUB_OUTPUT\n \n       - name: NVIDIA-SMI\n         run: |\n@@ -539,16 +546,17 @@ jobs:\n     secrets: inherit\n \n   check_new_failures:\n-    if: ${{ always() && inputs.ci_event == 'Daily CI' && needs.send_results.result == 'success' }}\n+    if: ${{ always() && needs.send_results.result == 'success' }}\n     name: Check new failures\n     needs: send_results\n     uses: ./.github/workflows/check_failed_tests.yml\n     with:\n       docker: ${{ inputs.docker }}\n-      start_sha: ${{ inputs.commit_sha || github.sha }}\n+      commit_sha: ${{ inputs.commit_sha || github.sha }}\n       job: ${{ inputs.job }}\n       slack_report_channel: ${{ inputs.slack_report_channel }}\n       ci_event: ${{ inputs.ci_event }}\n       report_repo_id: ${{ inputs.report_repo_id }}\n+      pr_number: ${{ inputs.pr_number }}\n \n     secrets: inherit"
      },
      {
        "filename": "utils/check_bad_commit.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -151,7 +151,7 @@ def find_bad_commit(target_test, start_commit, end_commit):\n \n     bash = f\"\"\"\n git bisect reset\n-git bisect start {start_commit} {end_commit}\n+git bisect start --first-parent {start_commit} {end_commit}\n git bisect run python3 target_script.py\n \"\"\"\n "
      },
      {
        "filename": "utils/notification_service.py",
        "status": "modified",
        "additions": 10,
        "deletions": 0,
        "changes": 10,
        "patch": "@@ -1521,6 +1521,16 @@ def pop_default(l: list[Any], i: int, default: Any) -> Any:\n                 token=os.environ[\"ACCESS_REPO_INFO_TOKEN\"], workflow_id=other_workflow_id, commit_sha=ci_sha\n             )\n             other_workflow_run_ids.append(other_workflow_run_id)\n+    # triggered via `issue_comment` for CI on pull requests (e.g. using the comment `run-slow:`)\n+    elif os.environ.get(\"GITHUB_EVENT_NAME\") in [\"issue_comment\"]:\n+        # TODO (ydshieh): Make this flexible once we implement `run-slow` for AMD CI and others.\n+        # The id of the workflow `.github/workflows/self-scheduled-caller.yml` (not of a workflow run of it).\n+        prev_workflow_id = \"90575235\"\n+        # TODO (ydshieh): It's better to make sure using the last completed scheduled workflow run with the commit being a parent\n+        #  of the PR's `merge_commit`.\n+        prev_workflow_run_id = get_last_daily_ci_workflow_run_id(\n+            token=os.environ[\"ACCESS_REPO_INFO_TOKEN\"], workflow_id=prev_workflow_id\n+        )\n     else:\n         prev_workflow_run_id = os.environ[\"PREV_WORKFLOW_RUN_ID\"]\n         other_workflow_run_id = os.environ[\"OTHER_WORKFLOW_RUN_ID\"]"
      },
      {
        "filename": "utils/pr_slow_ci_models.py",
        "status": "modified",
        "additions": 4,
        "deletions": 1,
        "changes": 5,
        "patch": "@@ -27,6 +27,7 @@\n \"\"\"\n \n import argparse\n+import json\n import os.path\n import re\n import string\n@@ -169,4 +170,6 @@ def check_model_names(model_name: str):\n         elif os.path.isdir(f\"tests/quantization/{model}\"):\n             final_list.append(f\"quantization/{model}\")\n \n-    print(sorted(set(final_list)))\n+    # Use `json.dumps` to get the double quotes instead of single quote, e.g. `[\"model/vit\"]`.\n+    # (to avoid some shell expansion issues when this script is called from a Github Actions workflow)\n+    print(json.dumps(sorted(set(final_list))))"
      },
      {
        "filename": "utils/process_bad_commit_report.py",
        "status": "modified",
        "additions": 19,
        "deletions": 15,
        "changes": 34,
        "patch": "@@ -45,6 +45,25 @@\n \n     report_repo_id = os.getenv(\"REPORT_REPO_ID\")\n \n+    with open(\"new_failures_with_bad_commit.json\") as fp:\n+        data = json.load(fp)\n+\n+    with open(f\"ci_results_{job_name}/job_links.json\") as fp:\n+        job_links = json.load(fp)\n+\n+    # Update `new_failures_with_bad_commit.json` with job links information before uploading to Hub repository\n+    #   - need to change `single-gpu` to `single` and same for `multi-gpu` to match the keys in `job_link`.\n+    for model, model_result in data.items():\n+        for device, failed_tests in model_result.items():\n+            for failed_test in failed_tests:\n+                key = model\n+                if list(job_links.keys()) == [job_name]:\n+                    key = job_name\n+                failed_test[\"job_link\"] = job_links[key][device.replace(\"-gpu\", \"\")]\n+\n+    with open(\"new_failures_with_bad_commit.json\", \"w\") as fp:\n+        json.dump(data, fp, indent=4, ensure_ascii=False)\n+\n     commit_info = api.upload_file(\n         path_or_fileobj=\"new_failures_with_bad_commit.json\",\n         path_in_repo=f\"{report_repo_folder}/ci_results_{job_name}/new_failures_with_bad_commit.json\",\n@@ -53,12 +72,6 @@\n         token=os.environ.get(\"TRANSFORMERS_CI_RESULTS_UPLOAD_TOKEN\", None),\n     )\n \n-    with open(\"new_failures_with_bad_commit.json\") as fp:\n-        data = json.load(fp)\n-\n-    with open(f\"ci_results_{job_name}/job_links.json\") as fp:\n-        job_links = json.load(fp)\n-\n     # TODO: extend\n     team_members = [\n         \"ArthurZucker\",\n@@ -101,16 +114,7 @@\n     for author, _data in new_data_full.items():\n         for model, model_result in _data.items():\n             for device, failed_tests in model_result.items():\n-                # prepare job_link and add it to each entry of new failed test information.\n-                # need to change from `single-gpu` to `single` and same for `multi-gpu` to match `job_link`.\n-                key = model\n-                if list(job_links.keys()) == [job_name]:\n-                    key = job_name\n-                job_link = job_links[key][device.replace(\"-gpu\", \"\")]\n-\n                 failed_tests = [x for x in failed_tests if x[\"author\"] == author or x[\"merged_by\"] == author]\n-                for x in failed_tests:\n-                    x.update({\"job_link\": job_link})\n                 model_result[device] = failed_tests\n             _data[model] = {k: v for k, v in model_result.items() if len(v) > 0}\n         new_data_full[author] = {k: v for k, v in _data.items() if len(v) > 0}"
      },
      {
        "filename": "utils/split_model_tests.py",
        "status": "modified",
        "additions": 14,
        "deletions": 5,
        "changes": 19,
        "patch": "@@ -40,10 +40,10 @@\n if __name__ == \"__main__\":\n     parser = argparse.ArgumentParser()\n     parser.add_argument(\n-        \"--models\",\n+        \"--subdirs\",\n         type=str,\n         default=\"\",\n-        help=\"the list of pre-computed model names.\",\n+        help=\"the list of pre-computed model names (directory names under `tests/models`) or directory names under `tests` (except `models`).\",\n     )\n     parser.add_argument(\n         \"--num_splits\",\n@@ -60,9 +60,18 @@\n     d1.remove(\"models\")\n     d = d2 + d1\n \n-    if args.models != \"\":\n-        model_tests = ast.literal_eval(args.models)\n-        d = sorted(filter(os.path.isdir, [f\"models/{x}\" for x in model_tests]))\n+    if args.subdirs != \"\":\n+        model_tests = ast.literal_eval(args.subdirs)\n+        # We handle both cases with and without prefix because `push-important-models.yml` returns the list without\n+        # the prefix (i.e. `models`) but `utils/pr_slow_ci_models.py` (called by `self-comment-ci.yml`) returns the\n+        # list with the prefix (`models`) and some directory names under `tests`.\n+        d = []\n+        for x in model_tests:\n+            if os.path.isdir(x):\n+                d.append(x)\n+            if os.path.isdir(f\"models/{x}\"):\n+                d.append(f\"models/{x}\")\n+        d = sorted(d)\n \n     num_jobs = len(d)\n     num_jobs_per_splits = num_jobs // args.num_splits"
      }
    ],
    "num_files": 12,
    "scraped_at": "2025-11-16T21:16:34.488800"
  },
  {
    "pr_number": 41897,
    "title": "[FPQuant] MXFP8 and MXFP4 backwards support",
    "body": "# What does this PR do?\r\n\r\nThis PR adds MXFP4 and MXFP8 backwards support in combination with MXFP4 forward, allowing for lightning-fast QAT on Blackwell GPUs.\r\n\r\nIt's blocked by `QuTLASS v0.2.0` and `fp_quant v0.3.0` release we're hoping to release within a day or two.\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker @Cyrilvallez\r\n- vision models: @yonigozlan @molbap\r\n- audio models: @eustlb @ebezzam @vasqu\r\n- multimodal models: @zucchini-nlp\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- generate: @zucchini-nlp (visual-language models) or @gante (all others)\r\n- continuous batching: @remi-or @ArthurZucker @McPatate\r\n- pipelines: @Rocketknight1\r\n- tokenizers: @ArthurZucker and @itazap\r\n- trainer: @SunMarc\r\n- attention: @vasqu @ArthurZucker @CyrilVallez\r\n- model loading (from pretrained, etc): @CyrilVallez\r\n- distributed: @3outeille @ArthurZucker\r\n- CIs: @ydshieh\r\n\r\nIntegrations:\r\n\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization: @SunMarc @MekkCyber\r\n- kernels: @MekkCyber @drbh\r\n- peft: @BenjaminBossan @githubnemo\r\n\r\nDevices/Backends:\r\n\r\n- AMD ROCm: @ivarflakstad\r\n- Intel XPU: @IlyasMoutawwakil\r\n- Ascend NPU: @ivarflakstad \r\n\r\nDocumentation: @stevhliu\r\n\r\nResearch projects are not maintained and should be taken as is.\r\n\r\n -->\r\n\r\n@SunMarc ",
    "html_url": "https://github.com/huggingface/transformers/pull/41897",
    "created_at": "2025-10-27T16:29:33Z",
    "merged_at": "2025-11-04T16:52:48Z",
    "merge_commit_sha": "020e713ac8e70bd2e72bcd12dc6bd1ada6162562",
    "base_ref": "main",
    "head_sha": "e3718018e1f600354bd92bbff751851cb7ce3e03",
    "user": "BlackSamorez",
    "files": [
      {
        "filename": "docker/transformers-quantization-latest-gpu/Dockerfile",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -81,7 +81,7 @@ RUN python3 -m pip uninstall -y flash-attn\n RUN cd transformers && python3 setup.py develop\n \n # Add fp-quant for quantization testing\n-RUN python3 -m pip install --no-cache-dir \"fp-quant>=0.2.0\"\n+RUN python3 -m pip install --no-cache-dir \"fp-quant>=0.3.2\"\n \n # Low usage or incompatible lib, will enable later on\n "
      },
      {
        "filename": "src/transformers/integrations/fp_quant.py",
        "status": "modified",
        "additions": 4,
        "deletions": 0,
        "changes": 4,
        "patch": "@@ -35,6 +35,10 @@ def adapt_fp_quant_config(config: FPQuantConfig):\n \n     if config.backward_dtype == \"bf16\":\n         backward_dtype = FPQuantDtype.BF16\n+    elif config.backward_dtype == \"mxfp8\":\n+        backward_dtype = FPQuantDtype.MXFP8\n+    elif config.backward_dtype == \"mxfp4\":\n+        backward_dtype = FPQuantDtype.MXFP4\n     else:\n         raise ValueError(f\"Unsupported backward dtype: {config.backward_dtype}\")\n "
      },
      {
        "filename": "src/transformers/utils/import_utils.py",
        "status": "modified",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "patch": "@@ -971,13 +971,13 @@ def is_quark_available() -> bool:\n @lru_cache\n def is_fp_quant_available():\n     is_available, fp_quant_version = _is_package_available(\"fp_quant\", return_version=True)\n-    return is_available and version.parse(fp_quant_version) >= version.parse(\"0.2.0\")\n+    return is_available and version.parse(fp_quant_version) >= version.parse(\"0.3.2\")\n \n \n @lru_cache\n def is_qutlass_available():\n     is_available, qutlass_version = _is_package_available(\"qutlass\", return_version=True)\n-    return is_available and version.parse(qutlass_version) >= version.parse(\"0.1.0\")\n+    return is_available and version.parse(qutlass_version) >= version.parse(\"0.2.0\")\n \n \n @lru_cache"
      },
      {
        "filename": "src/transformers/utils/quantization_config.py",
        "status": "modified",
        "additions": 6,
        "deletions": 2,
        "changes": 8,
        "patch": "@@ -1601,8 +1601,12 @@ def post_init(self):\n         else:\n             raise ValueError(\"Only 'mxfp4' and 'nvfp4' are supported for forward_dtype for now.\")\n \n-        if self.backward_dtype != \"bf16\":\n-            raise ValueError(\"Only 'bf16' is supported for backward_dtype for now.\")\n+        if self.backward_dtype not in [\"bf16\", \"mxfp8\", \"mxfp4\"]:\n+            raise ValueError(\"Only 'bf16', 'mxfp8' and 'mxfp4' are supported for backward_dtype for now.\")\n+\n+        if self.backward_dtype != \"bf16\" and self.forward_dtype != \"mxfp4\":\n+            raise ValueError(\"Only 'mxfp4' forward is compatible with non-bf16 backwards for now.\")\n+\n         if self.transform_init not in [\"hadamard\", \"identity\", \"gsr\"]:\n             raise ValueError(\"Only 'hadamard', 'identity' and 'gsr' are supported for transform_init.\")\n "
      },
      {
        "filename": "tests/quantization/fp_quant_integration/test_fp_quant.py",
        "status": "modified",
        "additions": 7,
        "deletions": 0,
        "changes": 7,
        "patch": "@@ -163,6 +163,13 @@ def getQuantizationConfig(cls):\n         return FPQuantConfig(forward_dtype=\"mxfp4\", pseudoquantization=False)\n \n \n+@require_qutlass\n+class FPQuantNVFP4Test(FPQuantBaseTest):\n+    @classmethod\n+    def getQuantizationConfig(cls):\n+        return FPQuantConfig(forward_dtype=\"nvfp4\", pseudoquantization=False)\n+\n+\n @require_qutlass\n class FPQuantMXFP4GS128Test(FPQuantBaseTest):\n     @classmethod"
      }
    ],
    "num_files": 5,
    "scraped_at": "2025-11-16T21:16:35.971124"
  },
  {
    "pr_number": 41892,
    "title": "Update some workflow files",
    "body": "# What does this PR do?\r\n\r\nMostly:\r\n\r\n- Make `docker/transformers-all-latest-gpu/Dockerfile` more readable and clean as we now need to handle `torchcodec` (using `cpu`) along with `torch` (`cuda`)\r\n\r\n- Remove `push-ci` stuff. We are not paying any attention to it. We have something running on a very small subset now.\r\n- Separate CI workflows and their docker images: with `flash-attn` and without it\r\n\r\n",
    "html_url": "https://github.com/huggingface/transformers/pull/41892",
    "created_at": "2025-10-27T12:26:34Z",
    "merged_at": "2025-10-29T13:42:05Z",
    "merge_commit_sha": "10d557123b42236dabfb70d40cf3d9ef57a445d0",
    "base_ref": "main",
    "head_sha": "7677b10bf70765721881edd2edd4b71739924645",
    "user": "ydshieh",
    "files": [
      {
        "filename": ".github/workflows/benchmark.yml",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -28,7 +28,7 @@ jobs:\n       (github.event_name == 'pull_request' && contains( github.event.pull_request.labels.*.name, 'run-benchmark') )||\r\n       (github.event_name == 'push' && github.ref == 'refs/heads/main')\r\n     container:\r\n-      image: huggingface/transformers-pytorch-gpu\r\n+      image: huggingface/transformers-all-latest-gpu\r\n       options: --gpus all --privileged --ipc host\r\n     steps:\r\n       - name: Get repo\r"
      },
      {
        "filename": ".github/workflows/benchmark_v2_a10_caller.yml",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -9,7 +9,7 @@ jobs:\n     uses: ./.github/workflows/benchmark_v2.yml\n     with:\n       runner: aws-g5-4xlarge-cache-use1-public-80\n-      container_image: huggingface/transformers-pytorch-gpu\n+      container_image: huggingface/transformers-all-latest-gpu\n       container_options: --gpus all --privileged --ipc host --shm-size \"16gb\"\n       commit_sha: ${{ github.sha }}\n       run_id: ${{ github.run_id }}"
      },
      {
        "filename": ".github/workflows/build-docker-images.yml",
        "status": "modified",
        "additions": 20,
        "deletions": 103,
        "changes": 123,
        "patch": "@@ -45,33 +45,20 @@ jobs:\n             REF=main\n           push: true\n           tags: huggingface/transformers-all-latest-gpu${{ inputs.image_postfix }}\n-      # Push CI images still need to be re-built daily\n-      -\n-        name: Build and push (for Push CI) in a daily basis\n-        # This condition allows `schedule` events, or `push` events that trigger this workflow NOT via `workflow_call`.\n-        # The later case is useful for manual image building for debugging purpose. Use another tag in this case!\n-        if: inputs.image_postfix != '-push-ci'\n-        uses: docker/build-push-action@v5\n-        with:\n-          context: ./docker/transformers-all-latest-gpu\n-          build-args: |\n-            REF=main\n-          push: true\n-          tags: huggingface/transformers-all-latest-gpu-push-ci\n \n       - name: Post to Slack\n         if: always()\n         uses: huggingface/hf-workflows/.github/actions/post-slack@main\n         with:\n           slack_channel: ${{ secrets.CI_SLACK_CHANNEL_DOCKER }}\n-          title: \ud83e\udd17 Results of the transformers-all-latest-gpu-push-ci docker build\n+          title: \ud83e\udd17 Results of the transformers-all-latest-gpu docker build\n           status: ${{ job.status }}\n           slack_token: ${{ secrets.SLACK_CIFEEDBACK_BOT_TOKEN }}\n \n-  latest-torch-deepspeed-docker:\n-    name: \"Latest PyTorch + DeepSpeed\"\n+  flash-attn-ci-image:\n+    name: \"PyTorch with Flash Attn [dev]\"\n     runs-on:\n-      group: aws-g4dn-2xlarge-cache\n+      group: aws-general-8-plus\n     steps:\n       -\n         name: Set up Docker Buildx\n@@ -89,26 +76,28 @@ jobs:\n         name: Build and push\n         uses: docker/build-push-action@v5\n         with:\n-          context: ./docker/transformers-pytorch-deepspeed-latest-gpu\n+          context: ./docker/transformers-all-latest-gpu\n           build-args: |\n-            REF=main\n+            REF=update_dockerfile\n+            PYTORCH=2.8.0\n+            TORCHCODEC=0.7.0\n+            FLASH_ATTN=yes\n           push: true\n-          tags: huggingface/transformers-pytorch-deepspeed-latest-gpu${{ inputs.image_postfix }}\n+          tags: huggingface/transformers-all-latest-gpu${{ inputs.image_postfix }}:flash-attn\n \n       - name: Post to Slack\n         if: always()\n         uses: huggingface/hf-workflows/.github/actions/post-slack@main\n         with:\n-          slack_channel: ${{ secrets.CI_SLACK_CHANNEL_DOCKER}}\n-          title: \ud83e\udd17 Results of the transformers-pytorch-deepspeed-latest-gpu docker build\n+          slack_channel: ${{ secrets.CI_SLACK_CHANNEL_DOCKER }}\n+          title: \ud83e\udd17 Results of the transformers-all-latest-gpu docker build\n           status: ${{ job.status }}\n           slack_token: ${{ secrets.SLACK_CIFEEDBACK_BOT_TOKEN }}\n \n-  # Can't build 2 images in a single job `latest-torch-deepspeed-docker` (for `nvcr.io/nvidia`)\n-  latest-torch-deepspeed-docker-for-push-ci-daily-build:\n-    name: \"Latest PyTorch + DeepSpeed (Push CI - Daily Build)\"\n+  latest-torch-deepspeed-docker:\n+    name: \"Latest PyTorch + DeepSpeed\"\n     runs-on:\n-      group: aws-general-8-plus\n+      group: aws-g4dn-2xlarge-cache\n     steps:\n       -\n         name: Set up Docker Buildx\n@@ -122,33 +111,27 @@ jobs:\n         with:\n           username: ${{ secrets.DOCKERHUB_USERNAME }}\n           password: ${{ secrets.DOCKERHUB_PASSWORD }}\n-      # Push CI images still need to be re-built daily\n       -\n-        name: Build and push (for Push CI) in a daily basis\n-        # This condition allows `schedule` events, or `push` events that trigger this workflow NOT via `workflow_call`.\n-        # The later case is useful for manual image building for debugging purpose. Use another tag in this case!\n-        if: inputs.image_postfix != '-push-ci'\n+        name: Build and push\n         uses: docker/build-push-action@v5\n         with:\n           context: ./docker/transformers-pytorch-deepspeed-latest-gpu\n           build-args: |\n             REF=main\n           push: true\n-          tags: huggingface/transformers-pytorch-deepspeed-latest-gpu-push-ci\n+          tags: huggingface/transformers-pytorch-deepspeed-latest-gpu${{ inputs.image_postfix }}\n \n       - name: Post to Slack\n         if: always()\n         uses: huggingface/hf-workflows/.github/actions/post-slack@main\n         with:\n-          slack_channel: ${{ secrets.CI_SLACK_CHANNEL_DOCKER }}\n-          title: \ud83e\udd17 Results of the transformers-pytorch-deepspeed-latest-gpu-push-ci docker build\n+          slack_channel: ${{ secrets.CI_SLACK_CHANNEL_DOCKER}}\n+          title: \ud83e\udd17 Results of the transformers-pytorch-deepspeed-latest-gpu docker build\n           status: ${{ job.status }}\n           slack_token: ${{ secrets.SLACK_CIFEEDBACK_BOT_TOKEN }}\n \n   doc-builder:\n     name: \"Doc builder\"\n-    # Push CI doesn't need this image\n-    if: inputs.image_postfix != '-push-ci'\n     runs-on:\n       group: aws-general-8-plus\n     steps:\n@@ -181,44 +164,6 @@ jobs:\n           status: ${{ job.status }}\n           slack_token: ${{ secrets.SLACK_CIFEEDBACK_BOT_TOKEN }}\n \n-  latest-pytorch:\n-    name: \"Latest PyTorch [dev]\"\n-    # Push CI doesn't need this image\n-    if: inputs.image_postfix != '-push-ci'\n-    runs-on:\n-      group: aws-general-8-plus\n-    steps:\n-      -\n-        name: Set up Docker Buildx\n-        uses: docker/setup-buildx-action@v3\n-      -\n-        name: Check out code\n-        uses: actions/checkout@v4\n-      -\n-        name: Login to DockerHub\n-        uses: docker/login-action@v3\n-        with:\n-          username: ${{ secrets.DOCKERHUB_USERNAME }}\n-          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n-      -\n-        name: Build and push\n-        uses: docker/build-push-action@v5\n-        with:\n-          context: ./docker/transformers-pytorch-gpu\n-          build-args: |\n-            REF=main\n-          push: true\n-          tags: huggingface/transformers-pytorch-gpu\n-\n-      - name: Post to Slack\n-        if: always()\n-        uses: huggingface/hf-workflows/.github/actions/post-slack@main\n-        with:\n-          slack_channel: ${{ secrets.CI_SLACK_CHANNEL_DOCKER }}\n-          title: \ud83e\udd17 Results of the huggingface/transformers-pytorch-gpudocker build\n-          status: ${{ job.status }}\n-          slack_token: ${{ secrets.SLACK_CIFEEDBACK_BOT_TOKEN }}\n-\n   latest-pytorch-amd:\n     name: \"Latest PyTorch (AMD) [dev]\"\n     runs-on:\n@@ -245,26 +190,13 @@ jobs:\n             REF=main\n           push: true\n           tags: huggingface/transformers-pytorch-amd-gpu${{ inputs.image_postfix }}\n-      # Push CI images still need to be re-built daily\n-      -\n-        name: Build and push (for Push CI) in a daily basis\n-        # This condition allows `schedule` events, or `push` events that trigger this workflow NOT via `workflow_call`.\n-        # The later case is useful for manual image building for debugging purpose. Use another tag in this case!\n-        if: inputs.image_postfix != '-push-ci'\n-        uses: docker/build-push-action@v5\n-        with:\n-          context: ./docker/transformers-pytorch-amd-gpu\n-          build-args: |\n-            REF=main\n-          push: true\n-          tags: huggingface/transformers-pytorch-amd-gpu-push-ci\n \n       - name: Post to Slack\n         if: always()\n         uses: huggingface/hf-workflows/.github/actions/post-slack@main\n         with:\n           slack_channel: ${{ secrets.CI_SLACK_CHANNEL_DOCKER }}\n-          title: \ud83e\udd17 Results of the huggingface/transformers-pytorch-amd-gpu-push-ci build\n+          title: \ud83e\udd17 Results of the huggingface/transformers-pytorch-amd-gpu build\n           status: ${{ job.status }}\n           slack_token: ${{ secrets.SLACK_CIFEEDBACK_BOT_TOKEN }}\n \n@@ -294,19 +226,6 @@ jobs:\n             REF=main\n           push: true\n           tags: huggingface/transformers-pytorch-deepspeed-amd-gpu${{ inputs.image_postfix }}\n-      # Push CI images still need to be re-built daily\n-      -\n-        name: Build and push (for Push CI) in a daily basis\n-        # This condition allows `schedule` events, or `push` events that trigger this workflow NOT via `workflow_call`.\n-        # The later case is useful for manual image building for debugging purpose. Use another tag in this case!\n-        if: inputs.image_postfix != '-push-ci'\n-        uses: docker/build-push-action@v5\n-        with:\n-          context: ./docker/transformers-pytorch-deepspeed-amd-gpu\n-          build-args: |\n-            REF=main\n-          push: true\n-          tags: huggingface/transformers-pytorch-deepspeed-amd-gpu-push-ci\n \n       - name: Post to Slack\n         if: always()\n@@ -319,8 +238,6 @@ jobs:\n \n   latest-quantization-torch-docker:\n     name: \"Latest Pytorch + Quantization [dev]\"\n-     # Push CI doesn't need this image\n-    if: inputs.image_postfix != '-push-ci'\n     runs-on:\n       group: aws-general-8-plus\n     steps:"
      },
      {
        "filename": ".github/workflows/push-important-models.yml",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -149,7 +149,7 @@ jobs:\n     with:\n       job: run_models_gpu\n       slack_report_channel: \"#transformers-ci-push\"\n-      docker: huggingface/transformers-all-latest-gpu\n+      docker: huggingface/transformers-all-latest-gpu:flash-attn\n       ci_event: push\n       report_repo_id: hf-internal-testing/transformers_ci_push\n       commit_sha: ${{ github.sha }}"
      },
      {
        "filename": ".github/workflows/self-push-amd-mi210-caller.yml",
        "status": "removed",
        "additions": 0,
        "deletions": 25,
        "changes": 25,
        "patch": "@@ -1,25 +0,0 @@\n-name: Self-hosted runner (AMD mi210 CI caller)\n-\n-on:\n-  #workflow_run:\n-  #  workflows: [\"Self-hosted runner (push-caller)\"]\n-  #  branches: [\"main\"]\n-  #  types: [completed]\n-  push:\n-    branches:\n-      - run_amd_push_ci_caller*\n-    paths:\n-      - \"src/**\"\n-      - \"tests/**\"\n-      - \".github/**\"\n-      - \"templates/**\"\n-      - \"utils/**\"\n-\n-jobs:\n-  run_amd_ci:\n-    name: AMD mi210\n-    if: (cancelled() != true) && ((github.event_name == 'workflow_run') || ((github.event_name == 'push') && startsWith(github.ref_name, 'run_amd_push_ci_caller')))\n-    uses: ./.github/workflows/self-push-amd.yml\n-    with:\n-      gpu_flavor: mi210\n-    secrets: inherit"
      },
      {
        "filename": ".github/workflows/self-push-amd-mi250-caller.yml",
        "status": "removed",
        "additions": 0,
        "deletions": 25,
        "changes": 25,
        "patch": "@@ -1,25 +0,0 @@\n-name: Self-hosted runner (AMD mi250 CI caller)\n-\n-on:\n-  #workflow_run:\n-  #  workflows: [\"Self-hosted runner (push-caller)\"]\n-  #  branches: [\"main\"]\n-  #  types: [completed]\n-  push:\n-    branches:\n-      - run_amd_push_ci_caller*\n-    paths:\n-      - \"src/**\"\n-      - \"tests/**\"\n-      - \".github/**\"\n-      - \"templates/**\"\n-      - \"utils/**\"\n-\n-jobs:\n-  run_amd_ci:\n-    name: AMD mi250\n-    if: (cancelled() != true) && ((github.event_name == 'workflow_run') || ((github.event_name == 'push') && startsWith(github.ref_name, 'run_amd_push_ci_caller')))\n-    uses: ./.github/workflows/self-push-amd.yml\n-    with:\n-      gpu_flavor: mi250\n-    secrets: inherit"
      },
      {
        "filename": ".github/workflows/self-push-amd.yml",
        "status": "removed",
        "additions": 0,
        "deletions": 334,
        "changes": 334,
        "patch": "@@ -1,334 +0,0 @@\n-name: Self-hosted runner AMD GPU (push)\n-\n-on:\n-  workflow_call:\n-    inputs:\n-      gpu_flavor:\n-        required: true\n-        type: string\n-\n-env:\n-  HF_HOME: /mnt/cache\n-  TRANSFORMERS_IS_CI: yes\n-  OMP_NUM_THREADS: 8\n-  MKL_NUM_THREADS: 8\n-  PYTEST_TIMEOUT: 60\n-  TF_FORCE_GPU_ALLOW_GROWTH: true\n-  HF_HUB_READ_TOKEN: ${{ secrets.HF_HUB_READ_TOKEN }}\n-\n-jobs:\n-  check_runner_status:\n-    name: Check Runner Status\n-    runs-on: ubuntu-22.04\n-    steps:\n-      - name: Checkout transformers\n-        uses: actions/checkout@v4\n-        with:\n-          fetch-depth: 2\n-\n-      - name: Check Runner Status\n-        run: python utils/check_self_hosted_runner.py --target_runners amd-mi210-single-gpu-ci-runner-docker --token ${{ secrets.ACCESS_REPO_INFO_TOKEN }}\n-\n-  check_runners:\n-    name: Check Runners\n-    needs: check_runner_status\n-    strategy:\n-      matrix:\n-        machine_type: [single-gpu, multi-gpu]\n-    runs-on: [self-hosted, amd-gpu, '${{ matrix.machine_type }}', '${{ inputs.gpu_flavor }}']\n-    container:\n-      image: huggingface/transformers-pytorch-amd-gpu-push-ci  # <--- We test only for PyTorch for now\n-      options: --device /dev/kfd --device /dev/dri --env ROCR_VISIBLE_DEVICES --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n-    steps:\n-      - name: ROCM-SMI\n-        run: |\n-          rocm-smi\n-      - name: ROCM-INFO\n-        run: |\n-          rocminfo  | grep \"Agent\" -A 14\n-      - name: Show ROCR environment\n-        run: |\n-          echo \"ROCR: $ROCR_VISIBLE_DEVICES\"\n-\n-  setup_gpu:\n-    name: Setup\n-    needs: check_runners\n-    strategy:\n-      matrix:\n-        machine_type: [single-gpu, multi-gpu]\n-    runs-on: [self-hosted, amd-gpu, '${{ matrix.machine_type }}', '${{ inputs.gpu_flavor }}']\n-    container:\n-      image: huggingface/transformers-pytorch-amd-gpu-push-ci  # <--- We test only for PyTorch for now\n-      options: --device /dev/kfd --device /dev/dri --env ROCR_VISIBLE_DEVICES --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n-    outputs:\n-      matrix: ${{ steps.set-matrix.outputs.matrix }}\n-      test_map: ${{ steps.set-matrix.outputs.test_map }}\n-    env:\n-      # `CI_BRANCH_PUSH`: The branch name from the push event\n-      # `CI_BRANCH_WORKFLOW_RUN`: The name of the branch on which this workflow is triggered by `workflow_run` event\n-      # `CI_SHA_PUSH`: The commit SHA from the push event\n-      # `CI_SHA_WORKFLOW_RUN`: The commit SHA that triggers this workflow by `workflow_run` event\n-      CI_BRANCH_PUSH: ${{ github.event.ref }}\n-      CI_BRANCH_WORKFLOW_RUN: ${{ github.event.workflow_run.head_branch }}\n-      CI_SHA_PUSH: ${{ github.event.head_commit.id }}\n-      CI_SHA_WORKFLOW_RUN: ${{ github.event.workflow_run.head_sha }}\n-    steps:\n-      # Necessary to get the correct branch name and commit SHA for `workflow_run` event\n-      # We also take into account the `push` event (we might want to test some changes in a branch)\n-      - name: Prepare custom environment variables\n-        shell: bash\n-        # `CI_BRANCH`: The non-empty branch name from the above two (one and only one of them is empty)\n-        # `CI_SHA`: The non-empty commit SHA from the above two (one and only one of them is empty)\n-        run: |\n-          CI_BRANCH_PUSH=${CI_BRANCH_PUSH/'refs/heads/'/''}\n-          echo $CI_BRANCH_PUSH\n-          echo $CI_BRANCH_WORKFLOW_RUN\n-          echo $CI_SHA_PUSH\n-          echo $CI_SHA_WORKFLOW_RUN\n-          [[ ! -z \"$CI_BRANCH_PUSH\" ]] && echo \"CI_BRANCH=$CI_BRANCH_PUSH\" >> $GITHUB_ENV || echo \"CI_BRANCH=$CI_BRANCH_WORKFLOW_RUN\" >> $GITHUB_ENV\n-          [[ ! -z \"$CI_SHA_PUSH\" ]] && echo \"CI_SHA=$CI_SHA_PUSH\" >> $GITHUB_ENV || echo \"CI_SHA=$CI_SHA_WORKFLOW_RUN\" >> $GITHUB_ENV\n-\n-      - name: print environment variables\n-        run: |\n-          echo \"env.CI_BRANCH = ${{ env.CI_BRANCH }}\"\n-          echo \"env.CI_SHA = ${{ env.CI_SHA }}\"\n-\n-      - name: Update clone using environment variables\n-        working-directory: /transformers\n-        run: |\n-          echo \"original branch = $(git branch --show-current)\"\n-          git fetch && git checkout ${{ env.CI_BRANCH }}\n-          echo \"updated branch = $(git branch --show-current)\"\n-          git checkout ${{ env.CI_SHA }}\n-          echo \"log = $(git log -n 1)\"\n-\n-      - name: Cleanup\n-        working-directory: /transformers\n-        run: |\n-          rm -rf tests/__pycache__\n-          rm -rf tests/models/__pycache__\n-          rm -rf reports\n-\n-      - name: Show installed libraries and their versions\n-        working-directory: /transformers\n-        run: pip freeze\n-\n-      - name: Fetch the tests to run\n-        working-directory: /transformers\n-        # TODO: add `git-python` in the docker images\n-        run: |\n-          pip install --upgrade git-python\n-          python3 utils/tests_fetcher.py --diff_with_last_commit | tee test_preparation.txt\n-\n-      - name: Report fetched tests\n-        uses: actions/upload-artifact@v4\n-        with:\n-          name: test_fetched\n-          path: /transformers/test_preparation.txt\n-\n-      - id: set-matrix\n-        name: Organize tests into models\n-        working-directory: /transformers\n-        # The `keys` is used as GitHub actions matrix for jobs, i.e. `models/bert`, `tokenization`, `pipeline`, etc.\n-        # The `test_map` is used to get the actual identified test files under each key.\n-        # If no test to run (so no `test_map.json` file), create a dummy map (empty matrix will fail)\n-        run: |\n-          if [ -f test_map.json ]; then\n-              keys=$(python3 -c 'import json; fp = open(\"test_map.json\"); test_map = json.load(fp); fp.close(); d = list(test_map.keys()); print(d)')\n-              test_map=$(python3 -c 'import json; fp = open(\"test_map.json\"); test_map = json.load(fp); fp.close(); print(test_map)')\n-          else\n-              keys=$(python3 -c 'keys = [\"dummy\"]; print(keys)')\n-              test_map=$(python3 -c 'test_map = {\"dummy\": []}; print(test_map)')\n-          fi\n-          echo $keys\n-          echo $test_map\n-          echo \"matrix=$keys\" >> $GITHUB_OUTPUT\n-          echo \"test_map=$test_map\" >> $GITHUB_OUTPUT\n-\n-  run_models_gpu:\n-    name: Model tests\n-    needs: setup_gpu\n-    # `dummy` means there is no test to run\n-    if: contains(fromJson(needs.setup_gpu.outputs.matrix), 'dummy') != true\n-    strategy:\n-      fail-fast: false\n-      matrix:\n-        folders: ${{ fromJson(needs.setup_gpu.outputs.matrix) }}\n-        machine_type: [single-gpu, multi-gpu]\n-    runs-on: [self-hosted, amd-gpu, '${{ matrix.machine_type }}', '${{ inputs.gpu_flavor }}']\n-    container:\n-      image: huggingface/transformers-pytorch-amd-gpu-push-ci  # <--- We test only for PyTorch for now\n-      options: --device /dev/kfd --device /dev/dri --env ROCR_VISIBLE_DEVICES --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n-    env:\n-      # For the meaning of these environment variables, see the job `Setup`\n-      CI_BRANCH_PUSH: ${{ github.event.ref }}\n-      CI_BRANCH_WORKFLOW_RUN: ${{ github.event.workflow_run.head_branch }}\n-      CI_SHA_PUSH: ${{ github.event.head_commit.id }}\n-      CI_SHA_WORKFLOW_RUN: ${{ github.event.workflow_run.head_sha }}\n-    steps:\n-      # Necessary to get the correct branch name and commit SHA for `workflow_run` event\n-      # We also take into account the `push` event (we might want to test some changes in a branch)\n-      - name: Prepare custom environment variables\n-        shell: bash\n-        # For the meaning of these environment variables, see the job `Setup`\n-        run: |\n-          CI_BRANCH_PUSH=${CI_BRANCH_PUSH/'refs/heads/'/''}\n-          echo $CI_BRANCH_PUSH\n-          echo $CI_BRANCH_WORKFLOW_RUN\n-          echo $CI_SHA_PUSH\n-          echo $CI_SHA_WORKFLOW_RUN\n-          [[ ! -z \"$CI_BRANCH_PUSH\" ]] && echo \"CI_BRANCH=$CI_BRANCH_PUSH\" >> $GITHUB_ENV || echo \"CI_BRANCH=$CI_BRANCH_WORKFLOW_RUN\" >> $GITHUB_ENV\n-          [[ ! -z \"$CI_SHA_PUSH\" ]] && echo \"CI_SHA=$CI_SHA_PUSH\" >> $GITHUB_ENV || echo \"CI_SHA=$CI_SHA_WORKFLOW_RUN\" >> $GITHUB_ENV\n-\n-      - name: print environment variables\n-        run: |\n-          echo \"env.CI_BRANCH = ${{ env.CI_BRANCH }}\"\n-          echo \"env.CI_SHA = ${{ env.CI_SHA }}\"\n-\n-      - name: Update clone using environment variables\n-        working-directory: /transformers\n-        run: |\n-          echo \"original branch = $(git branch --show-current)\"\n-          git fetch && git checkout ${{ env.CI_BRANCH }}\n-          echo \"updated branch = $(git branch --show-current)\"\n-          git checkout ${{ env.CI_SHA }}\n-          echo \"log = $(git log -n 1)\"\n-\n-      - name: Reinstall transformers in edit mode (remove the one installed during docker image build)\n-        working-directory: /transformers\n-        run: python3 -m pip uninstall -y transformers && python3 -m pip install -e .\n-\n-      - name: Echo folder ${{ matrix.folders }}\n-        shell: bash\n-        # For folders like `models/bert`, set an env. var. (`matrix_folders`) to `models_bert`, which will be used to\n-        # set the artifact folder names (because the character `/` is not allowed).\n-        run: |\n-          echo \"${{ matrix.folders }}\"\n-          echo \"${{ fromJson(needs.setup_gpu.outputs.test_map)[matrix.folders] }}\"\n-          matrix_folders=${{ matrix.folders }}\n-          matrix_folders=${matrix_folders/'models/'/'models_'}\n-          echo \"$matrix_folders\"\n-          echo \"matrix_folders=$matrix_folders\" >> $GITHUB_ENV\n-\n-      - name: ROCM-SMI\n-        run: |\n-          rocm-smi\n-      - name: ROCM-INFO\n-        run: |\n-          rocminfo  | grep \"Agent\" -A 14\n-      - name: Show ROCR environment\n-        run: |\n-          echo \"ROCR: $ROCR_VISIBLE_DEVICES\"\n-\n-      - name: Environment\n-        working-directory: /transformers\n-        run: |\n-          python3 utils/print_env.py\n-\n-      - name: Show installed libraries and their versions\n-        working-directory: /transformers\n-        run: pip freeze\n-\n-      - name: Run all non-slow selected tests on GPU\n-        working-directory: /transformers\n-        run: |\n-          python3 -m pytest -n 2 --dist=loadfile -v --make-reports=${{ matrix.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports ${{ fromJson(needs.setup_gpu.outputs.test_map)[matrix.folders] }} -m \"not not_device_test\"\n-\n-      - name: Failure short reports\n-        if: ${{ failure() }}\n-        continue-on-error: true\n-        run: cat /transformers/reports/${{ matrix.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports/failures_short.txt\n-\n-      - name: \"Test suite reports artifacts: ${{ matrix.machine_type }}_run_models_gpu_${{ env.matrix_folders }}_test_reports\"\n-        if: ${{ always() }}\n-        uses: actions/upload-artifact@v4\n-        with:\n-          name: ${{ matrix.machine_type }}_run_models_gpu_${{ env.matrix_folders }}_test_reports\n-          path: /transformers/reports/${{ matrix.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports\n-\n-  send_results:\n-    name: Send results to webhook\n-    runs-on: ubuntu-22.04\n-    if: always()\n-    needs: [\n-        check_runner_status,\n-        check_runners,\n-        setup_gpu,\n-        run_models_gpu,\n-#        run_tests_torch_cuda_extensions_single_gpu,\n-#        run_tests_torch_cuda_extensions_multi_gpu\n-    ]\n-    env:\n-      # For the meaning of these environment variables, see the job `Setup`\n-      CI_BRANCH_PUSH: ${{ github.event.ref }}\n-      CI_BRANCH_WORKFLOW_RUN: ${{ github.event.workflow_run.head_branch }}\n-      CI_SHA_PUSH: ${{ github.event.head_commit.id }}\n-      CI_SHA_WORKFLOW_RUN: ${{ github.event.workflow_run.head_sha }}\n-    steps:\n-      - name: Preliminary job status\n-        shell: bash\n-        # For the meaning of these environment variables, see the job `Setup`\n-        run: |\n-          echo \"Runner availability: ${{ needs.check_runner_status.result }}\"\n-          echo \"Setup status: ${{ needs.setup_gpu.result }}\"\n-          echo \"Runner status: ${{ needs.check_runners.result }}\"\n-\n-      # Necessary to get the correct branch name and commit SHA for `workflow_run` event\n-      # We also take into account the `push` event (we might want to test some changes in a branch)\n-      - name: Prepare custom environment variables\n-        shell: bash\n-        # For the meaning of these environment variables, see the job `Setup`\n-        run: |\n-          CI_BRANCH_PUSH=${CI_BRANCH_PUSH/'refs/heads/'/''}\n-          echo $CI_BRANCH_PUSH\n-          echo $CI_BRANCH_WORKFLOW_RUN\n-          echo $CI_SHA_PUSH\n-          echo $CI_SHA_WORKFLOW_RUN\n-          [[ ! -z \"$CI_BRANCH_PUSH\" ]] && echo \"CI_BRANCH=$CI_BRANCH_PUSH\" >> $GITHUB_ENV || echo \"CI_BRANCH=$CI_BRANCH_WORKFLOW_RUN\" >> $GITHUB_ENV\n-          [[ ! -z \"$CI_SHA_PUSH\" ]] && echo \"CI_SHA=$CI_SHA_PUSH\" >> $GITHUB_ENV || echo \"CI_SHA=$CI_SHA_WORKFLOW_RUN\" >> $GITHUB_ENV\n-\n-      - name: print environment variables\n-        run: |\n-          echo \"env.CI_BRANCH = ${{ env.CI_BRANCH }}\"\n-          echo \"env.CI_SHA = ${{ env.CI_SHA }}\"\n-\n-      - uses: actions/checkout@v4\n-        # To avoid failure when multiple commits are merged into `main` in a short period of time.\n-        # Checking out to an old commit beyond the fetch depth will get an error `fatal: reference is not a tree: ...\n-        # (Only required for `workflow_run` event, where we get the latest HEAD on `main` instead of the event commit)\n-        with:\n-          fetch-depth: 20\n-\n-      - name: Update clone using environment variables\n-        run: |\n-          echo \"original branch = $(git branch --show-current)\"\n-          git fetch && git checkout ${{ env.CI_BRANCH }}\n-          echo \"updated branch = $(git branch --show-current)\"\n-          git checkout ${{ env.CI_SHA }}\n-          echo \"log = $(git log -n 1)\"\n-\n-      - uses: actions/download-artifact@v4\n-      - name: Send message to Slack\n-        env:\n-          CI_SLACK_BOT_TOKEN: ${{ secrets.CI_SLACK_BOT_TOKEN }}\n-          CI_SLACK_CHANNEL_ID: ${{ secrets.CI_SLACK_CHANNEL_ID }}\n-          CI_SLACK_CHANNEL_ID_DAILY: ${{ secrets.CI_SLACK_CHANNEL_ID_DAILY }}\n-          CI_SLACK_CHANNEL_ID_AMD: ${{ secrets.CI_SLACK_CHANNEL_ID_AMD }}\n-          CI_SLACK_CHANNEL_DUMMY_TESTS: ${{ secrets.CI_SLACK_CHANNEL_DUMMY_TESTS }}\n-          CI_SLACK_REPORT_CHANNEL_ID: ${{ secrets.CI_SLACK_CHANNEL_ID_AMD }}\n-          ACCESS_REPO_INFO_TOKEN: ${{ secrets.ACCESS_REPO_INFO_TOKEN }}\n-          CI_EVENT: Push CI (AMD) - ${{ inputs.gpu_flavor }}\n-          CI_TITLE_PUSH: ${{ github.event.head_commit.message }}\n-          CI_TITLE_WORKFLOW_RUN: ${{ github.event.workflow_run.head_commit.message }}\n-          CI_SHA: ${{ env.CI_SHA }}\n-          RUNNER_STATUS: ${{ needs.check_runner_status.result }}\n-          RUNNER_ENV_STATUS: ${{ needs.check_runners.result }}\n-          SETUP_STATUS: ${{ needs.setup_gpu.result }}\n-\n-        # We pass `needs.setup_gpu.outputs.matrix` as the argument. A processing in `notification_service.py` to change\n-        # `models/bert` to `models_bert` is required, as the artifact names use `_` instead of `/`.\n-        run: |\n-          pip install huggingface_hub\n-          pip install slack_sdk\n-          pip show slack_sdk\n-          python utils/notification_service.py \"${{ needs.setup_gpu.outputs.matrix }}\""
      },
      {
        "filename": ".github/workflows/self-push-caller.yml",
        "status": "removed",
        "additions": 0,
        "deletions": 54,
        "changes": 54,
        "patch": "@@ -1,54 +0,0 @@\n-# Used to trigger self-push CI\n-name: Self-hosted runner (push-caller)\n-\n-on:\n-  push:\n-    branches:\n-      - main\n-    paths:\n-      - \"src/**\"\n-      - \"tests/**\"\n-      - \".github/**\"\n-      - \"templates/**\"\n-      - \"utils/**\"\n-\n-jobs:\n-  check-for-setup:\n-      runs-on: ubuntu-22.04\n-      name: Check if setup was changed\n-      outputs:\n-        changed: ${{ steps.was_changed.outputs.changed }}\n-      steps:\n-        - uses: actions/checkout@v4\n-          with: \n-            fetch-depth: \"2\"\n-        \n-        - name: Get changed files\n-          id: changed-files\n-          uses: tj-actions/changed-files@1c8e6069583811afb28f97afeaf8e7da80c6be5c\n-        \n-        - name: Was setup changed \n-          id: was_changed\n-          run: |\n-            for file in ${{ steps.changed-files.outputs.all_changed_files }}; do\n-              if [ `basename \"${file}\"` = \"setup.py\" ]; then\n-                echo \"changed=1\" >> $GITHUB_OUTPUT\n-              fi\n-            done\n-\n-  build-docker-containers:\n-    needs: check-for-setup\n-    if: (github.event_name == 'push') && (needs.check-for-setup.outputs.changed == '1')\n-    uses: ./.github/workflows/build-docker-images.yml\n-    with:\n-      image_postfix: \"-push-ci\"\n-    secrets: inherit\n-\n-  run_push_ci:\n-    name: Trigger Push CI\n-    runs-on: ubuntu-22.04\n-    if: ${{ always() }}\n-    needs: build-docker-containers\n-    steps:\n-      - name: Trigger push CI via workflow_run\n-        run: echo \"Trigger push CI via workflow_run\""
      },
      {
        "filename": ".github/workflows/self-push.yml",
        "status": "removed",
        "additions": 0,
        "deletions": 652,
        "changes": 652,
        "patch": "@@ -1,652 +0,0 @@\n-name: Self-hosted runner (push)\n-\n-on:\n-  workflow_run:\n-    workflows: [\"Self-hosted runner (push-caller)\"]\n-    branches: [\"main\"]\n-    types: [completed]\n-  push:\n-    branches:\n-      - ci_*\n-      - ci-*\n-    paths:\n-      - \"src/**\"\n-      - \"tests/**\"\n-      - \".github/**\"\n-      - \"templates/**\"\n-      - \"utils/**\"\n-  repository_dispatch:\n-\n-env:\n-  HF_HOME: /mnt/cache\n-  TRANSFORMERS_IS_CI: yes\n-  OMP_NUM_THREADS: 8\n-  MKL_NUM_THREADS: 8\n-  PYTEST_TIMEOUT: 60\n-  TF_FORCE_GPU_ALLOW_GROWTH: true\n-  CUDA_VISIBLE_DEVICES: 0,1\n-\n-jobs:\n-  setup:\n-    name: Setup\n-    strategy:\n-      matrix:\n-        machine_type: [aws-g5-4xlarge-cache, aws-g5-12xlarge-cache]\n-    runs-on:\n-      group: '${{ matrix.machine_type }}'\n-    container:\n-      image: huggingface/transformers-all-latest-gpu-push-ci\n-      options: --gpus all --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n-    outputs:\n-      matrix: ${{ steps.set-matrix.outputs.matrix }}\n-      test_map: ${{ steps.set-matrix.outputs.test_map }}\n-    env:\n-      # `CI_BRANCH_PUSH`: The branch name from the push event\n-      # `CI_BRANCH_WORKFLOW_RUN`: The name of the branch on which this workflow is triggered by `workflow_run` event\n-      # `CI_SHA_PUSH`: The commit SHA from the push event\n-      # `CI_SHA_WORKFLOW_RUN`: The commit SHA that triggers this workflow by `workflow_run` event\n-      CI_BRANCH_PUSH: ${{ github.event.ref }}\n-      CI_BRANCH_WORKFLOW_RUN: ${{ github.event.workflow_run.head_branch }}\n-      CI_SHA_PUSH: ${{ github.event.head_commit.id }}\n-      CI_SHA_WORKFLOW_RUN: ${{ github.event.workflow_run.head_sha }}\n-    steps:\n-      # Necessary to get the correct branch name and commit SHA for `workflow_run` event\n-      # We also take into account the `push` event (we might want to test some changes in a branch)\n-      - name: Prepare custom environment variables\n-        shell: bash\n-        # `CI_BRANCH`: The non-empty branch name from the above two (one and only one of them is empty)\n-        # `CI_SHA`: The non-empty commit SHA from the above two (one and only one of them is empty)\n-        run: |\n-          CI_BRANCH_PUSH=${CI_BRANCH_PUSH/'refs/heads/'/''}\n-          echo $CI_BRANCH_PUSH\n-          echo $CI_BRANCH_WORKFLOW_RUN\n-          echo $CI_SHA_PUSH\n-          echo $CI_SHA_WORKFLOW_RUN\n-          [[ ! -z \"$CI_BRANCH_PUSH\" ]] && echo \"CI_BRANCH=$CI_BRANCH_PUSH\" >> $GITHUB_ENV || echo \"CI_BRANCH=$CI_BRANCH_WORKFLOW_RUN\" >> $GITHUB_ENV\n-          [[ ! -z \"$CI_SHA_PUSH\" ]] && echo \"CI_SHA=$CI_SHA_PUSH\" >> $GITHUB_ENV || echo \"CI_SHA=$CI_SHA_WORKFLOW_RUN\" >> $GITHUB_ENV\n-\n-      - name: print environment variables\n-        run: |\n-          echo \"env.CI_BRANCH = ${{ env.CI_BRANCH }}\"\n-          echo \"env.CI_SHA = ${{ env.CI_SHA }}\"\n-\n-      - name: Update clone using environment variables\n-        working-directory: /transformers\n-        run: |\n-          echo \"original branch = $(git branch --show-current)\"\n-          git fetch && git checkout ${{ env.CI_BRANCH }}\n-          echo \"updated branch = $(git branch --show-current)\"\n-          git checkout ${{ env.CI_SHA }}\n-          echo \"log = $(git log -n 1)\"\n-\n-      - name: Cleanup\n-        working-directory: /transformers\n-        run: |\n-          rm -rf tests/__pycache__\n-          rm -rf tests/models/__pycache__\n-          rm -rf reports\n-\n-      - name: Show installed libraries and their versions\n-        working-directory: /transformers\n-        run: pip freeze\n-\n-      - name: Fetch the tests to run\n-        working-directory: /transformers\n-        # TODO: add `git-python` in the docker images\n-        run: |\n-          pip install --upgrade git-python\n-          python3 utils/tests_fetcher.py --diff_with_last_commit | tee test_preparation.txt\n-\n-      - name: Report fetched tests\n-        uses: actions/upload-artifact@v4\n-        with:\n-          name: test_fetched\n-          path: /transformers/test_preparation.txt\n-\n-      - id: set-matrix\n-        name: Organize tests into models\n-        working-directory: /transformers\n-        # The `keys` is used as GitHub actions matrix for jobs, i.e. `models/bert`, `tokenization`, `pipeline`, etc.\n-        # The `test_map` is used to get the actual identified test files under each key.\n-        # If no test to run (so no `test_map.json` file), create a dummy map (empty matrix will fail)\n-        run: |\n-          if [ -f test_map.json ]; then\n-              keys=$(python3 -c 'import json; fp = open(\"test_map.json\"); test_map = json.load(fp); fp.close(); d = list(test_map.keys()); print(d)')\n-              test_map=$(python3 -c 'import json; fp = open(\"test_map.json\"); test_map = json.load(fp); fp.close(); print(test_map)')\n-          else\n-              keys=$(python3 -c 'keys = [\"dummy\"]; print(keys)')\n-              test_map=$(python3 -c 'test_map = {\"dummy\": []}; print(test_map)')\n-          fi\n-          echo $keys\n-          echo $test_map\n-          echo \"matrix=$keys\" >> $GITHUB_OUTPUT\n-          echo \"test_map=$test_map\" >> $GITHUB_OUTPUT\n-\n-  run_tests_single_gpu:\n-    name: Model tests\n-    needs: setup\n-    # `dummy` means there is no test to run\n-    if: contains(fromJson(needs.setup.outputs.matrix), 'dummy') != true\n-    strategy:\n-      fail-fast: false\n-      matrix:\n-        folders: ${{ fromJson(needs.setup.outputs.matrix) }}\n-        machine_type: [aws-g5-4xlarge-cache]\n-    runs-on:\n-      group: '${{ matrix.machine_type }}'\n-    container:\n-      image: huggingface/transformers-all-latest-gpu-push-ci\n-      options: --gpus all --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n-    env:\n-      # For the meaning of these environment variables, see the job `Setup`\n-      CI_BRANCH_PUSH: ${{ github.event.ref }}\n-      CI_BRANCH_WORKFLOW_RUN: ${{ github.event.workflow_run.head_branch }}\n-      CI_SHA_PUSH: ${{ github.event.head_commit.id }}\n-      CI_SHA_WORKFLOW_RUN: ${{ github.event.workflow_run.head_sha }}\n-    steps:\n-      # Necessary to get the correct branch name and commit SHA for `workflow_run` event\n-      # We also take into account the `push` event (we might want to test some changes in a branch)\n-      - name: Prepare custom environment variables\n-        shell: bash\n-        # For the meaning of these environment variables, see the job `Setup`\n-        run: |\n-          CI_BRANCH_PUSH=${CI_BRANCH_PUSH/'refs/heads/'/''}\n-          echo $CI_BRANCH_PUSH\n-          echo $CI_BRANCH_WORKFLOW_RUN\n-          echo $CI_SHA_PUSH\n-          echo $CI_SHA_WORKFLOW_RUN\n-          [[ ! -z \"$CI_BRANCH_PUSH\" ]] && echo \"CI_BRANCH=$CI_BRANCH_PUSH\" >> $GITHUB_ENV || echo \"CI_BRANCH=$CI_BRANCH_WORKFLOW_RUN\" >> $GITHUB_ENV\n-          [[ ! -z \"$CI_SHA_PUSH\" ]] && echo \"CI_SHA=$CI_SHA_PUSH\" >> $GITHUB_ENV || echo \"CI_SHA=$CI_SHA_WORKFLOW_RUN\" >> $GITHUB_ENV\n-\n-      - name: print environment variables\n-        run: |\n-          echo \"env.CI_BRANCH = ${{ env.CI_BRANCH }}\"\n-          echo \"env.CI_SHA = ${{ env.CI_SHA }}\"\n-\n-      - name: Set `machine_type` for report and artifact names\n-        working-directory: /transformers\n-        shell: bash\n-        run: |\n-          echo \"${{ matrix.machine_type }}\"\n-\n-          if [ \"${{ matrix.machine_type }}\" = \"aws-g5-4xlarge-cache\" ]; then\n-            machine_type=single-gpu\n-          elif [ \"${{ matrix.machine_type }}\" = \"aws-g5-12xlarge-cache\" ]; then\n-            machine_type=multi-gpu\n-          else\n-            machine_type=${{ matrix.machine_type }}\n-          fi\n-\n-          echo \"$machine_type\"\n-          echo \"machine_type=$machine_type\" >> $GITHUB_ENV\n-\n-      - name: Update clone using environment variables\n-        working-directory: /transformers\n-        run: |\n-          echo \"original branch = $(git branch --show-current)\"\n-          git fetch && git checkout ${{ env.CI_BRANCH }}\n-          echo \"updated branch = $(git branch --show-current)\"\n-          git checkout ${{ env.CI_SHA }}\n-          echo \"log = $(git log -n 1)\"\n-\n-      - name: Reinstall transformers in edit mode (remove the one installed during docker image build)\n-        working-directory: /transformers\n-        run: python3 -m pip uninstall -y transformers && python3 -m pip install -e .\n-\n-      - name: Echo folder ${{ matrix.folders }}\n-        shell: bash\n-        # For folders like `models/bert`, set an env. var. (`matrix_folders`) to `models_bert`, which will be used to\n-        # set the artifact folder names (because the character `/` is not allowed).\n-        run: |\n-          echo \"${{ matrix.folders }}\"\n-          echo \"${{ fromJson(needs.setup.outputs.test_map)[matrix.folders] }}\"\n-          matrix_folders=${{ matrix.folders }}\n-          matrix_folders=${matrix_folders/'models/'/'models_'}\n-          echo \"$matrix_folders\"\n-          echo \"matrix_folders=$matrix_folders\" >> $GITHUB_ENV\n-\n-      - name: NVIDIA-SMI\n-        run: |\n-          nvidia-smi\n-\n-      - name: Environment\n-        working-directory: /transformers\n-        run: |\n-          python3 utils/print_env.py\n-\n-      - name: Show installed libraries and their versions\n-        working-directory: /transformers\n-        run: pip freeze\n-\n-      - name: Run all non-slow selected tests on GPU\n-        working-directory: /transformers\n-        run: |\n-          python3 -m pytest -n 2 --dist=loadfile -v --make-reports=${{ env.machine_type }}_tests_gpu_${{ matrix.folders }} ${{ fromJson(needs.setup.outputs.test_map)[matrix.folders] }}\n-\n-      - name: Failure short reports\n-        if: ${{ failure() }}\n-        continue-on-error: true\n-        run: cat /transformers/reports/${{ env.machine_type }}_tests_gpu_${{ matrix.folders }}/failures_short.txt\n-\n-      - name: \"Test suite reports artifacts: ${{ env.machine_type }}_run_all_tests_gpu_${{ env.matrix_folders }}_test_reports\"\n-        if: ${{ always() }}\n-        uses: actions/upload-artifact@v4\n-        with:\n-          name: ${{ env.machine_type }}_run_all_tests_gpu_${{ env.matrix_folders }}_test_reports\n-          path: /transformers/reports/${{ env.machine_type }}_tests_gpu_${{ matrix.folders }}\n-\n-  run_tests_multi_gpu:\n-    name: Model tests\n-    needs: setup\n-    # `dummy` means there is no test to run\n-    if: contains(fromJson(needs.setup.outputs.matrix), 'dummy') != true\n-    strategy:\n-      fail-fast: false\n-      matrix:\n-        folders: ${{ fromJson(needs.setup.outputs.matrix) }}\n-        machine_type: [aws-g5-12xlarge-cache]\n-    runs-on:\n-      group: '${{ matrix.machine_type }}'\n-    container:\n-      image: huggingface/transformers-all-latest-gpu-push-ci\n-      options: --gpus all --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n-    env:\n-      # For the meaning of these environment variables, see the job `Setup`\n-      CI_BRANCH_PUSH: ${{ github.event.ref }}\n-      CI_BRANCH_WORKFLOW_RUN: ${{ github.event.workflow_run.head_branch }}\n-      CI_SHA_PUSH: ${{ github.event.head_commit.id }}\n-      CI_SHA_WORKFLOW_RUN: ${{ github.event.workflow_run.head_sha }}\n-    steps:\n-      # Necessary to get the correct branch name and commit SHA for `workflow_run` event\n-      # We also take into account the `push` event (we might want to test some changes in a branch)\n-      - name: Prepare custom environment variables\n-        shell: bash\n-        # For the meaning of these environment variables, see the job `Setup`\n-        run: |\n-          CI_BRANCH_PUSH=${CI_BRANCH_PUSH/'refs/heads/'/''}\n-          echo $CI_BRANCH_PUSH\n-          echo $CI_BRANCH_WORKFLOW_RUN\n-          echo $CI_SHA_PUSH\n-          echo $CI_SHA_WORKFLOW_RUN\n-          [[ ! -z \"$CI_BRANCH_PUSH\" ]] && echo \"CI_BRANCH=$CI_BRANCH_PUSH\" >> $GITHUB_ENV || echo \"CI_BRANCH=$CI_BRANCH_WORKFLOW_RUN\" >> $GITHUB_ENV\n-          [[ ! -z \"$CI_SHA_PUSH\" ]] && echo \"CI_SHA=$CI_SHA_PUSH\" >> $GITHUB_ENV || echo \"CI_SHA=$CI_SHA_WORKFLOW_RUN\" >> $GITHUB_ENV\n-\n-      - name: print environment variables\n-        run: |\n-          echo \"env.CI_BRANCH = ${{ env.CI_BRANCH }}\"\n-          echo \"env.CI_SHA = ${{ env.CI_SHA }}\"\n-\n-      - name: Set `machine_type` for report and artifact names\n-        working-directory: /transformers\n-        shell: bash\n-        run: |\n-          echo \"${{ matrix.machine_type }}\"\n-\n-          if [ \"${{ matrix.machine_type }}\" = \"aws-g5-4xlarge-cache\" ]; then\n-            machine_type=single-gpu\n-          elif [ \"${{ matrix.machine_type }}\" = \"aws-g5-12xlarge-cache\" ]; then\n-            machine_type=multi-gpu\n-          else\n-            machine_type=${{ matrix.machine_type }}\n-          fi\n-\n-          echo \"$machine_type\"\n-          echo \"machine_type=$machine_type\" >> $GITHUB_ENV\n-\n-      - name: Update clone using environment variables\n-        working-directory: /transformers\n-        run: |\n-          echo \"original branch = $(git branch --show-current)\"\n-          git fetch && git checkout ${{ env.CI_BRANCH }}\n-          echo \"updated branch = $(git branch --show-current)\"\n-          git checkout ${{ env.CI_SHA }}\n-          echo \"log = $(git log -n 1)\"\n-\n-      - name: Reinstall transformers in edit mode (remove the one installed during docker image build)\n-        working-directory: /transformers\n-        run: python3 -m pip uninstall -y transformers && python3 -m pip install -e .\n-\n-      - name: Echo folder ${{ matrix.folders }}\n-        shell: bash\n-        # For folders like `models/bert`, set an env. var. (`matrix_folders`) to `models_bert`, which will be used to\n-        # set the artifact folder names (because the character `/` is not allowed).\n-        run: |\n-          echo \"${{ matrix.folders }}\"\n-          echo \"${{ fromJson(needs.setup.outputs.test_map)[matrix.folders] }}\"\n-          matrix_folders=${{ matrix.folders }}\n-          matrix_folders=${matrix_folders/'models/'/'models_'}\n-          echo \"$matrix_folders\"\n-          echo \"matrix_folders=$matrix_folders\" >> $GITHUB_ENV\n-\n-      - name: NVIDIA-SMI\n-        run: |\n-          nvidia-smi\n-\n-      - name: Environment\n-        working-directory: /transformers\n-        run: |\n-          python3 utils/print_env.py\n-\n-      - name: Show installed libraries and their versions\n-        working-directory: /transformers\n-        run: pip freeze\n-\n-      - name: Run all non-slow selected tests on GPU\n-        env:\n-          MKL_SERVICE_FORCE_INTEL: 1\n-        working-directory: /transformers\n-        run: |\n-          python3 -m pytest -n 2 --dist=loadfile -v --make-reports=${{ env.machine_type }}_tests_gpu_${{ matrix.folders }} ${{ fromJson(needs.setup.outputs.test_map)[matrix.folders] }}\n-\n-      - name: Failure short reports\n-        if: ${{ failure() }}\n-        continue-on-error: true\n-        run: cat /transformers/reports/${{ env.machine_type }}_tests_gpu_${{ matrix.folders }}/failures_short.txt\n-\n-      - name: \"Test suite reports artifacts: ${{ env.machine_type }}_run_all_tests_gpu_${{ env.matrix_folders }}_test_reports\"\n-        if: ${{ always() }}\n-        uses: actions/upload-artifact@v4\n-        with:\n-          name: ${{ env.machine_type }}_run_all_tests_gpu_${{ env.matrix_folders }}_test_reports\n-          path: /transformers/reports/${{ env.machine_type }}_tests_gpu_${{ matrix.folders }}\n-\n-  run_tests_torch_cuda_extensions_single_gpu:\n-    name: Torch CUDA extension tests\n-    needs: setup\n-    if: contains(fromJson(needs.setup.outputs.matrix), 'deepspeed') || contains(fromJson(needs.setup.outputs.matrix), 'extended')\n-    strategy:\n-      fail-fast: false\n-      matrix:\n-        machine_type: [aws-g5-4xlarge-cache]\n-    runs-on:\n-      group: '${{ matrix.machine_type }}'\n-    container:\n-      image: huggingface/transformers-pytorch-deepspeed-latest-gpu-push-ci\n-      options: --gpus all --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n-    env:\n-      # For the meaning of these environment variables, see the job `Setup`\n-      CI_BRANCH_PUSH: ${{ github.event.ref }}\n-      CI_BRANCH_WORKFLOW_RUN: ${{ github.event.workflow_run.head_branch }}\n-      CI_SHA_PUSH: ${{ github.event.head_commit.id }}\n-      CI_SHA_WORKFLOW_RUN: ${{ github.event.workflow_run.head_sha }}\n-    steps:\n-      # Necessary to get the correct branch name and commit SHA for `workflow_run` event\n-      # We also take into account the `push` event (we might want to test some changes in a branch)\n-      - name: Prepare custom environment variables\n-        shell: bash\n-        # For the meaning of these environment variables, see the job `Setup`\n-        run: |\n-          CI_BRANCH_PUSH=${CI_BRANCH_PUSH/'refs/heads/'/''}\n-          echo $CI_BRANCH_PUSH\n-          echo $CI_BRANCH_WORKFLOW_RUN\n-          echo $CI_SHA_PUSH\n-          echo $CI_SHA_WORKFLOW_RUN\n-          [[ ! -z \"$CI_BRANCH_PUSH\" ]] && echo \"CI_BRANCH=$CI_BRANCH_PUSH\" >> $GITHUB_ENV || echo \"CI_BRANCH=$CI_BRANCH_WORKFLOW_RUN\" >> $GITHUB_ENV\n-          [[ ! -z \"$CI_SHA_PUSH\" ]] && echo \"CI_SHA=$CI_SHA_PUSH\" >> $GITHUB_ENV || echo \"CI_SHA=$CI_SHA_WORKFLOW_RUN\" >> $GITHUB_ENV\n-\n-      - name: print environment variables\n-        run: |\n-          echo \"env.CI_BRANCH = ${{ env.CI_BRANCH }}\"\n-          echo \"env.CI_SHA = ${{ env.CI_SHA }}\"\n-\n-      - name: Set `machine_type` for report and artifact names\n-        working-directory: /workspace/transformers\n-        shell: bash\n-        run: |\n-          echo \"${{ matrix.machine_type }}\"\n-\n-          if [ \"${{ matrix.machine_type }}\" = \"aws-g5-4xlarge-cache\" ]; then\n-            machine_type=single-gpu\n-          elif [ \"${{ matrix.machine_type }}\" = \"aws-g5-12xlarge-cache\" ]; then\n-            machine_type=multi-gpu\n-          else\n-            machine_type=${{ matrix.machine_type }}\n-          fi\n-\n-          echo \"$machine_type\"\n-          echo \"machine_type=$machine_type\" >> $GITHUB_ENV\n-\n-      - name: Update clone using environment variables\n-        working-directory: /workspace/transformers\n-        run: |\n-          echo \"original branch = $(git branch --show-current)\"\n-          git fetch && git checkout ${{ env.CI_BRANCH }}\n-          echo \"updated branch = $(git branch --show-current)\"\n-          git checkout ${{ env.CI_SHA }}\n-          echo \"log = $(git log -n 1)\"\n-\n-      - name: Reinstall transformers in edit mode (remove the one installed during docker image build)\n-        working-directory: /workspace/transformers\n-        run: python3 -m pip uninstall -y transformers && python3 -m pip install -e .\n-\n-      - name: Remove cached torch extensions\n-        run: rm -rf /github/home/.cache/torch_extensions/\n-\n-      # To avoid unknown test failures\n-      - name: Pre build DeepSpeed *again*\n-        working-directory: /workspace\n-        run: |\n-          python3 -m pip uninstall -y deepspeed\n-          DS_BUILD_CPU_ADAM=1 DS_BUILD_FUSED_ADAM=1 python3 -m pip install deepspeed --global-option=\"build_ext\" --global-option=\"-j8\" --no-cache -v --disable-pip-version-check\n-\n-      - name: NVIDIA-SMI\n-        run: |\n-          nvidia-smi\n-\n-      - name: Environment\n-        working-directory: /workspace/transformers\n-        run: |\n-          python utils/print_env.py\n-\n-      - name: Show installed libraries and their versions\n-        working-directory: /workspace/transformers\n-        run: pip freeze\n-\n-      - name: Run all non-slow selected tests on GPU\n-        working-directory: /workspace/transformers\n-        # TODO: Here we pass all tests in the 2 folders for simplicity. It's better to pass only the identified tests.\n-        run: |\n-          python -m pytest -n 1 --dist=loadfile -v --make-reports=${{ env.machine_type }}_run_torch_cuda_extensions_gpu_test_reports tests/deepspeed tests/extended\n-\n-      - name: Failure short reports\n-        if: ${{ failure() }}\n-        continue-on-error: true\n-        run: cat /workspace/transformers/reports/${{ env.machine_type }}_run_torch_cuda_extensions_gpu_test_reports/failures_short.txt\n-\n-      - name: \"Test suite reports artifacts: ${{ env.machine_type }}_run_torch_cuda_extensions_gpu_test_reports\"\n-        if: ${{ always() }}\n-        uses: actions/upload-artifact@v4\n-        with:\n-          name: ${{ env.machine_type }}_run_torch_cuda_extensions_gpu_test_reports\n-          path: /workspace/transformers/reports/${{ env.machine_type }}_run_torch_cuda_extensions_gpu_test_reports\n-\n-  run_tests_torch_cuda_extensions_multi_gpu:\n-    name: Torch CUDA extension tests\n-    needs: setup\n-    if: contains(fromJson(needs.setup.outputs.matrix), 'deepspeed') || contains(fromJson(needs.setup.outputs.matrix), 'extended')\n-    strategy:\n-      fail-fast: false\n-      matrix:\n-        machine_type: [aws-g5-12xlarge-cache]\n-    runs-on:\n-      group: '${{ matrix.machine_type }}'\n-    container:\n-      image: huggingface/transformers-pytorch-deepspeed-latest-gpu-push-ci\n-      options: --gpus all --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n-    env:\n-      # For the meaning of these environment variables, see the job `Setup`\n-      CI_BRANCH_PUSH: ${{ github.event.ref }}\n-      CI_BRANCH_WORKFLOW_RUN: ${{ github.event.workflow_run.head_branch }}\n-      CI_SHA_PUSH: ${{ github.event.head_commit.id }}\n-      CI_SHA_WORKFLOW_RUN: ${{ github.event.workflow_run.head_sha }}\n-    steps:\n-      # Necessary to get the correct branch name and commit SHA for `workflow_run` event\n-      # We also take into account the `push` event (we might want to test some changes in a branch)\n-      - name: Prepare custom environment variables\n-        shell: bash\n-        # For the meaning of these environment variables, see the job `Setup`\n-        run: |\n-          CI_BRANCH_PUSH=${CI_BRANCH_PUSH/'refs/heads/'/''}\n-          echo $CI_BRANCH_PUSH\n-          echo $CI_BRANCH_WORKFLOW_RUN\n-          echo $CI_SHA_PUSH\n-          echo $CI_SHA_WORKFLOW_RUN\n-          [[ ! -z \"$CI_BRANCH_PUSH\" ]] && echo \"CI_BRANCH=$CI_BRANCH_PUSH\" >> $GITHUB_ENV || echo \"CI_BRANCH=$CI_BRANCH_WORKFLOW_RUN\" >> $GITHUB_ENV\n-          [[ ! -z \"$CI_SHA_PUSH\" ]] && echo \"CI_SHA=$CI_SHA_PUSH\" >> $GITHUB_ENV || echo \"CI_SHA=$CI_SHA_WORKFLOW_RUN\" >> $GITHUB_ENV\n-\n-      - name: print environment variables\n-        run: |\n-          echo \"env.CI_BRANCH = ${{ env.CI_BRANCH }}\"\n-          echo \"env.CI_SHA = ${{ env.CI_SHA }}\"\n-\n-      - name: Set `machine_type` for report and artifact names\n-        working-directory: /workspace/transformers\n-        shell: bash\n-        run: |\n-          echo \"${{ matrix.machine_type }}\"\n-\n-          if [ \"${{ matrix.machine_type }}\" = \"aws-g5-4xlarge-cache\" ]; then\n-            machine_type=single-gpu\n-          elif [ \"${{ matrix.machine_type }}\" = \"aws-g5-12xlarge-cache\" ]; then\n-            machine_type=multi-gpu\n-          else\n-            machine_type=${{ matrix.machine_type }}\n-          fi\n-\n-          echo \"$machine_type\"\n-          echo \"machine_type=$machine_type\" >> $GITHUB_ENV\n-\n-      - name: Update clone using environment variables\n-        working-directory: /workspace/transformers\n-        run: |\n-          echo \"original branch = $(git branch --show-current)\"\n-          git fetch && git checkout ${{ env.CI_BRANCH }}\n-          echo \"updated branch = $(git branch --show-current)\"\n-          git checkout ${{ env.CI_SHA }}\n-          echo \"log = $(git log -n 1)\"\n-\n-      - name: Reinstall transformers in edit mode (remove the one installed during docker image build)\n-        working-directory: /workspace/transformers\n-        run: python3 -m pip uninstall -y transformers && python3 -m pip install -e .\n-\n-      - name: Remove cached torch extensions\n-        run: rm -rf /github/home/.cache/torch_extensions/\n-\n-      # To avoid unknown test failures\n-      - name: Pre build DeepSpeed *again*\n-        working-directory: /workspace\n-        run: |\n-          python3 -m pip uninstall -y deepspeed\n-          DS_BUILD_CPU_ADAM=1 DS_BUILD_FUSED_ADAM=1 python3 -m pip install deepspeed --global-option=\"build_ext\" --global-option=\"-j8\" --no-cache -v --disable-pip-version-check\n-\n-      - name: NVIDIA-SMI\n-        run: |\n-          nvidia-smi\n-\n-      - name: Environment\n-        working-directory: /workspace/transformers\n-        run: |\n-          python utils/print_env.py\n-\n-      - name: Show installed libraries and their versions\n-        working-directory: /workspace/transformers\n-        run: pip freeze\n-\n-      - name: Run all non-slow selected tests on GPU\n-        working-directory: /workspace/transformers\n-        # TODO: Here we pass all tests in the 2 folders for simplicity. It's better to pass only the identified tests.\n-        run: |\n-          python -m pytest -n 1 --dist=loadfile -v --make-reports=${{ env.machine_type }}_run_torch_cuda_extensions_gpu_test_reports tests/deepspeed tests/extended\n-\n-      - name: Failure short reports\n-        if: ${{ failure() }}\n-        continue-on-error: true\n-        run: cat /workspace/transformers/reports/${{ env.machine_type }}_run_torch_cuda_extensions_gpu_test_reports/failures_short.txt\n-\n-      - name: \"Test suite reports artifacts: ${{ env.machine_type }}_run_torch_cuda_extensions_gpu_test_reports\"\n-        if: ${{ always() }}\n-        uses: actions/upload-artifact@v4\n-        with:\n-          name: ${{ env.machine_type }}_run_torch_cuda_extensions_gpu_test_reports\n-          path: /workspace/transformers/reports/${{ env.machine_type }}_run_torch_cuda_extensions_gpu_test_reports\n-\n-  send_results:\n-    name: Send results to webhook\n-    runs-on: ubuntu-22.04\n-    if: always()\n-    needs: [\n-        setup,\n-        run_tests_single_gpu,\n-        run_tests_multi_gpu,\n-        run_tests_torch_cuda_extensions_single_gpu,\n-        run_tests_torch_cuda_extensions_multi_gpu\n-    ]\n-    env:\n-      # For the meaning of these environment variables, see the job `Setup`\n-      CI_BRANCH_PUSH: ${{ github.event.ref }}\n-      CI_BRANCH_WORKFLOW_RUN: ${{ github.event.workflow_run.head_branch }}\n-      CI_SHA_PUSH: ${{ github.event.head_commit.id }}\n-      CI_SHA_WORKFLOW_RUN: ${{ github.event.workflow_run.head_sha }}\n-    steps:\n-      - name: Preliminary job status\n-        shell: bash\n-        # For the meaning of these environment variables, see the job `Setup`\n-        run: |\n-          echo \"Setup status: ${{ needs.setup.result }}\"\n-\n-      # Necessary to get the correct branch name and commit SHA for `workflow_run` event\n-      # We also take into account the `push` event (we might want to test some changes in a branch)\n-      - name: Prepare custom environment variables\n-        shell: bash\n-        # For the meaning of these environment variables, see the job `Setup`\n-        run: |\n-          CI_BRANCH_PUSH=${CI_BRANCH_PUSH/'refs/heads/'/''}\n-          echo $CI_BRANCH_PUSH\n-          echo $CI_BRANCH_WORKFLOW_RUN\n-          echo $CI_SHA_PUSH\n-          echo $CI_SHA_WORKFLOW_RUN\n-          [[ ! -z \"$CI_BRANCH_PUSH\" ]] && echo \"CI_BRANCH=$CI_BRANCH_PUSH\" >> $GITHUB_ENV || echo \"CI_BRANCH=$CI_BRANCH_WORKFLOW_RUN\" >> $GITHUB_ENV\n-          [[ ! -z \"$CI_SHA_PUSH\" ]] && echo \"CI_SHA=$CI_SHA_PUSH\" >> $GITHUB_ENV || echo \"CI_SHA=$CI_SHA_WORKFLOW_RUN\" >> $GITHUB_ENV\n-\n-      - name: print environment variables\n-        run: |\n-          echo \"env.CI_BRANCH = ${{ env.CI_BRANCH }}\"\n-          echo \"env.CI_SHA = ${{ env.CI_SHA }}\"\n-\n-      - uses: actions/checkout@v4\n-        # To avoid failure when multiple commits are merged into `main` in a short period of time.\n-        # Checking out to an old commit beyond the fetch depth will get an error `fatal: reference is not a tree: ...\n-        # (Only required for `workflow_run` event, where we get the latest HEAD on `main` instead of the event commit)\n-        with:\n-          fetch-depth: 20\n-\n-      - name: Update clone using environment variables\n-        run: |\n-          echo \"original branch = $(git branch --show-current)\"\n-          git fetch && git checkout ${{ env.CI_BRANCH }}\n-          echo \"updated branch = $(git branch --show-current)\"\n-          git checkout ${{ env.CI_SHA }}\n-          echo \"log = $(git log -n 1)\"\n-\n-      - uses: actions/download-artifact@v4\n-      - name: Send message to Slack\n-        env:\n-          CI_SLACK_BOT_TOKEN: ${{ secrets.CI_SLACK_BOT_TOKEN }}\n-          CI_SLACK_CHANNEL_ID: ${{ secrets.CI_SLACK_CHANNEL_ID }}\n-          CI_SLACK_CHANNEL_ID_DAILY: ${{ secrets.CI_SLACK_CHANNEL_ID_DAILY }}\n-          CI_SLACK_CHANNEL_DUMMY_TESTS: ${{ secrets.CI_SLACK_CHANNEL_DUMMY_TESTS }}\n-          CI_SLACK_REPORT_CHANNEL_ID: ${{ secrets.CI_SLACK_CHANNEL_ID }}\n-          ACCESS_REPO_INFO_TOKEN: ${{ secrets.ACCESS_REPO_INFO_TOKEN }}\n-          CI_EVENT: push\n-          CI_TITLE_PUSH: ${{ github.event.head_commit.message }}\n-          CI_TITLE_WORKFLOW_RUN: ${{ github.event.workflow_run.head_commit.message }}\n-          CI_SHA: ${{ env.CI_SHA }}\n-          SETUP_STATUS: ${{ needs.setup.result }}\n-\n-        # We pass `needs.setup.outputs.matrix` as the argument. A processing in `notification_service.py` to change\n-        # `models/bert` to `models_bert` is required, as the artifact names use `_` instead of `/`.\n-        run: |\n-          pip install huggingface_hub\n-          pip install slack_sdk\n-          pip show slack_sdk\n-          python utils/notification_service.py \"${{ needs.setup.outputs.matrix }}\""
      },
      {
        "filename": ".github/workflows/self-scheduled-caller.yml",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -63,7 +63,7 @@ jobs:\n     with:\n       job: run_pipelines_torch_gpu\n       slack_report_channel: \"#transformers-ci-daily-pipeline-torch\"\n-      docker: huggingface/transformers-pytorch-gpu\n+      docker: huggingface/transformers-all-latest-gpu\n       ci_event: Daily CI\n       report_repo_id: hf-internal-testing/transformers_daily_ci\n       commit_sha: ${{ github.sha }}"
      },
      {
        "filename": ".github/workflows/self-scheduled-flash-attn-caller.yml",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -51,7 +51,7 @@ jobs:\n     with:\n       job: run_models_gpu\n       slack_report_channel: \"#transformers-ci-flash-attn\"\n-      docker: huggingface/transformers-all-latest-gpu\n+      docker: huggingface/transformers-all-latest-gpu:flash-attn\n       ci_event: Daily CI\n       runner_type: \"a10\"\n       report_repo_id: hf-internal-testing/transformers_flash_attn_ci"
      },
      {
        "filename": ".github/workflows/self-scheduled.yml",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -165,7 +165,7 @@ jobs:\n     runs-on:\n       group: '${{ matrix.machine_type }}'\n     container:\n-      image: huggingface/transformers-pytorch-gpu\n+      image: huggingface/transformers-all-latest-gpu\n       options: --gpus all --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n     steps:\n       - name: Update clone"
      },
      {
        "filename": "docker/transformers-all-latest-gpu/Dockerfile",
        "status": "modified",
        "additions": 44,
        "deletions": 6,
        "changes": 50,
        "patch": "@@ -9,10 +9,15 @@ SHELL [\"sh\", \"-lc\"]\n # The following `ARG` are mainly used to specify the versions explicitly & directly in this docker file, and not meant\n # to be used as arguments for docker build (so far).\n \n-ARG PYTORCH='2.8.0'\n+ARG PYTORCH='2.9.0'\n # Example: `cu102`, `cu113`, etc.\n ARG CUDA='cu126'\n \n+# This needs to be compatible with the above `PYTORCH`.\n+ARG TORCHCODEC='0.8.0'\n+\n+ARG FLASH_ATTN='false'\n+\n RUN apt update\n RUN apt install -y git libsndfile1-dev tesseract-ocr espeak-ng python3 python3-pip ffmpeg git-lfs\n RUN git lfs install\n@@ -21,11 +26,44 @@ RUN python3 -m pip install --no-cache-dir --upgrade pip\n ARG REF=main\n RUN git clone https://github.com/huggingface/transformers && cd transformers && git checkout $REF\n \n+RUN python3 -m pip install --no-cache-dir -e ./transformers[dev]\n+\n # 1. Put several commands in a single `RUN` to avoid image/layer exporting issue. Could be revised in the future.\n-# 2. Regarding `torch` part, We might need to specify proper versions for `torchvision` and `torchaudio`.\n-#    Currently, let's not bother to specify their versions explicitly (so installed with their latest release versions).\n-# 3. For `torchcodec<0.8`: this is quickly added as torch 2.9.0 + torchcodec 0.8.0 fails on our CI env. Need to remove later once they work.\n-RUN python3 -m pip install --no-cache-dir -e ./transformers[dev,onnxruntime] && [ ${#PYTORCH} -gt 0 -a \"$PYTORCH\" != \"pre\" ] && VERSION='torch=='$PYTORCH'.*' ||  VERSION='torch'; echo \"export VERSION='$VERSION'\" >> ~/.profile && echo torch=$VERSION && [ \"$PYTORCH\" != \"pre\" ] && python3 -m pip install --no-cache-dir -U $VERSION torchvision torchaudio \"torchcodec<0.8\" --extra-index-url https://download.pytorch.org/whl/$CUDA || python3 -m pip install --no-cache-dir -U --pre torch torchvision torchaudio torchcodec --extra-index-url https://download.pytorch.org/whl/nightly/$CUDA\n+# 2. For `torchcodec`, use `cpu` as we don't have `libnvcuvid.so` on the host runner. See https://github.com/meta-pytorch/torchcodec/issues/912\n+#    **Important**: We need to specify `torchcodec` version if the torch version is not the latest stable one.\n+# 3. `set -e` means \"exit immediately if any command fails\".\n+RUN set -e; \\\n+    # Determine torch version\n+    if [ ${#PYTORCH} -gt 0 ] && [ \"$PYTORCH\" != \"pre\" ]; then \\\n+        VERSION=\"torch==${PYTORCH}.*\"; \\\n+        TORCHCODEC_VERSION=\"torchcodec==${TORCHCODEC}.*\"; \\\n+    else \\\n+        VERSION=\"torch\"; \\\n+        TORCHCODEC_VERSION=\"torchcodec\"; \\\n+    fi; \\\n+    \\\n+    # Log the version being installed\n+    echo \"Installing torch version: $VERSION\"; \\\n+    \\\n+    # Install PyTorch packages\n+    if [ \"$PYTORCH\" != \"pre\" ]; then \\\n+        python3 -m pip install --no-cache-dir -U \\\n+            $VERSION \\\n+            torchvision \\\n+            torchaudio \\\n+            --extra-index-url https://download.pytorch.org/whl/$CUDA; \\\n+        # We need to specify the version if the torch version is not the latest stable one.\n+        python3 -m pip install --no-cache-dir -U \\\n+            $TORCHCODEC_VERSION --extra-index-url https://download.pytorch.org/whl/cpu; \\\n+    else \\\n+        python3 -m pip install --no-cache-dir -U --pre \\\n+            torch \\\n+            torchvision \\\n+            torchaudio \\\n+            --extra-index-url https://download.pytorch.org/whl/nightly/$CUDA; \\\n+        python3 -m pip install --no-cache-dir -U --pre \\\n+            torchcodec --extra-index-url https://download.pytorch.org/whl/nightly/cpu; \\\n+    fi\n \n RUN python3 -m pip install --no-cache-dir -U timm\n \n@@ -54,7 +92,7 @@ RUN python3 -m pip install --no-cache-dir bitsandbytes\n RUN python3 -m pip install --no-cache-dir quanto\n \n # After using A10 as CI runner, let's run FA2 tests\n-RUN [ \"$PYTORCH\" != \"pre\" ] && python3 -m pip uninstall -y ninja && python3 -m pip install --no-cache-dir ninja && python3 -m pip install flash-attn --no-cache-dir --no-build-isolation || echo \"Don't install FA2 with nightly torch\"\n+RUN [ \"$FLASH_ATTN\" != \"false\" ] && python3 -m pip uninstall -y ninja && python3 -m pip install --no-cache-dir ninja && python3 -m pip install flash-attn --no-cache-dir --no-build-isolation || echo \"Don't install FA2 with nightly torch\"\n \n # TODO (ydshieh): check this again\n # `quanto` will install `ninja` which leads to many `CUDA error: an illegal memory access ...` in some model tests"
      }
    ],
    "num_files": 13,
    "scraped_at": "2025-11-16T21:16:36.540517"
  },
  {
    "pr_number": 41864,
    "title": "Fix AutoImageProcessor.register and documentation in auto processing modules",
    "body": "# What does this PR do?\r\n\r\nThis PR fixes several issues in the auto processing modules:\r\n\r\n1. **Fixes `AutoImageProcessor.register` bug**: Removes incorrect validation logic that was copied from tokenizers. The validation checked for `slow_image_processor_class` attribute consistency, but fast image processors don't have this attribute (unlike fast tokenizers), causing the `register` method to fail when registering custom image processors.\r\n\r\n2. **Fixes documentation errors**: Corrects copy-paste errors in docstrings where \"tokenizer\" was incorrectly used instead of the appropriate processor type (feature extractor, image processor, video processor).\r\n\r\n3. **Fixes typos**: Corrects \"fine\" \u2192 \"find\" in comments across multiple auto modules.\r\n\r\n4. **Improves `AutoVideoProcessor` trust handling**: Adds proper upstream repository extraction from `video_processor_auto_map` when resolving trust_remote_code.\r\n\r\n## Changes by file:\r\n\r\n- `feature_extraction_auto.py`: Fixed typo and corrected docstring to reference feature extractors instead of tokenizers\r\n- `image_processing_auto.py`: Removed incorrect `slow_image_processor_class` validation and fixed import in docstring example\r\n- `processing_auto.py`: Fixed typo in comment\r\n- `tokenization_auto.py`: Fixed typo in comment  \r\n- `video_processing_auto.py`: Fixed docstring reference and added upstream repo handling for trust_remote_code\r\n\r\n## Before submitting\r\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [x] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\n@ArthurZucker @Rocketknight1 (auto modules and processing)\r\n\r\n",
    "html_url": "https://github.com/huggingface/transformers/pull/41864",
    "created_at": "2025-10-25T19:33:20Z",
    "merged_at": "2025-11-06T07:43:07Z",
    "merge_commit_sha": "32e49f2884cdc23c172513d1a2cafe0f03255591",
    "base_ref": "main",
    "head_sha": "17312eb3bb331a16b28737c9075dcbe65c545d0a",
    "user": "MilkClouds",
    "files": [
      {
        "filename": "src/transformers/models/auto/feature_extraction_auto.py",
        "status": "modified",
        "additions": 13,
        "deletions": 13,
        "changes": 26,
        "patch": "@@ -93,7 +93,7 @@ def feature_extractor_class_from_name(class_name: str):\n         if getattr(extractor, \"__name__\", None) == class_name:\n             return extractor\n \n-    # We did not fine the class, but maybe it's because a dep is missing. In that case, the class will be in the main\n+    # We did not find the class, but maybe it's because a dep is missing. In that case, the class will be in the main\n     # init and we return the proper dummy to get an appropriate error message.\n     main_module = importlib.import_module(\"transformers\")\n     if hasattr(main_module, class_name):\n@@ -113,7 +113,7 @@ def get_feature_extractor_config(\n     **kwargs,\n ):\n     \"\"\"\n-    Loads the tokenizer configuration from a pretrained model tokenizer configuration.\n+    Loads the feature extractor configuration from a pretrained model feature extractor configuration.\n \n     Args:\n         pretrained_model_name_or_path (`str` or `os.PathLike`):\n@@ -122,7 +122,7 @@ def get_feature_extractor_config(\n             - a string, the *model id* of a pretrained model configuration hosted inside a model repo on\n               huggingface.co.\n             - a path to a *directory* containing a configuration file saved using the\n-              [`~PreTrainedTokenizer.save_pretrained`] method, e.g., `./my_model_directory/`.\n+              [`~FeatureExtractionMixin.save_pretrained`] method, e.g., `./my_model_directory/`.\n \n         cache_dir (`str` or `os.PathLike`, *optional*):\n             Path to a directory in which a downloaded pretrained model configuration should be cached if the standard\n@@ -141,7 +141,7 @@ def get_feature_extractor_config(\n             git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\n             identifier allowed by git.\n         local_files_only (`bool`, *optional*, defaults to `False`):\n-            If `True`, will only try to load the tokenizer configuration from local files.\n+            If `True`, will only try to load the feature extractor configuration from local files.\n \n     <Tip>\n \n@@ -150,22 +150,22 @@ def get_feature_extractor_config(\n     </Tip>\n \n     Returns:\n-        `Dict`: The configuration of the tokenizer.\n+        `Dict`: The configuration of the feature extractor.\n \n     Examples:\n \n     ```python\n     # Download configuration from huggingface.co and cache.\n-    tokenizer_config = get_tokenizer_config(\"google-bert/bert-base-uncased\")\n-    # This model does not have a tokenizer config so the result will be an empty dict.\n-    tokenizer_config = get_tokenizer_config(\"FacebookAI/xlm-roberta-base\")\n+    feature_extractor_config = get_feature_extractor_config(\"facebook/wav2vec2-base-960h\")\n+    # This model does not have a feature extractor config so the result will be an empty dict.\n+    feature_extractor_config = get_feature_extractor_config(\"FacebookAI/xlm-roberta-base\")\n \n-    # Save a pretrained tokenizer locally and you can reload its config\n-    from transformers import AutoTokenizer\n+    # Save a pretrained feature extractor locally and you can reload its config\n+    from transformers import AutoFeatureExtractor\n \n-    tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n-    tokenizer.save_pretrained(\"tokenizer-test\")\n-    tokenizer_config = get_tokenizer_config(\"tokenizer-test\")\n+    feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n+    feature_extractor.save_pretrained(\"feature-extractor-test\")\n+    feature_extractor_config = get_feature_extractor_config(\"feature-extractor-test\")\n     ```\"\"\"\n     resolved_config_file = cached_file(\n         pretrained_model_name_or_path,"
      },
      {
        "filename": "src/transformers/models/auto/image_processing_auto.py",
        "status": "modified",
        "additions": 2,
        "deletions": 15,
        "changes": 17,
        "patch": "@@ -260,7 +260,7 @@ def get_image_processor_config(\n             - a string, the *model id* of a pretrained model configuration hosted inside a model repo on\n               huggingface.co.\n             - a path to a *directory* containing a configuration file saved using the\n-              [`~PreTrainedTokenizer.save_pretrained`] method, e.g., `./my_model_directory/`.\n+              [`~ProcessorMixin.save_pretrained`] method, e.g., `./my_model_directory/`.\n \n         cache_dir (`str` or `os.PathLike`, *optional*):\n             Path to a directory in which a downloaded pretrained model configuration should be cached if the standard\n@@ -299,7 +299,7 @@ def get_image_processor_config(\n     image_processor_config = get_image_processor_config(\"FacebookAI/xlm-roberta-base\")\n \n     # Save a pretrained image processor locally and you can reload its config\n-    from transformers import AutoTokenizer\n+    from transformers import AutoImageProcessor\n \n     image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n     image_processor.save_pretrained(\"image-processor-test\")\n@@ -629,19 +629,6 @@ def register(\n         ):\n             raise ValueError(\"The `fast_image_processor_class` should inherit from `BaseImageProcessorFast`.\")\n \n-        if (\n-            slow_image_processor_class is not None\n-            and fast_image_processor_class is not None\n-            and issubclass(fast_image_processor_class, BaseImageProcessorFast)\n-            and fast_image_processor_class.slow_image_processor_class != slow_image_processor_class\n-        ):\n-            raise ValueError(\n-                \"The fast processor class you are passing has a `slow_image_processor_class` attribute that is not \"\n-                \"consistent with the slow processor class you passed (fast tokenizer has \"\n-                f\"{fast_image_processor_class.slow_image_processor_class} and you passed {slow_image_processor_class}. Fix one of those \"\n-                \"so they match!\"\n-            )\n-\n         # Avoid resetting a set slow/fast image processor if we are passing just the other ones.\n         if config_class in IMAGE_PROCESSOR_MAPPING._extra_content:\n             existing_slow, existing_fast = IMAGE_PROCESSOR_MAPPING[config_class]"
      },
      {
        "filename": "src/transformers/models/auto/processing_auto.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -175,7 +175,7 @@ def processor_class_from_name(class_name: str):\n         if getattr(processor, \"__name__\", None) == class_name:\n             return processor\n \n-    # We did not fine the class, but maybe it's because a dep is missing. In that case, the class will be in the main\n+    # We did not find the class, but maybe it's because a dep is missing. In that case, the class will be in the main\n     # init and we return the proper dummy to get an appropriate error message.\n     main_module = importlib.import_module(\"transformers\")\n     if hasattr(main_module, class_name):"
      },
      {
        "filename": "src/transformers/models/auto/tokenization_auto.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -815,7 +815,7 @@ def tokenizer_class_from_name(class_name: str) -> Union[type[Any], None]:\n             if getattr(tokenizer, \"__name__\", None) == class_name:\n                 return tokenizer\n \n-    # We did not fine the class, but maybe it's because a dep is missing. In that case, the class will be in the main\n+    # We did not find the class, but maybe it's because a dep is missing. In that case, the class will be in the main\n     # init and we return the proper dummy to get an appropriate error message.\n     main_module = importlib.import_module(\"transformers\")\n     if hasattr(main_module, class_name):"
      },
      {
        "filename": "src/transformers/models/auto/video_processing_auto.py",
        "status": "modified",
        "additions": 9,
        "deletions": 4,
        "changes": 13,
        "patch": "@@ -122,7 +122,7 @@ def get_video_processor_config(\n             - a string, the *model id* of a pretrained model configuration hosted inside a model repo on\n               huggingface.co.\n             - a path to a *directory* containing a configuration file saved using the\n-              [`~PreTrainedTokenizer.save_pretrained`] method, e.g., `./my_model_directory/`.\n+              [`~BaseVideoProcessor.save_pretrained`] method, e.g., `./my_model_directory/`.\n \n         cache_dir (`str` or `os.PathLike`, *optional*):\n             Path to a directory in which a downloaded pretrained model configuration should be cached if the standard\n@@ -313,9 +313,14 @@ def from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs):\n \n         has_remote_code = video_processor_auto_map is not None\n         has_local_code = video_processor_class is not None or type(config) in VIDEO_PROCESSOR_MAPPING\n-        trust_remote_code = resolve_trust_remote_code(\n-            trust_remote_code, pretrained_model_name_or_path, has_local_code, has_remote_code\n-        )\n+        if has_remote_code:\n+            if \"--\" in video_processor_auto_map:\n+                upstream_repo = video_processor_auto_map.split(\"--\")[0]\n+            else:\n+                upstream_repo = None\n+            trust_remote_code = resolve_trust_remote_code(\n+                trust_remote_code, pretrained_model_name_or_path, has_local_code, has_remote_code, upstream_repo\n+            )\n \n         if has_remote_code and trust_remote_code:\n             class_ref = video_processor_auto_map"
      }
    ],
    "num_files": 5,
    "scraped_at": "2025-11-16T21:16:41.495801"
  },
  {
    "pr_number": 41857,
    "title": "CI workflow for Flash Attn",
    "body": "# What does this PR do?\r\n\r\nAs discussed with @vasqu ",
    "html_url": "https://github.com/huggingface/transformers/pull/41857",
    "created_at": "2025-10-25T07:39:45Z",
    "merged_at": "2025-10-25T07:45:47Z",
    "merge_commit_sha": "e2e8dbed13c6a8455fd85c15c9fa91c99d609010",
    "base_ref": "main",
    "head_sha": "25872bd20caadf6157a34d0b6694a0e4244de601",
    "user": "ydshieh",
    "files": [
      {
        "filename": ".github/workflows/model_jobs.yml",
        "status": "modified",
        "additions": 4,
        "deletions": 1,
        "changes": 5,
        "patch": "@@ -28,6 +28,9 @@ on:\n       report_repo_id:\n         required: false\n         type: string\n+      pytest_marker:\n+        required: false\n+        type: string\n \n env:\n   HF_HOME: /mnt/cache\n@@ -137,7 +140,7 @@ jobs:\n       - name: Run all tests on GPU\n         working-directory: /transformers\n         run: |\n-          script -q -c \"PATCH_TESTING_METHODS_TO_COLLECT_OUTPUTS=yes _PATCHED_TESTING_METHODS_OUTPUT_DIR=/transformers/reports/${{ env.machine_type }}_${{ inputs.report_name_prefix }}_${{ env.matrix_folders }}_test_reports python3 -m pytest -rsfE -v --make-reports=${{ env.machine_type }}_${{ inputs.report_name_prefix }}_${{ env.matrix_folders }}_test_reports tests/${{ matrix.folders }}\" test_outputs.txt\n+          script -q -c \"PATCH_TESTING_METHODS_TO_COLLECT_OUTPUTS=yes _PATCHED_TESTING_METHODS_OUTPUT_DIR=/transformers/reports/${{ env.machine_type }}_${{ inputs.report_name_prefix }}_${{ env.matrix_folders }}_test_reports python3 -m pytest -rsfE -v -m '${{ inputs.pytest_marker }}' --make-reports=${{ env.machine_type }}_${{ inputs.report_name_prefix }}_${{ env.matrix_folders }}_test_reports tests/${{ matrix.folders }}\" test_outputs.txt\n           ls -la\n           # Extract the exit code from the output file\n           EXIT_CODE=$(tail -1 test_outputs.txt | grep -o 'COMMAND_EXIT_CODE=\"[0-9]*\"' | cut -d'\"' -f2)"
      },
      {
        "filename": ".github/workflows/self-scheduled-flash-attn-caller.yml",
        "status": "added",
        "additions": 60,
        "deletions": 0,
        "changes": 60,
        "patch": "@@ -0,0 +1,60 @@\n+name: Nvidia CI - Flash Attn\n+\n+on:\n+  repository_dispatch:\n+  schedule:\n+    - cron: \"17 2 * * *\"\n+  push:\n+    branches:\n+      - run_nvidia_ci_flash_attn*\n+  workflow_dispatch:\n+    inputs:\n+      prev_workflow_run_id:\n+        description: 'previous workflow run id to compare'\n+        type: string\n+        required: false\n+        default: \"\"\n+      other_workflow_run_id:\n+        description: 'other workflow run id to compare'\n+        type: string\n+        required: false\n+        default: \"\"\n+\n+\n+# Used for `push` to easily modify the target workflow runs to compare against\n+env:\n+    prev_workflow_run_id: \"\"\n+    other_workflow_run_id: \"\"\n+\n+\n+jobs:\n+  setup:\n+    name: Setup\n+    runs-on: ubuntu-22.04\n+    steps:\n+      - name: Setup\n+        run: |\n+          mkdir \"setup_values\"\n+          echo \"${{ inputs.prev_workflow_run_id || env.prev_workflow_run_id }}\" > \"setup_values/prev_workflow_run_id.txt\"\n+          echo \"${{ inputs.other_workflow_run_id || env.other_workflow_run_id }}\" > \"setup_values/other_workflow_run_id.txt\"\n+\n+      - name: Upload artifacts\n+        uses: actions/upload-artifact@v4\n+        with:\n+          name: setup_values\n+          path: setup_values\n+\n+\n+  model-ci:\n+    name: Model CI\n+    uses: ./.github/workflows/self-scheduled.yml\n+    with:\n+      job: run_models_gpu\n+      slack_report_channel: \"#transformers-ci-flash-attn\"\n+      docker: huggingface/transformers-all-latest-gpu\n+      ci_event: Daily CI\n+      runner_type: \"a10\"\n+      report_repo_id: hf-internal-testing/transformers_flash_attn_ci\n+      commit_sha: ${{ github.sha }}\n+      pytest_marker: \"flash_attn_test or flash_attn_3_test\"\n+    secrets: inherit"
      },
      {
        "filename": ".github/workflows/self-scheduled.yml",
        "status": "modified",
        "additions": 5,
        "deletions": 0,
        "changes": 5,
        "patch": "@@ -38,6 +38,10 @@ on:\n         default: \"\"\n         required: false\n         type: string\n+      pytest_marker:\n+        required: false\n+        type: string\n+\n \n env:\n   HF_HOME: /mnt/cache\n@@ -127,6 +131,7 @@ jobs:\n       commit_sha: ${{ inputs.commit_sha || github.sha }}\n       runner_type: ${{ inputs.runner_type }}\n       report_repo_id: ${{ inputs.report_repo_id }}\n+      pytest_marker: ${{ inputs.pytest_marker }}\n     secrets: inherit\n \n   run_trainer_and_fsdp_gpu:"
      },
      {
        "filename": "utils/notification_service.py",
        "status": "modified",
        "additions": 4,
        "deletions": 1,
        "changes": 5,
        "patch": "@@ -1407,7 +1407,10 @@ def pop_default(l: list[Any], i: int, default: Any) -> Any:\n     if not os.path.isdir(os.path.join(os.getcwd(), f\"ci_results_{job_name}\")):\n         os.makedirs(os.path.join(os.getcwd(), f\"ci_results_{job_name}\"))\n \n-    nvidia_daily_ci_workflow = \"huggingface/transformers/.github/workflows/self-scheduled-caller.yml\"\n+    nvidia_daily_ci_workflow = (\n+        \"huggingface/transformers/.github/workflows/self-scheduled-caller.yml\",\n+        \"huggingface/transformers/.github/workflows/self-scheduled-flash-attn-caller.yml\",\n+    )\n     amd_daily_ci_workflows = (\n         \"huggingface/transformers/.github/workflows/self-scheduled-amd-mi325-caller.yml\",\n         \"huggingface/transformers/.github/workflows/self-scheduled-amd-mi355-caller.yml\","
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T21:16:42.096834"
  },
  {
    "pr_number": 41818,
    "title": ":rotating_light: Fix gradient checkpointing for several models and improve test robustness  ",
    "body": "Support for gradient checkpointing was lost in the major refactoring in PR #38635 and this is the attempt to re-add it.\r\n\r\nI extended the tests to\r\n- test `use_reentrant=True` and `False`\r\n- make sure `model.train` is called so that gradient checkpointing works; this is a limiation of the tests currently used by GPTBigCode\r\n- make sure that one (the first) gradient checkpointing layer is called\r\n- make sure that the same non-zero grads are there for normal and checkpointing runs - this is something we tripped over before in PEFT due to the possibly incompletely stored runtime environment in the checkpointed forward step, see also peft#2826\r\n\r\nNote that the invocation of `GPTBigCodeBlock.forward` has changed:\r\n\r\n- `layer_past` is now passed as a keyword argument so that `GradientCheckpointingLayer.__call__` can see and filter this parameter (`use_reentrant=False` fails otherwise)\r\n- `{encoder_}hidden_states` are still passed as positional arguments so that `torch.utils.checkpoint.checkpoint` receives them as pos. args and computes gradients for these (kwargs would be filtered by `GradientCheckpointingLayer`).\r\n\r\n:rotating_light: Note that this is breaking compatibility by changing the forward signature in `GPTBigCodeBlock.forward`!",
    "html_url": "https://github.com/huggingface/transformers/pull/41818",
    "created_at": "2025-10-23T15:17:23Z",
    "merged_at": "2025-11-11T17:13:38Z",
    "merge_commit_sha": "fa22b569038540d31eacbf5d333a1e9aa0787131",
    "base_ref": "main",
    "head_sha": "195fbc846d9e8047ae0712bfc6517c8f5577118d",
    "user": "githubnemo",
    "files": [
      {
        "filename": "src/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py",
        "status": "modified",
        "additions": 6,
        "deletions": 5,
        "changes": 11,
        "patch": "@@ -26,6 +26,7 @@\n from ...generation import GenerationMixin\n from ...masking_utils import create_causal_mask\n from ...modeling_flash_attention_utils import is_flash_attn_available\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n     CausalLMOutputWithCrossAttentions,\n@@ -266,7 +267,7 @@ def forward(self, hidden_states: Optional[tuple[torch.FloatTensor]]) -> torch.Fl\n         return hidden_states\n \n \n-class GPTBigCodeBlock(nn.Module):\n+class GPTBigCodeBlock(GradientCheckpointingLayer):\n     def __init__(self, config, layer_idx=None):\n         super().__init__()\n         hidden_size = config.hidden_size\n@@ -291,9 +292,9 @@ def __init__(self, config, layer_idx=None):\n     def forward(\n         self,\n         hidden_states: Optional[tuple[torch.Tensor]],\n+        encoder_hidden_states: Optional[torch.Tensor] = None,\n         layer_past: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n-        encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         use_cache: Optional[bool] = False,\n         output_attentions: Optional[bool] = False,\n@@ -536,10 +537,10 @@ def forward(\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n             outputs = block(\n-                hidden_states,\n-                past_key_values,\n-                causal_mask,\n+                hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n+                layer_past=past_key_values,  # as keyword argument so it can be removed by GradientCheckpointingLayer\n+                attention_mask=causal_mask,\n                 encoder_attention_mask=encoder_attention_mask,\n                 use_cache=use_cache,\n                 output_attentions=output_attentions,"
      },
      {
        "filename": "src/transformers/models/swiftformer/modeling_swiftformer.py",
        "status": "modified",
        "additions": 2,
        "deletions": 1,
        "changes": 3,
        "patch": "@@ -21,6 +21,7 @@\n from torch import nn\n \n from ...activations import ACT2CLS\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithNoAttention, ImageClassifierOutputWithNoAttention\n from ...modeling_utils import PreTrainedModel\n from ...utils import auto_docstring, logging\n@@ -295,7 +296,7 @@ def forward(self, x):\n         return x\n \n \n-class SwiftFormerStage(nn.Module):\n+class SwiftFormerStage(GradientCheckpointingLayer):\n     \"\"\"\n     A Swiftformer stage consisting of a series of `SwiftFormerConvEncoder` blocks and a final\n     `SwiftFormerEncoderBlock`."
      },
      {
        "filename": "src/transformers/models/xlstm/modeling_xlstm.py",
        "status": "modified",
        "additions": 12,
        "deletions": 14,
        "changes": 26,
        "patch": "@@ -22,17 +22,21 @@\n from torch.nn import CrossEntropyLoss\n \n from ...generation import GenerationMixin\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_utils import PreTrainedModel\n from ...utils import ModelOutput, auto_docstring, can_return_tuple, is_xlstm_available\n from .configuration_xlstm import xLSTMConfig\n \n \n if is_xlstm_available():\n     from xlstm.xlstm_large.model import RMSNorm as xLSTMRMSNorm\n-    from xlstm.xlstm_large.model import mLSTMBlock as xLSTMBlock\n-    from xlstm.xlstm_large.model import mLSTMStateType, soft_cap\n+    from xlstm.xlstm_large.model import mLSTMBlock, mLSTMStateType, soft_cap\n \n     external_xlstm = True\n+\n+    class xLSTMBlock(GradientCheckpointingLayer, mLSTMBlock):\n+        pass\n+\n else:\n     from collections.abc import Callable\n     from functools import partial\n@@ -1164,7 +1168,7 @@ def forward(\n             y = self.out_proj(h_out)\n             return y, state\n \n-    class xLSTMBlock(nn.Module):\n+    class xLSTMBlock(GradientCheckpointingLayer):\n         def __init__(self, config: xLSTMConfig):\n             super().__init__()\n             self.config = config\n@@ -1457,17 +1461,11 @@ def forward(\n         else:\n             all_hidden_states = () if output_hidden_states else None\n             for layer_idx, xlstm_block in enumerate(self.blocks):\n-                if self.gradient_checkpointing and self.training:\n-                    hidden_states, rnn_state = self._gradient_checkpointing_func(\n-                        xlstm_block.__call__,\n-                        hidden_states,\n-                        cache_params.rnn_state[layer_idx] if cache_params is not None else None,\n-                    )\n-                else:\n-                    hidden_states, rnn_state = xlstm_block(\n-                        hidden_states,\n-                        state=cache_params.rnn_state[layer_idx] if cache_params is not None else None,\n-                    )\n+                hidden_states, rnn_state = xlstm_block(\n+                    hidden_states,\n+                    cache_params.rnn_state[layer_idx] if cache_params is not None else None,\n+                )\n+\n                 if cache_params:\n                     for state_idx in range(len(cache_params.rnn_state[layer_idx])):\n                         local_rnn_state = rnn_state[state_idx]"
      },
      {
        "filename": "src/transformers/models/zamba/modeling_zamba.py",
        "status": "modified",
        "additions": 15,
        "deletions": 27,
        "changes": 42,
        "patch": "@@ -32,6 +32,7 @@\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast, SequenceClassifierOutputWithPast\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n@@ -639,7 +640,7 @@ def forward(\n         return outputs\n \n \n-class ZambaMambaDecoderLayer(nn.Module):\n+class ZambaMambaDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: ZambaConfig, layer_idx: int):\n         super().__init__()\n         self.mamba = ZambaMambaMixer(config=config, layer_idx=layer_idx)\n@@ -708,7 +709,7 @@ def forward(\n         return outputs\n \n \n-class ZambaHybridLayer(nn.Module):\n+class ZambaHybridLayer(GradientCheckpointingLayer):\n     def __init__(self, shared_transf: ZambaAttentionDecoderLayer, linear: nn.Linear, mamba: ZambaMambaDecoderLayer):\n         super().__init__()\n         self.shared_transf = shared_transf\n@@ -942,31 +943,18 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer.__call__,\n-                    hidden_states,\n-                    original_hidden_states,\n-                    layer_idx,\n-                    attention_mask,\n-                    causal_mask,\n-                    past_key_values,\n-                    output_attentions,\n-                    use_cache,\n-                    cache_position,\n-                )\n-            else:\n-                layer_outputs = layer(\n-                    hidden_states,\n-                    original_hidden_states=original_hidden_states,\n-                    layer_idx=layer_idx,\n-                    attention_mask=attention_mask,\n-                    causal_mask=causal_mask,\n-                    past_key_values=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                )\n+            layer_outputs = layer(\n+                hidden_states,\n+                original_hidden_states,\n+                layer_idx,\n+                attention_mask,\n+                causal_mask,\n+                past_key_values=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+            )\n+\n             hidden_states = layer_outputs[0]\n \n             if output_attentions:"
      },
      {
        "filename": "src/transformers/models/zamba2/modeling_zamba2.py",
        "status": "modified",
        "additions": 16,
        "deletions": 29,
        "changes": 45,
        "patch": "@@ -34,6 +34,7 @@\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast, SequenceClassifierOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n@@ -1058,7 +1059,7 @@ def forward(\n         return outputs\n \n \n-class Zamba2MambaDecoderLayer(nn.Module):\n+class Zamba2MambaDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: Zamba2Config, layer_idx: int):\n         super().__init__()\n         self.mamba = Zamba2MambaMixer(config=config, layer_idx=layer_idx)\n@@ -1127,7 +1128,7 @@ def forward(\n         return outputs\n \n \n-class Zamba2HybridLayer(nn.Module):\n+class Zamba2HybridLayer(GradientCheckpointingLayer):\n     def __init__(\n         self, shared_transformer: Zamba2AttentionDecoderLayer, linear: nn.Linear, mamba: Zamba2MambaDecoderLayer\n     ):\n@@ -1344,33 +1345,19 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer.__call__,\n-                    hidden_states,\n-                    original_hidden_states,\n-                    layer_idx,\n-                    attention_mask,\n-                    causal_mask,\n-                    past_key_values,\n-                    output_attentions,\n-                    use_cache,\n-                    position_embeddings,\n-                    position_ids,\n-                )\n-            else:\n-                layer_outputs = layer(\n-                    hidden_states,\n-                    original_hidden_states=original_hidden_states,\n-                    layer_idx=layer_idx,\n-                    attention_mask=attention_mask,\n-                    causal_mask=causal_mask,\n-                    past_key_values=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    position_embeddings=position_embeddings,\n-                    position_ids=position_ids,\n-                )\n+            layer_outputs = layer(\n+                hidden_states,\n+                original_hidden_states,\n+                layer_idx,\n+                attention_mask,\n+                causal_mask,\n+                past_key_values=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                position_embeddings=position_embeddings,\n+                position_ids=position_ids,\n+            )\n+\n             hidden_states = layer_outputs[0]\n \n             if output_attentions:"
      },
      {
        "filename": "src/transformers/models/zamba2/modular_zamba2.py",
        "status": "modified",
        "additions": 13,
        "deletions": 27,
        "changes": 40,
        "patch": "@@ -1079,33 +1079,19 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer.__call__,\n-                    hidden_states,\n-                    original_hidden_states,\n-                    layer_idx,\n-                    attention_mask,\n-                    causal_mask,\n-                    past_key_values,\n-                    output_attentions,\n-                    use_cache,\n-                    position_embeddings,\n-                    position_ids,\n-                )\n-            else:\n-                layer_outputs = layer(\n-                    hidden_states,\n-                    original_hidden_states=original_hidden_states,\n-                    layer_idx=layer_idx,\n-                    attention_mask=attention_mask,\n-                    causal_mask=causal_mask,\n-                    past_key_values=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    position_embeddings=position_embeddings,\n-                    position_ids=position_ids,\n-                )\n+            layer_outputs = layer(\n+                hidden_states,\n+                original_hidden_states,\n+                layer_idx,\n+                attention_mask,\n+                causal_mask,\n+                past_key_values=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                position_embeddings=position_embeddings,\n+                position_ids=position_ids,\n+            )\n+\n             hidden_states = layer_outputs[0]\n \n             if output_attentions:"
      },
      {
        "filename": "tests/models/clvp/test_modeling_clvp.py",
        "status": "modified",
        "additions": 4,
        "deletions": 15,
        "changes": 19,
        "patch": "@@ -186,6 +186,10 @@ def test_training(self):\n     def test_training_gradient_checkpointing(self):\n         pass\n \n+    @unittest.skip(reason=\"ClvpEncoder does not output loss\")\n+    def test_gradient_checkpointing_enable_disable(self):\n+        pass\n+\n \n class ClvpDecoderTester:\n     def __init__(\n@@ -311,21 +315,6 @@ def test_training(self):\n         loss = model(**inputs).loss\n         loss.backward()\n \n-    def test_training_gradient_checkpointing(self):\n-        # we will only test the ClvpForCausalLM since it outputs loss\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.use_cache = False\n-        config.return_dict = True\n-\n-        model = ClvpForCausalLM(config)\n-        model.to(torch_device)\n-        model.gradient_checkpointing_enable()\n-        model.train()\n-        inputs = self._prepare_for_class(inputs_dict, ClvpForCausalLM, return_labels=True)\n-\n-        loss = model(**inputs).loss\n-        loss.backward()\n-\n     @unittest.skip(reason=\"Clvp `prepare_inputs_for_generation` function doesn't have cache position.\")\n     def test_generate_continue_from_inputs_embeds(self):\n         pass"
      },
      {
        "filename": "tests/models/gpt_bigcode/test_modeling_gpt_bigcode.py",
        "status": "modified",
        "additions": 7,
        "deletions": 7,
        "changes": 14,
        "patch": "@@ -307,12 +307,16 @@ def create_and_check_lm_head_model(self, config, input_ids, input_mask, token_ty\n         self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n \n     def create_and_check_forward_and_backwards(\n-        self, config, input_ids, input_mask, token_type_ids, *args, gradient_checkpointing=False\n+        self,\n+        config,\n+        input_ids,\n+        input_mask,\n+        token_type_ids,\n+        *args,\n     ):\n         model = GPTBigCodeForCausalLM(config)\n+        model.train()\n         model.to(torch_device)\n-        if gradient_checkpointing:\n-            model.gradient_checkpointing_enable()\n \n         result = model(input_ids, token_type_ids=token_type_ids, labels=input_ids)\n         self.parent.assertEqual(result.loss.shape, ())\n@@ -463,10 +467,6 @@ def test_gpt_bigcode_token_classification_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_gpt_bigcode_for_token_classification(*config_and_inputs)\n \n-    def test_gpt_bigcode_gradient_checkpointing(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_forward_and_backwards(*config_and_inputs, gradient_checkpointing=True)\n-\n     def test_gpt_bigcode_scale_attn_by_inverse_layer_idx(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs(scale_attn_by_inverse_layer_idx=True)\n         self.model_tester.create_and_check_forward_and_backwards(*config_and_inputs)"
      },
      {
        "filename": "tests/models/janus/test_modeling_janus.py",
        "status": "modified",
        "additions": 4,
        "deletions": 0,
        "changes": 4,
        "patch": "@@ -396,6 +396,10 @@ def test_model_get_set_embeddings(self):\n     def test_retain_grad_hidden_states_attentions(self):\n         pass\n \n+    @unittest.skip(\"Janus VQ module has no gradient checkpointing layers\")\n+    def test_gradient_checkpointing_enable_disable(self):\n+        pass\n+\n \n class JanusIntegrationTest(unittest.TestCase):\n     def setUp(self):"
      },
      {
        "filename": "tests/test_modeling_common.py",
        "status": "modified",
        "additions": 54,
        "deletions": 5,
        "changes": 59,
        "patch": "@@ -20,6 +20,7 @@\n import random\n import re\n import tempfile\n+import unittest.mock\n import warnings\n from collections import defaultdict\n from contextlib import contextmanager\n@@ -46,6 +47,7 @@\n     is_deepspeed_zero3_enabled,\n     unset_hf_deepspeed_config,\n )\n+from transformers.modeling_layers import GradientCheckpointingLayer\n from transformers.modeling_utils import _get_tied_weight_keys\n from transformers.models.auto import get_values\n from transformers.models.auto.modeling_auto import (\n@@ -827,6 +829,12 @@ def test_gradient_checkpointing_enable_disable(self):\n             model = model_class(copy.deepcopy(config))\n             self.assertFalse(model.is_gradient_checkpointing)\n \n+            # Gradient checkpointing is implemented via GradientCheckpointingLayer, if none is present this is likely\n+            # an implementation issue. Note we exclude clvp for now since they are still not using\n+            # GradientCheckpointingLayer.\n+            if config.model_type not in [\"clvp\", \"clvp_decoder\"]:\n+                self.assertTrue([m for m in model.modules() if isinstance(m, GradientCheckpointingLayer)])\n+\n             # check enable works\n             model.gradient_checkpointing_enable()\n             self.assertTrue(model.is_gradient_checkpointing)\n@@ -1151,22 +1159,63 @@ def check_training_gradient_checkpointing(self, gradient_checkpointing_kwargs=No\n                 config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n                 config.use_cache = False\n                 config.return_dict = True\n-                model = model_class(config)\n \n+                # make sure that test runs are consistent by disabling dropout\n+                #\n+                # Note: attention_probs_dropout_prob seem to influence classifier.bias in BertForMultipleChoice\n+                # (and other Bert derived models). Sometimes classifier.bias is None when\n+                # attention_probs_dropout_prob > 0. This might indicate a bug somewhere.\n+                if hasattr(config, \"hidden_dropout_prob\"):\n+                    config.hidden_dropout_prob = 0.0\n+                if hasattr(config, \"attention_probs_dropout_prob\"):\n+                    config.attention_probs_dropout_prob = 0.0\n+\n+                inputs = self._prepare_for_class(inputs_dict, model_class, return_labels=True)\n+\n+                torch.manual_seed(0)\n+                model = model_class(config)\n                 model.to(torch_device)\n-                model.gradient_checkpointing_enable(gradient_checkpointing_kwargs=gradient_checkpointing_kwargs)\n                 model.train()\n \n                 # unfreeze additional layers\n                 for p in model.parameters():\n                     p.requires_grad_(True)\n \n+                # do a non-checkpointing run, so we can compare the set of non-zero gradients later. we skip None\n+                # grads here to collect a reference set of modules that have non-zero gradients (to filter layers like\n+                # MoE that drop out parts of the model).\n                 optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n-\n-                inputs = self._prepare_for_class(inputs_dict, model_class, return_labels=True)\n+                torch.manual_seed(0)\n                 loss = model(**inputs).loss\n                 loss.backward()\n-                optimizer.step()\n+                grad_expected_params = [(n, p) for n, p in model.named_parameters() if p.grad is not None]\n+                non_zero_grads_normal = {n for n, p in grad_expected_params if p.grad.abs().sum() > 0}\n+\n+                # reset all gradients to zero for the comparison with the gradient checkpointing run\n+                optimizer.zero_grad()\n+\n+                # now enable gradient checkpointing and compare the gradients\n+                model.gradient_checkpointing_enable(gradient_checkpointing_kwargs=gradient_checkpointing_kwargs)\n+\n+                checkpointing_layer = next(m for m in model.modules() if isinstance(m, GradientCheckpointingLayer))\n+\n+                optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n+                with unittest.mock.patch.object(\n+                    checkpointing_layer, \"forward\", wraps=checkpointing_layer.forward\n+                ) as forward_mock:\n+                    torch.manual_seed(0)\n+                    loss = model(**inputs).loss\n+                    loss.backward()\n+                    optimizer.step()\n+\n+                    # test that gradient checkpointing is active as it would call the gradient checkpointing layer's\n+                    # forward more than once.\n+                    self.assertGreater(forward_mock.call_count, 1)\n+\n+                # check that all the parameters that had non-zero gradients before, have non-zero grads with gradient\n+                # checkpointing. divergence indicates a different forward-pass environment that needs special handling.\n+                non_zero_grads_gradcp = {n for n, p in grad_expected_params if p.grad.abs().sum() > 0}\n+                self.assertEqual(non_zero_grads_gradcp, non_zero_grads_normal)\n \n                 if self.test_all_params_have_gradient:\n                     for k, v in model.named_parameters():"
      }
    ],
    "num_files": 10,
    "scraped_at": "2025-11-16T21:16:50.040590"
  },
  {
    "pr_number": 41817,
    "title": "add fuyu fast image processors",
    "body": "# What does this PR do?\r\n\r\nThis PR introduces FuyuImageProcessorFast, providing a faster alternative to the original FuyuImageProcessor by leveraging torchvision for image transformations.\r\n\r\nKey changes include:\r\n\r\n* Implementation of FuyuImageProcessorFast inheriting from BaseImageProcessorFast.\r\n* Updates to tests/models/fuyu/test_image_processing_fuyu.py to include the fast processor, override save/load tests and fixed the image height and width in test_preprocess_with_tokenizer_info have been updated to values divisible by 30 (180x300), ensuring compatibility with FuyuImageProcessorFast and avoiding ValueError: image_height must be divisible by 30. All Fuyu image processing tests now pass.\r\n* Addition of documentation for FuyuImageProcessorFast \r\n\r\nFixes #36978\r\n\r\n\r\n## Before submitting\r\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),\r\n      Pull Request section?\r\n- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case. Was this discussed/approved via a Github issue or the forum? [Contributions Welcome] Add Fast Image Processors #36978](https://github.com/huggingface/transformers/issues/36978)\r\n\r\n- [x] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [x] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n@yonigozlan \r\n\r\n",
    "html_url": "https://github.com/huggingface/transformers/pull/41817",
    "created_at": "2025-10-23T14:43:41Z",
    "merged_at": "2025-11-04T15:45:03Z",
    "merge_commit_sha": "325810e7fccf8273599c58a525ae0011ea8ba3e6",
    "base_ref": "main",
    "head_sha": "de29121c99e94337dc1e1392352e2304aa0c6e35",
    "user": "DeXtAr47-oss",
    "files": [
      {
        "filename": "docs/source/en/model_doc/fuyu.md",
        "status": "modified",
        "additions": 7,
        "deletions": 2,
        "changes": 9,
        "patch": "@@ -75,11 +75,11 @@ A processor requires an image_processor and a tokenizer. Hence, inputs can be lo\n from PIL import Image\n from transformers import AutoTokenizer\n from transformers.models.fuyu.processing_fuyu import FuyuProcessor\n-from transformers.models.fuyu.image_processing_fuyu import FuyuImageProcessor\n+from transformers.models.fuyu.image_processing_fuyu_fast import FuyuImageProcessorFast\n \n \n tokenizer = AutoTokenizer.from_pretrained('adept-hf-collab/fuyu-8b')\n-image_processor = FuyuImageProcessor()\n+image_processor = FuyuImageProcessorFast()\n \n \n processor = FuyuProcessor(image_processor=image_processor, tokenizer=tokenizer)\n@@ -118,6 +118,11 @@ The `LlamaTokenizer` is used as it is a standard wrapper around sentencepiece.\n [[autodoc]] FuyuImageProcessor\n     - __call__\n \n+## FuyuImageProcessor\n+\n+[[autodoc]] FuyuImageProcessorFast\n+    - __call__\n+\n ## FuyuProcessor\n \n [[autodoc]] FuyuProcessor"
      },
      {
        "filename": "src/transformers/image_processing_utils_fast.py",
        "status": "modified",
        "additions": 6,
        "deletions": 3,
        "changes": 9,
        "patch": "@@ -227,6 +227,7 @@ def pad(\n         padding_mode: Optional[str] = \"constant\",\n         return_mask: bool = False,\n         disable_grouping: Optional[bool] = False,\n+        is_nested: Optional[bool] = False,\n         **kwargs,\n     ) -> Union[tuple[\"torch.Tensor\", \"torch.Tensor\"], \"torch.Tensor\"]:\n         \"\"\"\n@@ -257,7 +258,9 @@ def pad(\n         else:\n             pad_size = get_max_height_width(images)\n \n-        grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n+        grouped_images, grouped_images_index = group_images_by_shape(\n+            images, disable_grouping=disable_grouping, is_nested=is_nested\n+        )\n         processed_images_grouped = {}\n         processed_masks_grouped = {}\n         for shape, stacked_images in grouped_images.items():\n@@ -280,9 +283,9 @@ def pad(\n                 stacked_masks[..., : image_size[0], : image_size[1]] = 1\n                 processed_masks_grouped[shape] = stacked_masks\n \n-        processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n+        processed_images = reorder_images(processed_images_grouped, grouped_images_index, is_nested=is_nested)\n         if return_mask:\n-            processed_masks = reorder_images(processed_masks_grouped, grouped_images_index)\n+            processed_masks = reorder_images(processed_masks_grouped, grouped_images_index, is_nested=is_nested)\n             return processed_images, processed_masks\n \n         return processed_images"
      },
      {
        "filename": "src/transformers/models/auto/image_processing_auto.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -98,7 +98,7 @@\n             (\"eomt\", (\"EomtImageProcessor\", \"EomtImageProcessorFast\")),\n             (\"flava\", (\"FlavaImageProcessor\", \"FlavaImageProcessorFast\")),\n             (\"focalnet\", (\"BitImageProcessor\", \"BitImageProcessorFast\")),\n-            (\"fuyu\", (\"FuyuImageProcessor\", None)),\n+            (\"fuyu\", (\"FuyuImageProcessor\", \"FuyuImageProcessorFast\")),\n             (\"gemma3\", (\"Gemma3ImageProcessor\", \"Gemma3ImageProcessorFast\")),\n             (\"gemma3n\", (\"SiglipImageProcessor\", \"SiglipImageProcessorFast\")),\n             (\"git\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),"
      },
      {
        "filename": "src/transformers/models/fuyu/__init__.py",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -20,6 +20,7 @@\n if TYPE_CHECKING:\n     from .configuration_fuyu import *\n     from .image_processing_fuyu import *\n+    from .image_processing_fuyu_fast import *\n     from .modeling_fuyu import *\n     from .processing_fuyu import *\n else:"
      },
      {
        "filename": "src/transformers/models/fuyu/image_processing_fuyu.py",
        "status": "modified",
        "additions": 18,
        "deletions": 0,
        "changes": 18,
        "patch": "@@ -29,6 +29,7 @@\n     ChannelDimension,\n     ImageInput,\n     PILImageResampling,\n+    SizeDict,\n     get_image_size,\n     infer_channel_dimension_format,\n     is_scaled_image,\n@@ -37,6 +38,7 @@\n     to_numpy_array,\n     validate_preprocess_arguments,\n )\n+from ...processing_utils import ImagesKwargs\n from ...utils import (\n     TensorType,\n     filter_out_non_signature_kwargs,\n@@ -70,6 +72,21 @@ def make_list_of_list_of_images(\n     raise ValueError(\"images must be a list of list of images or a list of images or an image.\")\n \n \n+class FuyuImagesKwargs(ImagesKwargs, total=False):\n+    r\"\"\"\n+    patch_size (`dict[str, int]`, *optional*, defaults to `{\"height\": 30, \"width\": 30}`):\n+        Dictionary in the format `{\"height\": int, \"width\": int}` specifying the size of the patches.\n+    padding_value (`float`, *optional*, defaults to 1.0):\n+        The value to pad the image with.\n+    padding_mode (`str`, *optional*, defaults to \"constant\"):\n+        The padding mode to use when padding the image.\n+    \"\"\"\n+\n+    patch_size: Optional[SizeDict]\n+    padding_value: float\n+    padding_mode: str\n+\n+\n class FuyuBatchFeature(BatchFeature):\n     \"\"\"\n     BatchFeature class for Fuyu image processor and processor.\n@@ -232,6 +249,7 @@ class FuyuImageProcessor(BaseImageProcessor):\n         \"image_patch_indices_per_batch\",\n         \"image_patch_indices_per_subsequence\",\n     ]\n+    valid_kwargs = FuyuImagesKwargs\n \n     def __init__(\n         self,"
      },
      {
        "filename": "src/transformers/models/fuyu/image_processing_fuyu_fast.py",
        "status": "added",
        "additions": 382,
        "deletions": 0,
        "changes": 382,
        "patch": "@@ -0,0 +1,382 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Fast Image processor class for Fuyu.\"\"\"\n+\n+import math\n+from typing import Optional, Union\n+\n+import torch\n+\n+from ...image_processing_utils import get_size_dict\n+from ...image_processing_utils_fast import (\n+    BaseImageProcessorFast,\n+    group_images_by_shape,\n+    reorder_images,\n+)\n+from ...image_utils import (\n+    ImageInput,\n+    PILImageResampling,\n+    SizeDict,\n+)\n+from ...utils import (\n+    TensorType,\n+    auto_docstring,\n+    is_torchvision_available,\n+    logging,\n+    requires_backends,\n+)\n+from .image_processing_fuyu import FuyuBatchFeature, FuyuImagesKwargs, make_list_of_list_of_images\n+\n+\n+if is_torchvision_available():\n+    from torchvision.transforms.v2 import functional as F\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+@auto_docstring\n+class FuyuImageProcessorFast(BaseImageProcessorFast):\n+    do_resize = True\n+    size = {\"height\": 1080, \"width\": 1920}\n+    resample = PILImageResampling.BILINEAR\n+    do_pad = True\n+    padding_value = 1.0\n+    padding_mode = \"constant\"\n+    do_normalize = True\n+    image_mean = 0.5\n+    image_std = 0.5\n+    do_rescale = True\n+    rescale_factor = 1 / 255\n+    model_input_names = [\n+        \"images\",\n+        \"image_input_ids\",\n+        \"image_patches\",\n+        \"image_patch_indices_per_batch\",\n+        \"image_patch_indices_per_subsequence\",\n+    ]\n+    valid_kwargs = FuyuImagesKwargs\n+\n+    def _prepare_images_structure(\n+        self,\n+        images: ImageInput,\n+        expected_ndims: int = 3,\n+    ) -> ImageInput:\n+        images = self.fetch_images(images)\n+        return make_list_of_list_of_images(images)\n+\n+    def resize(\n+        self,\n+        image: torch.Tensor,\n+        size: SizeDict,\n+        interpolation: Optional[\"F.InterpolationMode\"] = None,\n+        antialias: bool = True,\n+        **kwargs,\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        Resize an image to fit within `(size[\"height\"], size[\"width\"])` while maintaining aspect ratio.\n+        Only resizes if the image is larger than the target size.\n+        Args:\n+            image (`torch.Tensor`):\n+                Image to resize.\n+            size (`SizeDict`):\n+                Dictionary in the format `{\"height\": int, \"width\": int}` specifying the max size of the output image.\n+            interpolation (`InterpolationMode`, *optional*, defaults to `InterpolationMode.BILINEAR`):\n+                `InterpolationMode` filter to use when resizing the image e.g. `InterpolationMode.BILINEAR`.\n+            antialias (`bool`, *optional*, defaults to `True`):\n+                Whether to apply antialiasing when resizing.\n+        \"\"\"\n+        interpolation = interpolation if interpolation is not None else F.InterpolationMode.BILINEAR\n+        image_height, image_width = image.shape[-2:]\n+        target_height, target_width = size.height, size.width\n+        # Only resize if image is larger than target\n+        if image_width <= target_width and image_height <= target_height:\n+            return image\n+        # Calculate optimal scale factor to fit within target size\n+        height_scale_factor = target_height / image_height\n+        width_scale_factor = target_width / image_width\n+        optimal_scale_factor = min(height_scale_factor, width_scale_factor)\n+\n+        new_height = int(image_height * optimal_scale_factor)\n+        new_width = int(image_width * optimal_scale_factor)\n+\n+        return super().resize(\n+            image, SizeDict(height=new_height, width=new_width), interpolation=interpolation, antialias=antialias\n+        )\n+\n+    def _preprocess(\n+        self,\n+        images: list[\"torch.Tensor\"],\n+        do_resize: bool,\n+        size: SizeDict,\n+        interpolation: Optional[\"F.InterpolationMode\"],\n+        do_rescale: bool,\n+        rescale_factor: float,\n+        do_normalize: bool,\n+        image_mean: Optional[Union[float, list[float]]],\n+        image_std: Optional[Union[float, list[float]]],\n+        do_pad: Optional[bool],\n+        padding_value: Optional[float],\n+        padding_mode: Optional[str],\n+        disable_grouping: Optional[bool],\n+        return_tensors: Optional[Union[str, TensorType]],\n+        **kwargs,\n+    ) -> FuyuBatchFeature:\n+        # Group images by size for batched resizing\n+        original_image_sizes = [batch_image[0].shape[-2:] for batch_image in images if batch_image]\n+        grouped_images, grouped_images_index = group_images_by_shape(\n+            images, disable_grouping=disable_grouping, is_nested=True\n+        )\n+        resized_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            if do_resize:\n+                stacked_images = self.resize(image=stacked_images, size=size, interpolation=interpolation)\n+            resized_images_grouped[shape] = stacked_images\n+        resized_images = reorder_images(resized_images_grouped, grouped_images_index, is_nested=True)\n+\n+        image_sizes = [batch_image[0].shape[-2:] for batch_image in resized_images if batch_image]\n+        image_unpadded_heights = [[image_size[0]] for image_size in image_sizes]\n+        image_unpadded_widths = [[image_size[1]] for image_size in image_sizes]\n+        image_scale_factors = [\n+            [resized_size[0] / original_size[0]]\n+            for original_size, resized_size in zip(original_image_sizes, image_sizes)\n+        ]\n+        if do_pad:\n+            resized_images = self.pad(\n+                resized_images,\n+                pad_size=size,\n+                fill_value=padding_value,\n+                padding_mode=padding_mode,\n+                disable_grouping=disable_grouping,\n+                is_nested=True,\n+            )\n+        # Group images by size for further processing\n+        # Needed in case do_resize is False, or resize returns images with different sizes\n+        grouped_images, grouped_images_index = group_images_by_shape(\n+            resized_images, disable_grouping=disable_grouping, is_nested=True\n+        )\n+        processed_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            # Fused rescale and normalize\n+            stacked_images = self.rescale_and_normalize(\n+                stacked_images, do_rescale, rescale_factor, do_normalize, image_mean, image_std\n+            )\n+            processed_images_grouped[shape] = stacked_images\n+        processed_images = reorder_images(processed_images_grouped, grouped_images_index, is_nested=True)\n+\n+        return FuyuBatchFeature(\n+            data={\n+                \"images\": processed_images,\n+                \"image_unpadded_heights\": image_unpadded_heights,\n+                \"image_unpadded_widths\": image_unpadded_widths,\n+                \"image_scale_factors\": image_scale_factors,\n+            },\n+            tensor_type=return_tensors,\n+        )\n+\n+    def get_num_patches(self, image_height: int, image_width: int, patch_size: Optional[SizeDict] = None) -> int:\n+        \"\"\"\n+        Calculate number of patches required to encode an image.\n+        Args:\n+            image_height (`int`):\n+                Height of the image.\n+            image_width (`int`):\n+                Width of the image.\n+            patch_size (`SizeDict`, *optional*):\n+                Dictionary in the format `{\"height\": int, \"width\": int}` specifying the size of the patches.\n+        \"\"\"\n+        if patch_size is None:\n+            patch_size = SizeDict(**self.patch_size)\n+        patch_height, patch_width = patch_size.height, patch_size.width\n+        if image_height % patch_height != 0:\n+            raise ValueError(f\"{image_height=} must be divisible by {patch_height}\")\n+        if image_width % patch_width != 0:\n+            raise ValueError(f\"{image_width=} must be divisible by {patch_width}\")\n+        num_patches_per_dim_h = image_height // patch_height\n+        num_patches_per_dim_w = image_width // patch_width\n+        num_patches = num_patches_per_dim_h * num_patches_per_dim_w\n+        return num_patches\n+\n+    def patchify_image(self, image: torch.Tensor, patch_size: Optional[SizeDict] = None) -> torch.Tensor:\n+        \"\"\"\n+        Convert an image into a tensor of patches using PyTorch's unfold operation.\n+        Args:\n+            image (`torch.Tensor`):\n+                Image to convert. Shape: [batch, channels, height, width]\n+            patch_size (`SizeDict`, *optional*):\n+                Dictionary in the format `{\"height\": int, \"width\": int}` specifying the size of the patches.\n+        \"\"\"\n+        requires_backends(self, [\"torch\"])\n+        if patch_size is None:\n+            patch_size = SizeDict(**self.patch_size)\n+        patch_height, patch_width = patch_size.height, patch_size.width\n+        batch_size, channels, _, _ = image.shape\n+        # Use unfold to extract patches\n+        unfolded_along_height = image.unfold(2, patch_height, patch_height)\n+        patches = unfolded_along_height.unfold(3, patch_width, patch_width)\n+        patches = patches.contiguous()\n+        # Reshape to [batch, num_patches, channels * patch_h * patch_w]\n+        patches = patches.view(batch_size, channels, -1, patch_height, patch_width)\n+        patches = patches.permute(0, 2, 3, 4, 1)\n+        patches = patches.reshape(batch_size, -1, channels * patch_height * patch_width)\n+        return patches\n+\n+    def preprocess_with_tokenizer_info(\n+        self,\n+        image_input: torch.Tensor,\n+        image_present: torch.Tensor,\n+        image_unpadded_h: torch.Tensor,\n+        image_unpadded_w: torch.Tensor,\n+        image_placeholder_id: int,\n+        image_newline_id: int,\n+        variable_sized: bool,\n+        patch_size: Optional[dict[str, int]] = None,\n+    ) -> FuyuBatchFeature:\n+        \"\"\"\n+        Process images for model input. In particular, variable-sized images are handled here.\n+\n+        Args:\n+            image_input (`torch.Tensor` of shape [batch_size, subsequence_size, num_channels, height, width]):\n+                Tensor of images padded to model input size.\n+            image_present (`torch.Tensor` of shape [batch_size, subsequence_size, num_images]):\n+                Tensor of 1s and 0s indicating whether an image is present.\n+            image_unpadded_h (`torch.Tensor` of shape [batch_size, subsequence_size]):\n+                Tensor of unpadded image heights.\n+            image_unpadded_w (`torch.Tensor` of shape [batch_size, subsequence_size]):\n+                Tensor of unpadded image widths.\n+            image_placeholder_id (int):\n+                The id of the image placeholder token. Comes from an associated tokenizer.\n+            image_newline_id (int):\n+                The id of the image newline token. Comes from an associated tokenizer.\n+            variable_sized (bool):\n+                Whether to process images as variable-sized.\n+            patch_size (`dict[str, int]`, *optional*):\n+                Size of the patches.\n+        \"\"\"\n+        requires_backends(self, [\"torch\"])\n+\n+        if patch_size is None:\n+            patch_size = SizeDict(**self.patch_size)\n+        else:\n+            patch_size = SizeDict(**patch_size)\n+        patch_height, patch_width = patch_size.height, patch_size.width\n+        # Only images that are present\n+        images: list[list[torch.Tensor]] = []\n+        batch_image_patches: list[list[torch.Tensor]] = []\n+        # Image input ids for every subsequence, including ones with no image present\n+        batch_image_input_ids: list[list[torch.Tensor]] = []\n+        for batch_index in range(image_input.shape[0]):\n+            image_input_ids = []\n+            image_patches = []\n+            for subseq_index in range(image_input.shape[1]):\n+                if image_present[batch_index, subseq_index]:\n+                    image = image_input[batch_index, subseq_index]\n+                    image_height, image_width = image.shape[1], image.shape[2]\n+                    if variable_sized:\n+                        # Calculate new dimensions based on unpadded size\n+                        # The min() is required here due to floating point issues\n+                        new_h = min(\n+                            image_height,\n+                            math.ceil(image_unpadded_h[batch_index, subseq_index] / patch_height) * patch_height,\n+                        )\n+                        new_w = min(\n+                            image_width,\n+                            math.ceil(image_unpadded_w[batch_index, subseq_index] / patch_width) * patch_width,\n+                        )\n+                        image = image[:, :new_h, :new_w]\n+                        image_height, image_width = new_h, new_w\n+                    num_patches = self.get_num_patches(\n+                        image_height=image_height, image_width=image_width, patch_size=patch_size\n+                    )\n+                    # Create tensor of placeholder IDs\n+                    tensor_of_image_ids = torch.full(\n+                        [num_patches], image_placeholder_id, dtype=torch.int32, device=image_input.device\n+                    )\n+                    # Patchify the image\n+                    patches = self.patchify_image(image=image.unsqueeze(0), patch_size=patch_size).squeeze(0)\n+                    assert num_patches == patches.shape[0]\n+                    if variable_sized:\n+                        # Terminate each line with newline ID\n+                        tensor_of_image_ids = tensor_of_image_ids.reshape(-1, image_width // patch_width)\n+                        newline_ids = torch.full(\n+                            [tensor_of_image_ids.shape[0], 1],\n+                            image_newline_id,\n+                            dtype=torch.int32,\n+                            device=image_input.device,\n+                        )\n+                        tensor_of_image_ids = torch.cat([tensor_of_image_ids, newline_ids], dim=1)\n+                        tensor_of_image_ids = tensor_of_image_ids.reshape(-1)\n+                    images.append([image])\n+                    image_input_ids.append(tensor_of_image_ids)\n+                    image_patches.append(patches)\n+                else:\n+                    image_input_ids.append(torch.tensor([], dtype=torch.int32, device=image_input.device))\n+            batch_image_input_ids.append(image_input_ids)\n+            batch_image_patches.append(image_patches)\n+        # Create image patch indices\n+        image_patch_indices_per_batch: list[list[torch.Tensor]] = []\n+        image_patch_indices_per_subsequence: list[list[torch.Tensor]] = []\n+\n+        for sample_image_input_ids in batch_image_input_ids:\n+            index_offset = 0\n+            per_batch_indices = []\n+            per_subsequence_indices = []\n+            for subseq_image_input_ids in sample_image_input_ids:\n+                # Indices of image patches\n+                patches_mask = subseq_image_input_ids == image_placeholder_id\n+                num_patches = torch.count_nonzero(patches_mask)\n+                indices = torch.arange(num_patches, dtype=torch.int64, device=subseq_image_input_ids.device).type_as(\n+                    subseq_image_input_ids\n+                )\n+                # Place those indices in the image input ids token stream, with -1 representing non-index tokens\n+                indices_in_stream_per_batch = torch.full_like(subseq_image_input_ids, -1)\n+                indices_in_stream_per_subsequence = torch.full_like(subseq_image_input_ids, -1)\n+                patches_inds = torch.nonzero(patches_mask, as_tuple=True)[0]\n+\n+                indices_in_stream_per_batch[patches_inds] = indices + index_offset\n+                indices_in_stream_per_subsequence[patches_inds] = indices\n+\n+                per_batch_indices.append(indices_in_stream_per_batch)\n+                per_subsequence_indices.append(indices_in_stream_per_subsequence)\n+                index_offset += num_patches\n+\n+            image_patch_indices_per_batch.append(per_batch_indices)\n+            image_patch_indices_per_subsequence.append(per_subsequence_indices)\n+        return FuyuBatchFeature(\n+            data={\n+                \"images\": images,\n+                \"image_input_ids\": batch_image_input_ids,\n+                \"image_patches\": batch_image_patches,\n+                \"image_patch_indices_per_batch\": image_patch_indices_per_batch,\n+                \"image_patch_indices_per_subsequence\": image_patch_indices_per_subsequence,\n+            }\n+        )\n+\n+    def _further_process_kwargs(\n+        self,\n+        patch_size: Optional[dict[str, int]] = None,\n+        **kwargs,\n+    ) -> dict:\n+        \"\"\"\n+        Process Fuyu-specific kwargs before validation.\n+        \"\"\"\n+        kwargs = super()._further_process_kwargs(**kwargs)\n+        if patch_size is not None:\n+            patch_size = SizeDict(**get_size_dict(patch_size, param_name=\"patch_size\"))\n+        kwargs[\"patch_size\"] = patch_size\n+        return kwargs\n+\n+\n+__all__ = [\"FuyuImageProcessorFast\"]"
      },
      {
        "filename": "tests/models/fuyu/test_image_processing_fuyu.py",
        "status": "modified",
        "additions": 432,
        "deletions": 29,
        "changes": 461,
        "patch": "@@ -1,63 +1,466 @@\n+import io\n import unittest\n \n+import httpx\n import numpy as np\n+import pytest\n+from packaging import version\n \n-from transformers import is_torch_available, is_vision_available\n+from transformers.image_utils import SizeDict\n from transformers.testing_utils import (\n     require_torch,\n+    require_torch_accelerator,\n     require_torchvision,\n     require_vision,\n+    slow,\n+    torch_device,\n )\n+from transformers.utils import is_torch_available, is_vision_available\n+\n+from ...test_image_processing_common import ImageProcessingTestMixin\n \n \n if is_torch_available() and is_vision_available():\n     import torch\n \n-    from transformers import FuyuImageProcessor\n+    from transformers import FuyuImageProcessor, FuyuImageProcessorFast\n \n if is_vision_available():\n     from PIL import Image\n \n \n+class FuyuImageProcessingTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=3,\n+        num_channels=3,\n+        image_size=18,\n+        min_resolution=30,\n+        max_resolution=400,\n+        do_resize=True,\n+        size=None,\n+        do_pad=True,\n+        do_normalize=True,\n+        image_mean=[0.5, 0.5, 0.5],\n+        image_std=[0.5, 0.5, 0.5],\n+        do_rescale=True,\n+        rescale_factor=1 / 255,\n+        patch_size=None,\n+    ):\n+        size = size if size is not None else {\"height\": 180, \"width\": 360}\n+        patch_size = patch_size if patch_size is not None else {\"height\": 30, \"width\": 30}\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.num_channels = num_channels\n+        self.image_size = image_size\n+        self.min_resolution = 30\n+        self.max_resolution = 360\n+        self.do_resize = do_resize\n+        self.size = size\n+        self.do_pad = do_pad\n+        self.do_normalize = do_normalize\n+        self.image_mean = image_mean\n+        self.image_std = image_std\n+        self.do_rescale = do_rescale\n+        self.rescale_factor = rescale_factor\n+        self.patch_size = patch_size\n+\n+    def prepare_image_processor_dict(self):\n+        return {\n+            \"do_resize\": self.do_resize,\n+            \"size\": self.size,\n+            \"do_pad\": self.do_pad,\n+            \"do_normalize\": self.do_normalize,\n+            \"image_mean\": self.image_mean,\n+            \"image_std\": self.image_std,\n+            \"do_rescale\": self.do_rescale,\n+            \"rescale_factor\": self.rescale_factor,\n+            \"patch_size\": self.patch_size,\n+        }\n+\n+    def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=False):\n+        \"\"\"Prepares a batch of images for testing\"\"\"\n+        if equal_resolution:\n+            image_inputs = [\n+                np.random.randint(\n+                    0, 256, (self.num_channels, self.max_resolution, self.max_resolution), dtype=np.uint8\n+                )\n+                for _ in range(self.batch_size)\n+            ]\n+        else:\n+            heights = [\n+                h - (h % 30) for h in np.random.randint(self.min_resolution, self.max_resolution, self.batch_size)\n+            ]\n+            widths = [\n+                w - (w % 30) for w in np.random.randint(self.min_resolution, self.max_resolution, self.batch_size)\n+            ]\n+\n+            image_inputs = [\n+                np.random.randint(0, 256, (self.num_channels, height, width), dtype=np.uint8)\n+                for height, width in zip(heights, widths)\n+            ]\n+\n+        if not numpify and not torchify:\n+            image_inputs = [Image.fromarray(np.moveaxis(img, 0, -1)) for img in image_inputs]\n+\n+        if torchify:\n+            image_inputs = [torch.from_numpy(img) for img in image_inputs]\n+\n+        return image_inputs\n+\n+    def expected_output_image_shape(self, images):\n+        return self.num_channels, self.size[\"height\"], self.size[\"width\"]\n+\n+\n @require_torch\n @require_vision\n @require_torchvision\n-class TestFuyuImageProcessor(unittest.TestCase):\n+class FuyuImageProcessorTest(ImageProcessingTestMixin, unittest.TestCase):\n+    image_processing_class = FuyuImageProcessor\n+    fast_image_processing_class = FuyuImageProcessorFast\n+\n+    # Skip tests that expect pixel_values output\n+    test_cast_dtype = None\n+\n     def setUp(self):\n-        self.size = {\"height\": 160, \"width\": 320}\n-        self.processor = FuyuImageProcessor(size=self.size, padding_value=1.0)\n-        self.batch_size = 3\n-        self.channels = 3\n-        self.height = 300\n-        self.width = 300\n+        self.image_processor_tester = FuyuImageProcessingTester(self)\n+        self.image_processor_dict = self.image_processor_tester.prepare_image_processor_dict()\n \n-        self.image_input = torch.rand(self.batch_size, self.channels, self.height, self.width)\n+        # Initialize image_processor_list (from ImageProcessingTestMixin)\n+        image_processor_list = []\n+        if self.test_slow_image_processor and self.image_processing_class:\n+            image_processor_list.append(self.image_processing_class)\n+        if self.test_fast_image_processor and self.fast_image_processing_class:\n+            image_processor_list.append(self.fast_image_processing_class)\n+        self.image_processor_list = image_processor_list\n \n-        self.image_patch_dim_h = 30\n-        self.image_patch_dim_w = 30\n-        self.sample_image = np.zeros((450, 210, 3), dtype=np.uint8)\n-        self.sample_image_pil = Image.fromarray(self.sample_image)\n+    def test_call_pil(self):\n+        \"\"\"Override to handle Fuyu's custom output structure\"\"\"\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False)\n+            for image in image_inputs:\n+                self.assertIsInstance(image, Image.Image)\n \n-    def test_patches(self):\n-        expected_num_patches = self.processor.get_num_patches(image_height=self.height, image_width=self.width)\n+            encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\")\n+            self.assertIn(\"images\", encoded_images)\n+            self.assertEqual(len(encoded_images.images), 1)\n+\n+            encoded_images = image_processing(image_inputs, return_tensors=\"pt\")\n+            self.assertIn(\"images\", encoded_images)\n+            self.assertEqual(len(encoded_images.images), self.image_processor_tester.batch_size)\n+\n+    def test_call_numpy(self):\n+        \"\"\"Override to handle Fuyu's custom output structure\"\"\"\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, numpify=True)\n+            for image in image_inputs:\n+                self.assertIsInstance(image, np.ndarray)\n+\n+            encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\")\n+            self.assertIn(\"images\", encoded_images)\n+            self.assertEqual(len(encoded_images.images), 1)\n+\n+            encoded_images = image_processing(image_inputs, return_tensors=\"pt\")\n+            self.assertIn(\"images\", encoded_images)\n+            self.assertEqual(len(encoded_images.images), self.image_processor_tester.batch_size)\n+\n+    def test_call_pytorch(self):\n+        \"\"\"Override to handle Fuyu's custom output structure\"\"\"\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n+\n+            for image in image_inputs:\n+                self.assertIsInstance(image, torch.Tensor)\n+\n+            encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\")\n+            self.assertIn(\"images\", encoded_images)\n+            self.assertEqual(len(encoded_images.images), 1)\n \n-        patches_final = self.processor.patchify_image(image=self.image_input)\n-        assert patches_final.shape[1] == expected_num_patches, (\n-            f\"Expected {expected_num_patches} patches, got {patches_final.shape[1]}.\"\n+            encoded_images = image_processing(image_inputs, return_tensors=\"pt\")\n+            self.assertIn(\"images\", encoded_images)\n+            self.assertEqual(len(encoded_images.images), self.image_processor_tester.batch_size)\n+\n+    def test_call_numpy_4_channels(self):\n+        \"\"\"Skip this test as Fuyu doesn't support arbitrary channels\"\"\"\n+        self.skipTest(\"Fuyu processor is designed for 3-channel RGB images\")\n+\n+    def test_slow_fast_equivalence(self):\n+        \"\"\"Override to handle Fuyu's custom output structure\"\"\"\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n+        dummy_image = Image.open(\n+            io.BytesIO(\n+                httpx.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", follow_redirects=True).content\n+            )\n         )\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        encoding_slow = image_processor_slow(dummy_image, return_tensors=\"pt\")\n+        encoding_fast = image_processor_fast(dummy_image, return_tensors=\"pt\")\n+\n+        self._assert_slow_fast_tensors_equivalence(encoding_slow.images[0][0], encoding_fast.images[0][0])\n+\n+    def test_slow_fast_equivalence_batched(self):\n+        \"\"\"Override to handle Fuyu's custom output structure\"\"\"\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n+\n+        dummy_images = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        encoding_slow = image_processor_slow(dummy_images, return_tensors=\"pt\")\n+        encoding_fast = image_processor_fast(dummy_images, return_tensors=\"pt\")\n+\n+        # Compare each image tensor\n+        for slow_img, fast_img in zip(encoding_slow.images, encoding_fast.images):\n+            self._assert_slow_fast_tensors_equivalence(slow_img[0], fast_img[0])\n+\n+    @slow\n+    @require_torch_accelerator\n+    @require_vision\n+    @pytest.mark.torch_compile_test\n+    def test_can_compile_fast_image_processor(self):\n+        if self.fast_image_processing_class is None:\n+            self.skipTest(\"Skipping compilation test as fast image processor is not defined\")\n+        if version.parse(torch.__version__) < version.parse(\"2.3\"):\n+            self.skipTest(reason=\"This test requires torch >= 2.3 to run.\")\n+\n+        torch.compiler.reset()\n+        input_image = torch.randint(0, 255, (3, 224, 224), dtype=torch.uint8)\n+        image_processor = self.fast_image_processing_class(**self.image_processor_dict)\n+        output_eager = image_processor(input_image, device=torch_device, return_tensors=\"pt\")\n+\n+        image_processor = torch.compile(image_processor, mode=\"reduce-overhead\")\n+        output_compiled = image_processor(input_image, device=torch_device, return_tensors=\"pt\")\n+        self._assert_slow_fast_tensors_equivalence(\n+            output_eager.images[0][0], output_compiled.images[0][0], atol=1e-4, rtol=1e-4, mean_atol=1e-5\n+        )\n+\n+    def test_image_processor_properties(self):\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class(**self.image_processor_dict)\n+            self.assertTrue(hasattr(image_processor, \"do_resize\"))\n+            self.assertTrue(hasattr(image_processor, \"size\"))\n+            self.assertTrue(hasattr(image_processor, \"do_pad\"))\n+            self.assertTrue(hasattr(image_processor, \"do_normalize\"))\n+            self.assertTrue(hasattr(image_processor, \"image_mean\"))\n+            self.assertTrue(hasattr(image_processor, \"image_std\"))\n+            self.assertTrue(hasattr(image_processor, \"do_rescale\"))\n+            self.assertTrue(hasattr(image_processor, \"rescale_factor\"))\n+            self.assertTrue(hasattr(image_processor, \"patch_size\"))\n+\n+    def test_patches(self):\n+        \"\"\"Test that patchify_image produces the expected number of patches.\"\"\"\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class(**self.image_processor_dict)\n+            batch_size = 3\n+            channels = 3\n+            height = 300\n+            width = 300\n+            image_input = torch.rand(batch_size, channels, height, width)\n+\n+            expected_num_patches = image_processor.get_num_patches(image_height=height, image_width=width)\n+            patches_final = image_processor.patchify_image(image=image_input)\n+\n+            self.assertEqual(patches_final.shape[1], expected_num_patches)\n+\n+    def test_patches_match_slow_fast(self):\n+        \"\"\"Test that fast processor produces same patches as slow processor.\"\"\"\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast patch equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(\n+                reason=\"Skipping slow/fast patch equivalence test as one of the image processors is not defined\"\n+            )\n+\n+        batch_size = 3\n+        channels = 3\n+        height = 300\n+        width = 300\n+        image_input = torch.rand(batch_size, channels, height, width)\n+\n+        processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        patches_fast = processor_fast.patchify_image(image=image_input)\n+        patches_slow = processor_slow.patchify_image(image=image_input)\n+\n+        self.assertEqual(patches_fast.shape, patches_slow.shape)\n+        torch.testing.assert_close(patches_fast, patches_slow, rtol=1e-4, atol=1e-4)\n \n     def test_scale_to_target_aspect_ratio(self):\n-        # (h:450, w:210) fitting (160, 320) -> (160, 210*160/450)\n-        scaled_image = self.processor.resize(self.sample_image, size=self.size)\n-        self.assertEqual(scaled_image.shape[0], 160)\n-        self.assertEqual(scaled_image.shape[1], 74)\n+        \"\"\"Test that resize maintains aspect ratio correctly.\"\"\"\n+        sample_image = np.zeros((450, 210, 3), dtype=np.uint8)\n+\n+        if self.test_slow_image_processor and self.image_processing_class:\n+            image_processor = self.image_processing_class(**self.image_processor_dict)\n+            scaled_image = image_processor.resize(sample_image, size=self.image_processor_dict[\"size\"])\n+            self.assertEqual(scaled_image.shape[0], 180)\n+            self.assertEqual(scaled_image.shape[1], 84)\n+\n+        if self.test_fast_image_processor and self.fast_image_processing_class:\n+            image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+            sample_tensor = torch.from_numpy(sample_image).permute(2, 0, 1).float()\n+\n+            size_dict = SizeDict(\n+                height=self.image_processor_dict[\"size\"][\"height\"], width=self.image_processor_dict[\"size\"][\"width\"]\n+            )\n+            scaled_image = image_processor_fast.resize(sample_tensor, size=size_dict)\n+\n+            self.assertEqual(scaled_image.shape[1], 180)\n+            self.assertEqual(scaled_image.shape[2], 84)\n \n     def test_apply_transformation_numpy(self):\n-        transformed_image = self.processor.preprocess(self.sample_image).images[0][0]\n-        self.assertEqual(transformed_image.shape[1], 160)\n-        self.assertEqual(transformed_image.shape[2], 320)\n+        \"\"\"Test preprocessing with numpy input.\"\"\"\n+        sample_image = np.zeros((450, 210, 3), dtype=np.uint8)\n+\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class(**self.image_processor_dict)\n+            transformed_image = image_processor.preprocess(sample_image).images[0][0]\n+            self.assertEqual(transformed_image.shape[1], 180)\n+            self.assertEqual(transformed_image.shape[2], 360)\n \n     def test_apply_transformation_pil(self):\n-        transformed_image = self.processor.preprocess(self.sample_image_pil).images[0][0]\n-        self.assertEqual(transformed_image.shape[1], 160)\n-        self.assertEqual(transformed_image.shape[2], 320)\n+        \"\"\"Test preprocessing with PIL input.\"\"\"\n+        sample_image = np.zeros((450, 210, 3), dtype=np.uint8)\n+        sample_image_pil = Image.fromarray(sample_image)\n+\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class(**self.image_processor_dict)\n+            transformed_image = image_processor.preprocess(sample_image_pil).images[0][0]\n+            self.assertEqual(transformed_image.shape[1], 180)\n+            self.assertEqual(transformed_image.shape[2], 360)\n+\n+    def test_preprocess_output_structure(self):\n+        \"\"\"Test that preprocess returns correct output structure.\"\"\"\n+        sample_image = np.zeros((450, 210, 3), dtype=np.uint8)\n+\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class(**self.image_processor_dict)\n+            result = image_processor.preprocess(sample_image)\n+\n+            self.assertIn(\"images\", result)\n+            self.assertIn(\"image_unpadded_heights\", result)\n+            self.assertIn(\"image_unpadded_widths\", result)\n+            self.assertIn(\"image_scale_factors\", result)\n+\n+            self.assertEqual(len(result.images), 1)\n+            self.assertEqual(len(result.images[0]), 1)\n+            self.assertEqual(len(result.image_unpadded_heights), 1)\n+            self.assertEqual(len(result.image_unpadded_widths), 1)\n+            self.assertEqual(len(result.image_scale_factors), 1)\n+\n+    def test_batch_processing(self):\n+        \"\"\"Test processing multiple images.\"\"\"\n+        sample_image = np.zeros((450, 210, 3), dtype=np.uint8)\n+        sample_image_pil = Image.fromarray(sample_image)\n+        images = [sample_image, sample_image_pil]\n+\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class(**self.image_processor_dict)\n+            result = image_processor.preprocess(images)\n+\n+            self.assertEqual(len(result.images), 2)\n+            for img in result.images:\n+                self.assertEqual(len(img), 1)\n+                if hasattr(img[0], \"shape\"):\n+                    if len(img[0].shape) == 3:\n+                        self.assertEqual(img[0].shape[1], 180)\n+                        self.assertEqual(img[0].shape[2], 360)\n+\n+    def test_pad_image_fast(self):\n+        \"\"\"Test that padding works correctly for fast processor.\"\"\"\n+        if not self.test_fast_image_processor or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Fast processor not available\")\n+\n+        from transformers.image_utils import SizeDict\n+\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        small_image = torch.rand(3, 100, 100)\n+        size_dict = SizeDict(height=180, width=360)\n+\n+        padded = image_processor_fast.pad([small_image], pad_size=size_dict, fill_value=1.0)[0]\n+        self.assertEqual(padded.shape[1], 180)\n+        self.assertEqual(padded.shape[2], 360)\n+\n+        self.assertTrue(torch.allclose(padded[:, 100:, :], torch.ones_like(padded[:, 100:, :])))\n+        self.assertTrue(torch.allclose(padded[:, :, 100:], torch.ones_like(padded[:, :, 100:])))\n+\n+    def test_preprocess_with_tokenizer_info(self):\n+        \"\"\"Test preprocess_with_tokenizer_info functionality.\"\"\"\n+        batch_size = 2\n+        subseq_size = 1\n+        channels = 3\n+        image_input = torch.rand(batch_size, subseq_size, channels, 180, 360)\n+        image_present = torch.ones(batch_size, subseq_size, dtype=torch.bool)\n+        image_unpadded_h = torch.tensor([[180], [180]])\n+        image_unpadded_w = torch.tensor([[360], [360]])\n+\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class(**self.image_processor_dict)\n+\n+            result = image_processor.preprocess_with_tokenizer_info(\n+                image_input=image_input,\n+                image_present=image_present,\n+                image_unpadded_h=image_unpadded_h,\n+                image_unpadded_w=image_unpadded_w,\n+                image_placeholder_id=100,\n+                image_newline_id=101,\n+                variable_sized=True,\n+            )\n+\n+            # Check output structure\n+            self.assertIn(\"images\", result)\n+            self.assertIn(\"image_input_ids\", result)\n+            self.assertIn(\"image_patches\", result)\n+            self.assertIn(\"image_patch_indices_per_batch\", result)\n+            self.assertIn(\"image_patch_indices_per_subsequence\", result)\n+\n+            # Check batch structure\n+            self.assertEqual(len(result.images), batch_size)\n+            self.assertEqual(len(result.image_input_ids), batch_size)\n+            self.assertEqual(len(result.image_patches), batch_size)\n+\n+    def test_device_handling_fast(self):\n+        \"\"\"Test that fast processor can handle device placement.\"\"\"\n+        if not self.test_fast_image_processor or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Fast processor not available\")\n+\n+        sample_image = np.zeros((450, 210, 3), dtype=np.uint8)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        if torch.cuda.is_available():\n+            result_cuda = image_processor_fast.preprocess(sample_image, device=\"cuda\")\n+            self.assertEqual(result_cuda.images[0][0].device.type, \"cuda\")\n+\n+        result_cpu = image_processor_fast.preprocess(sample_image, device=\"cpu\")\n+        self.assertEqual(result_cpu.images[0][0].device.type, \"cpu\")\n+\n+    def test_do_not_resize_if_smaller(self):\n+        \"\"\"Test that images smaller than target size are not resized.\"\"\"\n+        if not self.test_fast_image_processor or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Fast processor not available\")\n+\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        small_image = torch.rand(3, 100, 150)\n+        size_dict = SizeDict(height=180, width=360)\n+\n+        resized = image_processor_fast.resize(small_image, size=size_dict)\n+\n+        self.assertEqual(resized.shape[1], 100)\n+        self.assertEqual(resized.shape[2], 150)"
      }
    ],
    "num_files": 7,
    "scraped_at": "2025-11-16T21:16:50.377457"
  },
  {
    "pr_number": 41812,
    "title": "Fix invalid examples in QwenVL model docstrings and add Qwen3VL example",
    "body": "# What does this PR do?\r\nThis PR fixes the non-functional examples for Qwen2-VL and Qwen2.5-VL models, and adds a runnable example for Qwen3VL.\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\n## Before submitting\r\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\n@zucchini-nlp, @Rocketknight1\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker @Cyrilvallez\r\n- vision models: @yonigozlan @molbap\r\n- audio models: @eustlb @ebezzam @vasqu\r\n- multimodal models: @zucchini-nlp\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- generate: @zucchini-nlp (visual-language models) or @gante (all others)\r\n- continuous batching: @remi-or @ArthurZucker @McPatate\r\n- pipelines: @Rocketknight1\r\n- tokenizers: @ArthurZucker and @itazap\r\n- trainer: @SunMarc\r\n- attention: @vasqu @ArthurZucker @CyrilVallez\r\n- model loading (from pretrained, etc): @CyrilVallez\r\n- distributed: @3outeille @ArthurZucker\r\n- CIs: @ydshieh\r\n\r\nIntegrations:\r\n\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization: @SunMarc @MekkCyber\r\n- kernels: @MekkCyber @drbh\r\n- peft: @BenjaminBossan @githubnemo\r\n\r\nDevices/Backends:\r\n\r\n- AMD ROCm: @ivarflakstad\r\n- Intel XPU: @IlyasMoutawwakil\r\n- Ascend NPU: @ivarflakstad \r\n\r\nDocumentation: @stevhliu\r\n\r\nResearch projects are not maintained and should be taken as is.\r\n\r\n -->\r\n",
    "html_url": "https://github.com/huggingface/transformers/pull/41812",
    "created_at": "2025-10-23T12:28:17Z",
    "merged_at": "2025-10-29T12:34:13Z",
    "merge_commit_sha": "5462376a5c9d33963c5249668e1061ccc98dcbce",
    "base_ref": "main",
    "head_sha": "b0b1e5511017beefbc7a96eba8f739ba1cca110d",
    "user": "Xqle",
    "files": [
      {
        "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
        "status": "modified",
        "additions": 19,
        "deletions": 13,
        "changes": 32,
        "patch": "@@ -1453,8 +1453,6 @@ def forward(\n         Example:\n \n         ```python\n-        >>> from PIL import Image\n-        >>> import requests\n         >>> from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration\n \n         >>> model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\")\n@@ -1464,22 +1462,30 @@ def forward(\n             {\n                 \"role\": \"user\",\n                 \"content\": [\n-                    {\"type\": \"image\"},\n-                    {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n+                    {\n+                        \"type\": \"image\",\n+                        \"image\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\",\n+                    },\n+                    {\"type\": \"text\", \"text\": \"Describe the image.\"},\n                 ],\n-            },\n+            }\n         ]\n-        >>> url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n-        >>> image = Image.open(requests.get(url, stream=True).raw)\n \n-        >>> text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n-        >>> inputs = processor(text=[text], images=[image], vision_infos=[vision_infos])\n+        >>> inputs = processor.apply_chat_template(\n+            messages,\n+            tokenize=True,\n+            add_generation_prompt=True,\n+            return_dict=True,\n+            return_tensors=\"pt\"\n+        )\n \n         >>> # Generate\n-        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n-        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n-        \"The image shows a street scene with a red stop sign in the foreground. In the background, there is a large red gate with Chinese characters ...\"\n-        ```\"\"\"\n+        >>> generated_ids = model.generate(**inputs, max_new_tokens=1024)\n+        >>> generated_ids_trimmed = [out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]\n+        >>> output_text = processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+        >>> print(output_text)\n+        ```\n+        \"\"\"\n \n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = ("
      },
      {
        "filename": "src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py",
        "status": "modified",
        "additions": 19,
        "deletions": 13,
        "changes": 32,
        "patch": "@@ -684,8 +684,6 @@ def forward(\n         Example:\n \n         ```python\n-        >>> from PIL import Image\n-        >>> import requests\n         >>> from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration\n \n         >>> model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\")\n@@ -695,22 +693,30 @@ def forward(\n             {\n                 \"role\": \"user\",\n                 \"content\": [\n-                    {\"type\": \"image\"},\n-                    {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n+                    {\n+                        \"type\": \"image\",\n+                        \"image\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\",\n+                    },\n+                    {\"type\": \"text\", \"text\": \"Describe the image.\"},\n                 ],\n-            },\n+            }\n         ]\n-        >>> url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n-        >>> image = Image.open(requests.get(url, stream=True).raw)\n \n-        >>> text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n-        >>> inputs = processor(text=[text], images=[image], vision_infos=[vision_infos])\n+        >>> inputs = processor.apply_chat_template(\n+            messages,\n+            tokenize=True,\n+            add_generation_prompt=True,\n+            return_dict=True,\n+            return_tensors=\"pt\"\n+        )\n \n         >>> # Generate\n-        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n-        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n-        \"The image shows a street scene with a red stop sign in the foreground. In the background, there is a large red gate with Chinese characters ...\"\n-        ```\"\"\"\n+        >>> generated_ids = model.generate(**inputs, max_new_tokens=1024)\n+        >>> generated_ids_trimmed = [out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]\n+        >>> output_text = processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+        >>> print(output_text)\n+        ```\n+        \"\"\"\n \n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = ("
      },
      {
        "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
        "status": "modified",
        "additions": 19,
        "deletions": 13,
        "changes": 32,
        "patch": "@@ -1348,8 +1348,6 @@ def forward(\n         Example:\n \n         ```python\n-        >>> from PIL import Image\n-        >>> import requests\n         >>> from transformers import AutoProcessor, Qwen2VLForConditionalGeneration\n \n         >>> model = Qwen2VLForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n@@ -1359,22 +1357,30 @@ def forward(\n             {\n                 \"role\": \"user\",\n                 \"content\": [\n-                    {\"type\": \"image\"},\n-                    {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n+                    {\n+                        \"type\": \"image\",\n+                        \"image\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\",\n+                    },\n+                    {\"type\": \"text\", \"text\": \"Describe the image.\"},\n                 ],\n-            },\n+            }\n         ]\n-        >>> url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n-        >>> image = Image.open(requests.get(url, stream=True).raw)\n \n-        >>> text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n-        >>> inputs = processor(text=[text], images=[image], vision_infos=[vision_infos])\n+        >>> inputs = processor.apply_chat_template(\n+            messages,\n+            tokenize=True,\n+            add_generation_prompt=True,\n+            return_dict=True,\n+            return_tensors=\"pt\"\n+        )\n \n         >>> # Generate\n-        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n-        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n-        \"The image shows a street scene with a red stop sign in the foreground. In the background, there is a large red gate with Chinese characters ...\"\n-        ```\"\"\"\n+        >>> generated_ids = model.generate(**inputs, max_new_tokens=1024)\n+        >>> generated_ids_trimmed = [out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]\n+        >>> output_text = processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+        >>> print(output_text)\n+        ```\n+        \"\"\"\n \n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = ("
      },
      {
        "filename": "src/transformers/models/qwen3_vl/modeling_qwen3_vl.py",
        "status": "modified",
        "additions": 35,
        "deletions": 1,
        "changes": 36,
        "patch": "@@ -1369,8 +1369,42 @@ def forward(\n             The temporal, height and width of feature shape of each video in LLM.\n \n         Example:\n-            TODO: Add example\n+\n+        ```python\n+        >>> from transformers import AutoProcessor, Qwen3VLForConditionalGeneration\n+\n+        >>> model = Qwen3VLForConditionalGeneration.from_pretrained(\"Qwen/Qwen3-VL-8B-Instruct\")\n+        >>> processor = AutoProcessor.from_pretrained(\"Qwen/Qwen3-VL-8B-Instruct\")\n+\n+        >>> messages = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\n+                        \"type\": \"image\",\n+                        \"image\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\",\n+                    },\n+                    {\"type\": \"text\", \"text\": \"Describe the image.\"},\n+                ],\n+            }\n+        ]\n+\n+        >>> inputs = processor.apply_chat_template(\n+            messages,\n+            tokenize=True,\n+            add_generation_prompt=True,\n+            return_dict=True,\n+            return_tensors=\"pt\"\n+        )\n+\n+        >>> # Generate\n+        >>> generated_ids = model.generate(**inputs, max_new_tokens=1024)\n+        >>> generated_ids_trimmed = [out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]\n+        >>> output_text = processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+        >>> print(output_text)\n+        ```\n         \"\"\"\n+\n         outputs = self.model(\n             input_ids=input_ids,\n             pixel_values=pixel_values,"
      },
      {
        "filename": "src/transformers/models/qwen3_vl/modular_qwen3_vl.py",
        "status": "modified",
        "additions": 35,
        "deletions": 1,
        "changes": 36,
        "patch": "@@ -1134,8 +1134,42 @@ def forward(\n             The temporal, height and width of feature shape of each video in LLM.\n \n         Example:\n-            TODO: Add example\n+\n+        ```python\n+        >>> from transformers import AutoProcessor, Qwen3VLForConditionalGeneration\n+\n+        >>> model = Qwen3VLForConditionalGeneration.from_pretrained(\"Qwen/Qwen3-VL-8B-Instruct\")\n+        >>> processor = AutoProcessor.from_pretrained(\"Qwen/Qwen3-VL-8B-Instruct\")\n+\n+        >>> messages = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\n+                        \"type\": \"image\",\n+                        \"image\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\",\n+                    },\n+                    {\"type\": \"text\", \"text\": \"Describe the image.\"},\n+                ],\n+            }\n+        ]\n+\n+        >>> inputs = processor.apply_chat_template(\n+            messages,\n+            tokenize=True,\n+            add_generation_prompt=True,\n+            return_dict=True,\n+            return_tensors=\"pt\"\n+        )\n+\n+        >>> # Generate\n+        >>> generated_ids = model.generate(**inputs, max_new_tokens=1024)\n+        >>> generated_ids_trimmed = [out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]\n+        >>> output_text = processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+        >>> print(output_text)\n+        ```\n         \"\"\"\n+\n         outputs = self.model(\n             input_ids=input_ids,\n             pixel_values=pixel_values,"
      }
    ],
    "num_files": 5,
    "scraped_at": "2025-11-16T21:16:50.937248"
  },
  {
    "pr_number": 41790,
    "title": "Fix attention mask in mamba layers",
    "body": "# What does this PR do?\r\n\r\nAs per title. It was reported by LFM-VL team that the batched generation is outputting garbage with one of the checkpoints. I found that the masking is not being applied at all for mamba layers\r\n\r\nFirstly, mamba layer do not have a 4D attention weight and thus need a normal attention in 2D. Also we do not need to check if the attention has a certain shape, instead we only make sure it is applied in prefill stage\r\n\r\nThis fixes LFM-VL but Ig all mamba models are affected. I'm still surprised we didn't get issues before and that bigger checkpoints of LFM-VL generated normal text even without proper masking. I'm going to fix other mamba models and tag for review when ready",
    "html_url": "https://github.com/huggingface/transformers/pull/41790",
    "created_at": "2025-10-22T13:29:41Z",
    "merged_at": "2025-10-22T16:15:38Z",
    "merge_commit_sha": "87be5595081364ef99393feeaa60d71db3652679",
    "base_ref": "main",
    "head_sha": "a8b35a0273467ac687ae075a86c3b6fb8f57910d",
    "user": "zucchini-nlp",
    "files": [
      {
        "filename": "src/transformers/models/bamba/modeling_bamba.py",
        "status": "modified",
        "additions": 4,
        "deletions": 3,
        "changes": 7,
        "patch": "@@ -486,20 +486,21 @@ def segment_sum(input_tensor):\n     return tensor_segsum\n \n \n-is_fast_path_available = all((selective_state_update, causal_conv1d_fn, causal_conv1d_update))\n-\n-\n def apply_mask_to_padding_states(hidden_states, attention_mask):\n     \"\"\"\n     Tunes out the hidden states for padding tokens, see https://github.com/state-spaces/mamba/issues/66\n     \"\"\"\n+    # NOTE: attention mask is a 2D boolean tensor\n     if attention_mask is not None and attention_mask.shape[1] > 1 and attention_mask.shape[0] > 1:\n         dtype = hidden_states.dtype\n         hidden_states = (hidden_states * attention_mask[:, :, None]).to(dtype)\n \n     return hidden_states\n \n \n+is_fast_path_available = all((selective_state_update, causal_conv1d_fn, causal_conv1d_update))\n+\n+\n # Adapted from transformers.models.mamba2.modeling_mamba2.Mamba2Mixer\n class BambaMixer(nn.Module):\n     \"\"\""
      },
      {
        "filename": "src/transformers/models/bamba/modular_bamba.py",
        "status": "modified",
        "additions": 1,
        "deletions": 11,
        "changes": 12,
        "patch": "@@ -36,6 +36,7 @@\n )\n from transformers.models.mamba2.modeling_mamba2 import (\n     MambaRMSNormGated,\n+    apply_mask_to_padding_states,\n     pad_tensor_by_size,\n     reshape_into_chunks,\n     segment_sum,\n@@ -203,17 +204,6 @@ class BambaRMSNormGated(MambaRMSNormGated):\n     pass\n \n \n-def apply_mask_to_padding_states(hidden_states, attention_mask):\n-    \"\"\"\n-    Tunes out the hidden states for padding tokens, see https://github.com/state-spaces/mamba/issues/66\n-    \"\"\"\n-    if attention_mask is not None and attention_mask.shape[1] > 1 and attention_mask.shape[0] > 1:\n-        dtype = hidden_states.dtype\n-        hidden_states = (hidden_states * attention_mask[:, :, None]).to(dtype)\n-\n-    return hidden_states\n-\n-\n # Adapted from transformers.models.mamba2.modeling_mamba2.Mamba2Mixer\n class BambaMixer(nn.Module):\n     \"\"\""
      },
      {
        "filename": "src/transformers/models/falcon_h1/modeling_falcon_h1.py",
        "status": "modified",
        "additions": 4,
        "deletions": 3,
        "changes": 7,
        "patch": "@@ -521,20 +521,21 @@ def segment_sum(input_tensor):\n     return tensor_segsum\n \n \n-is_fast_path_available = all((selective_state_update, causal_conv1d_fn, causal_conv1d_update))\n-\n-\n def apply_mask_to_padding_states(hidden_states, attention_mask):\n     \"\"\"\n     Tunes out the hidden states for padding tokens, see https://github.com/state-spaces/mamba/issues/66\n     \"\"\"\n+    # NOTE: attention mask is a 2D boolean tensor\n     if attention_mask is not None and attention_mask.shape[1] > 1 and attention_mask.shape[0] > 1:\n         dtype = hidden_states.dtype\n         hidden_states = (hidden_states * attention_mask[:, :, None]).to(dtype)\n \n     return hidden_states\n \n \n+is_fast_path_available = all((selective_state_update, causal_conv1d_fn, causal_conv1d_update))\n+\n+\n # Adapted from transformers.models.mamba2.modeling_mamba2.Mamba2Mixer\n class FalconH1Mixer(nn.Module):\n     \"\"\""
      },
      {
        "filename": "src/transformers/models/falcon_h1/modular_falcon_h1.py",
        "status": "modified",
        "additions": 1,
        "deletions": 11,
        "changes": 12,
        "patch": "@@ -39,6 +39,7 @@\n )\n from transformers.models.mamba2.modeling_mamba2 import (\n     MambaRMSNormGated,\n+    apply_mask_to_padding_states,\n     pad_tensor_by_size,\n     reshape_into_chunks,\n     segment_sum,\n@@ -285,17 +286,6 @@ def forward(self, hidden_states, gate=None):\n         return hidden_states.to(input_dtype)\n \n \n-def apply_mask_to_padding_states(hidden_states, attention_mask):\n-    \"\"\"\n-    Tunes out the hidden states for padding tokens, see https://github.com/state-spaces/mamba/issues/66\n-    \"\"\"\n-    if attention_mask is not None and attention_mask.shape[1] > 1 and attention_mask.shape[0] > 1:\n-        dtype = hidden_states.dtype\n-        hidden_states = (hidden_states * attention_mask[:, :, None]).to(dtype)\n-\n-    return hidden_states\n-\n-\n # Adapted from transformers.models.mamba2.modeling_mamba2.Mamba2Mixer\n class FalconH1Mixer(nn.Module):\n     \"\"\""
      },
      {
        "filename": "src/transformers/models/granitemoehybrid/modeling_granitemoehybrid.py",
        "status": "modified",
        "additions": 4,
        "deletions": 3,
        "changes": 7,
        "patch": "@@ -322,20 +322,21 @@ def segment_sum(input_tensor):\n     return tensor_segsum\n \n \n-is_fast_path_available = all((selective_state_update, causal_conv1d_fn, causal_conv1d_update))\n-\n-\n def apply_mask_to_padding_states(hidden_states, attention_mask):\n     \"\"\"\n     Tunes out the hidden states for padding tokens, see https://github.com/state-spaces/mamba/issues/66\n     \"\"\"\n+    # NOTE: attention mask is a 2D boolean tensor\n     if attention_mask is not None and attention_mask.shape[1] > 1 and attention_mask.shape[0] > 1:\n         dtype = hidden_states.dtype\n         hidden_states = (hidden_states * attention_mask[:, :, None]).to(dtype)\n \n     return hidden_states\n \n \n+is_fast_path_available = all((selective_state_update, causal_conv1d_fn, causal_conv1d_update))\n+\n+\n # Adapted from transformers.models.mamba2.modeling_mamba2.Mamba2Mixer\n class GraniteMoeHybridMambaLayer(nn.Module):\n     \"\"\""
      },
      {
        "filename": "src/transformers/models/lfm2/modeling_lfm2.py",
        "status": "modified",
        "additions": 5,
        "deletions": 1,
        "changes": 6,
        "patch": "@@ -422,6 +422,7 @@ def apply_mask_to_padding_states(hidden_states, attention_mask):\n     \"\"\"\n     Tunes out the hidden states for padding tokens, see https://github.com/state-spaces/mamba/issues/66\n     \"\"\"\n+    # NOTE: attention mask is a 2D boolean tensor\n     if attention_mask is not None and attention_mask.shape[1] > 1 and attention_mask.shape[0] > 1:\n         dtype = hidden_states.dtype\n         hidden_states = (hidden_states * attention_mask[:, :, None]).to(dtype)\n@@ -665,15 +666,18 @@ def forward(\n             past_key_values=past_key_values,\n             position_ids=position_ids,\n         )\n+        # Skip masking for decoding stage. We check shape here to be compile-friendly\n+        linear_attention = attention_mask if inputs_embeds.shape[1] != 1 else None\n \n         hidden_states = inputs_embeds\n         position_embeddings = self.rotary_emb(hidden_states, position_ids=position_ids)\n \n         # decoder layers\n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n+            layer_mask = causal_mask if decoder_layer.is_attention_layer else linear_attention\n             hidden_states = decoder_layer(\n                 hidden_states,\n-                attention_mask=causal_mask,\n+                attention_mask=layer_mask,\n                 position_embeddings=position_embeddings,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,"
      },
      {
        "filename": "src/transformers/models/lfm2/modular_lfm2.py",
        "status": "modified",
        "additions": 4,
        "deletions": 1,
        "changes": 5,
        "patch": "@@ -473,15 +473,18 @@ def forward(\n             past_key_values=past_key_values,\n             position_ids=position_ids,\n         )\n+        # Skip masking for decoding stage. We check shape here to be compile-friendly\n+        linear_attention = attention_mask if inputs_embeds.shape[1] != 1 else None\n \n         hidden_states = inputs_embeds\n         position_embeddings = self.rotary_emb(hidden_states, position_ids=position_ids)\n \n         # decoder layers\n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n+            layer_mask = causal_mask if decoder_layer.is_attention_layer else linear_attention\n             hidden_states = decoder_layer(\n                 hidden_states,\n-                attention_mask=causal_mask,\n+                attention_mask=layer_mask,\n                 position_embeddings=position_embeddings,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,"
      },
      {
        "filename": "src/transformers/models/lfm2_moe/modeling_lfm2_moe.py",
        "status": "modified",
        "additions": 5,
        "deletions": 1,
        "changes": 6,
        "patch": "@@ -485,6 +485,7 @@ def apply_mask_to_padding_states(hidden_states, attention_mask):\n     \"\"\"\n     Tunes out the hidden states for padding tokens, see https://github.com/state-spaces/mamba/issues/66\n     \"\"\"\n+    # NOTE: attention mask is a 2D boolean tensor\n     if attention_mask is not None and attention_mask.shape[1] > 1 and attention_mask.shape[0] > 1:\n         dtype = hidden_states.dtype\n         hidden_states = (hidden_states * attention_mask[:, :, None]).to(dtype)\n@@ -732,15 +733,18 @@ def forward(\n             past_key_values=past_key_values,\n             position_ids=position_ids,\n         )\n+        # Skip masking for decoding stage. We check shape here to be compile-friendly\n+        linear_attention = attention_mask if inputs_embeds.shape[1] != 1 else None\n \n         hidden_states = inputs_embeds\n         position_embeddings = self.pos_emb(hidden_states, position_ids=position_ids)\n \n         # decoder layers\n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n+            layer_mask = causal_mask if decoder_layer.is_attention_layer else linear_attention\n             hidden_states = decoder_layer(\n                 hidden_states,\n-                attention_mask=causal_mask,\n+                attention_mask=layer_mask,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n                 cache_position=cache_position,"
      },
      {
        "filename": "src/transformers/models/lfm2_moe/modular_lfm2_moe.py",
        "status": "modified",
        "additions": 4,
        "deletions": 1,
        "changes": 5,
        "patch": "@@ -180,15 +180,18 @@ def forward(\n             past_key_values=past_key_values,\n             position_ids=position_ids,\n         )\n+        # Skip masking for decoding stage. We check shape here to be compile-friendly\n+        linear_attention = attention_mask if inputs_embeds.shape[1] != 1 else None\n \n         hidden_states = inputs_embeds\n         position_embeddings = self.pos_emb(hidden_states, position_ids=position_ids)\n \n         # decoder layers\n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n+            layer_mask = causal_mask if decoder_layer.is_attention_layer else linear_attention\n             hidden_states = decoder_layer(\n                 hidden_states,\n-                attention_mask=causal_mask,\n+                attention_mask=layer_mask,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n                 cache_position=cache_position,"
      },
      {
        "filename": "src/transformers/models/mamba2/modeling_mamba2.py",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -117,6 +117,7 @@ def apply_mask_to_padding_states(hidden_states, attention_mask):\n     \"\"\"\n     Tunes out the hidden states for padding tokens, see https://github.com/state-spaces/mamba/issues/66\n     \"\"\"\n+    # NOTE: attention mask is a 2D boolean tensor\n     if attention_mask is not None and attention_mask.shape[1] > 1 and attention_mask.shape[0] > 1:\n         dtype = hidden_states.dtype\n         hidden_states = (hidden_states * attention_mask[:, :, None]).to(dtype)"
      },
      {
        "filename": "src/transformers/models/qwen3_next/modeling_qwen3_next.py",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -430,6 +430,7 @@ def apply_mask_to_padding_states(hidden_states, attention_mask):\n     \"\"\"\n     Tunes out the hidden states for padding tokens, see https://github.com/state-spaces/mamba/issues/66\n     \"\"\"\n+    # NOTE: attention mask is a 2D boolean tensor\n     if attention_mask is not None and attention_mask.shape[1] > 1 and attention_mask.shape[0] > 1:\n         dtype = hidden_states.dtype\n         hidden_states = (hidden_states * attention_mask[:, :, None]).to(dtype)"
      },
      {
        "filename": "tests/models/lfm2_moe/test_modeling_lfm2_moe.py",
        "status": "modified",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "patch": "@@ -220,9 +220,9 @@ def test_model_1a8b_generation(self):\n     def test_model_1a8b_batched_chat_generation(self):\n         prompts = [\"Who are you?\", \"Complete the text: Lorem ipsum dolor \", \"The Meji Restoration in Japan ended\"]\n         EXPECTED_TEXT_COMPLETIONS = [\n-            \"Who are you??  \\nI am an artificial intelligence assistant designed to provide information, answer questions\",\n+            \"Who are you?, a language model designed to assist with information and tasks?  \\nI am\",\n             \"Complete the text: Lorem ipsum dolor ipsum dolor ipsum dolor ipsum dolor ipsum dolor\",\n-            \"The Meji Restoration in Japan ended (1868) marked the:  \\nA) Establishment of a constitutional\",\n+            \"The Meji Restoration in Japan ended or the Meiji Restoration (1868\u20131912) marked a pivotal\",\n         ]\n         set_seed(1789)\n         tokenizer = AutoTokenizer.from_pretrained(\"LiquidAI/LFM2-8B-A1B\", use_fast=False)"
      },
      {
        "filename": "tests/models/lfm2_vl/test_modeling_lfm2_vl.py",
        "status": "modified",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "patch": "@@ -300,7 +300,7 @@ def test_integration_test_batched(self):\n         )\n \n         # Create inputs\n-        text = [\"<image>In this image, we see\", \"<image>In this image, we see\"]\n+        text = [\"<image>In this image, we see\", \"<image>In this image, we see a cat\"]\n         images = [[self.image2], [self.image]]\n         inputs = self.processor(text=text, images=images, return_tensors=\"pt\", padding=True)\n         inputs.to(device=torch_device, dtype=torch.bfloat16)\n@@ -310,6 +310,6 @@ def test_integration_test_batched(self):\n \n         expected_generated_text = [\n             \"In this image, we see a panoramic view of the New York City skyline. The iconic Statics and the New York\",\n-            \"In this image, there is a cat on a bed with a cat on a bed with a cat on a bed with a cat on a bed\",\n+            \"In this image, we see a cat that is lying on its side on a cat bed.\",\n         ]\n         self.assertListEqual(generated_texts, expected_generated_text)"
      }
    ],
    "num_files": 13,
    "scraped_at": "2025-11-16T21:16:54.624374"
  },
  {
    "pr_number": 41778,
    "title": "Fix Qwen3-Omni RoPE",
    "body": "# What does this PR do?\r\n\r\nAs per title, after the last PR with RoPE refactoring, Qwen3-Omni model has issues when loading the model. One of the many sub-configs doesn't call standardization on RoPE which causes issues\r\n\r\nI also updated slow tests with correct checkpoint, right now they use Omni-2 checkpoints and thus do not test anything\r\n\r\ncc @BakerBunker ",
    "html_url": "https://github.com/huggingface/transformers/pull/41778",
    "created_at": "2025-10-22T09:28:17Z",
    "merged_at": "2025-11-06T08:30:40Z",
    "merge_commit_sha": "85c50557b97590538229f99a321ea88d03d6eaa7",
    "base_ref": "main",
    "head_sha": "ff3ae1aee7535114a933097f31b1193f3c2e3793",
    "user": "zucchini-nlp",
    "files": [
      {
        "filename": "src/transformers/models/qwen3_omni_moe/configuration_qwen3_omni_moe.py",
        "status": "modified",
        "additions": 15,
        "deletions": 4,
        "changes": 19,
        "patch": "@@ -347,6 +347,7 @@ def __init__(\n             tie_word_embeddings=tie_word_embeddings,\n             **kwargs,\n         )\n+        rope_config_validation(self, ignore_keys={\"mrope_section\", \"interleaved\", \"mrope_interleaved\"})\n \n \n class Qwen3OmniMoeThinkerConfig(PreTrainedConfig):\n@@ -947,8 +948,10 @@ class Qwen3OmniMoeCode2WavConfig(PreTrainedConfig):\n             Dimensionality of the hidden states and embeddings in the autoregressive transformer decoder.\n         max_position_embeddings (`int`, *optional*, defaults to 8000):\n             Maximum sequence length that the autoregressive decoder can handle. Determines positional embedding size.\n-        rope_theta (`float`, *optional*, defaults to 10000.0):\n-            The base period for rotary position embeddings (RoPE) applied to attention layers.\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         num_attention_heads (`int`, *optional*, defaults to 16):\n             Number of attention heads for each attention layer in the decoder.\n         num_key_value_heads (`int`, *optional*, defaults to 16):\n@@ -998,7 +1001,7 @@ def __init__(\n         codebook_size=2048,\n         hidden_size=1024,\n         max_position_embeddings=8000,\n-        rope_theta=10000,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n         num_attention_heads=16,\n         num_key_value_heads=16,\n         attention_bias=False,\n@@ -1019,7 +1022,6 @@ def __init__(\n         self.codebook_size = codebook_size\n         self.hidden_size = hidden_size\n         self.max_position_embeddings = max_position_embeddings\n-        self.rope_theta = rope_theta\n         self.num_attention_heads = num_attention_heads\n         self.num_key_value_heads = num_key_value_heads\n         self.attention_bias = attention_bias\n@@ -1035,6 +1037,15 @@ def __init__(\n         self.decoder_dim = decoder_dim\n         self.attention_dropout = attention_dropout\n \n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n+\n+        # Validate the correctness of rotary position embeddings parameters\n+        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n+        rope_config_validation(self)\n+\n     @property\n     def layer_types(self):\n         \"\"\""
      },
      {
        "filename": "src/transformers/models/qwen3_omni_moe/modeling_qwen3_omni_moe.py",
        "status": "modified",
        "additions": 18,
        "deletions": 79,
        "changes": 97,
        "patch": "@@ -2687,69 +2687,8 @@ class Qwen3OmniMoeTalkerOutputWithPast(MoeCausalLMOutputWithPast):\n     generation_step: Optional[int] = None\n \n \n-class Qwen3OmniMoeTalkerRotaryEmbedding(nn.Module):\n-    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n-\n-    def __init__(self, config: Qwen3OmniMoeConfig, device=None):\n-        super().__init__()\n-        self.max_seq_len_cached = config.max_position_embeddings\n-        self.original_max_seq_len = config.max_position_embeddings\n-\n-        self.config = config\n-\n-        self.rope_type = self.config.rope_parameters[\"rope_type\"]\n-        rope_init_fn: Callable = self.compute_default_rope_parameters\n-        if self.rope_type != \"default\":\n-            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n-        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n-\n-        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n-\n-    @staticmethod\n-    def compute_default_rope_parameters(\n-        config: Optional[Qwen3OmniMoeConfig] = None,\n-        device: Optional[\"torch.device\"] = None,\n-        seq_len: Optional[int] = None,\n-    ) -> tuple[\"torch.Tensor\", float]:\n-        \"\"\"\n-        Computes the inverse frequencies according to the original RoPE implementation\n-        Args:\n-            config ([`~transformers.PreTrainedConfig`]):\n-                The model configuration.\n-            device (`torch.device`):\n-                The device to use for initialization of the inverse frequencies.\n-            seq_len (`int`, *optional*):\n-                The current sequence length. Unused for this type of RoPE.\n-        Returns:\n-            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n-            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n-        \"\"\"\n-        base = config.rope_parameters[\"rope_theta\"]\n-        dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n-\n-        attention_factor = 1.0  # Unused in this type of RoPE\n-\n-        # Compute the inverse frequencies\n-        inv_freq = 1.0 / (\n-            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n-        )\n-        return inv_freq, attention_factor\n-\n-    @torch.no_grad()\n-    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n-    def forward(self, x, position_ids):\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n-        position_ids_expanded = position_ids[:, None, :].float()\n-\n-        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n-            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos() * self.attention_scaling\n-            sin = emb.sin() * self.attention_scaling\n-\n-        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+class Qwen3OmniMoeTalkerRotaryEmbedding(Qwen3OmniMoeThinkerTextRotaryEmbedding):\n+    pass\n \n \n class Qwen3OmniMoeTalkerTextMLP(nn.Module):\n@@ -3823,7 +3762,7 @@ def _get_talker_user_parts(\n     ):\n         user_talker_part = torch.empty(\n             (1, segment_end_index - im_start_index, self.config.talker_config.text_config.hidden_size),\n-            device=self.talker.device,\n+            device=thinker_hidden.device,\n             dtype=self.talker.dtype,\n         )\n \n@@ -3832,18 +3771,18 @@ def _get_talker_user_parts(\n         # Multimodal data exists\n         if user_mm_mask.any():\n             user_thinker_hidden_mm = thinker_hidden[:, im_start_index:segment_end_index][user_mm_mask]\n-            mm_hidden = self.talker.hidden_projection(user_thinker_hidden_mm).to(self.talker.device)\n+            mm_hidden = self.talker.hidden_projection(user_thinker_hidden_mm).to(thinker_hidden.device)\n             user_talker_part[user_mm_mask] = mm_hidden\n         user_thinker_embed = thinker_embed[:, im_start_index:segment_end_index][~user_mm_mask]\n-        user_text_hidden = self.talker.text_projection(user_thinker_embed).to(self.talker.device)\n+        user_text_hidden = self.talker.text_projection(user_thinker_embed).to(thinker_hidden.device)\n         user_talker_part[~user_mm_mask] = user_text_hidden\n         return user_talker_part\n \n     def _get_talker_assistant_parts(\n         self, im_start_index, segment_end_index, speaker_id, thinker_embed, tts_pad_embed, tts_bos_embed, tts_eos_embed\n     ):\n         assistant_hidden = self.talker.text_projection(thinker_embed[:, im_start_index:segment_end_index]).to(\n-            self.talker.device\n+            tts_pad_embed.device\n         )  # [1 t d]\n         assistant_text_hidden = torch.cat(\n             (\n@@ -3865,17 +3804,17 @@ def _get_talker_assistant_parts(\n                     self.config.talker_config.codec_bos_id,\n                 ]\n             ],\n-            device=self.talker.device,\n+            device=tts_pad_embed.device,\n             dtype=torch.long,\n         )\n         assistant_codec_hidden = torch.cat(\n             (\n                 torch.zeros(\n                     (1, 3, self.config.talker_config.text_config.hidden_size),\n-                    device=self.talker.device,\n+                    device=tts_pad_embed.device,\n                     dtype=self.talker.dtype,\n                 ),\n-                self.talker.get_input_embeddings()(codec_special_tokens).to(self.talker.device),\n+                self.talker.get_input_embeddings()(codec_special_tokens).to(tts_pad_embed.device),\n             ),\n             dim=1,\n         )\n@@ -3991,31 +3930,31 @@ def generate(\n         thinker_result = self.thinker.generate(input_ids=input_ids, **thinker_kwargs)\n \n         if not generate_audio:\n-            return thinker_result, None\n+            return thinker_result\n \n         # 2. Prepare talker input\n         thinker_embed = torch.cat([hidden_states[0] for hidden_states in thinker_result.hidden_states], dim=1).to(\n-            self.talker.device\n+            input_ids.device\n         )  # [1 t d]\n         thinker_hidden = torch.cat(\n             [\n                 hidden_states[self.config.talker_config.accept_hidden_layer]\n                 for hidden_states in thinker_result.hidden_states\n             ],\n             dim=1,\n-        ).to(self.talker.device)  # [1 t d]\n+        ).to(input_ids.device)  # [1 t d]\n         im_start_indexes = torch.cat(\n             (\n                 torch.nonzero(input_ids[0] == self.config.im_start_token_id).squeeze(),\n                 torch.tensor([thinker_result.sequences.shape[-1]], device=input_ids.device, dtype=input_ids.dtype),\n             ),\n             dim=-1,\n-        ).to(self.talker.device)  # Shape [n_starts + 1]; Take batch 0 since batched inference is not supported here.\n+        )  # Shape [n_starts + 1]; Take batch 0 since batched inference is not supported here.\n         multimodal_mask = (\n             (thinker_result.sequences == self.config.thinker_config.audio_token_id) |\n             (thinker_result.sequences == self.config.thinker_config.image_token_id) |\n             (thinker_result.sequences == self.config.thinker_config.video_token_id)\n-        ).to(self.talker.device)  # [1 t] # fmt: skip\n+        ).to(input_ids.device)  # [1 t] # fmt: skip\n \n         talker_special_tokens = torch.tensor(\n             [[self.config.tts_bos_token_id, self.config.tts_eos_token_id, self.config.tts_pad_token_id]],\n@@ -4024,7 +3963,7 @@ def generate(\n         )\n         tts_bos_embed, tts_eos_embed, tts_pad_embed = (\n             self.talker.text_projection(self.thinker.get_input_embeddings()(talker_special_tokens))\n-            .to(self.talker.device)\n+            .to(input_ids.device)\n             .chunk(3, dim=1)\n         )  # 3 * [1 1 d]\n \n@@ -4063,8 +4002,8 @@ def generate(\n                 continue\n             else:\n                 raise AssertionError(\"Expect role id after <|im_start|> (assistant, user, system)\")\n-        talker_input_embed = torch.cat([embed.to(self.talker.device) for embed in talker_input_embeds], dim=1)\n-        talker_input_id = torch.cat([embed.to(self.talker.device) for embed in talker_input_ids], dim=1)\n+        talker_input_embed = torch.cat([embed.to(input_ids.device) for embed in talker_input_embeds], dim=1)\n+        talker_input_id = torch.cat([embed.to(input_ids.device) for embed in talker_input_ids], dim=1)\n         talker_result = self.talker.generate(\n             inputs_embeds=talker_input_embed,\n             trailing_text_hidden=trailing_text_hidden,\n@@ -4079,7 +4018,7 @@ def generate(\n         )\n         talker_wavs = self.code2wav.chunked_decode(talker_codes, chunk_size=300, left_context_size=25)\n \n-        return thinker_result, talker_wavs.float()\n+        return thinker_result.sequences, talker_wavs.float()\n \n \n __all__ = ["
      },
      {
        "filename": "src/transformers/models/qwen3_omni_moe/modular_qwen3_omni_moe.py",
        "status": "modified",
        "additions": 32,
        "deletions": 22,
        "changes": 54,
        "patch": "@@ -217,7 +217,7 @@ def __init__(\n         # Validate the correctness of rotary position embeddings parameters\n         rope_theta = kwargs.get(\"rope_theta\", 1000000.0)\n         standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        rope_config_validation(self, ignore_keys={\"mrope_section\", \"interleaved\", \"mrope_interleaved\"})\n \n \n class Qwen3OmniMoeThinkerConfig(Qwen2_5OmniThinkerConfig):\n@@ -581,8 +581,10 @@ class Qwen3OmniMoeCode2WavConfig(PreTrainedConfig):\n             Dimensionality of the hidden states and embeddings in the autoregressive transformer decoder.\n         max_position_embeddings (`int`, *optional*, defaults to 8000):\n             Maximum sequence length that the autoregressive decoder can handle. Determines positional embedding size.\n-        rope_theta (`float`, *optional*, defaults to 10000.0):\n-            The base period for rotary position embeddings (RoPE) applied to attention layers.\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         num_attention_heads (`int`, *optional*, defaults to 16):\n             Number of attention heads for each attention layer in the decoder.\n         num_key_value_heads (`int`, *optional*, defaults to 16):\n@@ -632,7 +634,7 @@ def __init__(\n         codebook_size=2048,\n         hidden_size=1024,\n         max_position_embeddings=8000,\n-        rope_theta=10000,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n         num_attention_heads=16,\n         num_key_value_heads=16,\n         attention_bias=False,\n@@ -653,7 +655,6 @@ def __init__(\n         self.codebook_size = codebook_size\n         self.hidden_size = hidden_size\n         self.max_position_embeddings = max_position_embeddings\n-        self.rope_theta = rope_theta\n         self.num_attention_heads = num_attention_heads\n         self.num_key_value_heads = num_key_value_heads\n         self.attention_bias = attention_bias\n@@ -669,6 +670,15 @@ def __init__(\n         self.decoder_dim = decoder_dim\n         self.attention_dropout = attention_dropout\n \n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n+\n+        # Validate the correctness of rotary position embeddings parameters\n+        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n+        rope_config_validation(self)\n+\n     @property\n     def layer_types(self):\n         \"\"\"\n@@ -1681,7 +1691,7 @@ class Qwen3OmniMoeTalkerOutputWithPast(MoeCausalLMOutputWithPast):\n     generation_step: Optional[int] = None\n \n \n-class Qwen3OmniMoeTalkerRotaryEmbedding(Qwen3RotaryEmbedding):\n+class Qwen3OmniMoeTalkerRotaryEmbedding(Qwen3OmniMoeThinkerTextRotaryEmbedding):\n     pass\n \n \n@@ -2312,7 +2322,7 @@ def _get_talker_user_parts(\n     ):\n         user_talker_part = torch.empty(\n             (1, segment_end_index - im_start_index, self.config.talker_config.text_config.hidden_size),\n-            device=self.talker.device,\n+            device=thinker_hidden.device,\n             dtype=self.talker.dtype,\n         )\n \n@@ -2321,18 +2331,18 @@ def _get_talker_user_parts(\n         # Multimodal data exists\n         if user_mm_mask.any():\n             user_thinker_hidden_mm = thinker_hidden[:, im_start_index:segment_end_index][user_mm_mask]\n-            mm_hidden = self.talker.hidden_projection(user_thinker_hidden_mm).to(self.talker.device)\n+            mm_hidden = self.talker.hidden_projection(user_thinker_hidden_mm).to(thinker_hidden.device)\n             user_talker_part[user_mm_mask] = mm_hidden\n         user_thinker_embed = thinker_embed[:, im_start_index:segment_end_index][~user_mm_mask]\n-        user_text_hidden = self.talker.text_projection(user_thinker_embed).to(self.talker.device)\n+        user_text_hidden = self.talker.text_projection(user_thinker_embed).to(thinker_hidden.device)\n         user_talker_part[~user_mm_mask] = user_text_hidden\n         return user_talker_part\n \n     def _get_talker_assistant_parts(\n         self, im_start_index, segment_end_index, speaker_id, thinker_embed, tts_pad_embed, tts_bos_embed, tts_eos_embed\n     ):\n         assistant_hidden = self.talker.text_projection(thinker_embed[:, im_start_index:segment_end_index]).to(\n-            self.talker.device\n+            tts_pad_embed.device\n         )  # [1 t d]\n         assistant_text_hidden = torch.cat(\n             (\n@@ -2354,17 +2364,17 @@ def _get_talker_assistant_parts(\n                     self.config.talker_config.codec_bos_id,\n                 ]\n             ],\n-            device=self.talker.device,\n+            device=tts_pad_embed.device,\n             dtype=torch.long,\n         )\n         assistant_codec_hidden = torch.cat(\n             (\n                 torch.zeros(\n                     (1, 3, self.config.talker_config.text_config.hidden_size),\n-                    device=self.talker.device,\n+                    device=tts_pad_embed.device,\n                     dtype=self.talker.dtype,\n                 ),\n-                self.talker.get_input_embeddings()(codec_special_tokens).to(self.talker.device),\n+                self.talker.get_input_embeddings()(codec_special_tokens).to(tts_pad_embed.device),\n             ),\n             dim=1,\n         )\n@@ -2480,31 +2490,31 @@ def generate(\n         thinker_result = self.thinker.generate(input_ids=input_ids, **thinker_kwargs)\n \n         if not generate_audio:\n-            return thinker_result, None\n+            return thinker_result\n \n         # 2. Prepare talker input\n         thinker_embed = torch.cat([hidden_states[0] for hidden_states in thinker_result.hidden_states], dim=1).to(\n-            self.talker.device\n+            input_ids.device\n         )  # [1 t d]\n         thinker_hidden = torch.cat(\n             [\n                 hidden_states[self.config.talker_config.accept_hidden_layer]\n                 for hidden_states in thinker_result.hidden_states\n             ],\n             dim=1,\n-        ).to(self.talker.device)  # [1 t d]\n+        ).to(input_ids.device)  # [1 t d]\n         im_start_indexes = torch.cat(\n             (\n                 torch.nonzero(input_ids[0] == self.config.im_start_token_id).squeeze(),\n                 torch.tensor([thinker_result.sequences.shape[-1]], device=input_ids.device, dtype=input_ids.dtype),\n             ),\n             dim=-1,\n-        ).to(self.talker.device)  # Shape [n_starts + 1]; Take batch 0 since batched inference is not supported here.\n+        )  # Shape [n_starts + 1]; Take batch 0 since batched inference is not supported here.\n         multimodal_mask = (\n             (thinker_result.sequences == self.config.thinker_config.audio_token_id) |\n             (thinker_result.sequences == self.config.thinker_config.image_token_id) |\n             (thinker_result.sequences == self.config.thinker_config.video_token_id)\n-        ).to(self.talker.device)  # [1 t] # fmt: skip\n+        ).to(input_ids.device)  # [1 t] # fmt: skip\n \n         talker_special_tokens = torch.tensor(\n             [[self.config.tts_bos_token_id, self.config.tts_eos_token_id, self.config.tts_pad_token_id]],\n@@ -2513,7 +2523,7 @@ def generate(\n         )\n         tts_bos_embed, tts_eos_embed, tts_pad_embed = (\n             self.talker.text_projection(self.thinker.get_input_embeddings()(talker_special_tokens))\n-            .to(self.talker.device)\n+            .to(input_ids.device)\n             .chunk(3, dim=1)\n         )  # 3 * [1 1 d]\n \n@@ -2552,8 +2562,8 @@ def generate(\n                 continue\n             else:\n                 raise AssertionError(\"Expect role id after <|im_start|> (assistant, user, system)\")\n-        talker_input_embed = torch.cat([embed.to(self.talker.device) for embed in talker_input_embeds], dim=1)\n-        talker_input_id = torch.cat([embed.to(self.talker.device) for embed in talker_input_ids], dim=1)\n+        talker_input_embed = torch.cat([embed.to(input_ids.device) for embed in talker_input_embeds], dim=1)\n+        talker_input_id = torch.cat([embed.to(input_ids.device) for embed in talker_input_ids], dim=1)\n         talker_result = self.talker.generate(\n             inputs_embeds=talker_input_embed,\n             trailing_text_hidden=trailing_text_hidden,\n@@ -2568,7 +2578,7 @@ def generate(\n         )\n         talker_wavs = self.code2wav.chunked_decode(talker_codes, chunk_size=300, left_context_size=25)\n \n-        return thinker_result, talker_wavs.float()\n+        return thinker_result.sequences, talker_wavs.float()\n \n \n class Qwen3OmniMoeProcessorKwargs(Qwen2_5OmniProcessorKwargs):"
      },
      {
        "filename": "tests/models/qwen3_omni_moe/test_modeling_qwen3_omni_moe.py",
        "status": "modified",
        "additions": 34,
        "deletions": 32,
        "changes": 66,
        "patch": "@@ -619,7 +619,9 @@ def test_get_rope_index_video_with_audio(self):\n @require_torch\n class Qwen2_5OmniModelIntegrationTest(unittest.TestCase):\n     def setUp(self):\n-        self.processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-Omni-7B\")\n+        self.processor = AutoProcessor.from_pretrained(\n+            \"Qwen/Qwen3-Omni-30B-A3B-Instruct\", min_pixels=28 * 28, max_pixels=56 * 56\n+        )\n         self.audio_url = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/glass-breaking-151256.mp3\"\n         self.audio_url_additional = (\n             \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/f2641_0_throatclearing.wav\"\n@@ -650,7 +652,7 @@ def tearDown(self):\n     @slow\n     def test_small_model_integration_test(self):\n         model = Qwen3OmniMoeForConditionalGeneration.from_pretrained(\n-            \"Qwen/Qwen2.5-Omni-7B\", dtype=torch.bfloat16, device_map=\"auto\"\n+            \"Qwen/Qwen3-Omni-30B-A3B-Instruct\", dtype=torch.bfloat16, device_map=\"auto\"\n         )\n \n         text = self.processor.apply_chat_template(self.messages, tokenize=False, add_generation_prompt=True)\n@@ -660,35 +662,35 @@ def test_small_model_integration_test(self):\n \n         expected_input_ids = torch.tensor(\n             [\n-                151644,\n-                8948,\n-                198,\n-                2610,\n-                525,\n-                264,\n-                10950,\n-                17847,\n-                13,\n-                151645,\n-                198,\n                 151644,\n                 872,\n                 198,\n-                151647,\n-                151646,\n-                151646,\n+                151669,\n+                151675,\n+                151675,\n+                151675,\n+                151675,\n+                151675,\n+                151675,\n+                151675,\n+                151675,\n+                151675,\n+                151675,\n+                151675,\n+                151675,\n+                151675,\n             ]\n         )\n-        assert torch.allclose(expected_input_ids, inputs.input_ids[0][:17], atol=3e-3)\n+        torch.allclose(expected_input_ids, inputs.input_ids[0][:17], atol=3e-3)\n \n         expected_pixel_slice = torch.tensor(\n             [\n-                [0.8792, 0.8792, 0.9084],\n-                [1.1858, 1.1858, 1.2296],\n-                [1.2004, 1.2004, 1.2150],\n-                [1.4340, 1.4340, 1.4194],\n-                [1.3902, 1.4048, 1.4194],\n-                [1.5216, 1.5362, 1.5362],\n+                [0.5234, 0.6016, 0.6562],\n+                [0.9297, 0.9375, 0.9453],\n+                [0.4902, 0.5078, 0.4902],\n+                [0.8438, 0.8438, 0.8359],\n+                [0.9688, 0.9688, 0.9688],\n+                [0.9609, 0.9531, 0.9531],\n             ],\n             dtype=torch.bfloat16,\n             device=\"cpu\",\n@@ -703,7 +705,7 @@ def test_small_model_integration_test(self):\n         )\n \n         EXPECTED_DECODED_TEXT = Expectations({\n-            (\"cuda\", (8, 6)): \"system\\nYou are a helpful assistant.\\nuser\\nWhat's that sound and what kind of dog is this?\\nassistant\\nThe sound is glass shattering, and the dog is a Labrador Retriever.\",\n+            (\"cuda\", (8, 6)): \"user\\nWhat's that sound and what kind of dog is this?\\nassistant\\nBased on the audio and visual information, here is a breakdown of what you're hearing and seeing:-\",\n             (\"rocm\", (9, 4)): \"system\\nYou are a helpful assistant.\\nuser\\nWhat's that sound and what kind of dog is this?\\nassistant\\nThe sound is glass shattering, and the dog is a Labrador Retriever.\",\n         }).get_expectation()  # fmt: skip\n \n@@ -713,7 +715,7 @@ def test_small_model_integration_test(self):\n     @slow\n     def test_small_model_integration_test_batch(self):\n         model = Qwen3OmniMoeForConditionalGeneration.from_pretrained(\n-            \"Qwen/Qwen2.5-Omni-7B\", dtype=torch.bfloat16, device_map=\"auto\"\n+            \"Qwen/Qwen3-Omni-30B-A3B-Instruct\", dtype=torch.bfloat16, device_map=\"auto\"\n         )\n         text = self.processor.apply_chat_template(self.messages, tokenize=False, add_generation_prompt=True)\n         inputs = self.processor(\n@@ -735,8 +737,8 @@ def test_small_model_integration_test_batch(self):\n                     \"system\\nYou are a helpful assistant.\\nuser\\nWhat's that sound and what kind of dog is this?\\nassistant\\nThe sound is of glass shattering, and the dog in the picture is a Labrador Retriever\",\n                 ],\n                 (\"cuda\", 8): [\n-                    \"system\\nYou are a helpful assistant.\\nuser\\nWhat's that sound and what kind of dog is this?\\nassistant\\nThe sound is glass shattering, and the dog is a Labrador Retriever.\",\n-                    \"system\\nYou are a helpful assistant.\\nuser\\nWhat's that sound and what kind of dog is this?\\nassistant\\nThe sound is glass shattering, and the dog is a Labrador Retriever.\",\n+                    \"user\\nWhat's that sound and what kind of dog is this?\\nassistant\\nBased on the audio and visual information, here is a breakdown of what you're hearing and seeing:\\n\\n\",\n+                    \"user\\nWhat's that sound and what kind of dog is this?\\nassistant\\nBased on the audio and visual information, here is a breakdown of what you're hearing and seeing:\\n\\n\"\n                 ],\n                 (\"rocm\", (9, 4)): [\n                     \"system\\nYou are a helpful assistant.\\nuser\\nWhat's that sound and what kind of dog is this?\\nassistant\\nThe sound is glass shattering, and the dog is a Labrador Retriever.\",\n@@ -751,7 +753,7 @@ def test_small_model_integration_test_batch(self):\n     @slow\n     def test_small_model_integration_test_multiturn(self):\n         model = Qwen3OmniMoeForConditionalGeneration.from_pretrained(\n-            \"Qwen/Qwen2.5-Omni-7B\", dtype=torch.bfloat16, device_map=\"auto\"\n+            \"Qwen/Qwen3-Omni-30B-A3B-Instruct\", dtype=torch.bfloat16, device_map=\"auto\"\n         )\n \n         messages = [\n@@ -787,7 +789,7 @@ def test_small_model_integration_test_multiturn(self):\n             **inputs, thinker_temperature=0, thinker_do_sample=False, return_audio=False, thinker_max_new_tokens=20\n         )\n \n-        EXPECTED_DECODED_TEXT = \"system\\nYou are a helpful assistant.\\nuser\\nWhat's that sound and what kind of dog is this?\\nassistant\\nThe sound is glass shattering, and the dog appears to be a Labrador Retriever.\\nuser\\nHow about this one?\\nassistant\\nThe sound is a cough.\"\n+        EXPECTED_DECODED_TEXT = \"user\\nWhat's that sound and what kind of dog is this?\\nassistant\\nThe sound is glass shattering, and the dog appears to be a Labrador Retriever.\\nuser\\nHow about this one?\\nassistant\\nThe sound is a person coughing.\"\n \n         self.assertEqual(\n             self.processor.decode(output[0], skip_special_tokens=True),\n@@ -797,7 +799,7 @@ def test_small_model_integration_test_multiturn(self):\n     @slow\n     def test_small_model_integration_test_w_audio(self):\n         model = Qwen3OmniMoeForConditionalGeneration.from_pretrained(\n-            \"Qwen/Qwen2.5-Omni-7B\", dtype=torch.bfloat16, device_map=\"auto\"\n+            \"Qwen/Qwen3-Omni-30B-A3B-Instruct\", dtype=torch.bfloat16, device_map=\"auto\"\n         )\n         audio_url = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/guess_age_gender.wav\"\n \n@@ -834,7 +836,7 @@ def test_small_model_integration_test_w_audio(self):\n         EXPECTED_DECODED_TEXTS = Expectations(\n             {\n                 (\"cuda\", 7): \"system\\nYou are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\\nuser\\n\\nassistant\\nWell, I can try. But it's not always that accurate. I might be able to make\",\n-                (\"cuda\", 8): \"system\\nYou are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\\nuser\\n\\nassistant\\nWell, I can't really guess your age and gender just from your voice. There are so many\",\n+                (\"cuda\", 8): \"'system\\nYou are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\\nuser\\n\\nassistant\\nYes, I can analyze audio inputs to understand spoken content, and I can also make inferences about'\",\n             }\n         )  # fmt: skip\n         EXPECTED_DECODED_TEXT = EXPECTED_DECODED_TEXTS.get_expectation()\n@@ -850,7 +852,7 @@ def test_small_model_integration_test_w_audio(self):\n     @require_torch_gpu\n     def test_small_model_integration_test_batch_flashatt2(self):\n         model = Qwen3OmniMoeForConditionalGeneration.from_pretrained(\n-            \"Qwen/Qwen2.5-Omni-7B\",\n+            \"Qwen/Qwen3-Omni-30B-A3B-Instruct\",\n             dtype=torch.bfloat16,\n             attn_implementation=\"flash_attention_2\",\n             device_map=\"auto\","
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T21:16:57.130942"
  },
  {
    "pr_number": 41765,
    "title": "[kernels]\u00a0Add Tests & CI for kernels",
    "body": "# What does this PR do?\r\n\r\nAdds tests for kernels, and proper daily CI, and slack notifications\r\n\r\nrun example : https://github.com/huggingface/transformers/actions/runs/18688016017/job/53285883834",
    "html_url": "https://github.com/huggingface/transformers/pull/41765",
    "created_at": "2025-10-21T11:00:27Z",
    "merged_at": "2025-11-03T15:36:52Z",
    "merge_commit_sha": "a623cda4271c739bd53b874b713d4479a19ff907",
    "base_ref": "main",
    "head_sha": "24ae78111b98586cd7af30fa6961969488a98dd7",
    "user": "MekkCyber",
    "files": [
      {
        "filename": ".github/workflows/self-scheduled-caller.yml",
        "status": "modified",
        "additions": 12,
        "deletions": 0,
        "changes": 12,
        "patch": "@@ -118,3 +118,15 @@ jobs:\n       report_repo_id: hf-internal-testing/transformers_daily_ci\n       commit_sha: ${{ github.sha }}\n     secrets: inherit\n+\n+  kernels-ci:\n+    name: Kernels CI\n+    uses: ./.github/workflows/self-scheduled.yml\n+    with:\n+      job: run_kernels_gpu\n+      slack_report_channel: \"#transformers-ci-daily-kernels\"\n+      docker: huggingface/transformers-all-latest-gpu\n+      ci_event: Daily CI\n+      report_repo_id: hf-internal-testing/transformers_daily_ci\n+      commit_sha: ${{ github.sha }}\n+    secrets: inherit\n\\ No newline at end of file"
      },
      {
        "filename": ".github/workflows/self-scheduled.yml",
        "status": "modified",
        "additions": 65,
        "deletions": 0,
        "changes": 65,
        "patch": "@@ -463,6 +463,70 @@ jobs:\n           name: ${{ env.machine_type }}_run_quantization_torch_gpu_${{ env.matrix_folders }}_test_reports\n           path: /transformers/reports/${{ env.machine_type }}_run_quantization_torch_gpu_${{ matrix.folders }}_test_reports\n \n+  run_kernels_gpu:\n+    if: ${{ inputs.job == 'run_kernels_gpu' }}\n+    name: Kernel tests\n+    strategy:\n+      fail-fast: false\n+      matrix:\n+        machine_type: [aws-g5-4xlarge-cache]\n+    runs-on:\n+      group: '${{ matrix.machine_type }}'\n+    container:\n+      image: ${{ inputs.docker }}\n+      options: --gpus all --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n+    steps:\n+      - name: Update clone\n+        working-directory: /transformers\n+        run: git fetch && git checkout ${{ inputs.commit_sha || github.sha }}\n+\n+      - name: Reinstall transformers in edit mode\n+        working-directory: /transformers\n+        run: python3 -m pip uninstall -y transformers && python3 -m pip install -e .[testing]\n+  \n+      - name: Install kernels\n+        working-directory: /transformers\n+        run: python3 -m pip install -U kernels\n+  \n+      - name: NVIDIA-SMI\n+        run: nvidia-smi\n+\n+      - name: Environment\n+        working-directory: /transformers\n+        run: python3 utils/print_env.py\n+\n+      - name: Show installed libraries and their versions\n+        working-directory: /transformers\n+        run: pip freeze\n+\n+      - name: Set `machine_type` for report and artifact names\n+        working-directory: /transformers\n+        shell: bash\n+        run: |\n+          if [ \"${{ matrix.machine_type }}\" = \"aws-g5-4xlarge-cache\" ]; then\n+            machine_type=single-gpu\n+          else\n+            machine_type=${{ matrix.machine_type }}\n+          fi\n+          echo \"machine_type=$machine_type\" >> $GITHUB_ENV\n+    \n+      - name: Run kernel tests on GPU\n+        working-directory: /transformers\n+        run: |\n+          python3 -m pytest -v --make-reports=${{ env.machine_type }}_run_kernels_gpu_test_reports tests/kernels/test_kernels.py\n+\n+      - name: Failure short reports\n+        if: ${{ failure() }}\n+        continue-on-error: true\n+        run: cat /transformers/reports/${{ env.machine_type }}_run_kernels_gpu_test_reports/failures_short.txt\n+\n+      - name: \"Test suite reports artifacts: ${{ env.machine_type }}_run_kernels_gpu_test_reports\"\n+        if: ${{ always() }}\n+        uses: actions/upload-artifact@v4\n+        with:\n+          name: ${{ env.machine_type }}_run_kernels_gpu_test_reports\n+          path: /transformers/reports/${{ env.machine_type }}_run_kernels_gpu_test_reports\n+\n   run_extract_warnings:\n     # Let's only do this for the job `run_models_gpu` to simplify the (already complex) logic.\n     if: ${{ always() && inputs.job == 'run_models_gpu' }}\n@@ -515,6 +579,7 @@ jobs:\n       run_examples_gpu,\n       run_torch_cuda_extensions_gpu,\n       run_quantization_torch_gpu,\n+      run_kernels_gpu,\n       run_extract_warnings\n     ]\n     if: always() && !cancelled()"
      },
      {
        "filename": "src/transformers/integrations/hub_kernels.py",
        "status": "modified",
        "additions": 7,
        "deletions": 4,
        "changes": 11,
        "patch": "@@ -51,10 +51,13 @@\n             )\n         },\n         \"RMSNorm\": {\n-            \"cuda\": LayerRepository(\n-                repo_id=\"kernels-community/liger_kernels\",\n-                layer_name=\"LigerRMSNorm\",\n-            ),\n+            \"cuda\": {\n+                Mode.INFERENCE: LayerRepository(\n+                    repo_id=\"kernels-community/liger_kernels\",\n+                    layer_name=\"LigerRMSNorm\",\n+                    # revision=\"pure-layer-test\",\n+                ),\n+            },\n             \"rocm\": {\n                 Mode.INFERENCE: LayerRepository(\n                     repo_id=\"kernels-community/liger_kernels\","
      },
      {
        "filename": "tests/kernels/test_kernels.py",
        "status": "added",
        "additions": 403,
        "deletions": 0,
        "changes": 403,
        "patch": "@@ -0,0 +1,403 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+# Run the test: CUDA_VISIBLE_DEVICES=0 RUN_SLOW=1 pytest -sv tests/kernels/test_kernels.py\n+\n+\n+import copy\n+import types\n+from unittest.mock import patch\n+\n+from transformers import AutoModelForCausalLM, AutoTokenizer, KernelConfig\n+from transformers.integrations.hub_kernels import (\n+    _HUB_KERNEL_MAPPING,\n+    _KERNEL_MODULE_MAPPING,\n+    is_kernel,\n+    lazy_load_kernel,\n+    load_and_register_attn_kernel,\n+)\n+from transformers.masking_utils import ALL_MASK_ATTENTION_FUNCTIONS\n+from transformers.modeling_utils import ALL_ATTENTION_FUNCTIONS\n+from transformers.testing_utils import (\n+    TestCasePlus,\n+    cleanup,\n+    require_kernels,\n+    require_torch_accelerator,\n+    slow,\n+    torch_device,\n+)\n+from transformers.utils.import_utils import is_kernels_available\n+\n+\n+if is_kernels_available():\n+    import kernels as kernels_pkg\n+    from kernels import Device, Mode, kernelize\n+\n+\n+@require_kernels\n+@slow\n+class TestHubKernels(TestCasePlus):\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.model_id = \"unsloth/Llama-3.2-1B-Instruct\"\n+        cls.tokenizer = AutoTokenizer.from_pretrained(cls.model_id)\n+        cls.model_kernelized = AutoModelForCausalLM.from_pretrained(\n+            cls.model_id, use_kernels=True, device_map=torch_device\n+        )\n+        cls.model_not_kernelized = AutoModelForCausalLM.from_pretrained(\n+            cls.model_id, use_kernels=False, device_map=torch_device\n+        )\n+        cls.input = \"Hello\"\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        for attr in [\n+            \"model_kernelized\",\n+            \"model_not_kernelized\",\n+            \"tokenizer\",\n+        ]:\n+            if hasattr(cls, attr):\n+                try:\n+                    delattr(cls, attr)\n+                except Exception:\n+                    pass\n+\n+        # Clear any temporary kernel module cache entries populated by tests\n+        try:\n+            keys_to_remove = [\n+                k for k, v in list(_KERNEL_MODULE_MAPPING.items()) if v is None or isinstance(v, types.ModuleType)\n+            ]\n+            for k in keys_to_remove:\n+                _KERNEL_MODULE_MAPPING.pop(k, None)\n+        except Exception:\n+            pass\n+\n+    def tearDown(self):\n+        # Free accelerator memory/cache and trigger GC\n+        cleanup(torch_device, gc_collect=True)\n+\n+    @require_torch_accelerator\n+    def test_forward(self):\n+        tokenized_input = self.tokenizer(self.input, return_tensors=\"pt\").input_ids.to(self.model_kernelized.device)\n+        output_ = self.model_kernelized.generate(tokenized_input, max_new_tokens=10, do_sample=False)\n+        output = self.tokenizer.decode(output_[0], skip_special_tokens=True)\n+\n+        self.EXPECTED_OUTPUT = set()\n+        self.EXPECTED_OUTPUT.add(\"Hello, I'm looking for a reliable and trustworthy online\")\n+\n+        self.assertTrue(output in self.EXPECTED_OUTPUT)\n+\n+    def test_getter_use_kernels(self):\n+        self.assertTrue(self.model_kernelized.use_kernels)\n+        self.assertFalse(self.model_not_kernelized.use_kernels)\n+\n+    def assert_kernelized_forward_is_different(self, kernelized_model, not_kernelized_model):\n+        \"\"\"\n+        Iterate over modules and check if the forward method is different between\n+        the kernelized and not kernelized models. Break on first difference, else continue.\n+        Finally, assert that at least one forward is different.\n+        \"\"\"\n+        found_difference = False\n+        for (name1, module1), (name2, module2) in zip(\n+            kernelized_model.named_modules(), not_kernelized_model.named_modules()\n+        ):\n+            # Only compare modules with the same name\n+            if name1 != name2:\n+                continue\n+            # Check if both modules have a 'forward' attribute\n+            if hasattr(module1, \"forward\") and hasattr(module2, \"forward\"):\n+                # Compare the code objects of the forward methods\n+                code1 = getattr(module1.forward, \"__code__\", None)\n+                code2 = getattr(module2.forward, \"__code__\", None)\n+                if code1 is not None and code2 is not None:\n+                    if code1 is not code2:\n+                        found_difference = True\n+                        break\n+        self.assertTrue(\n+            found_difference,\n+            \"No module's forward method was different between kernelized and not kernelized models.\",\n+        )\n+\n+    def assert_kernelized_forward_is_the_same(self, model_1, model_2):\n+        \"\"\"\n+        Iterate over modules and check if the forward method is the same between\n+        the kernelized and not kernelized models. Break on first difference, else continue.\n+        Finally, assert that at least one forward is the same.\n+        \"\"\"\n+        no_difference = True\n+        for (name1, module1), (name2, module2) in zip(model_1.named_modules(), model_2.named_modules()):\n+            # Only compare modules with the same name\n+            if name1 != name2:\n+                continue\n+            # Check if both modules have a 'forward' attribute\n+            if hasattr(module1, \"forward\") and hasattr(module2, \"forward\"):\n+                # Compare the code objects of the forward methods\n+                code1 = getattr(module1.forward, \"__code__\", None)\n+                code2 = getattr(module2.forward, \"__code__\", None)\n+                if code1 is not None and code2 is not None:\n+                    if code1 != code2:\n+                        no_difference = False\n+                        break\n+        self.assertTrue(\n+            no_difference,\n+            \"All module's forward methods were the same between the two models\",\n+        )\n+\n+    def test_kernelize(self):\n+        model = copy.deepcopy(self.model_not_kernelized)\n+        kernelize(model, mode=Mode.INFERENCE, device=Device(type=model.device.type))  # type: ignore[arg-type]\n+        self.assert_kernelized_forward_is_different(model, self.model_not_kernelized)\n+        self.assert_kernelized_forward_is_the_same(model, self.model_kernelized)\n+        del model\n+\n+    def test_setter_use_kernels(self):\n+        model = copy.deepcopy(self.model_not_kernelized)\n+        model.use_kernels = True\n+        self.assertTrue(model.use_kernels)\n+        self.assert_kernelized_forward_is_different(model, self.model_not_kernelized)\n+        self.assert_kernelized_forward_is_the_same(model, self.model_kernelized)\n+        del model\n+\n+    def test_unkernelize(self):\n+        model = copy.deepcopy(self.model_kernelized)\n+\n+        with self.assertLogs(\"transformers.modeling_utils\", level=\"WARNING\") as cm:\n+            model.use_kernels = False\n+\n+        self.assertTrue(\n+            any(\n+                \"Disabling kernels at runtime is a no-op as there is no 'unkernelize' routine; keeping current kernels active.\"\n+                in msg\n+                for msg in cm.output\n+            )\n+        )\n+\n+        self.assertFalse(model.use_kernels)\n+        del model\n+\n+    def test_kernels_mapping(self):\n+        kernel_config = KernelConfig(kernel_mapping={\"RMSNorm\": \"kernels-community/layer_norm:LlamaRMSNorm\"})\n+        model = AutoModelForCausalLM.from_pretrained(\n+            \"unsloth/Llama-3.2-1B-Instruct\", use_kernels=True, device_map=torch_device, kernel_config=kernel_config\n+        )\n+\n+        EXPECTED_OUTPUT = set()\n+        EXPECTED_OUTPUT.add(\"Hello, I'm looking for a reliable and trustworthy online\")\n+\n+        tokenized_input = self.tokenizer(self.input, return_tensors=\"pt\").input_ids.to(model.device)\n+        output = model.generate(tokenized_input, max_new_tokens=10, do_sample=False)\n+        output = self.tokenizer.decode(output[0], skip_special_tokens=True)\n+        self.assertTrue(output in EXPECTED_OUTPUT)\n+\n+        del model\n+\n+    def test_faulty_kernel_mapping_layer_name(self):\n+        kernel_config = KernelConfig(kernel_mapping={\"RMSNorm1\": \"kernels-community/layer_norm:LlamaRMSNorm\"})\n+        with self.assertRaises(ValueError):\n+            _ = AutoModelForCausalLM.from_pretrained(\n+                \"unsloth/Llama-3.2-1B-Instruct\", use_kernels=True, device_map=torch_device, kernel_config=kernel_config\n+            )\n+\n+    def test_faulty_kernel_mapping_type(self):\n+        kernel_config = KernelConfig(kernel_mapping={\"RMSNorm\": 1})\n+        with self.assertRaises(ValueError):\n+            _ = AutoModelForCausalLM.from_pretrained(\n+                \"unsloth/Llama-3.2-1B-Instruct\", use_kernels=True, device_map=torch_device, kernel_config=kernel_config\n+            )\n+\n+\n+@require_kernels\n+class TestKernelUtilities(TestCasePlus):\n+    def test_is_kernel_regex(self):\n+        valid = [\n+            \"org/model\",\n+            \"org/model@main\",\n+            \"org/model:my_func\",\n+            \"org/model@v1.2.3:my_func\",\n+            \"flash|org/model@rev:fn\",\n+        ]\n+        invalid = [\n+            \"org//model\",\n+            \"org/model:too:many\",\n+            \"org/model@rev:fn:extra\",\n+            \"/org/model\",\n+            \"org:model\",\n+        ]\n+        for s in valid:\n+            self.assertTrue(is_kernel(s.split(\"|\")[-1]))\n+        for s in invalid:\n+            self.assertFalse(is_kernel(s))\n+\n+    def test_lazy_load_kernel_success_and_cache(self):\n+        sentinel = types.SimpleNamespace(name=\"sentinel\")\n+\n+        original_get_kernel = getattr(kernels_pkg, \"get_kernel\")\n+        try:\n+\n+            def fake_get_kernel(repo_id, revision=None, version=None):\n+                self.assertIn(repo_id, {\"kernels-community/causal-conv1d\"})\n+                return sentinel\n+\n+            setattr(kernels_pkg, \"get_kernel\", fake_get_kernel)\n+            _KERNEL_MODULE_MAPPING.pop(\"causal-conv1d\", None)\n+\n+            mod1 = lazy_load_kernel(\"causal-conv1d\")\n+            self.assertIs(mod1, sentinel)\n+            mod2 = lazy_load_kernel(\"causal-conv1d\")\n+            self.assertIs(mod2, sentinel)\n+        finally:\n+            setattr(kernels_pkg, \"get_kernel\", original_get_kernel)\n+            # Ensure cache is cleared to avoid holding onto module references across tests\n+            _KERNEL_MODULE_MAPPING.pop(\"causal-conv1d\", None)\n+\n+    def test_lazy_load_kernel_unknown(self):\n+        name = \"unknown-kernel-name\"\n+        _KERNEL_MODULE_MAPPING.pop(name, None)\n+        mod = lazy_load_kernel(name)\n+        self.assertIsNone(mod)\n+        self.assertIn(name, _KERNEL_MODULE_MAPPING)\n+        # Cleanup cache entry to avoid growth across tests\n+        _KERNEL_MODULE_MAPPING.pop(name, None)\n+\n+    def test_lazy_load_kernel_version(self):\n+        HUB = _HUB_KERNEL_MAPPING\n+        name = \"causal-conv1d\"\n+        version_spec = \">=0.0.4,<0.1.0\"\n+        original_get_kernel = getattr(kernels_pkg, \"get_kernel\")\n+        original_entry = HUB.get(name, None)\n+\n+        # Use a real ModuleType so caching short-circuits on the second call\n+        sentinel_mod = types.ModuleType(\"sentinel_kernel_module\")\n+        call_count = {\"n\": 0}\n+\n+        try:\n+            # Inject dict-style mapping with repo_id and version\n+            HUB[name] = {\"repo_id\": \"kernels-community/causal-conv1d\", \"version\": version_spec}  # type: ignore[assignment]\n+            _KERNEL_MODULE_MAPPING.pop(name, None)\n+\n+            def fake_get_kernel(repo_id, revision=None, version=None, user_agent=None):\n+                call_count[\"n\"] += 1\n+                self.assertEqual(repo_id, \"kernels-community/causal-conv1d\")\n+                self.assertIsNone(revision, \"revision must not be set when version is provided\")\n+                self.assertEqual(version, version_spec)\n+                return sentinel_mod\n+\n+            # Patch kernels.get_kernel so lazy_load_kernel picks it up on import\n+            setattr(kernels_pkg, \"get_kernel\", fake_get_kernel)\n+\n+            # Act\n+            mod1 = lazy_load_kernel(name)\n+            mod2 = lazy_load_kernel(name)\n+\n+            # Assert\n+            self.assertIs(mod1, sentinel_mod)\n+            self.assertIs(mod2, sentinel_mod)\n+            self.assertEqual(call_count[\"n\"], 1, \"second call should hit the cache\")\n+        finally:\n+            # Restore patched function and mapping to avoid side effects\n+            setattr(kernels_pkg, \"get_kernel\", original_get_kernel)\n+            if original_entry is None:\n+                HUB.pop(name, None)\n+            else:\n+                HUB[name] = original_entry\n+            _KERNEL_MODULE_MAPPING.pop(name, None)\n+\n+\n+@require_kernels\n+class TestAttentionKernelRegistration(TestCasePlus):\n+    def test_load_and_register_flash_attn_like_kernel(self):\n+        kernel_obj = types.SimpleNamespace(flash_attn_varlen_func=lambda *a, **k: None)\n+\n+        with (\n+            patch(\"transformers.integrations.hub_kernels.get_kernel\", return_value=kernel_obj),\n+            patch(\"transformers.integrations.hub_kernels.lazy_import_flash_attention\", return_value=None),\n+        ):\n+            attn_impl = \"org/model\"\n+            load_and_register_attn_kernel(attn_impl)\n+            self.assertIn(attn_impl, ALL_ATTENTION_FUNCTIONS.valid_keys())\n+            # Cleanup registration to avoid leaking functions across tests\n+            try:\n+                ALL_ATTENTION_FUNCTIONS.pop(attn_impl, None)\n+            except Exception:\n+                pass\n+            try:\n+                ALL_MASK_ATTENTION_FUNCTIONS.pop(attn_impl, None)\n+            except Exception:\n+                pass\n+\n+    def test_load_and_register_named_function_kernel(self):\n+        def my_attention(*args, **kwargs):\n+            return None\n+\n+        kernel_obj = types.SimpleNamespace(my_func=my_attention)\n+        with patch(\"transformers.integrations.hub_kernels.get_kernel\", return_value=kernel_obj):\n+            attn_impl = \"org/model:my_func\"\n+            load_and_register_attn_kernel(attn_impl)\n+            self.assertIn(attn_impl, ALL_ATTENTION_FUNCTIONS.valid_keys())\n+            # Cleanup registration to avoid leaking functions across tests\n+            try:\n+                ALL_ATTENTION_FUNCTIONS.pop(attn_impl, None)\n+            except Exception:\n+                pass\n+            try:\n+                ALL_MASK_ATTENTION_FUNCTIONS.pop(attn_impl, None)\n+            except Exception:\n+                pass\n+\n+\n+@require_kernels\n+class TestUseKernelsLifecycle(TestCasePlus):\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.model_id = \"unsloth/Llama-3.2-1B-Instruct\"\n+        cls.model = AutoModelForCausalLM.from_pretrained(cls.model_id, use_kernels=False, device_map=torch_device)\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        # Delete large objects to drop references early\n+        if hasattr(cls, \"model\"):\n+            try:\n+                del cls.model\n+            except Exception:\n+                pass\n+\n+    def tearDown(self):\n+        # Free accelerator memory/cache and trigger GC\n+        cleanup(torch_device, gc_collect=True)\n+\n+    def test_setting_use_kernels_twice_does_not_rekernelize(self):\n+        call_count = {\"n\": 0}\n+\n+        def spy_kernelize(*args, **kwargs):\n+            call_count[\"n\"] += 1\n+\n+        with patch.object(kernels_pkg, \"kernelize\", side_effect=spy_kernelize):\n+            self.model.use_kernels = True\n+            self.assertTrue(self.model.use_kernels)\n+            self.assertEqual(call_count[\"n\"], 1)\n+            self.model.use_kernels = True\n+            self.assertEqual(call_count[\"n\"], 1)\n+\n+    def test_train_eval_calls_kernelize_with_correct_mode(self):\n+        last_modes = []\n+\n+        def spy_kernelize(model, device=None, mode=None):\n+            last_modes.append(mode)\n+\n+        with patch.object(kernels_pkg, \"kernelize\", side_effect=spy_kernelize):\n+            self.model.use_kernels = True\n+            self.model.train(True)\n+            self.assertTrue(any(m == Mode.TRAINING for m in last_modes))\n+            self.model.eval()\n+            self.assertTrue(any(m == Mode.INFERENCE for m in last_modes))"
      },
      {
        "filename": "utils/notification_service.py",
        "status": "modified",
        "additions": 4,
        "deletions": 0,
        "changes": 4,
        "patch": "@@ -40,6 +40,7 @@\n     \"run_examples_gpu\": \"Examples directory\",\n     \"run_torch_cuda_extensions_gpu\": \"DeepSpeed\",\n     \"run_quantization_torch_gpu\": \"Quantization\",\n+    \"run_kernels_gpu\": \"Kernels\",\n }\n \n # The values are used as the file names where to save the corresponding CI job results.\n@@ -50,6 +51,7 @@\n     \"Examples directory\": \"example\",\n     \"DeepSpeed\": \"deepspeed\",\n     \"Quantization\": \"quantization\",\n+    \"Kernels\": \"kernels\",\n }\n \n NON_MODEL_TEST_MODULES = [\n@@ -65,6 +67,7 @@\n     \"utils\",\n     \"fsdp\",\n     \"quantization\",\n+    \"kernels\",\n ]\n \n \n@@ -1301,6 +1304,7 @@ def pop_default(l: list[Any], i: int, default: Any) -> Any:\n         \"PyTorch pipelines\": \"run_pipelines_torch_gpu_test_reports\",\n         \"Examples directory\": \"run_examples_gpu_test_reports\",\n         \"DeepSpeed\": \"run_torch_cuda_extensions_gpu_test_reports\",\n+        \"Kernels\": \"run_kernels_gpu_test_reports\",\n     }\n \n     if ci_event in [\"push\", \"Nightly CI\"] or ci_event.startswith(\"Past CI\"):"
      }
    ],
    "num_files": 5,
    "scraped_at": "2025-11-16T21:16:59.715865"
  },
  {
    "pr_number": 41758,
    "title": "Fixed incorrect model_type for qwen2vl and qwen2.5vl when config is saved and loaded again",
    "body": "# What does this PR do?\r\nFixes the issue where if you save the config and load it again it would return the incorrect model_type . \r\nMinor fix in __getattribute__ method of the config class for both models .\r\n\r\n\r\nFixes # 41746\r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),\r\n      Pull Request section?\r\n- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.  - https://github.com/huggingface/transformers/issues/41746\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n@zucchini-nlp \r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n",
    "html_url": "https://github.com/huggingface/transformers/pull/41758",
    "created_at": "2025-10-21T08:27:38Z",
    "merged_at": "2025-10-21T10:54:58Z",
    "merge_commit_sha": "ede7976cd2462ce868a0058c339c6b21baf7fc04",
    "base_ref": "main",
    "head_sha": "4ac562c79c686cac933ba899b4d04071fdd8ebc8",
    "user": "i3hz",
    "files": [
      {
        "filename": "src/transformers/models/qwen2_5_vl/configuration_qwen2_5_vl.py",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -313,6 +313,8 @@ def __setattr__(self, key, value):\n \n     def __getattribute__(self, key):\n         if \"text_config\" in super().__getattribute__(\"__dict__\") and key not in [\n+            \"_name_or_path\",\n+            \"model_type\",\n             \"dtype\",\n             \"_attn_implementation_internal\",\n         ]:"
      },
      {
        "filename": "src/transformers/models/qwen2_vl/configuration_qwen2_vl.py",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -301,6 +301,8 @@ def __setattr__(self, key, value):\n \n     def __getattribute__(self, key):\n         if \"text_config\" in super().__getattribute__(\"__dict__\") and key not in [\n+            \"_name_or_path\",\n+            \"model_type\",\n             \"dtype\",\n             \"_attn_implementation_internal\",\n         ]:"
      },
      {
        "filename": "tests/models/qwen2_5_vl/test_modeling_qwen2_5_vl.py",
        "status": "modified",
        "additions": 11,
        "deletions": 0,
        "changes": 11,
        "patch": "@@ -235,6 +235,17 @@ def test_text_config(self):\n         self.assertEqual(base_config.patch_size, 8)\n         self.assertNotEqual(base_config.vision_config.patch_size, 8)\n \n+        # Test for making sure config save and load preserves correct model type\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        self.assertEqual(config.model_type, \"qwen2_5_vl\")\n+\n+        with tempfile.TemporaryDirectory() as tmp_dir:\n+            config.save_pretrained(tmp_dir)\n+\n+            loaded_config = Qwen2_5_VLConfig.from_pretrained(tmp_dir)\n+            self.assertEqual(loaded_config.model_type, \"qwen2_5_vl\")\n+\n     def test_mismatching_num_image_tokens(self):\n         \"\"\"\n         Tests that VLMs through an error with explicit message saying what is wrong"
      },
      {
        "filename": "tests/models/qwen2_vl/test_modeling_qwen2_vl.py",
        "status": "modified",
        "additions": 11,
        "deletions": 0,
        "changes": 11,
        "patch": "@@ -215,6 +215,17 @@ def test_text_config(self):\n         self.assertEqual(base_config.patch_size, 8)\n         self.assertNotEqual(base_config.vision_config.patch_size, 8)\n \n+        # Test for making sure config save and load preserves correct model type\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        self.assertEqual(config.model_type, \"qwen2_vl\")\n+\n+        with tempfile.TemporaryDirectory() as tmp_dir:\n+            config.save_pretrained(tmp_dir)\n+\n+            loaded_config = Qwen2VLConfig.from_pretrained(tmp_dir)\n+            self.assertEqual(loaded_config.model_type, \"qwen2_vl\")\n+\n     def test_mismatching_num_image_tokens(self):\n         \"\"\"\n         Tests that VLMs through an error with explicit message saying what is wrong"
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T21:17:00.797535"
  },
  {
    "pr_number": 41750,
    "title": ":rotating_light: [`Clip`] Fix masking and enable flash attention on all model types",
    "body": "Clip used old mask APIs leading to a confused usage:\r\n- A causal mask (normal triu mask)\r\n- A padding mask (encoder mask == only accounting for padding)\r\n- Add both of above == final mask --> causal mask with padding\r\n\r\n^ works only for interfaces with support for 4D masks which disabled FA usage in general.\r\n\r\nThis PR now correctly changes this to the new API which handles padding automatically. We have to additionally pass the `is_causal` kwarg to dynamically switch between modality types (text == causal, image == full). This is only enabled through recent PRs (fa #39707, sdpa #41692).\r\n\r\nCloses #41673\r\nFixes #41668",
    "html_url": "https://github.com/huggingface/transformers/pull/41750",
    "created_at": "2025-10-20T14:53:33Z",
    "merged_at": "2025-10-24T18:44:10Z",
    "merge_commit_sha": "7a833d1ccd41673030c85107f65f454c0c3222f5",
    "base_ref": "main",
    "head_sha": "eccf8c7793e5302c8d23ccb133d5d933d1e35a16",
    "user": "vasqu",
    "files": [
      {
        "filename": "src/transformers/models/clip/modeling_clip.py",
        "status": "modified",
        "additions": 14,
        "deletions": 42,
        "changes": 56,
        "patch": "@@ -22,7 +22,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...modeling_attn_mask_utils import _create_4d_causal_attention_mask, _prepare_4d_attention_mask\n+from ...masking_utils import create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling, ImageClassifierOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n@@ -310,7 +310,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n-        causal_attention_mask: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n@@ -324,15 +323,6 @@ def forward(\n         queries = queries.view(batch_size, seq_length, -1, self.head_dim).transpose(1, 2)\n         keys = keys.view(batch_size, seq_length, -1, self.head_dim).transpose(1, 2)\n         values = values.view(batch_size, seq_length, -1, self.head_dim).transpose(1, 2)\n-        # CLIP text model uses both `causal_attention_mask` and `attention_mask`\n-        # in case FA2 kernel is called, `is_causal` should be inferred from `causal_attention_mask`\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            self.is_causal = causal_attention_mask is not None\n-        else:\n-            if attention_mask is not None and causal_attention_mask is not None:\n-                attention_mask = attention_mask + causal_attention_mask\n-            elif causal_attention_mask is not None:\n-                attention_mask = causal_attention_mask\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -344,13 +334,12 @@ def forward(\n             keys,\n             values,\n             attention_mask,\n-            is_causal=self.is_causal,\n             scaling=self.scale,\n             dropout=0.0 if not self.training else self.dropout,\n             **kwargs,\n         )\n \n-        attn_output = attn_output.reshape(batch_size, seq_length, embed_dim).contiguous()\n+        attn_output = attn_output.reshape(batch_size, seq_length, -1).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n         return attn_output, attn_weights\n@@ -384,16 +373,14 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: torch.Tensor,\n-        causal_attention_mask: torch.Tensor,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> torch.FloatTensor:\n         residual = hidden_states\n \n         hidden_states = self.layer_norm1(hidden_states)\n-        hidden_states, attn_weights = self.self_attn(\n+        hidden_states, _ = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n-            causal_attention_mask=causal_attention_mask,\n             **kwargs,\n         )\n         hidden_states = residual + hidden_states\n@@ -497,7 +484,6 @@ def forward(\n         self,\n         inputs_embeds,\n         attention_mask: Optional[torch.Tensor] = None,\n-        causal_attention_mask: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutput:\n         r\"\"\"\n@@ -512,21 +498,13 @@ def forward(\n                 - 1 for tokens that are **not masked**,\n                 - 0 for tokens that are **masked**.\n \n-                [What are attention masks?](../glossary#attention-mask)\n-            causal_attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Causal mask for the text model. Mask values selected in `[0, 1]`:\n-\n-                - 1 for tokens that are **not masked**,\n-                - 0 for tokens that are **masked**.\n-\n                 [What are attention masks?](../glossary#attention-mask)\n         \"\"\"\n         hidden_states = inputs_embeds\n         for encoder_layer in self.layers:\n             hidden_states = encoder_layer(\n                 hidden_states,\n                 attention_mask,\n-                causal_attention_mask,\n                 **kwargs,\n             )\n \n@@ -563,17 +541,19 @@ def forward(\n \n         hidden_states = self.embeddings(input_ids=input_ids, position_ids=position_ids)\n \n-        causal_attention_mask = _create_4d_causal_attention_mask(\n-            input_shape, hidden_states.dtype, device=hidden_states.device\n+        attention_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=hidden_states,\n+            attention_mask=attention_mask,\n+            cache_position=torch.arange(hidden_states.shape[1], device=hidden_states.device),\n+            past_key_values=None,\n         )\n \n-        if attention_mask is not None and self.config._attn_implementation != \"flash_attention_2\":\n-            attention_mask = _prepare_4d_attention_mask(attention_mask, hidden_states.dtype)\n-\n+        kwargs.pop(\"is_causal\", None)\n         encoder_outputs: BaseModelOutput = self.encoder(\n             inputs_embeds=hidden_states,\n             attention_mask=attention_mask,\n-            causal_attention_mask=causal_attention_mask,\n+            is_causal=True,\n             **kwargs,\n         )\n \n@@ -618,7 +598,6 @@ class CLIPTextModel(CLIPPreTrainedModel):\n     input_modalities = \"text\"\n \n     _no_split_modules = [\"CLIPTextEmbeddings\", \"CLIPEncoderLayer\"]\n-    _supports_flash_attn = False  # mask creation only accounts for sdpa/eager\n \n     def __init__(self, config: CLIPTextConfig):\n         super().__init__(config)\n@@ -632,8 +611,7 @@ def get_input_embeddings(self) -> nn.Module:\n     def set_input_embeddings(self, value):\n         self.text_model.embeddings.token_embedding = value\n \n-    @check_model_inputs()\n-    @can_return_tuple\n+    @check_model_inputs(tie_last_hidden_states=False)\n     @auto_docstring\n     def forward(\n         self,\n@@ -726,7 +704,6 @@ def get_input_embeddings(self) -> nn.Module:\n         return self.vision_model.embeddings.patch_embedding\n \n     @check_model_inputs(tie_last_hidden_states=False)\n-    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -766,7 +743,6 @@ def forward(\n class CLIPModel(CLIPPreTrainedModel):\n     config: CLIPConfig\n     _no_split_modules = [\"CLIPTextEmbeddings\", \"CLIPEncoderLayer\", \"CLIPVisionEmbeddings\"]\n-    _supports_flash_attn = False  # mask creation only accounts for sdpa/eager\n \n     def __init__(self, config: CLIPConfig):\n         super().__init__(config)\n@@ -966,7 +942,6 @@ class CLIPTextModelWithProjection(CLIPPreTrainedModel):\n     config: CLIPTextConfig\n     input_modalities = \"text\"\n \n-    _supports_flash_attn = False\n     _no_split_modules = [\"CLIPTextEmbeddings\", \"CLIPEncoderLayer\"]\n \n     def __init__(self, config: CLIPTextConfig):\n@@ -986,8 +961,7 @@ def get_input_embeddings(self) -> nn.Module:\n     def set_input_embeddings(self, value):\n         self.text_model.embeddings.token_embedding = value\n \n-    @check_model_inputs()\n-    @can_return_tuple\n+    @check_model_inputs(tie_last_hidden_states=False)\n     @auto_docstring\n     def forward(\n         self,\n@@ -1049,7 +1023,6 @@ def get_input_embeddings(self) -> nn.Module:\n         return self.vision_model.embeddings.patch_embedding\n \n     @check_model_inputs(tie_last_hidden_states=False)\n-    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -1117,8 +1090,7 @@ def __init__(self, config: CLIPConfig) -> None:\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @check_model_inputs()\n-    @can_return_tuple\n+    @check_model_inputs(tie_last_hidden_states=False)\n     @auto_docstring\n     def forward(\n         self,"
      },
      {
        "filename": "src/transformers/models/metaclip_2/modeling_metaclip_2.py",
        "status": "modified",
        "additions": 15,
        "deletions": 52,
        "changes": 67,
        "patch": "@@ -12,7 +12,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...modeling_attn_mask_utils import _create_4d_causal_attention_mask, _prepare_4d_attention_mask\n+from ...masking_utils import create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling, ImageClassifierOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n@@ -200,7 +200,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n-        causal_attention_mask: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n@@ -214,15 +213,6 @@ def forward(\n         queries = queries.view(batch_size, seq_length, -1, self.head_dim).transpose(1, 2)\n         keys = keys.view(batch_size, seq_length, -1, self.head_dim).transpose(1, 2)\n         values = values.view(batch_size, seq_length, -1, self.head_dim).transpose(1, 2)\n-        # METACLIP_2 text model uses both `causal_attention_mask` and `attention_mask`\n-        # in case FA2 kernel is called, `is_causal` should be inferred from `causal_attention_mask`\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            self.is_causal = causal_attention_mask is not None\n-        else:\n-            if attention_mask is not None and causal_attention_mask is not None:\n-                attention_mask = attention_mask + causal_attention_mask\n-            elif causal_attention_mask is not None:\n-                attention_mask = causal_attention_mask\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -234,13 +224,12 @@ def forward(\n             keys,\n             values,\n             attention_mask,\n-            is_causal=self.is_causal,\n             scaling=self.scale,\n             dropout=0.0 if not self.training else self.dropout,\n             **kwargs,\n         )\n \n-        attn_output = attn_output.reshape(batch_size, seq_length, embed_dim).contiguous()\n+        attn_output = attn_output.reshape(batch_size, seq_length, -1).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n         return attn_output, attn_weights\n@@ -274,16 +263,14 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: torch.Tensor,\n-        causal_attention_mask: torch.Tensor,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> torch.FloatTensor:\n         residual = hidden_states\n \n         hidden_states = self.layer_norm1(hidden_states)\n-        hidden_states, attn_weights = self.self_attn(\n+        hidden_states, _ = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n-            causal_attention_mask=causal_attention_mask,\n             **kwargs,\n         )\n         hidden_states = residual + hidden_states\n@@ -387,7 +374,6 @@ def forward(\n         self,\n         inputs_embeds,\n         attention_mask: Optional[torch.Tensor] = None,\n-        causal_attention_mask: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutput:\n         r\"\"\"\n@@ -402,21 +388,13 @@ def forward(\n                 - 1 for tokens that are **not masked**,\n                 - 0 for tokens that are **masked**.\n \n-                [What are attention masks?](../glossary#attention-mask)\n-            causal_attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Causal mask for the text model. Mask values selected in `[0, 1]`:\n-\n-                - 1 for tokens that are **not masked**,\n-                - 0 for tokens that are **masked**.\n-\n                 [What are attention masks?](../glossary#attention-mask)\n         \"\"\"\n         hidden_states = inputs_embeds\n         for encoder_layer in self.layers:\n             hidden_states = encoder_layer(\n                 hidden_states,\n                 attention_mask,\n-                causal_attention_mask,\n                 **kwargs,\n             )\n \n@@ -437,36 +415,32 @@ def __init__(self, config: MetaClip2TextConfig):\n         # For `pooled_output` computation\n         self.eos_token_id = config.eos_token_id\n \n-    @check_model_inputs(tie_last_hidden_states=False)\n     @auto_docstring\n     def forward(\n         self,\n         input_ids,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        use_cache: Optional[bool] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutputWithPooling:\n         input_shape = input_ids.size()\n         input_ids = input_ids.view(-1, input_shape[-1])\n \n         hidden_states = self.embeddings(input_ids=input_ids, position_ids=position_ids)\n \n-        # CLIP's text model uses causal mask, prepare it here.\n-        # https://github.com/openai/CLIP/blob/cfcffb90e69f37bf2ff1e988237a0fbe41f33c04/clip/model.py#L324\n-        causal_attention_mask = _create_4d_causal_attention_mask(\n-            input_shape, hidden_states.dtype, device=hidden_states.device\n+        attention_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=hidden_states,\n+            attention_mask=attention_mask,\n+            cache_position=torch.arange(hidden_states.shape[1], device=hidden_states.device),\n+            past_key_values=None,\n         )\n \n-        # expand attention_mask\n-        if attention_mask is not None and self.config._attn_implementation != \"flash_attention_2\":\n-            # [batch_size, seq_len] -> [batch_size, 1, tgt_seq_len, src_seq_len]\n-            attention_mask = _prepare_4d_attention_mask(attention_mask, hidden_states.dtype)\n-\n+        kwargs.pop(\"is_causal\", None)\n         encoder_outputs: BaseModelOutput = self.encoder(\n             inputs_embeds=hidden_states,\n             attention_mask=attention_mask,\n-            causal_attention_mask=causal_attention_mask,\n+            is_causal=True,\n             **kwargs,\n         )\n \n@@ -527,7 +501,6 @@ class MetaClip2TextModel(MetaClip2PreTrainedModel):\n     input_modalities = \"text\"\n \n     _no_split_modules = [\"MetaClip2TextEmbeddings\", \"MetaClip2EncoderLayer\"]\n-    _supports_flash_attn = False  # mask creation only accounts for sdpa/eager\n \n     def __init__(self, config: MetaClip2TextConfig):\n         super().__init__(config)\n@@ -541,16 +514,13 @@ def get_input_embeddings(self) -> nn.Module:\n     def set_input_embeddings(self, value):\n         self.text_model.embeddings.token_embedding = value\n \n-    @check_model_inputs()\n-    @can_return_tuple\n+    @check_model_inputs(tie_last_hidden_states=False)\n     @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutputWithPooling:\n         r\"\"\"\n@@ -630,7 +600,6 @@ class MetaClip2TextModelWithProjection(MetaClip2PreTrainedModel):\n     config: MetaClip2TextConfig\n     input_modalities = \"text\"\n \n-    _supports_flash_attn = False\n     _no_split_modules = [\"MetaClip2TextEmbeddings\", \"MetaClip2EncoderLayer\"]\n \n     def __init__(self, config: MetaClip2TextConfig):\n@@ -650,16 +619,13 @@ def get_input_embeddings(self) -> nn.Module:\n     def set_input_embeddings(self, value):\n         self.text_model.embeddings.token_embedding = value\n \n-    @check_model_inputs()\n-    @can_return_tuple\n+    @check_model_inputs(tie_last_hidden_states=False)\n     @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> MetaClip2TextModelOutput:\n         r\"\"\"\n@@ -792,7 +758,6 @@ class MetaClip2Model(MetaClip2PreTrainedModel):\n \n     config: MetaClip2Config\n     _no_split_modules = [\"MetaClip2TextEmbeddings\", \"MetaClip2EncoderLayer\", \"MetaClip2VisionEmbeddings\"]\n-    _supports_flash_attn = False  # mask creation only accounts for sdpa/eager\n \n     def __init__(self, config: MetaClip2Config):\n         super().__init__(config)\n@@ -1078,7 +1043,7 @@ def get_input_embeddings(self) -> nn.Module:\n         return self.vision_model.embeddings.patch_embedding\n \n     @check_model_inputs(tie_last_hidden_states=False)\n-    @can_return_tuple\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n@@ -1187,7 +1152,6 @@ def get_input_embeddings(self) -> nn.Module:\n         return self.vision_model.embeddings.patch_embedding\n \n     @check_model_inputs(tie_last_hidden_states=False)\n-    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -1254,8 +1218,7 @@ def __init__(self, config: MetaClip2Config) -> None:\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @check_model_inputs()\n-    @can_return_tuple\n+    @check_model_inputs(tie_last_hidden_states=False)\n     @auto_docstring\n     def forward(\n         self,"
      },
      {
        "filename": "src/transformers/models/metaclip_2/modular_metaclip_2.py",
        "status": "modified",
        "additions": 19,
        "deletions": 59,
        "changes": 78,
        "patch": "@@ -3,19 +3,18 @@\n import torch\n from torch import nn\n \n-from ...modeling_attn_mask_utils import _create_4d_causal_attention_mask, _prepare_4d_attention_mask\n+from ...masking_utils import create_causal_mask\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n-from ...modeling_utils import PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n from ...utils.generic import check_model_inputs\n from ..clip.configuration_clip import CLIPConfig, CLIPTextConfig, CLIPVisionConfig\n from ..clip.modeling_clip import (\n     CLIPMLP,\n     CLIPAttention,\n-    CLIPEncoderLayer,\n     CLIPForImageClassification,\n     CLIPModel,\n+    CLIPPreTrainedModel,\n     CLIPTextEmbeddings,\n     CLIPTextModel,\n     CLIPTextModelWithProjection,\n@@ -214,24 +213,9 @@ class MetaClip2MLP(CLIPMLP):\n     pass\n \n \n-class MetaClip2EncoderLayer(CLIPEncoderLayer):\n-    pass\n-\n-\n @auto_docstring\n-class MetaClip2PreTrainedModel(PreTrainedModel):\n-    config: MetaClip2Config\n+class MetaClip2PreTrainedModel(CLIPPreTrainedModel):\n     base_model_prefix = \"metaclip_2\"\n-    input_modalities = [\"image\", \"text\"]\n-    supports_gradient_checkpointing = True\n-    _supports_sdpa = True\n-    _supports_flash_attn = True\n-    _supports_flex_attn = True\n-    _supports_attention_backend = True\n-    _can_record_outputs = {\n-        \"hidden_states\": MetaClip2EncoderLayer,\n-        \"attentions\": MetaClip2Attention,\n-    }\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n@@ -291,36 +275,32 @@ def _init_weights(self, module):\n \n \n class MetaClip2TextTransformer(CLIPTextTransformer):\n-    @check_model_inputs(tie_last_hidden_states=False)\n     @auto_docstring\n     def forward(\n         self,\n         input_ids,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        use_cache: Optional[bool] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutputWithPooling:\n         input_shape = input_ids.size()\n         input_ids = input_ids.view(-1, input_shape[-1])\n \n         hidden_states = self.embeddings(input_ids=input_ids, position_ids=position_ids)\n \n-        # CLIP's text model uses causal mask, prepare it here.\n-        # https://github.com/openai/CLIP/blob/cfcffb90e69f37bf2ff1e988237a0fbe41f33c04/clip/model.py#L324\n-        causal_attention_mask = _create_4d_causal_attention_mask(\n-            input_shape, hidden_states.dtype, device=hidden_states.device\n+        attention_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=hidden_states,\n+            attention_mask=attention_mask,\n+            cache_position=torch.arange(hidden_states.shape[1], device=hidden_states.device),\n+            past_key_values=None,\n         )\n \n-        # expand attention_mask\n-        if attention_mask is not None and self.config._attn_implementation != \"flash_attention_2\":\n-            # [batch_size, seq_len] -> [batch_size, 1, tgt_seq_len, src_seq_len]\n-            attention_mask = _prepare_4d_attention_mask(attention_mask, hidden_states.dtype)\n-\n+        kwargs.pop(\"is_causal\", None)\n         encoder_outputs: BaseModelOutput = self.encoder(\n             inputs_embeds=hidden_states,\n             attention_mask=attention_mask,\n-            causal_attention_mask=causal_attention_mask,\n+            is_causal=True,\n             **kwargs,\n         )\n \n@@ -372,22 +352,13 @@ class MetaClip2TextModel(CLIPTextModel):\n     >>> pooled_output = outputs.pooler_output  # pooled (EOS token) states\n     ```\"\"\"\n \n-    def __init__(self, config: MetaClip2TextConfig):\n-        super().__init__(config)\n-        self.text_model = MetaClip2TextTransformer(config)\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    @check_model_inputs()\n-    @can_return_tuple\n+    @check_model_inputs(tie_last_hidden_states=False)\n     @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ):\n         r\"\"\"\n@@ -409,8 +380,6 @@ def forward(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n             **kwargs,\n         )\n \n@@ -446,24 +415,13 @@ class MetaClip2TextModelWithProjection(CLIPTextModelWithProjection):\n     >>> text_embeds = outputs.text_embeds\n     ```\"\"\"\n \n-    def __init__(self, config: MetaClip2TextConfig):\n-        super().__init__(config)\n-\n-        text_model = MetaClip2TextModel._from_config(config)\n-        self.text_model = text_model.text_model\n-\n-        self.text_projection = nn.Linear(config.hidden_size, config.projection_dim, bias=False)\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n+    @check_model_inputs(tie_last_hidden_states=False)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ):\n         r\"\"\"\n@@ -484,8 +442,6 @@ def forward(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n             **kwargs,\n         )\n \n@@ -550,6 +506,8 @@ def __init__(self, config: MetaClip2Config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @can_return_tuple\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -694,7 +652,7 @@ class MetaClip2VisionModel(CLIPVisionModel):\n     ```\"\"\"\n \n     @check_model_inputs(tie_last_hidden_states=False)\n-    @can_return_tuple\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n@@ -764,6 +722,8 @@ class MetaClip2VisionModelWithProjection(CLIPVisionModelWithProjection):\n     >>> image_embeds = outputs.image_embeds\n     ```\"\"\"\n \n+    @check_model_inputs(tie_last_hidden_states=False)\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,"
      },
      {
        "filename": "src/transformers/models/mlcd/modeling_mlcd.py",
        "status": "modified",
        "additions": 65,
        "deletions": 124,
        "changes": 189,
        "patch": "@@ -25,12 +25,12 @@\n import torch.nn as nn\n \n from ...activations import ACT2FN\n-from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, torch_int\n+from ...utils.generic import check_model_inputs\n from .configuration_mlcd import MLCDVisionConfig\n \n \n@@ -259,7 +259,7 @@ def forward(\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n-        **kwargs: Unpack[FlashAttentionKwargs],\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n         batch_size, seq_length = hidden_states.shape[:-1]\n@@ -316,7 +316,7 @@ def forward(\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = False,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.FloatTensor]:\n         \"\"\"\n         Args:\n@@ -328,18 +328,15 @@ def forward(\n                 Represents absolute positional embeddings for the query and key in the attention mechanism.\n             attention_mask (`torch.FloatTensor`):\n                 Attention mask of shape `(batch, 1, q_len, k_v_seq_len)` where padding elements are indicated by very large negative values.\n-            output_attentions (`bool`, *optional*, defaults to `False`):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n         \"\"\"\n         residual = hidden_states\n \n         hidden_states = self.layer_norm1(hidden_states)\n-        hidden_states, attn_weights = self.self_attn(\n+        hidden_states, _ = self.self_attn(\n             hidden_states=hidden_states,\n             position_embeddings=position_embeddings,\n             attention_mask=attention_mask,\n-            output_attentions=output_attentions,\n+            **kwargs,\n         )\n         hidden_states = residual + hidden_states\n \n@@ -348,12 +345,7 @@ def forward(\n         hidden_states = self.mlp(hidden_states)\n         hidden_states = residual + hidden_states\n \n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (attn_weights,)\n-\n-        return outputs\n+        return hidden_states\n \n \n class MLCDEncoder(nn.Module):\n@@ -377,9 +369,7 @@ def forward(\n         inputs_embeds: torch.FloatTensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, BaseModelOutput]:\n         r\"\"\"\n         Args:\n@@ -395,53 +385,68 @@ def forward(\n                 - 1 for tokens that are **not masked**,\n                 - 0 for tokens that are **masked**.\n                 [What are attention masks?](../glossary#attention-mask)\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-            output_hidden_states (`bool`, *optional*):\n-                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n-                for more detail.\n-            return_dict (`bool`, *optional*):\n-                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n         \"\"\"\n-\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-\n-        encoder_states = () if output_hidden_states else None\n-        all_attentions = () if output_attentions else None\n-\n         hidden_states = inputs_embeds\n-        for idx, encoder_layer in enumerate(self.layers):\n-            if output_hidden_states:\n-                encoder_states = encoder_states + (hidden_states,)\n-            layer_outputs = encoder_layer(\n-                hidden_states=hidden_states,\n-                position_embeddings=position_embeddings,\n-                attention_mask=attention_mask,\n-                output_attentions=output_attentions,\n+        for encoder_layer in self.layers:\n+            hidden_states = encoder_layer(\n+                hidden_states,\n+                position_embeddings,\n+                attention_mask,\n+                **kwargs,\n             )\n \n-            hidden_states = layer_outputs[0]\n-\n-            if output_attentions:\n-                all_attentions = all_attentions + (layer_outputs[1],)\n-\n-        if output_hidden_states:\n-            encoder_states = encoder_states + (hidden_states,)\n-\n-        if not return_dict:\n-            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n         return BaseModelOutput(\n             last_hidden_state=hidden_states,\n-            hidden_states=encoder_states,\n-            attentions=all_attentions,\n         )\n \n \n+@auto_docstring\n+class MLCDPreTrainedModel(PreTrainedModel):\n+    config: MLCDVisionConfig\n+    base_model_prefix = \"mlcd\"\n+    supports_gradient_checkpointing = True\n+    accepts_loss_kwargs = False\n+    _supports_flash_attn = True\n+    _supports_sdpa = True\n+    _supports_flex_attn = True\n+    _supports_attention_backend = True\n+    _can_record_outputs = {\n+        \"hidden_states\": MLCDEncoderLayer,\n+        \"attentions\": MLCDAttention,\n+    }\n+\n+    def _init_weights(self, module):\n+        \"\"\"Initialize the weights\"\"\"\n+        factor = self.config.initializer_factor\n+        if isinstance(module, MLCDVisionEmbeddings):\n+            factor = self.config.initializer_factor\n+            nn.init.normal_(module.class_embedding, mean=0.0, std=module.embed_dim**-0.5 * factor)\n+            nn.init.normal_(module.patch_embedding.weight, std=module.config.initializer_range * factor)\n+        elif isinstance(module, MLCDAttention):\n+            factor = self.config.initializer_factor\n+            in_proj_std = (module.embed_dim**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n+            out_proj_std = (module.embed_dim**-0.5) * factor\n+            nn.init.normal_(module.q_proj.weight, std=in_proj_std)\n+            nn.init.normal_(module.k_proj.weight, std=in_proj_std)\n+            nn.init.normal_(module.v_proj.weight, std=in_proj_std)\n+            nn.init.normal_(module.out_proj.weight, std=out_proj_std)\n+        elif isinstance(module, MLCDMLP):\n+            factor = self.config.initializer_factor\n+            in_proj_std = (module.config.hidden_size**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n+            fc_std = (2 * module.config.hidden_size) ** -0.5 * factor\n+            nn.init.normal_(module.fc1.weight, std=fc_std)\n+            nn.init.normal_(module.fc2.weight, std=in_proj_std)\n+        elif isinstance(module, MLCDVisionTransformer):\n+            factor = self.config.initializer_factor\n+            pos_emb_std = (module.config.hidden_size // module.config.num_attention_heads // 2) ** -0.5 * factor\n+            nn.init.normal_(module.class_pos_emb, mean=0.0, std=pos_emb_std)\n+        elif isinstance(module, nn.LayerNorm):\n+            module.bias.data.zero_()\n+            module.weight.data.fill_(1.0)\n+        elif isinstance(module, nn.Linear) and module.bias is not None:\n+            module.bias.data.zero_()\n+\n+\n class MLCDVisionTransformer(nn.Module):\n     def __init__(self, config: MLCDVisionConfig):\n         super().__init__()\n@@ -459,16 +464,8 @@ def __init__(self, config: MLCDVisionConfig):\n     def forward(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, BaseModelOutputWithPooling]:\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-\n         if pixel_values is None:\n             raise ValueError(\"You have to specify pixel_values\")\n \n@@ -486,66 +483,19 @@ def forward(\n         encoder_outputs = self.encoder(\n             inputs_embeds=hidden_states,\n             position_embeddings=position_embeddings,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            **kwargs,\n         )\n \n         last_hidden_state = encoder_outputs[0]\n         pooled_output = last_hidden_state[:, 0, :]\n         pooled_output = self.post_layernorm(pooled_output)\n \n-        if not return_dict:\n-            return (last_hidden_state, pooled_output) + encoder_outputs[1:]\n-\n         return BaseModelOutputWithPooling(\n             last_hidden_state=last_hidden_state,\n             pooler_output=pooled_output,\n-            hidden_states=encoder_outputs.hidden_states,\n-            attentions=encoder_outputs.attentions,\n         )\n \n \n-@auto_docstring\n-class MLCDPreTrainedModel(PreTrainedModel):\n-    config: MLCDVisionConfig\n-    base_model_prefix = \"mlcd\"\n-    supports_gradient_checkpointing = True\n-    _supports_flash_attn = True\n-    _supports_sdpa = True\n-\n-    def _init_weights(self, module):\n-        \"\"\"Initialize the weights\"\"\"\n-        factor = self.config.initializer_factor\n-        if isinstance(module, MLCDVisionEmbeddings):\n-            factor = self.config.initializer_factor\n-            nn.init.normal_(module.class_embedding, mean=0.0, std=module.embed_dim**-0.5 * factor)\n-            nn.init.normal_(module.patch_embedding.weight, std=module.config.initializer_range * factor)\n-        elif isinstance(module, MLCDAttention):\n-            factor = self.config.initializer_factor\n-            in_proj_std = (module.embed_dim**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n-            out_proj_std = (module.embed_dim**-0.5) * factor\n-            nn.init.normal_(module.q_proj.weight, std=in_proj_std)\n-            nn.init.normal_(module.k_proj.weight, std=in_proj_std)\n-            nn.init.normal_(module.v_proj.weight, std=in_proj_std)\n-            nn.init.normal_(module.out_proj.weight, std=out_proj_std)\n-        elif isinstance(module, MLCDMLP):\n-            factor = self.config.initializer_factor\n-            in_proj_std = (module.config.hidden_size**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n-            fc_std = (2 * module.config.hidden_size) ** -0.5 * factor\n-            nn.init.normal_(module.fc1.weight, std=fc_std)\n-            nn.init.normal_(module.fc2.weight, std=in_proj_std)\n-        elif isinstance(module, MLCDVisionTransformer):\n-            factor = self.config.initializer_factor\n-            pos_emb_std = (module.config.hidden_size // module.config.num_attention_heads // 2) ** -0.5 * factor\n-            nn.init.normal_(module.class_pos_emb, mean=0.0, std=pos_emb_std)\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n-        elif isinstance(module, nn.Linear) and module.bias is not None:\n-            module.bias.data.zero_()\n-\n-\n @auto_docstring(\n     custom_intro=\"\"\"\n     The vision model from M_L_C_D without any head or projection on top.\n@@ -566,13 +516,12 @@ def __init__(self, config: MLCDVisionConfig):\n     def get_input_embeddings(self) -> nn.Module:\n         return self.vision_model.embeddings.patch_embedding\n \n+    @check_model_inputs(tie_last_hidden_states=False)\n     @auto_docstring\n     def forward(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, BaseModelOutputWithPooling]:\n         r\"\"\"\n         Example:\n@@ -596,17 +545,9 @@ def forward(\n         >>> print(f\"Number of attention layers: {len(outputs.attentions)}\")\n         >>> print(f\"Attention shape: {outputs.attentions[0].shape}\")\n         ```\"\"\"\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-\n         return self.vision_model(\n             pixel_values=pixel_values,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            **kwargs,\n         )\n \n "
      },
      {
        "filename": "src/transformers/models/mlcd/modular_mlcd.py",
        "status": "modified",
        "additions": 66,
        "deletions": 125,
        "changes": 191,
        "patch": "@@ -19,11 +19,11 @@\n import torch.nn as nn\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import auto_docstring, logging\n+from ...utils import TransformersKwargs, auto_docstring, logging\n+from ...utils.generic import check_model_inputs\n from ..clip.modeling_clip import (\n     CLIPMLP,\n     CLIPAttention,\n@@ -206,7 +206,7 @@ def forward(\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n-        **kwargs: Unpack[FlashAttentionKwargs],\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         batch_size, seq_length = hidden_states.shape[:-1]\n \n@@ -258,7 +258,7 @@ def forward(\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = False,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.FloatTensor]:\n         \"\"\"\n         Args:\n@@ -270,18 +270,15 @@ def forward(\n                 Represents absolute positional embeddings for the query and key in the attention mechanism.\n             attention_mask (`torch.FloatTensor`):\n                 Attention mask of shape `(batch, 1, q_len, k_v_seq_len)` where padding elements are indicated by very large negative values.\n-            output_attentions (`bool`, *optional*, defaults to `False`):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n         \"\"\"\n         residual = hidden_states\n \n         hidden_states = self.layer_norm1(hidden_states)\n-        hidden_states, attn_weights = self.self_attn(\n+        hidden_states, _ = self.self_attn(\n             hidden_states=hidden_states,\n             position_embeddings=position_embeddings,\n             attention_mask=attention_mask,\n-            output_attentions=output_attentions,\n+            **kwargs,\n         )\n         hidden_states = residual + hidden_states\n \n@@ -290,12 +287,7 @@ def forward(\n         hidden_states = self.mlp(hidden_states)\n         hidden_states = residual + hidden_states\n \n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (attn_weights,)\n-\n-        return outputs\n+        return hidden_states\n \n \n class MLCDEncoder(CLIPEncoder):\n@@ -316,9 +308,7 @@ def forward(\n         inputs_embeds: torch.FloatTensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, BaseModelOutput]:\n         r\"\"\"\n         Args:\n@@ -334,107 +324,18 @@ def forward(\n                 - 1 for tokens that are **not masked**,\n                 - 0 for tokens that are **masked**.\n                 [What are attention masks?](../glossary#attention-mask)\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-            output_hidden_states (`bool`, *optional*):\n-                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n-                for more detail.\n-            return_dict (`bool`, *optional*):\n-                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n         \"\"\"\n-\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-\n-        encoder_states = () if output_hidden_states else None\n-        all_attentions = () if output_attentions else None\n-\n         hidden_states = inputs_embeds\n-        for idx, encoder_layer in enumerate(self.layers):\n-            if output_hidden_states:\n-                encoder_states = encoder_states + (hidden_states,)\n-            layer_outputs = encoder_layer(\n-                hidden_states=hidden_states,\n-                position_embeddings=position_embeddings,\n-                attention_mask=attention_mask,\n-                output_attentions=output_attentions,\n+        for encoder_layer in self.layers:\n+            hidden_states = encoder_layer(\n+                hidden_states,\n+                position_embeddings,\n+                attention_mask,\n+                **kwargs,\n             )\n \n-            hidden_states = layer_outputs[0]\n-\n-            if output_attentions:\n-                all_attentions = all_attentions + (layer_outputs[1],)\n-\n-        if output_hidden_states:\n-            encoder_states = encoder_states + (hidden_states,)\n-\n-        if not return_dict:\n-            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n         return BaseModelOutput(\n             last_hidden_state=hidden_states,\n-            hidden_states=encoder_states,\n-            attentions=all_attentions,\n-        )\n-\n-\n-class MLCDVisionTransformer(CLIPVisionTransformer):\n-    def __init__(self, config: MLCDVisionConfig):\n-        super().__init__(config)\n-        self.vision_rotary_embedding = MLCDRotaryEmbedding(config.hidden_size // config.num_attention_heads // 2)\n-        self.class_pos_emb = nn.Parameter(torch.randn(1, config.hidden_size // config.num_attention_heads // 2))\n-\n-    @auto_docstring\n-    def forward(\n-        self,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[tuple, BaseModelOutputWithPooling]:\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-\n-        if pixel_values is None:\n-            raise ValueError(\"You have to specify pixel_values\")\n-\n-        num_patches_height = pixel_values.shape[-2] // self.config.patch_size\n-        num_patches_width = pixel_values.shape[-1] // self.config.patch_size\n-        rotary_pos_emb = self.vision_rotary_embedding(num_patches_height, num_patches_width)\n-        rotary_pos_emb = rotary_pos_emb.to(self.class_pos_emb.device)\n-        rotary_pos_emb = torch.cat([self.class_pos_emb, rotary_pos_emb], dim=0)\n-        emb = torch.cat((rotary_pos_emb, rotary_pos_emb), dim=-1)\n-        position_embeddings = (emb.cos(), emb.sin())\n-\n-        hidden_states = self.embeddings(pixel_values)\n-        hidden_states = self.pre_layrnorm(hidden_states)\n-\n-        encoder_outputs = self.encoder(\n-            inputs_embeds=hidden_states,\n-            position_embeddings=position_embeddings,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-        )\n-\n-        last_hidden_state = encoder_outputs[0]\n-        pooled_output = last_hidden_state[:, 0, :]\n-        pooled_output = self.post_layernorm(pooled_output)\n-\n-        if not return_dict:\n-            return (last_hidden_state, pooled_output) + encoder_outputs[1:]\n-\n-        return BaseModelOutputWithPooling(\n-            last_hidden_state=last_hidden_state,\n-            pooler_output=pooled_output,\n-            hidden_states=encoder_outputs.hidden_states,\n-            attentions=encoder_outputs.attentions,\n         )\n \n \n@@ -443,8 +344,15 @@ class MLCDPreTrainedModel(PreTrainedModel):\n     config: MLCDVisionConfig\n     base_model_prefix = \"mlcd\"\n     supports_gradient_checkpointing = True\n+    accepts_loss_kwargs = False\n     _supports_flash_attn = True\n     _supports_sdpa = True\n+    _supports_flex_attn = True\n+    _supports_attention_backend = True\n+    _can_record_outputs = {\n+        \"hidden_states\": MLCDEncoderLayer,\n+        \"attentions\": MLCDAttention,\n+    }\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n@@ -478,14 +386,55 @@ def _init_weights(self, module):\n             module.bias.data.zero_()\n \n \n+class MLCDVisionTransformer(CLIPVisionTransformer):\n+    def __init__(self, config: MLCDVisionConfig):\n+        super().__init__(config)\n+        self.vision_rotary_embedding = MLCDRotaryEmbedding(config.hidden_size // config.num_attention_heads // 2)\n+        self.class_pos_emb = nn.Parameter(torch.randn(1, config.hidden_size // config.num_attention_heads // 2))\n+\n+    @auto_docstring\n+    def forward(\n+        self,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[tuple, BaseModelOutputWithPooling]:\n+        if pixel_values is None:\n+            raise ValueError(\"You have to specify pixel_values\")\n+\n+        num_patches_height = pixel_values.shape[-2] // self.config.patch_size\n+        num_patches_width = pixel_values.shape[-1] // self.config.patch_size\n+        rotary_pos_emb = self.vision_rotary_embedding(num_patches_height, num_patches_width)\n+        rotary_pos_emb = rotary_pos_emb.to(self.class_pos_emb.device)\n+        rotary_pos_emb = torch.cat([self.class_pos_emb, rotary_pos_emb], dim=0)\n+        emb = torch.cat((rotary_pos_emb, rotary_pos_emb), dim=-1)\n+        position_embeddings = (emb.cos(), emb.sin())\n+\n+        hidden_states = self.embeddings(pixel_values)\n+        hidden_states = self.pre_layrnorm(hidden_states)\n+\n+        encoder_outputs = self.encoder(\n+            inputs_embeds=hidden_states,\n+            position_embeddings=position_embeddings,\n+            **kwargs,\n+        )\n+\n+        last_hidden_state = encoder_outputs[0]\n+        pooled_output = last_hidden_state[:, 0, :]\n+        pooled_output = self.post_layernorm(pooled_output)\n+\n+        return BaseModelOutputWithPooling(\n+            last_hidden_state=last_hidden_state,\n+            pooler_output=pooled_output,\n+        )\n+\n+\n class MLCDVisionModel(CLIPVisionModel):\n+    @check_model_inputs(tie_last_hidden_states=False)\n     @auto_docstring\n     def forward(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, BaseModelOutputWithPooling]:\n         r\"\"\"\n         Example:\n@@ -509,17 +458,9 @@ def forward(\n         >>> print(f\"Number of attention layers: {len(outputs.attentions)}\")\n         >>> print(f\"Attention shape: {outputs.attentions[0].shape}\")\n         ```\"\"\"\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-\n         return self.vision_model(\n             pixel_values=pixel_values,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            **kwargs,\n         )\n \n "
      },
      {
        "filename": "tests/models/mlcd/test_modeling_mlcd.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -146,7 +146,7 @@ class MLCDVisionModelIntegrationTest(unittest.TestCase):\n     @slow\n     def test_inference(self):\n         model_name = \"DeepGlint-AI/mlcd-vit-bigG-patch14-448\"\n-        model = MLCDVisionModel.from_pretrained(model_name).to(torch_device)\n+        model = MLCDVisionModel.from_pretrained(model_name, attn_implementation=\"eager\").to(torch_device)\n         processor = AutoProcessor.from_pretrained(model_name)\n \n         # process single image"
      }
    ],
    "num_files": 6,
    "scraped_at": "2025-11-16T21:17:02.173611"
  },
  {
    "pr_number": 41725,
    "title": "Add GLPNImageProcessorFast ",
    "body": "# What does this PR do?\r\n\r\nThis PR adds a **fast image processor for the GLPN model**, implemented as `GLPNImageProcessorFast`.  \r\n\r\nFixes # (issue)\r\n\r\n## Before submitting\r\n- Implements `GLPNImageProcessorFast` using `BaseImageProcessorFast`.\r\n- Adds tests and documentation updates.\r\n\r\n### \ud83e\uddea Testing\r\n- All tests pass except for the (`test_slow_fast_equivalence_batched`). I would like some help here. \r\n\r\n### \ud83d\udcc4 Files updated\r\n- `src/transformers/models/glpn/image_processing_glpn_fast.py`\r\n- `src/transformers/models/glpn/__init__.py`\r\n- `src/transformers/models/auto/image_processing_auto.py`\r\n- `tests/models/glpn/test_image_processing_glpn.py`\r\n- `docs/source/en/model_doc/glpn.md`\r\n\r\n## Before submitting\r\n- [x] Read the [contributor guidelines](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request).\r\n- [x] Updated documentation and tests.\r\n- [x] Verified style and quality with `make style` and `make quality`.\r\n\r\n## Who can review?\r\n\r\n@yonigozlan @molbap\r\n",
    "html_url": "https://github.com/huggingface/transformers/pull/41725",
    "created_at": "2025-10-19T01:23:05Z",
    "merged_at": "2025-11-04T15:44:52Z",
    "merge_commit_sha": "9a19171fad3025f57fae72d8f3598f44b68102e5",
    "base_ref": "main",
    "head_sha": "60df2c6ce408cc623aef0551f3968e9fe3d1295b",
    "user": "Aravind-11",
    "files": [
      {
        "filename": "docs/source/en/model_doc/glpn.md",
        "status": "modified",
        "additions": 5,
        "deletions": 0,
        "changes": 5,
        "patch": "@@ -61,6 +61,11 @@ A list of official Hugging Face and community (indicated by \ud83c\udf0e) resources to h\n [[autodoc]] GLPNImageProcessor\n     - preprocess\n \n+## GLPNImageProcessorFast\n+\n+[[autodoc]] GLPNImageProcessorFast\n+    - preprocess\n+\n ## GLPNModel\n \n [[autodoc]] GLPNModel"
      },
      {
        "filename": "src/transformers/image_processing_utils_fast.py",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -305,6 +305,8 @@ def resize(\n                 Dictionary in the format `{\"height\": int, \"width\": int}` specifying the size of the output image.\n             interpolation (`InterpolationMode`, *optional*, defaults to `InterpolationMode.BILINEAR`):\n                 `InterpolationMode` filter to use when resizing the image e.g. `InterpolationMode.BICUBIC`.\n+            antialias (`bool`, *optional*, defaults to `True`):\n+                Whether to use antialiasing.\n \n         Returns:\n             `torch.Tensor`: The resized image."
      },
      {
        "filename": "src/transformers/models/auto/image_processing_auto.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -103,7 +103,7 @@\n             (\"gemma3n\", (\"SiglipImageProcessor\", \"SiglipImageProcessorFast\")),\n             (\"git\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),\n             (\"glm4v\", (\"Glm4vImageProcessor\", \"Glm4vImageProcessorFast\")),\n-            (\"glpn\", (\"GLPNImageProcessor\", None)),\n+            (\"glpn\", (\"GLPNImageProcessor\", \"GLPNImageProcessorFast\")),\n             (\"got_ocr2\", (\"GotOcr2ImageProcessor\", \"GotOcr2ImageProcessorFast\")),\n             (\"grounding-dino\", (\"GroundingDinoImageProcessor\", \"GroundingDinoImageProcessorFast\")),\n             (\"groupvit\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),"
      },
      {
        "filename": "src/transformers/models/glpn/__init__.py",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -21,6 +21,7 @@\n     from .configuration_glpn import *\n     from .feature_extraction_glpn import *\n     from .image_processing_glpn import *\n+    from .image_processing_glpn_fast import *\n     from .modeling_glpn import *\n else:\n     import sys"
      },
      {
        "filename": "src/transformers/models/glpn/image_processing_glpn.py",
        "status": "modified",
        "additions": 22,
        "deletions": 1,
        "changes": 23,
        "patch": "@@ -39,6 +39,7 @@\n     valid_images,\n     validate_preprocess_arguments,\n )\n+from ...processing_utils import ImagesKwargs\n from ...utils import TensorType, filter_out_non_signature_kwargs, logging, requires_backends\n \n \n@@ -49,6 +50,17 @@\n logger = logging.get_logger(__name__)\n \n \n+class GLPNImageProcessorKwargs(ImagesKwargs, total=False):\n+    \"\"\"\n+    size_divisor (`int`, *optional*, defaults to 32):\n+        When `do_resize` is `True`, images are resized so their height and width are rounded down to the closest\n+        multiple of `size_divisor`.\n+    \"\"\"\n+\n+    size_divisor: int\n+    resample: PILImageResampling\n+\n+\n @requires(backends=(\"vision\",))\n class GLPNImageProcessor(BaseImageProcessor):\n     r\"\"\"\n@@ -66,22 +78,27 @@ class GLPNImageProcessor(BaseImageProcessor):\n         do_rescale (`bool`, *optional*, defaults to `True`):\n             Whether or not to apply the scaling factor (to make pixel values floats between 0. and 1.). Can be\n             overridden by `do_rescale` in `preprocess`.\n+        rescale_factor (`float`, *optional*, defaults to `1 / 255`):\n+            The scaling factor to apply to the pixel values. Can be overridden by `rescale_factor` in `preprocess`.\n     \"\"\"\n \n     model_input_names = [\"pixel_values\"]\n+    valid_kwargs = GLPNImageProcessorKwargs\n \n     def __init__(\n         self,\n         do_resize: bool = True,\n         size_divisor: int = 32,\n         resample=PILImageResampling.BILINEAR,\n         do_rescale: bool = True,\n+        rescale_factor: Optional[float] = 1 / 255,\n         **kwargs,\n     ) -> None:\n         self.do_resize = do_resize\n         self.do_rescale = do_rescale\n         self.size_divisor = size_divisor\n         self.resample = resample\n+        self.rescale_factor = rescale_factor\n         super().__init__(**kwargs)\n \n     def resize(\n@@ -142,6 +159,7 @@ def preprocess(\n         size_divisor: Optional[int] = None,\n         resample=None,\n         do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n         return_tensors: Optional[Union[TensorType, str]] = None,\n         data_format: ChannelDimension = ChannelDimension.FIRST,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n@@ -181,6 +199,7 @@ def preprocess(\n         \"\"\"\n         do_resize = do_resize if do_resize is not None else self.do_resize\n         do_rescale = do_rescale if do_rescale is not None else self.do_rescale\n+        rescale_factor = rescale_factor if rescale_factor is not None else self.rescale_factor\n         size_divisor = size_divisor if size_divisor is not None else self.size_divisor\n         resample = resample if resample is not None else self.resample\n \n@@ -217,7 +236,9 @@ def preprocess(\n             ]\n \n         if do_rescale:\n-            images = [self.rescale(image, scale=1 / 255, input_data_format=input_data_format) for image in images]\n+            images = [\n+                self.rescale(image, scale=rescale_factor, input_data_format=input_data_format) for image in images\n+            ]\n \n         images = [\n             to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format) for image in images"
      },
      {
        "filename": "src/transformers/models/glpn/image_processing_glpn_fast.py",
        "status": "added",
        "additions": 136,
        "deletions": 0,
        "changes": 136,
        "patch": "@@ -0,0 +1,136 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Fast Image processor class for GLPN.\"\"\"\n+\n+from typing import Optional, Union\n+\n+import torch\n+from torchvision.transforms.v2 import functional as F\n+\n+from ...image_processing_utils import BatchFeature\n+from ...image_processing_utils_fast import BaseImageProcessorFast, group_images_by_shape, reorder_images\n+from ...image_utils import (\n+    PILImageResampling,\n+    SizeDict,\n+)\n+from ...utils import (\n+    TensorType,\n+    auto_docstring,\n+    requires_backends,\n+)\n+from .image_processing_glpn import GLPNImageProcessorKwargs\n+\n+\n+@auto_docstring\n+class GLPNImageProcessorFast(BaseImageProcessorFast):\n+    do_resize = True\n+    do_rescale = True\n+    rescale_factor = 1 / 255\n+    resample = PILImageResampling.BILINEAR\n+    size_divisor = 32\n+    valid_kwargs = GLPNImageProcessorKwargs\n+\n+    def _validate_preprocess_kwargs(self, **kwargs):\n+        # pop `do_resize` to not raise an error as `size` is not None\n+        kwargs.pop(\"do_resize\", None)\n+        return super()._validate_preprocess_kwargs(**kwargs)\n+\n+    def resize(\n+        self,\n+        image: \"torch.Tensor\",\n+        size_divisor: int,\n+        interpolation: Optional[\"F.InterpolationMode\"] = None,\n+        antialias: bool = True,\n+        **kwargs,\n+    ) -> \"torch.Tensor\":\n+        \"\"\"\n+        Resize an image to `(size[\"height\"], size[\"width\"])`.\n+\n+        Args:\n+            image (`torch.Tensor`):\n+                Image to resize.\n+            size (`SizeDict`):\n+                Dictionary in the format `{\"height\": int, \"width\": int}` specifying the size of the output image.\n+            interpolation (`InterpolationMode`, *optional*, defaults to `InterpolationMode.BILINEAR`):\n+                `InterpolationMode` filter to use when resizing the image e.g. `InterpolationMode.BICUBIC`.\n+            antialias (`bool`, *optional*, defaults to `True`):\n+                Whether to use antialiasing.\n+\n+        Returns:\n+            `torch.Tensor`: The resized image.\n+        \"\"\"\n+        height, width = image.shape[-2:]\n+        # Rounds the height and width down to the closest multiple of size_divisor\n+        new_h = height // size_divisor * size_divisor\n+        new_w = width // size_divisor * size_divisor\n+        return super().resize(\n+            image, SizeDict(height=new_h, width=new_w), interpolation=interpolation, antialias=antialias\n+        )\n+\n+    def _preprocess(\n+        self,\n+        images: list[\"torch.Tensor\"],\n+        do_resize: bool,\n+        size_divisor: Optional[int] = None,\n+        interpolation: Optional[\"F.InterpolationMode\"] = None,\n+        do_rescale: bool = True,\n+        rescale_factor: Optional[float] = 1 / 255,\n+        do_normalize: bool = False,\n+        image_mean: Optional[Union[float, list[float]]] = None,\n+        image_std: Optional[Union[float, list[float]]] = None,\n+        disable_grouping: Optional[bool] = None,\n+        return_tensors: Optional[Union[str, TensorType]] = None,\n+        resample: Optional[PILImageResampling] = None,\n+        **kwargs,\n+    ) -> BatchFeature:\n+        grouped_images, grouped_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n+        processed_groups = {}\n+\n+        for shape, stacked_images in grouped_images.items():\n+            if do_resize:\n+                stacked_images = self.resize(stacked_images, size_divisor=size_divisor, interpolation=interpolation)\n+            stacked_images = self.rescale_and_normalize(\n+                stacked_images, do_rescale, rescale_factor, do_normalize, image_mean, image_std\n+            )\n+            processed_groups[shape] = stacked_images\n+\n+        processed_images = reorder_images(processed_groups, grouped_index)\n+        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n+        return BatchFeature(data={\"pixel_values\": processed_images}, tensor_type=return_tensors)\n+\n+    def post_process_depth_estimation(self, outputs, target_sizes=None):\n+        \"\"\"\n+        Convert raw model outputs to final depth predictions.\n+        Mirrors slow GLPN: PyTorch interpolate w/ bicubic, align_corners=False.\n+        \"\"\"\n+        requires_backends(self, \"torch\")\n+        predicted_depth = outputs.predicted_depth\n+\n+        results = []\n+        target_sizes = target_sizes or [None] * predicted_depth.shape[0]\n+        for depth, target_size in zip(predicted_depth, target_sizes):\n+            if target_size is not None:\n+                # Add batch and channel dimensions for interpolation\n+                depth_4d = depth[None, None, ...]\n+                resized = torch.nn.functional.interpolate(\n+                    depth_4d, size=target_size, mode=\"bicubic\", align_corners=False\n+                )\n+                depth = resized.squeeze(0).squeeze(0)\n+            results.append({\"predicted_depth\": depth})\n+\n+        return results\n+\n+\n+__all__ = [\"GLPNImageProcessorFast\"]"
      },
      {
        "filename": "tests/models/glpn/test_image_processing_glpn.py",
        "status": "modified",
        "additions": 61,
        "deletions": 6,
        "changes": 67,
        "patch": "@@ -18,7 +18,7 @@\n import numpy as np\n \n from transformers.testing_utils import require_torch, require_vision\n-from transformers.utils import is_torch_available, is_vision_available\n+from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n \n@@ -31,6 +31,9 @@\n \n     from transformers import GLPNImageProcessor\n \n+    if is_torchvision_available():\n+        from transformers import GLPNImageProcessorFast\n+\n \n class GLPNImageProcessingTester:\n     def __init__(\n@@ -87,19 +90,32 @@ def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=F\n             torchify=torchify,\n         )\n \n+    def prepare_depth_outputs(self):\n+        if not is_torch_available():\n+            return None\n+        depth_tensors = prepare_image_inputs(\n+            batch_size=self.batch_size,\n+            num_channels=1,\n+            min_resolution=self.min_resolution,\n+            max_resolution=self.max_resolution,\n+            equal_resolution=True,\n+            torchify=True,\n+        )\n+        depth_tensors = [depth_tensor.squeeze(0) for depth_tensor in depth_tensors]\n+        stacked_depth_tensors = torch.stack(depth_tensors, dim=0)\n+        return type(\"DepthOutput\", (), {\"predicted_depth\": stacked_depth_tensors})\n+\n \n @require_torch\n @require_vision\n class GLPNImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n     image_processing_class = GLPNImageProcessor if is_vision_available() else None\n+    fast_image_processing_class = GLPNImageProcessorFast if is_torchvision_available() else None\n \n     def setUp(self):\n         super().setUp()\n         self.image_processor_tester = GLPNImageProcessingTester(self)\n-\n-    @property\n-    def image_processor_dict(self):\n-        return self.image_processor_tester.prepare_image_processor_dict()\n+        self.image_processor_dict = self.image_processor_tester.prepare_image_processor_dict()\n \n     def test_image_processor_properties(self):\n         image_processing = self.image_processing_class(**self.image_processor_dict)\n@@ -115,7 +131,6 @@ def test_call_pil(self):\n         image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False)\n         for image in image_inputs:\n             self.assertIsInstance(image, Image.Image)\n-\n         # Test not batched input (GLPNImageProcessor doesn't support batching)\n         encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").pixel_values\n         expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs)\n@@ -161,3 +176,43 @@ def test_call_numpy_4_channels(self):\n         expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs)\n         self.assertTrue(tuple(encoded_images.shape) == (1, *expected_output_image_shape))\n         self.image_processing_class.num_channels = 3\n+\n+    # override as glpn image processors don't support heterogeneous batching\n+    @require_vision\n+    @require_torch\n+    def test_slow_fast_equivalence_batched(self):\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n+\n+        dummy_images = self.image_processor_tester.prepare_image_inputs(equal_resolution=True, torchify=True)\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        encoding_slow = image_processor_slow(dummy_images, return_tensors=\"pt\")\n+        encoding_fast = image_processor_fast(dummy_images, return_tensors=\"pt\")\n+\n+        self._assert_slow_fast_tensors_equivalence(encoding_slow.pixel_values, encoding_fast.pixel_values)\n+\n+    def test_post_process_depth_equivalence(self):\n+        # Check that both processors produce equivalent post-processed depth maps\n+        if self.fast_image_processing_class is None:\n+            self.skipTest(\"TorchVision not available\")\n+\n+        outputs = self.image_processor_tester.prepare_depth_outputs()\n+        slow = self.image_processing_class(**self.image_processor_dict)\n+        fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        # target_sizes simulate resized inference outputs\n+        target_sizes = [(240, 320)] * self.image_processor_tester.batch_size\n+        processed_slow = slow.post_process_depth_estimation(outputs, target_sizes=target_sizes)\n+        processed_fast = fast.post_process_depth_estimation(outputs, target_sizes=target_sizes)\n+\n+        # Compare per-sample predicted depth tensors\n+        for pred_slow, pred_fast in zip(processed_slow, processed_fast):\n+            depth_slow = pred_slow[\"predicted_depth\"]\n+            depth_fast = pred_fast[\"predicted_depth\"]\n+            torch.testing.assert_close(depth_fast, depth_slow, atol=1e-1, rtol=1e-3)\n+            self.assertLessEqual(torch.mean(torch.abs(depth_fast.float() - depth_slow.float())).item(), 5e-3)"
      }
    ],
    "num_files": 7,
    "scraped_at": "2025-11-16T21:17:05.746058"
  },
  {
    "pr_number": 41691,
    "title": "Remove skipped tests without parents",
    "body": "# What does this PR do?\r\n\r\nTensorflow tests that were still present! And a function not used anymore after https://github.com/huggingface/transformers/pull/41683 and https://github.com/huggingface/transformers/pull/41688\r\n",
    "html_url": "https://github.com/huggingface/transformers/pull/41691",
    "created_at": "2025-10-17T14:18:30Z",
    "merged_at": "2025-10-17T14:25:40Z",
    "merge_commit_sha": "39b6d3bf7e30b92b2de50a31ee991557d41ab568",
    "base_ref": "main",
    "head_sha": "bede5c6954f1d7777b9b1e9ccc1d6513fc49bcce",
    "user": "Cyrilvallez",
    "files": [
      {
        "filename": "tests/models/mpnet/test_modeling_mpnet.py",
        "status": "modified",
        "additions": 0,
        "deletions": 4,
        "changes": 4,
        "patch": "@@ -242,10 +242,6 @@ def test_for_question_answering(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_mpnet_for_question_answering(*config_and_inputs)\n \n-    @unittest.skip(reason=\"TFMPNet adds poolers to all models, unlike the PT model class.\")\n-    def test_tf_from_pt_safetensors(self):\n-        return\n-\n \n @require_torch\n class MPNetModelIntegrationTest(unittest.TestCase):"
      },
      {
        "filename": "tests/models/speech_to_text/test_modeling_speech_to_text.py",
        "status": "modified",
        "additions": 0,
        "deletions": 4,
        "changes": 4,
        "patch": "@@ -600,10 +600,6 @@ def test_resize_embeddings_untied(self):\n     def test_generate_without_input_ids(self):\n         pass\n \n-    @unittest.skip(reason=\"Test failing,  @RocketNight is looking into it\")\n-    def test_tf_from_pt_safetensors(self):\n-        pass\n-\n \n @require_torch\n @require_torchaudio"
      },
      {
        "filename": "tests/models/tapas/test_modeling_tapas.py",
        "status": "modified",
        "additions": 0,
        "deletions": 4,
        "changes": 4,
        "patch": "@@ -520,10 +520,6 @@ def test_for_sequence_classification(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_for_sequence_classification(*config_and_inputs)\n \n-    @unittest.skip(reason=\"tfp is not defined even if installed. FIXME @Arthur in a followup PR!\")\n-    def test_tf_from_pt_safetensors(self):\n-        pass\n-\n \n def prepare_tapas_single_inputs_for_inference():\n     # Here we prepare a single table-question pair to test TAPAS inference on:"
      },
      {
        "filename": "tests/test_modeling_common.py",
        "status": "modified",
        "additions": 0,
        "deletions": 8,
        "changes": 8,
        "patch": "@@ -1344,14 +1344,6 @@ def test_attention_outputs(self):\n                     [self.model_tester.num_attention_heads, encoder_seq_length, encoder_key_length],\n                 )\n \n-    # This is copied from `torch/testing/_internal/jit_utils.py::clear_class_registry`\n-    def clear_torch_jit_class_registry(self):\n-        torch._C._jit_clear_class_registry()\n-        torch.jit._recursive.concrete_type_store = torch.jit._recursive.ConcreteTypeStore()\n-        # torch 1.8 has no `_clear_class_state` in `torch.jit._state`\n-        if hasattr(torch.jit._state, \"_clear_class_state\"):\n-            torch.jit._state._clear_class_state()\n-\n     def test_hidden_states_output(self):\n         def check_hidden_states_output(inputs_dict, config, model_class):\n             model = model_class(copy.deepcopy(config))"
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T21:17:10.991250"
  },
  {
    "pr_number": 41672,
    "title": "feat: add benchmark v2 ci with results pushed to dataset",
    "body": "Reusing the old `benchmark.yml` workflow for the benchmark v2, we'll eventually rename and clean up everything once we're confident it works nicely.\r\n\r\nBenchmark results will be pushed to a dataset, for now located [here](https://huggingface.co/datasets/hf-benchmarks/transformers).\r\n\r\nI chose JSONL for simplicity of parsing in the frontend space, and file name have the following format: `f\"benchmark_run_{timestamp}.jsonl\"`.\r\n\r\nAlso did a little naming refactoring and added extra metadata",
    "html_url": "https://github.com/huggingface/transformers/pull/41672",
    "created_at": "2025-10-16T20:21:06Z",
    "merged_at": "2025-10-20T07:56:58Z",
    "merge_commit_sha": "71db0d49e99884566026c140f8b12b61056fa8dc",
    "base_ref": "main",
    "head_sha": "213f4f92086981c65d65edcb0002217ad8900241",
    "user": "McPatate",
    "files": [
      {
        "filename": ".github/workflows/benchmark.yml",
        "status": "modified",
        "additions": 10,
        "deletions": 21,
        "changes": 31,
        "patch": "@@ -1,14 +1,19 @@\n name: Self-hosted runner (benchmark)\r\n \r\n on:\r\n-  workflow_dispatch:\r\n+  push:\r\n+    branches: [main]\r\n+  pull_request:\r\n+    types: [ opened, labeled, reopened, synchronize ]\r\n \r\n concurrency:\r\n   group: ${{ github.workflow }}-${{ github.head_ref || github.run_id }}\r\n   cancel-in-progress: true\r\n \r\n env:\r\n   HF_HOME: /mnt/cache\r\n+  DATASET_ID: hf-benchmarks/transformers\r\n+  MODEL_ID: meta-llama/Llama-3.1-8B-Instruct\r\n \r\n jobs:\r\n   benchmark:\r\n@@ -31,26 +36,12 @@ jobs:\n         with:\r\n           ref: ${{ github.event.pull_request.head.sha || github.sha }}\r\n \r\n-      - name: Install libpq-dev & psql\r\n-        run: |\r\n-          apt update\r\n-          apt install -y libpq-dev postgresql-client\r\n-\r\n       - name: Install benchmark script dependencies\r\n-        run: python3 -m pip install -r benchmark/requirements.txt\r\n+        run: python3 -m pip install -r benchmark_v2/requirements.txt kernels\r\n \r\n       - name: Reinstall transformers in edit mode (remove the one installed during docker image build)\r\n         working-directory: /transformers\r\n-        run: python3 -m pip uninstall -y transformers && python3 -m pip install -e \".[torch]\"\r\n-\r\n-      - name: Run database init script\r\n-        run: |\r\n-          psql -f benchmark/utils/init_db.sql\r\n-        env:\r\n-          PGDATABASE: metrics\r\n-          PGHOST: ${{ secrets.TRANSFORMERS_BENCHMARKS_PGHOST }}\r\n-          PGUSER: transformers_benchmarks\r\n-          PGPASSWORD: ${{ secrets.TRANSFORMERS_BENCHMARKS_PGPASSWORD }}\r\n+        run: python3 -m pip uninstall -y transformers && python3 -m pip install -e \".[torch]\" && python3 -m pip uninstall -y torchvision # temp fix\r\n \r\n       - name: Run benchmark\r\n         run: |\r\n@@ -61,13 +52,11 @@ jobs:\n             commit_id=$GITHUB_SHA\r\n           fi\r\n           commit_msg=$(git show -s --format=%s | cut -c1-70)\r\n-          python3 benchmark/benchmarks_entrypoint.py \"huggingface/transformers\" \"$BRANCH_NAME\" \"$commit_id\" \"$commit_msg\"\r\n+          python3 benchmark_v2/run_benchmarks.py -b 32 -s 128 -n 256 --branch-name \"$BRANCH_NAME\" --commit-id \"$commit_id\" --commit-message \"$commit_msg\" --model-id \"$MODEL_ID\" --log-level INFO --push-result-to-dataset \"$DATASET_ID\"\r\n         env:\r\n           HF_TOKEN: ${{ secrets.HF_HUB_READ_TOKEN }}\r\n+          PUSH_TO_HUB_TOKEN: ${{ secrets.PUSH_TO_HUB_TOKEN }}\r\n           # Enable this to see debug logs\r\n           # HF_HUB_VERBOSITY: debug\r\n           # TRANSFORMERS_VERBOSITY: debug\r\n-          PGHOST: ${{ secrets.TRANSFORMERS_BENCHMARKS_PGHOST }}\r\n-          PGUSER: transformers_benchmarks\r\n-          PGPASSWORD: ${{ secrets.TRANSFORMERS_BENCHMARKS_PGPASSWORD }}\r\n           BRANCH_NAME: ${{ github.head_ref || github.ref_name }}\r"
      },
      {
        "filename": "benchmark_v2/framework/benchmark_config.py",
        "status": "modified",
        "additions": 7,
        "deletions": 8,
        "changes": 15,
        "patch": "@@ -22,7 +22,7 @@ def __init__(\n         self,\n         warmup_iterations: int = 5,\n         measurement_iterations: int = 20,\n-        gpu_monitoring: bool = False,  # False by default because it slows down the benchmark by a lot\n+        gpu_monitoring: bool = True,  # NOTE: you may want to disable this at times as we have obsvered it could heavily slow down benchmarks on AMD\n         batch_size: int = 1,\n         sequence_length: int = 128,\n         num_tokens_to_generate: int = 128,\n@@ -136,7 +136,7 @@ def cross_generate_configs(\n     batch_size: int = 1,\n     sequence_length: int = 128,\n     num_tokens_to_generate: int = 128,\n-    gpu_monitoring: bool = False,  # this slows down the benchmark by a lot so we disable it by default\n+    gpu_monitoring: bool = True,\n ) -> list[BenchmarkConfig]:\n     # Create kwargs common to all configs\n     kwargs = {\n@@ -169,7 +169,7 @@ def generate_all_configs(\n     batch_size: int = 1,\n     sequence_length: int = 128,\n     num_tokens_to_generate: int = 128,\n-    gpu_monitoring: bool = False,\n+    gpu_monitoring: bool = True,\n ) -> list[BenchmarkConfig]:\n     all_attn_implementations = [\n         (\"flash_attention_2\", None),\n@@ -197,7 +197,6 @@ def generate_main_configs(\n     batch_size: int = 1,\n     sequence_length: int = 128,\n     num_tokens_to_generate: int = 128,\n-    gpu_monitoring: bool = False,\n ) -> list[BenchmarkConfig]:\n     # Create kwargs common to all configs\n     kwargs = {\n@@ -206,10 +205,10 @@ def generate_main_configs(\n         \"batch_size\": batch_size,\n         \"sequence_length\": sequence_length,\n         \"num_tokens_to_generate\": num_tokens_to_generate,\n-        \"gpu_monitoring\": gpu_monitoring,\n     }\n     return [  # TODO: test max-autotune instead of default\n-        BenchmarkConfig(attn_implementation=\"flex_attention\", compile_mode=\"default\", **kwargs),\n-        BenchmarkConfig(attn_implementation=\"eager\", compile_mode=\"default\", **kwargs),\n-        BenchmarkConfig(attn_implementation=\"flash_attention_2\", **kwargs),\n+        BenchmarkConfig(attn_implementation=\"flex_attention\", compile_mode=\"default\", gpu_monitoring=False, **kwargs),\n+        BenchmarkConfig(attn_implementation=\"flex_attention\", compile_mode=\"default\", gpu_monitoring=True, **kwargs),\n+        BenchmarkConfig(attn_implementation=\"eager\", compile_mode=\"default\", gpu_monitoring=True, **kwargs),\n+        BenchmarkConfig(attn_implementation=\"flash_attention_2\", gpu_monitoring=True, **kwargs),\n     ]"
      },
      {
        "filename": "benchmark_v2/framework/benchmark_runner.py",
        "status": "modified",
        "additions": 76,
        "deletions": 9,
        "changes": 85,
        "patch": "@@ -4,13 +4,16 @@\n import os\n import pathlib\n import re\n+import tempfile\n import time\n from contextlib import nullcontext\n from datetime import datetime\n from queue import Queue\n from typing import Any\n \n import torch\n+from datasets import Dataset\n+from huggingface_hub import HfApi\n from tqdm import trange\n \n from transformers import (\n@@ -50,6 +53,8 @@\n     \"Its instability ended in the coup of 18 Brumaire and the establishment of the Consulate, with Napoleon Bonaparte as First Consul.\",\n ])  # fmt: skip\n \n+PUSH_TO_HUB_TOKEN = os.getenv(\"PUSH_TO_HUB_TOKEN\", None)\n+\n \n def compact_json_numeric_arrays(data: dict):\n     # Match arrays that contain only numbers (ints/floats), whitespace, commas, and newlines\n@@ -120,15 +125,19 @@ def flush_memory():\n \n class BenchmarkStreamer(BaseStreamer):\n     def __init__(self, **kwargs) -> None:\n+        self.timeout = kwargs.pop(\"timeout\", 10)\n         self.timestamps = []\n         self.text_queue = Queue()\n+        self.stop_signal = None\n \n     def put(self, value):\n         \"\"\"Receives tokens and logs the timestamp of the generation.\"\"\"\n         self.timestamps.append(time.perf_counter())\n+        self.text_queue.put(value)\n \n     def end(self):\n         self.timestamps.append(time.perf_counter())\n+        self.text_queue.put(self.stop_signal)\n \n     def __iter__(self):\n         return self\n@@ -144,13 +153,22 @@ def __next__(self):\n class BenchmarkRunner:\n     \"\"\"Main benchmark runner that coordinates benchmark execution.\"\"\"\n \n-    def __init__(self, logger: logging.Logger, output_dir: str | None = None, commit_id: str | None = None) -> None:\n+    def __init__(\n+        self,\n+        logger: logging.Logger,\n+        output_dir: str | None = None,\n+        branch_name: str | None = None,\n+        commit_id: str | None = None,\n+        commit_message: str | None = None,\n+    ) -> None:\n         # Those stay constant for the whole run\n         self.logger = logger\n         if output_dir is None:\n             output_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), \"benchmark_results\")\n         self.output_dir = output_dir\n+        self.branch_name = branch_name\n         self.commit_id = get_git_revision() if commit_id is None else commit_id\n+        self.commit_message = commit_message\n         os.makedirs(self.output_dir, exist_ok=True)\n         self.profile_dir = None\n         # Attributes that are reset for each model\n@@ -163,7 +181,7 @@ def cleanup(self) -> None:\n         self.model = None\n         flush_memory()\n \n-    def setup_one_run(self, model_id: str, config: BenchmarkConfig) -> None:\n+    def setup_benchmark(self, model_id: str, config: BenchmarkConfig) -> None:\n         # Some attributes only need to be set once per model\n         if self._setup_for != model_id:\n             self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n@@ -200,10 +218,13 @@ def setup_one_run(self, model_id: str, config: BenchmarkConfig) -> None:\n         self.model = self.model.eval().to(config.device)\n \n         # Kernelize the model if needed\n-        if config.kernelize:\n+        if config.kernelize and kernelize is not None and Mode is not None:\n             self.model = kernelize(self.model, mode=Mode.INFERENCE)\n \n-    def run_one_benchmark(self, model_id: str, config: BenchmarkConfig, num_tokens_to_profile: int = 0) -> None:\n+    def run_benchmark(\n+        self, model_id: str, config: BenchmarkConfig, num_tokens_to_profile: int = 0\n+    ) -> dict[str, Any] | None:\n+        \"\"\"Run a single benchmark with the given model ID and config.\"\"\"\n         sdpa_ctx = nullcontext()\n         if config.attn_implementation == \"sdpa\":\n             sdpa_backend = get_sdpa_backend(config.sdpa_backend)\n@@ -243,7 +264,12 @@ def run_one_benchmark(self, model_id: str, config: BenchmarkConfig, num_tokens_t\n                 self.profile_generate(num_tokens_to_profile, config.name)\n \n             return {\n-                \"metadata\": BenchmarkMetadata(model_id=model_id, commit_id=self.commit_id),\n+                \"metadata\": BenchmarkMetadata(\n+                    model_id=model_id,\n+                    branch_name=self.branch_name,\n+                    commit_id=self.commit_id,\n+                    commit_message=self.commit_message,\n+                ),\n                 \"measurements\": result,\n                 \"config\": config,\n             }\n@@ -305,7 +331,8 @@ def run_benchmarks(\n         benchmark_configs: list[BenchmarkConfig],\n         num_tokens_to_profile: int = 0,\n         pretty_print_summary: bool = True,\n-    ) -> dict[str, Any]:\n+    ) -> tuple[str, dict[str, Any]]:\n+        \"\"\"Run multiple benchmarks for the given model ID and list of benchmark configs.\"\"\"\n         all_results = {}\n         timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n         start_time = time.perf_counter()\n@@ -324,14 +351,14 @@ def run_benchmarks(\n                 continue\n \n             # Otherwise, run the benchmark\n-            self.setup_one_run(model_id, config)\n+            self.setup_benchmark(model_id, config)\n             self.logger.info(\n                 f\"Running benchmark of model {model_id} with scenario: {config.name} ({i + 1}/{n_configs})\"\n             )\n \n             # Launch benchmark in a try/except block to avoid stopping the whole run if one benchmark fails\n             try:\n-                results = self.run_one_benchmark(model_id, config, num_tokens_to_profile)\n+                results = self.run_benchmark(model_id, config, num_tokens_to_profile)\n                 if results is not None:\n                     all_results[config.hash] = results\n \n@@ -358,7 +385,7 @@ def run_benchmarks(\n                 result[\"measurements\"].pprint(batch_size=result[\"config\"].batch_size, tabs=1)\n             print(\"=\" * 100)\n \n-        return all_results\n+        return (timestamp, all_results)\n \n     def save_results(self, model_name: str, results: dict, timestamp: str = \"\") -> str:\n         \"\"\"Save benchmark results to JSON file.\"\"\"\n@@ -387,3 +414,43 @@ def save_results(self, model_name: str, results: dict, timestamp: str = \"\") -> s\n \n         self.logger.info(f\"Results saved to {filepath}\")\n         return filepath\n+\n+    def push_results_to_hub(self, dataset_id: str, results: dict[Any, Any], timestamp: str) -> None:\n+        if PUSH_TO_HUB_TOKEN is None:\n+            raise ValueError(\n+                \"PUSH_TO_HUB_TOKEN is not set, cannot push results to the Hub. When setting dataset_id, please also set the PUSH_TO_HUB_TOKEN environment variable.\"\n+            )\n+\n+        n_results = len(results)\n+        self.logger.info(f\"Pushing {n_results} results to: {dataset_id}\")\n+        rows = []\n+        for cfg_hash, entry in results.items():\n+            row = {\n+                \"benchmark_config_hash\": cfg_hash,\n+                \"config\": entry[\"config\"].to_dict(),\n+                \"measurements\": entry[\"measurements\"].to_dict(),\n+                \"metadata\": entry[\"metadata\"].to_dict(),\n+            }\n+            rows.append(row)\n+\n+        ds = Dataset.from_list(rows)\n+        with tempfile.TemporaryDirectory() as tmp:\n+            jsonl_path = os.path.join(tmp, \"data.jsonl\")\n+            with open(jsonl_path, \"w\") as f:\n+                json_lines = []\n+                for ex in ds:\n+                    json_lines.append(json.dumps(ex, ensure_ascii=False))\n+                f.write(\"\\n\".join(json_lines))\n+\n+            api = HfApi()\n+            # NOTE: we expect the repository to already exist\n+            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\") if not timestamp else timestamp\n+            file_name = f\"benchmark_run_{timestamp}.jsonl\"\n+            api.upload_file(\n+                path_or_fileobj=jsonl_path,\n+                path_in_repo=file_name,\n+                repo_id=dataset_id,\n+                repo_type=\"dataset\",\n+                token=PUSH_TO_HUB_TOKEN,\n+            )\n+        self.logger.info(f\"Succesfully uploaded results to: {dataset_id}\")"
      },
      {
        "filename": "benchmark_v2/framework/data_classes.py",
        "status": "modified",
        "additions": 10,
        "deletions": 3,
        "changes": 13,
        "patch": "@@ -1,5 +1,5 @@\n from dataclasses import dataclass\n-from datetime import datetime\n+from datetime import datetime, timezone\n from typing import Any\n \n import numpy as np\n@@ -59,19 +59,26 @@ class BenchmarkMetadata:\n \n     model_id: str\n     timestamp: str\n+    branch_name: str\n     commit_id: str\n+    commit_message: str\n     hardware_info: HardwareInfo\n \n-    def __init__(self, model_id: str, commit_id: str):\n+    def __init__(self, model_id: str, commit_id: str, branch_name: str = \"main\", commit_message: str = \"\") -> None:\n         self.model_id = model_id\n-        self.timestamp = datetime.utcnow().isoformat()\n+        self.timestamp = datetime.now(timezone.utc).isoformat()\n+        self.branch_name = branch_name\n         self.commit_id = commit_id\n+        self.commit_message = commit_message\n         self.hardware_info = HardwareInfo()\n \n     def to_dict(self) -> dict[str, Any]:\n         return {\n+            \"model_id\": self.model_id,\n             \"timestamp\": self.timestamp,\n+            \"branch_name\": self.branch_name,\n             \"commit_id\": self.commit_id,\n+            \"commit_message\": self.commit_message,\n             \"hardware_info\": self.hardware_info.to_dict(),\n         }\n "
      },
      {
        "filename": "benchmark_v2/requirements.txt",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -4,4 +4,4 @@ gpustat>=1.0.0\n torch>=2.0.0\n transformers>=4.30.0\n datasets>=2.10.0\n-huggingface_hub>=0.16.0 \n\\ No newline at end of file\n+huggingface_hub>=0.16.0"
      },
      {
        "filename": "benchmark_v2/run_benchmarks.py",
        "status": "modified",
        "additions": 32,
        "deletions": 6,
        "changes": 38,
        "patch": "@@ -33,9 +33,8 @@\n     parser.add_argument(\"--output-dir\", type=str, default=None, help=\"Output dir for benchmark results\")\n     parser.add_argument(\"--log-level\", type=str, choices=[\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\"], default=\"INFO\")\n     parser.add_argument(\"--model-id\", type=str, help=\"Specific model ID to benchmark (if supported by benchmarks)\")\n-\n-    parser.add_argument(\"--warmup\", type=int, default=3, help=\"Number of warmup iterations\")\n-    parser.add_argument(\"--iterations\", type=int, default=10, help=\"Number of measurement iterations\")\n+    parser.add_argument(\"--warmup\", \"-w\", type=int, default=3, help=\"Number of warmup iterations\")\n+    parser.add_argument(\"--iterations\", \"-i\", type=int, default=10, help=\"Number of measurement iterations\")\n \n     parser.add_argument(\"--batch-size\", \"-b\", type=int, nargs=\"+\", help=\"Batch size\")\n     parser.add_argument(\"--sequence-length\", \"-s\", type=int, nargs=\"+\", help=\"Sequence length\")\n@@ -44,7 +43,20 @@\n     parser.add_argument(\"--cross-generate\", action=\"store_true\", help=\"Cross-generate all combinations of configs\")\n     parser.add_argument(\"--num-tokens-to-profile\", \"-p\", type=int, default=0, help=\"Number of tokens to profile\")\n \n+    parser.add_argument(\"--branch-name\", type=str, help=\"Git branch name\")\n     parser.add_argument(\"--commit-id\", type=str, help=\"Git commit ID (if not provided, will auto-detect from git)\")\n+    parser.add_argument(\"--commit-message\", type=str, help=\"Git commit message\")\n+\n+    parser.add_argument(\n+        \"--no-gpu-monitoring\", action=\"store_true\", help=\"Disables GPU monitoring during benchmark runs\"\n+    )\n+\n+    parser.add_argument(\n+        \"--push-result-to-dataset\",\n+        type=str,\n+        default=None,\n+        help=\"Name of the dataset to push results to. If not provided, results are not pushed to the Hub.\",\n+    )\n     args = parser.parse_args()\n \n     # Setup logging\n@@ -76,6 +88,7 @@\n                 batch_size=args.batch_size[0],\n                 sequence_length=args.sequence_length[0],\n                 num_tokens_to_generate=args.num_tokens_to_generate[0],\n+                gpu_monitoring=not args.no_gpu_monitoring,\n             )\n         else:\n             benchmark_configs = generate_main_configs(\n@@ -106,11 +119,24 @@\n                     cfg_dict.pop(\"name\")\n                     benchmark_configs.append(BenchmarkConfig.from_dict(cfg_dict))\n \n-    runner = BenchmarkRunner(logger, args.output_dir, args.commit_id)\n-    results = runner.run_benchmarks(\n+    runner = BenchmarkRunner(\n+        logger,\n+        args.output_dir,\n+        args.branch_name,\n+        args.commit_id,\n+        args.commit_message,\n+    )\n+    timestamp, results = runner.run_benchmarks(\n         args.model_id,\n         benchmark_configs,\n         args.num_tokens_to_profile,\n         pretty_print_summary=True,\n     )\n-    # runner.save_results(args.model_id, results)\n+\n+    dataset_id = args.push_result_to_dataset\n+    if dataset_id is not None and len(results) > 0:\n+        runner.push_results_to_hub(\n+            dataset_id,\n+            results,\n+            timestamp,\n+        )"
      }
    ],
    "num_files": 6,
    "scraped_at": "2025-11-16T21:17:15.995964"
  },
  {
    "pr_number": 41662,
    "title": "Small changes to benchmarking script",
    "body": "This PR:\r\n- adds throughput to the final pretty print at the end of the benchmark runs\r\n- adds the information of the output shape to the decoded text of the output (usefull to double check if throughput is right)\r\n- changes the behavior of the `run_benchmarks.py` script so that it will only run 3 configs unless instructed to do otherwise\r\n\r\nThis way, anyone can run a \"quick\"\r\n```\r\npython benchmark_v2/run_benchmarks.py --model-id \"meta-llama/Meta-Llama-3-8B\" -b 32 -s 128 -n 256\r\n```\r\nafter changing the `generate` code or some model specific code to check perf did not take a big hit. \r\nThanks @SunMarc for the suggestion!",
    "html_url": "https://github.com/huggingface/transformers/pull/41662",
    "created_at": "2025-10-16T14:19:13Z",
    "merged_at": "2025-10-16T15:25:49Z",
    "merge_commit_sha": "f7c33abab3a6233a51d7d4fd116625be14df68ff",
    "base_ref": "main",
    "head_sha": "ed5f5fef324b63eb093fb551fb482a1afd54afcb",
    "user": "remi-or",
    "files": [
      {
        "filename": "benchmark_v2/framework/benchmark_config.py",
        "status": "modified",
        "additions": 15,
        "deletions": 18,
        "changes": 33,
        "patch": "@@ -104,7 +104,7 @@ def to_dict(self) -> dict[str, Any]:\n             \"attn_implementation\": self.attn_implementation,\n             \"sdpa_backend\": self.sdpa_backend,\n             \"compile_mode\": self.compile_mode,\n-            \"compile_options\": self.compile_options,\n+            \"compile_options\": self.compile_options | {},  # to avoid inplace modification of the original dict\n             \"kernelize\": self.kernelize,\n         }\n \n@@ -191,28 +191,25 @@ def generate_all_configs(\n     )\n \n \n-def generate_default_configs(\n+def generate_main_configs(\n     warmup_iterations: int = 5,\n     measurement_iterations: int = 20,\n     batch_size: int = 1,\n     sequence_length: int = 128,\n     num_tokens_to_generate: int = 128,\n     gpu_monitoring: bool = False,\n ) -> list[BenchmarkConfig]:\n-    all_attn_implementations = [\n-        (\"flash_attention_2\", None),\n-        (\"eager\", None),\n-        (\"sdpa\", \"math\"),\n-        (\"sdpa\", \"flash_attention\"),  # note: this one can fail with compile because of attn mask\n+    # Create kwargs common to all configs\n+    kwargs = {\n+        \"warmup_iterations\": warmup_iterations,\n+        \"measurement_iterations\": measurement_iterations,\n+        \"batch_size\": batch_size,\n+        \"sequence_length\": sequence_length,\n+        \"num_tokens_to_generate\": num_tokens_to_generate,\n+        \"gpu_monitoring\": gpu_monitoring,\n+    }\n+    return [  # TODO: test max-autotune instead of default\n+        BenchmarkConfig(attn_implementation=\"flex_attention\", compile_mode=\"default\", **kwargs),\n+        BenchmarkConfig(attn_implementation=\"eager\", compile_mode=\"default\", **kwargs),\n+        BenchmarkConfig(attn_implementation=\"flash_attention_2\", **kwargs),\n     ]\n-    return cross_generate_configs(\n-        attn_impl_and_sdpa_backend=all_attn_implementations,\n-        compiled_mode=[None, \"max-autotune\"],\n-        kernelized=[False, KERNELIZATION_AVAILABLE],\n-        warmup_iterations=warmup_iterations,\n-        measurement_iterations=measurement_iterations,\n-        batch_size=batch_size,\n-        sequence_length=sequence_length,\n-        num_tokens_to_generate=num_tokens_to_generate,\n-        gpu_monitoring=gpu_monitoring,\n-    )"
      },
      {
        "filename": "benchmark_v2/framework/benchmark_runner.py",
        "status": "modified",
        "additions": 11,
        "deletions": 10,
        "changes": 21,
        "patch": "@@ -144,11 +144,11 @@ def __next__(self):\n class BenchmarkRunner:\n     \"\"\"Main benchmark runner that coordinates benchmark execution.\"\"\"\n \n-    def __init__(\n-        self, logger: logging.Logger, output_dir: str = \"benchmark_results\", commit_id: str | None = None\n-    ) -> None:\n+    def __init__(self, logger: logging.Logger, output_dir: str | None = None, commit_id: str | None = None) -> None:\n         # Those stay constant for the whole run\n         self.logger = logger\n+        if output_dir is None:\n+            output_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), \"benchmark_results\")\n         self.output_dir = output_dir\n         self.commit_id = get_git_revision() if commit_id is None else commit_id\n         os.makedirs(self.output_dir, exist_ok=True)\n@@ -214,7 +214,7 @@ def run_one_benchmark(self, model_id: str, config: BenchmarkConfig, num_tokens_t\n \n             # Quick validation: try one measurement first to see if this scenario works\n             flush_memory()\n-            e2e_latency, token_generation_times, decoded_output, gpu_metrics = self.time_generate(\n+            e2e_latency, token_generation_times, shape_and_decoded_output, gpu_metrics = self.time_generate(\n                 max_new_tokens=1, gpu_monitor=None\n             )\n             if e2e_latency < 0:\n@@ -231,11 +231,11 @@ def run_one_benchmark(self, model_id: str, config: BenchmarkConfig, num_tokens_t\n             result = BenchmarkResult()\n             self.logger.info(f\"Benchmarking with {config.measurement_iterations} iterations.\")\n             for _ in trange(config.measurement_iterations):\n-                e2e_latency, token_generation_times, decoded_output, gpu_metrics = self.time_generate(\n+                e2e_latency, token_generation_times, shape_and_decoded_output, gpu_metrics = self.time_generate(\n                     max_new_tokens=config.num_tokens_to_generate,\n                     gpu_monitor=(GPUMonitor(logger=self.logger) if config.gpu_monitoring else None),\n                 )\n-                result.accumulate(e2e_latency, token_generation_times, decoded_output, gpu_metrics)\n+                result.accumulate(e2e_latency, token_generation_times, shape_and_decoded_output, gpu_metrics)\n             self.logger.info(\"Benchmarking done. Cleaning up.\")\n \n             # Profile if needed\n@@ -277,10 +277,11 @@ def time_generate(\n             raise RuntimeError(f\"Generated {new_tokens} tokens, expected {max_new_tokens}\")\n         # Decode outputs\n         decoded_output = self.tokenizer.decode(outputs[0, input_tokens:], skip_special_tokens=True)\n+        shape_and_decoded_output = f\"{tuple(outputs.shape)} | {decoded_output}\"\n         # Compute intermediate quantities\n         e2e_latency = wall_time_1 - wall_time_0\n         token_generation_times = [t - wall_time_0 for t in streamer.timestamps[1:]]\n-        return e2e_latency, token_generation_times, decoded_output, gpu_metrics\n+        return e2e_latency, token_generation_times, shape_and_decoded_output, gpu_metrics\n \n     def profile_generate(self, num_tokens_to_profile: int, config_name: str) -> None:\n         \"\"\"Profile the latency of a call to model.generate() with the given (inputs) and (max_new_tokens).\"\"\"\n@@ -351,10 +352,10 @@ def run_benchmarks(\n                 first_metadata = all_results[first_key][\"metadata\"].to_dict()\n                 hardware_info = first_metadata.pop(\"hardware_info\")\n                 pretty_print_dict(first_metadata | hardware_info, tabs=1)\n-            for value in all_results.values():\n+            for result in all_results.values():\n                 print(\"=\" * 100)\n-                print(f\"Config: {value['config'].infer_name(compact=False)}\\n\")\n-                value[\"measurements\"].pprint(tabs=1)\n+                print(f\"Config: {result['config'].infer_name(compact=False)}\\n\")\n+                result[\"measurements\"].pprint(batch_size=result[\"config\"].batch_size, tabs=1)\n             print(\"=\" * 100)\n \n         return all_results"
      },
      {
        "filename": "benchmark_v2/framework/data_classes.py",
        "status": "modified",
        "additions": 29,
        "deletions": 21,
        "changes": 50,
        "patch": "@@ -82,19 +82,19 @@ class BenchmarkResult:\n     def __init__(self) -> None:\n         self.e2e_latency = []\n         self.token_generation_times = []  # time at which each token was generated (relative to start of the generation)\n-        self.decoded_outputs = []\n+        self.shape_and_decoded_outputs = []\n         self.gpu_metrics = []\n \n     def accumulate(\n         self,\n         e2e_latency: float,\n         token_generation_times: list[float],\n-        decoded_output: str,\n+        shape_and_decoded_output: str,\n         gpu_metrics: GPURawMetrics | None,\n     ) -> None:\n         self.e2e_latency.append(e2e_latency)\n         self.token_generation_times.append(token_generation_times)\n-        self.decoded_outputs.append(decoded_output)\n+        self.shape_and_decoded_outputs.append(shape_and_decoded_output)\n         self.gpu_metrics.append(gpu_metrics)\n \n     def to_dict(self) -> dict[str, None | int | float]:\n@@ -106,7 +106,7 @@ def to_dict(self) -> dict[str, None | int | float]:\n         return {\n             \"e2e_latency\": self.e2e_latency,\n             \"token_generation_times\": self.token_generation_times,\n-            \"decoded_outputs\": self.decoded_outputs,\n+            \"shape_and_decoded_outputs\": self.shape_and_decoded_outputs,\n             \"gpu_metrics\": gpu_metrics,\n         }\n \n@@ -123,7 +123,7 @@ def from_dict(cls, data: dict[str, None | int | float]) -> \"BenchmarkResult\":\n             new_instance.accumulate(\n                 e2e_latency=data[\"e2e_latency\"][i],\n                 token_generation_times=data[\"token_generation_times\"][i],\n-                decoded_output=data[\"decoded_output\"][i],\n+                shape_and_decoded_output=data[\"shape_and_decoded_outputs\"][i],\n                 gpu_metrics=gpu_metrics[i],\n             )\n         return new_instance\n@@ -134,19 +134,27 @@ def get_measured_ttft(self) -> list[float]:\n     def get_measured_itl(self) -> list[float]:\n         return [(dt[-1] - dt[0]) / (len(dt) - 1) for dt in self.token_generation_times if len(dt) > 1]\n \n-    def pprint(self, tabs: int = 0) -> None:\n-        collated_stats = equalize_lengths_and_collate(\n-            [\n-                add_unit_to_duration(compute_basic_statistics(self.e2e_latency)),\n-                add_unit_to_duration(compute_basic_statistics(self.get_measured_ttft())),\n-                add_unit_to_duration(compute_basic_statistics(self.get_measured_itl())),\n-            ]\n-        )\n-        pretty_print_dict(\n-            {\n-                \"E2E Latency\": collated_stats[0],\n-                \"Time to First Token\": collated_stats[1],\n-                \"Inter-Token Latency\": collated_stats[2],\n-            },\n-            tabs=tabs,\n-        )\n+    def get_throughput(self, batch_size: int) -> float:\n+        return [\n+            batch_size * len(dt) / e2e_latency\n+            for e2e_latency, dt in zip(self.e2e_latency, self.token_generation_times)\n+        ]\n+\n+    def pprint(self, batch_size: int = 0, tabs: int = 0) -> None:\n+        stats_to_collate = [\n+            add_unit_to_duration(compute_basic_statistics(self.e2e_latency)),\n+            add_unit_to_duration(compute_basic_statistics(self.get_measured_ttft())),\n+            add_unit_to_duration(compute_basic_statistics(self.get_measured_itl())),\n+        ]\n+        if batch_size > 0:\n+            throughput_stats = compute_basic_statistics(self.get_throughput(batch_size))\n+            stats_to_collate.append({key: f\"{value:.2f}tok/s\" for key, value in throughput_stats.items()})\n+        collated_stats = equalize_lengths_and_collate(stats_to_collate)\n+        dict_to_pprint = {\n+            \"E2E Latency\": collated_stats[0],\n+            \"Time to First Token\": collated_stats[1],\n+            \"Inter-Token Latency\": collated_stats[2],\n+        }\n+        if batch_size > 0:\n+            dict_to_pprint[\"Throughput\"] = collated_stats[3]\n+        pretty_print_dict(dict_to_pprint, tabs=tabs)"
      },
      {
        "filename": "benchmark_v2/run_benchmarks.py",
        "status": "modified",
        "additions": 33,
        "deletions": 28,
        "changes": 61,
        "patch": "@@ -20,28 +20,28 @@\n \n import argparse\n import logging\n-import random\n import sys\n import uuid\n \n-from framework.benchmark_config import BenchmarkConfig, generate_all_configs\n+from framework.benchmark_config import BenchmarkConfig, generate_all_configs, generate_main_configs\n from framework.benchmark_runner import BenchmarkRunner\n \n \n if __name__ == \"__main__\":\n     # Parse arguments\n     parser = argparse.ArgumentParser()\n-    parser.add_argument(\"--output-dir\", type=str, default=\"benchmark_results\", help=\"Output dir for benchmark results\")\n+    parser.add_argument(\"--output-dir\", type=str, default=None, help=\"Output dir for benchmark results\")\n     parser.add_argument(\"--log-level\", type=str, choices=[\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\"], default=\"INFO\")\n     parser.add_argument(\"--model-id\", type=str, help=\"Specific model ID to benchmark (if supported by benchmarks)\")\n \n-    parser.add_argument(\"--warmup\", type=int, default=5, help=\"Number of warmup iterations\")\n-    parser.add_argument(\"--iterations\", type=int, default=20, help=\"Number of measurement iterations\")\n+    parser.add_argument(\"--warmup\", type=int, default=3, help=\"Number of warmup iterations\")\n+    parser.add_argument(\"--iterations\", type=int, default=10, help=\"Number of measurement iterations\")\n \n     parser.add_argument(\"--batch-size\", \"-b\", type=int, nargs=\"+\", help=\"Batch size\")\n     parser.add_argument(\"--sequence-length\", \"-s\", type=int, nargs=\"+\", help=\"Sequence length\")\n     parser.add_argument(\"--num-tokens-to-generate\", \"-n\", type=int, nargs=\"+\", help=\"Number of tokens to generate\")\n \n+    parser.add_argument(\"--cross-generate\", action=\"store_true\", help=\"Cross-generate all combinations of configs\")\n     parser.add_argument(\"--num-tokens-to-profile\", \"-p\", type=int, default=0, help=\"Number of tokens to profile\")\n \n     parser.add_argument(\"--commit-id\", type=str, help=\"Git commit ID (if not provided, will auto-detect from git)\")\n@@ -69,42 +69,47 @@\n \n     # If there is only one (batch_size, sequence_length, num_tokens_to_generate), we benchmark across configs\n     elif len(args.batch_size) * len(args.sequence_length) * len(args.num_tokens_to_generate) == 1:\n-        benchmark_configs = generate_all_configs(\n+        if args.cross_generate:\n+            benchmark_configs = generate_all_configs(\n+                warmup_iterations=args.warmup,\n+                measurement_iterations=args.iterations,\n+                batch_size=args.batch_size[0],\n+                sequence_length=args.sequence_length[0],\n+                num_tokens_to_generate=args.num_tokens_to_generate[0],\n+            )\n+        else:\n+            benchmark_configs = generate_main_configs(\n+                warmup_iterations=args.warmup,\n+                measurement_iterations=args.iterations,\n+                batch_size=args.batch_size[0],\n+                sequence_length=args.sequence_length[0],\n+                num_tokens_to_generate=args.num_tokens_to_generate[0],\n+            )\n+\n+    # Otherwise, we benchmark across all combinations of dimensions\n+    else:\n+        main_config = generate_main_configs(\n             warmup_iterations=args.warmup,\n             measurement_iterations=args.iterations,\n             batch_size=args.batch_size[0],\n             sequence_length=args.sequence_length[0],\n             num_tokens_to_generate=args.num_tokens_to_generate[0],\n-        )\n-        random.shuffle(benchmark_configs)\n-\n-    # Otherwise, we benchmark across all combinations of dimensions\n-    else:\n-        kwargs = {\n-            \"warmup_iterations\": args.warmup,\n-            \"measurement_iterations\": args.iterations,\n-            \"gpu_monitoring\": False,\n-            \"batch_size\": args.batch_size[0],\n-            \"sequence_length\": args.sequence_length[0],\n-            \"num_tokens_to_generate\": args.num_tokens_to_generate[0],\n-            \"attn_implementation\": \"flex_attention\",\n-            \"sdpa_backend\": None,\n-            \"compile_mode\": \"default\",\n-            \"kernelize\": False,\n-        }\n+        )[0]\n         benchmark_configs = []\n         for num_tokens_to_generate in args.num_tokens_to_generate:\n             for sequence_length in args.sequence_length:\n                 for batch_size in args.batch_size:\n-                    kwargs[\"batch_size\"] = batch_size\n-                    kwargs[\"sequence_length\"] = sequence_length\n-                    kwargs[\"num_tokens_to_generate\"] = num_tokens_to_generate\n-                    benchmark_configs.append(BenchmarkConfig(**kwargs))\n+                    cfg_dict = main_config.to_dict()\n+                    cfg_dict[\"batch_size\"] = batch_size\n+                    cfg_dict[\"sequence_length\"] = sequence_length\n+                    cfg_dict[\"num_tokens_to_generate\"] = num_tokens_to_generate\n+                    cfg_dict.pop(\"name\")\n+                    benchmark_configs.append(BenchmarkConfig.from_dict(cfg_dict))\n \n     runner = BenchmarkRunner(logger, args.output_dir, args.commit_id)\n     results = runner.run_benchmarks(\n         args.model_id,\n-        benchmark_configs[:3],\n+        benchmark_configs,\n         args.num_tokens_to_profile,\n         pretty_print_summary=True,\n     )"
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T21:17:18.771857"
  },
  {
    "pr_number": 41647,
    "title": "[Fix] Deepseek V3 expert bias routing",
    "body": "# What does this PR do?\r\n\r\nBy chance we noticed that https://github.com/huggingface/transformers/pull/40132 seems to have introduced a bug in the Deepseek V3 routing implementation: The Deepseek-V3 technical report explicitly states\r\n> Note that the bias term is only used for routing. The gating value, which will be multiplied with\r\nthe FFN output, is still derived from the original affinity score\r\n\r\nThis was the case in transformers until https://github.com/huggingface/transformers/pull/40132 which changed the routing code such that the gating values are now derived from the tensor with the added bias term. I wrote a quick fix for the Deepseek-V3 model in this PR, not sure if other models are also affected. Can you please have a look @ArthurZucker and confirm that this is indeed a bug?\r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker @Cyrilvallez\r\n- vision models: @yonigozlan @molbap\r\n- audio models: @eustlb @ebezzam @vasqu\r\n- multimodal models: @zucchini-nlp\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- generate: @zucchini-nlp (visual-language models) or @gante (all others)\r\n- continuous batching: @remi-or @ArthurZucker @McPatate\r\n- pipelines: @Rocketknight1\r\n- tokenizers: @ArthurZucker and @itazap\r\n- trainer: @SunMarc\r\n- attention: @vasqu @ArthurZucker @CyrilVallez\r\n- model loading (from pretrained, etc): @CyrilVallez\r\n- distributed: @3outeille @ArthurZucker\r\n- CIs: @ydshieh\r\n\r\nIntegrations:\r\n\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization: @SunMarc @MekkCyber\r\n- kernels: @MekkCyber @drbh\r\n- peft: @BenjaminBossan @githubnemo\r\n\r\nDevices/Backends:\r\n\r\n- AMD ROCm: @ivarflakstad\r\n- Intel XPU: @IlyasMoutawwakil\r\n- Ascend NPU: @ivarflakstad \r\n\r\nDocumentation: @stevhliu\r\n\r\nResearch projects are not maintained and should be taken as is.\r\n\r\n -->\r\n",
    "html_url": "https://github.com/huggingface/transformers/pull/41647",
    "created_at": "2025-10-16T08:00:13Z",
    "merged_at": "2025-10-16T14:04:48Z",
    "merge_commit_sha": "8725ce10edb29771fb9a1aa108e6a04859efe973",
    "base_ref": "main",
    "head_sha": "c57e8b38dbb408c7ea96b3d1990a0e18654c810f",
    "user": "fjosw",
    "files": [
      {
        "filename": "src/transformers/models/deepseek_v3/modeling_deepseek_v3.py",
        "status": "modified",
        "additions": 5,
        "deletions": 3,
        "changes": 8,
        "patch": "@@ -176,9 +176,11 @@ def __init__(self, config):\n \n     def route_tokens_to_experts(self, router_logits):\n         router_logits = router_logits.sigmoid()\n-        router_logits = router_logits + self.gate.e_score_correction_bias\n+        router_logits_for_choice = router_logits + self.gate.e_score_correction_bias\n         group_scores = (\n-            router_logits.view(-1, self.n_group, self.n_routed_experts // self.n_group).topk(2, dim=-1)[0].sum(dim=-1)\n+            router_logits_for_choice.view(-1, self.n_group, self.n_routed_experts // self.n_group)\n+            .topk(2, dim=-1)[0]\n+            .sum(dim=-1)\n         )\n         group_idx = torch.topk(group_scores, k=self.topk_group, dim=-1, sorted=False)[1]\n         group_mask = torch.zeros_like(group_scores)\n@@ -188,7 +190,7 @@ def route_tokens_to_experts(self, router_logits):\n             .expand(-1, self.n_group, self.n_routed_experts // self.n_group)\n             .reshape(-1, self.n_routed_experts)\n         )\n-        scores_for_choice = router_logits.masked_fill(~score_mask.bool(), 0.0)\n+        scores_for_choice = router_logits_for_choice.masked_fill(~score_mask.bool(), 0.0)\n         topk_indices = torch.topk(scores_for_choice, k=self.top_k, dim=-1, sorted=False)[1]\n         topk_weights = router_logits.gather(1, topk_indices)\n         if self.norm_topk_prob:"
      },
      {
        "filename": "src/transformers/models/deepseek_v3/modular_deepseek_v3.py",
        "status": "modified",
        "additions": 5,
        "deletions": 3,
        "changes": 8,
        "patch": "@@ -132,9 +132,11 @@ def __init__(self, config):\n \n     def route_tokens_to_experts(self, router_logits):\n         router_logits = router_logits.sigmoid()\n-        router_logits = router_logits + self.gate.e_score_correction_bias\n+        router_logits_for_choice = router_logits + self.gate.e_score_correction_bias\n         group_scores = (\n-            router_logits.view(-1, self.n_group, self.n_routed_experts // self.n_group).topk(2, dim=-1)[0].sum(dim=-1)\n+            router_logits_for_choice.view(-1, self.n_group, self.n_routed_experts // self.n_group)\n+            .topk(2, dim=-1)[0]\n+            .sum(dim=-1)\n         )\n         group_idx = torch.topk(group_scores, k=self.topk_group, dim=-1, sorted=False)[1]\n         group_mask = torch.zeros_like(group_scores)\n@@ -144,7 +146,7 @@ def route_tokens_to_experts(self, router_logits):\n             .expand(-1, self.n_group, self.n_routed_experts // self.n_group)\n             .reshape(-1, self.n_routed_experts)\n         )\n-        scores_for_choice = router_logits.masked_fill(~score_mask.bool(), 0.0)\n+        scores_for_choice = router_logits_for_choice.masked_fill(~score_mask.bool(), 0.0)\n         topk_indices = torch.topk(scores_for_choice, k=self.top_k, dim=-1, sorted=False)[1]\n         topk_weights = router_logits.gather(1, topk_indices)\n         if self.norm_topk_prob:"
      },
      {
        "filename": "src/transformers/models/glm4_moe/modeling_glm4_moe.py",
        "status": "modified",
        "additions": 5,
        "deletions": 3,
        "changes": 8,
        "patch": "@@ -319,9 +319,11 @@ def __init__(self, config):\n \n     def route_tokens_to_experts(self, router_logits):\n         router_logits = router_logits.sigmoid()\n-        router_logits = router_logits + self.gate.e_score_correction_bias\n+        router_logits_for_choice = router_logits + self.gate.e_score_correction_bias\n         group_scores = (\n-            router_logits.view(-1, self.n_group, self.n_routed_experts // self.n_group).topk(2, dim=-1)[0].sum(dim=-1)\n+            router_logits_for_choice.view(-1, self.n_group, self.n_routed_experts // self.n_group)\n+            .topk(2, dim=-1)[0]\n+            .sum(dim=-1)\n         )\n         group_idx = torch.topk(group_scores, k=self.topk_group, dim=-1, sorted=False)[1]\n         group_mask = torch.zeros_like(group_scores)\n@@ -331,7 +333,7 @@ def route_tokens_to_experts(self, router_logits):\n             .expand(-1, self.n_group, self.n_routed_experts // self.n_group)\n             .reshape(-1, self.n_routed_experts)\n         )\n-        scores_for_choice = router_logits.masked_fill(~score_mask.bool(), 0.0)\n+        scores_for_choice = router_logits_for_choice.masked_fill(~score_mask.bool(), 0.0)\n         topk_indices = torch.topk(scores_for_choice, k=self.top_k, dim=-1, sorted=False)[1]\n         topk_weights = router_logits.gather(1, topk_indices)\n         if self.norm_topk_prob:"
      },
      {
        "filename": "src/transformers/models/glm4v_moe/modeling_glm4v_moe.py",
        "status": "modified",
        "additions": 5,
        "deletions": 3,
        "changes": 8,
        "patch": "@@ -314,9 +314,11 @@ def __init__(self, config: Glm4vMoeTextConfig):\n \n     def route_tokens_to_experts(self, router_logits):\n         router_logits = router_logits.sigmoid()\n-        router_logits = router_logits + self.gate.e_score_correction_bias\n+        router_logits_for_choice = router_logits + self.gate.e_score_correction_bias\n         group_scores = (\n-            router_logits.view(-1, self.n_group, self.n_routed_experts // self.n_group).topk(2, dim=-1)[0].sum(dim=-1)\n+            router_logits_for_choice.view(-1, self.n_group, self.n_routed_experts // self.n_group)\n+            .topk(2, dim=-1)[0]\n+            .sum(dim=-1)\n         )\n         group_idx = torch.topk(group_scores, k=self.topk_group, dim=-1, sorted=False)[1]\n         group_mask = torch.zeros_like(group_scores)\n@@ -326,7 +328,7 @@ def route_tokens_to_experts(self, router_logits):\n             .expand(-1, self.n_group, self.n_routed_experts // self.n_group)\n             .reshape(-1, self.n_routed_experts)\n         )\n-        scores_for_choice = router_logits.masked_fill(~score_mask.bool(), 0.0)\n+        scores_for_choice = router_logits_for_choice.masked_fill(~score_mask.bool(), 0.0)\n         topk_indices = torch.topk(scores_for_choice, k=self.top_k, dim=-1, sorted=False)[1]\n         topk_weights = router_logits.gather(1, topk_indices)\n         if self.norm_topk_prob:"
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T21:17:21.679826"
  },
  {
    "pr_number": 41635,
    "title": "Enforce check_auto_docstring",
    "body": "# What does this PR do?\r\n\r\nFix some small issues with auto_docstring and raise an error instead of just warning if something is wrong when running check_auto_docstring",
    "html_url": "https://github.com/huggingface/transformers/pull/41635",
    "created_at": "2025-10-15T18:07:22Z",
    "merged_at": "2025-11-11T16:05:55Z",
    "merge_commit_sha": "df45a92cea0385970fabb21f8c329150cc3d5a9c",
    "base_ref": "main",
    "head_sha": "f2ed1da694d955a14f66d6ab7bcc16d09cdb9d4e",
    "user": "yonigozlan",
    "files": [
      {
        "filename": "src/transformers/models/deprecated/gptsan_japanese/modeling_gptsan_japanese.py",
        "status": "modified",
        "additions": 1,
        "deletions": 2,
        "changes": 3,
        "patch": "@@ -22,7 +22,7 @@\n from ....cache_utils import Cache\n from ....modeling_outputs import MoECausalLMOutputWithPast, MoEModelOutputWithPastAndCrossAttentions\n from ....modeling_utils import PreTrainedModel\n-from ....utils import DUMMY_INPUTS, DUMMY_MASK, auto_docstring\n+from ....utils import DUMMY_INPUTS, DUMMY_MASK\n from .configuration_gptsan_japanese import GPTSanJapaneseConfig\n \n \n@@ -635,7 +635,6 @@ def __init__(self, config: GPTSanJapaneseConfig):\n     def set_input_embeddings(self, new_embeddings):\n         self.embed_tokens = new_embeddings\n \n-    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,"
      },
      {
        "filename": "src/transformers/models/nougat/tokenization_nougat_fast.py",
        "status": "modified",
        "additions": 0,
        "deletions": 13,
        "changes": 13,
        "patch": "@@ -23,9 +23,7 @@\n \n import numpy as np\n \n-from transformers.tokenization_utils_base import INIT_TOKENIZER_DOCSTRING\n from transformers.tokenization_utils_fast import PreTrainedTokenizerFast\n-from transformers.utils import add_end_docstrings\n \n from ...utils import is_levenshtein_available, is_nltk_available, logging, requires_backends\n \n@@ -40,16 +38,6 @@\n logger = logging.get_logger(__name__)\n \n \n-INIT_TOKENIZER_DOCSTRING += \"\"\"\n-        tokenizer_object ([`tokenizers.Tokenizer`]):\n-            A [`tokenizers.Tokenizer`] object from \ud83e\udd17 tokenizers to instantiate from. See [Using tokenizers from \ud83e\udd17\n-            tokenizers](../fast_tokenizers) for more information.\n-        tokenizer_file ([`str`]):\n-            A path to a local JSON file representing a previously serialized [`tokenizers.Tokenizer`] object from \ud83e\udd17\n-            tokenizers.\n-\"\"\"\n-\n-\n VOCAB_FILES_NAMES = {\"tokenizer_file\": \"tokenizer.json\"}\n \n \n@@ -358,7 +346,6 @@ def remove_slice_from_lines(lines, clean_text, slice) -> str:\n     return to_delete.strip()\n \n \n-@add_end_docstrings(INIT_TOKENIZER_DOCSTRING)\n class NougatTokenizerFast(PreTrainedTokenizerFast):\n     \"\"\"\n     Fast tokenizer for Nougat (backed by HuggingFace tokenizers library)."
      },
      {
        "filename": "src/transformers/utils/auto_docstring.py",
        "status": "modified",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "patch": "@@ -1729,9 +1729,9 @@ def auto_method_docstring(\n     model_name_lowercase, class_name, config_class = _get_model_info(func, parent_class)\n     func_documentation = func.__doc__\n     if custom_args is not None and func_documentation is not None:\n-        func_documentation = set_min_indent(custom_args, indent_level + 4) + \"\\n\" + func_documentation\n+        func_documentation = \"\\n\" + set_min_indent(custom_args.strip(\"\\n\"), 0) + \"\\n\" + func_documentation\n     elif custom_args is not None:\n-        func_documentation = custom_args\n+        func_documentation = \"\\n\" + set_min_indent(custom_args.strip(\"\\n\"), 0)\n \n     # Add intro to the docstring before args description if needed\n     if custom_intro is not None:"
      },
      {
        "filename": "utils/check_docstrings.py",
        "status": "modified",
        "additions": 4,
        "deletions": 0,
        "changes": 4,
        "patch": "@@ -1378,6 +1378,10 @@ def check_auto_docstrings(overwrite: bool = False, check_all: bool = False):\n             print(f\"[ERROR] Docstring needs to be filled for the following arguments in {candidate_file}:\")\n             for warning in fill_docstring_args_warnings:\n                 print(warning)\n+        if missing_docstring_args_warnings or docstring_args_ro_remove_warnings or fill_docstring_args_warnings:\n+            raise ValueError(\n+                \"There was at least one problem when checking docstrings of objects decorated with @auto_docstring.\"\n+            )\n \n \n def check_docstrings(overwrite: bool = False, check_all: bool = False):"
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T21:17:24.183638"
  },
  {
    "pr_number": 41627,
    "title": "[`Executorch`] Simplify for encoder models",
    "body": "Now that we include a fast path using no vmapping, we can revert the extra treatment. Followup to #41586",
    "html_url": "https://github.com/huggingface/transformers/pull/41627",
    "created_at": "2025-10-15T15:49:13Z",
    "merged_at": "2025-10-16T11:57:52Z",
    "merge_commit_sha": "44539827d55254546dff5249d976419c798d2f63",
    "base_ref": "main",
    "head_sha": "a621bc1f50b5a35d9a47f50635d78f0ba0f07cbc",
    "user": "vasqu",
    "files": [
      {
        "filename": "src/transformers/integrations/executorch.py",
        "status": "modified",
        "additions": 0,
        "deletions": 144,
        "changes": 144,
        "patch": "@@ -26,7 +26,6 @@\n from ..generation.configuration_utils import GenerationConfig\n from ..masking_utils import (\n     ALL_MASK_ATTENTION_FUNCTIONS,\n-    _ignore_bidirectional_mask_sdpa,\n     _ignore_causal_mask_sdpa,\n     _is_torch_greater_or_equal_than_2_5,\n     prepare_padding_mask,\n@@ -193,101 +192,6 @@ def generate(\n         pass\n \n \n-class TorchExportableModuleForEncoderOnlyLM(torch.nn.Module):\n-    \"\"\"\n-    A recipe module designed to make a `PreTrainedModel` exportable with `torch.export`,\n-    specifically for encoder-only LM. This module ensures that the exported model is compatible\n-    with further lowering and execution in `ExecuTorch`.\n-    \"\"\"\n-\n-    def __init__(self, model: PreTrainedModel) -> None:\n-        \"\"\"\n-        Initializes the exportable module.\n-\n-        Args:\n-            model (`PreTrainedModel`): The pretrained model to wrap.\n-        \"\"\"\n-        super().__init__()\n-\n-        self.model = model\n-        # This is the same as sdpa, but mask creation does not use `vmap` which is not exportable\n-        ALL_MASK_ATTENTION_FUNCTIONS.register(\n-            \"sdpa_bidirectional_mask_without_vmap\", sdpa_bidirectional_mask_without_vmap\n-        )\n-        ALL_ATTENTION_FUNCTIONS.register(\"sdpa_bidirectional_mask_without_vmap\", ALL_ATTENTION_FUNCTIONS[\"sdpa\"])\n-        self.model.config._attn_implementation = \"sdpa_bidirectional_mask_without_vmap\"\n-\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.Tensor] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-    ) -> torch.Tensor:\n-        \"\"\"\n-        Forward pass of the module, which is compatible with the ExecuTorch llm runner.\n-\n-        Args:\n-            input_ids (`torch.Tensor`): Tensor representing current input token id to the module.\n-            inputs_embeds (`torch.Tensor`): Tensor representing current input embeddings to the module.\n-            cache_position (`torch.Tensor`): Tensor representing current input position in the cache.\n-\n-        Returns:\n-            torch.Tensor: Logits output from the model.\n-        \"\"\"\n-        return self.model.forward(\n-            input_ids=input_ids,\n-            inputs_embeds=inputs_embeds,\n-            attention_mask=attention_mask,\n-        )\n-\n-    def export(\n-        self,\n-        input_ids: Optional[torch.Tensor] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        strict: Optional[bool] = None,\n-    ) -> torch.export.ExportedProgram:\n-        \"\"\"\n-        Export the wrapped module using `torch.export`.\n-\n-        Args:\n-            input_ids (`Optional[torch.Tensor]`):\n-                Tensor representing current input token id to the module. Must specify either this or inputs_embeds.\n-            inputs_embeds (`Optional[torch.Tensor]`):\n-                Tensor representing current input embeddings to the module. Must specify either this or input_ids.\n-            strict(`Optional[bool]`):\n-                Flag to instruct `torch.export` to use `torchdynamo`.\n-\n-        Returns:\n-            torch.export.ExportedProgram: The exported program that can be used for inference.\n-\n-        \"\"\"\n-        if not (input_ids is None) ^ (inputs_embeds is None):\n-            raise ValueError(\"Need to specify either input_ids or inputs_embeds.\")\n-\n-        if input_ids is not None:\n-            input_kwargs = {\n-                \"input_ids\": input_ids,\n-                \"attention_mask\": attention_mask if attention_mask is not None else torch.ones_like(input_ids),\n-            }\n-        else:\n-            input_kwargs = {\n-                \"inputs_embeds\": inputs_embeds,\n-                \"attention_mask\": attention_mask\n-                if attention_mask is not None\n-                else torch.ones_like(inputs_embeds)[..., 0],\n-            }\n-\n-        exported_program = torch.export.export(\n-            self.model,\n-            args=(),\n-            kwargs=input_kwargs,\n-            strict=strict if strict is not None else True,\n-        )\n-\n-        return exported_program\n-\n-\n class TorchExportableModuleForDecoderOnlyLM(torch.nn.Module):\n     \"\"\"\n     A recipe module designed to make a `PreTrainedModel` exportable with `torch.export`,\n@@ -1296,51 +1200,3 @@ def sdpa_mask_without_vmap(\n     if not _is_torch_greater_or_equal_than_2_5 and allow_torch_fix:\n         causal_mask |= torch.all(~causal_mask, dim=-1, keepdim=True)\n     return causal_mask\n-\n-\n-def sdpa_bidirectional_mask_without_vmap(\n-    kv_length: int,\n-    kv_offset: int = 0,\n-    attention_mask: Optional[torch.Tensor] = None,\n-    allow_torch_fix: bool = True,\n-    allow_is_bidirectional_skip: bool = True,\n-    **kwargs,\n-) -> Optional[torch.Tensor]:\n-    \"\"\"\n-    Create a 4D boolean mask of shape `(batch_size, 1, query_length, kv_length)` where a value of True indicates that\n-    the element should take part in the attention computation, and False that it should not.\n-\n-    This is similar to `masking_utils.sdpa_mask` but does not use `vmap` which is incompatible with export.\n-    Additionally, surrounding logic for causal masks is omitted for simplicity.\n-\n-    Args:\n-        kv_length (`int`):\n-            The size that the key and value states will have during the attention computation.\n-        kv_offset (`int`, optional):\n-            An optional offset to indicate at which first position the key and values states will refer to.\n-        attention_mask (`torch.Tensor`, optional):\n-            The 2D attention mask corresponding to padded tokens of shape (batch_size, number_of_seen_tokens+q_length)\n-        allow_torch_fix (`bool`, optional):\n-            Whether to update the mask in case a query is not attending to any tokens, to solve a bug in torch's older\n-            versions. We need an arg to skip it when using eager. By default `True`.\n-        allow_is_bidirectional_skip (`bool`, optional):\n-            Whether to allow to return `None` for the mask under conditions where we do not have to add any bias,\n-            i.e. full attention without any padding. Default to `True`.\n-    \"\"\"\n-    # Potentially pad the 2D mask, and slice it correctly\n-    padding_mask = prepare_padding_mask(attention_mask, kv_length, kv_offset, _slice=False)\n-\n-    # Under specific conditions, we can avoid materializing the mask\n-    if allow_is_bidirectional_skip and _ignore_bidirectional_mask_sdpa(padding_mask):\n-        return None\n-\n-    bidirectional_mask = None\n-    if padding_mask is not None:\n-        bidirectional_mask = padding_mask[:, None, None, :]\n-\n-    # Due to a bug in some older torch version, we need to update the mask in case a query is not attending to any\n-    # tokens (due to padding). See details in https://github.com/pytorch/pytorch/issues/110213\n-    if not _is_torch_greater_or_equal_than_2_5 and allow_torch_fix and bidirectional_mask is not None:\n-        bidirectional_mask |= torch.all(~bidirectional_mask, dim=-1, keepdim=True)\n-\n-    return bidirectional_mask"
      },
      {
        "filename": "tests/models/albert/test_modeling_albert.py",
        "status": "modified",
        "additions": 5,
        "deletions": 9,
        "changes": 14,
        "patch": "@@ -337,8 +337,6 @@ def test_export(self):\n         if version.parse(torch.__version__) < version.parse(\"2.4.0\"):\n             self.skipTest(reason=\"This test requires torch >= 2.4 to run.\")\n \n-        from transformers.integrations.executorch import TorchExportableModuleForEncoderOnlyLM\n-\n         distilbert_model = \"albert/albert-base-v2\"\n         device = \"cpu\"\n         attn_implementation = \"sdpa\"\n@@ -365,15 +363,13 @@ def test_export(self):\n             [\"capital\", \"capitol\", \"comune\", \"arrondissement\", \"bastille\"],\n         )\n \n-        exportable_module = TorchExportableModuleForEncoderOnlyLM(model)\n-        exported_program = exportable_module.export(\n-            input_ids=inputs[\"input_ids\"],\n-            attention_mask=inputs[\"attention_mask\"],\n+        exported_program = torch.export.export(\n+            model,\n+            args=(inputs[\"input_ids\"],),\n+            kwargs={\"attention_mask\": inputs[\"attention_mask\"]},\n             strict=True,\n         )\n \n-        result = exported_program.module().forward(\n-            input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"]\n-        )\n+        result = exported_program.module().forward(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n         ep_predicted_mask = tokenizer.decode(result.logits[0, 4].topk(5).indices)\n         self.assertEqual(eg_predicted_mask, ep_predicted_mask)"
      },
      {
        "filename": "tests/models/bert/test_modeling_bert.py",
        "status": "modified",
        "additions": 5,
        "deletions": 9,
        "changes": 14,
        "patch": "@@ -709,8 +709,6 @@ def test_export(self):\n         if version.parse(torch.__version__) < version.parse(\"2.4.0\"):\n             self.skipTest(reason=\"This test requires torch >= 2.4 to run.\")\n \n-        from transformers.integrations.executorch import TorchExportableModuleForEncoderOnlyLM\n-\n         bert_model = \"google-bert/bert-base-uncased\"\n         device = \"cpu\"\n         attn_implementation = \"sdpa\"\n@@ -735,15 +733,13 @@ def test_export(self):\n         eg_predicted_mask = tokenizer.decode(logits[0, 6].topk(5).indices)\n         self.assertEqual(eg_predicted_mask.split(), [\"carpenter\", \"waiter\", \"barber\", \"mechanic\", \"salesman\"])\n \n-        exportable_module = TorchExportableModuleForEncoderOnlyLM(model)\n-        exported_program = exportable_module.export(\n-            input_ids=inputs[\"input_ids\"],\n-            attention_mask=inputs[\"attention_mask\"],\n+        exported_program = torch.export.export(\n+            model,\n+            args=(inputs[\"input_ids\"],),\n+            kwargs={\"attention_mask\": inputs[\"attention_mask\"]},\n             strict=True,\n         )\n \n-        result = exported_program.module().forward(\n-            input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"]\n-        )\n+        result = exported_program.module().forward(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n         ep_predicted_mask = tokenizer.decode(result.logits[0, 6].topk(5).indices)\n         self.assertEqual(eg_predicted_mask, ep_predicted_mask)"
      },
      {
        "filename": "tests/models/distilbert/test_modeling_distilbert.py",
        "status": "modified",
        "additions": 5,
        "deletions": 9,
        "changes": 14,
        "patch": "@@ -404,8 +404,6 @@ def test_export(self):\n         if not is_torch_greater_or_equal_than_2_4:\n             self.skipTest(reason=\"This test requires torch >= 2.4 to run.\")\n \n-        from transformers.integrations.executorch import TorchExportableModuleForEncoderOnlyLM\n-\n         distilbert_model = \"distilbert-base-uncased\"\n         device = \"cpu\"\n         attn_implementation = \"sdpa\"\n@@ -432,15 +430,13 @@ def test_export(self):\n             [\"capital\", \"birthplace\", \"northernmost\", \"centre\", \"southernmost\"],\n         )\n \n-        exportable_module = TorchExportableModuleForEncoderOnlyLM(model)\n-        exported_program = exportable_module.export(\n-            input_ids=inputs[\"input_ids\"],\n-            attention_mask=inputs[\"attention_mask\"],\n+        exported_program = torch.export.export(\n+            model,\n+            args=(inputs[\"input_ids\"],),\n+            kwargs={\"attention_mask\": inputs[\"attention_mask\"]},\n             strict=True,\n         )\n \n-        result = exported_program.module().forward(\n-            input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"]\n-        )\n+        result = exported_program.module().forward(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n         exported_predicted_mask = tokenizer.decode(result.logits[0, 4].topk(5).indices)\n         self.assertEqual(eager_predicted_mask, exported_predicted_mask)"
      },
      {
        "filename": "tests/models/mobilebert/test_modeling_mobilebert.py",
        "status": "modified",
        "additions": 5,
        "deletions": 9,
        "changes": 14,
        "patch": "@@ -395,8 +395,6 @@ def test_export(self):\n         if version.parse(torch.__version__) < version.parse(\"2.4.0\"):\n             self.skipTest(reason=\"This test requires torch >= 2.4 to run.\")\n \n-        from transformers.integrations.executorch import TorchExportableModuleForEncoderOnlyLM\n-\n         mobilebert_model = \"google/mobilebert-uncased\"\n         device = \"cpu\"\n         attn_implementation = \"eager\"\n@@ -420,15 +418,13 @@ def test_export(self):\n         eg_predicted_mask = tokenizer.decode(logits[0, 6].topk(5).indices)\n         self.assertEqual(eg_predicted_mask.split(), [\"carpenter\", \"waiter\", \"mechanic\", \"teacher\", \"clerk\"])\n \n-        exportable_module = TorchExportableModuleForEncoderOnlyLM(model)\n-        exported_program = exportable_module.export(\n-            input_ids=inputs[\"input_ids\"],\n-            attention_mask=inputs[\"attention_mask\"],\n+        exported_program = torch.export.export(\n+            model,\n+            args=(inputs[\"input_ids\"],),\n+            kwargs={\"attention_mask\": inputs[\"attention_mask\"]},\n             strict=True,\n         )\n \n-        result = exported_program.module().forward(\n-            input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"]\n-        )\n+        result = exported_program.module().forward(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n         ep_predicted_mask = tokenizer.decode(result.logits[0, 6].topk(5).indices)\n         self.assertEqual(eg_predicted_mask, ep_predicted_mask)"
      },
      {
        "filename": "tests/models/roberta/test_modeling_roberta.py",
        "status": "modified",
        "additions": 5,
        "deletions": 9,
        "changes": 14,
        "patch": "@@ -691,8 +691,6 @@ def test_export(self):\n         if not is_torch_greater_or_equal_than_2_4:\n             self.skipTest(reason=\"This test requires torch >= 2.4 to run.\")\n \n-        from transformers.integrations.executorch import TorchExportableModuleForEncoderOnlyLM\n-\n         roberta_model = \"FacebookAI/roberta-base\"\n         device = \"cpu\"\n         attn_implementation = \"sdpa\"\n@@ -717,15 +715,13 @@ def test_export(self):\n         eager_predicted_mask = tokenizer.decode(logits[0, 6].topk(5).indices)\n         self.assertEqual(eager_predicted_mask.split(), [\"happiness\", \"love\", \"peace\", \"freedom\", \"simplicity\"])\n \n-        exportable_module = TorchExportableModuleForEncoderOnlyLM(model)\n-        exported_program = exportable_module.export(\n-            input_ids=inputs[\"input_ids\"],\n-            attention_mask=inputs[\"attention_mask\"],\n+        exported_program = torch.export.export(\n+            model,\n+            args=(inputs[\"input_ids\"],),\n+            kwargs={\"attention_mask\": inputs[\"attention_mask\"]},\n             strict=True,\n         )\n \n-        result = exported_program.module().forward(\n-            input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"]\n-        )\n+        result = exported_program.module().forward(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n         exported_predicted_mask = tokenizer.decode(result.logits[0, 6].topk(5).indices)\n         self.assertEqual(eager_predicted_mask, exported_predicted_mask)"
      }
    ],
    "num_files": 6,
    "scraped_at": "2025-11-16T21:17:25.221784"
  },
  {
    "pr_number": 41626,
    "title": "[v5] Return a BatchEncoding dict from apply_chat_template by default",
    "body": "Tokenizers return a BatchEncoding dict by default, but `apply_chat_template` doesn't. This is just an accident of how I wrote it originally, which we were stuck with for backward compatibility reasons. Ideally, I think `apply_chat_template` should return exactly the same format as tokenizers, since it also performs tokenization most of the time. It's now `v5` time, so we can start making that happen :sweat_smile:\r\n\r\nThis PR also updates tests, and removes very old `test_tokenization_for_chat` tests. These model-specific tests don't do anything useful anymore, since the `apply_chat_template` functionality is unified across tokenizers; they're mostly a legacy leftover from when model classes used to need custom chat tokenization functions.",
    "html_url": "https://github.com/huggingface/transformers/pull/41626",
    "created_at": "2025-10-15T15:00:46Z",
    "merged_at": "2025-10-31T13:50:26Z",
    "merge_commit_sha": "5f8d02f2f12e771d59d473702bc61a7e7c4a6255",
    "base_ref": "main",
    "head_sha": "137015ec595cd3627825ea0e84201ad3b3c5a153",
    "user": "Rocketknight1",
    "files": [
      {
        "filename": "src/transformers/models/voxtral/processing_voxtral.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -206,7 +206,7 @@ def apply_chat_template(\n         tokenizer_kwargs = {**processed_kwargs[\"template_kwargs\"], **text_kwargs}\n         tokenizer_kwargs[\"return_tensors\"] = None  # let's not return tensors here\n         tokenize = tokenizer_kwargs.pop(\"tokenize\", False)\n-        return_dict = tokenizer_kwargs.pop(\"return_dict\", False)\n+        return_dict = tokenizer_kwargs.pop(\"return_dict\", True)\n \n         encoded_instruct_inputs = self.tokenizer.apply_chat_template(\n             conversations,"
      },
      {
        "filename": "src/transformers/processing_utils.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -1603,7 +1603,7 @@ def apply_chat_template(\n             conversations = [conversation]\n \n         tokenize = processed_kwargs[\"template_kwargs\"].pop(\"tokenize\", False)\n-        return_dict = processed_kwargs[\"template_kwargs\"].pop(\"return_dict\", False)\n+        return_dict = processed_kwargs[\"template_kwargs\"].pop(\"return_dict\", True)\n         mm_load_kwargs = processed_kwargs[\"mm_load_kwargs\"]\n \n         if tokenize:"
      },
      {
        "filename": "src/transformers/tokenization_mistral_common.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -1378,7 +1378,7 @@ def apply_chat_template(\n         truncation: bool = False,\n         max_length: Optional[int] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n-        return_dict: bool = False,\n+        return_dict: bool = True,\n         **kwargs,\n     ) -> Union[str, list[int], list[str], list[list[int]], BatchEncoding]:\n         \"\"\""
      },
      {
        "filename": "src/transformers/tokenization_utils_base.py",
        "status": "modified",
        "additions": 12,
        "deletions": 11,
        "changes": 23,
        "patch": "@@ -1588,7 +1588,7 @@ def apply_chat_template(\n         truncation: bool = False,\n         max_length: Optional[int] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n-        return_dict: bool = False,\n+        return_dict: bool = True,\n         return_assistant_tokens_mask: bool = False,\n         tokenizer_kwargs: Optional[dict[str, Any]] = None,\n         **kwargs,\n@@ -1661,14 +1661,11 @@ def apply_chat_template(\n             set, will return a dict of tokenizer outputs instead.\n         \"\"\"\n \n-        if return_dict and not tokenize:\n-            raise ValueError(\n-                \"`return_dict=True` is incompatible with `tokenize=False`, because there is no dict \"\n-                \"of tokenizer outputs to return.\"\n-            )\n+        if not tokenize:\n+            return_dict = False  # dicts are only returned by the tokenizer anyway\n \n-        if return_assistant_tokens_mask and not return_dict:\n-            raise ValueError(\"`return_assistant_tokens_mask=True` is incompatible with `return_dict=False`\")\n+        if return_assistant_tokens_mask and not (return_dict and tokenize):\n+            raise ValueError(\"`return_assistant_tokens_mask=True` requires `return_dict=True` and `tokenize=True`\")\n \n         if tokenizer_kwargs is None:\n             tokenizer_kwargs = {}\n@@ -1783,13 +1780,17 @@ def encode_message_with_chat_template(\n             )\n \n         if conversation_history is None or len(conversation_history) == 0:\n-            return self.apply_chat_template([message], add_generation_prompt=False, tokenize=True, **kwargs)\n+            return self.apply_chat_template(\n+                [message], add_generation_prompt=False, tokenize=True, return_dict=False, **kwargs\n+            )\n \n         conversation = conversation_history + [message]\n-        tokens = self.apply_chat_template(conversation, add_generation_prompt=False, tokenize=True, **kwargs)\n+        tokens = self.apply_chat_template(\n+            conversation, add_generation_prompt=False, tokenize=True, return_dict=False, **kwargs\n+        )\n \n         prefix_tokens = self.apply_chat_template(\n-            conversation_history, add_generation_prompt=False, tokenize=True, **kwargs\n+            conversation_history, add_generation_prompt=False, tokenize=True, return_dict=False, **kwargs\n         )\n         # It's possible that the prefix tokens are not a prefix of the full list of tokens.\n         # For example, if the prefix is `<s>User: Hi` and the full conversation is `<s>User: Hi</s><s>Assistant: Hello`."
      },
      {
        "filename": "tests/models/blenderbot/test_tokenization_blenderbot.py",
        "status": "modified",
        "additions": 0,
        "deletions": 22,
        "changes": 22,
        "patch": "@@ -18,7 +18,6 @@\n from functools import cached_property\n \n from transformers import BlenderbotTokenizer, BlenderbotTokenizerFast\n-from transformers.testing_utils import require_jinja\n \n \n class Blenderbot3BTokenizerTests(unittest.TestCase):\n@@ -51,24 +50,3 @@ def test_3B_tokenization_same_as_parlai(self):\n     def test_3B_tokenization_same_as_parlai_rust_tokenizer(self):\n         assert self.rust_tokenizer_3b.add_prefix_space\n         assert self.rust_tokenizer_3b([\" Sam\", \"Sam\"]).input_ids == [[5502, 2], [5502, 2]]\n-\n-    @require_jinja\n-    def test_tokenization_for_chat(self):\n-        tok = self.tokenizer_3b\n-        test_chats = [\n-            [{\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"}, {\"role\": \"user\", \"content\": \"Hello!\"}],\n-            [\n-                {\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"},\n-                {\"role\": \"user\", \"content\": \"Hello!\"},\n-                {\"role\": \"assistant\", \"content\": \"Nice to meet you.\"},\n-            ],\n-            [{\"role\": \"assistant\", \"content\": \"Nice to meet you.\"}, {\"role\": \"user\", \"content\": \"Hello!\"}],\n-        ]\n-        tokenized_chats = [tok.apply_chat_template(test_chat) for test_chat in test_chats]\n-        expected_tokens = [\n-            [553, 366, 265, 4792, 3879, 73, 311, 21, 228, 228, 6950, 8, 2],\n-            [553, 366, 265, 4792, 3879, 73, 311, 21, 228, 228, 6950, 8, 228, 3490, 287, 2273, 304, 21, 2],\n-            [3490, 287, 2273, 304, 21, 228, 228, 6950, 8, 2],\n-        ]\n-        for tokenized_chat, expected_tokens in zip(tokenized_chats, expected_tokens):\n-            self.assertListEqual(tokenized_chat, expected_tokens)"
      },
      {
        "filename": "tests/models/bloom/test_tokenization_bloom.py",
        "status": "modified",
        "additions": 1,
        "deletions": 23,
        "changes": 24,
        "patch": "@@ -18,7 +18,7 @@\n from datasets import load_dataset\n \n from transformers import BloomTokenizerFast\n-from transformers.testing_utils import require_jinja, require_tokenizers\n+from transformers.testing_utils import require_tokenizers\n \n from ...test_tokenization_common import TokenizerTesterMixin\n \n@@ -137,28 +137,6 @@ def test_encodings_from_xnli_dataset(self):\n         predicted_text = [tokenizer.decode(x, clean_up_tokenization_spaces=False) for x in output_tokens]\n         self.assertListEqual(predicted_text, input_text)\n \n-    @require_jinja\n-    def test_tokenization_for_chat(self):\n-        tokenizer = self.get_rust_tokenizer()\n-        tokenizer.chat_template = \"{% for message in messages %}{{ message.content }}{{ eos_token }}{% endfor %}\"\n-        test_chats = [\n-            [{\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"}, {\"role\": \"user\", \"content\": \"Hello!\"}],\n-            [\n-                {\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"},\n-                {\"role\": \"user\", \"content\": \"Hello!\"},\n-                {\"role\": \"assistant\", \"content\": \"Nice to meet you.\"},\n-            ],\n-            [{\"role\": \"assistant\", \"content\": \"Nice to meet you.\"}, {\"role\": \"user\", \"content\": \"Hello!\"}],\n-        ]\n-        tokenized_chats = [tokenizer.apply_chat_template(test_chat) for test_chat in test_chats]\n-        expected_tokens = [\n-            [5448, 1306, 267, 66799, 44799, 37143, 17, 2, 59414, 4, 2],\n-            [5448, 1306, 267, 66799, 44799, 37143, 17, 2, 59414, 4, 2, 229126, 427, 11890, 1152, 17, 2],\n-            [229126, 427, 11890, 1152, 17, 2, 59414, 4, 2],\n-        ]\n-        for tokenized_chat, expected_tokens in zip(tokenized_chats, expected_tokens):\n-            self.assertListEqual(tokenized_chat, expected_tokens)\n-\n     def test_add_prefix_space_fast(self):\n         tokenizer_w_prefix = self.get_rust_tokenizer(add_prefix_space=True)\n         tokenizer_wo_prefix = self.get_rust_tokenizer(add_prefix_space=False)"
      },
      {
        "filename": "tests/models/cohere/test_tokenization_cohere.py",
        "status": "modified",
        "additions": 0,
        "deletions": 26,
        "changes": 26,
        "patch": "@@ -146,32 +146,6 @@ def test_pretrained_model_lists(self):\n         self.assertGreaterEqual(len(self.tokenizer_class.pretrained_vocab_files_map), 1)\n         self.assertGreaterEqual(len(list(self.tokenizer_class.pretrained_vocab_files_map.values())[0]), 1)\n \n-    @require_jinja\n-    def test_tokenization_for_chat(self):\n-        tokenizer = self.get_rust_tokenizer()\n-        test_chats = [\n-            [{\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"}, {\"role\": \"user\", \"content\": \"Hello!\"}],\n-            [\n-                {\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"},\n-                {\"role\": \"user\", \"content\": \"Hello!\"},\n-                {\"role\": \"assistant\", \"content\": \"Nice to meet you.\"},\n-            ],\n-        ]\n-        tokenized_chats = [tokenizer.apply_chat_template(test_chat) for test_chat in test_chats]\n-        # fmt: off\n-        expected_tokens = [\n-            [5, 36, 99, 59, 60, 41, 58, 60, 71, 55, 46, 71, 60, 61, 58, 54, 71, 60, 55, 51, 45, 54, 99, 38, 36, 99, 59, 65, 59, 60, 45, 53, 71, 60, 55, 51, 45, 54, 99, 38, 65, 243, 394, 204, 336, 84, 88, 887, 374, 216, 74, 286, 22, 8, 36, 99, 59, 60, 41, 58, 60, 71, 55, 46, 71, 60, 61, 58, 54, 71, 60, 55, 51, 45, 54, 99, 38, 36, 99, 61, 59, 45, 58, 71, 60, 55, 51, 45, 54, 99, 38, 48, 420, 87, 9, 8],\n-            [5, 36, 99, 59, 60, 41, 58, 60, 71, 55, 46, 71, 60, 61, 58, 54, 71, 60, 55, 51, 45, 54, 99, 38, 36, 99, 59, 65,\n-            59, 60, 45, 53, 71, 60, 55, 51, 45, 54, 99, 38, 65, 243, 394, 204, 336, 84, 88, 887, 374, 216, 74, 286, 22, 8,\n-            36, 99, 59, 60, 41, 58, 60, 71, 55, 46, 71, 60, 61, 58, 54, 71, 60, 55, 51, 45, 54, 99, 38, 36, 99, 61, 59,\n-            45, 58, 71, 60, 55, 51, 45, 54, 99, 38, 48, 420, 87, 9, 8, 36, 99, 59, 60, 41, 58, 60, 71, 55, 46, 71, 60, 61,\n-            58, 54, 71, 60, 55, 51, 45, 54, 99, 38, 36, 99, 43, 48, 41, 60, 42, 55, 60, 71, 60, 55, 51, 45, 54, 99, 38,\n-            54, 567, 235, 693, 276, 411, 243, 22, 8]\n-        ]\n-        # fmt: on\n-        for tokenized_chat, expected_tokens in zip(tokenized_chats, expected_tokens):\n-            self.assertListEqual(tokenized_chat, expected_tokens)\n-\n     @require_jinja\n     def test_tokenization_for_tool_use(self):\n         tokenizer = self.get_rust_tokenizer()"
      },
      {
        "filename": "tests/models/gemma/test_tokenization_gemma.py",
        "status": "modified",
        "additions": 0,
        "deletions": 20,
        "changes": 20,
        "patch": "@@ -27,7 +27,6 @@\n from transformers.testing_utils import (\n     get_tests_dir,\n     nested_simplify,\n-    require_jinja,\n     require_read_token,\n     require_sentencepiece,\n     require_tokenizers,\n@@ -428,25 +427,6 @@ def test_some_edge_cases(self):\n         # a dummy prefix space is not added by the sp_model as it was de-activated\n         self.assertEqual(tokens, tokenizer.sp_model.encode(\"\u2581\u2581\", out_type=str))\n \n-    @require_jinja\n-    def test_tokenization_for_chat(self):\n-        tokenizer = GemmaTokenizer.from_pretrained(\"hf-internal-testing/dummy-gemma\")\n-\n-        test_chats = [\n-            [{\"role\": \"user\", \"content\": \"Hello!\"}],\n-            [\n-                {\"role\": \"user\", \"content\": \"Hello!\"},\n-                {\"role\": \"assistant\", \"content\": \"Nice to meet you.\"},\n-            ],\n-            [{\"role\": \"user\", \"content\": \"Hello!\"}],\n-        ]\n-        # Matt: The third test case tests the default system message, but if this is ever changed in the\n-        #       class/repo code then that test will fail, and the case will need to be updated.\n-        tokenized_chats = [tokenizer.apply_chat_template(test_chat) for test_chat in test_chats]\n-        expected_tokens = [[235322, 235371, 571, 235298, 2997, 73786, 1645, 108, 4521, 149907, 235371, 571, 235298, 615, 73786, 108], [235322, 235371, 571, 235298, 2997, 73786, 1645, 108, 4521, 149907, 235371, 571, 235298, 615, 73786, 108, 235322, 235371, 571, 235298, 2997, 73786, 105776, 108, 7731, 577, 4664, 692, 35606, 235371, 571, 235298, 615, 73786, 108], [235322, 235371, 571, 235298, 2997, 73786, 1645, 108, 4521, 149907, 235371, 571, 235298, 615, 73786, 108]]  # fmt: skip\n-        for tokenized_chat, expected_tokens in zip(tokenized_chats, expected_tokens):\n-            self.assertListEqual(tokenized_chat, expected_tokens)\n-\n     def test_save_fast_load_slow(self):\n         # Ensure that we can save a fast tokenizer and load it as a slow tokenizer\n         slow_tokenizer = self.tokenizer"
      },
      {
        "filename": "tests/models/gpt2/test_tokenization_gpt2.py",
        "status": "modified",
        "additions": 1,
        "deletions": 23,
        "changes": 24,
        "patch": "@@ -19,7 +19,7 @@\n \n from transformers import AutoTokenizer, GPT2Tokenizer, GPT2TokenizerFast\n from transformers.models.gpt2.tokenization_gpt2 import VOCAB_FILES_NAMES\n-from transformers.testing_utils import require_jinja, require_tiktoken, require_tokenizers\n+from transformers.testing_utils import require_tiktoken, require_tokenizers\n \n from ...test_tokenization_common import TokenizerTesterMixin\n \n@@ -281,28 +281,6 @@ def test_special_tokens_mask_input_pairs_and_bos_token(self):\n                 filtered_sequence = [x for x in filtered_sequence if x is not None]\n                 self.assertEqual(encoded_sequence, filtered_sequence)\n \n-    @require_jinja\n-    def test_tokenization_for_chat(self):\n-        tokenizer = GPT2Tokenizer.from_pretrained(self.tmpdirname)\n-        tokenizer.chat_template = \"{% for message in messages %}{{ message.content }}{{ eos_token }}{% endfor %}\"\n-        test_chats = [\n-            [{\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"}, {\"role\": \"user\", \"content\": \"Hello!\"}],\n-            [\n-                {\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"},\n-                {\"role\": \"user\", \"content\": \"Hello!\"},\n-                {\"role\": \"assistant\", \"content\": \"Nice to meet you.\"},\n-            ],\n-            [{\"role\": \"assistant\", \"content\": \"Nice to meet you.\"}, {\"role\": \"user\", \"content\": \"Hello!\"}],\n-        ]\n-        tokenized_chats = [tokenizer.apply_chat_template(test_chat) for test_chat in test_chats]\n-        # fmt: off\n-        expected_tokens = [[20, 1, 20, 10, 20, 4, 3, 10, 20, 10, 20, 3, 0, 20, 20, 20, 0, 10, 20, 20, 20, 6, 20, 1, 6, 20, 20, 20, 3, 0, 0, 1, 20, 20],\n-                          [20, 1, 20, 10, 20, 4, 3, 10, 20, 10, 20, 3, 0, 20, 20, 20, 0, 10, 20, 20, 20, 6, 20, 1, 6, 20, 20, 20, 3, 0, 0, 1, 20, 20, 20, 7, 20, 3, 10, 6, 1, 10, 20, 3, 3, 6, 10, 20, 1, 20, 20, 20],\n-                          [20, 7, 20, 3, 10, 6, 1, 10, 20, 3, 3, 6, 10, 20, 1, 20, 20, 20, 20, 3, 0, 0, 1, 20, 20]]\n-        # fmt: on\n-        for tokenized_chat, expected_tokens in zip(tokenized_chats, expected_tokens):\n-            self.assertListEqual(tokenized_chat, expected_tokens)\n-\n     @require_tiktoken\n     def test_tokenization_tiktoken(self):\n         from tiktoken import encoding_name_for_model"
      },
      {
        "filename": "tests/models/gpt_sw3/test_tokenization_gpt_sw3.py",
        "status": "modified",
        "additions": 1,
        "deletions": 34,
        "changes": 35,
        "patch": "@@ -15,7 +15,7 @@\n import unittest\n \n from transformers import GPTSw3Tokenizer\n-from transformers.testing_utils import get_tests_dir, require_jinja, require_sentencepiece, require_tokenizers, slow\n+from transformers.testing_utils import get_tests_dir, require_sentencepiece, require_tokenizers, slow\n \n from ...test_tokenization_common import TokenizerTesterMixin\n \n@@ -127,36 +127,3 @@ def test_tokenizer_integration(self):\n             model_name=\"AI-Sweden-Models/gpt-sw3-126m\",\n             sequences=sequences,\n         )\n-\n-    @require_jinja\n-    def test_tokenization_for_chat(self):\n-        tokenizer = GPTSw3Tokenizer(SAMPLE_VOCAB)\n-        tokenizer.chat_template = (\n-            \"{{ eos_token }}{{ bos_token }}\"\n-            \"{% for message in messages %}\"\n-            \"{% if message['role'] == 'user' %}{{ 'User: ' + message['content']}}\"\n-            \"{% else %}{{ 'Bot: ' + message['content']}}{% endif %}\"\n-            \"{{ message['text'] }}{{ bos_token }}\"\n-            \"{% endfor %}\"\n-            \"Bot:\"\n-        )\n-        # This is in English, but it's just here to make sure the chat control tokens are being added properly\n-        test_chats = [\n-            [{\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"}, {\"role\": \"user\", \"content\": \"Hello!\"}],\n-            [\n-                {\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"},\n-                {\"role\": \"user\", \"content\": \"Hello!\"},\n-                {\"role\": \"assistant\", \"content\": \"Nice to meet you.\"},\n-            ],\n-            [{\"role\": \"assistant\", \"content\": \"Nice to meet you.\"}, {\"role\": \"user\", \"content\": \"Hello!\"}],\n-        ]\n-        tokenized_chats = [tokenizer.apply_chat_template(test_chat) for test_chat in test_chats]\n-        # fmt: off\n-        expected_tokens = [\n-            [2000, 1, 575, 541, 419, 530, 339, 265, 878, 708, 727, 275, 347, 541, 260, 1, 968, 263, 314, 419, 366, 354, 294, 360, 1, 575, 541, 419],\n-            [2000, 1, 575, 541, 419, 530, 339, 265, 878, 708, 727, 275, 347, 541, 260, 1, 968, 263, 314, 419, 366, 354, 294, 360, 1, 575, 541, 419, 984, 429, 281, 264, 1261, 291, 260, 1, 575, 541, 419],\n-            [2000, 1, 575, 541, 419, 984, 429, 281, 264, 1261, 291, 260, 1, 968, 263, 314, 419, 366, 354, 294, 360, 1, 575, 541, 419]\n-            ]\n-        # fmt: on\n-        for tokenized_chat, expected_tokens in zip(tokenized_chats, expected_tokens):\n-            self.assertListEqual(tokenized_chat, expected_tokens)"
      },
      {
        "filename": "tests/models/llama/test_tokenization_llama.py",
        "status": "modified",
        "additions": 0,
        "deletions": 27,
        "changes": 27,
        "patch": "@@ -32,7 +32,6 @@\n from transformers.testing_utils import (\n     get_tests_dir,\n     nested_simplify,\n-    require_jinja,\n     require_read_token,\n     require_sentencepiece,\n     require_tiktoken,\n@@ -702,32 +701,6 @@ def test_fast_post_processor(self):\n         with self.assertRaises(ValueError):\n             tokenizer = LlamaTokenizerFast(SAMPLE_VOCAB, eos_token=None, add_bos_token=True, add_eos_token=True)\n \n-    @require_jinja\n-    def test_tokenization_for_chat(self):\n-        tokenizer = LlamaTokenizer.from_pretrained(\"huggyllama/llama-7b\", legacy=False)\n-\n-        test_chats = [\n-            [{\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"}, {\"role\": \"user\", \"content\": \"Hello!\"}],\n-            [\n-                {\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"},\n-                {\"role\": \"user\", \"content\": \"Hello!\"},\n-                {\"role\": \"assistant\", \"content\": \"Nice to meet you.\"},\n-            ],\n-            [{\"role\": \"user\", \"content\": \"Hello!\"}],\n-        ]\n-        # Matt: The third test case tests the default system message, but if this is ever changed in the\n-        #       class/repo code then that test will fail, and the case will need to be updated.\n-        tokenized_chats = [tokenizer.apply_chat_template(test_chat) for test_chat in test_chats]\n-        # fmt: off\n-        expected_tokens = [\n-            [1, 29961, 25580, 29962, 3532, 14816, 29903, 6778, 13, 3492, 526, 263, 8444, 13563, 7451, 29889, 13, 29966, 829, 14816, 29903, 6778, 13, 13, 10994, 29991, 518, 29914, 25580, 29962],\n-            [1, 29961, 25580, 29962, 3532, 14816, 29903, 6778, 13, 3492, 526, 263, 8444, 13563, 7451, 29889, 13, 29966, 829, 14816, 29903, 6778, 13, 13, 10994, 29991, 518, 29914, 25580, 29962, 20103, 304, 5870, 366, 29889, 29871, 2],\n-            [1, 29961, 25580, 29962, 15043, 29991, 518, 29914, 25580, 29962]\n-        ]\n-        # fmt: on\n-        for tokenized_chat, expected_tokens in zip(tokenized_chats, expected_tokens):\n-            self.assertListEqual(tokenized_chat, expected_tokens)\n-\n \n @require_sentencepiece\n @require_tokenizers"
      },
      {
        "filename": "tests/test_tokenization_mistral_common.py",
        "status": "modified",
        "additions": 38,
        "deletions": 24,
        "changes": 62,
        "patch": "@@ -799,7 +799,9 @@ def test_apply_chat_template_basic(self):\n \n         # Test 2:\n         # without tokenize\n-        self.assertEqual(self.tokenizer.apply_chat_template(conversation, tokenize=True), expected_tokenized.tokens)\n+        self.assertEqual(\n+            self.tokenizer.apply_chat_template(conversation, tokenize=True).input_ids, expected_tokenized.tokens\n+        )\n \n         with self.assertRaises(\n             ValueError, msg=\"Kwargs [unk_args] are not supported by `MistralCommonTokenizer.apply_chat_template`.\"\n@@ -824,7 +826,7 @@ def test_apply_chat_template_continue_final_message(self):\n             expected_tokenized.text,\n         )\n         self.assertEqual(\n-            self.tokenizer.apply_chat_template(conversation, tokenize=True, continue_final_message=True),\n+            self.tokenizer.apply_chat_template(conversation, tokenize=True, continue_final_message=True).input_ids,\n             expected_tokenized.tokens,\n         )\n \n@@ -846,7 +848,7 @@ def test_apply_chat_template_with_add_generation_prompt(self):\n             token_outputs = self.tokenizer.apply_chat_template(\n                 conversation, tokenize=True, add_generation_prompt=add_generation_prompt\n             )\n-            self.assertEqual(token_outputs, expected_tokenized.tokens)\n+            self.assertEqual(token_outputs.input_ids, expected_tokenized.tokens)\n \n         # Test 2:\n         # with continue_final_message\n@@ -958,18 +960,16 @@ def test_apply_chat_template_with_image(self):\n                 },\n             ]\n \n-            output = self.tokenizer.apply_chat_template(conversation, tokenize=True)\n+            output = self.tokenizer.apply_chat_template(conversation).input_ids\n             self.assertEqual(output, expected_tokenized.tokens)\n \n-        output_dict = self.tokenizer.apply_chat_template(conversation, tokenize=True, return_dict=True)\n+        output_dict = self.tokenizer.apply_chat_template(conversation, tokenize=True)\n         self.assertEqual(output_dict[\"input_ids\"], expected_tokenized.tokens)\n         self.assertEqual(len(output_dict[\"pixel_values\"]), len(expected_tokenized.images))\n         for o, e in zip(output_dict[\"pixel_values\"], expected_tokenized.images):\n             self.assertTrue(np.allclose(o, e))\n \n-        output_dict = self.tokenizer.apply_chat_template(\n-            conversation, tokenize=True, return_dict=True, return_tensors=\"pt\"\n-        )\n+        output_dict = self.tokenizer.apply_chat_template(conversation, tokenize=True, return_tensors=\"pt\")\n         self.assertEqual(output_dict[\"input_ids\"].tolist()[0], expected_tokenized.tokens)\n         expected_images_pt_tensor = torch.from_numpy(np.stack(expected_tokenized.images))\n         self.assertTrue(torch.allclose(output_dict[\"pixel_values\"], expected_images_pt_tensor))\n@@ -1013,7 +1013,7 @@ def test_apply_chat_template_with_audio(self):\n                 },\n             ]\n \n-            output = self.tokenizer_audio.apply_chat_template(conversation, tokenize=True)\n+            output = self.tokenizer_audio.apply_chat_template(conversation, tokenize=True).input_ids\n             self.assertEqual(output, expected_tokenized.tokens)\n \n         output_dict = self.tokenizer_audio.apply_chat_template(conversation, tokenize=True, return_dict=True)\n@@ -1041,14 +1041,14 @@ def test_apply_chat_template_with_truncation(self):\n         # Test 1:\n         # with truncation\n         self.assertEqual(\n-            self.tokenizer.apply_chat_template(conversation, tokenize=True, truncation=True, max_length=20),\n+            self.tokenizer.apply_chat_template(conversation, tokenize=True, truncation=True, max_length=20).input_ids,\n             expected_tokenized.tokens[:20],\n         )\n \n         # Test 2:\n         # without truncation\n         self.assertEqual(\n-            self.tokenizer.apply_chat_template(conversation, tokenize=True, truncation=False, max_length=20),\n+            self.tokenizer.apply_chat_template(conversation, tokenize=True, truncation=False, max_length=20).input_ids,\n             expected_tokenized.tokens,\n         )\n \n@@ -1130,7 +1130,7 @@ def test_batch_apply_chat_template(self):\n         ]\n \n         text_outputs = self.tokenizer.apply_chat_template(conversations, tools=tools, tokenize=False)\n-        token_outputs = self.tokenizer.apply_chat_template(conversations, tools=tools, tokenize=True)\n+        token_outputs = self.tokenizer.apply_chat_template(conversations, tools=tools, tokenize=True).input_ids\n \n         self.assertEqual(len(text_outputs), len(token_outputs))\n         self.assertEqual(len(text_outputs), len(expected_tokenized))\n@@ -1202,7 +1202,7 @@ def test_batch_apply_chat_template_images(self):\n             ChatCompletionRequest.from_openai(ref_conversation)\n         )\n \n-        output = self.tokenizer.apply_chat_template(conversations, tokenize=True)\n+        output = self.tokenizer.apply_chat_template(conversations, tokenize=True).input_ids\n         self.assertEqual(output, [expected_tokenized.tokens] * 3)\n \n         output = self.tokenizer.apply_chat_template(conversations, tokenize=True, return_dict=True)\n@@ -1248,7 +1248,9 @@ def test_batch_apply_chat_template_with_continue_final_message(self):\n             for conversation in conversations\n         ]\n \n-        token_outputs = self.tokenizer.apply_chat_template(conversations, tokenize=True, continue_final_message=True)\n+        token_outputs = self.tokenizer.apply_chat_template(\n+            conversations, tokenize=True, continue_final_message=True\n+        ).input_ids\n \n         for output, expected in zip(token_outputs, expected_tokenized):\n             self.assertEqual(output, expected.tokens)\n@@ -1297,7 +1299,7 @@ def test_batch_apply_chat_template_with_add_generation_prompt(self):\n             ]\n             token_outputs = self.tokenizer.apply_chat_template(\n                 conversations, tokenize=True, add_generation_prompt=add_generation_prompt\n-            )\n+            ).input_ids\n             for output, expected in zip(token_outputs, expected_tokenized):\n                 self.assertEqual(output, expected.tokens)\n \n@@ -1331,7 +1333,7 @@ def test_batch_apply_chat_template_with_truncation(\n         # with truncation\n         token_outputs = self.tokenizer.apply_chat_template(\n             self.fixture_conversations, tokenize=True, truncation=True, max_length=20\n-        )\n+        ).input_ids\n \n         for output, expected in zip(token_outputs, self.tokenized_fixture_conversations):\n             self.assertEqual(output, expected.tokens[:20])\n@@ -1340,7 +1342,7 @@ def test_batch_apply_chat_template_with_truncation(\n         # without truncation\n         token_outputs = self.tokenizer.apply_chat_template(\n             self.fixture_conversations, tokenize=True, truncation=False, max_length=20\n-        )\n+        ).input_ids\n         self.assertEqual(len(token_outputs), len(self.tokenized_fixture_conversations))\n         for output, expected in zip(token_outputs, self.tokenized_fixture_conversations):\n             self.assertEqual(output, expected.tokens)\n@@ -1358,15 +1360,17 @@ def test_batch_apply_chat_template_with_padding(\n         for padding in [True, \"max_length\", PaddingStrategy.LONGEST, PaddingStrategy.MAX_LENGTH]:\n             if padding == PaddingStrategy.MAX_LENGTH:\n                 # No padding if no max length is provided\n-                token_outputs = self.tokenizer.apply_chat_template(self.fixture_conversations, padding=padding)\n+                token_outputs = self.tokenizer.apply_chat_template(\n+                    self.fixture_conversations, padding=padding, return_dict=False\n+                )\n                 self.assertEqual(len(token_outputs), len(self.tokenized_fixture_conversations))\n                 for output, expected in zip(token_outputs, self.tokenized_fixture_conversations):\n                     self.assertEqual(output, expected.tokens)\n \n             max_length = 20 if padding == PaddingStrategy.MAX_LENGTH else None\n \n             token_outputs = self.tokenizer.apply_chat_template(\n-                self.fixture_conversations, tokenize=True, padding=padding, max_length=max_length\n+                self.fixture_conversations, tokenize=True, padding=padding, max_length=max_length, return_dict=False\n             )\n \n             if padding != PaddingStrategy.MAX_LENGTH:\n@@ -1390,7 +1394,7 @@ def test_batch_apply_chat_template_with_padding(\n \n         for padding in [False, \"do_not_pad\", PaddingStrategy.DO_NOT_PAD]:\n             token_outputs = self.tokenizer.apply_chat_template(\n-                self.fixture_conversations, tokenize=True, padding=padding\n+                self.fixture_conversations, tokenize=True, padding=padding, return_dict=False\n             )\n             self.assertEqual(len(token_outputs), len(self.tokenized_fixture_conversations))\n             for output, expected in zip(token_outputs, self.tokenized_fixture_conversations):\n@@ -1402,7 +1406,12 @@ def test_batch_apply_chat_template_with_padding_and_truncation(\n         max_length = 20\n         for padding in [True, \"max_length\", PaddingStrategy.LONGEST, PaddingStrategy.MAX_LENGTH]:\n             token_outputs = self.tokenizer.apply_chat_template(\n-                self.fixture_conversations, tokenize=True, truncation=True, padding=padding, max_length=max_length\n+                self.fixture_conversations,\n+                tokenize=True,\n+                truncation=True,\n+                padding=padding,\n+                max_length=max_length,\n+                return_dict=False,\n             )\n             self.assertEqual(len(token_outputs), len(self.tokenized_fixture_conversations))\n             for output, expected in zip(token_outputs, self.tokenized_fixture_conversations):\n@@ -1411,7 +1420,12 @@ def test_batch_apply_chat_template_with_padding_and_truncation(\n                 )\n         for padding in [False, \"do_not_pad\", PaddingStrategy.DO_NOT_PAD]:\n             token_outputs = self.tokenizer.apply_chat_template(\n-                self.fixture_conversations, tokenize=True, truncation=True, padding=padding, max_length=max_length\n+                self.fixture_conversations,\n+                tokenize=True,\n+                truncation=True,\n+                padding=padding,\n+                max_length=max_length,\n+                return_dict=False,\n             )\n             self.assertEqual(len(token_outputs), len(self.tokenized_fixture_conversations))\n             for output, expected in zip(token_outputs, self.tokenized_fixture_conversations):\n@@ -1421,7 +1435,7 @@ def test_batch_apply_chat_template_return_tensors(self):\n         # Test 1:\n         # with tokenize\n         token_outputs = self.tokenizer.apply_chat_template(\n-            self.fixture_conversations, tokenize=True, return_tensors=\"pt\", padding=True\n+            self.fixture_conversations, tokenize=True, return_tensors=\"pt\", padding=True, return_dict=False\n         )\n         self.assertIsInstance(token_outputs, torch.Tensor)\n         self.assertEqual(\n@@ -1432,7 +1446,7 @@ def test_batch_apply_chat_template_return_tensors(self):\n         # Test 2:\n         # without tokenize, should ignore return_tensors\n         token_outputs = self.tokenizer.apply_chat_template(\n-            self.fixture_conversations, tokenize=False, return_tensors=\"pt\", padding=True\n+            self.fixture_conversations, tokenize=False, return_tensors=\"pt\", padding=True, return_dict=False\n         )\n         self.assertEqual(token_outputs, [t.text for t in self.tokenized_fixture_conversations])\n "
      },
      {
        "filename": "tests/tokenization/test_tokenization_utils.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -323,7 +323,7 @@ def test_encode_message(self):\n         ]\n \n         # First, test the default case, where we encode the whole conversation at once\n-        whole_conversation_tokens = tokenizer.apply_chat_template(conversation, tokenize=True)\n+        whole_conversation_tokens = tokenizer.apply_chat_template(conversation, tokenize=True, return_dict=False)\n \n         # Now, test the message-by-message encoding\n         tokens = []"
      }
    ],
    "num_files": 13,
    "scraped_at": "2025-11-16T21:17:25.496723"
  },
  {
    "pr_number": 41625,
    "title": "[`Masks`] Fix mask handling in eager for vision models",
    "body": "As per title, some vision models use masks, let's sync with bert this time and reduce errors that could be introduced",
    "html_url": "https://github.com/huggingface/transformers/pull/41625",
    "created_at": "2025-10-15T15:00:17Z",
    "merged_at": "2025-10-16T14:27:26Z",
    "merge_commit_sha": "bf815e9b5ea076f758cc58f73f2be2d36237f9ec",
    "base_ref": "main",
    "head_sha": "b0a0ae01889ae4790058b103b2a1654baf314cd5",
    "user": "vasqu",
    "files": [
      {
        "filename": "src/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py",
        "status": "modified",
        "additions": 11,
        "deletions": 8,
        "changes": 19,
        "patch": "@@ -96,25 +96,28 @@ def forward(self, input_values: torch.Tensor) -> torch.Tensor:\n         return embeddings\n \n \n-# Copied from transformers.models.vit.modeling_vit.eager_attention_forward\n+# Copied from transformers.models.bert.modeling_bert.eager_attention_forward\n def eager_attention_forward(\n     module: nn.Module,\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n     attention_mask: Optional[torch.Tensor],\n-    scaling: float,\n+    scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n-    # Normalize the attention scores to probabilities.\n-    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n+        attn_weights = attn_weights + attention_mask\n \n-    # This is actually dropping out entire tokens to attend to, which might\n-    # seem a bit unusual, but is taken from the original Transformer paper.\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n     attn_output = torch.matmul(attn_weights, value)"
      },
      {
        "filename": "src/transformers/models/deit/modeling_deit.py",
        "status": "modified",
        "additions": 11,
        "deletions": 8,
        "changes": 19,
        "patch": "@@ -161,25 +161,28 @@ def forward(self, pixel_values: torch.Tensor) -> torch.Tensor:\n         return x\n \n \n-# Copied from transformers.models.vit.modeling_vit.eager_attention_forward\n+# Copied from transformers.models.bert.modeling_bert.eager_attention_forward\n def eager_attention_forward(\n     module: nn.Module,\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n     attention_mask: Optional[torch.Tensor],\n-    scaling: float,\n+    scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n-    # Normalize the attention scores to probabilities.\n-    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n+        attn_weights = attn_weights + attention_mask\n \n-    # This is actually dropping out entire tokens to attend to, which might\n-    # seem a bit unusual, but is taken from the original Transformer paper.\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n     attn_output = torch.matmul(attn_weights, value)"
      },
      {
        "filename": "src/transformers/models/dinov2/modeling_dinov2.py",
        "status": "modified",
        "additions": 11,
        "deletions": 8,
        "changes": 19,
        "patch": "@@ -149,25 +149,28 @@ def forward(self, pixel_values: torch.Tensor) -> torch.Tensor:\n         return embeddings\n \n \n-# Copied from transformers.models.vit.modeling_vit.eager_attention_forward\n+# Copied from transformers.models.bert.modeling_bert.eager_attention_forward\n def eager_attention_forward(\n     module: nn.Module,\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n     attention_mask: Optional[torch.Tensor],\n-    scaling: float,\n+    scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n-    # Normalize the attention scores to probabilities.\n-    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n+        attn_weights = attn_weights + attention_mask\n \n-    # This is actually dropping out entire tokens to attend to, which might\n-    # seem a bit unusual, but is taken from the original Transformer paper.\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n     attn_output = torch.matmul(attn_weights, value)"
      },
      {
        "filename": "src/transformers/models/dinov2_with_registers/modeling_dinov2_with_registers.py",
        "status": "modified",
        "additions": 10,
        "deletions": 7,
        "changes": 17,
        "patch": "@@ -176,18 +176,21 @@ def eager_attention_forward(\n     key: torch.Tensor,\n     value: torch.Tensor,\n     attention_mask: Optional[torch.Tensor],\n-    scaling: float,\n+    scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n-    # Normalize the attention scores to probabilities.\n-    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n+        attn_weights = attn_weights + attention_mask\n \n-    # This is actually dropping out entire tokens to attend to, which might\n-    # seem a bit unusual, but is taken from the original Transformer paper.\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n     attn_output = torch.matmul(attn_weights, value)"
      },
      {
        "filename": "src/transformers/models/dinov3_vit/modeling_dinov3_vit.py",
        "status": "modified",
        "additions": 10,
        "deletions": 7,
        "changes": 17,
        "patch": "@@ -194,18 +194,21 @@ def eager_attention_forward(\n     key: torch.Tensor,\n     value: torch.Tensor,\n     attention_mask: Optional[torch.Tensor],\n-    scaling: float,\n+    scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n-    # Normalize the attention scores to probabilities.\n-    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n+        attn_weights = attn_weights + attention_mask\n \n-    # This is actually dropping out entire tokens to attend to, which might\n-    # seem a bit unusual, but is taken from the original Transformer paper.\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n     attn_output = torch.matmul(attn_weights, value)"
      },
      {
        "filename": "src/transformers/models/dpt/modeling_dpt.py",
        "status": "modified",
        "additions": 13,
        "deletions": 9,
        "changes": 22,
        "patch": "@@ -32,7 +32,8 @@\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, DepthEstimatorOutput, SemanticSegmenterOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n-from ...utils import ModelOutput, auto_docstring, logging, torch_int\n+from ...processing_utils import Unpack\n+from ...utils import ModelOutput, TransformersKwargs, auto_docstring, logging, torch_int\n from ...utils.backbone_utils import load_backbone\n from ...utils.generic import can_return_tuple, check_model_inputs\n from .configuration_dpt import DPTConfig\n@@ -267,25 +268,28 @@ def forward(self, pixel_values: torch.Tensor) -> torch.Tensor:\n         return embeddings\n \n \n-# Copied from transformers.models.vit.modeling_vit.eager_attention_forward\n+# Copied from transformers.models.bert.modeling_bert.eager_attention_forward\n def eager_attention_forward(\n     module: nn.Module,\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n     attention_mask: Optional[torch.Tensor],\n-    scaling: float,\n+    scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n-    # Normalize the attention scores to probabilities.\n-    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n+        attn_weights = attn_weights + attention_mask\n \n-    # This is actually dropping out entire tokens to attend to, which might\n-    # seem a bit unusual, but is taken from the original Transformer paper.\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n     attn_output = torch.matmul(attn_weights, value)"
      },
      {
        "filename": "src/transformers/models/ijepa/modeling_ijepa.py",
        "status": "modified",
        "additions": 10,
        "deletions": 7,
        "changes": 17,
        "patch": "@@ -147,18 +147,21 @@ def eager_attention_forward(\n     key: torch.Tensor,\n     value: torch.Tensor,\n     attention_mask: Optional[torch.Tensor],\n-    scaling: float,\n+    scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n-    # Normalize the attention scores to probabilities.\n-    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n+        attn_weights = attn_weights + attention_mask\n \n-    # This is actually dropping out entire tokens to attend to, which might\n-    # seem a bit unusual, but is taken from the original Transformer paper.\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n     attn_output = torch.matmul(attn_weights, value)"
      },
      {
        "filename": "src/transformers/models/videomae/modeling_videomae.py",
        "status": "modified",
        "additions": 11,
        "deletions": 8,
        "changes": 19,
        "patch": "@@ -178,25 +178,28 @@ def forward(self, pixel_values):\n         return embeddings\n \n \n-# Copied from transformers.models.vit.modeling_vit.eager_attention_forward\n+# Copied from transformers.models.bert.modeling_bert.eager_attention_forward\n def eager_attention_forward(\n     module: nn.Module,\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n     attention_mask: Optional[torch.Tensor],\n-    scaling: float,\n+    scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n-    # Normalize the attention scores to probabilities.\n-    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n+        attn_weights = attn_weights + attention_mask\n \n-    # This is actually dropping out entire tokens to attend to, which might\n-    # seem a bit unusual, but is taken from the original Transformer paper.\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n     attn_output = torch.matmul(attn_weights, value)"
      },
      {
        "filename": "src/transformers/models/vit/modeling_vit.py",
        "status": "modified",
        "additions": 11,
        "deletions": 7,
        "changes": 18,
        "patch": "@@ -167,24 +167,28 @@ def forward(self, pixel_values: torch.Tensor, interpolate_pos_encoding: bool = F\n         return embeddings\n \n \n+# Copied from transformers.models.bert.modeling_bert.eager_attention_forward\n def eager_attention_forward(\n     module: nn.Module,\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n     attention_mask: Optional[torch.Tensor],\n-    scaling: float,\n+    scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n-    # Normalize the attention scores to probabilities.\n-    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n+        attn_weights = attn_weights + attention_mask\n \n-    # This is actually dropping out entire tokens to attend to, which might\n-    # seem a bit unusual, but is taken from the original Transformer paper.\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n     attn_output = torch.matmul(attn_weights, value)"
      },
      {
        "filename": "src/transformers/models/vit_mae/modeling_vit_mae.py",
        "status": "modified",
        "additions": 11,
        "deletions": 8,
        "changes": 19,
        "patch": "@@ -326,25 +326,28 @@ def forward(self, pixel_values, interpolate_pos_encoding: bool = False):\n         return x\n \n \n-# Copied from transformers.models.vit.modeling_vit.eager_attention_forward\n+# Copied from transformers.models.bert.modeling_bert.eager_attention_forward\n def eager_attention_forward(\n     module: nn.Module,\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n     attention_mask: Optional[torch.Tensor],\n-    scaling: float,\n+    scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n-    # Normalize the attention scores to probabilities.\n-    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n+        attn_weights = attn_weights + attention_mask\n \n-    # This is actually dropping out entire tokens to attend to, which might\n-    # seem a bit unusual, but is taken from the original Transformer paper.\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n     attn_output = torch.matmul(attn_weights, value)"
      },
      {
        "filename": "src/transformers/models/vit_msn/modeling_vit_msn.py",
        "status": "modified",
        "additions": 11,
        "deletions": 8,
        "changes": 19,
        "patch": "@@ -163,25 +163,28 @@ def forward(self, pixel_values: torch.Tensor, interpolate_pos_encoding: bool = F\n         return embeddings\n \n \n-# Copied from transformers.models.vit.modeling_vit.eager_attention_forward\n+# Copied from transformers.models.bert.modeling_bert.eager_attention_forward\n def eager_attention_forward(\n     module: nn.Module,\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n     attention_mask: Optional[torch.Tensor],\n-    scaling: float,\n+    scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n-    # Normalize the attention scores to probabilities.\n-    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n+        attn_weights = attn_weights + attention_mask\n \n-    # This is actually dropping out entire tokens to attend to, which might\n-    # seem a bit unusual, but is taken from the original Transformer paper.\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n     attn_output = torch.matmul(attn_weights, value)"
      },
      {
        "filename": "src/transformers/models/vitpose_backbone/modeling_vitpose_backbone.py",
        "status": "modified",
        "additions": 13,
        "deletions": 9,
        "changes": 22,
        "patch": "@@ -30,7 +30,8 @@\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BackboneOutput, BaseModelOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n-from ...utils import auto_docstring, logging\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, auto_docstring, logging\n from ...utils.backbone_utils import BackboneMixin\n from ...utils.generic import check_model_inputs\n from .configuration_vitpose_backbone import VitPoseBackboneConfig\n@@ -95,25 +96,28 @@ def forward(self, pixel_values: torch.Tensor) -> torch.Tensor:\n         return embeddings\n \n \n-# Copied from transformers.models.vit.modeling_vit.eager_attention_forward\n+# Copied from transformers.models.bert.modeling_bert.eager_attention_forward\n def eager_attention_forward(\n     module: nn.Module,\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n     attention_mask: Optional[torch.Tensor],\n-    scaling: float,\n+    scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n-    # Normalize the attention scores to probabilities.\n-    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n+        attn_weights = attn_weights + attention_mask\n \n-    # This is actually dropping out entire tokens to attend to, which might\n-    # seem a bit unusual, but is taken from the original Transformer paper.\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n     attn_output = torch.matmul(attn_weights, value)"
      },
      {
        "filename": "src/transformers/models/vivit/modeling_vivit.py",
        "status": "modified",
        "additions": 11,
        "deletions": 8,
        "changes": 19,
        "patch": "@@ -156,25 +156,28 @@ def forward(self, pixel_values: torch.Tensor, interpolate_pos_encoding: bool = F\n         return embeddings\n \n \n-# Copied from transformers.models.vit.modeling_vit.eager_attention_forward\n+# Copied from transformers.models.bert.modeling_bert.eager_attention_forward\n def eager_attention_forward(\n     module: nn.Module,\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n     attention_mask: Optional[torch.Tensor],\n-    scaling: float,\n+    scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n-    # Normalize the attention scores to probabilities.\n-    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n+        attn_weights = attn_weights + attention_mask\n \n-    # This is actually dropping out entire tokens to attend to, which might\n-    # seem a bit unusual, but is taken from the original Transformer paper.\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n     attn_output = torch.matmul(attn_weights, value)"
      },
      {
        "filename": "src/transformers/models/yolos/modeling_yolos.py",
        "status": "modified",
        "additions": 11,
        "deletions": 8,
        "changes": 19,
        "patch": "@@ -211,25 +211,28 @@ def forward(self, pixel_values: torch.Tensor) -> torch.Tensor:\n         return embeddings\n \n \n-# Copied from transformers.models.vit.modeling_vit.eager_attention_forward\n+# Copied from transformers.models.bert.modeling_bert.eager_attention_forward\n def eager_attention_forward(\n     module: nn.Module,\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n     attention_mask: Optional[torch.Tensor],\n-    scaling: float,\n+    scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n-    # Normalize the attention scores to probabilities.\n-    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n+        attn_weights = attn_weights + attention_mask\n \n-    # This is actually dropping out entire tokens to attend to, which might\n-    # seem a bit unusual, but is taken from the original Transformer paper.\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n     attn_output = torch.matmul(attn_weights, value)"
      }
    ],
    "num_files": 14,
    "scraped_at": "2025-11-16T21:17:25.820794"
  },
  {
    "pr_number": 41624,
    "title": "Fix serving continuous batching",
    "body": "# What does this PR do?\r\n\r\nServing is broken with continuous batching to due recent PRs. This PR fixes it ",
    "html_url": "https://github.com/huggingface/transformers/pull/41624",
    "created_at": "2025-10-15T14:49:47Z",
    "merged_at": "2025-10-16T15:24:22Z",
    "merge_commit_sha": "9839d57a0244f86125c89981f6301b48309b4913",
    "base_ref": "main",
    "head_sha": "5c12811e5a3cb4de1f5edc72fc70a97c1b14a70f",
    "user": "SunMarc",
    "files": [
      {
        "filename": "docs/source/en/serving.md",
        "status": "modified",
        "additions": 2,
        "deletions": 3,
        "changes": 5,
        "patch": "@@ -380,7 +380,7 @@ CB is opt-in and currently applies to chat completions.\n ```sh\n transformers serve \\\n   --continuous-batching\n-  --attn_implementation sdpa_paged\n+  --attn_implementation \"sdpa\"\n ```\n \n ### Performance tips\n@@ -390,11 +390,10 @@ transformers serve \\\n ```sh\n transformers serve \\\n   --continuous_batching \\\n-  --attn_implementation paged_attention\n+  --attn_implementation \"flash_attention_2\"\n ```\n \n > [!TIP]\n-> If you choose `paged_attention`, you must install `flash-attn` separately: `pip install flash-attn --no-build-isolation`\n \n - `--dtype {bfloat16|float16}` typically improve throughput and memory use vs. `float32`\n "
      },
      {
        "filename": "src/transformers/generation/continuous_batching/continuous_api.py",
        "status": "modified",
        "additions": 0,
        "deletions": 8,
        "changes": 8,
        "patch": "@@ -929,14 +929,6 @@ def request_id_iter(self, request_id: str) -> Generator[GenerationOutput]:\n             if self.batch_processor is not None:\n                 request_cancelled = self.batch_processor.scheduler.request_is_cancelled(request_id)\n \n-    @staticmethod\n-    def supported_attention_implementations() -> set[str]:\n-        return {\"eager_paged\", \"sdpa_paged\", \"flash_attention_2\"}\n-\n-    @staticmethod\n-    def default_attention_implementation() -> str:\n-        return \"sdpa_paged\"\n-\n     @traced\n     def warmup(self, batch_processor: ContinuousBatchProcessor) -> None:\n         stream = torch.cuda.Stream(device=self.model.device)"
      },
      {
        "filename": "src/transformers/modeling_utils.py",
        "status": "modified",
        "additions": 8,
        "deletions": 8,
        "changes": 16,
        "patch": "@@ -2426,30 +2426,30 @@ def get_correct_attn_implementation(self, requested_attention: Optional[str], is\n         if applicable_attention not in [\"eager\"] + ALL_ATTENTION_FUNCTIONS.valid_keys():\n             message = (\n                 f'Specified `attn_implementation=\"{applicable_attention}\"` is not supported. The only possible arguments are '\n-                '`attn_implementation=\"eager\"`'\n+                '`attn_implementation=\"eager\"`, `\"paged|eager\"`'\n             )\n             # check `supports_flash_attn_2` for BC with custom code. TODO: remove after a few releases\n             if self._supports_flash_attn or getattr(self, \"_supports_flash_attn_2\", False):\n-                message += ', `\"attn_implementation=flash_attention_3\"`, `\"attn_implementation=flash_attention_2\"`'\n+                message += ', `\"attn_implementation=flash_attention_3\"`, `\"attn_implementation=flash_attention_2\"`, `\"attn_implementation=paged|flash_attention_2\"`'\n             if self._supports_sdpa:\n-                message += ', `\"attn_implementation=sdpa\"'\n+                message += ', `\"attn_implementation=sdpa\"`, `\"attn_implementation=paged|spda\"`'\n             if self._supports_flex_attn:\n                 message += ', `\"attn_implementation=flex_attention\"`'\n             raise ValueError(message + \".\")\n \n         # Perform relevant checks\n-        if applicable_attention == \"flash_attention_2\":\n+        if \"flash_attention_2\" in applicable_attention:\n             self._flash_attn_2_can_dispatch(is_init_check)\n-        elif applicable_attention == \"flash_attention_3\":\n+        elif \"flash_attention_3\" in applicable_attention:\n             self._flash_attn_3_can_dispatch(is_init_check)\n-        elif applicable_attention == \"flex_attention\":\n+        elif \"flex_attention\" in applicable_attention:\n             self._flex_attn_can_dispatch(is_init_check)\n-        elif applicable_attention == \"sdpa\":\n+        elif \"sdpa\" in applicable_attention:\n             # Sdpa is the default, so we try it and fallback to eager otherwise when not possible\n             try:\n                 self._sdpa_can_dispatch(is_init_check)\n             except (ValueError, ImportError) as e:\n-                if requested_attention == \"sdpa\":\n+                if requested_attention is not None and \"sdpa\" in requested_attention:\n                     raise e\n                 applicable_attention = \"eager\"\n "
      },
      {
        "filename": "src/transformers/utils/import_utils.py",
        "status": "modified",
        "additions": 7,
        "deletions": 0,
        "changes": 7,
        "patch": "@@ -1167,6 +1167,13 @@ def is_mistral_common_available() -> bool:\n     return _is_package_available(\"mistral_common\")\n \n \n+@lru_cache\n+def is_opentelemetry_available() -> bool:\n+    return _is_package_available(\"opentelemetry\") and version.parse(\n+        importlib.metadata.version(\"opentelemetry-api\")\n+    ) >= version.parse(\"1.30.0\")\n+\n+\n def check_torch_load_is_safe() -> None:\n     if not is_torch_greater_or_equal(\"2.6\"):\n         raise ValueError("
      },
      {
        "filename": "src/transformers/utils/metrics.py",
        "status": "modified",
        "additions": 8,
        "deletions": 3,
        "changes": 11,
        "patch": "@@ -5,6 +5,8 @@\n from enum import Enum\n from typing import Any, Optional, Union\n \n+from .import_utils import is_opentelemetry_available\n+\n \n class RequestStatus(Enum):\n     \"\"\"Status of a generation request through its lifecycle.\"\"\"\n@@ -18,12 +20,12 @@ class RequestStatus(Enum):\n     FAILED = \"failed\"\n \n \n-try:\n+if is_opentelemetry_available():\n     from opentelemetry import metrics\n     from opentelemetry.trace import Status, StatusCode, get_tracer\n \n     _has_opentelemetry = True\n-except ImportError:\n+else:\n     _has_opentelemetry = False\n \n \n@@ -183,7 +185,10 @@ def _setup_metrics(self):\n         \"\"\"Initialize OpenTelemetry metrics and tracing if the library is available.\"\"\"\n \n         if not _has_opentelemetry:\n-            logger.info(\"OpenTelemetry is not installed. Metrics and tracing will not be recorded.\")\n+            logger.info(\n+                \"OpenTelemetry is not installed. Metrics and tracing will not be recorded.\"\n+                \"You can install it with `pip install opentelemetry-api>=1.30.0`\"\n+            )\n             return\n \n         self.meter = metrics.get_meter(\"transformers.generation.continuous_batch_processor\")"
      }
    ],
    "num_files": 5,
    "scraped_at": "2025-11-16T21:17:26.111555"
  },
  {
    "pr_number": 41612,
    "title": "Fix EncoderDecoder cache",
    "body": "In #41569 we restored thr `__iter__` method to `DynamicCache` but I missed the fact that it was also removed from `EncoderDecoderCache`. This PR fixes that and modifies the `__init__` of `EncoderDecoderCache` in the case of DDP, so it is compatible with the new system.",
    "html_url": "https://github.com/huggingface/transformers/pull/41612",
    "created_at": "2025-10-15T10:39:31Z",
    "merged_at": "2025-10-16T12:55:42Z",
    "merge_commit_sha": "eef9fb2af3db888cf93f81b425f9db453336726c",
    "base_ref": "main",
    "head_sha": "c3f2af117695ca9146c0bbaa9d374bc7c073b94a",
    "user": "remi-or",
    "files": [
      {
        "filename": "src/transformers/cache_utils.py",
        "status": "modified",
        "additions": 27,
        "deletions": 14,
        "changes": 41,
        "patch": "@@ -937,7 +937,7 @@ class DynamicCache(Cache):\n \n     def __init__(\n         self,\n-        ddp_cache_data: Optional[Iterable[tuple[Optional[torch.Tensor], torch.Tensor, torch.Tensor]]] = None,\n+        ddp_cache_data: Optional[Iterable[tuple[Optional[torch.Tensor], ...]]] = None,\n         config: Optional[PreTrainedConfig] = None,\n         offloading: bool = False,\n         offload_only_non_sliding: bool = False,\n@@ -970,17 +970,21 @@ def __init__(\n         # In this case, use the passed data to already fill in the Cache\n         if ddp_cache_data is not None:\n             # Init all the layers with the data\n-            for layer_idx, (sliding_window_tensor, key_states, value_states) in enumerate(ddp_cache_data):\n+            for layer_idx, kv_and_optional_sliding in enumerate(ddp_cache_data):\n                 # If the config was not passed above, initialize a new cache layer for each entry of the ddp_data\n                 if config is None:\n+                    # kv_and_optional_sliding contains at least two elements: the key and value states. It can also\n+                    # contain a third element, which is an optional sliding window tensor.\n+                    sliding_window_tensor = kv_and_optional_sliding[2] if len(kv_and_optional_sliding) == 3 else None\n+                    # If there is a sliding window tensor, use it to initialize the layer\n                     if sliding_window_tensor is not None:\n                         # Since the same layer is dispatched across replicas, sliding_window is the same for all\n                         sliding_window = sliding_window_tensor[0].item()\n                         layers.append(DynamicSlidingWindowLayer(sliding_window=sliding_window))\n                     else:\n                         layers.append(DynamicLayer())\n                 # Update the layer with the data\n-                _, _ = layers[layer_idx].update(key_states, value_states)\n+                _, _ = layers[layer_idx].update(kv_and_optional_sliding[0], kv_and_optional_sliding[1])\n \n         # If neither of config nor ddp_data was passed, then simply lazy init a full cache of DynamicLayer\n         if len(layers) == 0:\n@@ -994,7 +998,7 @@ def __init__(\n \n     def __iter__(self):\n         for layer in self.layers:\n-            yield getattr(layer, \"_sliding_window_tensor\", None), layer.keys, layer.values\n+            yield layer.keys, layer.values, getattr(layer, \"_sliding_window_tensor\", None)\n \n \n class StaticCache(Cache):\n@@ -1166,17 +1170,21 @@ class EncoderDecoderCache(Cache):\n     \"\"\"\n \n     def __init__(self, *caches) -> None:\n-        # For dp and ddp support, if only one argument is passed, it should be an iterable of tuples of tensors\n+        # For dp and ddp support, if only one argument is passed, it should be an iterable of DynamicCache ddp data\n         if len(caches) == 1:\n-            self.self_attention_cache = DynamicCache()\n-            self.cross_attention_cache = DynamicCache()\n-            # Populate cache from the iterable\n-            for layer_idx, key_value_states in enumerate(caches[0]):\n-                key_states, value_states = key_value_states[:2]\n-                self.self_attention_cache.update(key_states, value_states, layer_idx)\n-                if len(key_value_states) > 2:\n-                    key_states, value_states = key_value_states[2:]\n-                    self.cross_attention_cache.update(key_states, value_states, layer_idx)\n+            self_attention_cache_data, cross_attention_cache_data = [], []\n+            for combined_cache_data in caches[0]:\n+                if len(combined_cache_data) == 6:  # two tuple of style (self_attn_k, self_attn_v, self_attn_sliding)\n+                    self_attention_cache_data.append(combined_cache_data[:3])\n+                    cross_attention_cache_data.append(combined_cache_data[3:])\n+                # To support old DDP-style init, we handle the case where the tuple has no sliding window tensor\n+                elif len(combined_cache_data) == 4:  # two tuple of style (self_attn_k, self_attn_v)\n+                    self_attention_cache_data.append(combined_cache_data[:2])\n+                    cross_attention_cache_data.append(combined_cache_data[2:])\n+                else:\n+                    raise ValueError(f\"Expected {len(combined_cache_data) = } to be 4 or 6.\\n{combined_cache_data = }\")\n+            self.self_attention_cache = DynamicCache(self_attention_cache_data)\n+            self.cross_attention_cache = DynamicCache(cross_attention_cache_data)\n         # Otherwise, we should get two arguments, a self-attention cache and a cross-attention cache\n         elif len(caches) == 2:\n             if not isinstance(caches[0], Cache) or not isinstance(caches[1], Cache):\n@@ -1191,6 +1199,11 @@ def __init__(self, *caches) -> None:\n         for layer_idx in range(len(self.cross_attention_cache)):\n             self.is_updated[layer_idx] = bool(self.cross_attention_cache.get_seq_length(layer_idx) > 0)\n \n+    def __iter__(self):\n+        \"\"\"Returns tuples of style (self_attn_k, self_attn_v, self_attn_sliding, cross_attn_k, cross_attn_v, cross_attn_sliding)\"\"\"\n+        for self_attention_layer, cross_attention_layer in zip(self.self_attention_cache, self.cross_attention_cache):\n+            yield self_attention_layer + cross_attention_layer\n+\n     def __repr__(self) -> str:\n         return (\n             f\"{self.__class__.__name__}(self_attention_cache={self.self_attention_cache}, cross_attention_cache=\""
      },
      {
        "filename": "src/transformers/models/rag/modeling_rag.py",
        "status": "modified",
        "additions": 16,
        "deletions": 14,
        "changes": 30,
        "patch": "@@ -1187,22 +1187,24 @@ def _reorder_stacked(hidden_states, new_order):\n         reordered_past = ()\n         for idx in range(len(past_key_values)):\n             if isinstance(past_key_values, EncoderDecoderCache):\n-                layer_past = (\n-                    past_key_values.self_attention_cache.layers[idx].keys,\n-                    past_key_values.self_attention_cache.layers[idx].values,\n-                    past_key_values.cross_attention_cache.layers[idx].keys,\n-                    past_key_values.cross_attention_cache.layers[idx].values,\n+                self_attention_k, self_attention_v, cross_attention_k, cross_attention_v = (\n+                    _reorder_stacked(x, beam_idx.to(x.device))\n+                    for x in (\n+                        past_key_values.self_attention_cache.layers[idx].keys,\n+                        past_key_values.self_attention_cache.layers[idx].values,\n+                        past_key_values.cross_attention_cache.layers[idx].keys,\n+                        past_key_values.cross_attention_cache.layers[idx].values,\n+                    )\n                 )\n+                new_tuple = (self_attention_k, self_attention_v, cross_attention_k, cross_attention_v)\n             else:\n-                layer_past = (past_key_values.layers[idx].keys, past_key_values.layers[idx].values)\n-            # get the correct batch idx from decoder layer's batch dim for cross and self-attn\n-            reordered_past += (\n-                tuple(_reorder_stacked(past_state, beam_idx.to(past_state.device)) for past_state in layer_past),\n-            )\n-\n-        # Cast back to the correct cache class\n-        reordered_cache = type(past_key_values)(reordered_past)\n-        return reordered_cache\n+                self_attention_k, self_attention_v = (\n+                    _reorder_stacked(x, beam_idx.to(x.device))\n+                    for x in (past_key_values.layers[idx].keys, past_key_values.layers[idx].values)\n+                )\n+                new_tuple = (self_attention_k, self_attention_v)\n+            reordered_past += (new_tuple,)\n+        return type(past_key_values)(reordered_past)\n \n     def marginalize(self, seq_logits, doc_scores, n_docs=None):\n         n_docs = n_docs if n_docs is not None else self.config.n_docs"
      },
      {
        "filename": "src/transformers/models/whisper/generation_whisper.py",
        "status": "modified",
        "additions": 12,
        "deletions": 8,
        "changes": 20,
        "patch": "@@ -1180,12 +1180,14 @@ def split_by_batch_index(values, key, batch_idx, is_shortform, beam_indices=None\n                     return None\n                 all_past_key_values = []\n                 for layer_idx in range(self.config.decoder_layers):\n-                    layer_past_key_values = []\n-                    for cache_cls in [values.self_attention_cache, values.cross_attention_cache]:\n-                        for v in [cache_cls.layers[layer_idx].keys, cache_cls.layers[layer_idx].values]:\n-                            layer_past_key_values.append(v[batch_idx][None].cpu())\n-                    all_past_key_values.append(tuple(layer_past_key_values))\n-                return EncoderDecoderCache(tuple(all_past_key_values))\n+                    layer_cache = (\n+                        values.self_attention_cache.layers[layer_idx].keys[batch_idx][None].cpu(),\n+                        values.self_attention_cache.layers[layer_idx].values[batch_idx][None].cpu(),\n+                        values.cross_attention_cache.layers[layer_idx].keys[batch_idx][None].cpu(),\n+                        values.cross_attention_cache.layers[layer_idx].values[batch_idx][None].cpu(),\n+                    )\n+                    all_past_key_values.append(layer_cache)\n+                return EncoderDecoderCache(all_past_key_values)\n \n             return values[batch_idx].cpu()\n \n@@ -1224,7 +1226,7 @@ def _stack_split_outputs(self, seek_outputs, model_output_type, device, kwargs):\n                 if seek_outputs[0][key] is not None:\n                     all_past_key_values = []\n                     for layer_idx in range(len(seek_outputs[0][key])):\n-                        layer_past_key_values = tuple(\n+                        self_attention_k, self_attention_v, cross_attention_k, cross_attention_v = (\n                             torch.stack(\n                                 [\n                                     getattr(getattr(sub_output[key], sub_cache).layers[layer_idx], sub_key)\n@@ -1236,7 +1238,9 @@ def _stack_split_outputs(self, seek_outputs, model_output_type, device, kwargs):\n                             for sub_cache in [\"self_attention_cache\", \"cross_attention_cache\"]\n                             for sub_key in [\"keys\", \"values\"]\n                         )\n-                        all_past_key_values.append(layer_past_key_values)\n+                        all_past_key_values.append(\n+                            (self_attention_k, self_attention_v, cross_attention_k, cross_attention_v)\n+                        )\n                     outputs[key] = EncoderDecoderCache(tuple(all_past_key_values))\n                 else:\n                     outputs[key] = None"
      },
      {
        "filename": "tests/utils/test_modeling_utils.py",
        "status": "modified",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "patch": "@@ -1807,8 +1807,8 @@ def test_cache_when_needed_at_train_time(self):\n         # simulate injecting virtual tokens like in prefix tuning\n         num_virtual_tokens = 3\n         past_key_values = [\n-            (None, torch.randn(1, 2, num_virtual_tokens, 8), torch.randn(1, 2, num_virtual_tokens, 8)),\n-            (None, torch.randn(1, 2, num_virtual_tokens, 8), torch.randn(1, 2, num_virtual_tokens, 8)),\n+            (torch.randn(1, 2, num_virtual_tokens, 8), torch.randn(1, 2, num_virtual_tokens, 8)),\n+            (torch.randn(1, 2, num_virtual_tokens, 8), torch.randn(1, 2, num_virtual_tokens, 8)),\n         ]\n         past_key_values = DynamicCache(past_key_values)\n         model_inputs[\"attention_mask\"] = torch.cat("
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T21:17:28.321872"
  },
  {
    "pr_number": 41607,
    "title": "[v5] Delete `videos` from image processing classes ",
    "body": "# What does this PR do?\r\n\r\nAs per title, it was deprecated for v5",
    "html_url": "https://github.com/huggingface/transformers/pull/41607",
    "created_at": "2025-10-15T09:56:46Z",
    "merged_at": "2025-10-21T10:03:31Z",
    "merge_commit_sha": "ee3a1002e2a3a4b60b66a1f9b034fbbe00c60e87",
    "base_ref": "main",
    "head_sha": "8cc6bbb30a00966e6245a6b41386f892895ab701",
    "user": "zucchini-nlp",
    "files": [
      {
        "filename": "docs/source/en/model_doc/instructblipvideo.md",
        "status": "modified",
        "additions": 0,
        "deletions": 5,
        "changes": 5,
        "patch": "@@ -63,11 +63,6 @@ The attributes can be obtained from model config, as `model.config.num_query_tok\n [[autodoc]] InstructBlipVideoVideoProcessor\n     - preprocess\n \n-## InstructBlipVideoImageProcessor\n-\n-[[autodoc]] InstructBlipVideoImageProcessor\n-    - preprocess\n-\n ## InstructBlipVideoVisionModel\n \n [[autodoc]] InstructBlipVideoVisionModel"
      },
      {
        "filename": "docs/source/en/model_doc/llava_next_video.md",
        "status": "modified",
        "additions": 0,
        "deletions": 4,
        "changes": 4,
        "patch": "@@ -247,10 +247,6 @@ model = LlavaNextVideoForConditionalGeneration.from_pretrained(\n \n [[autodoc]] LlavaNextVideoProcessor\n \n-## LlavaNextVideoImageProcessor\n-\n-[[autodoc]] LlavaNextVideoImageProcessor\n-\n ## LlavaNextVideoVideoProcessor\n \n [[autodoc]] LlavaNextVideoVideoProcessor"
      },
      {
        "filename": "src/transformers/models/auto/image_processing_auto.py",
        "status": "modified",
        "additions": 1,
        "deletions": 2,
        "changes": 3,
        "patch": "@@ -114,7 +114,6 @@\n             (\"ijepa\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n             (\"imagegpt\", (\"ImageGPTImageProcessor\", \"ImageGPTImageProcessorFast\")),\n             (\"instructblip\", (\"BlipImageProcessor\", \"BlipImageProcessorFast\")),\n-            (\"instructblipvideo\", (\"InstructBlipVideoImageProcessor\", None)),\n             (\"janus\", (\"JanusImageProcessor\", \"JanusImageProcessorFast\")),\n             (\"kosmos-2\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),\n             (\"kosmos-2.5\", (\"Kosmos2_5ImageProcessor\", \"Kosmos2_5ImageProcessorFast\")),\n@@ -126,7 +125,7 @@\n             (\"llama4\", (\"Llama4ImageProcessor\", \"Llama4ImageProcessorFast\")),\n             (\"llava\", (\"LlavaImageProcessor\", \"LlavaImageProcessorFast\")),\n             (\"llava_next\", (\"LlavaNextImageProcessor\", \"LlavaNextImageProcessorFast\")),\n-            (\"llava_next_video\", (\"LlavaNextVideoImageProcessor\", None)),\n+            (\"llava_next_video\", (\"LlavaNextImageProcessor\", \"LlavaNextImageProcessorFast\")),\n             (\"llava_onevision\", (\"LlavaOnevisionImageProcessor\", \"LlavaOnevisionImageProcessorFast\")),\n             (\"mask2former\", (\"Mask2FormerImageProcessor\", \"Mask2FormerImageProcessorFast\")),\n             (\"maskformer\", (\"MaskFormerImageProcessor\", \"MaskFormerImageProcessorFast\")),"
      },
      {
        "filename": "src/transformers/models/glm4v/image_processing_glm4v.py",
        "status": "modified",
        "additions": 0,
        "deletions": 4,
        "changes": 4,
        "patch": "@@ -313,7 +313,6 @@ def _preprocess(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        videos: Optional[VideoInput] = None,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n         resample: Optional[PILImageResampling] = None,\n@@ -335,9 +334,6 @@ def preprocess(\n             images (`ImageInput`):\n                 Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If\n                 passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n-            videos (`VideoInput`):\n-                Video to preprocess. Expects a single or batch of videos with pixel values ranging from 0 to 255. If\n-                passing in videos with pixel values between 0 and 1, set `do_rescale=False`.\n             do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n                 Whether to resize the image.\n             size (`Dict[str, int]`, *optional*, defaults to `self.size`):"
      },
      {
        "filename": "src/transformers/models/instructblipvideo/image_processing_instructblipvideo.py",
        "status": "removed",
        "additions": 0,
        "deletions": 327,
        "changes": 327,
        "patch": "@@ -1,327 +0,0 @@\n-# coding=utf-8\n-# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-\"\"\"\n-Image processor class for InstructBLIPVideo. Largely copy of Blip2Processor with addition of a video processing abilities\n-\"\"\"\n-\n-from typing import Optional, Union\n-\n-import numpy as np\n-\n-from ...image_processing_utils import BaseImageProcessor, BatchFeature, get_size_dict\n-from ...image_transforms import convert_to_rgb, resize, to_channel_dimension_format\n-from ...image_utils import (\n-    OPENAI_CLIP_MEAN,\n-    OPENAI_CLIP_STD,\n-    ChannelDimension,\n-    ImageInput,\n-    PILImageResampling,\n-    infer_channel_dimension_format,\n-    is_scaled_image,\n-    to_numpy_array,\n-    valid_images,\n-    validate_preprocess_arguments,\n-)\n-from ...utils import TensorType, filter_out_non_signature_kwargs, logging\n-from ...video_utils import VideoInput, make_batched_videos\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-\n-# TODO (raushan): processor can be removed after v5 release. Kept for backwards compatibility\n-# Copied from transformers.models.blip.image_processing_blip.BlipImageProcessor with Blip->InstructBlipVideo, BLIP->InstructBLIPVideo\n-class InstructBlipVideoImageProcessor(BaseImageProcessor):\n-    r\"\"\"\n-    Constructs a InstructBLIPVideo image processor.\n-\n-    Args:\n-        do_resize (`bool`, *optional*, defaults to `True`):\n-            Whether to resize the image's (height, width) dimensions to the specified `size`. Can be overridden by the\n-            `do_resize` parameter in the `preprocess` method.\n-        size (`dict`, *optional*, defaults to `{\"height\": 384, \"width\": 384}`):\n-            Size of the output image after resizing. Can be overridden by the `size` parameter in the `preprocess`\n-            method.\n-        resample (`PILImageResampling`, *optional*, defaults to `Resampling.BICUBIC`):\n-            Resampling filter to use if resizing the image. Only has an effect if `do_resize` is set to `True`. Can be\n-            overridden by the `resample` parameter in the `preprocess` method.\n-        do_rescale (`bool`, *optional*, defaults to `True`):\n-            Whether to rescale the image by the specified scale `rescale_factor`. Can be overridden by the\n-            `do_rescale` parameter in the `preprocess` method.\n-        rescale_factor (`int` or `float`, *optional*, defaults to `1/255`):\n-            Scale factor to use if rescaling the image. Only has an effect if `do_rescale` is set to `True`. Can be\n-            overridden by the `rescale_factor` parameter in the `preprocess` method.\n-        do_normalize (`bool`, *optional*, defaults to `True`):\n-            Whether to normalize the image. Can be overridden by the `do_normalize` parameter in the `preprocess`\n-            method. Can be overridden by the `do_normalize` parameter in the `preprocess` method.\n-        image_mean (`float` or `list[float]`, *optional*, defaults to `IMAGENET_STANDARD_MEAN`):\n-            Mean to use if normalizing the image. This is a float or list of floats the length of the number of\n-            channels in the image. Can be overridden by the `image_mean` parameter in the `preprocess` method. Can be\n-            overridden by the `image_mean` parameter in the `preprocess` method.\n-        image_std (`float` or `list[float]`, *optional*, defaults to `IMAGENET_STANDARD_STD`):\n-            Standard deviation to use if normalizing the image. This is a float or list of floats the length of the\n-            number of channels in the image. Can be overridden by the `image_std` parameter in the `preprocess` method.\n-            Can be overridden by the `image_std` parameter in the `preprocess` method.\n-        do_convert_rgb (`bool`, *optional*, defaults to `True`):\n-            Whether to convert the image to RGB.\n-    \"\"\"\n-\n-    model_input_names = [\"pixel_values\"]\n-\n-    def __init__(\n-        self,\n-        do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = PILImageResampling.BICUBIC,\n-        do_rescale: bool = True,\n-        rescale_factor: Union[int, float] = 1 / 255,\n-        do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_convert_rgb: bool = True,\n-        **kwargs,\n-    ) -> None:\n-        super().__init__(**kwargs)\n-        size = size if size is not None else {\"height\": 384, \"width\": 384}\n-        size = get_size_dict(size, default_to_square=True)\n-\n-        self.do_resize = do_resize\n-        self.size = size\n-        self.resample = resample\n-        self.do_rescale = do_rescale\n-        self.rescale_factor = rescale_factor\n-        self.do_normalize = do_normalize\n-        self.image_mean = image_mean if image_mean is not None else OPENAI_CLIP_MEAN\n-        self.image_std = image_std if image_std is not None else OPENAI_CLIP_STD\n-        self.do_convert_rgb = do_convert_rgb\n-\n-    # Copied from transformers.models.vit.image_processing_vit.ViTImageProcessor.resize with PILImageResampling.BILINEAR->PILImageResampling.BICUBIC\n-    def resize(\n-        self,\n-        image: np.ndarray,\n-        size: dict[str, int],\n-        resample: PILImageResampling = PILImageResampling.BICUBIC,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-        **kwargs,\n-    ) -> np.ndarray:\n-        \"\"\"\n-        Resize an image to `(size[\"height\"], size[\"width\"])`.\n-\n-        Args:\n-            image (`np.ndarray`):\n-                Image to resize.\n-            size (`dict[str, int]`):\n-                Dictionary in the format `{\"height\": int, \"width\": int}` specifying the size of the output image.\n-            resample (`PILImageResampling`, *optional*, defaults to `PILImageResampling.BICUBIC`):\n-                `PILImageResampling` filter to use when resizing the image e.g. `PILImageResampling.BICUBIC`.\n-            data_format (`ChannelDimension` or `str`, *optional*):\n-                The channel dimension format for the output image. If unset, the channel dimension format of the input\n-                image is used. Can be one of:\n-                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n-                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n-                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n-            input_data_format (`ChannelDimension` or `str`, *optional*):\n-                The channel dimension format for the input image. If unset, the channel dimension format is inferred\n-                from the input image. Can be one of:\n-                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n-                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n-                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n-\n-        Returns:\n-            `np.ndarray`: The resized image.\n-        \"\"\"\n-        size = get_size_dict(size)\n-        if \"height\" not in size or \"width\" not in size:\n-            raise ValueError(f\"The `size` dictionary must contain the keys `height` and `width`. Got {size.keys()}\")\n-\n-        output_size = (size[\"height\"], size[\"width\"])\n-        return resize(\n-            image,\n-            size=output_size,\n-            resample=resample,\n-            data_format=data_format,\n-            input_data_format=input_data_format,\n-            **kwargs,\n-        )\n-\n-    # Ignore copy\n-    @filter_out_non_signature_kwargs()\n-    def preprocess(\n-        self,\n-        images: Optional[VideoInput] = None,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        do_convert_rgb: Optional[bool] = None,\n-        data_format: ChannelDimension = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-    ) -> BatchFeature:\n-        \"\"\"\n-        Preprocess a video or batch of images/videos.\n-\n-        Args:\n-            videos (`VideoInput`):\n-                Video frames to preprocess. Expects a single or batch of videos as a list of frames with pixel values\n-                ranging from 0 to 255. If passing in video with pixel values between 0 and 1, set `do_rescale=False`.\n-            do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n-                Whether to resize the video.\n-            size (`dict[str, int]`, *optional*, defaults to `self.size`):\n-                Controls the size of the video after `resize`. The shortest edge of the image is resized to\n-                `size[\"shortest_edge\"]` whilst preserving the aspect ratio. If the longest edge of this resized image\n-                is > `int(size[\"shortest_edge\"] * (1333 / 800))`, then the image is resized again to make the longest\n-                edge equal to `int(size[\"shortest_edge\"] * (1333 / 800))`.\n-            resample (`PILImageResampling`, *optional*, defaults to `self.resample`):\n-                Resampling filter to use if resizing the video. Only has an effect if `do_resize` is set to `True`.\n-            do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n-                Whether to rescale the video values between [0 - 1].\n-            rescale_factor (`float`, *optional*, defaults to `self.rescale_factor`):\n-                Rescale factor to rescale the video by if `do_rescale` is set to `True`.\n-            do_normalize (`bool`, *optional*, defaults to `self.do_normalize`):\n-                Whether to normalize the video.\n-            image_mean (`float` or `list[float]`, *optional*, defaults to `self.image_mean`):\n-                Image mean to normalize the video by if `do_normalize` is set to `True`.\n-            image_std (`float` or `list[float]`, *optional*, defaults to `self.image_std`):\n-                Image standard deviation to normalize the video by if `do_normalize` is set to `True`.\n-            do_convert_rgb (`bool`, *optional*, defaults to `self.do_convert_rgb`):\n-                Whether to convert the image to RGB.\n-            return_tensors (`str` or `TensorType`, *optional*):\n-                The type of tensors to return. Can be one of:\n-                    - Unset: Return a list of `np.ndarray`.\n-                    - `TensorType.PYTORCH` or `'pt'`: Return a batch of type `torch.Tensor`.\n-                    - `TensorType.NUMPY` or `'np'`: Return a batch of type `np.ndarray`.\n-            data_format (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`):\n-                The channel dimension format for the output image. Can be one of:\n-                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n-                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n-                - Unset: Use the channel dimension format of the input image.\n-            input_data_format (`ChannelDimension` or `str`, *optional*):\n-                The channel dimension format for the input image. If unset, the channel dimension format is inferred\n-                from the input image. Can be one of:\n-                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n-                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n-                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n-        \"\"\"\n-        do_resize = do_resize if do_resize is not None else self.do_resize\n-        resample = resample if resample is not None else self.resample\n-        do_rescale = do_rescale if do_rescale is not None else self.do_rescale\n-        rescale_factor = rescale_factor if rescale_factor is not None else self.rescale_factor\n-        do_normalize = do_normalize if do_normalize is not None else self.do_normalize\n-        image_mean = image_mean if image_mean is not None else self.image_mean\n-        image_std = image_std if image_std is not None else self.image_std\n-        do_convert_rgb = do_convert_rgb if do_convert_rgb is not None else self.do_convert_rgb\n-\n-        size = size if size is not None else self.size\n-        size = get_size_dict(size, default_to_square=False)\n-\n-        videos = make_batched_videos(images)\n-        logger.warning(\n-            \"`InstructBlipVideoImageProcessor` is deprecated and will be removed in v5.0. \"\n-            \"We recommend to load an instance of `InstructBlipVideoVideoProcessor` to process videos for the model. \"\n-        )\n-\n-        validate_preprocess_arguments(\n-            do_rescale=do_rescale,\n-            rescale_factor=rescale_factor,\n-            do_normalize=do_normalize,\n-            image_mean=image_mean,\n-            image_std=image_std,\n-            do_resize=do_resize,\n-            size=size,\n-            resample=resample,\n-        )\n-\n-        if not valid_images(videos):\n-            raise ValueError(\"Invalid input type. Must be of type PIL.Image.Image, numpy.ndarray, or torch.Tensor\")\n-\n-        pixel_values = [\n-            [\n-                self._preprocess_image(\n-                    image=frame,\n-                    do_resize=do_resize,\n-                    size=size,\n-                    resample=resample,\n-                    do_rescale=do_rescale,\n-                    rescale_factor=rescale_factor,\n-                    do_normalize=do_normalize,\n-                    image_mean=image_mean,\n-                    image_std=image_std,\n-                    do_convert_rgb=do_convert_rgb,\n-                    data_format=data_format,\n-                    input_data_format=input_data_format,\n-                )\n-                for frame in video\n-            ]\n-            for video in videos\n-        ]\n-\n-        encoded_outputs = BatchFeature(data={\"pixel_values\": pixel_values}, tensor_type=return_tensors)\n-        return encoded_outputs\n-\n-    # Ignore copy\n-    def _preprocess_image(\n-        self,\n-        image: Optional[ImageInput] = None,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_convert_rgb: Optional[bool] = None,\n-        data_format: ChannelDimension = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-    ) -> np.ndarray:\n-        # PIL RGBA images are converted to RGB\n-        if do_convert_rgb:\n-            image = convert_to_rgb(image)\n-\n-        # All transformations expect numpy arrays.\n-        image = to_numpy_array(image)\n-\n-        if do_rescale and is_scaled_image(image):\n-            logger.warning_once(\n-                \"It looks like you are trying to rescale already rescaled video frames. If the input\"\n-                \" images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\"\n-            )\n-\n-        if input_data_format is None:\n-            # We assume that all images have the same channel dimension format.\n-            input_data_format = infer_channel_dimension_format(image)\n-\n-        if do_resize:\n-            image = self.resize(image=image, size=size, resample=resample, input_data_format=input_data_format)\n-\n-        if do_rescale:\n-            image = self.rescale(image=image, scale=rescale_factor, input_data_format=input_data_format)\n-\n-        if do_normalize:\n-            image = self.normalize(image=image, mean=image_mean, std=image_std, input_data_format=input_data_format)\n-\n-        image = to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format)\n-\n-        return image\n-\n-\n-__all__ = [\"InstructBlipVideoImageProcessor\"]"
      },
      {
        "filename": "src/transformers/models/instructblipvideo/processing_instructblipvideo.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -41,7 +41,7 @@ class InstructBlipVideoProcessor(ProcessorMixin):\n     Constructs an InstructBLIPVideo processor which wraps a InstructBLIP image processor and a LLaMa/T5 tokenizer into a single\n     processor.\n \n-    [`InstructBlipVideoProcessor`] offers all the functionalities of [`InstructBlipVideoImageProcessor`] and [`AutoTokenizer`]. See the\n+    [`InstructBlipVideoProcessor`] offers all the functionalities of [`InstructBlipVideoVideoProcessor`] and [`AutoTokenizer`]. See the\n     docstring of [`~InstructBlipVideoProcessor.__call__`] and [`~InstructBlipVideoProcessor.decode`] for more information.\n \n     Args:"
      },
      {
        "filename": "src/transformers/models/llava_next_video/convert_llava_next_video_weights_to_hf.py",
        "status": "modified",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "patch": "@@ -34,8 +34,8 @@\n     LlavaNextImageProcessor,\n     LlavaNextVideoConfig,\n     LlavaNextVideoForConditionalGeneration,\n-    LlavaNextVideoImageProcessor,\n     LlavaNextVideoProcessor,\n+    LlavaNextVideoVideoProcessor,\n )\n \n \n@@ -187,7 +187,7 @@ def convert_llava_to_hf(model_id, pytorch_dump_folder_path, push_to_hub=False):\n     tokenizer.add_tokens(AddedToken(\"<image>\", special=True, normalized=False), special_tokens=True)\n \n     image_processor = LlavaNextImageProcessor.from_pretrained(vision_model_id)\n-    video_processor = LlavaNextVideoImageProcessor.from_pretrained(vision_model_id)\n+    video_processor = LlavaNextVideoVideoProcessor.from_pretrained(vision_model_id)\n     processor = LlavaNextVideoProcessor(\n         tokenizer=tokenizer,\n         video_processor=video_processor,"
      },
      {
        "filename": "src/transformers/models/llava_next_video/image_processing_llava_next_video.py",
        "status": "removed",
        "additions": 0,
        "deletions": 401,
        "changes": 401,
        "patch": "@@ -1,401 +0,0 @@\n-# coding=utf-8\n-# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Image processor class for LLaVa-NeXT-Video.\"\"\"\n-\n-from typing import Optional, Union\n-\n-import numpy as np\n-\n-from ...image_processing_utils import BaseImageProcessor, BatchFeature, get_size_dict\n-from ...image_transforms import (\n-    convert_to_rgb,\n-    get_resize_output_image_size,\n-    resize,\n-    to_channel_dimension_format,\n-)\n-from ...image_utils import (\n-    OPENAI_CLIP_MEAN,\n-    OPENAI_CLIP_STD,\n-    ChannelDimension,\n-    ImageInput,\n-    PILImageResampling,\n-    infer_channel_dimension_format,\n-    is_scaled_image,\n-    make_flat_list_of_images,\n-    to_numpy_array,\n-    validate_preprocess_arguments,\n-)\n-from ...utils import TensorType, logging\n-from ...video_utils import VideoInput, make_batched_videos\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-\n-class LlavaNextVideoImageProcessor(BaseImageProcessor):\n-    r\"\"\"\n-    Constructs a LLaVa-NeXT-Video video processor. Based on [`CLIPImageProcessor`] with incorporation of processing each video frame.\n-\n-    Args:\n-        do_resize (`bool`, *optional*, defaults to `True`):\n-            Whether to resize the image's (height, width) dimensions to the specified `size`. Can be overridden by\n-            `do_resize` in the `preprocess` method.\n-        size (`dict[str, int]` *optional*, defaults to `{\"shortest_edge\": 224}`):\n-            Size of the image after resizing. The shortest edge of the image is resized to size[\"shortest_edge\"], with\n-            the longest edge resized to keep the input aspect ratio. Can be overridden by `size` in the `preprocess`\n-            method.\n-        image_grid_pinpoints (`List` *optional*, defaults to `[[672, 336], [336, 672], [672, 672], [336, 1008], [1008, 336]]`):\n-            A list of possible resolutions to use for processing high resolution images. The best resolution is selected\n-            based on the original size of the image. Can be overridden by `image_grid_pinpoints` in the `preprocess`\n-            method. Not used for processing videos.\n-        resample (`PILImageResampling`, *optional*, defaults to `Resampling.BICUBIC`):\n-            Resampling filter to use if resizing the image. Can be overridden by `resample` in the `preprocess` method.\n-        do_center_crop (`bool`, *optional*, defaults to `True`):\n-            Whether to center crop the image to the specified `crop_size`. Can be overridden by `do_center_crop` in the\n-            `preprocess` method.\n-        crop_size (`dict[str, int]` *optional*, defaults to 224):\n-            Size of the output image after applying `center_crop`. Can be overridden by `crop_size` in the `preprocess`\n-            method.\n-        do_rescale (`bool`, *optional*, defaults to `True`):\n-            Whether to rescale the image by the specified scale `rescale_factor`. Can be overridden by `do_rescale` in\n-            the `preprocess` method.\n-        rescale_factor (`int` or `float`, *optional*, defaults to `1/255`):\n-            Scale factor to use if rescaling the image. Can be overridden by `rescale_factor` in the `preprocess`\n-            method.\n-        do_normalize (`bool`, *optional*, defaults to `True`):\n-            Whether to normalize the image. Can be overridden by `do_normalize` in the `preprocess` method.\n-        image_mean (`float` or `list[float]`, *optional*, defaults to `[0.48145466, 0.4578275, 0.40821073]`):\n-            Mean to use if normalizing the image. This is a float or list of floats the length of the number of\n-            channels in the image. Can be overridden by the `image_mean` parameter in the `preprocess` method.\n-        image_std (`float` or `list[float]`, *optional*, defaults to `[0.26862954, 0.26130258, 0.27577711]`):\n-            Standard deviation to use if normalizing the image. This is a float or list of floats the length of the\n-            number of channels in the image. Can be overridden by the `image_std` parameter in the `preprocess` method.\n-            Can be overridden by the `image_std` parameter in the `preprocess` method.\n-        do_convert_rgb (`bool`, *optional*, defaults to `True`):\n-            Whether to convert the image to RGB.\n-    \"\"\"\n-\n-    model_input_names = [\"pixel_values_videos\"]\n-\n-    def __init__(\n-        self,\n-        do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n-        image_grid_pinpoints: Optional[list] = None,\n-        resample: PILImageResampling = PILImageResampling.BICUBIC,\n-        do_center_crop: bool = True,\n-        crop_size: Optional[dict[str, int]] = None,\n-        do_rescale: bool = True,\n-        rescale_factor: Union[int, float] = 1 / 255,\n-        do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_convert_rgb: bool = True,\n-        **kwargs,\n-    ) -> None:\n-        super().__init__(**kwargs)\n-        size = size if size is not None else {\"shortest_edge\": 224}\n-        size = get_size_dict(size, default_to_square=False)\n-        crop_size = crop_size if crop_size is not None else {\"height\": 224, \"width\": 224}\n-        crop_size = get_size_dict(crop_size, default_to_square=True, param_name=\"crop_size\")\n-\n-        self.do_resize = do_resize\n-        self.size = size\n-        self.image_grid_pinpoints = image_grid_pinpoints\n-        self.resample = resample\n-        self.do_center_crop = do_center_crop\n-        self.crop_size = crop_size\n-        self.do_rescale = do_rescale\n-        self.rescale_factor = rescale_factor\n-        self.do_normalize = do_normalize\n-        self.image_mean = image_mean if image_mean is not None else OPENAI_CLIP_MEAN\n-        self.image_std = image_std if image_std is not None else OPENAI_CLIP_STD\n-        self.do_convert_rgb = do_convert_rgb\n-\n-    # Copied from transformers.models.clip.image_processing_clip.CLIPImageProcessor.resize with CLIP->LLaVa\n-    def resize(\n-        self,\n-        image: np.ndarray,\n-        size: dict[str, int],\n-        resample: PILImageResampling = PILImageResampling.BICUBIC,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-        **kwargs,\n-    ) -> np.ndarray:\n-        \"\"\"\n-        Resize an image. The shortest edge of the image is resized to size[\"shortest_edge\"], with the longest edge\n-        resized to keep the input aspect ratio.\n-\n-        Args:\n-            image (`np.ndarray`):\n-                Image to resize.\n-            size (`dict[str, int]`):\n-                Size of the output image.\n-            resample (`PILImageResampling`, *optional*, defaults to `PILImageResampling.BICUBIC`):\n-                Resampling filter to use when resiizing the image.\n-            data_format (`str` or `ChannelDimension`, *optional*):\n-                The channel dimension format of the image. If not provided, it will be the same as the input image.\n-            input_data_format (`ChannelDimension` or `str`, *optional*):\n-                The channel dimension format of the input image. If not provided, it will be inferred.\n-        \"\"\"\n-        default_to_square = True\n-        if \"shortest_edge\" in size:\n-            size = size[\"shortest_edge\"]\n-            default_to_square = False\n-        elif \"height\" in size and \"width\" in size:\n-            size = (size[\"height\"], size[\"width\"])\n-        else:\n-            raise ValueError(\"Size must contain either 'shortest_edge' or 'height' and 'width'.\")\n-\n-        output_size = get_resize_output_image_size(\n-            image,\n-            size=size,\n-            default_to_square=default_to_square,\n-            input_data_format=input_data_format,\n-        )\n-\n-        return resize(\n-            image,\n-            size=output_size,\n-            resample=resample,\n-            data_format=data_format,\n-            input_data_format=input_data_format,\n-            **kwargs,\n-        )\n-\n-    def _preprocess(\n-        self,\n-        images: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_center_crop: Optional[bool] = None,\n-        crop_size: Optional[int] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_convert_rgb: Optional[bool] = None,\n-        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-    ) -> list[np.ndarray]:\n-        \"\"\"\n-        Preprocess an image or batch of images. Copy of the `preprocess` method from `CLIPImageProcessor`.\n-\n-        Args:\n-            images (`ImageInput`):\n-                Batch of frames (one video) to preprocess. Expects a batch of frames with pixel values ranging from 0 to 255. If\n-                passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n-            do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n-                Whether to resize the image.\n-            size (`dict[str, int]`, *optional*, defaults to `self.size`):\n-                Size of the image after resizing. Shortest edge of the image is resized to size[\"shortest_edge\"], with\n-                the longest edge resized to keep the input aspect ratio.\n-            resample (`int`, *optional*, defaults to `self.resample`):\n-                Resampling filter to use if resizing the image. This can be one of the enum `PILImageResampling`. Only\n-                has an effect if `do_resize` is set to `True`.\n-            do_center_crop (`bool`, *optional*, defaults to `self.do_center_crop`):\n-                Whether to center crop the image.\n-            crop_size (`dict[str, int]`, *optional*, defaults to `self.crop_size`):\n-                Size of the center crop. Only has an effect if `do_center_crop` is set to `True`.\n-            do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n-                Whether to rescale the image.\n-            rescale_factor (`float`, *optional*, defaults to `self.rescale_factor`):\n-                Rescale factor to rescale the image by if `do_rescale` is set to `True`.\n-            do_normalize (`bool`, *optional*, defaults to `self.do_normalize`):\n-                Whether to normalize the image.\n-            image_mean (`float` or `list[float]`, *optional*, defaults to `self.image_mean`):\n-                Image mean to use for normalization. Only has an effect if `do_normalize` is set to `True`.\n-            image_std (`float` or `list[float]`, *optional*, defaults to `self.image_std`):\n-                Image standard deviation to use for normalization. Only has an effect if `do_normalize` is set to\n-                `True`.\n-            data_format (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`):\n-                The channel dimension format for the output image. Can be one of:\n-                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n-                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n-                - Unset: Use the channel dimension format of the input image.\n-            input_data_format (`ChannelDimension` or `str`, *optional*):\n-                The channel dimension format for the input image. If unset, the channel dimension format is inferred\n-                from the input image. Can be one of:\n-                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n-                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n-                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n-        \"\"\"\n-        images = make_flat_list_of_images(images)\n-\n-        if do_convert_rgb:\n-            images = [convert_to_rgb(image) for image in images]\n-\n-        # All transformations expect numpy arrays.\n-        images = [to_numpy_array(image) for image in images]\n-\n-        if do_rescale and is_scaled_image(images[0]):\n-            logger.warning_once(\n-                \"It looks like you are trying to rescale already rescaled images. If the input\"\n-                \" images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\"\n-            )\n-        if input_data_format is None:\n-            # We assume that all images have the same channel dimension format.\n-            input_data_format = infer_channel_dimension_format(images[0])\n-\n-        all_images = []\n-        for image in images:\n-            if do_resize:\n-                image = self.resize(image=image, size=size, resample=resample, input_data_format=input_data_format)\n-\n-            if do_center_crop:\n-                image = self.center_crop(image=image, size=crop_size, input_data_format=input_data_format)\n-\n-            if do_rescale:\n-                image = self.rescale(image=image, scale=rescale_factor, input_data_format=input_data_format)\n-\n-            if do_normalize:\n-                image = self.normalize(\n-                    image=image, mean=image_mean, std=image_std, input_data_format=input_data_format\n-                )\n-\n-            all_images.append(image)\n-        images = [\n-            to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format)\n-            for image in all_images\n-        ]\n-\n-        return images\n-\n-    def preprocess(\n-        self,\n-        images: VideoInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_center_crop: Optional[bool] = None,\n-        crop_size: Optional[int] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_convert_rgb: Optional[bool] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-    ):\n-        \"\"\"\n-        Args:\n-            images (`VideoInput`):\n-                Videos to preprocess. Expects a single or batch of videos with pixel values ranging from 0 to 255. If\n-                passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n-            do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n-                Whether to resize the video.\n-            size (`dict[str, int]`, *optional*, defaults to `self.size`):\n-                Size of the video after resizing. Shortest edge of the video is resized to size[\"shortest_edge\"], with\n-                the longest edge resized to keep the input aspect ratio.\n-            resample (`int`, *optional*, defaults to `self.resample`):\n-                Resampling filter to use if resizing the video. This can be one of the enum `PILImageResampling`. Only\n-                has an effect if `do_resize` is set to `True`.\n-            do_center_crop (`bool`, *optional*, defaults to `self.do_center_crop`):\n-                Whether to center crop the video.\n-            crop_size (`dict[str, int]`, *optional*, defaults to `self.crop_size`):\n-                Size of the center crop. Only has an effect if `do_center_crop` is set to `True`.\n-            do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n-                Whether to rescale the video.\n-            rescale_factor (`float`, *optional*, defaults to `self.rescale_factor`):\n-                Rescale factor to rescale the video by if `do_rescale` is set to `True`.\n-            do_normalize (`bool`, *optional*, defaults to `self.do_normalize`):\n-                Whether to normalize the video.\n-            image_mean (`float` or `list[float]`, *optional*, defaults to `self.image_mean`):\n-                Frame mean to use for normalization. Only has an effect if `do_normalize` is set to `True`.\n-            image_std (`float` or `list[float]`, *optional*, defaults to `self.image_std`):\n-                Frame standard deviation to use for normalization. Only has an effect if `do_normalize` is set to\n-                `True`.\n-            do_convert_rgb (`bool`, *optional*, defaults to `self.do_convert_rgb`):\n-                Whether to convert the video to RGB.\n-            return_tensors (`str` or `TensorType`, *optional*):\n-                The type of tensors to return. Can be one of:\n-                - Unset: Return a list of `np.ndarray`.\n-                - `TensorType.PYTORCH` or `'pt'`: Return a batch of type `torch.Tensor`.\n-                - `TensorType.NUMPY` or `'np'`: Return a batch of type `np.ndarray`.\n-            data_format (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`):\n-                The channel dimension format for the output image. Can be one of:\n-                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n-                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n-                - Unset: Use the channel dimension format of the input image.\n-            input_data_format (`ChannelDimension` or `str`, *optional*):\n-                The channel dimension format for the input image. If unset, the channel dimension format is inferred\n-                from the input image. Can be one of:\n-                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n-                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n-                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n-        \"\"\"\n-        do_resize = do_resize if do_resize is not None else self.do_resize\n-        size = size if size is not None else self.size\n-        size = get_size_dict(size, param_name=\"size\", default_to_square=False)\n-        resample = resample if resample is not None else self.resample\n-        do_center_crop = do_center_crop if do_center_crop is not None else self.do_center_crop\n-        crop_size = crop_size if crop_size is not None else self.crop_size\n-        crop_size = get_size_dict(crop_size, param_name=\"crop_size\", default_to_square=True)\n-        do_rescale = do_rescale if do_rescale is not None else self.do_rescale\n-        rescale_factor = rescale_factor if rescale_factor is not None else self.rescale_factor\n-        do_normalize = do_normalize if do_normalize is not None else self.do_normalize\n-        image_mean = image_mean if image_mean is not None else self.image_mean\n-        image_std = image_std if image_std is not None else self.image_std\n-        do_convert_rgb = do_convert_rgb if do_convert_rgb is not None else self.do_convert_rgb\n-\n-        images = self.fetch_images(images)\n-        images = make_batched_videos(images)\n-        logger.warning(\n-            \"`LlavaNextVideoImageProcessor` is deprecated and will be removed in v5.0. \"\n-            \"We recommend to load an instance of `LlavaNextVideoVideoProcessor` to process videos for the model. \"\n-        )\n-\n-        validate_preprocess_arguments(\n-            do_rescale=do_rescale,\n-            rescale_factor=rescale_factor,\n-            do_normalize=do_normalize,\n-            image_mean=image_mean,\n-            image_std=image_std,\n-            do_center_crop=do_center_crop,\n-            crop_size=crop_size,\n-            do_resize=do_resize,\n-            size=size,\n-            resample=resample,\n-        )\n-\n-        # preprocess each video frame by frame\n-        pixel_values = [\n-            self._preprocess(\n-                frames,\n-                do_resize=do_resize,\n-                size=size,\n-                resample=resample,\n-                do_center_crop=do_center_crop,\n-                crop_size=crop_size,\n-                do_rescale=do_rescale,\n-                rescale_factor=rescale_factor,\n-                do_normalize=do_normalize,\n-                image_mean=image_mean,\n-                image_std=image_std,\n-                data_format=data_format,\n-                input_data_format=input_data_format,\n-            )\n-            for frames in images\n-        ]\n-\n-        data = {\"pixel_values_videos\": pixel_values}\n-        return BatchFeature(data=data, tensor_type=return_tensors)\n-\n-\n-__all__ = [\"LlavaNextVideoImageProcessor\"]"
      },
      {
        "filename": "src/transformers/models/llava_next_video/processing_llava_next_video.py",
        "status": "modified",
        "additions": 3,
        "deletions": 3,
        "changes": 6,
        "patch": "@@ -49,7 +49,7 @@ class LlavaNextVideoProcessor(ProcessorMixin):\n     Constructs a LLaVa-NeXT-Video processor which wraps a LLaVa-NeXT image processor, LLaVa-NeXT-Video video processor and\n     a LLaMa tokenizer into a single processor.\n \n-    [`LlavaNextVideoProcessor`] offers all the functionalities of [`LlavaNextImageProcessor`], [`LlavaNextVideoImageProcessor`] and\n+    [`LlavaNextVideoProcessor`] offers all the functionalities of [`LlavaNextImageProcessor`], [`LlavaNextVideoVideoProcessor`] and\n     [`LlamaTokenizerFast`]. See the [`~LlavaNextVideoProcessor.__call__`] and [`~LlavaNextVideoProcessor.decode`] for more information.\n \n     Args:\n@@ -124,8 +124,8 @@ def __call__(\n         and `kwargs` arguments to LlamaTokenizerFast's [`~LlamaTokenizerFast.__call__`] if `text` is not `None` to encode\n         the text. To prepare the image(s), this method forwards the `images` and `kwargs` arguments to\n         LlavaNextImageProcessor's [`~LlavaNextImageProcessor.__call__`] if `images` is not `None`. To prepare the video(s),\n-        this method forwards the `videos` and `kwargs` arguments to LlavaNextVideoImageProcessor's\n-        [`~LlavaNextVideoImageProcessor.__call__`] if `videos` is not `None`. Please refer to the docstring\n+        this method forwards the `videos` and `kwargs` arguments to LlavaNextVideoVideoProcessor's\n+        [`~LlavaNextVideoVideoProcessor.__call__`] if `videos` is not `None`. Please refer to the docstring\n         of the above two methods for more information.\n \n         Args:"
      },
      {
        "filename": "src/transformers/models/qwen2_5_omni/processing_qwen2_5_omni.py",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -341,11 +341,13 @@ def model_input_names(self):\n         tokenizer_input_names = self.tokenizer.model_input_names\n         feature_extractor_input_names = self.feature_extractor.model_input_names\n         image_processor_input_names = self.image_processor.model_input_names\n+        video_processor_input_names = self.video_processor.model_input_names\n         return list(\n             dict.fromkeys(\n                 tokenizer_input_names\n                 + feature_extractor_input_names\n                 + image_processor_input_names\n+                + video_processor_input_names\n                 + [\"feature_attention_mask\"]\n                 + [\"video_second_per_grid\"]\n             )"
      },
      {
        "filename": "src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py",
        "status": "modified",
        "additions": 4,
        "deletions": 1,
        "changes": 5,
        "patch": "@@ -858,7 +858,10 @@ class Qwen2_5_VLProcessor(Qwen2VLProcessor):\n     def model_input_names(self):\n         tokenizer_input_names = self.tokenizer.model_input_names\n         image_processor_input_names = self.image_processor.model_input_names\n-        names_from_processor = list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n+        video_processor_input_names = self.video_processor.model_input_names\n+        names_from_processor = list(\n+            dict.fromkeys(tokenizer_input_names + image_processor_input_names + video_processor_input_names)\n+        )\n         return names_from_processor + [\"second_per_grid_ts\"]\n \n     def __call__("
      },
      {
        "filename": "src/transformers/models/qwen2_5_vl/processing_qwen2_5_vl.py",
        "status": "modified",
        "additions": 4,
        "deletions": 1,
        "changes": 5,
        "patch": "@@ -255,7 +255,10 @@ def post_process_image_text_to_text(\n     def model_input_names(self):\n         tokenizer_input_names = self.tokenizer.model_input_names\n         image_processor_input_names = self.image_processor.model_input_names\n-        names_from_processor = list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n+        video_processor_input_names = self.video_processor.model_input_names\n+        names_from_processor = list(\n+            dict.fromkeys(tokenizer_input_names + image_processor_input_names + video_processor_input_names)\n+        )\n         return names_from_processor + [\"second_per_grid_ts\"]\n \n "
      },
      {
        "filename": "src/transformers/models/qwen2_vl/image_processing_qwen2_vl.py",
        "status": "modified",
        "additions": 25,
        "deletions": 66,
        "changes": 91,
        "patch": "@@ -46,7 +46,7 @@\n )\n from ...processing_utils import ImagesKwargs\n from ...utils import TensorType, logging\n-from ...video_utils import VideoInput, make_batched_videos\n+from ...video_utils import VideoInput\n \n \n logger = logging.get_logger(__name__)\n@@ -137,7 +137,7 @@ class Qwen2VLImageProcessor(BaseImageProcessor):\n             The merge size of the vision encoder to llm encoder.\n     \"\"\"\n \n-    model_input_names = [\"pixel_values\", \"image_grid_thw\", \"pixel_values_videos\", \"video_grid_thw\"]\n+    model_input_names = [\"pixel_values\", \"image_grid_thw\"]\n     valid_kwargs = Qwen2VLImageProcessorKwargs\n \n     def __init__(\n@@ -322,7 +322,6 @@ def _preprocess(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        videos: Optional[VideoInput] = None,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n         min_pixels: Optional[int] = None,\n@@ -346,9 +345,6 @@ def preprocess(\n             images (`ImageInput`):\n                 Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If\n                 passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n-            videos (`VideoInput`):\n-                Video to preprocess. Expects a single or batch of videos with pixel values ranging from 0 to 255. If\n-                passing in videos with pixel values between 0 and 1, set `do_rescale=False`.\n             do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n                 Whether to resize the image.\n             size (`dict[str, int]`, *optional*, defaults to `self.size`):\n@@ -442,67 +438,30 @@ def preprocess(\n         )\n \n         data = {}\n-        if images is not None:\n-            pixel_values, vision_grid_thws = [], []\n-            for image in images:\n-                patches, image_grid_thw = self._preprocess(\n-                    image,\n-                    do_resize=do_resize,\n-                    size=size,\n-                    resample=resample,\n-                    do_rescale=do_rescale,\n-                    rescale_factor=rescale_factor,\n-                    do_normalize=do_normalize,\n-                    image_mean=image_mean,\n-                    image_std=image_std,\n-                    patch_size=patch_size,\n-                    temporal_patch_size=temporal_patch_size,\n-                    merge_size=merge_size,\n-                    data_format=data_format,\n-                    do_convert_rgb=do_convert_rgb,\n-                    input_data_format=input_data_format,\n-                )\n-                pixel_values.extend(patches)\n-                vision_grid_thws.append(image_grid_thw)\n-            pixel_values = np.array(pixel_values)\n-            vision_grid_thws = np.array(vision_grid_thws)\n-            data.update({\"pixel_values\": pixel_values, \"image_grid_thw\": vision_grid_thws})\n-\n-        # kept for BC only and should be removed after v5.0\n-        if videos is not None:\n-            logger.warning(\n-                \"`Qwen2VLImageProcessor` works only with image inputs and doesn't process videos anymore. \"\n-                \"This is a deprecated behavior and will be removed in v5.0. \"\n-                \"Your videos should be forwarded to `Qwen2VLVideoProcessor`. \"\n-            )\n-            videos = make_batched_videos(videos)\n-            pixel_values_videos, vision_grid_thws_videos = [], []\n-            for images in videos:\n-                patches, video_grid_thw = self._preprocess(\n-                    images,\n-                    do_resize=do_resize,\n-                    size=size,\n-                    resample=resample,\n-                    do_rescale=do_rescale,\n-                    rescale_factor=rescale_factor,\n-                    do_normalize=do_normalize,\n-                    image_mean=image_mean,\n-                    image_std=image_std,\n-                    patch_size=patch_size,\n-                    temporal_patch_size=temporal_patch_size,\n-                    merge_size=merge_size,\n-                    data_format=data_format,\n-                    do_convert_rgb=do_convert_rgb,\n-                    input_data_format=input_data_format,\n-                )\n-                pixel_values_videos.extend(patches)\n-                vision_grid_thws_videos.append(video_grid_thw)\n-            data.update(\n-                {\n-                    \"pixel_values_videos\": np.array(pixel_values_videos),\n-                    \"video_grid_thw\": np.array(vision_grid_thws_videos),\n-                }\n+        pixel_values, vision_grid_thws = [], []\n+        for image in images:\n+            patches, image_grid_thw = self._preprocess(\n+                image,\n+                do_resize=do_resize,\n+                size=size,\n+                resample=resample,\n+                do_rescale=do_rescale,\n+                rescale_factor=rescale_factor,\n+                do_normalize=do_normalize,\n+                image_mean=image_mean,\n+                image_std=image_std,\n+                patch_size=patch_size,\n+                temporal_patch_size=temporal_patch_size,\n+                merge_size=merge_size,\n+                data_format=data_format,\n+                do_convert_rgb=do_convert_rgb,\n+                input_data_format=input_data_format,\n             )\n+            pixel_values.extend(patches)\n+            vision_grid_thws.append(image_grid_thw)\n+        pixel_values = np.array(pixel_values)\n+        vision_grid_thws = np.array(vision_grid_thws)\n+        data.update({\"pixel_values\": pixel_values, \"image_grid_thw\": vision_grid_thws})\n \n         return BatchFeature(data=data, tensor_type=return_tensors)\n "
      },
      {
        "filename": "src/transformers/models/qwen2_vl/image_processing_qwen2_vl_fast.py",
        "status": "modified",
        "additions": 6,
        "deletions": 26,
        "changes": 32,
        "patch": "@@ -44,7 +44,6 @@\n     auto_docstring,\n     logging,\n )\n-from ...video_utils import VideoInput, make_batched_videos\n from .image_processing_qwen2_vl import Qwen2VLImageProcessorKwargs, smart_resize\n \n \n@@ -67,7 +66,7 @@ class Qwen2VLImageProcessorFast(BaseImageProcessorFast):\n     min_pixels = None\n     max_pixels = None\n     valid_kwargs = Qwen2VLImageProcessorKwargs\n-    model_input_names = [\"pixel_values\", \"image_grid_thw\", \"pixel_values_videos\", \"video_grid_thw\"]\n+    model_input_names = [\"pixel_values\", \"image_grid_thw\"]\n \n     def __init__(self, **kwargs: Unpack[Qwen2VLImageProcessorKwargs]):\n         size = kwargs.pop(\"size\", None)\n@@ -113,15 +112,13 @@ def _further_process_kwargs(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        videos: Optional[VideoInput] = None,\n         **kwargs: Unpack[Qwen2VLImageProcessorKwargs],\n     ) -> BatchFeature:\n-        return super().preprocess(images, videos, **kwargs)\n+        return super().preprocess(images, **kwargs)\n \n     def _preprocess_image_like_inputs(\n         self,\n         images: ImageInput,\n-        videos: VideoInput,\n         do_convert_rgb: bool,\n         input_data_format: ChannelDimension,\n         device: Optional[Union[str, \"torch.device\"]] = None,\n@@ -134,27 +131,10 @@ def _preprocess_image_like_inputs(\n         \"\"\"\n         # Prepare input images\n         batch_feature = BatchFeature()\n-        if images is not None:\n-            images = self._prepare_image_like_inputs(\n-                images=images, do_convert_rgb=do_convert_rgb, input_data_format=input_data_format, device=device\n-            )\n-            batch_feature = self._preprocess(images, **kwargs)\n-        if videos is not None:\n-            logger.warning(\n-                \"`Qwen2VLImageProcessorFast` works only with image inputs and doesn't process videos anymore. \"\n-                \"This is a deprecated behavior and will be removed in v5.0. \"\n-                \"Your videos should be forwarded to `Qwen2VLVideoProcessor`. \"\n-            )\n-            # Can't change _prepare_images_structure to work with videos because it also needs to work with images.\n-            videos = make_batched_videos(videos)\n-            videos = [\n-                torch.stack(self._prepare_image_like_inputs(video, do_convert_rgb, input_data_format, device))\n-                for video in videos\n-            ]\n-            video_outputs = self._preprocess(videos, **kwargs)\n-            batch_feature.update(\n-                {\"pixel_values_videos\": video_outputs.pixel_values, \"video_grid_thw\": video_outputs.image_grid_thw}\n-            )\n+        images = self._prepare_image_like_inputs(\n+            images=images, do_convert_rgb=do_convert_rgb, input_data_format=input_data_format, device=device\n+        )\n+        batch_feature = self._preprocess(images, **kwargs)\n         return batch_feature\n \n     def _preprocess("
      },
      {
        "filename": "src/transformers/models/qwen3_omni_moe/processing_qwen3_omni_moe.py",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -340,11 +340,13 @@ def model_input_names(self):\n         tokenizer_input_names = self.tokenizer.model_input_names\n         feature_extractor_input_names = self.feature_extractor.model_input_names\n         image_processor_input_names = self.image_processor.model_input_names\n+        video_processor_input_names = self.video_processor.model_input_names\n         return list(\n             dict.fromkeys(\n                 tokenizer_input_names\n                 + feature_extractor_input_names\n                 + image_processor_input_names\n+                + video_processor_input_names\n                 + [\"feature_attention_mask\"]\n                 + [\"video_second_per_grid\"]\n             )"
      },
      {
        "filename": "src/transformers/models/video_llama_3/image_processing_video_llama_3_fast.py",
        "status": "modified",
        "additions": 13,
        "deletions": 44,
        "changes": 57,
        "patch": "@@ -34,14 +34,10 @@\n     SizeDict,\n )\n from ...processing_utils import Unpack\n-from ...utils import TensorType, auto_docstring, logging\n-from ...video_utils import VideoInput, make_batched_videos\n+from ...utils import TensorType, auto_docstring\n from .image_processing_video_llama_3 import VideoLlama3ImageProcessorKwargs\n \n \n-logger = logging.get_logger(__name__)\n-\n-\n def smart_resize(\n     height: int, width: int, factor: int = 28, min_pixels: int = 56 * 56, max_pixels: int = 14 * 14 * 4 * 1280\n ):\n@@ -91,9 +87,6 @@ class VideoLlama3ImageProcessorFast(BaseImageProcessorFast):\n         \"pixel_values\",\n         \"image_grid_thw\",\n         \"image_merge_sizes\",\n-        \"pixel_values_videos\",\n-        \"video_grid_thw\",\n-        \"video_merge_sizes\",\n     ]\n \n     def __init__(self, **kwargs: Unpack[VideoLlama3ImageProcessorKwargs]):\n@@ -140,15 +133,13 @@ def _further_process_kwargs(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        videos: Optional[VideoInput] = None,\n         **kwargs: Unpack[VideoLlama3ImageProcessorKwargs],\n     ) -> BatchFeature:\n-        return super().preprocess(images, videos, **kwargs)\n+        return super().preprocess(images, **kwargs)\n \n     def _preprocess_image_like_inputs(\n         self,\n         images: ImageInput,\n-        videos: VideoInput,\n         do_convert_rgb: bool,\n         input_data_format: ChannelDimension,\n         device: Optional[Union[str, \"torch.device\"]] = None,\n@@ -161,39 +152,17 @@ def _preprocess_image_like_inputs(\n         \"\"\"\n         # Prepare input images\n         batch_feature = BatchFeature()\n-        if images is not None:\n-            if kwargs[\"temporal_patch_size\"] != 1:\n-                raise ValueError(\"`temporal_patch_size` must be 1 for VideoLLaMA3\")\n-            images = self._prepare_image_like_inputs(\n-                images=images, do_convert_rgb=do_convert_rgb, input_data_format=input_data_format, device=device\n-            )\n-            batch_feature = self._preprocess(images, **kwargs)\n-            batch_feature[\"image_merge_sizes\"] = torch.tensor(\n-                [kwargs[\"merge_size\"]] * batch_feature.image_grid_thw.size(0),\n-                dtype=batch_feature.image_grid_thw.dtype,\n-                device=batch_feature.image_grid_thw.device,\n-            )\n-        if videos is not None:\n-            logger.warning(\n-                \"`VideoLlama3ImageProcessorFast` works only with image inputs and doesn't process videos anymore. \"\n-                \"This is a deprecated behavior and will be removed in v5.0. \"\n-                \"Your videos should be forwarded to `VideoLlama3VideoProcessor`. \"\n-            )\n-            # Can't change _prepare_images_structure to work with videos because it also needs to work with images.\n-            videos = make_batched_videos(videos)\n-            videos = [\n-                torch.stack(self._prepare_image_like_inputs(video, do_convert_rgb, input_data_format, device))\n-                for video in videos\n-            ]\n-            video_outputs = self._preprocess(videos, **kwargs)\n-            batch_feature.update(\n-                {\"pixel_values_videos\": video_outputs.pixel_values, \"video_grid_thw\": video_outputs.image_grid_thw}\n-            )\n-            batch_feature[\"video_merge_sizes\"] = torch.tensor(\n-                [kwargs[\"merge_size\"]] * video_outputs.image_grid_thw.size(0),\n-                dtype=video_outputs.image_grid_thw.dtype,\n-                device=video_outputs.image_grid_thw.device,\n-            )\n+        if kwargs[\"temporal_patch_size\"] != 1:\n+            raise ValueError(\"`temporal_patch_size` must be 1 for VideoLLaMA3\")\n+        images = self._prepare_image_like_inputs(\n+            images=images, do_convert_rgb=do_convert_rgb, input_data_format=input_data_format, device=device\n+        )\n+        batch_feature = self._preprocess(images, **kwargs)\n+        batch_feature[\"image_merge_sizes\"] = torch.tensor(\n+            [kwargs[\"merge_size\"]] * batch_feature.image_grid_thw.size(0),\n+            dtype=batch_feature.image_grid_thw.dtype,\n+            device=batch_feature.image_grid_thw.device,\n+        )\n         return batch_feature\n \n     def _preprocess("
      },
      {
        "filename": "src/transformers/models/video_llama_3/modular_video_llama_3.py",
        "status": "modified",
        "additions": 11,
        "deletions": 38,
        "changes": 49,
        "patch": "@@ -50,7 +50,6 @@\n from ...video_utils import (\n     VideoInput,\n     group_videos_by_shape,\n-    make_batched_videos,\n     reorder_videos,\n )\n from ..auto import CONFIG_MAPPING, AutoConfig\n@@ -1446,55 +1445,29 @@ class VideoLlama3ImageProcessorFast(Qwen2VLImageProcessorFast):\n         \"pixel_values\",\n         \"image_grid_thw\",\n         \"image_merge_sizes\",\n-        \"pixel_values_videos\",\n-        \"video_grid_thw\",\n-        \"video_merge_sizes\",\n     ]\n \n     def _preprocess_image_like_inputs(\n         self,\n         images: ImageInput,\n-        videos: VideoInput,\n         do_convert_rgb: bool,\n         input_data_format: ChannelDimension,\n         device: Optional[Union[str, \"torch.device\"]] = None,\n         **kwargs: Unpack[VideoLlama3ImageProcessorKwargs],\n     ) -> BatchFeature:\n         # Prepare input images\n         batch_feature = BatchFeature()\n-        if images is not None:\n-            if kwargs[\"temporal_patch_size\"] != 1:\n-                raise ValueError(\"`temporal_patch_size` must be 1 for VideoLLaMA3\")\n-            images = self._prepare_image_like_inputs(\n-                images=images, do_convert_rgb=do_convert_rgb, input_data_format=input_data_format, device=device\n-            )\n-            batch_feature = self._preprocess(images, **kwargs)\n-            batch_feature[\"image_merge_sizes\"] = torch.tensor(\n-                [kwargs[\"merge_size\"]] * batch_feature.image_grid_thw.size(0),\n-                dtype=batch_feature.image_grid_thw.dtype,\n-                device=batch_feature.image_grid_thw.device,\n-            )\n-        if videos is not None:\n-            logger.warning(\n-                \"`VideoLlama3ImageProcessorFast` works only with image inputs and doesn't process videos anymore. \"\n-                \"This is a deprecated behavior and will be removed in v5.0. \"\n-                \"Your videos should be forwarded to `VideoLlama3VideoProcessor`. \"\n-            )\n-            # Can't change _prepare_images_structure to work with videos because it also needs to work with images.\n-            videos = make_batched_videos(videos)\n-            videos = [\n-                torch.stack(self._prepare_image_like_inputs(video, do_convert_rgb, input_data_format, device))\n-                for video in videos\n-            ]\n-            video_outputs = self._preprocess(videos, **kwargs)\n-            batch_feature.update(\n-                {\"pixel_values_videos\": video_outputs.pixel_values, \"video_grid_thw\": video_outputs.image_grid_thw}\n-            )\n-            batch_feature[\"video_merge_sizes\"] = torch.tensor(\n-                [kwargs[\"merge_size\"]] * video_outputs.image_grid_thw.size(0),\n-                dtype=video_outputs.image_grid_thw.dtype,\n-                device=video_outputs.image_grid_thw.device,\n-            )\n+        if kwargs[\"temporal_patch_size\"] != 1:\n+            raise ValueError(\"`temporal_patch_size` must be 1 for VideoLLaMA3\")\n+        images = self._prepare_image_like_inputs(\n+            images=images, do_convert_rgb=do_convert_rgb, input_data_format=input_data_format, device=device\n+        )\n+        batch_feature = self._preprocess(images, **kwargs)\n+        batch_feature[\"image_merge_sizes\"] = torch.tensor(\n+            [kwargs[\"merge_size\"]] * batch_feature.image_grid_thw.size(0),\n+            dtype=batch_feature.image_grid_thw.dtype,\n+            device=batch_feature.image_grid_thw.device,\n+        )\n         return batch_feature\n \n "
      },
      {
        "filename": "src/transformers/models/video_llava/image_processing_video_llava.py",
        "status": "modified",
        "additions": 19,
        "deletions": 58,
        "changes": 77,
        "patch": "@@ -39,7 +39,6 @@\n     validate_preprocess_arguments,\n )\n from ...utils import TensorType, filter_out_non_signature_kwargs, logging\n-from ...video_utils import VideoInput, make_batched_videos\n \n \n logger = logging.get_logger(__name__)\n@@ -172,7 +171,6 @@ def resize(\n     def preprocess(\n         self,\n         images: Optional[list[ImageInput]] = None,\n-        videos: Optional[list[VideoInput]] = None,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n         resample: Optional[PILImageResampling] = None,\n@@ -195,9 +193,6 @@ def preprocess(\n             images (`ImageInput`, *optional*):\n                 List of images to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If\n                 passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n-            videos (`VideoInput`, *optional*):\n-                List of videos to preprocess. Expects a single or batch of videos with pixel values ranging from 0 to 255. If\n-                passing in videos with pixel values between 0 and 1, set `do_rescale=False`.\n             do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n                 Whether to resize the image.\n             size (`dict[str, int]`, *optional*, defaults to `self.size`):\n@@ -261,60 +256,26 @@ def preprocess(\n         if images is not None and not valid_images(images):\n             raise ValueError(\"Invalid input type. Must be of type PIL.Image.Image, numpy.ndarray, or torch.Tensor\")\n \n-        data = {}\n-        if videos is not None:\n-            logger.warning(\n-                \"`VideoLlavaImageProcessor` works only with image inputs and doesn't process videos anymore. \"\n-                \"This is a deprecated behavior and will be removed in v5.0. \"\n-                \"Your videos should be forwarded to `VideoLlavaVideoProcessor`. \"\n+        pixel_values_images = [\n+            self._preprocess_image(\n+                image=image,\n+                do_resize=do_resize,\n+                size=size,\n+                resample=resample,\n+                do_rescale=do_rescale,\n+                rescale_factor=rescale_factor,\n+                do_normalize=do_normalize,\n+                image_mean=image_mean,\n+                image_std=image_std,\n+                do_center_crop=do_center_crop,\n+                crop_size=crop_size,\n+                do_convert_rgb=do_convert_rgb,\n+                data_format=data_format,\n+                input_data_format=input_data_format,\n             )\n-            videos = make_batched_videos(videos)\n-            pixel_values_videos = [\n-                [\n-                    self._preprocess_image(\n-                        image=frame,\n-                        do_resize=do_resize,\n-                        size=size,\n-                        resample=resample,\n-                        do_rescale=do_rescale,\n-                        rescale_factor=rescale_factor,\n-                        do_normalize=do_normalize,\n-                        image_mean=image_mean,\n-                        image_std=image_std,\n-                        do_center_crop=do_center_crop,\n-                        crop_size=crop_size,\n-                        do_convert_rgb=do_convert_rgb,\n-                        data_format=data_format,\n-                        input_data_format=input_data_format,\n-                    )\n-                    for frame in video\n-                ]\n-                for video in videos\n-            ]\n-            data[\"pixel_values_videos\"] = pixel_values_videos\n-\n-        if images is not None:\n-            pixel_values_images = [\n-                self._preprocess_image(\n-                    image=image,\n-                    do_resize=do_resize,\n-                    size=size,\n-                    resample=resample,\n-                    do_rescale=do_rescale,\n-                    rescale_factor=rescale_factor,\n-                    do_normalize=do_normalize,\n-                    image_mean=image_mean,\n-                    image_std=image_std,\n-                    do_center_crop=do_center_crop,\n-                    crop_size=crop_size,\n-                    do_convert_rgb=do_convert_rgb,\n-                    data_format=data_format,\n-                    input_data_format=input_data_format,\n-                )\n-                for image in images\n-            ]\n-            data[\"pixel_values_images\"] = pixel_values_images\n-\n+            for image in images\n+        ]\n+        data = {\"pixel_values_images\": pixel_values_images}\n         encoded_outputs = BatchFeature(data, tensor_type=return_tensors)\n \n         return encoded_outputs"
      },
      {
        "filename": "tests/models/qwen2_vl/test_image_processing_qwen2_vl.py",
        "status": "modified",
        "additions": 0,
        "deletions": 43,
        "changes": 43,
        "patch": "@@ -274,31 +274,6 @@ def test_nested_input(self):\n             self.assertTrue((encoded_images_nested == encoded_images).all())\n             self.assertTrue((image_grid_thws_nested == expected_image_grid_thws).all())\n \n-    def test_video_inputs(self):\n-        for image_processing_class in self.image_processor_list:\n-            image_processing = image_processing_class(**self.image_processor_dict)\n-            expected_dims_by_frames = {1: 34300, 2: 34300, 3: 68600, 4: 68600, 5: 102900, 6: 102900}\n-\n-            for num_frames, expected_dims in expected_dims_by_frames.items():\n-                image_processor_tester = Qwen2VLImageProcessingTester(self, num_frames=num_frames)\n-                video_inputs = image_processor_tester.prepare_video_inputs(equal_resolution=True)\n-                process_out = image_processing(None, videos=video_inputs, return_tensors=\"pt\")\n-                encoded_video = process_out.pixel_values_videos\n-                expected_output_video_shape = (expected_dims, 1176)\n-                self.assertEqual(tuple(encoded_video.shape), expected_output_video_shape)\n-\n-    def test_custom_patch_size(self):\n-        for image_processing_class in self.image_processor_list:\n-            image_processing = image_processing_class(**self.image_processor_dict)\n-\n-            for patch_size in (1, 3, 5, 7):\n-                image_processor_tester = Qwen2VLImageProcessingTester(self, patch_size=patch_size)\n-                video_inputs = image_processor_tester.prepare_video_inputs(equal_resolution=True)\n-                process_out = image_processing(None, videos=video_inputs, return_tensors=\"pt\")\n-                encoded_video = process_out.pixel_values_videos\n-                expected_output_video_shape = (171500, 1176)\n-                self.assertEqual(tuple(encoded_video.shape), expected_output_video_shape)\n-\n     def test_custom_image_size(self):\n         for image_processing_class in self.image_processor_list:\n             image_processing = image_processing_class(**self.image_processor_dict)\n@@ -325,24 +300,6 @@ def test_custom_pixels(self):\n                 # Just checking that it doesn't raise an error\n                 image_processor(image_inputs, return_tensors=\"pt\")\n \n-    def test_temporal_padding(self):\n-        for image_processing_class in self.image_processor_list:\n-            # Initialize image_processing\n-            image_processing = image_processing_class(**self.image_processor_dict)\n-            # Create random video inputs with a number of frames not divisible by temporal_patch_size\n-            image_processor_tester = Qwen2VLImageProcessingTester(self, num_frames=5, temporal_patch_size=4)\n-            video_inputs = image_processor_tester.prepare_video_inputs(equal_resolution=True)\n-\n-            # Process the video inputs\n-            process_out = image_processing(None, videos=video_inputs, return_tensors=\"pt\")\n-            encoded_video = process_out.pixel_values_videos\n-\n-            # Check the shape after padding\n-            expected_output_video_shape = (102900, 1176)  # Adjusted based on padding\n-            self.assertEqual(tuple(encoded_video.shape), expected_output_video_shape)\n-            # Check divisibility by temporal_patch_size\n-            self.assertEqual(encoded_video.shape[0] % 4, 0)\n-\n     @require_vision\n     @require_torch\n     def test_slow_fast_equivalence(self):"
      }
    ],
    "num_files": 19,
    "scraped_at": "2025-11-16T21:17:29.105375"
  },
  {
    "pr_number": 41605,
    "title": "Fix fp32_ln for various models",
    "body": "This PR fixes the test `test_flash_attn_2_fp32_ln` for several models:\r\n- `bark` was failing the test because it call `_flash_attention_forward` directly without checking the `queries` dtype, and so the test could fail if the dtype was `torch.float32`. To fix this we re-factored out a code block into a function  `get_target_dtype`  that takes care of infering whether to cast the fp32 tesnor to fp16 or bf16, and added a called to it before the call to FA\r\n- same for `stablelm`\r\n- `mllama` was failing the test because `MllamaTextSelfAttention` lacks the `is_causal`attribute, which was added and set to True (it's a text attention so it's causal, as discussed in #39182)\r\n- same for `kosmos2` but the test still fails for many many other reasons\r\n\r\nThe list of fixed test is here:\r\n```\r\nFAILED tests/models/bark/test_modeling_bark.py::BarkSemanticModelTest::test_flash_attn_2_fp32_ln - RuntimeError: FlashAttention only support fp16 and bf16 data type\r\nFAILED tests/models/bark/test_modeling_bark.py::BarkCoarseModelTest::test_flash_attn_2_fp32_ln - RuntimeError: FlashAttention only support fp16 and bf16 data type\r\nFAILED tests/models/mllama/test_modeling_mllama.py::MllamaForCausalLMModelTest::test_flash_attn_2_fp32_ln - AttributeError: 'MllamaTextSelfAttention' object has no attribute 'is_causal'\r\nFAILED tests/models/mllama/test_modeling_mllama.py::MllamaForConditionalGenerationModelTest::test_flash_attn_2_fp32_ln - AttributeError: 'MllamaTextSelfAttention' object has no attribute 'is_causal'\r\nFAILED tests/models/stablelm/test_modeling_stablelm.py::StableLmModelTest::test_flash_attn_2_fp32_ln - RuntimeError: FlashAttention only support fp16 and bf16 data type\r\n```",
    "html_url": "https://github.com/huggingface/transformers/pull/41605",
    "created_at": "2025-10-15T09:44:32Z",
    "merged_at": "2025-10-16T12:18:50Z",
    "merge_commit_sha": "2935a1be19f12176c455cb65d67dc5a3bb84cd77",
    "base_ref": "main",
    "head_sha": "1bb5bcef2c49aa77d8380189feae8418bbd83296",
    "user": "remi-or",
    "files": [
      {
        "filename": "src/transformers/integrations/flash_attention.py",
        "status": "modified",
        "additions": 14,
        "deletions": 9,
        "changes": 23,
        "patch": "@@ -11,6 +11,19 @@\n _use_top_left_mask = flash_attn_supports_top_left_mask()\n \n \n+def get_target_dtype(query: torch.Tensor, module: torch.nn.Module) -> torch.dtype:\n+    \"\"\"If the query is in float32, return a target dtype compatible with flash attention. Return None otherwise.\"\"\"\n+    if query.dtype == torch.float32:\n+        if torch.is_autocast_enabled():\n+            return torch.get_autocast_gpu_dtype()\n+        # Handle the case where the model is quantized\n+        elif hasattr(module.config, \"_pre_quantization_dtype\"):\n+            return module.config._pre_quantization_dtype\n+        else:\n+            return next(layer for layer in module.modules() if isinstance(layer, torch.nn.Linear)).weight.dtype\n+    return None\n+\n+\n def flash_attention_forward(\n     module: torch.nn.Module,\n     query: torch.Tensor,\n@@ -48,15 +61,7 @@ def flash_attention_forward(\n     # cast them back in the correct dtype just to be sure everything works as expected.\n     # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n     # in fp32. (usually our RMSNorm modules handle it correctly)\n-    target_dtype = None\n-    if query.dtype == torch.float32:\n-        if torch.is_autocast_enabled():\n-            target_dtype = torch.get_autocast_gpu_dtype()\n-        # Handle the case where the model is quantized\n-        elif hasattr(module.config, \"_pre_quantization_dtype\"):\n-            target_dtype = module.config._pre_quantization_dtype\n-        else:\n-            target_dtype = next(layer for layer in module.modules() if isinstance(layer, torch.nn.Linear)).weight.dtype\n+    target_dtype = get_target_dtype(query, module)\n \n     # Instead of relying on the value set in the module directly, we use the is_causal passed in kwargs if it is presented\n     is_causal = kwargs.pop(\"is_causal\", None)"
      },
      {
        "filename": "src/transformers/models/bark/modeling_bark.py",
        "status": "modified",
        "additions": 5,
        "deletions": 0,
        "changes": 5,
        "patch": "@@ -57,6 +57,7 @@\n \n \n if is_flash_attn_available():\n+    from ...integrations.flash_attention import get_target_dtype\n     from ...modeling_flash_attention_utils import _flash_attention_forward\n \n \n@@ -78,6 +79,7 @@ def __init__(self, config, is_causal=False, layer_idx=None):\n         self.embed_dim = config.hidden_size\n         self.num_heads = config.num_heads\n         self.head_dim = self.embed_dim // self.num_heads\n+        self.config = config\n \n         if config.hidden_size % config.num_heads != 0:\n             raise ValueError(\n@@ -228,6 +230,8 @@ def forward(\n         if past_key_values is not None:\n             key, value = past_key_values.update(key, value, self.layer_idx, {\"cache_position\": cache_position})\n \n+        target_dtype = get_target_dtype(query, self)  # if the query is in float32, this is the dtype to cast to for FA\n+\n         attn_output = _flash_attention_forward(\n             query,\n             key,\n@@ -237,6 +241,7 @@ def forward(\n             dropout=self.dropout if self.training else 0.0,\n             use_top_left_mask=self._flash_attn_uses_top_left_mask,\n             is_causal=self.is_causal,\n+            target_dtype=target_dtype,\n         )\n \n         attn_output = self._merge_heads(attn_output, self.num_heads, self.head_dim)"
      },
      {
        "filename": "src/transformers/models/blt/modeling_blt.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -280,12 +280,12 @@ def __init__(self, config: BltConfig, layer_idx: int):\n         self.scaling = self.head_dim**-0.5\n         self.rope_theta = config.rope_theta\n         self.layer_idx = layer_idx\n+        self.is_causal = True\n \n         self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n         self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n         self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n         self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n-        self.is_causal = True\n \n     def forward(\n         self,"
      },
      {
        "filename": "src/transformers/models/kosmos2/modeling_kosmos2.py",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -680,6 +680,7 @@ def __init__(\n         self.num_heads = num_heads\n         self.dropout = dropout\n         self.head_dim = embed_dim // num_heads\n+        self.is_causal = True\n \n         if (self.head_dim * num_heads) != self.embed_dim:\n             raise ValueError("
      },
      {
        "filename": "src/transformers/models/mllama/modeling_mllama.py",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -519,6 +519,7 @@ def __init__(self, config: MllamaTextConfig, layer_idx: int):\n         self.scaling = self.head_dim**-0.5\n         self.rope_theta = config.rope_theta\n         self.layer_idx = layer_idx\n+        self.is_causal = True\n \n         self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n         self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)"
      },
      {
        "filename": "src/transformers/models/stablelm/modeling_stablelm.py",
        "status": "modified",
        "additions": 4,
        "deletions": 0,
        "changes": 4,
        "patch": "@@ -52,6 +52,7 @@\n \n \n if is_flash_attn_available():\n+    from ...integrations.flash_attention import get_target_dtype\n     from ...modeling_flash_attention_utils import _flash_attention_forward\n \n \n@@ -495,6 +496,8 @@ def forward(\n \n         dropout_rate = self.attention_dropout.p if self.training else 0.0\n \n+        target_dtype = get_target_dtype(query_states, self)\n+\n         attn_output = _flash_attention_forward(\n             query_states,\n             key_states,\n@@ -505,6 +508,7 @@ def forward(\n             dropout=dropout_rate,\n             use_top_left_mask=self._flash_attn_uses_top_left_mask,\n             is_causal=self.is_causal,\n+            target_dtype=target_dtype,\n         )\n \n         attn_output = attn_output.reshape(bsz, q_len, self.hidden_size).contiguous()"
      }
    ],
    "num_files": 6,
    "scraped_at": "2025-11-16T21:17:29.373689"
  },
  {
    "pr_number": 41577,
    "title": "[kernels] refactor function kernel calling",
    "body": "# What does this PR do?\r\n\r\nThis should simplify lazy kernel loading in Transformers.  \r\nWe simply define a mapping between each kernel name and the repository it should be pulled from, then load it using the `lazy_load_kernel` function. This function adds the kernel to a global cache shared across all models.  \r\nIf the kernel isn\u2019t available, we check whether it\u2019s installed as a module for backward compatibility; otherwise, we return `None`.\r\n",
    "html_url": "https://github.com/huggingface/transformers/pull/41577",
    "created_at": "2025-10-14T12:58:37Z",
    "merged_at": "2025-10-16T13:43:03Z",
    "merge_commit_sha": "1fb3fc4db0e87fd7c2f57a36b6b32ee6fa69c50c",
    "base_ref": "main",
    "head_sha": "374040458947013699454f267606233374be6303",
    "user": "MekkCyber",
    "files": [
      {
        "filename": "src/transformers/integrations/hub_kernels.py",
        "status": "modified",
        "additions": 55,
        "deletions": 0,
        "changes": 55,
        "patch": "@@ -14,12 +14,16 @@\n import re\n from collections.abc import Callable\n from functools import partial\n+from types import ModuleType\n from typing import Optional, Union\n \n from ..modeling_flash_attention_utils import lazy_import_flash_attention\n+from ..utils import logging\n from .flash_attention import flash_attention_forward\n \n \n+logger = logging.get_logger(__name__)\n+\n try:\n     from kernels import (\n         Device,\n@@ -158,6 +162,13 @@ def register_kernel_mapping(*args, **kwargs):\n         raise RuntimeError(\"register_kernel_mapping requires `kernels` to be installed. Run `pip install kernels`.\")\n \n \n+_HUB_KERNEL_MAPPING: dict[str, str] = {\n+    \"causal-conv1d\": \"kernels-community/causal-conv1d\",\n+}\n+\n+_KERNEL_MODULE_MAPPING: dict[str, Optional[ModuleType]] = {}\n+\n+\n def is_kernel(attn_implementation: Optional[str]) -> bool:\n     \"\"\"Check whether `attn_implementation` matches a kernel pattern from the hub.\"\"\"\n     return (\n@@ -220,9 +231,53 @@ def load_and_register_attn_kernel(attn_implementation: str, attention_wrapper: O\n     ALL_MASK_ATTENTION_FUNCTIONS.register(attn_implementation, ALL_MASK_ATTENTION_FUNCTIONS[\"flash_attention_2\"])\n \n \n+def lazy_load_kernel(kernel_name: str, mapping: dict[str, Optional[ModuleType]] = _KERNEL_MODULE_MAPPING):\n+    if kernel_name in mapping and isinstance(mapping[kernel_name], ModuleType):\n+        return mapping[kernel_name]\n+    if kernel_name not in _HUB_KERNEL_MAPPING:\n+        logger.warning(f\"Kernel {kernel_name} not found in _HUB_KERNEL_MAPPING\")\n+        mapping[kernel_name] = None\n+        return None\n+    if _kernels_available:\n+        from kernels import get_kernel\n+\n+        try:\n+            kernel = get_kernel(_HUB_KERNEL_MAPPING[kernel_name])\n+            mapping[kernel_name] = kernel\n+        except FileNotFoundError:\n+            mapping[kernel_name] = None\n+\n+    else:\n+        # Try to import is_{kernel_name}_available from ..utils\n+        import importlib\n+\n+        new_kernel_name = kernel_name.replace(\"-\", \"_\")\n+        func_name = f\"is_{new_kernel_name}_available\"\n+\n+        try:\n+            utils_mod = importlib.import_module(\"..utils.import_utils\", __package__)\n+            is_kernel_available = getattr(utils_mod, func_name, None)\n+        except Exception:\n+            is_kernel_available = None\n+\n+        if callable(is_kernel_available) and is_kernel_available():\n+            # Try to import the module \"{kernel_name}\" from parent package level\n+            try:\n+                module = importlib.import_module(f\"{kernel_name}\")\n+                mapping[kernel_name] = module\n+                return module\n+            except Exception:\n+                mapping[kernel_name] = None\n+        else:\n+            mapping[kernel_name] = None\n+\n+    return mapping[kernel_name]\n+\n+\n __all__ = [\n     \"LayerRepository\",\n     \"use_kernel_forward_from_hub\",\n     \"register_kernel_mapping\",\n     \"replace_kernel_forward_from_hub\",\n+    \"lazy_load_kernel\",\n ]"
      },
      {
        "filename": "src/transformers/models/falcon_mamba/modeling_falcon_mamba.py",
        "status": "modified",
        "additions": 19,
        "deletions": 32,
        "changes": 51,
        "patch": "@@ -30,12 +30,11 @@\n from ...activations import ACT2FN\n from ...configuration_utils import PreTrainedConfig\n from ...generation import GenerationMixin\n+from ...integrations.hub_kernels import lazy_load_kernel\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_utils import PreTrainedModel\n from ...utils import ModelOutput, auto_docstring, logging\n from ...utils.import_utils import (\n-    is_causal_conv1d_available,\n-    is_kernels_available,\n     is_mamba_ssm_available,\n     is_mambapy_available,\n )\n@@ -162,33 +161,6 @@ def reset(self):\n             self.ssm_states[layer_idx].zero_()\n \n \n-def _lazy_load_causal_conv1d():\n-    global _causal_conv1d_cache\n-    if _causal_conv1d_cache is not None:\n-        return _causal_conv1d_cache\n-\n-    if is_kernels_available():\n-        from kernels import get_kernel\n-\n-        try:\n-            _causal_conv1d_kernel = get_kernel(\"kernels-community/causal-conv1d\")\n-        except FileNotFoundError:\n-            # no kernel binary match, fallback to slow path\n-            _causal_conv1d_cache = (None, None)\n-        else:\n-            _causal_conv1d_cache = (_causal_conv1d_kernel.causal_conv1d_update, _causal_conv1d_kernel.causal_conv1d_fn)\n-    elif is_causal_conv1d_available():\n-        from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\n-\n-        _causal_conv1d_cache = (causal_conv1d_update, causal_conv1d_fn)\n-    else:\n-        _causal_conv1d_cache = (None, None)\n-    return _causal_conv1d_cache\n-\n-\n-_causal_conv1d_cache = None\n-\n-\n def rms_forward(hidden_states, variance_epsilon=1e-6):\n     \"\"\"\n     Calculates simple RMSNorm with no learnable weights. `MambaRMSNorm` will\n@@ -268,7 +240,12 @@ def __init__(self, config: FalconMambaConfig, layer_idx: int):\n         self.rms_eps = config.mixer_rms_eps\n \n     def warn_slow_implementation(self):\n-        causal_conv1d_update, causal_conv1d_fn = _lazy_load_causal_conv1d()\n+        causal_conv1d = lazy_load_kernel(\"causal-conv1d\")\n+        causal_conv1d_update, causal_conv1d_fn = (\n+            (causal_conv1d.causal_conv1d_update, causal_conv1d.causal_conv1d_fn)\n+            if causal_conv1d is not None\n+            else (None, None)\n+        )\n         is_fast_path_available = all(\n             (selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)\n         )\n@@ -323,7 +300,12 @@ def cuda_kernels_forward(\n             )\n \n         else:\n-            causal_conv1d_update, causal_conv1d_fn = _lazy_load_causal_conv1d()\n+            causal_conv1d = lazy_load_kernel(\"causal-conv1d\")\n+            causal_conv1d_update, causal_conv1d_fn = (\n+                (causal_conv1d.causal_conv1d_update, causal_conv1d.causal_conv1d_fn)\n+                if causal_conv1d is not None\n+                else (None, None)\n+            )\n             hidden_states, gate = projected_states.chunk(2, dim=1)\n \n             if attention_mask is not None:\n@@ -518,7 +500,12 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.LongTensor] = None,\n     ):\n-        causal_conv1d_update, causal_conv1d_fn = _lazy_load_causal_conv1d()\n+        causal_conv1d = lazy_load_kernel(\"causal-conv1d\")\n+        causal_conv1d_update, causal_conv1d_fn = (\n+            (causal_conv1d.causal_conv1d_update, causal_conv1d.causal_conv1d_fn)\n+            if causal_conv1d is not None\n+            else (None, None)\n+        )\n         is_fast_path_available = all(\n             (selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)\n         )"
      },
      {
        "filename": "src/transformers/models/falcon_mamba/modular_falcon_mamba.py",
        "status": "modified",
        "additions": 19,
        "deletions": 6,
        "changes": 25,
        "patch": "@@ -19,6 +19,7 @@\n import torch\n from torch import nn\n \n+from ...integrations.hub_kernels import lazy_load_kernel\n from ...utils import auto_docstring, logging\n from ...utils.import_utils import (\n     is_mamba_ssm_available,\n@@ -35,7 +36,6 @@\n     MambaOutput,\n     MambaPreTrainedModel,\n     MambaRMSNorm,\n-    _lazy_load_causal_conv1d,\n )\n \n \n@@ -54,8 +54,6 @@\n else:\n     selective_state_update, selective_scan_fn, mamba_inner_fn = None, None, None\n \n-_causal_conv1d_cache = None\n-\n \n class FalconMambaConfig(MambaConfig):\n     \"\"\"\n@@ -258,7 +256,12 @@ def rms_forward(hidden_states, variance_epsilon=1e-6):\n \n class FalconMambaMixer(MambaMixer):\n     def warn_slow_implementation(self):\n-        causal_conv1d_update, causal_conv1d_fn = _lazy_load_causal_conv1d()\n+        causal_conv1d = lazy_load_kernel(\"causal-conv1d\")\n+        causal_conv1d_update, causal_conv1d_fn = (\n+            (causal_conv1d.causal_conv1d_update, causal_conv1d.causal_conv1d_fn)\n+            if causal_conv1d is not None\n+            else (None, None)\n+        )\n         is_fast_path_available = all(\n             (selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)\n         )\n@@ -324,7 +327,12 @@ def cuda_kernels_forward(\n             )\n \n         else:\n-            causal_conv1d_update, causal_conv1d_fn = _lazy_load_causal_conv1d()\n+            causal_conv1d = lazy_load_kernel(\"causal-conv1d\")\n+            causal_conv1d_update, causal_conv1d_fn = (\n+                (causal_conv1d.causal_conv1d_update, causal_conv1d.causal_conv1d_fn)\n+                if causal_conv1d is not None\n+                else (None, None)\n+            )\n             hidden_states, gate = projected_states.chunk(2, dim=1)\n \n             if attention_mask is not None:\n@@ -518,7 +526,12 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.LongTensor] = None,\n     ):\n-        causal_conv1d_update, causal_conv1d_fn = _lazy_load_causal_conv1d()\n+        causal_conv1d = lazy_load_kernel(\"causal-conv1d\")\n+        causal_conv1d_update, causal_conv1d_fn = (\n+            (causal_conv1d.causal_conv1d_update, causal_conv1d.causal_conv1d_fn)\n+            if causal_conv1d is not None\n+            else (None, None)\n+        )\n         is_fast_path_available = all(\n             (selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)\n         )"
      },
      {
        "filename": "src/transformers/models/mamba/modeling_mamba.py",
        "status": "modified",
        "additions": 19,
        "deletions": 31,
        "changes": 50,
        "patch": "@@ -25,6 +25,7 @@\n from ...activations import ACT2FN\n from ...configuration_utils import PreTrainedConfig\n from ...generation import GenerationMixin\n+from ...integrations.hub_kernels import lazy_load_kernel\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n@@ -33,8 +34,6 @@\n     logging,\n )\n from ...utils.import_utils import (\n-    is_causal_conv1d_available,\n-    is_kernels_available,\n     is_mamba_ssm_available,\n     is_mambapy_available,\n )\n@@ -54,32 +53,6 @@\n else:\n     selective_state_update, selective_scan_fn, mamba_inner_fn = None, None, None\n \n-_causal_conv1d_cache = None\n-\n-\n-def _lazy_load_causal_conv1d():\n-    global _causal_conv1d_cache\n-    if _causal_conv1d_cache is not None:\n-        return _causal_conv1d_cache\n-\n-    if is_kernels_available():\n-        from kernels import get_kernel\n-\n-        try:\n-            _causal_conv1d_kernel = get_kernel(\"kernels-community/causal-conv1d\")\n-        except FileNotFoundError:\n-            # no kernel binary match, fallback to slow path\n-            _causal_conv1d_cache = (None, None)\n-        else:\n-            _causal_conv1d_cache = (_causal_conv1d_kernel.causal_conv1d_update, _causal_conv1d_kernel.causal_conv1d_fn)\n-    elif is_causal_conv1d_available():\n-        from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\n-\n-        _causal_conv1d_cache = (causal_conv1d_update, causal_conv1d_fn)\n-    else:\n-        _causal_conv1d_cache = (None, None)\n-    return _causal_conv1d_cache\n-\n \n class MambaCache:\n     \"\"\"\n@@ -236,7 +209,12 @@ def __init__(self, config: MambaConfig, layer_idx: int):\n         self.warn_slow_implementation()\n \n     def warn_slow_implementation(self):\n-        causal_conv1d_update, causal_conv1d_fn = _lazy_load_causal_conv1d()\n+        causal_conv1d = lazy_load_kernel(\"causal-conv1d\")\n+        causal_conv1d_update, causal_conv1d_fn = (\n+            (causal_conv1d.causal_conv1d_update, causal_conv1d.causal_conv1d_fn)\n+            if causal_conv1d is not None\n+            else (None, None)\n+        )\n         is_fast_path_available = all(\n             (selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)\n         )\n@@ -287,7 +265,12 @@ def cuda_kernels_forward(\n             )\n \n         else:\n-            causal_conv1d_update, causal_conv1d_fn = _lazy_load_causal_conv1d()\n+            causal_conv1d = lazy_load_kernel(\"causal-conv1d\")\n+            causal_conv1d_update, causal_conv1d_fn = (\n+                (causal_conv1d.causal_conv1d_update, causal_conv1d.causal_conv1d_fn)\n+                if causal_conv1d is not None\n+                else (None, None)\n+            )\n             hidden_states, gate = projected_states.chunk(2, dim=1)\n \n             if attention_mask is not None:\n@@ -451,7 +434,12 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.LongTensor] = None,\n     ):\n-        causal_conv1d_update, causal_conv1d_fn = _lazy_load_causal_conv1d()\n+        causal_conv1d = lazy_load_kernel(\"causal-conv1d\")\n+        causal_conv1d_update, causal_conv1d_fn = (\n+            (causal_conv1d.causal_conv1d_update, causal_conv1d.causal_conv1d_fn)\n+            if causal_conv1d is not None\n+            else (None, None)\n+        )\n         is_fast_path_available = all(\n             (selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)\n         )"
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T21:17:36.120795"
  },
  {
    "pr_number": 41572,
    "title": "Gemma3 fixes",
    "body": "This PR fixes three things in `gemma3`:\r\n- a multiple-device error where `torch.where` takes some of its coefficients from a tensor that is not on the right device and is a full_like, so we just replace it with the filling element\r\n- an error in the `flash_attn_inference_equivalence` which is due to the model needing more parameters than are generated by defualt. To avoid this, we add a flag that specifies if we need to check the forward pass with training or not, and make this check default for both and left padding (cc. @vasqu )\r\n- the test `flash_attn_from_config` was failing for the same reasons (`token_type_ids` is required as a model input when training) so I added a `.eval()` to avoid this. It does not seem the model needs to be in train mode for this test, but I can also add an option to the test to only call `.eval()` if a flag is passed",
    "html_url": "https://github.com/huggingface/transformers/pull/41572",
    "created_at": "2025-10-14T11:02:36Z",
    "merged_at": "2025-10-14T16:33:27Z",
    "merge_commit_sha": "9e4199ede396f136b3dff1e918816fcc3a65f0a0",
    "base_ref": "main",
    "head_sha": "673cecc4db388bdaeaed9b001d20e13f1c65a0ff",
    "user": "remi-or",
    "files": [
      {
        "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -798,7 +798,7 @@ def create_causal_mask_mapping(\n         is_previous_image = nn.functional.pad(is_image, (1, 0), value=0)[:, :-1]\n         new_image_start = is_image & ~is_previous_image\n         image_group_ids = torch.cumsum(new_image_start.int(), dim=1) - 1\n-        image_group_ids = torch.where(is_image, image_group_ids, torch.full_like(token_type_ids, -1))\n+        image_group_ids = torch.where(is_image, image_group_ids, -1)\n         mask_kwargs[\"or_mask_function\"] = token_type_ids_mask_function(\n             token_type_ids.to(cache_position.device), image_group_ids\n         )"
      },
      {
        "filename": "src/transformers/models/gemma3/modular_gemma3.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -764,7 +764,7 @@ def create_causal_mask_mapping(\n         is_previous_image = nn.functional.pad(is_image, (1, 0), value=0)[:, :-1]\n         new_image_start = is_image & ~is_previous_image\n         image_group_ids = torch.cumsum(new_image_start.int(), dim=1) - 1\n-        image_group_ids = torch.where(is_image, image_group_ids, torch.full_like(token_type_ids, -1))\n+        image_group_ids = torch.where(is_image, image_group_ids, -1)\n         mask_kwargs[\"or_mask_function\"] = token_type_ids_mask_function(\n             token_type_ids.to(cache_position.device), image_group_ids\n         )"
      },
      {
        "filename": "tests/models/gemma3/test_modeling_gemma3.py",
        "status": "modified",
        "additions": 17,
        "deletions": 0,
        "changes": 17,
        "patch": "@@ -19,6 +19,7 @@\n \n import pytest\n from parameterized import parameterized\n+from pytest import mark\n \n from transformers import (\n     AutoModelForCausalLM,\n@@ -33,9 +34,11 @@\n     is_flash_attn_2_available,\n     require_deterministic_for_xpu,\n     require_flash_attn,\n+    require_flash_attn_3,\n     require_read_token,\n     require_torch,\n     require_torch_accelerator,\n+    require_torch_gpu,\n     require_torch_large_accelerator,\n     slow,\n     torch_device,\n@@ -342,6 +345,20 @@ def test_automodelforcausallm(self):\n             for_causal_lm = AutoModelForCausalLM.from_pretrained(tmp_dir)\n             self.assertIsInstance(for_causal_lm, Gemma3ForConditionalGeneration)\n \n+    @require_flash_attn\n+    @require_torch_gpu\n+    @mark.flash_attn_test\n+    @slow\n+    def test_flash_attn_2_from_config(self):\n+        self.flash_attn_from_config(attn_implementation=\"flash_attention_2\", test_fwd_in_train=False)\n+\n+    @require_flash_attn_3\n+    @require_torch_gpu\n+    @mark.flash_attn_3_test\n+    @slow\n+    def test_flash_attn_3_from_config(self):\n+        self.flash_attn_from_config(attn_implementation=\"flash_attention_3\", test_fwd_in_train=False)\n+\n \n @slow\n @require_torch_accelerator"
      },
      {
        "filename": "tests/test_modeling_common.py",
        "status": "modified",
        "additions": 10,
        "deletions": 5,
        "changes": 15,
        "patch": "@@ -2976,7 +2976,7 @@ def test_model_is_small(self):\n \n     def flash_attn_inference_equivalence(\n         self, attn_implementation: str, padding_side: str, atol: float = 4e-2, rtol: float = 4e-2\n-    ):\n+    ) -> None:\n         r\"\"\"\n         Tests the equivalence between the eager and flash attention implementations.\n         This test is only for inference and runs with `dtype=torch.bfloat16`.\n@@ -3114,9 +3114,6 @@ def flash_attn_inference_equivalence(\n                 torch.testing.assert_close(logits_1_eager, logits_1_fa, atol=atol, rtol=rtol)\n                 if padding_side == \"left\":\n                     torch.testing.assert_close(logits_2_eager[1:], logits_2_fa[1:], atol=atol, rtol=rtol)\n-                    # Check it can run in training mode\n-                    model.train()\n-                    _ = model(**second_inputs)\n                 else:\n                     torch.testing.assert_close(logits_2_eager[:-1], logits_2_fa[:-1], atol=atol, rtol=rtol)\n \n@@ -3651,7 +3648,7 @@ def test_flash_attn_2_can_compile_with_attention_mask_None_without_graph_break(s\n \n         assert not loss.isnan().any()\n \n-    def flash_attn_from_config(self, attn_implementation: str):\n+    def flash_attn_from_config(self, attn_implementation: str, test_fwd_in_train: bool = True):\n         r\"\"\"\n         Tests if the model can be loaded with `attn_implementation` from the config and if the\n         weights are not randomly initialized.\n@@ -3669,6 +3666,14 @@ def flash_attn_from_config(self, attn_implementation: str):\n                 config, attn_implementation=attn_implementation, dtype=torch.bfloat16\n             ).to(torch_device)\n \n+            # By default, we perform the forward pass in train mode, because it's more sctrict than eval mode. If the\n+            # forward pass is successful in train mode, it will also be successful in eval mode. But since some models\n+            # (eg. gemma3) need different inputs in train mode we have the option to test the forward pass in eval mode.\n+            if test_fwd_in_train:\n+                fa_model = fa_model.train()\n+            else:\n+                fa_model = fa_model.eval()\n+\n             dummy_input = inputs_dict[fa_model.main_input_name]\n             if dummy_input.dtype in [torch.float32, torch.float16]:\n                 dummy_input = dummy_input.to(torch.bfloat16)"
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T21:17:37.599993"
  },
  {
    "pr_number": 41536,
    "title": "[Qwen3VL] fix device mismatch error for FSDP2 training",
    "body": "# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFor FSDP2, parameters might be on a meta device, and the weight.device attribute may not accurately reflect where the actual computation will happen during forward passes.\r\n\r\n```log\r\n  File \"transformers/models/qwen3_vl_moe/modeling_qwen3_vl_moe.py\", line 776, in forward\r\n    pos_embeds = self.fast_pos_embed_interpolate(grid_thw)\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"transformers/models/qwen3_vl_moe/modeling_qwen3_vl_moe.py\", line 745, in fast_pos_embed_interpolate\r\n    pos_embeds = self.pos_embed(idx_tensor) * weight_tensor[:, :, None]\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"torch/nn/modules/module.py\", line 1879, in _call_impl\r\n    return inner()\r\n           ^^^^^^^\r\n  File \"torch/nn/modules/module.py\", line 1827, in inner\r\n    result = forward_call(*args, **kwargs)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"torch/nn/modules/sparse.py\", line 192, in forward\r\n    return F.embedding(\r\n           ^^^^^^^^^^^^\r\n  File \"torch/nn/functional.py\", line 2546, in embedding\r\n    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nRuntimeError: Expected all tensors to be on the same device, but got index is on cpu, different from other tensors on cuda:0 (when checking argument in method wrapper_CUDA__index_select)\r\n```\r\nhttps://github.com/volcengine/verl/pull/3686#issuecomment-3380981817\r\n\r\nSince the device for grid_thw is pretty much dependent on the user-side implementation (passed as a parameter for the forward method), I think it's better to take the device of grid_thw for unifying the device of idx_tensor and weight_tensor, so that user-side implementation can have more control over this and to guarantee nothing can go wrong here. User-side code can ensure the input grid is on the same device as the positional embedding weight, as the size of grid_thw is small, so there shouldn't be too much overhead. This is tested on [verl](https://github.com/volcengine/verl) and worked fine.\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [X] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker @Cyrilvallez\r\n- vision models: @yonigozlan @molbap\r\n- audio models: @eustlb @ebezzam @vasqu\r\n- multimodal models: @zucchini-nlp\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- generate: @zucchini-nlp (visual-language models) or @gante (all others)\r\n- continuous batching: @remi-or @ArthurZucker @McPatate\r\n- pipelines: @Rocketknight1\r\n- tokenizers: @ArthurZucker and @itazap\r\n- trainer: @zach-huggingface @SunMarc\r\n- attention: @vasqu @ArthurZucker @CyrilVallez\r\n- model loading (from pretrained, etc): @CyrilVallez\r\n- distributed: @3outeille @ArthurZucker @S1ro1\r\n- CIs: @ydshieh\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @SunMarc @zach-huggingface\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber\r\n- kernels: @MekkCyber @drbh\r\n- peft: @BenjaminBossan @githubnemo\r\n\r\nDevices/Backends:\r\n\r\n- AMD ROCm: @ivarflakstad\r\n- Intel XPU: @IlyasMoutawwakil\r\n- Ascend NPU: @ivarflakstad \r\n\r\nDocumentation: @stevhliu\r\n\r\nResearch projects are not maintained and should be taken as is.\r\n\r\n -->\r\n\r\n@yonigozlan @molbap @ArthurZucker @Cyrilvallez @zucchini-nlp",
    "html_url": "https://github.com/huggingface/transformers/pull/41536",
    "created_at": "2025-10-12T18:46:38Z",
    "merged_at": "2025-10-14T10:28:25Z",
    "merge_commit_sha": "b3e3c3dc93f29770a768d6943c9fb9d377e5edce",
    "base_ref": "main",
    "head_sha": "90abc00f3183111d03d87929753817e8cef44861",
    "user": "HollowMan6",
    "files": [
      {
        "filename": "src/transformers/models/qwen3_omni_moe/modeling_qwen3_omni_moe.py",
        "status": "modified",
        "additions": 4,
        "deletions": 5,
        "changes": 9,
        "patch": "@@ -1099,6 +1099,7 @@ def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:\n \n     def fast_pos_embed_interpolate(self, grid_thw):\n         grid_ts, grid_hs, grid_ws = grid_thw[:, 0], grid_thw[:, 1], grid_thw[:, 2]\n+        device = grid_thw.device\n \n         idx_list = [[] for _ in range(4)]\n         weight_list = [[] for _ in range(4)]\n@@ -1136,11 +1137,9 @@ def fast_pos_embed_interpolate(self, grid_thw):\n                 idx_list[i].extend(indices[i].tolist())\n                 weight_list[i].extend(weights[i].tolist())\n \n-        idx_tensor = torch.tensor(idx_list, dtype=torch.long, device=self.pos_embed.weight.device)\n-        weight_tensor = torch.tensor(\n-            weight_list, dtype=self.pos_embed.weight.dtype, device=self.pos_embed.weight.device\n-        )\n-        pos_embeds = self.pos_embed(idx_tensor) * weight_tensor[:, :, None]\n+        idx_tensor = torch.tensor(idx_list, dtype=torch.long, device=device)\n+        weight_tensor = torch.tensor(weight_list, dtype=self.pos_embed.weight.dtype, device=device)\n+        pos_embeds = self.pos_embed(idx_tensor).to(device) * weight_tensor[:, :, None]\n         patch_pos_embeds = pos_embeds[0] + pos_embeds[1] + pos_embeds[2] + pos_embeds[3]\n \n         patch_pos_embeds = patch_pos_embeds.split([h * w for h, w in zip(grid_hs, grid_ws)])"
      },
      {
        "filename": "src/transformers/models/qwen3_vl/modeling_qwen3_vl.py",
        "status": "modified",
        "additions": 4,
        "deletions": 5,
        "changes": 9,
        "patch": "@@ -639,6 +639,7 @@ def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:\n \n     def fast_pos_embed_interpolate(self, grid_thw):\n         grid_ts, grid_hs, grid_ws = grid_thw[:, 0], grid_thw[:, 1], grid_thw[:, 2]\n+        device = grid_thw.device\n \n         idx_list = [[] for _ in range(4)]\n         weight_list = [[] for _ in range(4)]\n@@ -676,11 +677,9 @@ def fast_pos_embed_interpolate(self, grid_thw):\n                 idx_list[i].extend(indices[i].tolist())\n                 weight_list[i].extend(weights[i].tolist())\n \n-        idx_tensor = torch.tensor(idx_list, dtype=torch.long, device=self.pos_embed.weight.device)\n-        weight_tensor = torch.tensor(\n-            weight_list, dtype=self.pos_embed.weight.dtype, device=self.pos_embed.weight.device\n-        )\n-        pos_embeds = self.pos_embed(idx_tensor) * weight_tensor[:, :, None]\n+        idx_tensor = torch.tensor(idx_list, dtype=torch.long, device=device)\n+        weight_tensor = torch.tensor(weight_list, dtype=self.pos_embed.weight.dtype, device=device)\n+        pos_embeds = self.pos_embed(idx_tensor).to(device) * weight_tensor[:, :, None]\n         patch_pos_embeds = pos_embeds[0] + pos_embeds[1] + pos_embeds[2] + pos_embeds[3]\n \n         patch_pos_embeds = patch_pos_embeds.split([h * w for h, w in zip(grid_hs, grid_ws)])"
      },
      {
        "filename": "src/transformers/models/qwen3_vl/modular_qwen3_vl.py",
        "status": "modified",
        "additions": 4,
        "deletions": 5,
        "changes": 9,
        "patch": "@@ -615,6 +615,7 @@ def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:\n \n     def fast_pos_embed_interpolate(self, grid_thw):\n         grid_ts, grid_hs, grid_ws = grid_thw[:, 0], grid_thw[:, 1], grid_thw[:, 2]\n+        device = grid_thw.device\n \n         idx_list = [[] for _ in range(4)]\n         weight_list = [[] for _ in range(4)]\n@@ -652,11 +653,9 @@ def fast_pos_embed_interpolate(self, grid_thw):\n                 idx_list[i].extend(indices[i].tolist())\n                 weight_list[i].extend(weights[i].tolist())\n \n-        idx_tensor = torch.tensor(idx_list, dtype=torch.long, device=self.pos_embed.weight.device)\n-        weight_tensor = torch.tensor(\n-            weight_list, dtype=self.pos_embed.weight.dtype, device=self.pos_embed.weight.device\n-        )\n-        pos_embeds = self.pos_embed(idx_tensor) * weight_tensor[:, :, None]\n+        idx_tensor = torch.tensor(idx_list, dtype=torch.long, device=device)\n+        weight_tensor = torch.tensor(weight_list, dtype=self.pos_embed.weight.dtype, device=device)\n+        pos_embeds = self.pos_embed(idx_tensor).to(device) * weight_tensor[:, :, None]\n         patch_pos_embeds = pos_embeds[0] + pos_embeds[1] + pos_embeds[2] + pos_embeds[3]\n \n         patch_pos_embeds = patch_pos_embeds.split([h * w for h, w in zip(grid_hs, grid_ws)])"
      },
      {
        "filename": "src/transformers/models/qwen3_vl_moe/modeling_qwen3_vl_moe.py",
        "status": "modified",
        "additions": 4,
        "deletions": 5,
        "changes": 9,
        "patch": "@@ -661,6 +661,7 @@ def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:\n \n     def fast_pos_embed_interpolate(self, grid_thw):\n         grid_ts, grid_hs, grid_ws = grid_thw[:, 0], grid_thw[:, 1], grid_thw[:, 2]\n+        device = grid_thw.device\n \n         idx_list = [[] for _ in range(4)]\n         weight_list = [[] for _ in range(4)]\n@@ -698,11 +699,9 @@ def fast_pos_embed_interpolate(self, grid_thw):\n                 idx_list[i].extend(indices[i].tolist())\n                 weight_list[i].extend(weights[i].tolist())\n \n-        idx_tensor = torch.tensor(idx_list, dtype=torch.long, device=self.pos_embed.weight.device)\n-        weight_tensor = torch.tensor(\n-            weight_list, dtype=self.pos_embed.weight.dtype, device=self.pos_embed.weight.device\n-        )\n-        pos_embeds = self.pos_embed(idx_tensor) * weight_tensor[:, :, None]\n+        idx_tensor = torch.tensor(idx_list, dtype=torch.long, device=device)\n+        weight_tensor = torch.tensor(weight_list, dtype=self.pos_embed.weight.dtype, device=device)\n+        pos_embeds = self.pos_embed(idx_tensor).to(device) * weight_tensor[:, :, None]\n         patch_pos_embeds = pos_embeds[0] + pos_embeds[1] + pos_embeds[2] + pos_embeds[3]\n \n         patch_pos_embeds = patch_pos_embeds.split([h * w for h, w in zip(grid_hs, grid_ws)])"
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T21:17:44.165355"
  },
  {
    "pr_number": 41534,
    "title": "Add VideoMAE video processor ",
    "body": "## What does this PR do?\r\n\r\n- add a dedicated `VideoMAEVideoProcessor` that decodes/samples videos via TorchCodec and emits `pixel_values` ready for VideoMAE models\r\n- document the new processor alongside the existing image processor so users can discover the GPU-friendly path\r\n- cover the processor with torchvision-gated regression tests to ensure serialization, sampling, and output naming stay stable\r\n\r\nFixes #41520 \r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),\r\n      Pull Request section?\r\n- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case. (follow-up to the discussion with @zucchini-nlp on video processors)\r\n- [x] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [x] Did you write any new necessary tests?\r\n",
    "html_url": "https://github.com/huggingface/transformers/pull/41534",
    "created_at": "2025-10-12T14:05:59Z",
    "merged_at": "2025-10-13T13:42:27Z",
    "merge_commit_sha": "3813a8e3a1663993b3ec44c455cab8af1beca2b5",
    "base_ref": "main",
    "head_sha": "47e175c3ebcad2553581fe87d6d2f6aaee7b645a",
    "user": "Aki-07",
    "files": [
      {
        "filename": "docs/source/en/model_doc/videomae.md",
        "status": "modified",
        "additions": 5,
        "deletions": 0,
        "changes": 5,
        "patch": "@@ -90,6 +90,11 @@ to fine-tune a VideoMAE model on a custom dataset.\n [[autodoc]] VideoMAEImageProcessor\n     - preprocess\n \n+## VideoMAEVideoProcessor\n+\n+[[autodoc]] VideoMAEVideoProcessor\n+    - preprocess\n+\n ## VideoMAEModel\n \n [[autodoc]] VideoMAEModel"
      },
      {
        "filename": "src/transformers/models/auto/video_processing_auto.py",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -62,6 +62,7 @@\n             (\"sam2_video\", \"Sam2VideoVideoProcessor\"),\n             (\"smolvlm\", \"SmolVLMVideoProcessor\"),\n             (\"video_llava\", \"VideoLlavaVideoProcessor\"),\n+            (\"videomae\", \"VideoMAEVideoProcessor\"),\n             (\"vjepa2\", \"VJEPA2VideoProcessor\"),\n         ]\n     )"
      },
      {
        "filename": "src/transformers/models/videomae/__init__.py",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -22,6 +22,7 @@\n     from .feature_extraction_videomae import *\n     from .image_processing_videomae import *\n     from .modeling_videomae import *\n+    from .video_processing_videomae import *\n else:\n     import sys\n "
      },
      {
        "filename": "src/transformers/models/videomae/modeling_videomae.py",
        "status": "modified",
        "additions": 14,
        "deletions": 111,
        "changes": 125,
        "patch": "@@ -440,72 +440,25 @@ def forward(\n         Examples:\n \n         ```python\n-        >>> import av\n-        >>> import numpy as np\n-\n-        >>> from transformers import AutoImageProcessor, VideoMAEModel\n+        >>> import torch\n+        >>> from transformers import VideoMAEVideoProcessor, VideoMAEModel\n         >>> from huggingface_hub import hf_hub_download\n \n-        >>> np.random.seed(0)\n-\n-\n-        >>> def read_video_pyav(container, indices):\n-        ...     '''\n-        ...     Decode the video with PyAV decoder.\n-        ...     Args:\n-        ...         container (`av.container.input.InputContainer`): PyAV container.\n-        ...         indices (`list[int]`): List of frame indices to decode.\n-        ...     Returns:\n-        ...         result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n-        ...     '''\n-        ...     frames = []\n-        ...     container.seek(0)\n-        ...     start_index = indices[0]\n-        ...     end_index = indices[-1]\n-        ...     for i, frame in enumerate(container.decode(video=0)):\n-        ...         if i > end_index:\n-        ...             break\n-        ...         if i >= start_index and i in indices:\n-        ...             frames.append(frame)\n-        ...     return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n-\n-\n-        >>> def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n-        ...     '''\n-        ...     Sample a given number of frame indices from the video.\n-        ...     Args:\n-        ...         clip_len (`int`): Total number of frames to sample.\n-        ...         frame_sample_rate (`int`): Sample every n-th frame.\n-        ...         seg_len (`int`): Maximum allowed index of sample's last frame.\n-        ...     Returns:\n-        ...         indices (`list[int]`): List of sampled frame indices\n-        ...     '''\n-        ...     converted_len = int(clip_len * frame_sample_rate)\n-        ...     end_idx = np.random.randint(converted_len, seg_len)\n-        ...     start_idx = end_idx - converted_len\n-        ...     indices = np.linspace(start_idx, end_idx, num=clip_len)\n-        ...     indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n-        ...     return indices\n-\n-\n-        >>> # video clip consists of 300 frames (10 seconds at 30 FPS)\n-        >>> file_path = hf_hub_download(\n+        >>> # replace this with your own video file\n+        >>> video_path = hf_hub_download(\n         ...     repo_id=\"nielsr/video-demo\", filename=\"eating_spaghetti.mp4\", repo_type=\"dataset\"\n         ... )\n-        >>> container = av.open(file_path)\n \n-        >>> # sample 16 frames\n-        >>> indices = sample_frame_indices(clip_len=16, frame_sample_rate=1, seg_len=container.streams.video[0].frames)\n-        >>> video = read_video_pyav(container, indices)\n-\n-        >>> image_processor = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n+        >>> video_processor = VideoMAEVideoProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n         >>> model = VideoMAEModel.from_pretrained(\"MCG-NJU/videomae-base\")\n \n         >>> # prepare video for the model\n-        >>> inputs = image_processor(list(video), return_tensors=\"pt\")\n+        >>> inputs = video_processor(video_path, return_tensors=\"pt\")\n \n         >>> # forward pass\n-        >>> outputs = model(**inputs)\n+        >>> with torch.no_grad():\n+        ...     outputs = model(**inputs)\n+\n         >>> last_hidden_states = outputs.last_hidden_state\n         >>> list(last_hidden_states.shape)\n         [1, 1568, 768]\n@@ -764,69 +717,19 @@ def forward(\n         Examples:\n \n         ```python\n-        >>> import av\n         >>> import torch\n-        >>> import numpy as np\n-\n-        >>> from transformers import AutoImageProcessor, VideoMAEForVideoClassification\n+        >>> from transformers import VideoMAEVideoProcessor, VideoMAEForVideoClassification\n         >>> from huggingface_hub import hf_hub_download\n \n-        >>> np.random.seed(0)\n-\n-\n-        >>> def read_video_pyav(container, indices):\n-        ...     '''\n-        ...     Decode the video with PyAV decoder.\n-        ...     Args:\n-        ...         container (`av.container.input.InputContainer`): PyAV container.\n-        ...         indices (`list[int]`): List of frame indices to decode.\n-        ...     Returns:\n-        ...         result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n-        ...     '''\n-        ...     frames = []\n-        ...     container.seek(0)\n-        ...     start_index = indices[0]\n-        ...     end_index = indices[-1]\n-        ...     for i, frame in enumerate(container.decode(video=0)):\n-        ...         if i > end_index:\n-        ...             break\n-        ...         if i >= start_index and i in indices:\n-        ...             frames.append(frame)\n-        ...     return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n-\n-\n-        >>> def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n-        ...     '''\n-        ...     Sample a given number of frame indices from the video.\n-        ...     Args:\n-        ...         clip_len (`int`): Total number of frames to sample.\n-        ...         frame_sample_rate (`int`): Sample every n-th frame.\n-        ...         seg_len (`int`): Maximum allowed index of sample's last frame.\n-        ...     Returns:\n-        ...         indices (`list[int]`): List of sampled frame indices\n-        ...     '''\n-        ...     converted_len = int(clip_len * frame_sample_rate)\n-        ...     end_idx = np.random.randint(converted_len, seg_len)\n-        ...     start_idx = end_idx - converted_len\n-        ...     indices = np.linspace(start_idx, end_idx, num=clip_len)\n-        ...     indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n-        ...     return indices\n-\n-\n-        >>> # video clip consists of 300 frames (10 seconds at 30 FPS)\n-        >>> file_path = hf_hub_download(\n+        >>> # replace this with your own video file\n+        >>> video_path = hf_hub_download(\n         ...     repo_id=\"nielsr/video-demo\", filename=\"eating_spaghetti.mp4\", repo_type=\"dataset\"\n         ... )\n-        >>> container = av.open(file_path)\n-\n-        >>> # sample 16 frames\n-        >>> indices = sample_frame_indices(clip_len=16, frame_sample_rate=1, seg_len=container.streams.video[0].frames)\n-        >>> video = read_video_pyav(container, indices)\n \n-        >>> image_processor = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base-finetuned-kinetics\")\n+        >>> video_processor = VideoMAEVideoProcessor.from_pretrained(\"MCG-NJU/videomae-base-finetuned-kinetics\")\n         >>> model = VideoMAEForVideoClassification.from_pretrained(\"MCG-NJU/videomae-base-finetuned-kinetics\")\n \n-        >>> inputs = image_processor(list(video), return_tensors=\"pt\")\n+        >>> inputs = video_processor(video_path, return_tensors=\"pt\")\n \n         >>> with torch.no_grad():\n         ...     outputs = model(**inputs)"
      },
      {
        "filename": "src/transformers/models/videomae/video_processing_videomae.py",
        "status": "added",
        "additions": 43,
        "deletions": 0,
        "changes": 43,
        "patch": "@@ -0,0 +1,43 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Video processor class for VideoMAE.\"\"\"\n+\n+from ...image_utils import IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD, PILImageResampling\n+from ...video_processing_utils import BaseVideoProcessor\n+\n+\n+class VideoMAEVideoProcessor(BaseVideoProcessor):\n+    resample = PILImageResampling.BILINEAR\n+    image_mean = IMAGENET_STANDARD_MEAN\n+    image_std = IMAGENET_STANDARD_STD\n+    size = {\"shortest_edge\": 224}\n+    default_to_square = False\n+    crop_size = {\"height\": 224, \"width\": 224}\n+    do_resize = True\n+    do_center_crop = True\n+    do_rescale = True\n+    rescale_factor = 1 / 255\n+    do_normalize = True\n+    do_convert_rgb = True\n+    do_sample_frames = False  # Set to False for backward compatibility with image processor workflows.\n+    model_input_names = [\"pixel_values\"]\n+\n+    def preprocess(self, videos, **kwargs):\n+        batch = super().preprocess(videos, **kwargs)\n+        batch[\"pixel_values\"] = batch.pop(\"pixel_values_videos\")\n+        return batch\n+\n+\n+__all__ = [\"VideoMAEVideoProcessor\"]"
      },
      {
        "filename": "tests/models/videomae/test_video_processing_videomae.py",
        "status": "added",
        "additions": 160,
        "deletions": 0,
        "changes": 160,
        "patch": "@@ -0,0 +1,160 @@\n+# coding=utf-8\n+# Copyright 2025 HuggingFace Inc.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import unittest\n+\n+import torch\n+from PIL import Image\n+\n+from transformers.image_utils import IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD\n+from transformers.testing_utils import require_torch, require_torchvision, require_vision\n+from transformers.utils import is_torchvision_available, is_vision_available\n+\n+from ...test_video_processing_common import VideoProcessingTestMixin, prepare_video_inputs\n+\n+\n+if is_vision_available():\n+    if is_torchvision_available():\n+        from transformers import VideoMAEImageProcessor, VideoMAEVideoProcessor\n+\n+\n+class VideoMAEVideoProcessingTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=5,\n+        num_frames=8,\n+        num_channels=3,\n+        image_size=18,\n+        min_resolution=30,\n+        max_resolution=80,\n+        do_resize=True,\n+        size=None,\n+        do_center_crop=True,\n+        crop_size=None,\n+        do_rescale=True,\n+        rescale_factor=1 / 255,\n+        do_normalize=True,\n+        image_mean=IMAGENET_STANDARD_MEAN,\n+        image_std=IMAGENET_STANDARD_STD,\n+        do_convert_rgb=True,\n+    ):\n+        super().__init__()\n+        size = size if size is not None else {\"shortest_edge\": 20}\n+        crop_size = crop_size if crop_size is not None else {\"height\": 18, \"width\": 18}\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.num_frames = num_frames\n+        self.num_channels = num_channels\n+        self.image_size = image_size\n+        self.min_resolution = min_resolution\n+        self.max_resolution = max_resolution\n+        self.do_resize = do_resize\n+        self.size = size\n+        self.do_center_crop = do_center_crop\n+        self.crop_size = crop_size\n+        self.do_rescale = do_rescale\n+        self.rescale_factor = rescale_factor\n+        self.do_normalize = do_normalize\n+        self.image_mean = image_mean\n+        self.image_std = image_std\n+        self.do_convert_rgb = do_convert_rgb\n+\n+    def prepare_video_processor_dict(self):\n+        return {\n+            \"do_resize\": self.do_resize,\n+            \"size\": self.size,\n+            \"do_center_crop\": self.do_center_crop,\n+            \"crop_size\": self.crop_size,\n+            \"do_rescale\": self.do_rescale,\n+            \"rescale_factor\": self.rescale_factor,\n+            \"do_normalize\": self.do_normalize,\n+            \"image_mean\": self.image_mean,\n+            \"image_std\": self.image_std,\n+            \"do_convert_rgb\": self.do_convert_rgb,\n+        }\n+\n+    def expected_output_video_shape(self, videos):\n+        return self.num_frames, self.num_channels, self.crop_size[\"height\"], self.crop_size[\"width\"]\n+\n+    def prepare_video_inputs(self, equal_resolution=False, return_tensors=\"pil\"):\n+        videos = prepare_video_inputs(\n+            batch_size=self.batch_size,\n+            num_frames=self.num_frames,\n+            num_channels=self.num_channels,\n+            min_resolution=self.min_resolution,\n+            max_resolution=self.max_resolution,\n+            equal_resolution=equal_resolution,\n+            return_tensors=return_tensors,\n+        )\n+\n+        return videos\n+\n+\n+@require_torch\n+@require_vision\n+@require_torchvision\n+class VideoMAEVideoProcessingTest(VideoProcessingTestMixin, unittest.TestCase):\n+    fast_video_processing_class = VideoMAEVideoProcessor if is_torchvision_available() else None\n+    input_name = \"pixel_values\"\n+\n+    def setUp(self):\n+        super().setUp()\n+        self.video_processor_tester = VideoMAEVideoProcessingTester(self)\n+\n+    @property\n+    def video_processor_dict(self):\n+        return self.video_processor_tester.prepare_video_processor_dict()\n+\n+    def test_video_processor_properties(self):\n+        video_processing = self.fast_video_processing_class(**self.video_processor_dict)\n+        self.assertTrue(hasattr(video_processing, \"do_resize\"))\n+        self.assertTrue(hasattr(video_processing, \"size\"))\n+        self.assertTrue(hasattr(video_processing, \"do_center_crop\"))\n+        self.assertTrue(hasattr(video_processing, \"center_crop\"))\n+        self.assertTrue(hasattr(video_processing, \"do_normalize\"))\n+        self.assertTrue(hasattr(video_processing, \"image_mean\"))\n+        self.assertTrue(hasattr(video_processing, \"image_std\"))\n+        self.assertTrue(hasattr(video_processing, \"do_convert_rgb\"))\n+        self.assertTrue(hasattr(video_processing, \"model_input_names\"))\n+        self.assertIn(\"pixel_values\", video_processing.model_input_names)\n+\n+    def test_pixel_value_identity(self):\n+        \"\"\"\n+        Verify that VideoMAEVideoProcessor (TorchCodec-based) produces pixel tensors\n+        numerically similar to those from VideoMAEImageProcessor (PIL-based).\n+        Minor (<1%) differences are expected due to color conversion and interpolation.\n+        \"\"\"\n+        video = self.video_processor_tester.prepare_video_inputs(return_tensors=\"np\")\n+        video_processor = VideoMAEVideoProcessor(**self.video_processor_dict)\n+        image_processor = VideoMAEImageProcessor(**self.video_processor_dict)\n+\n+        video_frames_np = video[0]\n+        video_frames_pil = [Image.fromarray(frame.astype(\"uint8\")) for frame in video_frames_np]\n+        video_out = video_processor(video_frames_pil, return_tensors=\"pt\")\n+        image_out = image_processor(video_frames_pil, return_tensors=\"pt\")\n+\n+        torch.testing.assert_close(\n+            video_out[\"pixel_values\"],\n+            image_out[\"pixel_values\"],\n+            rtol=5e-2,\n+            atol=1e-2,\n+            msg=(\n+                \"Pixel values differ slightly between VideoMAEVideoProcessor \"\n+                \"and VideoMAEImageProcessor. \"\n+                \"Differences \u22641% are expected due to YUV\u2192RGB conversion and \"\n+                \"interpolation behavior in different decoders.\"\n+            ),\n+        )"
      }
    ],
    "num_files": 6,
    "scraped_at": "2025-11-16T21:17:44.791440"
  },
  {
    "pr_number": 41514,
    "title": "delete some tokenizer tests using pickle",
    "body": "# What does this PR do?\r\n\r\nThere is no room for `pickle` within `transformers`!",
    "html_url": "https://github.com/huggingface/transformers/pull/41514",
    "created_at": "2025-10-10T13:54:34Z",
    "merged_at": "2025-10-14T12:50:52Z",
    "merge_commit_sha": "abf5b57a684e665f03514535a53a668ddcc72303",
    "base_ref": "main",
    "head_sha": "15d109ff9376bca531ed89a004e3801a0f4126a2",
    "user": "ydshieh",
    "files": [
      {
        "filename": "tests/models/bert_japanese/test_tokenization_bert_japanese.py",
        "status": "modified",
        "additions": 0,
        "deletions": 63,
        "changes": 63,
        "patch": "@@ -14,7 +14,6 @@\n \n \n import os\n-import pickle\n import unittest\n \n from transformers import AutoTokenizer\n@@ -103,26 +102,6 @@ def test_full_tokenizer(self):\n         self.assertListEqual(tokens, [\"\u3053\u3093\u306b\u3061\u306f\", \"\u3001\", \"\u4e16\u754c\", \"\u3002\", \"\u3053\u3093\", \"##\u3070\u3093\u306f\", \"\u3001\", \"\u4e16\u754c\", \"\u3002\"])\n         self.assertListEqual(tokenizer.convert_tokens_to_ids(tokens), [3, 12, 10, 14, 4, 9, 12, 10, 14])\n \n-    def test_pickle_mecab_tokenizer(self):\n-        tokenizer = self.tokenizer_class(self.vocab_file, word_tokenizer_type=\"mecab\")\n-        self.assertIsNotNone(tokenizer)\n-\n-        text = \"\u3053\u3093\u306b\u3061\u306f\u3001\u4e16\u754c\u3002\\n\u3053\u3093\u3070\u3093\u306f\u3001\u4e16\u754c\u3002\"\n-        tokens = tokenizer.tokenize(text)\n-        self.assertListEqual(tokens, [\"\u3053\u3093\u306b\u3061\u306f\", \"\u3001\", \"\u4e16\u754c\", \"\u3002\", \"\u3053\u3093\", \"##\u3070\u3093\u306f\", \"\u3001\", \"\u4e16\u754c\", \"\u3002\"])\n-        self.assertListEqual(tokenizer.convert_tokens_to_ids(tokens), [3, 12, 10, 14, 4, 9, 12, 10, 14])\n-\n-        filename = os.path.join(self.tmpdirname, \"tokenizer.bin\")\n-        with open(filename, \"wb\") as handle:\n-            pickle.dump(tokenizer, handle)\n-\n-        with open(filename, \"rb\") as handle:\n-            tokenizer_new = pickle.load(handle)\n-\n-        tokens_loaded = tokenizer_new.tokenize(text)\n-\n-        self.assertListEqual(tokens, tokens_loaded)\n-\n     def test_mecab_full_tokenizer_with_mecab_kwargs(self):\n         tokenizer = self.tokenizer_class(\n             self.vocab_file, word_tokenizer_type=\"mecab\", mecab_kwargs={\"mecab_dic\": \"ipadic\"}\n@@ -198,27 +177,6 @@ def test_mecab_tokenizer_no_normalize(self):\n             [\"\uff71\uff6f\uff8c\uff9f\uff99\u30b9\u30c8\u30a2\", \"\u3067\", \"iPhone\", \"\uff18\", \"\u304c\", \"\u767a\u58f2\", \"\u3055\", \"\u308c\", \"\u305f\", \"\u3000\", \"\u3002\"],\n         )\n \n-    @require_sudachi_projection\n-    def test_pickle_sudachi_tokenizer(self):\n-        tokenizer = self.tokenizer_class(self.vocab_file, word_tokenizer_type=\"sudachi\")\n-        self.assertIsNotNone(tokenizer)\n-\n-        text = \"\u3053\u3093\u306b\u3061\u306f\u3001\u4e16\u754c\u3002\\n\u3053\u3093\u3070\u3093\u306f\u3001\u4e16\u754c\u3002\"\n-        tokens = tokenizer.tokenize(text)\n-        self.assertListEqual(tokens, [\"\u3053\u3093\u306b\u3061\u306f\", \"\u3001\", \"\u4e16\u754c\", \"\u3002\", \"\u3053\u3093\", \"##\u3070\u3093\u306f\", \"\u3001\", \"\u4e16\u754c\", \"\u3002\"])\n-        self.assertListEqual(tokenizer.convert_tokens_to_ids(tokens), [3, 12, 10, 14, 4, 9, 12, 10, 14])\n-\n-        filename = os.path.join(self.tmpdirname, \"tokenizer.bin\")\n-        with open(filename, \"wb\") as handle:\n-            pickle.dump(tokenizer, handle)\n-\n-        with open(filename, \"rb\") as handle:\n-            tokenizer_new = pickle.load(handle)\n-\n-        tokens_loaded = tokenizer_new.tokenize(text)\n-\n-        self.assertListEqual(tokens, tokens_loaded)\n-\n     @require_sudachi_projection\n     def test_sudachi_tokenizer_core(self):\n         tokenizer = SudachiTokenizer(sudachi_dict_type=\"core\")\n@@ -293,27 +251,6 @@ def test_sudachi_tokenizer_trim_whitespace(self):\n             [\"\u30a2\u30c3\u30d7\u30eb\", \"\u30b9\u30c8\u30a2\", \"\u3067\", \"iPhone\", \"8\", \"\u304c\", \"\u767a\u58f2\", \"\u3055\", \"\u308c\", \"\u305f\", \"\u3002\"],\n         )\n \n-    @require_jumanpp\n-    def test_pickle_jumanpp_tokenizer(self):\n-        tokenizer = self.tokenizer_class(self.vocab_file, word_tokenizer_type=\"jumanpp\")\n-        self.assertIsNotNone(tokenizer)\n-\n-        text = \"\u3053\u3093\u306b\u3061\u306f\u3001\u4e16\u754c\u3002\\n\u3053\u3093\u3070\u3093\u306f\u3001\u4e16\u754c\u3002\"\n-        tokens = tokenizer.tokenize(text)\n-        self.assertListEqual(tokens, [\"\u3053\u3093\u306b\u3061\u306f\", \"\u3001\", \"\u4e16\u754c\", \"\u3002\", \"\u3053\u3093\", \"##\u3070\u3093\u306f\", \"\u3001\", \"\u4e16\u754c\", \"\u3002\"])\n-        self.assertListEqual(tokenizer.convert_tokens_to_ids(tokens), [3, 12, 10, 14, 4, 9, 12, 10, 14])\n-\n-        filename = os.path.join(self.tmpdirname, \"tokenizer.bin\")\n-        with open(filename, \"wb\") as handle:\n-            pickle.dump(tokenizer, handle)\n-\n-        with open(filename, \"rb\") as handle:\n-            tokenizer_new = pickle.load(handle)\n-\n-        tokens_loaded = tokenizer_new.tokenize(text)\n-\n-        self.assertListEqual(tokens, tokens_loaded)\n-\n     @require_jumanpp\n     def test_jumanpp_tokenizer(self):\n         tokenizer = JumanppTokenizer()"
      },
      {
        "filename": "tests/models/code_llama/test_tokenization_code_llama.py",
        "status": "modified",
        "additions": 0,
        "deletions": 12,
        "changes": 12,
        "patch": "@@ -13,7 +13,6 @@\n # limitations under the License.\n \n import os\n-import pickle\n import shutil\n import tempfile\n import unittest\n@@ -293,17 +292,6 @@ def test_tokenizer_integration(self):\n             padding=False,\n         )\n \n-    def test_picklable(self):\n-        with tempfile.NamedTemporaryFile() as f:\n-            shutil.copyfile(SAMPLE_VOCAB, f.name)\n-            tokenizer = CodeLlamaTokenizer(f.name, keep_accents=True)\n-            pickled_tokenizer = pickle.dumps(tokenizer)\n-        pickle.loads(pickled_tokenizer)\n-\n-    @unittest.skip(reason=\"worker 'gw4' crashed on CI, passing locally.\")\n-    def test_pickle_subword_regularization_tokenizer(self):\n-        pass\n-\n     @unittest.skip(reason=\"worker 'gw4' crashed on CI, passing locally.\")\n     def test_subword_regularization_tokenizer(self):\n         pass"
      },
      {
        "filename": "tests/models/gemma/test_tokenization_gemma.py",
        "status": "modified",
        "additions": 0,
        "deletions": 4,
        "changes": 4,
        "patch": "@@ -140,10 +140,6 @@ def test_tokenizer_integration(self):\n             padding=False,\n         )\n \n-    @unittest.skip(reason=\"worker 'gw4' crashed on CI, passing locally.\")\n-    def test_pickle_subword_regularization_tokenizer(self):\n-        pass\n-\n     @unittest.skip(reason=\"worker 'gw4' crashed on CI, passing locally.\")\n     def test_subword_regularization_tokenizer(self):\n         pass"
      },
      {
        "filename": "tests/models/llama/test_tokenization_llama.py",
        "status": "modified",
        "additions": 0,
        "deletions": 12,
        "changes": 12,
        "patch": "@@ -13,7 +13,6 @@\n # limitations under the License.\n \n import os\n-import pickle\n import shutil\n import tempfile\n import unittest\n@@ -291,17 +290,6 @@ def test_tokenizer_integration(self):\n             padding=False,\n         )\n \n-    def test_picklable(self):\n-        with tempfile.NamedTemporaryFile() as f:\n-            shutil.copyfile(SAMPLE_VOCAB, f.name)\n-            tokenizer = LlamaTokenizer(f.name, keep_accents=True)\n-            pickled_tokenizer = pickle.dumps(tokenizer)\n-        pickle.loads(pickled_tokenizer)\n-\n-    @unittest.skip(reason=\"worker 'gw4' crashed on CI, passing locally.\")\n-    def test_pickle_subword_regularization_tokenizer(self):\n-        pass\n-\n     @unittest.skip(reason=\"worker 'gw4' crashed on CI, passing locally.\")\n     def test_subword_regularization_tokenizer(self):\n         pass"
      },
      {
        "filename": "tests/models/moshi/test_tokenization_moshi.py",
        "status": "modified",
        "additions": 0,
        "deletions": 15,
        "changes": 15,
        "patch": "@@ -13,9 +13,6 @@\n # limitations under the License.\n \n import inspect\n-import pickle\n-import shutil\n-import tempfile\n import unittest\n \n from transformers import (\n@@ -171,18 +168,6 @@ def test_special_tokens_initialization(self):\n \n                 self.assertTrue(special_token_id in r_output)\n \n-    def test_picklable(self):\n-        with tempfile.NamedTemporaryFile() as f:\n-            shutil.copyfile(SAMPLE_VOCAB, f.name)\n-            tokenizer = PreTrainedTokenizerFast(\n-                tokenizer_object=MoshiConverter(vocab_file=f.name).converted(),\n-                bos_token=\"<s>\",\n-                unk_token=\"<unk>\",\n-                eos_token=\"</s>\",\n-            )\n-            pickled_tokenizer = pickle.dumps(tokenizer)\n-        pickle.loads(pickled_tokenizer)\n-\n     def test_training_new_tokenizer(self):\n         # This feature only exists for fast tokenizers\n         if not self.test_rust_tokenizer:"
      },
      {
        "filename": "tests/models/pop2piano/test_tokenization_pop2piano.py",
        "status": "modified",
        "additions": 0,
        "deletions": 19,
        "changes": 19,
        "patch": "@@ -15,8 +15,6 @@\n Please note that Pop2PianoTokenizer is too far from our usual tokenizers and thus cannot use the TokenizerTesterMixin class.\n \"\"\"\n \n-import os\n-import pickle\n import shutil\n import tempfile\n import unittest\n@@ -224,23 +222,6 @@ def test_save_and_load_tokenizer(self):\n \n         shutil.rmtree(tmpdirname)\n \n-    def test_pickle_tokenizer(self):\n-        tmpdirname = tempfile.mkdtemp()\n-\n-        notes = self.get_input_notes()\n-        subwords = self.tokenizer(notes)[\"token_ids\"]\n-\n-        filename = os.path.join(tmpdirname, \"tokenizer.bin\")\n-        with open(filename, \"wb\") as handle:\n-            pickle.dump(self.tokenizer, handle)\n-\n-        with open(filename, \"rb\") as handle:\n-            tokenizer_new = pickle.load(handle)\n-\n-        subwords_loaded = tokenizer_new(notes)[\"token_ids\"]\n-\n-        self.assertListEqual(subwords, subwords_loaded)\n-\n     def test_padding_side_in_kwargs(self):\n         tokenizer_p = Pop2PianoTokenizer.from_pretrained(\"sweetcocoa/pop2piano\", padding_side=\"left\")\n         self.assertEqual(tokenizer_p.padding_side, \"left\")"
      },
      {
        "filename": "tests/models/seamless_m4t/test_tokenization_seamless_m4t.py",
        "status": "modified",
        "additions": 0,
        "deletions": 4,
        "changes": 4,
        "patch": "@@ -426,10 +426,6 @@ def test_training_new_tokenizer(self):\n \n         self.assertDictEqual(tokenizer.special_tokens_map, new_tokenizer.special_tokens_map)\n \n-    @unittest.skip(reason=\"Fails because of the hack of adding <unk> in _tokenize\")\n-    def test_pickle_subword_regularization_tokenizer(self):\n-        pass\n-\n     @unittest.skip(reason=\"Fails because of the hack of adding <unk> in _tokenize\")\n     def test_subword_regularization_tokenizer(self):\n         pass"
      },
      {
        "filename": "tests/models/siglip/test_tokenization_siglip.py",
        "status": "modified",
        "additions": 0,
        "deletions": 4,
        "changes": 4,
        "patch": "@@ -207,10 +207,6 @@ def test_eos_in_input(self):\n     def test_subword_regularization_tokenizer(self):\n         pass\n \n-    @unittest.skip(reason=\"SiglipTokenizer strips the punctuation\")\n-    def test_pickle_subword_regularization_tokenizer(self):\n-        pass\n-\n     # Copied from tests.models.t5.test_tokenization_t5.T5TokenizationTest.test_special_tokens_initialization with T5->Siglip\n     def test_special_tokens_initialization(self):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:"
      },
      {
        "filename": "tests/models/speecht5/test_tokenization_speecht5.py",
        "status": "modified",
        "additions": 0,
        "deletions": 4,
        "changes": 4,
        "patch": "@@ -143,10 +143,6 @@ def test_add_tokens_tokenizer(self):\n                 self.assertEqual(tokens[0], tokenizer.eos_token_id)\n                 self.assertEqual(tokens[-3], tokenizer.pad_token_id)\n \n-    @unittest.skip\n-    def test_pickle_subword_regularization_tokenizer(self):\n-        pass\n-\n     @unittest.skip\n     def test_subword_regularization_tokenizer(self):\n         pass"
      },
      {
        "filename": "tests/models/xglm/test_tokenization_xglm.py",
        "status": "modified",
        "additions": 0,
        "deletions": 10,
        "changes": 10,
        "patch": "@@ -12,9 +12,6 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-import pickle\n-import shutil\n-import tempfile\n import unittest\n from functools import cached_property\n \n@@ -141,13 +138,6 @@ def test_full_tokenizer(self):\n     def big_tokenizer(self):\n         return XGLMTokenizer.from_pretrained(\"facebook/xglm-564M\")\n \n-    def test_picklable_without_disk(self):\n-        with tempfile.NamedTemporaryFile() as f:\n-            shutil.copyfile(SAMPLE_VOCAB, f.name)\n-            tokenizer = XGLMTokenizer(f.name, keep_accents=True)\n-            pickled_tokenizer = pickle.dumps(tokenizer)\n-        pickle.loads(pickled_tokenizer)\n-\n     def test_rust_and_python_full_tokenizers(self):\n         if not self.test_rust_tokenizer:\n             self.skipTest(reason=\"test_rust_tokenizer is set to False\")"
      },
      {
        "filename": "tests/models/xlm_roberta/test_tokenization_xlm_roberta.py",
        "status": "modified",
        "additions": 0,
        "deletions": 8,
        "changes": 8,
        "patch": "@@ -12,7 +12,6 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-import pickle\n import shutil\n import tempfile\n import unittest\n@@ -215,13 +214,6 @@ def test_save_pretrained(self):\n     def big_tokenizer(self):\n         return XLMRobertaTokenizer.from_pretrained(\"FacebookAI/xlm-roberta-base\")\n \n-    def test_picklable_without_disk(self):\n-        with tempfile.NamedTemporaryFile() as f:\n-            shutil.copyfile(SAMPLE_VOCAB, f.name)\n-            tokenizer = XLMRobertaTokenizer(f.name, keep_accents=True)\n-            pickled_tokenizer = pickle.dumps(tokenizer)\n-        pickle.loads(pickled_tokenizer)\n-\n     def test_rust_and_python_full_tokenizers(self):\n         if not self.test_rust_tokenizer:\n             self.skipTest(reason=\"test_rust_tokenizer is set to False\")"
      },
      {
        "filename": "tests/test_tokenization_common.py",
        "status": "modified",
        "additions": 0,
        "deletions": 51,
        "changes": 51,
        "patch": "@@ -18,7 +18,6 @@\n import itertools\n import json\n import os\n-import pickle\n import re\n import shutil\n import tempfile\n@@ -520,28 +519,6 @@ def test_subword_regularization_tokenizer(self) -> None:\n             },\n         )\n \n-    def test_pickle_subword_regularization_tokenizer(self) -> None:\n-        if not self.test_sentencepiece:\n-            self.skipTest(reason=\"test_sentencepiece is set to False\")\n-\n-        \"\"\"Google pickle __getstate__ __setstate__ if you are struggling with this.\"\"\"\n-        # Subword regularization is only available for the slow tokenizer.\n-        sp_model_kwargs = {\"enable_sampling\": True, \"alpha\": 0.1, \"nbest_size\": -1}\n-        tokenizer = self.get_tokenizer(sp_model_kwargs=sp_model_kwargs)\n-        tokenizer_bin = pickle.dumps(tokenizer)\n-        del tokenizer\n-        tokenizer_new = pickle.loads(tokenizer_bin)\n-\n-        run_test_in_subprocess(\n-            test_case=self,\n-            target_func=_test_subword_regularization_tokenizer,\n-            inputs={\n-                \"tokenizer\": tokenizer_new,\n-                \"sp_model_kwargs\": sp_model_kwargs,\n-                \"test_sentencepiece_ignore_case\": self.test_sentencepiece_ignore_case,\n-            },\n-        )\n-\n     def test_save_sentencepiece_tokenizer(self) -> None:\n         if not self.test_sentencepiece or not self.test_slow_tokenizer:\n             self.skipTest(reason=\"test_sentencepiece or test_slow_tokenizer is set to False\")\n@@ -827,34 +804,6 @@ def test_save_and_load_tokenizer(self):\n \n                 shutil.rmtree(tmpdirname)\n \n-    def test_pickle_tokenizer(self):\n-        \"\"\"Google pickle __getstate__ __setstate__ if you are struggling with this.\"\"\"\n-        tokenizers = self.get_tokenizers()\n-        for tokenizer in tokenizers:\n-            with self.subTest(f\"{tokenizer.__class__.__name__}\"):\n-                self.assertIsNotNone(tokenizer)\n-\n-                text = \"Munich and Berlin are nice cities\"\n-                subwords = tokenizer.tokenize(text)\n-\n-                filename = os.path.join(self.tmpdirname, \"tokenizer.bin\")\n-                with open(filename, \"wb\") as handle:\n-                    pickle.dump(tokenizer, handle)\n-\n-                with open(filename, \"rb\") as handle:\n-                    tokenizer_new = pickle.load(handle)\n-\n-                subwords_loaded = tokenizer_new.tokenize(text)\n-\n-                self.assertListEqual(subwords, subwords_loaded)\n-\n-    @require_tokenizers\n-    def test_pickle_added_tokens(self):\n-        tok1 = AddedToken(\"<s>\", rstrip=True, lstrip=True, normalized=False, single_word=True)\n-        tok2 = pickle.loads(pickle.dumps(tok1))\n-\n-        self.assertEqual(tok1.__getstate__(), tok2.__getstate__())\n-\n     def test_added_tokens_do_lower_case(self):\n         tokenizers = self.get_tokenizers(do_lower_case=True)\n         for tokenizer in tokenizers:"
      },
      {
        "filename": "tests/tokenization/test_tokenization_utils.py",
        "status": "modified",
        "additions": 0,
        "deletions": 65,
        "changes": 65,
        "patch": "@@ -16,11 +16,8 @@\n \"\"\"\n \n import os\n-import pickle\n import tempfile\n import unittest\n-from collections.abc import Callable\n-from typing import Optional\n \n import numpy as np\n \n@@ -66,28 +63,6 @@ def check_tokenizer_from_pretrained(self, tokenizer_class):\n                 special_tok_id = tokenizer.convert_tokens_to_ids(special_tok)\n                 self.assertIsInstance(special_tok_id, int)\n \n-    def assert_dump_and_restore(self, be_original: BatchEncoding, equal_op: Optional[Callable] = None):\n-        batch_encoding_str = pickle.dumps(be_original)\n-        self.assertIsNotNone(batch_encoding_str)\n-\n-        be_restored = pickle.loads(batch_encoding_str)\n-\n-        # Ensure is_fast is correctly restored\n-        self.assertEqual(be_restored.is_fast, be_original.is_fast)\n-\n-        # Ensure encodings are potentially correctly restored\n-        if be_original.is_fast:\n-            self.assertIsNotNone(be_restored.encodings)\n-        else:\n-            self.assertIsNone(be_restored.encodings)\n-\n-        # Ensure the keys are the same\n-        for original_v, restored_v in zip(be_original.values(), be_restored.values()):\n-            if equal_op:\n-                self.assertTrue(equal_op(restored_v, original_v))\n-            else:\n-                self.assertEqual(restored_v, original_v)\n-\n     @slow\n     def test_pretrained_tokenizers(self):\n         self.check_tokenizer_from_pretrained(GPT2Tokenizer)\n@@ -96,46 +71,6 @@ def test_tensor_type_from_str(self):\n         self.assertEqual(TensorType(\"pt\"), TensorType.PYTORCH)\n         self.assertEqual(TensorType(\"np\"), TensorType.NUMPY)\n \n-    @require_tokenizers\n-    def test_batch_encoding_pickle(self):\n-        tokenizer_p = BertTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n-        tokenizer_r = BertTokenizerFast.from_pretrained(\"google-bert/bert-base-cased\")\n-\n-        # Python no tensor\n-        with self.subTest(\"BatchEncoding (Python, return_tensors=None)\"):\n-            self.assert_dump_and_restore(tokenizer_p(\"Small example to encode\"))\n-\n-        with self.subTest(\"BatchEncoding (Python, return_tensors=NUMPY)\"):\n-            self.assert_dump_and_restore(\n-                tokenizer_p(\"Small example to encode\", return_tensors=TensorType.NUMPY), np.array_equal\n-            )\n-\n-        with self.subTest(\"BatchEncoding (Rust, return_tensors=None)\"):\n-            self.assert_dump_and_restore(tokenizer_r(\"Small example to encode\"))\n-\n-        with self.subTest(\"BatchEncoding (Rust, return_tensors=NUMPY)\"):\n-            self.assert_dump_and_restore(\n-                tokenizer_r(\"Small example to encode\", return_tensors=TensorType.NUMPY), np.array_equal\n-            )\n-\n-    @require_torch\n-    @require_tokenizers\n-    def test_batch_encoding_pickle_pt(self):\n-        import torch\n-\n-        tokenizer_p = BertTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n-        tokenizer_r = BertTokenizerFast.from_pretrained(\"google-bert/bert-base-cased\")\n-\n-        with self.subTest(\"BatchEncoding (Python, return_tensors=PYTORCH)\"):\n-            self.assert_dump_and_restore(\n-                tokenizer_p(\"Small example to encode\", return_tensors=TensorType.PYTORCH), torch.equal\n-            )\n-\n-        with self.subTest(\"BatchEncoding (Rust, return_tensors=PYTORCH)\"):\n-            self.assert_dump_and_restore(\n-                tokenizer_r(\"Small example to encode\", return_tensors=TensorType.PYTORCH), torch.equal\n-            )\n-\n     @require_tokenizers\n     def test_batch_encoding_is_fast(self):\n         tokenizer_p = BertTokenizer.from_pretrained(\"google-bert/bert-base-cased\")"
      }
    ],
    "num_files": 13,
    "scraped_at": "2025-11-16T21:17:46.160624"
  },
  {
    "pr_number": 41507,
    "title": "[kernels] rm mra kernels",
    "body": "# What does this PR do?\r\n\r\nRemoves the mra kernels, and uses kernels from the hub : https://huggingface.co/kernels-community/mra instead",
    "html_url": "https://github.com/huggingface/transformers/pull/41507",
    "created_at": "2025-10-10T09:49:55Z",
    "merged_at": "2025-10-14T11:34:04Z",
    "merge_commit_sha": "8fe4db53994cdccf7629284add65655e6ce73af4",
    "base_ref": "main",
    "head_sha": "3d5f6a4aa53573b3bceae2d486dd36cd46495f46",
    "user": "MekkCyber",
    "files": [
      {
        "filename": "src/transformers/kernels/mra/cuda_kernel.cu",
        "status": "removed",
        "additions": 0,
        "deletions": 383,
        "changes": 383,
        "patch": "@@ -1,383 +0,0 @@\n-#include \"cuda_kernel.h\"\n-\n-//////////////////////////////////////////////////////////////////////////////////////////////////\n-//////////////////////////////////////////////////////////////////////////////////////////////////\n-\n-__global__ void index_max_cuda_kernel(\n-  float *index_vals,       // [batch_size, 32, num_block]\n-  int   *indices,        // [batch_size, num_block]\n-  float *max_vals,        // [batch_size, A_num_block * 32]\n-  float *max_vals_scatter,   // [batch_size, 32, num_block]\n-  long batch_size,\n-  long A_num_block,\n-  long B_num_block,\n-  long num_block\n-) {\n-\n-  long batch_idx = blockIdx.x;\n-\n-  long thread_idx = threadIdx.x;\n-  long num_thread = blockDim.x;\n-\n-  extern __shared__ float buffer[];\n-  int *max_buffer = (int*)buffer;\n-\n-  for (int i = 0; i < A_num_block * 32; i = i + num_thread) {\n-    int idx = i + thread_idx;\n-    if (idx < A_num_block * 32) {\n-      max_buffer[idx] = -1e8;\n-    }\n-  }\n-  __syncthreads();\n-\n-  int *indices_pt = &indices[batch_idx * num_block];\n-  float *index_vals_pt = &index_vals[batch_idx * num_block * 32];\n-\n-  for (int idx_start = 0; idx_start < 32 * num_block; idx_start = idx_start + num_thread) {\n-    int idx = idx_start + thread_idx;\n-    int A_block_idx = indices_pt[idx % num_block] / B_num_block;\n-    atomicMax(&max_buffer[A_block_idx * 32 + idx / num_block], (int)(index_vals_pt[idx] * 1000));\n-  }\n-  __syncthreads();\n-  \n-  float *max_vals_pt = &max_vals[batch_idx * A_num_block * 32];\n-  for (int i = 0; i < A_num_block * 32; i = i + num_thread) {\n-    int idx = i + thread_idx;\n-    if (idx < A_num_block * 32) {\n-      max_vals_pt[idx] = (float)max_buffer[idx] / 1000.;\n-    }\n-  }\n-  \n-  float *max_vals_scatter_pt = &max_vals_scatter[batch_idx * num_block * 32];\n-  for (int idx_start = 0; idx_start < 32 * num_block; idx_start = idx_start + num_thread) {\n-    int idx = idx_start + thread_idx;\n-    int A_block_idx = indices_pt[idx % num_block] / B_num_block;\n-    max_vals_scatter_pt[idx] = (float)max_buffer[A_block_idx * 32 + idx / num_block] / 1000.;\n-  }\n-\n-}\n-\n-__global__ void mm_to_sparse_cuda_kernel(\n-  float *dense_A,   // [batch_size, A_num_block, dim, 32]\n-  float *dense_B,   // [batch_size, B_num_block, dim, 32]\n-  int   *indices,   // [batch_size, num_block]\n-  float *sparse_C,  // [batch_size, num_block, 32, 32]\n-  long batch_size,\n-  long A_num_block,\n-  long B_num_block,\n-  long dim,\n-  long num_block\n-) {\n-\n-  long batch_idx = blockIdx.y;\n-  long block_idx = blockIdx.x * blockDim.y + threadIdx.y;\n-\n-  long thread_idx = threadIdx.x;\n-\n-  __shared__ float buffer[4096];\n-  float *A_buffer = &buffer[threadIdx.y * 1024]; // [2, 8, 32]\n-  float *B_buffer = &buffer[threadIdx.y * 1024 + 512]; // [2, 8, 32]\n-\n-  long batch_idx__block_idx = batch_idx * num_block + block_idx;\n-\n-  long AB_block_idx = indices[batch_idx__block_idx];\n-  float *dense_A_pt = &dense_A[(batch_idx * A_num_block + AB_block_idx / B_num_block) * dim * 32];\n-  float *dense_B_pt = &dense_B[(batch_idx * B_num_block + AB_block_idx % B_num_block) * dim * 32];\n-\n-  int reg_1_idx = thread_idx / 8;    // [0000000011111111222222223333333344444444555555556666666677777777]\n-  int reg_2_idx = thread_idx % 8;    // [0123456701234567012345670123456701234567012345670123456701234567]\n-\n-  float reg_1[8];\n-  float reg_2[8];\n-\n-  float reg_array[16] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};\n-\n-  #pragma unroll\n-  for (int i = 0; i < 4; i++) {\n-    A_buffer[i * 64 + thread_idx] = dense_A_pt[i * 64 + thread_idx];\n-    B_buffer[i * 64 + thread_idx] = dense_B_pt[i * 64 + thread_idx];\n-  }\n-\n-  __syncthreads();\n-\n-  #pragma unroll\n-  for (int i = 0; i < 4; i++) {\n-    reg_1[i] = A_buffer[reg_1_idx * 4 + i];\n-    reg_2[i] = B_buffer[reg_2_idx * 4 + i];\n-  }\n-\n-  for (int dim_stride = 1; dim_stride < (dim / 8); dim_stride++) {\n-\n-    #pragma unroll\n-    for (int i = 0; i < 4; i++) {\n-      A_buffer[(dim_stride % 2) * 256 + i * 64 + thread_idx] = dense_A_pt[dim_stride * 256 + i * 64 + thread_idx];\n-      B_buffer[(dim_stride % 2) * 256 + i * 64 + thread_idx] = dense_B_pt[dim_stride * 256 + i * 64 + thread_idx];\n-    }\n-\n-    #pragma unroll\n-    for (int mini_dim_idx = 1; mini_dim_idx < 8; mini_dim_idx++) {\n-      #pragma unroll\n-      for (int i = 0; i < 4; i++) {\n-        reg_1[(mini_dim_idx % 2) * 4 + i] = A_buffer[((dim_stride - 1) % 2) * 256 + mini_dim_idx * 32 + reg_1_idx * 4 + i];\n-        reg_2[(mini_dim_idx % 2) * 4 + i] = B_buffer[((dim_stride - 1) % 2) * 256 + mini_dim_idx * 32 + reg_2_idx * 4 + i];\n-      }\n-      #pragma unroll\n-      for (int i = 0; i < 4; i++) {\n-        #pragma unroll\n-        for (int j = 0; j < 4; j++) {\n-          reg_array[i * 4 + j] += reg_1[((mini_dim_idx - 1) % 2) * 4 + i] * reg_2[((mini_dim_idx - 1) % 2) * 4 + j];\n-        }\n-      }\n-    }\n-\n-    __syncthreads();\n-\n-    #pragma unroll\n-    for (int i = 0; i < 4; i++) {\n-      reg_1[i] = A_buffer[(dim_stride % 2) * 256 + reg_1_idx * 4 + i];\n-      reg_2[i] = B_buffer[(dim_stride % 2) * 256 + reg_2_idx * 4 + i];\n-    }\n-\n-    #pragma unroll\n-    for (int i = 0; i < 4; i++) {\n-      #pragma unroll\n-      for (int j = 0; j < 4; j++) {\n-        reg_array[i * 4 + j] += reg_1[4 + i] * reg_2[4 + j];\n-      }\n-    }\n-\n-  }\n-\n-  #pragma unroll\n-  for (int mini_dim_idx = 1; mini_dim_idx < 8; mini_dim_idx++) {\n-    #pragma unroll\n-    for (int i = 0; i < 4; i++) {\n-      reg_1[(mini_dim_idx % 2) * 4 + i] = A_buffer[256 + mini_dim_idx * 32 + reg_1_idx * 4 + i];\n-      reg_2[(mini_dim_idx % 2) * 4 + i] = B_buffer[256 + mini_dim_idx * 32 + reg_2_idx * 4 + i];\n-    }\n-    #pragma unroll\n-    for (int i = 0; i < 4; i++) {\n-      #pragma unroll\n-      for (int j = 0; j < 4; j++) {\n-        reg_array[i * 4 + j] += reg_1[((mini_dim_idx - 1) % 2) * 4 + i] * reg_2[((mini_dim_idx - 1) % 2) * 4 + j];\n-      }\n-    }\n-  }\n-  #pragma unroll\n-  for (int i = 0; i < 4; i++) {\n-    #pragma unroll\n-    for (int j = 0; j < 4; j++) {\n-      reg_array[i * 4 + j] += reg_1[4 + i] * reg_2[4 + j];\n-    }\n-  }\n-  __syncthreads();\n-\n-  float *C_buffer = &buffer[threadIdx.y * 1024]; // [32, 32]\n-\n-  #pragma unroll\n-  for (int i = 0; i < 4; i++) {\n-    #pragma unroll\n-    for (int j = 0; j < 4; j++) {\n-      C_buffer[(reg_2_idx * 4 + j) * 32 + reg_1_idx * 4 + i] = reg_array[i * 4 + j];\n-    }\n-  }\n-  __syncthreads();\n-\n-  float *sparse_C_pt = &sparse_C[batch_idx__block_idx * 1024];\n-\n-  #pragma unroll\n-  for (int i = 0; i < 16; i++) {\n-    sparse_C_pt[i * 64 + thread_idx] = C_buffer[i * 64 + thread_idx];\n-  }\n-\n-}\n-\n-__global__ void sparse_dense_mm_cuda_kernel(\n-  float *sparse_A,  // [batch_size, num_block, 32, 32]\n-  int   *indices,   // [batch_size, num_block]\n-  float *dense_B,   // [batch_size, B_num_block, dim, 32]\n-  float *dense_C,   // [batch_size, A_num_block, dim, 32]\n-  long batch_size,\n-  long A_num_block,\n-  long B_num_block,\n-  long dim,\n-  long num_block\n-) {\n-\n-  long batch_idx = blockIdx.y;\n-  long block_idx = blockIdx.x * blockDim.y + threadIdx.y;\n-\n-  long thread_idx = threadIdx.x;\n-\n-  __shared__ float buffer[6144];\n-  float *A_buffer = &buffer[threadIdx.y * 3072]; // [32, 32]\n-  float *B_buffer = &buffer[threadIdx.y * 3072 + 1024]; // [32, 64]\n-\n-  long batch_idx__block_idx = batch_idx * num_block + block_idx;\n-\n-  float *sparse_A_pt = &sparse_A[batch_idx__block_idx * 1024];\n-  #pragma unroll\n-  for (int i = 0; i < 8; i++) {\n-    A_buffer[i * 128 + thread_idx] = sparse_A_pt[i * 128 + thread_idx];\n-  }\n-\n-  long AB_block_idx = indices[batch_idx__block_idx];\n-  float *dense_B_pt = &dense_B[(batch_idx * B_num_block + AB_block_idx % B_num_block) * 32 * dim];\n-  float *dense_C_pt = &dense_C[(batch_idx * A_num_block + AB_block_idx / B_num_block) * 32 * dim];\n-\n-  // [0000000011111111222222223333333344444444555555556666666677777777]\n-  // [0123456701234567012345670123456701234567012345670123456701234567]\n-  int reg_1_idx = thread_idx / 8;\n-  int reg_2_idx = thread_idx % 8;\n-\n-  float reg_1[8];\n-  float reg_2[8];\n-\n-  float reg_array[16];\n-\n-  for (int dim_stride = 0; dim_stride < dim; dim_stride = dim_stride + 64) {\n-\n-    #pragma unroll\n-    for (int i = 0; i < 16; i++) {\n-      B_buffer[i * 128 + thread_idx] = dense_B_pt[dim_stride * 32 + i * 128 + thread_idx];\n-    }\n-\n-    #pragma unroll\n-    for (int i = 0; i < 16; i++) {\n-      reg_array[i] = 0;\n-    }\n-\n-    __syncthreads();\n-\n-    #pragma unroll\n-    for (int i = 0; i < 4; i++) {\n-      reg_1[i] = B_buffer[(reg_1_idx * 4 + i) * 32];\n-      reg_2[i] = A_buffer[reg_2_idx * 4 + i];\n-    }\n-\n-    #pragma unroll\n-    for (int mini_dim_idx = 1; mini_dim_idx < 32; mini_dim_idx++) {\n-      #pragma unroll\n-      for (int i = 0; i < 4; i++) {\n-        reg_1[(mini_dim_idx % 2) * 4 + i] = B_buffer[(reg_1_idx * 4 + i) * 32 + mini_dim_idx];\n-        reg_2[(mini_dim_idx % 2) * 4 + i] = A_buffer[mini_dim_idx * 32 + reg_2_idx * 4 + i];\n-      }\n-      #pragma unroll\n-      for (int i = 0; i < 4; i++) {\n-        #pragma unroll\n-        for (int j = 0; j < 4; j++) {\n-          reg_array[i * 4 + j] += reg_1[((mini_dim_idx - 1) % 2) * 4 + i] * reg_2[((mini_dim_idx - 1) % 2) * 4 + j];\n-        }\n-      }\n-    }\n-\n-    #pragma unroll\n-    for (int i = 0; i < 4; i++) {\n-      #pragma unroll\n-      for (int j = 0; j < 4; j++) {\n-        reg_array[i * 4 + j] += reg_1[4 + i] * reg_2[4 + j];\n-      }\n-    }\n-\n-    __syncthreads();\n-\n-    float *C_buffer = &buffer[threadIdx.y * 3072 + 1024]; // [64, 32]\n-\n-    #pragma unroll\n-    for (int i = 0; i < 4; i++) {\n-      #pragma unroll\n-      for (int j = 0; j < 4; j++) {\n-        C_buffer[(reg_1_idx * 4 + i) * 32 + reg_2_idx * 4 + j] = reg_array[i * 4 + j];\n-      }\n-    }\n-    __syncthreads();\n-\n-    #pragma unroll\n-    for (int i = 0; i < 16; i++) {\n-      atomicAdd(&dense_C_pt[dim_stride * 32 + i * 128 + thread_idx], C_buffer[i * 128 + thread_idx]);\n-    }\n-    __syncthreads();\n-\n-  }\n-\n-}\n-\n-\n-__global__ void reduce_sum_cuda_kernel(\n-  float *sparse_A,  // [batch_size, num_block, 32, 32]\n-  int   *indices,   // [batch_size, num_block]\n-  float *dense_C,   // [batch_size, A_num_block, 32]\n-  long batch_size,\n-  long A_num_block,\n-  long B_num_block,\n-  long num_block\n-) {\n-\n-  long batch_idx = blockIdx.y;\n-  long block_idx = blockIdx.x * blockDim.y + threadIdx.y;\n-\n-  long thread_idx = threadIdx.x;\n-\n-  long batch_idx__block_idx = batch_idx * num_block + block_idx;\n-\n-  long AB_block_idx = indices[batch_idx__block_idx];\n-  float *sparse_A_pt = &sparse_A[batch_idx__block_idx * 1024];\n-\n-  float reg_array[16];\n-  float value = 0;\n-\n-  #pragma unroll\n-  for (int i = 0; i < 8; i++) {\n-    reg_array[i] = sparse_A_pt[i * 32 + thread_idx];\n-  }\n-  #pragma unroll\n-  for (int stride = 8; stride < 32; stride = stride + 8) {\n-    #pragma unroll\n-    for (int i = 0; i < 8; i++) {\n-      reg_array[(stride + i) % 16] = sparse_A_pt[(stride + i) * 32 + thread_idx];\n-    }\n-    #pragma unroll\n-    for (int i = 0; i < 8; i++) {\n-      value = value + reg_array[(stride - 8 + i) % 16];\n-    }\n-  }\n-  #pragma unroll\n-  for (int i = 0; i < 8; i++) {\n-    value = value + reg_array[8 + i];\n-  }\n-\n-  float *dense_C_pt = &dense_C[(batch_idx * A_num_block + AB_block_idx / B_num_block) * 32];\n-\n-  atomicAdd(&dense_C_pt[thread_idx], value);\n-\n-}\n-\n-__global__ void scatter_cuda_kernel(\n-  float *dense_A,   // [batch_size, A_num_block, 32]\n-  int   *indices,   // [batch_size, num_block]\n-  float *sparse_C,  // [batch_size, num_block, 32, 32]\n-  long batch_size,\n-  long A_num_block,\n-  long B_num_block,\n-  long num_block\n-) {\n-\n-  long batch_idx = blockIdx.y;\n-  long block_idx = blockIdx.x * blockDim.y + threadIdx.y;\n-\n-  long thread_idx = threadIdx.x;\n-\n-  long batch_idx__block_idx = batch_idx * num_block + block_idx;\n-\n-  long AB_block_idx = indices[batch_idx__block_idx];\n-  float *dense_A_pt = &dense_A[(batch_idx * A_num_block + AB_block_idx / B_num_block) * 32];\n-  float *sparse_C_pt = &sparse_C[(batch_idx * num_block + block_idx) * 1024];\n-\n-  float value = dense_A_pt[thread_idx];\n-\n-  #pragma unroll\n-  for (int i = 0; i < 32; i++) {\n-    sparse_C_pt[i * 32 + thread_idx] = value;\n-  }\n-\n-}"
      },
      {
        "filename": "src/transformers/kernels/mra/cuda_kernel.h",
        "status": "removed",
        "additions": 0,
        "deletions": 59,
        "changes": 59,
        "patch": "@@ -1,59 +0,0 @@\n-\n-#define WARP_SIZE 32\n-#define FULL_MASK 0xffffffff\n-#define OPTIMAL_THREADS 256\n-\n-__global__ void index_max_cuda_kernel(\n-  float *index_vals,       // [batch_size, 32, num_block]\n-  int   *indices,        // [batch_size, num_block]\n-  float *max_vals,        // [batch_size, A_num_block * 32]\n-  float *max_vals_scatter,   // [batch_size, 32, num_block]\n-  long batch_size,\n-  long A_num_block,\n-  long B_num_block,\n-  long num_block\n-);\n-\n-__global__ void mm_to_sparse_cuda_kernel(\n-  float *dense_A,   // [batch_size, A_num_block, dim, 32]\n-  float *dense_B,   // [batch_size, B_num_block, dim, 32]\n-  int   *indices,   // [batch_size, num_block]\n-  float *sparse_C,  // [batch_size, num_block, 32, 32]\n-  long batch_size,\n-  long A_num_block,\n-  long B_num_block,\n-  long dim,\n-  long num_block\n-);\n-\n-__global__ void sparse_dense_mm_cuda_kernel(\n-  float *sparse_A,  // [batch_size, num_block, 32, 32]\n-  int   *indices,   // [batch_size, num_block]\n-  float *dense_B,   // [batch_size, B_num_block, dim, 32]\n-  float *dense_C,   // [batch_size, A_num_block, dim, 32]\n-  long batch_size,\n-  long A_num_block,\n-  long B_num_block,\n-  long dim,\n-  long num_block\n-);\n-\n-__global__ void reduce_sum_cuda_kernel(\n-  float *sparse_A,  // [batch_size, num_block, 32, 32]\n-  int   *indices,   // [batch_size, num_block]\n-  float *dense_C,   // [batch_size, A_num_block, 32]\n-  long batch_size,\n-  long A_num_block,\n-  long B_num_block,\n-  long num_block\n-);\n-\n-__global__ void scatter_cuda_kernel(\n-  float *dense_A,   // [batch_size, A_num_block, 32]\n-  int   *indices,   // [batch_size, num_block]\n-  float *sparse_C,  // [batch_size, num_block, 32, 32]\n-  long batch_size,\n-  long A_num_block,\n-  long B_num_block,\n-  long num_block\n-);"
      },
      {
        "filename": "src/transformers/kernels/mra/cuda_launch.cu",
        "status": "removed",
        "additions": 0,
        "deletions": 154,
        "changes": 154,
        "patch": "@@ -1,154 +0,0 @@\n-#include <torch/extension.h>\n-#include <ATen/ATen.h>\n-#include \"cuda_launch.h\"\n-#include \"cuda_kernel.h\"\n-#include <vector>\n-\n-//////////////////////////////////////////////////////////////////////////////////////////////////\n-//////////////////////////////////////////////////////////////////////////////////////////////////\n-\n-std::vector<at::Tensor> index_max_kernel(\n-  at::Tensor index_vals,  // [batch_size, 32, num_block]\n-  at::Tensor indices,     // [batch_size, num_block],\n-  int A_num_block,\n-  int B_num_block\n-) {\n-  int batch_size = indices.size(0);\n-  int num_block = indices.size(1);\n-\n-  at::Tensor max_vals = at::zeros({batch_size, A_num_block * 32}, index_vals.options());\n-  at::Tensor max_vals_scatter = at::zeros({batch_size, 32, num_block}, index_vals.options());\n-\n-  dim3 threads(256);\n-  dim3 blocks(batch_size);\n-  int shared_mem = A_num_block * 32 * sizeof(float);\n-\n-  index_max_cuda_kernel<<<blocks, threads, shared_mem>>>(\n-    index_vals.data_ptr<float>(),\n-    indices.data_ptr<int>(),\n-    max_vals.data_ptr<float>(),\n-    max_vals_scatter.data_ptr<float>(),\n-    batch_size,\n-    A_num_block,\n-    B_num_block,\n-    num_block\n-  );\n-\n-  return {max_vals, max_vals_scatter};\n-}\n-\n-at::Tensor mm_to_sparse_kernel(\n-  at::Tensor dense_A,  // [batch_size, A_num_block, dim, 32]\n-  at::Tensor dense_B,  // [batch_size, B_num_block, dim, 32]\n-  at::Tensor indices   // [batch_size, num_block]\n-) {\n-  int batch_size = dense_A.size(0);\n-  int A_num_block = dense_A.size(1);\n-  int B_num_block = dense_B.size(1);\n-  int dim = dense_A.size(2);\n-  int num_block = indices.size(1);\n-\n-  at::Tensor sparse_C = at::zeros({batch_size, num_block, 32, 32}, dense_A.options());\n-\n-  dim3 threads(64, 4);\n-  dim3 blocks(num_block / 4, batch_size);\n-\n-  mm_to_sparse_cuda_kernel<<<blocks, threads>>>(\n-    dense_A.data_ptr<float>(),\n-    dense_B.data_ptr<float>(),\n-    indices.data_ptr<int>(),\n-    sparse_C.data_ptr<float>(),\n-    batch_size,\n-    A_num_block,\n-    B_num_block,\n-    dim,\n-    num_block\n-  );\n-\n-  return sparse_C;\n-}\n-\n-at::Tensor sparse_dense_mm_kernel(\n-  at::Tensor sparse_A,  // [batch_size, num_block, 32, 32]\n-  at::Tensor indices,   // [batch_size, num_block]\n-  at::Tensor dense_B,   // [batch_size, B_num_block, dim, 32]\n-  int A_num_block\n-) {\n-  int batch_size = sparse_A.size(0);\n-  int num_block = sparse_A.size(1);\n-  int B_num_block = dense_B.size(1);\n-  int dim = dense_B.size(2);\n-\n-  at::Tensor dense_C = at::zeros({batch_size, A_num_block, dim, 32}, dense_B.options());\n-\n-  dim3 threads(128, 2);\n-  dim3 blocks(num_block / 2, batch_size);\n-\n-  sparse_dense_mm_cuda_kernel<<<blocks, threads>>>(\n-    sparse_A.data_ptr<float>(),\n-    indices.data_ptr<int>(),\n-    dense_B.data_ptr<float>(),\n-    dense_C.data_ptr<float>(),\n-    batch_size,\n-    A_num_block,\n-    B_num_block,\n-    dim,\n-    num_block\n-  );\n-\n-  return dense_C;\n-}\n-\n-at::Tensor reduce_sum_kernel(\n-  at::Tensor sparse_A,  // [batch_size, num_block, 32, 32]\n-  at::Tensor indices,   // [batch_size, num_block]\n-  int A_num_block,\n-  int B_num_block\n-) {\n-  int batch_size = sparse_A.size(0);\n-  int num_block = sparse_A.size(1);\n-\n-  at::Tensor dense_C = at::zeros({batch_size, A_num_block, 32}, sparse_A.options());\n-\n-  dim3 threads(32, 4);\n-  dim3 blocks(num_block / 4, batch_size);\n-\n-  reduce_sum_cuda_kernel<<<blocks, threads>>>(\n-    sparse_A.data_ptr<float>(),\n-    indices.data_ptr<int>(),\n-    dense_C.data_ptr<float>(),\n-    batch_size,\n-    A_num_block,\n-    B_num_block,\n-    num_block\n-  );\n-\n-  return dense_C;\n-}\n-\n-at::Tensor scatter_kernel(\n-  at::Tensor dense_A,   // [batch_size, A_num_block, 32]\n-  at::Tensor indices,   // [batch_size, num_block]\n-  int B_num_block\n-) {\n-  int batch_size = dense_A.size(0);\n-  int A_num_block = dense_A.size(1);\n-  int num_block = indices.size(1);\n-\n-  at::Tensor sparse_C = at::zeros({batch_size, num_block, 32, 32}, dense_A.options());\n-\n-  dim3 threads(32, 4);\n-  dim3 blocks(num_block / 4, batch_size);\n-\n-  scatter_cuda_kernel<<<blocks, threads>>>(\n-    dense_A.data_ptr<float>(),\n-    indices.data_ptr<int>(),\n-    sparse_C.data_ptr<float>(),\n-    batch_size,\n-    A_num_block,\n-    B_num_block,\n-    num_block\n-  );\n-\n-  return sparse_C;\n-}"
      },
      {
        "filename": "src/transformers/kernels/mra/cuda_launch.h",
        "status": "removed",
        "additions": 0,
        "deletions": 39,
        "changes": 39,
        "patch": "@@ -1,39 +0,0 @@\n-#include <torch/extension.h>\n-#include <ATen/ATen.h>\n-#include <vector>\n-\n-#define min(a, b) ((a)<(b)?(a):(b))\n-#define max(a, b) ((a)>(b)?(a):(b))\n-\n-std::vector<at::Tensor> index_max_kernel(\n-  at::Tensor index_vals,\n-  at::Tensor indices,\n-  int A_num_block,\n-  int B_num_block\n-);\n-\n-at::Tensor mm_to_sparse_kernel(\n-  at::Tensor dense_A,\n-  at::Tensor dense_B,\n-  at::Tensor indices\n-);\n-\n-at::Tensor sparse_dense_mm_kernel(\n-  at::Tensor sparse_A,\n-  at::Tensor indices,\n-  at::Tensor dense_B,\n-  int A_num_block\n-);\n-\n-at::Tensor reduce_sum_kernel(\n-  at::Tensor sparse_A,\n-  at::Tensor indices,\n-  int A_num_block,\n-  int B_num_block\n-);\n-\n-at::Tensor scatter_kernel(\n-  at::Tensor dense_A,\n-  at::Tensor indices,\n-  int B_num_block\n-);"
      },
      {
        "filename": "src/transformers/kernels/mra/torch_extension.cpp",
        "status": "removed",
        "additions": 0,
        "deletions": 78,
        "changes": 78,
        "patch": "@@ -1,78 +0,0 @@\n-#include <torch/extension.h>\n-#include <ATen/ATen.h>\n-#include \"cuda_launch.h\"\n-#include <vector>\n-\n-std::vector<at::Tensor> index_max(\n-  at::Tensor index_vals,\n-  at::Tensor indices,\n-  int A_num_block,\n-  int B_num_block\n-) {\n-  return index_max_kernel(\n-    index_vals,\n-    indices,\n-    A_num_block,\n-    B_num_block\n-  );\n-}\n-\n-at::Tensor mm_to_sparse(\n-  at::Tensor dense_A,\n-  at::Tensor dense_B,\n-  at::Tensor indices\n-) {\n-  return mm_to_sparse_kernel(\n-    dense_A,\n-    dense_B,\n-    indices\n-  );\n-}\n-\n-at::Tensor sparse_dense_mm(\n-  at::Tensor sparse_A,\n-  at::Tensor indices,\n-  at::Tensor dense_B,\n-  int A_num_block\n-) {\n-  return sparse_dense_mm_kernel(\n-    sparse_A,\n-    indices,\n-    dense_B,\n-    A_num_block\n-  );\n-}\n-\n-at::Tensor reduce_sum(\n-  at::Tensor sparse_A,\n-  at::Tensor indices,\n-  int A_num_block,\n-  int B_num_block\n-) {\n-  return reduce_sum_kernel(\n-    sparse_A,\n-    indices,\n-    A_num_block,\n-    B_num_block\n-  );\n-}\n-\n-at::Tensor scatter(\n-  at::Tensor dense_A,\n-  at::Tensor indices,\n-  int B_num_block\n-) {\n-  return scatter_kernel(\n-    dense_A,\n-    indices,\n-    B_num_block\n-  );\n-}\n-\n-PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n-  m.def(\"index_max\", &index_max, \"index_max (CUDA)\");\n-  m.def(\"mm_to_sparse\", &mm_to_sparse, \"mm_to_sparse (CUDA)\");\n-  m.def(\"sparse_dense_mm\", &sparse_dense_mm, \"sparse_dense_mm (CUDA)\");\n-  m.def(\"reduce_sum\", &reduce_sum, \"reduce_sum (CUDA)\");\n-  m.def(\"scatter\", &scatter, \"scatter (CUDA)\");\n-}"
      },
      {
        "filename": "src/transformers/models/mra/modeling_mra.py",
        "status": "modified",
        "additions": 12,
        "deletions": 10,
        "changes": 22,
        "patch": "@@ -15,13 +15,11 @@\n \"\"\"PyTorch MRA model.\"\"\"\n \n import math\n-from pathlib import Path\n from typing import Optional, Union\n \n import torch\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n-from torch.utils.cpp_extension import load\n \n from ...activations import ACT2FN\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -35,7 +33,14 @@\n )\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward\n-from ...utils import auto_docstring, is_cuda_platform, is_ninja_available, is_torch_cuda_available, logging\n+from ...utils import (\n+    auto_docstring,\n+    is_cuda_platform,\n+    is_kernels_available,\n+    is_ninja_available,\n+    is_torch_cuda_available,\n+    logging,\n+)\n from .configuration_mra import MraConfig\n \n \n@@ -46,14 +51,11 @@\n \n def load_cuda_kernels():\n     global mra_cuda_kernel\n-    src_folder = Path(__file__).resolve().parent.parent.parent / \"kernels\" / \"mra\"\n-\n-    def append_root(files):\n-        return [src_folder / file for file in files]\n-\n-    src_files = append_root([\"cuda_kernel.cu\", \"cuda_launch.cu\", \"torch_extension.cpp\"])\n+    if not is_kernels_available():\n+        raise ImportError(\"kernels is not installed, please install it with `pip install kernels`\")\n+    from kernels import get_kernel\n \n-    mra_cuda_kernel = load(\"cuda_kernel\", src_files, verbose=True)\n+    mra_cuda_kernel = get_kernel(\"kernels-community/mra\")\n \n \n def sparse_max(sparse_qk_prod, indices, query_num_block, key_num_block):"
      }
    ],
    "num_files": 6,
    "scraped_at": "2025-11-16T21:17:47.833456"
  },
  {
    "pr_number": 41504,
    "title": "Revert `local_rank` deletion and some cleaning",
    "body": "# What does this PR do?\r\n\r\nThis PR removes some bits that I forgot when removing `logging_dir`. Also, we need to keep `local_rank` as torch.distributed.launch inject `local_rank` in the script. I will deprecate at once torch removes it from their codebase + we don't support this version of pytorch which is in a super long time ",
    "html_url": "https://github.com/huggingface/transformers/pull/41504",
    "created_at": "2025-10-10T09:05:21Z",
    "merged_at": "2025-10-10T10:23:04Z",
    "merge_commit_sha": "f9f8bf5a1062ce0293cbd42be0126e17a15446e9",
    "base_ref": "main",
    "head_sha": "e0bcb483ef84c7b8301e1208df9c32b01f23bb59",
    "user": "SunMarc",
    "files": [
      {
        "filename": "examples/legacy/seq2seq/seq2seq_trainer.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -144,7 +144,7 @@ def _get_train_sampler(self) -> Optional[torch.utils.data.Sampler]:\n \n             return (\n                 RandomSampler(self.train_dataset)\n-                if self.args.local_rank == -1\n+                if self.args.local_process_index == -1\n                 else DistributedSampler(self.train_dataset)\n             )\n "
      },
      {
        "filename": "src/transformers/training_args.py",
        "status": "modified",
        "additions": 6,
        "deletions": 0,
        "changes": 6,
        "patch": "@@ -996,6 +996,12 @@ class TrainingArguments:\n             )\n         },\n     )\n+    local_rank: int = field(\n+        default=-1,\n+        metadata={\n+            \"help\": \"When using torch.distributed.launch (Deprecated), it will pass `local_rank` in the script, so we need this for the parser. To get the local rank, prefer using the property `local_process_index`\"\n+        },\n+    )\n     ddp_backend: Optional[str] = field(\n         default=None,\n         metadata={"
      },
      {
        "filename": "tests/deepspeed/test_deepspeed.py",
        "status": "modified",
        "additions": 5,
        "deletions": 10,
        "changes": 15,
        "patch": "@@ -518,7 +518,6 @@ def test_hf_ds_config_mismatch(self):\n \n         with mockenv_context(**self.dist_env_1_gpu):\n             trainer = get_regression_trainer(\n-                local_rank=0,\n                 fp16=fp16,\n                 deepspeed=ds_config,\n                 per_device_train_batch_size=per_device_train_batch_size,\n@@ -552,7 +551,7 @@ def test_hf_scheduler_hf_optimizer(self):\n             ds_config_zero2_dict[\"zero_optimization\"][\"offload_optimizer\"][\"device\"] = \"none\"\n             ds_config_zero2_dict[\"fp16\"][\"initial_scale_power\"] = 1  # force optimizer on the first step\n             trainer = get_regression_trainer(\n-                a=a, local_rank=0, fp16=True, deepspeed=ds_config_zero2_dict, output_dir=self.get_auto_remove_tmp_dir()\n+                a=a, fp16=True, deepspeed=ds_config_zero2_dict, output_dir=self.get_auto_remove_tmp_dir()\n             )\n             trainer.train()\n         new_a = trainer.model.a.item()\n@@ -566,7 +565,7 @@ def test_ds_scheduler_hf_optimizer(self):\n             ds_config_zero2_dict[\"zero_optimization\"][\"offload_optimizer\"][\"device\"] = \"none\"\n             ds_config_zero2_dict[\"fp16\"][\"initial_scale_power\"] = 1  # force optimizer on the first step\n             trainer = get_regression_trainer(\n-                a=a, local_rank=0, fp16=True, deepspeed=ds_config_zero2_dict, output_dir=self.get_auto_remove_tmp_dir()\n+                a=a, fp16=True, deepspeed=ds_config_zero2_dict, output_dir=self.get_auto_remove_tmp_dir()\n             )\n             trainer.train()\n         new_a = trainer.model.a.item()\n@@ -580,7 +579,7 @@ def test_hf_scheduler_ds_optimizer(self):\n             ds_config_zero2_dict[\"zero_optimization\"][\"offload_optimizer\"][\"device\"] = \"none\"\n             ds_config_zero2_dict[\"fp16\"][\"initial_scale_power\"] = 1  # force optimizer on the first step\n             trainer = get_regression_trainer(\n-                a=a, local_rank=0, fp16=True, deepspeed=ds_config_zero2_dict, output_dir=self.get_auto_remove_tmp_dir()\n+                a=a, fp16=True, deepspeed=ds_config_zero2_dict, output_dir=self.get_auto_remove_tmp_dir()\n             )\n             trainer.train()\n         new_a = trainer.model.a.item()\n@@ -598,7 +597,7 @@ def test_stage3_nvme_offload(self):\n             ds_config_zero3_dict[\"zero_optimization\"][\"offload_param\"] = nvme_config\n             ds_config_zero3_dict[\"zero_optimization\"][\"stage3_gather_16bit_weights_on_model_save\"] = True\n             trainer = get_regression_trainer(\n-                local_rank=0, fp16=True, deepspeed=ds_config_zero3_dict, output_dir=self.get_auto_remove_tmp_dir()\n+                fp16=True, deepspeed=ds_config_zero3_dict, output_dir=self.get_auto_remove_tmp_dir()\n             )\n             with CaptureLogger(deepspeed_logger) as cl:\n                 trainer.train()\n@@ -616,7 +615,6 @@ def model_init():\n                 return model\n \n             trainer = get_regression_trainer(\n-                local_rank=0,\n                 fp16=True,\n                 model_init=model_init,\n                 deepspeed=ds_config_zero3_dict,\n@@ -642,7 +640,7 @@ def test_hf_optimizer_with_offload(self, stage, dtype):\n         ds_config_dict[\"zero_optimization\"][\"offload_optimizer\"][\"device\"] = \"cpu\"\n         ds_config_dict[\"zero_force_ds_cpu_optimizer\"] = False  # offload is not efficient w/o CPUAdam\n         with mockenv_context(**self.dist_env_1_gpu):\n-            kwargs = {\"local_rank\": 0, \"deepspeed\": ds_config_dict, \"output_dir\": self.get_auto_remove_tmp_dir()}\n+            kwargs = {\"deepspeed\": ds_config_dict, \"output_dir\": self.get_auto_remove_tmp_dir()}\n             kwargs[dtype] = True\n             trainer = get_regression_trainer(**kwargs)\n             with CaptureLogger(deepspeed_logger) as cl:\n@@ -659,7 +657,6 @@ def test_fake_notebook_no_launcher(self, stage, dtype):\n         # to reset `deepspeed_logger.handlers[0].setStream(sys.stdout)` or directly capture from the deepspeed_logger.\n         with mockenv_context(**self.dist_env_1_gpu):\n             kwargs = {\n-                \"local_rank\": 0,\n                 \"deepspeed\": self.get_config_dict(stage),\n                 \"output_dir\": self.get_auto_remove_tmp_dir(),\n             }\n@@ -683,7 +680,6 @@ def test_early_get_last_lr(self, stage, dtype):\n             kwargs = {\n                 \"a\": a,\n                 \"b\": b,\n-                \"local_rank\": 0,\n                 \"train_len\": 8,\n                 \"deepspeed\": self.get_config_dict(stage),\n                 \"per_device_train_batch_size\": 8,\n@@ -729,7 +725,6 @@ def test_gradient_accumulation(self, stage, dtype):\n         kwargs = {\n             \"a\": a,\n             \"b\": b,\n-            \"local_rank\": 0,\n             \"train_len\": train_len,\n             \"deepspeed\": self.get_config_dict(stage),\n             \"output_dir\": self.get_auto_remove_tmp_dir(),"
      },
      {
        "filename": "tests/trainer/test_trainer.py",
        "status": "modified",
        "additions": 1,
        "deletions": 7,
        "changes": 8,
        "patch": "@@ -1437,9 +1437,7 @@ def test_training_arguments_are_left_untouched(self):\n         args = TrainingArguments(tmp_dir, report_to=[])\n         dict1, dict2 = args.to_dict(), trainer.args.to_dict()\n         for key in dict1:\n-            # Logging dir can be slightly different as they default to something with the time.\n-            if key != \"logging_dir\":\n-                self.assertEqual(dict1[key], dict2[key])\n+            self.assertEqual(dict1[key], dict2[key])\n \n     def test_number_of_steps_in_training(self):\n         # Regular training has n_epochs * len(train_dl) steps\n@@ -5433,7 +5431,6 @@ def hp_name(trial):\n                 num_train_epochs=4,\n                 disable_tqdm=True,\n                 load_best_model_at_end=True,\n-                logging_dir=\"runs\",\n                 run_name=\"test\",\n                 model_init=model_init,\n             )\n@@ -5482,7 +5479,6 @@ def compute_objective(metrics: dict[str, float]) -> list[float]:\n                 num_train_epochs=10,\n                 disable_tqdm=True,\n                 load_best_model_at_end=True,\n-                logging_dir=\"runs\",\n                 run_name=\"test\",\n                 model_init=model_init,\n                 compute_metrics=AlmostAccuracy(),\n@@ -5572,7 +5568,6 @@ def hp_name(params):\n                 num_train_epochs=4,\n                 disable_tqdm=True,\n                 load_best_model_at_end=True,\n-                logging_dir=\"runs\",\n                 run_name=\"test\",\n                 model_init=model_init,\n             )\n@@ -6170,7 +6165,6 @@ def model_init(config):\n                 num_train_epochs=4,\n                 disable_tqdm=True,\n                 load_best_model_at_end=True,\n-                logging_dir=\"runs\",\n                 run_name=\"test\",\n                 model_init=model_init,\n             )"
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T21:17:49.871168"
  },
  {
    "pr_number": 41503,
    "title": "Fix some tests",
    "body": "# What does this PR do?\r\n\r\nThis PR removes some last remnants of the old cache format, and fixes related tests. I also updated contrastive search on the hub https://huggingface.co/transformers-community/contrastive-search/discussions/3 to fix the related tests with the new format",
    "html_url": "https://github.com/huggingface/transformers/pull/41503",
    "created_at": "2025-10-10T08:41:49Z",
    "merged_at": "2025-10-10T09:05:09Z",
    "merge_commit_sha": "e8194fe84f6622ea06593a2a371382bda43749c1",
    "base_ref": "main",
    "head_sha": "3bfeaef9901e6909b31d799a16ef5cdb527d0ee8",
    "user": "Cyrilvallez",
    "files": [
      {
        "filename": "docs/source/ar/llm_tutorial_optimization.md",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -472,7 +472,7 @@ for _ in range(5):\n   next_token_id = torch.argmax(next_logits, dim=-1)\n \n   print(\"shape of input_ids\", next_token_id.shape)\n-  print(\"length of key-value cache\", len(past_key_values[0][0]))  # past_key_values are of shape [num_layers, 0 for k, 1 for v, batch_size, length, hidden_dim]\n+  print(\"length of key-value cache\", past_key_values.get_seq_length())  # past_key_values are of shape [num_layers, 0 for k, 1 for v, batch_size, length, hidden_dim]\n   generated_tokens.append(next_token_id.item())\n \n generated_text = tokenizer.batch_decode(generated_tokens)"
      },
      {
        "filename": "docs/source/en/llm_tutorial_optimization.md",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -484,7 +484,7 @@ for _ in range(5):\n   next_token_id = torch.argmax(next_logits, dim=-1)\n \n   print(\"shape of input_ids\", next_token_id.shape)\n-  print(\"length of key-value cache\", len(past_key_values[0][0]))  # past_key_values are of shape [num_layers, 0 for k, 1 for v, batch_size, length, hidden_dim]\n+  print(\"length of key-value cache\", past_key_values.get_seq_length())  # past_key_values are of shape [num_layers, 0 for k, 1 for v, batch_size, length, hidden_dim]\n   generated_tokens.append(next_token_id.item())\n \n generated_text = tokenizer.batch_decode(generated_tokens)"
      },
      {
        "filename": "docs/source/ko/llm_tutorial_optimization.md",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -457,7 +457,7 @@ for _ in range(5):\n   next_token_id = torch.argmax(next_logits, dim=-1)\n \n   print(\"shape of input_ids\", next_token_id.shape)\n-  print(\"length of key-value cache\", len(past_key_values[0][0]))  # past_key_values \ud615\ud0dc: [num_layers, 0 for k, 1 for v, batch_size, length, hidden_dim]\n+  print(\"length of key-value cache\", past_key_values.get_seq_length())  # past_key_values \ud615\ud0dc: [num_layers, 0 for k, 1 for v, batch_size, length, hidden_dim]\n   generated_tokens.append(next_token_id.item())\n \n generated_text = tokenizer.batch_decode(generated_tokens)"
      },
      {
        "filename": "src/transformers/models/big_bird/modeling_big_bird.py",
        "status": "modified",
        "additions": 1,
        "deletions": 7,
        "changes": 8,
        "patch": "@@ -1689,13 +1689,7 @@ def forward(\n         batch_size, seq_length = input_shape\n         device = input_ids.device if input_ids is not None else inputs_embeds.device\n \n-        past_key_values_length = 0\n-        if past_key_values is not None:\n-            past_key_values_length = (\n-                past_key_values[0][0].shape[-2]\n-                if not isinstance(past_key_values, Cache)\n-                else past_key_values.get_seq_length()\n-            )\n+        past_key_values_length = 0 if past_key_values is None else past_key_values.get_seq_length()\n \n         if attention_mask is None:\n             attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)"
      },
      {
        "filename": "src/transformers/models/blip/modeling_blip_text.py",
        "status": "modified",
        "additions": 1,
        "deletions": 7,
        "changes": 8,
        "patch": "@@ -674,13 +674,7 @@ def forward(\n         else:\n             raise ValueError(\"You have to specify either input_ids or inputs_embeds or encoder_embeds\")\n \n-        past_key_values_length = 0\n-        if past_key_values is not None:\n-            past_key_values_length = (\n-                past_key_values[0][0].shape[-2]\n-                if not isinstance(past_key_values, Cache)\n-                else past_key_values.get_seq_length()\n-            )\n+        past_key_values_length = 0 if past_key_values is None else past_key_values.get_seq_length()\n \n         if attention_mask is None:\n             attention_mask = torch.ones((batch_size, seq_length + past_key_values_length)).to(device)"
      },
      {
        "filename": "src/transformers/models/ctrl/modeling_ctrl.py",
        "status": "modified",
        "additions": 0,
        "deletions": 34,
        "changes": 34,
        "patch": "@@ -250,18 +250,6 @@ def forward(\n         **kwargs,  # NOOP kwargs, for now\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPast]:\n         r\"\"\"\n-        input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n-            `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values[0].shape[-2]`\n-            (`sequence_length` of input past key value states). Indices of input sequence tokens in the vocabulary.\n-\n-            If `past_key_values` is used, only input IDs that do not have their past calculated should be passed as\n-            `input_ids`.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.__call__`] and\n-            [`PreTrainedTokenizer.encode`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-\n         Example:\n \n         ```python\n@@ -424,17 +412,6 @@ def forward(\n         **kwargs,\n     ) -> Union[tuple[torch.Tensor], CausalLMOutputWithPast]:\n         r\"\"\"\n-        input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n-            `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values[0].shape[-2]`\n-            (`sequence_length` of input past key value states). Indices of input sequence tokens in the vocabulary.\n-\n-            If `past_key_values` is used, only input IDs that do not have their past calculated should be passed as\n-            `input_ids`.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.__call__`] and\n-            [`PreTrainedTokenizer.encode`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\n             `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\n@@ -572,17 +549,6 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[tuple[torch.Tensor], SequenceClassifierOutput]:\n         r\"\"\"\n-        input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n-            `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values[0].shape[-2]`\n-            (`sequence_length` of input past key value states). Indices of input sequence tokens in the vocabulary.\n-\n-            If `past_key_values` is used, only input IDs that do not have their past calculated should be passed as\n-            `input_ids`.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.__call__`] and\n-            [`PreTrainedTokenizer.encode`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If"
      },
      {
        "filename": "src/transformers/models/megatron_bert/modeling_megatron_bert.py",
        "status": "modified",
        "additions": 1,
        "deletions": 7,
        "changes": 8,
        "patch": "@@ -644,13 +644,7 @@ def forward(\n         batch_size, seq_length = input_shape\n         device = input_ids.device if input_ids is not None else inputs_embeds.device\n \n-        past_key_values_length = 0\n-        if past_key_values is not None:\n-            past_key_values_length = (\n-                past_key_values[0][0].shape[-2]\n-                if not isinstance(past_key_values, Cache)\n-                else past_key_values.get_seq_length()\n-            )\n+        past_key_values_length = 0 if past_key_values is None else past_key_values.get_seq_length()\n \n         if attention_mask is None:\n             attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)"
      },
      {
        "filename": "src/transformers/models/pix2struct/modeling_pix2struct.py",
        "status": "modified",
        "additions": 0,
        "deletions": 1,
        "changes": 1,
        "patch": "@@ -160,7 +160,6 @@ def forward(\n         \"\"\"\n         # Input is (batch_size, seq_length, dim)\n         # Mask is (batch_size, key_length) (non-causal) or (batch_size, key_length, key_length)\n-        # past_key_values[0] is (batch_size, n_heads, q_len - 1, dim_per_head)\n         batch_size, seq_length = hidden_states.shape[:2]\n \n         def to_projection_shape(states):"
      },
      {
        "filename": "src/transformers/models/rembert/modeling_rembert.py",
        "status": "modified",
        "additions": 1,
        "deletions": 7,
        "changes": 8,
        "patch": "@@ -579,13 +579,7 @@ def forward(\n         batch_size, seq_length = input_shape\n         device = input_ids.device if input_ids is not None else inputs_embeds.device\n \n-        past_key_values_length = 0\n-        if past_key_values is not None:\n-            past_key_values_length = (\n-                past_key_values[0][0].shape[-2]\n-                if not isinstance(past_key_values, Cache)\n-                else past_key_values.get_seq_length()\n-            )\n+        past_key_values_length = 0 if past_key_values is None else past_key_values.get_seq_length()\n \n         if attention_mask is None:\n             attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)"
      },
      {
        "filename": "src/transformers/models/roformer/modeling_roformer.py",
        "status": "modified",
        "additions": 1,
        "deletions": 7,
        "changes": 8,
        "patch": "@@ -736,13 +736,7 @@ def forward(\n         batch_size, seq_length = input_shape\n         device = input_ids.device if input_ids is not None else inputs_embeds.device\n \n-        past_key_values_length = 0\n-        if past_key_values is not None:\n-            past_key_values_length = (\n-                past_key_values[0][0].shape[-2]\n-                if not isinstance(past_key_values, Cache)\n-                else past_key_values.get_seq_length()\n-            )\n+        past_key_values_length = 0 if past_key_values is None else past_key_values.get_seq_length()\n \n         if attention_mask is None:\n             attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)"
      },
      {
        "filename": "src/transformers/models/speecht5/modeling_speecht5.py",
        "status": "modified",
        "additions": 1,
        "deletions": 8,
        "changes": 9,
        "patch": "@@ -805,14 +805,7 @@ def forward(\n         else:\n             raise ValueError(\"You have to specify `decoder_input_ids`\")\n \n-        past_key_values_length = 0\n-        if past_key_values is not None:\n-            past_key_values_length = (\n-                past_key_values[0][0].shape[-2]\n-                if not isinstance(past_key_values, Cache)\n-                else past_key_values.get_seq_length()\n-            )\n-\n+        past_key_values_length = 0 if past_key_values is None else past_key_values.get_seq_length()\n         positions = self.embed_positions(input_ids, past_key_values_length)\n \n         inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale"
      },
      {
        "filename": "tests/generation/test_utils.py",
        "status": "modified",
        "additions": 13,
        "deletions": 8,
        "changes": 21,
        "patch": "@@ -4665,7 +4665,7 @@ def test_generate_custom_cache_position(self):\n             value=1,\n         )\n         inputs_2b[\"past_key_values\"] = outputs_1b.past_key_values\n-        cache_length_1b = outputs_1b.past_key_values[0][0].shape[-2]\n+        cache_length_1b = outputs_1b.past_key_values.get_seq_length()\n         inputs_2b[\"cache_position\"] = torch.arange(\n             cache_length_1b,\n             cache_length_1b + inputs_2b[\"input_ids\"].shape[1],\n@@ -4677,14 +4677,19 @@ def test_generate_custom_cache_position(self):\n \n         # The two sets of generated text and past kv should be equal to each other\n         self.assertTrue(has_similar_generate_outputs(traditional_outputs, incremental_outputs))\n-        for layer_idx in range(len(traditional_outputs.past_key_values)):\n-            for kv_idx in range(len(traditional_outputs.past_key_values[layer_idx])):\n-                self.assertTrue(\n-                    torch.allclose(\n-                        traditional_outputs.past_key_values[layer_idx][kv_idx],\n-                        incremental_outputs.past_key_values[layer_idx][kv_idx],\n+        cache1, cache2 = traditional_outputs.past_key_values, incremental_outputs.past_key_values\n+        for idx in range(len(cache1)):\n+            if isinstance(cache1, EncoderDecoderCache):\n+                for subcache in [\"self_attention_cache\", \"cross_attention_cache\"]:\n+                    torch.testing.assert_close(\n+                        getattr(cache1, subcache).layers[idx].keys, getattr(cache2, subcache).layers[idx].keys\n                     )\n-                )\n+                    torch.testing.assert_close(\n+                        getattr(cache1, subcache).layers[idx].values, getattr(cache2, subcache).layers[idx].values\n+                    )\n+            else:\n+                torch.testing.assert_close(cache1.layers[idx].keys, cache2.layers[idx].keys)\n+                torch.testing.assert_close(cache1.layers[idx].values, cache2.layers[idx].values)\n \n     @pytest.mark.generate\n     @parameterized.expand("
      }
    ],
    "num_files": 12,
    "scraped_at": "2025-11-16T21:17:50.141351"
  },
  {
    "pr_number": 41495,
    "title": "Rm yoso kernel",
    "body": "# What does this PR do?\r\n\r\nRemoving the yoso kernels, and using https://huggingface.co/kernels-community/yoso instead",
    "html_url": "https://github.com/huggingface/transformers/pull/41495",
    "created_at": "2025-10-09T23:41:30Z",
    "merged_at": "2025-10-10T08:50:13Z",
    "merge_commit_sha": "3585737746e5c73a37b6d43f429ca6f56f1e3da5",
    "base_ref": "main",
    "head_sha": "73b687e06cb2c3a0397685f6bb86875e66443147",
    "user": "MekkCyber",
    "files": [
      {
        "filename": "src/transformers/kernels/yoso/common.h",
        "status": "removed",
        "additions": 0,
        "deletions": 10,
        "changes": 10,
        "patch": "@@ -1,10 +0,0 @@\n-\n-#define min(a, b) ((a)<(b)?(a):(b))\n-#define max(a, b) ((a)>(b)?(a):(b))\n-#define ceil_divide(a, b) ((a)/(b)+((a)%(b)!=0))\n-#define select(cond, a, b) ((cond)?(a):(b))\n-#define PI 3.141592\n-#define EPSILON 1e-8\n-#define MAX_VAL 1e12\n-#define MIN_VAL -1e12\n-#define EMPTY_VALUE -1"
      },
      {
        "filename": "src/transformers/kernels/yoso/common_cuda.h",
        "status": "removed",
        "additions": 0,
        "deletions": 9,
        "changes": 9,
        "patch": "@@ -1,9 +0,0 @@\n-\n-#define MAX_THREADS_PER_BLOCK 1024\n-#define OPTIMAL_THREADS_PER_BLOCK 256\n-#define WARP_SIZE 32\n-#define MAX_NUM_BLOCK_X 2147483647\n-#define MAX_NUM_BLOCK_Y 65535\n-#define MAX_NUM_BLOCK_Z 65535\n-#define MAX_SHARED_MEM_PER_BLOCK 48000\n-#define FULL_MASK 0xffffffff"
      },
      {
        "filename": "src/transformers/kernels/yoso/common_cuda_device.h",
        "status": "removed",
        "additions": 0,
        "deletions": 79,
        "changes": 79,
        "patch": "@@ -1,79 +0,0 @@\n-\n-#include \"common.h\"\n-\n-template<typename T>\n-__device__ int set_insert(T *set, int set_size, T value) {\n-  int slot = value % set_size;\n-  int start_slot = slot;\n-  while (true) {\n-    T prev = atomicCAS(&set[slot], EMPTY_VALUE, value);\n-    if (prev == EMPTY_VALUE || prev == value) {\n-      return slot;\n-    }\n-    slot = (slot + 1) % set_size;\n-    if (slot == start_slot) {\n-      return -1;\n-    }\n-  }\n-  return -1;\n-}\n-\n-template<typename T>\n-__device__ int set_lookup(T *set, int set_size, T value) {\n-  int slot = value % set_size;\n-  int start_slot = slot;\n-  while (true) {\n-    if (set[slot] == value) {\n-      return slot;\n-    }\n-    slot = (slot + 1) % set_size;\n-    if (slot == start_slot) {\n-      return -1;\n-    }\n-  }\n-  return -1;\n-}\n-\n-template<typename T>\n-__device__ void init_buffer(T init_value, T *buffer, int buffer_size, int num_threads, int thread_id) {\n-  __syncthreads();\n-  for (int i = 0; i < buffer_size; i = i + num_threads) {\n-    int offset_idx = i + thread_id;\n-    if (offset_idx < buffer_size) {\n-      buffer[offset_idx] = init_value;\n-    }\n-  }\n-  __syncthreads();\n-}\n-\n-template<typename T>\n-__device__ void copy_data(T *src_pt, T *dist_pt, int data_length, int num_threads, int thread_id) {\n-  __syncthreads();\n-  for (int i = 0; i < data_length; i = i + num_threads) {\n-    int offset_idx = i + thread_id;\n-    if (offset_idx < data_length) {\n-      dist_pt[offset_idx] = src_pt[offset_idx];\n-    }\n-  }\n-  __syncthreads();\n-}\n-\n-template<typename T>\n-__device__ void init_buffer_nonblocking(T init_value, T *buffer, int buffer_size, int num_threads, int thread_id) {\n-  for (int i = 0; i < buffer_size; i = i + num_threads) {\n-    int offset_idx = i + thread_id;\n-    if (offset_idx < buffer_size) {\n-      buffer[offset_idx] = init_value;\n-    }\n-  }\n-}\n-\n-template<typename T>\n-__device__ void copy_data_nonblocking(T *src_pt, T *dist_pt, int data_length, int num_threads, int thread_id) {\n-  for (int i = 0; i < data_length; i = i + num_threads) {\n-    int offset_idx = i + thread_id;\n-    if (offset_idx < data_length) {\n-      dist_pt[offset_idx] = src_pt[offset_idx];\n-    }\n-  }\n-}"
      },
      {
        "filename": "src/transformers/kernels/yoso/fast_lsh_cumulation.cu",
        "status": "removed",
        "additions": 0,
        "deletions": 588,
        "changes": 588,
        "patch": "@@ -1,588 +0,0 @@\n-// File from https://github.com/mlpen/YOSO/blob/main/encoders/backbones/efficient_attentions/yoso/yoso_v1/cuda/fast_lsh_cumulation.cu\n-\n-#include <torch/extension.h>\n-#include <ATen/ATen.h>\n-#include \"fast_lsh_cumulation.h\"\n-#include \"fast_lsh_cumulation_cuda.h\"\n-#include \"common_cuda.h\"\n-#include \"common.h\"\n-#include <vector>\n-//////////////////////////////////////////////////////////////////////////////////////////////////\n-//////////////////////////////////////////////////////////////////////////////////////////////////\n-\n-std::vector<at::Tensor> fast_hash_ver1_kernel(\n-  at::Tensor query_mask,\n-  at::Tensor query_vector,\n-  at::Tensor key_mask,\n-  at::Tensor key_vector,\n-  int num_hash_f,\n-  int hash_code_len,\n-  bool use_cuda\n-) {\n-\n-  int batch_size = query_vector.size(0);\n-  int num_query = query_vector.size(1);\n-  int num_key = key_vector.size(1);\n-  int vector_dim = query_vector.size(2);\n-\n-  int num_hash_per_part = vector_dim / hash_code_len;\n-  int num_part = max(1, ceil_divide(num_hash_f, num_hash_per_part));\n-\n-  at::Tensor Dmat = 2 * at::randint(0, 2, {batch_size, 3, num_part, vector_dim}, query_mask.options()) - 1;\n-  at::Tensor query_hash_code = at::zeros({batch_size, num_query, num_hash_f}, query_mask.options());\n-  at::Tensor key_hash_code = at::zeros({batch_size, num_key, num_hash_f}, key_mask.options());\n-\n-  int *query_mask_ptr = query_mask.data_ptr<int>();\n-  float *query_vector_ptr = query_vector.data_ptr<float>();\n-  int *key_mask_ptr = key_mask.data_ptr<int>();\n-  float *key_vector_ptr = key_vector.data_ptr<float>();\n-\n-  int *Dmat_ptr = Dmat.data_ptr<int>();\n-\n-  int *query_hash_code_ptr = query_hash_code.data_ptr<int>();\n-  int *key_hash_code_ptr = key_hash_code.data_ptr<int>();\n-\n-  if (use_cuda) {\n-    {\n-      dim3 threads(vector_dim);\n-      dim3 blocks(num_part, num_query, batch_size);\n-      int shared_mem = vector_dim * sizeof(float);\n-      fast_hash_ver1_cuda_kernel<<<blocks, threads, shared_mem>>>(\n-        query_mask_ptr,\n-        query_vector_ptr,\n-        Dmat_ptr,\n-        query_hash_code_ptr,\n-        batch_size,\n-        num_query,\n-        vector_dim,\n-        num_part,\n-        num_hash_f,\n-        hash_code_len\n-      );\n-    }\n-    {\n-      dim3 threads(vector_dim);\n-      dim3 blocks(num_part, num_key, batch_size);\n-      int shared_mem = vector_dim * sizeof(float);\n-      fast_hash_ver1_cuda_kernel<<<blocks, threads, shared_mem>>>(\n-        key_mask_ptr,\n-        key_vector_ptr,\n-        Dmat_ptr,\n-        key_hash_code_ptr,\n-        batch_size,\n-        num_key,\n-        vector_dim,\n-        num_part,\n-        num_hash_f,\n-        hash_code_len\n-      );\n-    }\n-  }\n-\n-  return {query_hash_code, key_hash_code};\n-\n-}\n-\n-at::Tensor lsh_cumulation_ver1_kernel(\n-  at::Tensor query_mask,\n-  at::Tensor query_hash_code,\n-  at::Tensor key_mask,\n-  at::Tensor key_hash_code,\n-  at::Tensor value,\n-  int hashtable_capacity,\n-  bool use_cuda\n-) {\n-\n-  int batch_size = query_hash_code.size(0);\n-  int num_hash_f = query_hash_code.size(2);\n-\n-  int num_query = query_hash_code.size(1);\n-  int num_key = key_hash_code.size(1);\n-  int value_dim = value.size(2);\n-\n-  at::Tensor hashtable_value = at::empty({batch_size, num_hash_f, hashtable_capacity, WARP_SIZE}, value.options());\n-  at::Tensor cumulation_value = at::zeros({batch_size, num_query, value_dim}, value.options());\n-\n-  if (use_cuda) {\n-    int threads_x = WARP_SIZE;\n-    int threads_y = OPTIMAL_THREADS_PER_BLOCK / WARP_SIZE;\n-    int block_x_step1 = num_key / threads_y;\n-    int block_x_step2 = num_query / threads_y;\n-    int block_y = batch_size;\n-\n-    dim3 threads(threads_x, threads_y);\n-    dim3 blocks_step1(block_x_step1, block_y);\n-    dim3 blocks_step2(block_x_step2, block_y);\n-\n-    int *query_mask_ptr = query_mask.data_ptr<int>();\n-    int *query_hash_code_ptr = query_hash_code.data_ptr<int>();\n-    int *key_mask_ptr = key_mask.data_ptr<int>();\n-    int *key_hash_code_ptr = key_hash_code.data_ptr<int>();\n-    float *value_ptr = value.data_ptr<float>();\n-    float *hashtable_value_ptr = hashtable_value.data_ptr<float>();\n-    float *cumulation_value_ptr = cumulation_value.data_ptr<float>();\n-\n-    for (int value_offset = 0; value_offset < value_dim; value_offset = value_offset + WARP_SIZE) {\n-\n-      cudaMemset(hashtable_value_ptr, 0, (batch_size * num_hash_f * hashtable_capacity * WARP_SIZE) * sizeof(float));\n-\n-      lsh_cumulation_ver1_step1_cuda_kernel<<<blocks_step1, threads>>>(\n-        key_mask_ptr,\n-        key_hash_code_ptr,\n-        value_ptr,\n-        hashtable_value_ptr,\n-        batch_size,\n-        num_hash_f,\n-        hashtable_capacity,\n-        num_key,\n-        value_dim,\n-        value_offset\n-      );\n-\n-      lsh_cumulation_ver1_step2_cuda_kernel<<<blocks_step2, threads>>>(\n-        query_mask_ptr,\n-        query_hash_code_ptr,\n-        hashtable_value_ptr,\n-        cumulation_value_ptr,\n-        batch_size,\n-        num_hash_f,\n-        hashtable_capacity,\n-        num_query,\n-        value_dim,\n-        value_offset\n-      );\n-    }\n-\n-  }\n-\n-  return cumulation_value;\n-\n-}\n-\n-at::Tensor lsh_weighted_cumulation_ver1_kernel(\n-  at::Tensor query_mask,\n-  at::Tensor query_hash_code,\n-  at::Tensor query_weight,\n-  at::Tensor key_mask,\n-  at::Tensor key_hash_code,\n-  at::Tensor key_weight,\n-  at::Tensor value,\n-  int hashtable_capacity,\n-  bool use_cuda\n-) {\n-\n-  int batch_size = query_hash_code.size(0);\n-  int num_hash_f = query_hash_code.size(2);\n-\n-  int num_query = query_hash_code.size(1);\n-  int num_key = key_hash_code.size(1);\n-  int value_dim = value.size(2);\n-  int weight_dim = query_weight.size(2);\n-\n-  at::Tensor hashtable_value = at::zeros({batch_size, num_hash_f, hashtable_capacity, WARP_SIZE}, value.options());\n-  at::Tensor cumulation_value = at::zeros({batch_size, num_query, value_dim}, value.options());\n-\n-  if (use_cuda) {\n-    int threads_x = WARP_SIZE;\n-    int threads_y = OPTIMAL_THREADS_PER_BLOCK / WARP_SIZE;\n-    int block_x_step1 = num_key / threads_y;\n-    int block_x_step2 = num_query / threads_y;\n-    int block_y = batch_size;\n-\n-    dim3 threads(threads_x, threads_y);\n-    dim3 blocks_step1(block_x_step1, block_y);\n-    dim3 blocks_step2(block_x_step2, block_y);\n-\n-    int *query_mask_ptr = query_mask.data_ptr<int>();\n-    int *query_hash_code_ptr = query_hash_code.data_ptr<int>();\n-    float *query_weight_ptr = query_weight.data_ptr<float>();\n-    int *key_mask_ptr = key_mask.data_ptr<int>();\n-    int *key_hash_code_ptr = key_hash_code.data_ptr<int>();\n-    float *key_weight_ptr = key_weight.data_ptr<float>();\n-    float *value_ptr = value.data_ptr<float>();\n-    float *hashtable_value_ptr = hashtable_value.data_ptr<float>();\n-    float *cumulation_value_ptr = cumulation_value.data_ptr<float>();\n-\n-    for (int value_offset = 0; value_offset < value_dim; value_offset = value_offset + WARP_SIZE) {\n-      for (int weight_idx = 0; weight_idx < weight_dim; weight_idx++) {\n-\n-        cudaMemset(hashtable_value_ptr, 0, (batch_size * num_hash_f * hashtable_capacity * WARP_SIZE) * sizeof(float));\n-\n-        lsh_weighted_cumulation_ver1_step1_cuda_kernel<<<blocks_step1, threads>>>(\n-          key_mask_ptr,\n-          key_hash_code_ptr,\n-          key_weight_ptr,\n-          value_ptr,\n-          hashtable_value_ptr,\n-          batch_size,\n-          num_hash_f,\n-          hashtable_capacity,\n-          num_key,\n-          value_dim,\n-          weight_dim,\n-          value_offset,\n-          weight_idx\n-        );\n-\n-        lsh_weighted_cumulation_ver1_step2_cuda_kernel<<<blocks_step2, threads>>>(\n-          query_mask_ptr,\n-          query_hash_code_ptr,\n-          query_weight_ptr,\n-          hashtable_value_ptr,\n-          cumulation_value_ptr,\n-          batch_size,\n-          num_hash_f,\n-          hashtable_capacity,\n-          num_query,\n-          value_dim,\n-          weight_dim,\n-          value_offset,\n-          weight_idx\n-        );\n-      }\n-    }\n-\n-  }\n-\n-  return cumulation_value;\n-\n-}\n-\n-at::Tensor lsh_weighted_cumulation_ver2_kernel(\n-  at::Tensor query_mask,\n-  at::Tensor query_hash_code,\n-  at::Tensor query_weight,\n-  at::Tensor key_mask,\n-  at::Tensor key_hash_code,\n-  at::Tensor key_weight,\n-  at::Tensor value,\n-  int hashtable_capacity,\n-  bool use_cuda\n-) {\n-\n-  int batch_size = query_hash_code.size(0);\n-  int num_hash_f = query_hash_code.size(2);\n-\n-  int num_query = query_hash_code.size(1);\n-  int num_key = key_hash_code.size(1);\n-  int value_dim = value.size(2);\n-  int weight_dim = query_weight.size(2);\n-\n-  at::Tensor count_sort_table = at::zeros({batch_size, num_hash_f, hashtable_capacity}, query_hash_code.options());\n-  at::Tensor key_sorted_idxes = at::zeros({batch_size, num_hash_f, num_key}, query_hash_code.options());\n-  at::Tensor query_info = at::zeros({batch_size, num_query, 2, num_hash_f}, query_hash_code.options());\n-  at::Tensor cumulation_value = at::zeros({batch_size, num_query, value_dim}, value.options());\n-\n-  if (use_cuda) {\n-\n-    int *query_mask_ptr = query_mask.data_ptr<int>();\n-    int *query_hash_code_ptr = query_hash_code.data_ptr<int>();\n-    float *query_weight_ptr = query_weight.data_ptr<float>();\n-    int *key_mask_ptr = key_mask.data_ptr<int>();\n-    int *key_hash_code_ptr = key_hash_code.data_ptr<int>();\n-    float *key_weight_ptr = key_weight.data_ptr<float>();\n-    float *value_ptr = value.data_ptr<float>();\n-\n-    int *count_sort_table_ptr = count_sort_table.data_ptr<int>();\n-    int *key_sorted_idxes_ptr = key_sorted_idxes.data_ptr<int>();\n-    int *query_info_ptr = query_info.data_ptr<int>();\n-\n-    float *cumulation_value_ptr = cumulation_value.data_ptr<float>();\n-\n-    {\n-      dim3 threads_step13(num_hash_f, max(1, OPTIMAL_THREADS_PER_BLOCK / num_hash_f));\n-      dim3 blocks_step13(num_key / max(1, OPTIMAL_THREADS_PER_BLOCK / num_hash_f), batch_size);\n-      dim3 threads_step2(min(hashtable_capacity, OPTIMAL_THREADS_PER_BLOCK));\n-      dim3 blocks_step2(num_hash_f, batch_size);\n-      int shared_mem = hashtable_capacity * sizeof(float);\n-      count_sort_step1_cuda_kernel<<<blocks_step13, threads_step13>>>(\n-        key_mask_ptr,\n-        key_hash_code_ptr,\n-        count_sort_table_ptr,\n-        batch_size,\n-        num_hash_f,\n-        hashtable_capacity,\n-        num_key\n-      );\n-      count_sort_step2_cuda_kernel<<<blocks_step2, threads_step2, shared_mem>>>(\n-        count_sort_table_ptr,\n-        batch_size,\n-        num_hash_f,\n-        hashtable_capacity\n-      );\n-      count_sort_step3_cuda_kernel<<<blocks_step13, threads_step13>>>(\n-        key_mask_ptr,\n-        key_hash_code_ptr,\n-        count_sort_table_ptr,\n-        key_sorted_idxes_ptr,\n-        batch_size,\n-        num_hash_f,\n-        hashtable_capacity,\n-        num_key\n-      );\n-    }\n-    {\n-      dim3 threads(num_hash_f, max(1, OPTIMAL_THREADS_PER_BLOCK / num_hash_f));\n-      dim3 blocks(num_query / max(1, OPTIMAL_THREADS_PER_BLOCK / num_hash_f), batch_size);\n-      extract_query_info_cuda_kernel<<<blocks, threads>>>(\n-        query_mask_ptr,\n-        query_hash_code_ptr,\n-        count_sort_table_ptr,\n-        query_info_ptr,\n-        batch_size,\n-        num_hash_f,\n-        hashtable_capacity,\n-        num_query\n-      );\n-    }\n-    {\n-      dim3 threads(WARP_SIZE, OPTIMAL_THREADS_PER_BLOCK / WARP_SIZE);\n-      dim3 blocks(num_query, num_hash_f, batch_size);\n-      int shared_mem = (weight_dim + WARP_SIZE) * sizeof(float);\n-      lsh_weighted_cumulation_ver2_step2_cuda_kernel<<<blocks, threads, shared_mem>>>(\n-        query_mask_ptr,\n-        query_info_ptr,\n-        key_sorted_idxes_ptr,\n-        query_weight_ptr,\n-        key_weight_ptr,\n-        value_ptr,\n-        cumulation_value_ptr,\n-        batch_size,\n-        num_hash_f,\n-        num_query,\n-        num_key,\n-        value_dim,\n-        weight_dim\n-      );\n-    }\n-  }\n-\n-  return cumulation_value;\n-\n-}\n-\n-at::Tensor lsh_weighted_cumulation_ver3_kernel(\n-  at::Tensor query_mask,\n-  at::Tensor query_hash_code,\n-  at::Tensor query_weight,\n-  at::Tensor key_mask,\n-  at::Tensor key_hash_code,\n-  at::Tensor key_weight,\n-  at::Tensor value,\n-  int hashtable_capacity,\n-  bool use_cuda\n-) {\n-\n-  int batch_size = query_hash_code.size(0);\n-  int num_hash_f = query_hash_code.size(2);\n-\n-  int num_query = query_hash_code.size(1);\n-  int num_key = key_hash_code.size(1);\n-  int value_dim = value.size(2);\n-  int weight_dim = query_weight.size(2);\n-\n-  at::Tensor count_sort_table = at::zeros({batch_size, num_hash_f, hashtable_capacity}, query_hash_code.options());\n-  at::Tensor query_sorted_idxes = at::zeros({batch_size, num_hash_f, num_query}, query_hash_code.options());\n-  at::Tensor key_info = at::zeros({batch_size, num_key, 2, num_hash_f}, query_hash_code.options());\n-  at::Tensor cumulation_value = at::zeros({batch_size, num_query, value_dim}, value.options());\n-\n-  if (use_cuda) {\n-\n-    int *query_mask_ptr = query_mask.data_ptr<int>();\n-    int *query_hash_code_ptr = query_hash_code.data_ptr<int>();\n-    float *query_weight_ptr = query_weight.data_ptr<float>();\n-    int *key_mask_ptr = key_mask.data_ptr<int>();\n-    int *key_hash_code_ptr = key_hash_code.data_ptr<int>();\n-    float *key_weight_ptr = key_weight.data_ptr<float>();\n-    float *value_ptr = value.data_ptr<float>();\n-\n-    int *count_sort_table_ptr = count_sort_table.data_ptr<int>();\n-    int *query_sorted_idxes_ptr = query_sorted_idxes.data_ptr<int>();\n-    int *key_info_ptr = key_info.data_ptr<int>();\n-\n-    float *cumulation_value_ptr = cumulation_value.data_ptr<float>();\n-\n-    {\n-      dim3 threads_step13(num_hash_f, max(1, OPTIMAL_THREADS_PER_BLOCK / num_hash_f));\n-      dim3 blocks_step13(num_query / max(1, OPTIMAL_THREADS_PER_BLOCK / num_hash_f), batch_size);\n-      dim3 threads_step2(min(hashtable_capacity, OPTIMAL_THREADS_PER_BLOCK));\n-      dim3 blocks_step2(num_hash_f, batch_size);\n-      int shared_mem = hashtable_capacity * sizeof(float);\n-      count_sort_step1_cuda_kernel<<<blocks_step13, threads_step13>>>(\n-        query_mask_ptr,\n-        query_hash_code_ptr,\n-        count_sort_table_ptr,\n-        batch_size,\n-        num_hash_f,\n-        hashtable_capacity,\n-        num_query\n-      );\n-      count_sort_step2_cuda_kernel<<<blocks_step2, threads_step2, shared_mem>>>(\n-        count_sort_table_ptr,\n-        batch_size,\n-        num_hash_f,\n-        hashtable_capacity\n-      );\n-      count_sort_step3_cuda_kernel<<<blocks_step13, threads_step13>>>(\n-        query_mask_ptr,\n-        query_hash_code_ptr,\n-        count_sort_table_ptr,\n-        query_sorted_idxes_ptr,\n-        batch_size,\n-        num_hash_f,\n-        hashtable_capacity,\n-        num_query\n-      );\n-    }\n-    {\n-      dim3 threads(num_hash_f, max(1, OPTIMAL_THREADS_PER_BLOCK / num_hash_f));\n-      dim3 blocks(num_key / max(1, OPTIMAL_THREADS_PER_BLOCK / num_hash_f), batch_size);\n-      extract_query_info_cuda_kernel<<<blocks, threads>>>(\n-        key_mask_ptr,\n-        key_hash_code_ptr,\n-        count_sort_table_ptr,\n-        key_info_ptr,\n-        batch_size,\n-        num_hash_f,\n-        hashtable_capacity,\n-        num_key\n-      );\n-    }\n-    {\n-      dim3 threads(WARP_SIZE, OPTIMAL_THREADS_PER_BLOCK / WARP_SIZE);\n-      dim3 blocks(num_key, num_hash_f, batch_size);\n-      int shared_mem = (weight_dim + value_dim + WARP_SIZE) * sizeof(float);\n-      lsh_weighted_cumulation_ver3_step2_cuda_kernel<<<blocks, threads, shared_mem>>>(\n-        query_sorted_idxes_ptr,\n-        key_mask_ptr,\n-        key_info_ptr,\n-        query_weight_ptr,\n-        key_weight_ptr,\n-        value_ptr,\n-        cumulation_value_ptr,\n-        batch_size,\n-        num_hash_f,\n-        num_query,\n-        num_key,\n-        value_dim,\n-        weight_dim\n-      );\n-    }\n-  }\n-\n-  return cumulation_value;\n-\n-}\n-\n-at::Tensor lsh_weighted_cumulation_ver4_kernel(\n-  at::Tensor query_mask,\n-  at::Tensor query_hash_code,\n-  at::Tensor query_weight,\n-  at::Tensor key_mask,\n-  at::Tensor key_hash_code,\n-  at::Tensor key_weight,\n-  at::Tensor value,\n-  int hashtable_capacity,\n-  bool use_cuda\n-) {\n-\n-  int batch_size = query_hash_code.size(0);\n-  int num_hash_f = query_hash_code.size(2);\n-\n-  int num_query = query_hash_code.size(1);\n-  int num_key = key_hash_code.size(1);\n-  int value_dim = value.size(2);\n-  int weight_dim = query_weight.size(2);\n-\n-  at::Tensor count_sort_table = at::zeros({batch_size, num_hash_f, hashtable_capacity}, query_hash_code.options());\n-  at::Tensor query_sorted_idxes = at::zeros({batch_size, num_hash_f, num_query}, query_hash_code.options());\n-  at::Tensor key_info = at::zeros({batch_size, num_key, 2, num_hash_f}, query_hash_code.options());\n-  at::Tensor cumulation_value = at::zeros({batch_size, num_query, value_dim}, value.options());\n-\n-  if (use_cuda) {\n-\n-    int *query_mask_ptr = query_mask.data_ptr<int>();\n-    int *query_hash_code_ptr = query_hash_code.data_ptr<int>();\n-    float *query_weight_ptr = query_weight.data_ptr<float>();\n-    int *key_mask_ptr = key_mask.data_ptr<int>();\n-    int *key_hash_code_ptr = key_hash_code.data_ptr<int>();\n-    float *key_weight_ptr = key_weight.data_ptr<float>();\n-    float *value_ptr = value.data_ptr<float>();\n-\n-    int *count_sort_table_ptr = count_sort_table.data_ptr<int>();\n-    int *query_sorted_idxes_ptr = query_sorted_idxes.data_ptr<int>();\n-    int *key_info_ptr = key_info.data_ptr<int>();\n-\n-    float *cumulation_value_ptr = cumulation_value.data_ptr<float>();\n-\n-    {\n-      dim3 threads_step13(num_hash_f, max(1, OPTIMAL_THREADS_PER_BLOCK / num_hash_f));\n-      dim3 blocks_step13(num_query / max(1, OPTIMAL_THREADS_PER_BLOCK / num_hash_f), batch_size);\n-      dim3 threads_step2(min(hashtable_capacity, OPTIMAL_THREADS_PER_BLOCK));\n-      dim3 blocks_step2(num_hash_f, batch_size);\n-      int shared_mem = hashtable_capacity * sizeof(float);\n-      count_sort_step1_cuda_kernel<<<blocks_step13, threads_step13>>>(\n-        query_mask_ptr,\n-        query_hash_code_ptr,\n-        count_sort_table_ptr,\n-        batch_size,\n-        num_hash_f,\n-        hashtable_capacity,\n-        num_query\n-      );\n-      count_sort_step2_cuda_kernel<<<blocks_step2, threads_step2, shared_mem>>>(\n-        count_sort_table_ptr,\n-        batch_size,\n-        num_hash_f,\n-        hashtable_capacity\n-      );\n-      count_sort_step3_cuda_kernel<<<blocks_step13, threads_step13>>>(\n-        query_mask_ptr,\n-        query_hash_code_ptr,\n-        count_sort_table_ptr,\n-        query_sorted_idxes_ptr,\n-        batch_size,\n-        num_hash_f,\n-        hashtable_capacity,\n-        num_query\n-      );\n-    }\n-    {\n-      dim3 threads(num_hash_f, max(1, OPTIMAL_THREADS_PER_BLOCK / num_hash_f));\n-      dim3 blocks(num_key / max(1, OPTIMAL_THREADS_PER_BLOCK / num_hash_f), batch_size);\n-      extract_query_info_cuda_kernel<<<blocks, threads>>>(\n-        key_mask_ptr,\n-        key_hash_code_ptr,\n-        count_sort_table_ptr,\n-        key_info_ptr,\n-        batch_size,\n-        num_hash_f,\n-        hashtable_capacity,\n-        num_key\n-      );\n-    }\n-    {\n-      dim3 threads(WARP_SIZE, OPTIMAL_THREADS_PER_BLOCK / WARP_SIZE);\n-      dim3 blocks(num_key, batch_size);\n-      int shared_mem = (weight_dim + value_dim + 2 * num_hash_f) * sizeof(float);\n-      lsh_weighted_cumulation_ver4_step2_cuda_kernel<<<blocks, threads, shared_mem>>>(\n-        query_sorted_idxes_ptr,\n-        key_mask_ptr,\n-        key_info_ptr,\n-        query_weight_ptr,\n-        key_weight_ptr,\n-        value_ptr,\n-        cumulation_value_ptr,\n-        batch_size,\n-        num_hash_f,\n-        num_query,\n-        num_key,\n-        value_dim,\n-        weight_dim\n-      );\n-    }\n-  }\n-\n-  return cumulation_value;\n-\n-}"
      },
      {
        "filename": "src/transformers/kernels/yoso/fast_lsh_cumulation.h",
        "status": "removed",
        "additions": 0,
        "deletions": 71,
        "changes": 71,
        "patch": "@@ -1,71 +0,0 @@\n-#include <torch/extension.h>\n-#include <ATen/ATen.h>\n-#include <vector>\n-\n-std::vector<at::Tensor> fast_hash_ver1_kernel(\n-  at::Tensor query_mask,\n-  at::Tensor query_vector,\n-  at::Tensor key_mask,\n-  at::Tensor key_vector,\n-  int num_hash_f,\n-  int hash_code_len,\n-  bool use_cuda\n-);\n-\n-at::Tensor lsh_cumulation_ver1_kernel(\n-  at::Tensor query_mask,\n-  at::Tensor query_hash_code,\n-  at::Tensor key_mask,\n-  at::Tensor key_hash_code,\n-  at::Tensor value,\n-  int hashtable_capacity,\n-  bool use_cuda\n-);\n-\n-at::Tensor lsh_weighted_cumulation_ver1_kernel(\n-  at::Tensor query_mask,\n-  at::Tensor query_hash_code,\n-  at::Tensor query_weight,\n-  at::Tensor key_mask,\n-  at::Tensor key_hash_code,\n-  at::Tensor key_weight,\n-  at::Tensor value,\n-  int hashtable_capacity,\n-  bool use_cuda\n-);\n-\n-at::Tensor lsh_weighted_cumulation_ver2_kernel(\n-  at::Tensor query_mask,\n-  at::Tensor query_hash_code,\n-  at::Tensor query_weight,\n-  at::Tensor key_mask,\n-  at::Tensor key_hash_code,\n-  at::Tensor key_weight,\n-  at::Tensor value,\n-  int hashtable_capacity,\n-  bool use_cuda\n-);\n-\n-at::Tensor lsh_weighted_cumulation_ver3_kernel(\n-  at::Tensor query_mask,\n-  at::Tensor query_hash_code,\n-  at::Tensor query_weight,\n-  at::Tensor key_mask,\n-  at::Tensor key_hash_code,\n-  at::Tensor key_weight,\n-  at::Tensor value,\n-  int hashtable_capacity,\n-  bool use_cuda\n-);\n-\n-at::Tensor lsh_weighted_cumulation_ver4_kernel(\n-  at::Tensor query_mask,\n-  at::Tensor query_hash_code,\n-  at::Tensor query_weight,\n-  at::Tensor key_mask,\n-  at::Tensor key_hash_code,\n-  at::Tensor key_weight,\n-  at::Tensor value,\n-  int hashtable_capacity,\n-  bool use_cuda\n-);"
      },
      {
        "filename": "src/transformers/kernels/yoso/fast_lsh_cumulation_cuda.cu",
        "status": "removed",
        "additions": 0,
        "deletions": 825,
        "changes": 825,
        "patch": "@@ -1,825 +0,0 @@\n-// File from https://github.com/mlpen/YOSO/blob/main/encoders/backbones/efficient_attentions/yoso/yoso_v1/cuda/fast_lsh_cumulation_cuda.cu\n-\n-#include \"fast_lsh_cumulation_cuda.h\"\n-#include \"common_cuda_device.h\"\n-#include \"common_cuda.h\"\n-#include \"common.h\"\n-#include <stdio.h>\n-//////////////////////////////////////////////////////////////////////////////////////////////////\n-//////////////////////////////////////////////////////////////////////////////////////////////////\n-\n-inline __device__ void fast_hadamard_transform(float *vector_buffer, int vector_dim, int dim_idx) {\n-  int stride = vector_dim / 2;\n-  while (stride > (WARP_SIZE / 2)) {\n-    __syncthreads();\n-    int sign = 1 - ((dim_idx / stride) % 2) * 2;\n-    float val1 = vector_buffer[dim_idx];\n-    float val2 = vector_buffer[dim_idx + sign * stride];\n-    __syncthreads();\n-    vector_buffer[dim_idx] = float(sign) * val1 + val2;\n-    stride = stride / 2;\n-  }\n-\n-  float val = vector_buffer[dim_idx];\n-  #pragma unroll\n-  for (stride = (WARP_SIZE / 2); stride > 0; stride = stride / 2) {\n-    int sign = 1 - ((dim_idx / stride) % 2) * 2;\n-    val = float(sign) * val + __shfl_xor_sync(FULL_MASK, val, stride);\n-  }\n-  vector_buffer[dim_idx] = val;\n-}\n-\n-__global__ void fast_hash_ver1_cuda_kernel(\n-  int *mask,        // [batch_size, num_vector]\n-  float *vector,    // [batch_size, num_vector, vector_dim]\n-  int *Dmat,        // [batch_size, 3, num_part, vector_dim]\n-  int *hash_code,   // [batch_size, num_vector, num_hash_f]\n-  int batch_size,\n-  int num_vector,\n-  int vector_dim,\n-  int num_part,\n-  int num_hash_f,\n-  int hash_code_len\n-) {\n-\n-  int batch_idx = blockIdx.z;\n-  int vector_idx = blockIdx.y;\n-  int part_idx = blockIdx.x;\n-\n-  int dim_idx = threadIdx.x;\n-\n-  int batch_idx__vector_idx = batch_idx * num_vector + vector_idx;\n-  if (mask[batch_idx__vector_idx] == 0) {\n-    return;\n-  }\n-\n-  extern __shared__ float buffer[];\n-  float *vector_buffer = buffer;\n-\n-  vector_buffer[dim_idx] = vector[batch_idx__vector_idx * vector_dim + dim_idx];\n-\n-  vector_buffer[dim_idx] = vector_buffer[dim_idx] * (float)Dmat[((batch_idx * 3 + 0) * num_part + part_idx) * vector_dim + dim_idx];\n-  fast_hadamard_transform(vector_buffer, vector_dim, dim_idx);\n-  vector_buffer[dim_idx] = vector_buffer[dim_idx] * (float)Dmat[((batch_idx * 3 + 1) * num_part + part_idx) * vector_dim + dim_idx];\n-  fast_hadamard_transform(vector_buffer, vector_dim, dim_idx);\n-  vector_buffer[dim_idx] = vector_buffer[dim_idx] * (float)Dmat[((batch_idx * 3 + 2) * num_part + part_idx) * vector_dim + dim_idx];\n-  fast_hadamard_transform(vector_buffer, vector_dim, dim_idx);\n-\n-  int num_hash_per_part = vector_dim / hash_code_len;\n-  if (hash_code_len == 8 || hash_code_len == 16) {\n-    int code = select(vector_buffer[dim_idx] > 0, 1 << (dim_idx % hash_code_len), 0);\n-    for (int offset = 1; offset < hash_code_len; offset = offset * 2) {\n-      code += __shfl_xor_sync(FULL_MASK, code, offset);\n-    }\n-    if (dim_idx % hash_code_len == 0) {\n-      int hash_f_idx = part_idx * num_hash_per_part + dim_idx / hash_code_len;\n-      if (hash_f_idx < num_hash_f) {\n-        hash_code[batch_idx__vector_idx * num_hash_f + hash_f_idx] = code;\n-      }\n-    }\n-  } else {\n-    vector_buffer[dim_idx] = select(vector_buffer[dim_idx] > 0, 1 << (dim_idx % hash_code_len), 0);\n-    __syncthreads();\n-    if (dim_idx < num_hash_per_part) {\n-      int code = 0;\n-      for (int i = 0; i < hash_code_len; i++) {\n-        code += vector_buffer[dim_idx * hash_code_len + i];\n-      }\n-      int hash_f_idx = part_idx * num_hash_per_part + dim_idx;\n-      if (hash_f_idx < num_hash_f) {\n-        hash_code[batch_idx__vector_idx * num_hash_f + hash_f_idx] = code;\n-      }\n-    }\n-  }\n-}\n-\n-__global__ void lsh_cumulation_ver1_step1_cuda_kernel(\n-  int *key_mask,           // [batch_size, num_key]\n-  int *key_hash_code,      // [batch_size, num_key, num_hash_f]\n-  float *value,            // [batch_size, num_key, value_dim]\n-  float *hashtable_value,  // [batch_size, num_hash_f, hashtable_capacity, WARP_SIZE]\n-  int batch_size,\n-  int num_hash_f,\n-  int hashtable_capacity,\n-  int num_key,\n-  int value_dim,\n-  int offset_warp\n-) {\n-\n-  int warp_thread_idx = threadIdx.x;\n-\n-  int batch_idx = blockIdx.y;\n-  int key_idx = blockIdx.x * blockDim.y + threadIdx.y;\n-\n-  int batch_idx__key_idx = batch_idx * num_key + key_idx;\n-  if (key_mask[batch_idx__key_idx] == 0) {\n-    return;\n-  }\n-\n-  if (num_hash_f > WARP_SIZE) {\n-    float warp_value = value[batch_idx__key_idx * value_dim + offset_warp + warp_thread_idx];\n-    for (int hash_f_start = 0; hash_f_start < num_hash_f; hash_f_start = hash_f_start + WARP_SIZE) {\n-      int warp_hashcode = key_hash_code[batch_idx__key_idx * num_hash_f + hash_f_start + warp_thread_idx];\n-      #pragma unroll\n-      for (int hash_f_offset = 0; hash_f_offset < WARP_SIZE; hash_f_offset++) {\n-        int current_hashcode = warp_hashcode;\n-        current_hashcode = __shfl_sync(FULL_MASK, current_hashcode, hash_f_offset);\n-        int hashtable_idx = (batch_idx * num_hash_f + (hash_f_start + hash_f_offset)) * hashtable_capacity + current_hashcode;\n-        atomicAdd(&hashtable_value[hashtable_idx * WARP_SIZE + warp_thread_idx], warp_value);\n-      }\n-    }\n-  } else {\n-    float warp_value = value[batch_idx__key_idx * value_dim + offset_warp + warp_thread_idx];\n-    int warp_hashcode = 0;\n-    if (warp_thread_idx < num_hash_f) {\n-      warp_hashcode = key_hash_code[batch_idx__key_idx * num_hash_f + warp_thread_idx];\n-    }\n-    for (int hash_f_idx = 0; hash_f_idx < num_hash_f; hash_f_idx++) {\n-      int current_hashcode = warp_hashcode;\n-      current_hashcode = __shfl_sync(FULL_MASK, current_hashcode, hash_f_idx);\n-      int hashtable_idx = (batch_idx * num_hash_f + hash_f_idx) * hashtable_capacity + current_hashcode;\n-      atomicAdd(&hashtable_value[hashtable_idx * WARP_SIZE + warp_thread_idx], warp_value);\n-    }\n-  }\n-\n-}\n-\n-__global__ void lsh_cumulation_ver1_step2_cuda_kernel(\n-  int *query_mask,         // [batch_size, num_query]\n-  int *query_hash_code,    // [batch_size, num_query, num_hash_f]\n-  float *hashtable_value,  // [batch_size, num_hash_f, hashtable_capacity, WARP_SIZE]\n-  float *cumulation_value, // [batch_size, num_query, value_dim]\n-  int batch_size,\n-  int num_hash_f,\n-  int hashtable_capacity,\n-  int num_query,\n-  int value_dim,\n-  int offset_warp\n-) {\n-\n-  int warp_thread_idx = threadIdx.x;\n-\n-  int batch_idx = blockIdx.y;\n-  int query_idx = blockIdx.x * blockDim.y + threadIdx.y;\n-\n-  int batch_idx__query_idx = batch_idx * num_query + query_idx;\n-  if (query_mask[batch_idx__query_idx] == 0) {\n-    return;\n-  }\n-\n-  if (num_hash_f > WARP_SIZE) {\n-    float warp_value = 0;\n-    for (int hash_f_start = 0; hash_f_start < num_hash_f; hash_f_start = hash_f_start + WARP_SIZE) {\n-      int warp_hashcode = query_hash_code[batch_idx__query_idx * num_hash_f + hash_f_start + warp_thread_idx];\n-      #pragma unroll\n-      for (int hash_f_offset = 0; hash_f_offset < WARP_SIZE; hash_f_offset++) {\n-        int current_hashcode = warp_hashcode;\n-        current_hashcode = __shfl_sync(FULL_MASK, current_hashcode, hash_f_offset);\n-        int hashtable_idx = (batch_idx * num_hash_f + (hash_f_start + hash_f_offset)) * hashtable_capacity + current_hashcode;\n-        warp_value = warp_value + hashtable_value[hashtable_idx * WARP_SIZE + warp_thread_idx];\n-      }\n-    }\n-    cumulation_value[batch_idx__query_idx * value_dim + offset_warp + warp_thread_idx] = warp_value / float(num_hash_f);\n-  } else {\n-    float warp_value = 0;\n-    int warp_hashcode = 0;\n-    if (warp_thread_idx < num_hash_f) {\n-      warp_hashcode = query_hash_code[batch_idx__query_idx * num_hash_f + warp_thread_idx];\n-    }\n-    for (int hash_f_idx = 0; hash_f_idx < num_hash_f; hash_f_idx++) {\n-      int current_hashcode = warp_hashcode;\n-      current_hashcode = __shfl_sync(FULL_MASK, current_hashcode, hash_f_idx);\n-      int hashtable_idx = (batch_idx * num_hash_f + hash_f_idx) * hashtable_capacity + current_hashcode;\n-      warp_value = warp_value + hashtable_value[hashtable_idx * WARP_SIZE + warp_thread_idx];\n-    }\n-    cumulation_value[batch_idx__query_idx * value_dim + offset_warp + warp_thread_idx] = warp_value / float(num_hash_f);\n-  }\n-\n-}\n-\n-__global__ void lsh_weighted_cumulation_ver1_step1_cuda_kernel(\n-  int *key_mask,            // [batch_size, num_key]\n-  int *key_hash_code,       // [batch_size, num_key, num_hash_f]\n-  float *key_weight,        // [batch_size, num_key, weight_dim]\n-  float *value,             // [batch_size, num_key, value_dim]\n-  float *hashtable_value,   // [batch_size, num_hash_f, hashtable_capacity, WARP_SIZE]\n-  int batch_size,\n-  int num_hash_f,\n-  int hashtable_capacity,\n-  int num_key,\n-  int value_dim,\n-  int weight_dim,\n-  int offset_warp,\n-  int weight_idx\n-) {\n-\n-  int warp_thread_idx = threadIdx.x;\n-\n-  int batch_idx = blockIdx.y;\n-  int key_idx = blockIdx.x * blockDim.y + threadIdx.y;\n-\n-  int batch_idx__key_idx = batch_idx * num_key + key_idx;\n-  if (key_mask[batch_idx__key_idx] == 0) {\n-    return;\n-  }\n-\n-  if (num_hash_f > WARP_SIZE) {\n-    float warp_value = key_weight[batch_idx__key_idx * weight_dim + weight_idx] * value[batch_idx__key_idx * value_dim + offset_warp + warp_thread_idx];\n-    for (int hash_f_start = 0; hash_f_start < num_hash_f; hash_f_start = hash_f_start + WARP_SIZE) {\n-      int warp_hashcode = key_hash_code[batch_idx__key_idx * num_hash_f + hash_f_start + warp_thread_idx];\n-      #pragma unroll\n-      for (int hash_f_offset = 0; hash_f_offset < WARP_SIZE; hash_f_offset++) {\n-        int current_hashcode = warp_hashcode;\n-        current_hashcode = __shfl_sync(FULL_MASK, current_hashcode, hash_f_offset);\n-        int hashtable_idx = (batch_idx * num_hash_f + (hash_f_start + hash_f_offset)) * hashtable_capacity + current_hashcode;\n-        atomicAdd(&hashtable_value[hashtable_idx * WARP_SIZE + warp_thread_idx], warp_value);\n-      }\n-    }\n-  } else {\n-    float warp_value = key_weight[batch_idx__key_idx * weight_dim + weight_idx] * value[batch_idx__key_idx * value_dim + offset_warp + warp_thread_idx];\n-    int warp_hashcode = 0;\n-    if (warp_thread_idx < num_hash_f) {\n-      warp_hashcode = key_hash_code[batch_idx__key_idx * num_hash_f + warp_thread_idx];\n-    }\n-    for (int hash_f_idx = 0; hash_f_idx < num_hash_f; hash_f_idx++) {\n-      int current_hashcode = warp_hashcode;\n-      current_hashcode = __shfl_sync(FULL_MASK, current_hashcode, hash_f_idx);\n-      int hashtable_idx = (batch_idx * num_hash_f + hash_f_idx) * hashtable_capacity + current_hashcode;\n-      atomicAdd(&hashtable_value[hashtable_idx * WARP_SIZE + warp_thread_idx], warp_value);\n-    }\n-  }\n-\n-}\n-\n-__global__ void lsh_weighted_cumulation_ver1_step2_cuda_kernel(\n-  int *query_mask,          // [batch_size, num_query]\n-  int *query_hash_code,     // [batch_size, num_query, num_hash_f]\n-  float *query_weight,      // [batch_size, num_query, weight_dim]\n-  float *hashtable_value,   // [batch_size, num_hash_f, hashtable_capacity, WARP_SIZE]\n-  float *cumulation_value,  // [batch_size, num_query, value_dim]\n-  int batch_size,\n-  int num_hash_f,\n-  int hashtable_capacity,\n-  int num_query,\n-  int value_dim,\n-  int weight_dim,\n-  int offset_warp,\n-  int weight_idx\n-) {\n-\n-  int warp_thread_idx = threadIdx.x;\n-\n-  int batch_idx = blockIdx.y;\n-  int query_idx = blockIdx.x * blockDim.y + threadIdx.y;\n-\n-  int batch_idx__query_idx = batch_idx * num_query + query_idx;\n-  if (query_mask[batch_idx__query_idx] == 0) {\n-    return;\n-  }\n-\n-  if (num_hash_f > WARP_SIZE) {\n-    float warp_value = 0;\n-    for (int hash_f_start = 0; hash_f_start < num_hash_f; hash_f_start = hash_f_start + WARP_SIZE) {\n-      int warp_hashcode = query_hash_code[batch_idx__query_idx * num_hash_f + hash_f_start + warp_thread_idx];\n-      #pragma unroll\n-      for (int hash_f_offset = 0; hash_f_offset < WARP_SIZE; hash_f_offset++) {\n-        int current_hashcode = warp_hashcode;\n-        current_hashcode = __shfl_sync(FULL_MASK, current_hashcode, hash_f_offset);\n-        int hashtable_idx = (batch_idx * num_hash_f + (hash_f_start + hash_f_offset)) * hashtable_capacity + current_hashcode;\n-        warp_value = warp_value + hashtable_value[hashtable_idx * WARP_SIZE + warp_thread_idx];\n-      }\n-    }\n-    float warp_weight = query_weight[batch_idx__query_idx * weight_dim + weight_idx];\n-    cumulation_value[batch_idx__query_idx * value_dim + offset_warp + warp_thread_idx] += warp_weight * warp_value / float(num_hash_f);\n-  } else {\n-    float warp_value = 0;\n-    int warp_hashcode = 0;\n-    if (warp_thread_idx < num_hash_f) {\n-      warp_hashcode = query_hash_code[batch_idx__query_idx * num_hash_f + warp_thread_idx];\n-    }\n-    for (int hash_f_idx = 0; hash_f_idx < num_hash_f; hash_f_idx++) {\n-      int current_hashcode = warp_hashcode;\n-      current_hashcode = __shfl_sync(FULL_MASK, current_hashcode, hash_f_idx);\n-      int hashtable_idx = (batch_idx * num_hash_f + hash_f_idx) * hashtable_capacity + current_hashcode;\n-      warp_value = warp_value + hashtable_value[hashtable_idx * WARP_SIZE + warp_thread_idx];\n-    }\n-    float warp_weight = query_weight[batch_idx__query_idx * weight_dim + weight_idx];\n-    cumulation_value[batch_idx__query_idx * value_dim + offset_warp + warp_thread_idx] += warp_weight * warp_value / float(num_hash_f);\n-  }\n-\n-}\n-\n-__global__ void count_sort_step1_cuda_kernel(\n-  int *key_mask,         // [batch_size, num_key]\n-  int *key_hash_code,    // [batch_size, num_key, num_hash_f]\n-  int *count_sort_table, // [batch_size, num_hash_f, hashtable_capacity]\n-  int batch_size,\n-  int num_hash_f,\n-  int hashtable_capacity,\n-  int num_key\n-) {\n-\n-  int batch_idx = blockIdx.y;\n-  int key_idx = blockIdx.x * blockDim.y + threadIdx.y;\n-  int hash_f_idx = threadIdx.x;\n-\n-  int batch_idx__key_idx = batch_idx * num_key + key_idx;\n-  if (key_mask[batch_idx__key_idx] == 0) {\n-    return;\n-  }\n-\n-  int hash_code = key_hash_code[batch_idx__key_idx * num_hash_f + hash_f_idx];\n-  atomicAdd(&count_sort_table[(batch_idx * num_hash_f + hash_f_idx) * hashtable_capacity + hash_code], 1);\n-\n-}\n-\n-__global__ void count_sort_step2_cuda_kernel(\n-  int *count_sort_table,  // [batch_size, num_hash_f, hashtable_capacity]\n-  int batch_size,\n-  int num_hash_f,\n-  int hashtable_capacity\n-) {\n-\n-  int batch_idx = blockIdx.y;\n-  int hash_f_idx = blockIdx.x;\n-\n-  int num_threads = blockDim.x;\n-  int thread_id = threadIdx.x;\n-\n-  int batch_idx__hash_f_idx = batch_idx * num_hash_f + hash_f_idx;\n-\n-  extern __shared__ float buffer[];\n-  int *table_buffer = (int*)buffer;\n-\n-  if (thread_id == 0) {\n-    table_buffer[0] = 0;\n-  }\n-  copy_data<int>(&count_sort_table[batch_idx__hash_f_idx * hashtable_capacity], &table_buffer[1], hashtable_capacity - 1, num_threads, thread_id);\n-\n-  for (int table_idx_start = 0; table_idx_start < hashtable_capacity; table_idx_start = table_idx_start + num_threads) {\n-    int thread_value = table_buffer[table_idx_start + thread_id];\n-    int next_thread_value = 0;\n-    for (int offset = 1; offset < WARP_SIZE; offset = offset << 1) {\n-      next_thread_value = __shfl_up_sync(FULL_MASK, thread_value, offset);\n-      if (thread_id % WARP_SIZE >= offset) {\n-        thread_value = thread_value + next_thread_value;\n-      }\n-    }\n-    table_buffer[table_idx_start + thread_id] = thread_value;\n-  }\n-  __syncthreads();\n-\n-  if (hashtable_capacity > WARP_SIZE) {\n-    if (thread_id < WARP_SIZE) {\n-      for (int table_idx_start = WARP_SIZE; table_idx_start < hashtable_capacity; table_idx_start = table_idx_start + WARP_SIZE) {\n-        table_buffer[table_idx_start + thread_id] += table_buffer[table_idx_start - 1];\n-      }\n-    }\n-  }\n-\n-  copy_data<int>(table_buffer, &count_sort_table[batch_idx__hash_f_idx * hashtable_capacity], hashtable_capacity, num_threads, thread_id);\n-\n-}\n-\n-\n-__global__ void count_sort_step3_cuda_kernel(\n-  int *key_mask,          // [batch_size, num_key]\n-  int *key_hash_code,     // [batch_size, num_key, num_hash_f]\n-  int *count_sort_table,  // [batch_size, num_hash_f, hashtable_capacity]\n-  int *key_sorted_idxes,  // [batch_size, num_hash_f, num_key]\n-  int batch_size,\n-  int num_hash_f,\n-  int hashtable_capacity,\n-  int num_key\n-) {\n-\n-  int batch_idx = blockIdx.y;\n-  int key_idx = blockIdx.x * blockDim.y + threadIdx.y;\n-  int hash_f_idx = threadIdx.x;\n-\n-  int batch_idx__key_idx = batch_idx * num_key + key_idx;\n-  if (key_mask[batch_idx__key_idx] == 0) {\n-    return;\n-  }\n-\n-  int batch_idx__hash_f_idx = batch_idx * num_hash_f + hash_f_idx;\n-\n-  int hash_code = key_hash_code[batch_idx__key_idx * num_hash_f + hash_f_idx];\n-  int sort_idx = atomicAdd(&count_sort_table[batch_idx__hash_f_idx * hashtable_capacity + hash_code], 1);\n-  key_sorted_idxes[batch_idx__hash_f_idx * num_key + sort_idx] = key_idx;\n-\n-}\n-\n-__global__ void extract_query_info_cuda_kernel(\n-  int *query_mask,       // [batch_size, num_query]\n-  int *query_hash_code,  // [batch_size, num_query, num_hash_f]\n-  int *count_sort_table, // [batch_size, num_hash_f, hashtable_capacity]\n-  int *query_info,       // [batch_size, num_query, 2, num_hash_f]\n-  int batch_size,\n-  int num_hash_f,\n-  int hashtable_capacity,\n-  int num_query\n-) {\n-\n-  int batch_idx = blockIdx.y;\n-  int query_idx = blockIdx.x * blockDim.y + threadIdx.y;\n-  int hash_f_idx = threadIdx.x;\n-\n-  int batch_idx__query_idx = batch_idx * num_query + query_idx;\n-  if (query_mask[batch_idx__query_idx] == 0) {\n-    return;\n-  }\n-\n-  int hash_code = query_hash_code[batch_idx__query_idx * num_hash_f + hash_f_idx];\n-  int batch_idx__hash_f_idx__hash_code = (batch_idx * num_hash_f + hash_f_idx) * hashtable_capacity + hash_code;\n-\n-  int key_offset = select(hash_code == 0, 0, count_sort_table[batch_idx__hash_f_idx__hash_code - 1]);\n-  int key_count = count_sort_table[batch_idx__hash_f_idx__hash_code] - key_offset;\n-\n-  query_info[batch_idx__query_idx * 2 * num_hash_f + hash_f_idx] = key_offset;\n-  query_info[(batch_idx__query_idx * 2 + 1) * num_hash_f + hash_f_idx] = key_count;\n-\n-}\n-\n-__global__ void lsh_weighted_cumulation_ver2_step2_cuda_kernel(\n-  int *query_mask,         // [batch_size, num_query]\n-  int *query_info,         // [batch_size, num_query, 2, num_hash_f]\n-  int *key_sorted_idxes,   // [batch_size, num_hash_f, num_key]\n-  float *query_weight,     // [batch_size, num_query, weight_dim]\n-  float *key_weight,       // [batch_size, num_key, weight_dim]\n-  float *value,            // [batch_size, num_key, value_dim]\n-  float *cumulation_value, // [batch_size, num_query, value_dim]\n-  int batch_size,\n-  int num_hash_f,\n-  int num_query,\n-  int num_key,\n-  int value_dim,\n-  int weight_dim\n-) {\n-\n-  int batch_idx = blockIdx.z;\n-  int hash_f_idx = blockIdx.y;\n-  int query_idx = blockIdx.x;\n-\n-  int num_threads = blockDim.y * blockDim.x;\n-  int thread_id = threadIdx.y * blockDim.x + threadIdx.x;\n-\n-  int num_warps = blockDim.y;\n-  int warp_idx = threadIdx.y;\n-  int warp_thread_idx = threadIdx.x;\n-\n-  int batch_idx__query_idx = batch_idx * num_query + query_idx;\n-  if (query_mask[batch_idx__query_idx] == 0) {\n-    return;\n-  }\n-\n-  int key_offset = query_info[batch_idx__query_idx * 2 * num_hash_f + hash_f_idx];\n-  int key_count = query_info[(batch_idx__query_idx * 2 + 1) * num_hash_f + hash_f_idx];\n-\n-  if (key_count == 0) {\n-    return;\n-  }\n-\n-  extern __shared__ float buffer[];\n-\n-  if (key_count == 1) {\n-    if (warp_idx == 0) {\n-      int key_idx = key_sorted_idxes[(batch_idx * num_hash_f + hash_f_idx) * num_key + key_offset];\n-      int batch_idx__key_idx = batch_idx * num_key + key_idx;\n-      float weight = 0;\n-      for (int weight_offset = 0; weight_offset < weight_dim; weight_offset = weight_offset + WARP_SIZE) {\n-        int weight_dim_idx = weight_offset + warp_thread_idx;\n-        float val = query_weight[batch_idx__query_idx * weight_dim + weight_dim_idx] * key_weight[batch_idx__key_idx * weight_dim + weight_dim_idx];\n-        #pragma unroll\n-        for (int offset = 1; offset < WARP_SIZE; offset = offset << 1) {\n-          val += __shfl_xor_sync(FULL_MASK, val, offset);\n-        }\n-        weight = weight + val;\n-      }\n-      weight = weight / float(num_hash_f);\n-      for (int value_offset = 0; value_offset < value_dim; value_offset = value_offset + WARP_SIZE) {\n-        int value_dim_idx = value_offset + warp_thread_idx;\n-        float val = value[batch_idx__key_idx * value_dim + value_dim_idx];\n-        atomicAdd(&cumulation_value[batch_idx__query_idx * value_dim + value_dim_idx], weight * val);\n-      }\n-    }\n-  } else {\n-    float *weight_buffer = buffer;\n-    int *key_idxes_buffer = (int*)&buffer[weight_dim];\n-\n-    copy_data_nonblocking<float>(&query_weight[batch_idx__query_idx * weight_dim], weight_buffer, weight_dim, num_threads, thread_id);\n-\n-    while (key_count > 0) {\n-      int work_size = min(WARP_SIZE, key_count);\n-      copy_data_nonblocking<int>(&key_sorted_idxes[(batch_idx * num_hash_f + hash_f_idx) * num_key + key_offset], key_idxes_buffer, work_size, num_threads, thread_id);\n-      __syncthreads();\n-      for (int work_offset = 0; work_offset < WARP_SIZE; work_offset = work_offset + num_warps) {\n-        int work_idx = work_offset + warp_idx;\n-        if (work_idx < key_count) {\n-          int key_idx = key_idxes_buffer[work_idx];\n-          int batch_idx__key_idx = batch_idx * num_key + key_idx;\n-          float weight = 0;\n-          for (int weight_offset = 0; weight_offset < weight_dim; weight_offset = weight_offset + WARP_SIZE) {\n-            int weight_dim_idx = weight_offset + warp_thread_idx;\n-            float val = weight_buffer[weight_dim_idx] * key_weight[batch_idx__key_idx * weight_dim + weight_dim_idx];\n-            #pragma unroll\n-            for (int offset = 1; offset < WARP_SIZE; offset = offset << 1) {\n-              val += __shfl_xor_sync(FULL_MASK, val, offset);\n-            }\n-            weight = weight + val;\n-          }\n-          weight = weight / float(num_hash_f);\n-          for (int value_offset = 0; value_offset < value_dim; value_offset = value_offset + WARP_SIZE) {\n-            int value_dim_idx = value_offset + warp_thread_idx;\n-            float val = value[batch_idx__key_idx * value_dim + value_dim_idx];\n-            atomicAdd(&cumulation_value[batch_idx__query_idx * value_dim + value_dim_idx], weight * val);\n-          }\n-        }\n-      }\n-      key_count = key_count - work_size;\n-      key_offset = key_offset + work_size;\n-    }\n-  }\n-\n-}\n-\n-__global__ void lsh_weighted_cumulation_ver3_step2_cuda_kernel(\n-  int *query_sorted_idxes,   // [batch_size, num_hash_f, num_query]\n-  int *key_mask,             // [batch_size, num_key]\n-  int *key_info,             // [batch_size, num_key, 2, num_hash_f]\n-  float *query_weight,       // [batch_size, num_query, weight_dim]\n-  float *key_weight,         // [batch_size, num_key, weight_dim]\n-  float *value,              // [batch_size, num_key, value_dim]\n-  float *cumulation_value,   // [batch_size, num_query, value_dim]\n-  int batch_size,\n-  int num_hash_f,\n-  int num_query,\n-  int num_key,\n-  int value_dim,\n-  int weight_dim\n-) {\n-\n-  int batch_idx = blockIdx.z;\n-  int hash_f_idx = blockIdx.y;\n-  int key_idx = blockIdx.x;\n-\n-  int num_threads = blockDim.y * blockDim.x;\n-  int thread_id = threadIdx.y * blockDim.x + threadIdx.x;\n-\n-  int num_warps = blockDim.y;\n-  int warp_idx = threadIdx.y;\n-  int warp_thread_idx = threadIdx.x;\n-\n-  int batch_idx__key_idx = batch_idx * num_key + key_idx;\n-  if (key_mask[batch_idx__key_idx] == 0) {\n-    return;\n-  }\n-\n-  int query_offset = key_info[batch_idx__key_idx * 2 * num_hash_f + hash_f_idx];\n-  int query_count = key_info[(batch_idx__key_idx * 2 + 1) * num_hash_f + hash_f_idx];\n-\n-  if (query_count == 0) {\n-    return;\n-  }\n-\n-  extern __shared__ float buffer[];\n-\n-  if (query_count == 1) {\n-    if (warp_idx == 0) {\n-      int query_idx = query_sorted_idxes[(batch_idx * num_hash_f + hash_f_idx) * num_query + query_offset];\n-      int batch_idx__query_idx = batch_idx * num_query + query_idx;\n-      float weight = 0;\n-      for (int weight_offset = 0; weight_offset < weight_dim; weight_offset = weight_offset + WARP_SIZE) {\n-        int weight_dim_idx = weight_offset + warp_thread_idx;\n-        float val = key_weight[batch_idx__key_idx * weight_dim + weight_dim_idx] * query_weight[batch_idx__query_idx * weight_dim + weight_dim_idx];\n-        #pragma unroll\n-        for (int offset = 1; offset < WARP_SIZE; offset = offset << 1) {\n-          val += __shfl_xor_sync(FULL_MASK, val, offset);\n-        }\n-        weight = weight + val;\n-      }\n-      weight = weight / float(num_hash_f);\n-      for (int value_offset = 0; value_offset < value_dim; value_offset = value_offset + WARP_SIZE) {\n-        int value_dim_idx = value_offset + warp_thread_idx;\n-        float val = value[batch_idx__key_idx * value_dim + value_dim_idx];\n-        atomicAdd(&cumulation_value[batch_idx__query_idx * value_dim + value_dim_idx], weight * val);\n-      }\n-    }\n-  } else {\n-    float *weight_buffer = buffer;\n-    float *value_buffer = &buffer[weight_dim];\n-    int *query_idxes_buffer = (int*)&buffer[weight_dim + value_dim];\n-\n-    copy_data_nonblocking<float>(&key_weight[batch_idx__key_idx * weight_dim], weight_buffer, weight_dim, num_threads, thread_id);\n-    copy_data_nonblocking<float>(&value[batch_idx__key_idx * value_dim], value_buffer, value_dim, num_threads, thread_id);\n-\n-    while (query_count > 0) {\n-      int work_size = min(WARP_SIZE, query_count);\n-      copy_data_nonblocking<int>(&query_sorted_idxes[(batch_idx * num_hash_f + hash_f_idx) * num_query + query_offset], query_idxes_buffer, work_size, num_threads, thread_id);\n-      __syncthreads();\n-      for (int work_offset = 0; work_offset < WARP_SIZE; work_offset = work_offset + num_warps) {\n-        int work_idx = work_offset + warp_idx;\n-        if (work_idx < query_count) {\n-          int query_idx = query_idxes_buffer[work_idx];\n-          int batch_idx__query_idx = batch_idx * num_query + query_idx;\n-          float weight = 0;\n-          for (int weight_offset = 0; weight_offset < weight_dim; weight_offset = weight_offset + WARP_SIZE) {\n-            int weight_dim_idx = weight_offset + warp_thread_idx;\n-            float val = weight_buffer[weight_dim_idx] * query_weight[batch_idx__query_idx * weight_dim + weight_dim_idx];\n-            #pragma unroll\n-            for (int offset = 1; offset < WARP_SIZE; offset = offset << 1) {\n-              val += __shfl_xor_sync(FULL_MASK, val, offset);\n-            }\n-            weight = weight + val;\n-          }\n-          weight = weight / float(num_hash_f);\n-          for (int value_offset = 0; value_offset < value_dim; value_offset = value_offset + WARP_SIZE) {\n-            int value_dim_idx = value_offset + warp_thread_idx;\n-            float val = value_buffer[value_dim_idx];\n-            atomicAdd(&cumulation_value[batch_idx__query_idx * value_dim + value_dim_idx], weight * val);\n-          }\n-        }\n-      }\n-      query_count = query_count - work_size;\n-      query_offset = query_offset + work_size;\n-    }\n-  }\n-\n-}\n-\n-__global__ void lsh_weighted_cumulation_ver4_step2_cuda_kernel(\n-  int *query_sorted_idxes,   // [batch_size, num_hash_f, num_query]\n-  int *key_mask,             // [batch_size, num_key]\n-  int *key_info,             // [batch_size, num_key, 2, num_hash_f]\n-  float *query_weight,       // [batch_size, num_query, weight_dim]\n-  float *key_weight,         // [batch_size, num_key, weight_dim]\n-  float *value,              // [batch_size, num_key, value_dim]\n-  float *cumulation_value,   // [batch_size, num_query, value_dim]\n-  int batch_size,\n-  int num_hash_f,\n-  int num_query,\n-  int num_key,\n-  int value_dim,\n-  int weight_dim\n-) {\n-\n-  int batch_idx = blockIdx.y;\n-  int key_idx = blockIdx.x;\n-\n-  int num_threads = blockDim.y * blockDim.x;\n-  int thread_id = threadIdx.y * blockDim.x + threadIdx.x;\n-\n-  int num_warps = blockDim.y;\n-  int warp_idx = threadIdx.y;\n-  int warp_thread_idx = threadIdx.x;\n-\n-  int batch_idx__key_idx = batch_idx * num_key + key_idx;\n-  if (key_mask[batch_idx__key_idx] == 0) {\n-    return;\n-  }\n-\n-  extern __shared__ float buffer[];\n-  float *weight_buffer = buffer;\n-  float *value_buffer = &buffer[weight_dim];\n-  int *key_info_buffer = (int*)&buffer[weight_dim + value_dim];\n-\n-  copy_data_nonblocking<float>(&key_weight[batch_idx__key_idx * weight_dim], weight_buffer, weight_dim, num_threads, thread_id);\n-  copy_data_nonblocking<float>(&value[batch_idx__key_idx * value_dim], value_buffer, value_dim, num_threads, thread_id);\n-  copy_data_nonblocking<int>(&key_info[batch_idx__key_idx * 2 * num_hash_f], key_info_buffer, 2 * num_hash_f, num_threads, thread_id);\n-\n-  int *query_offset_buffer = key_info_buffer;\n-  int *query_count_buffer = &key_info_buffer[num_hash_f];\n-\n-  const int hashtable_size = 1024 + OPTIMAL_THREADS_PER_BLOCK;\n-  __shared__ int hashtable_query[hashtable_size];\n-  __shared__ int hashtable_count[hashtable_size];\n-  __shared__ int inserted_query[hashtable_size];\n-  __shared__ int query_counter[1];\n-\n-  int hash_f_idx_base = 0;\n-\n-  while (true) {\n-\n-    init_buffer_nonblocking<int>(EMPTY_VALUE, hashtable_query, hashtable_size, num_threads, thread_id);\n-    init_buffer_nonblocking<int>(0, hashtable_count, hashtable_size, num_threads, thread_id);\n-    init_buffer_nonblocking<int>(EMPTY_VALUE, inserted_query, hashtable_size, num_threads, thread_id);\n-    init_buffer_nonblocking<int>(0, query_counter, 1, num_threads, thread_id);\n-    __syncthreads();\n-\n-    while (hash_f_idx_base < num_hash_f) {\n-\n-      int hash_f_idx = hash_f_idx_base + warp_idx;\n-      int batch_idx__hash_f_idx = batch_idx * num_hash_f + hash_f_idx;\n-\n-      int stop_flag = 0;\n-\n-      int query_offset = query_offset_buffer[hash_f_idx];\n-      int query_count = query_count_buffer[hash_f_idx];\n-\n-      while (query_count > 0) {\n-\n-        int work_size = min(query_count, WARP_SIZE);\n-\n-        // try inserting query to set and check whether the query is new\n-        int found_new_query = 0;\n-        int query_idx = -1;\n-        if (warp_thread_idx < work_size) {\n-          query_idx = query_sorted_idxes[batch_idx__hash_f_idx * num_query + query_offset + warp_thread_idx];\n-          int slot = set_insert<int>(hashtable_query, hashtable_size, query_idx);\n-          if (slot >= 0) {\n-            found_new_query = atomicAdd(&hashtable_count[slot], 1) == 0;\n-          }\n-        }\n-\n-        // compute cumulative offset\n-        int position_offset = found_new_query;\n-        int next_position_offset = 0;\n-        #pragma unroll\n-        for (int offset = 1; offset < WARP_SIZE; offset = offset << 1) {\n-          next_position_offset = __shfl_up_sync(FULL_MASK, position_offset, offset);\n-          if (thread_id % WARP_SIZE >= offset) {\n-            position_offset = position_offset + next_position_offset;\n-          }\n-        }\n-\n-        // get the inserted query list end index\n-        int inserted_query_base = 0;\n-        if (thread_id % WARP_SIZE == WARP_SIZE - 1) {\n-          inserted_query_base = atomicAdd(query_counter, position_offset);\n-        }\n-        inserted_query_base = __shfl_sync(FULL_MASK, inserted_query_base, WARP_SIZE - 1);\n-\n-        // insert new queries to list\n-        int insert_idx = inserted_query_base + position_offset - 1;\n-        if (found_new_query) {\n-          inserted_query[insert_idx] = query_idx;\n-        }\n-\n-        // remove inserted queries from list\n-        query_offset_buffer[hash_f_idx] += work_size;\n-        query_count_buffer[hash_f_idx] -= work_size;\n-        query_offset += work_size;\n-        query_count -= work_size;\n-\n-        // if list is almost full, stop inserting\n-        if (inserted_query_base + OPTIMAL_THREADS_PER_BLOCK > hashtable_size) {\n-          stop_flag = 1;\n-          break;\n-        }\n-\n-      }\n-\n-      if (stop_flag) {\n-        break;\n-      }\n-\n-      hash_f_idx_base = hash_f_idx_base + num_warps;\n-\n-    }\n-\n-    __syncthreads();\n-\n-    int num_distinct_query = query_counter[0];\n-\n-    if (num_distinct_query > 0) {\n-      for (int idx_base = 0; idx_base < num_distinct_query; idx_base = idx_base + num_warps) {\n-        int idx = idx_base + warp_idx;\n-        if (idx < num_distinct_query) {\n-          int query_idx = inserted_query[idx];\n-          int batch_idx__query_idx = batch_idx * num_query + query_idx;\n-\n-          int slot = set_lookup<int>(hashtable_query, hashtable_size, query_idx);\n-          int duplicate_count = hashtable_count[slot];\n-\n-          float weight = 0;\n-          for (int weight_idx_base = 0; weight_idx_base < weight_dim; weight_idx_base = weight_idx_base + WARP_SIZE) {\n-            int weight_dim_idx = weight_idx_base + warp_thread_idx;\n-            float val = weight_buffer[weight_dim_idx] * query_weight[batch_idx__query_idx * weight_dim + weight_dim_idx];\n-            #pragma unroll\n-            for (int offset = 1; offset < WARP_SIZE; offset = offset << 1) {\n-              val += __shfl_xor_sync(FULL_MASK, val, offset);\n-            }\n-            weight = weight + val;\n-          }\n-\n-          weight = (float)duplicate_count * weight / float(num_hash_f);\n-\n-          for (int value_idx_base = 0; value_idx_base < value_dim; value_idx_base = value_idx_base + WARP_SIZE) {\n-            int value_dim_idx = value_idx_base + warp_thread_idx;\n-            float val = value_buffer[value_dim_idx];\n-            atomicAdd(&cumulation_value[batch_idx__query_idx * value_dim + value_dim_idx], weight * val);\n-          }\n-        }\n-      }\n-    } else {\n-\n-      // all computation is completed if num_distinct_query == 0\n-      break;\n-\n-    }\n-\n-    __syncthreads();\n-\n-  }\n-\n-}"
      },
      {
        "filename": "src/transformers/kernels/yoso/fast_lsh_cumulation_cuda.h",
        "status": "removed",
        "additions": 0,
        "deletions": 157,
        "changes": 157,
        "patch": "@@ -1,157 +0,0 @@\n-__global__ void fast_hash_ver1_cuda_kernel(\n-  int *mask,        // [batch_size, num_vector]\n-  float *vector,    // [batch_size, num_vector, vector_dim]\n-  int *Dmat,        // [3, num_part, vector_dim]\n-  int *hash_code,   // [batch_size, num_vector, num_hash_f]\n-  int batch_size,\n-  int num_vector,\n-  int vector_dim,\n-  int num_part,\n-  int num_hash_f,\n-  int hash_code_len\n-);\n-\n-__global__ void lsh_cumulation_ver1_step1_cuda_kernel(\n-  int *key_mask,           // [batch_size, num_key]\n-  int *key_hash_code,      // [batch_size, num_key, num_hash_f]\n-  float *value,            // [batch_size, num_key, value_dim]\n-  float *hashtable_value,  // [batch_size, num_hash_f, hashtable_capacity, value_dim]\n-  int batch_size,\n-  int num_hash_f,\n-  int hashtable_capacity,\n-  int num_key,\n-  int value_dim,\n-  int offset_warp\n-);\n-\n-__global__ void lsh_cumulation_ver1_step2_cuda_kernel(\n-  int *query_mask,         // [batch_size, num_query]\n-  int *query_hash_code,    // [batch_size, num_query, num_hash_f]\n-  float *hashtable_value,  // [batch_size, num_hash_f, hashtable_capacity, value_dim]\n-  float *cumulation_value, // [batch_size, num_query, value_dim]\n-  int batch_size,\n-  int num_hash_f,\n-  int hashtable_capacity,\n-  int num_query,\n-  int value_dim,\n-  int offset_warp\n-);\n-\n-__global__ void lsh_weighted_cumulation_ver1_step1_cuda_kernel(\n-  int *key_mask,            // [batch_size, num_key]\n-  int *key_hash_code,       // [batch_size, num_key, num_hash_f]\n-  float *key_weight,        // [batch_size, num_key, weight_dim]\n-  float *value,             // [batch_size, num_key, value_dim]\n-  float *hashtable_value,   // [batch_size, num_hash_f, hashtable_capacity, WARP_SIZE]\n-  int batch_size,\n-  int num_hash_f,\n-  int hashtable_capacity,\n-  int num_key,\n-  int value_dim,\n-  int weight_dim,\n-  int offset_warp,\n-  int weight_idx\n-);\n-\n-__global__ void lsh_weighted_cumulation_ver1_step2_cuda_kernel(\n-  int *query_mask,          // [batch_size, num_query]\n-  int *query_hash_code,     // [batch_size, num_query, num_hash_f]\n-  float *query_weight,      // [batch_size, num_query, weight_dim]\n-  float *hashtable_value,   // [batch_size, num_hash_f, hashtable_capacity, WARP_SIZE]\n-  float *cumulation_value,  // [batch_size, num_query, value_dim]\n-  int batch_size,\n-  int num_hash_f,\n-  int hashtable_capacity,\n-  int num_query,\n-  int value_dim,\n-  int weight_dim,\n-  int offset_warp,\n-  int weight_idx\n-);\n-\n-__global__ void count_sort_step1_cuda_kernel(\n-  int *key_mask,         // [batch_size, num_key]\n-  int *key_hash_code,    // [batch_size, num_key, num_hash_f]\n-  int *count_sort_table, // [batch_size, num_hash_f, hashtable_capacity]\n-  int batch_size,\n-  int num_hash_f,\n-  int hashtable_capacity,\n-  int num_key\n-);\n-\n-__global__ void count_sort_step2_cuda_kernel(\n-  int *count_sort_table,  // [batch_size, num_hash_f, hashtable_capacity]\n-  int batch_size,\n-  int num_hash_f,\n-  int hashtable_capacity\n-);\n-\n-__global__ void count_sort_step3_cuda_kernel(\n-  int *key_mask,          // [batch_size, num_key]\n-  int *key_hash_code,     // [batch_size, num_key, num_hash_f]\n-  int *count_sort_table,  // [batch_size, num_hash_f, hashtable_capacity]\n-  int *key_sorted_idxes,  // [batch_size, num_hash_f, num_key]\n-  int batch_size,\n-  int num_hash_f,\n-  int hashtable_capacity,\n-  int num_key\n-);\n-\n-__global__ void extract_query_info_cuda_kernel(\n-  int *query_mask,       // [batch_size, num_query]\n-  int *query_hash_code,  // [batch_size, num_query, num_hash_f]\n-  int *count_sort_table, // [batch_size, num_hash_f, hashtable_capacity]\n-  int *query_info,       // [batch_size, num_query, 2, num_hash_f]\n-  int batch_size,\n-  int num_hash_f,\n-  int hashtable_capacity,\n-  int num_query\n-);\n-\n-__global__ void lsh_weighted_cumulation_ver2_step2_cuda_kernel(\n-  int *query_mask,         // [batch_size, num_query]\n-  int *query_info,         // [batch_size, num_query, 2, num_hash_f]\n-  int *key_sorted_idxes,   // [batch_size, num_hash_f, num_key]\n-  float *query_weight,     // [batch_size, num_query, weight_dim]\n-  float *key_weight,       // [batch_size, num_key, weight_dim]\n-  float *value,            // [batch_size, num_key, value_dim]\n-  float *cumulation_value, // [batch_size, num_query, value_dim]\n-  int batch_size,\n-  int num_hash_f,\n-  int num_query,\n-  int num_key,\n-  int value_dim,\n-  int weight_dim\n-);\n-\n-__global__ void lsh_weighted_cumulation_ver3_step2_cuda_kernel(\n-  int *query_sorted_idxes,   // [batch_size, num_hash_f, num_query]\n-  int *key_mask,             // [batch_size, num_key]\n-  int *key_info,             // [batch_size, num_key, 2, num_hash_f]\n-  float *query_weight,       // [batch_size, num_query, weight_dim]\n-  float *key_weight,         // [batch_size, num_key, weight_dim]\n-  float *value,              // [batch_size, num_key, value_dim]\n-  float *cumulation_value,   // [batch_size, num_query, value_dim]\n-  int batch_size,\n-  int num_hash_f,\n-  int num_query,\n-  int num_key,\n-  int value_dim,\n-  int weight_dim\n-);\n-\n-__global__ void lsh_weighted_cumulation_ver4_step2_cuda_kernel(\n-  int *query_sorted_idxes,   // [batch_size, num_hash_f, num_query]\n-  int *key_mask,             // [batch_size, num_key]\n-  int *key_info,             // [batch_size, num_key, 2, num_hash_f]\n-  float *query_weight,       // [batch_size, num_query, weight_dim]\n-  float *key_weight,         // [batch_size, num_key, weight_dim]\n-  float *value,              // [batch_size, num_key, value_dim]\n-  float *cumulation_value,   // [batch_size, num_query, value_dim]\n-  int batch_size,\n-  int num_hash_f,\n-  int num_query,\n-  int num_key,\n-  int value_dim,\n-  int weight_dim\n-);"
      },
      {
        "filename": "src/transformers/kernels/yoso/fast_lsh_cumulation_torch.cpp",
        "status": "removed",
        "additions": 0,
        "deletions": 128,
        "changes": 128,
        "patch": "@@ -1,128 +0,0 @@\n-#include <torch/extension.h>\n-#include <ATen/ATen.h>\n-#include \"fast_lsh_cumulation.h\"\n-#include \"common_cuda.h\"\n-#include <vector>\n-\n-std::vector<at::Tensor> fast_hash(\n-  at::Tensor query_mask,\n-  at::Tensor query_vector,\n-  at::Tensor key_mask,\n-  at::Tensor key_vector,\n-  int num_hash_f,\n-  int hash_code_len,\n-  bool use_cuda,\n-  int version\n-) {\n-  return fast_hash_ver1_kernel(\n-    query_mask,\n-    query_vector,\n-    key_mask,\n-    key_vector,\n-    num_hash_f,\n-    hash_code_len,\n-    use_cuda\n-  );\n-}\n-\n-at::Tensor lsh_cumulation(\n-  at::Tensor query_mask,         // [batch_size, num_query]\n-  at::Tensor query_hash_code,    // [batch_size, num_query, num_hash_f]\n-  at::Tensor key_mask,           // [batch_size, num_key]\n-  at::Tensor key_hash_code,      // [batch_size, num_key, num_hash_f]\n-  at::Tensor value,              // [batch_size, num_key, value_dim]\n-  int hashtable_capacity,\n-  bool use_cuda,\n-  int version\n-) {\n-  return lsh_cumulation_ver1_kernel(\n-    query_mask,\n-    query_hash_code,\n-    key_mask,\n-    key_hash_code,\n-    value,\n-    hashtable_capacity,\n-    use_cuda\n-  );\n-}\n-\n-at::Tensor lsh_weighted_cumulation(\n-  at::Tensor query_mask,         // [batch_size, num_query]\n-  at::Tensor query_hash_code,    // [batch_size, num_query, num_hash_f]\n-  at::Tensor query_weight,       // [batch_size, num_query, weight_dim]\n-  at::Tensor key_mask,           // [batch_size, num_key]\n-  at::Tensor key_hash_code,      // [batch_size, num_key, num_hash_f]\n-  at::Tensor key_weight,         // [batch_size, num_key, weight_dim]\n-  at::Tensor value,              // [batch_size, num_key, value_dim]\n-  int hashtable_capacity,\n-  bool use_cuda,\n-  int version\n-) {\n-  if (version == 1) {\n-    return lsh_weighted_cumulation_ver1_kernel(\n-      query_mask,\n-      query_hash_code,\n-      query_weight,\n-      key_mask,\n-      key_hash_code,\n-      key_weight,\n-      value,\n-      hashtable_capacity,\n-      use_cuda\n-    );\n-  } else if (version == 2) {\n-    return lsh_weighted_cumulation_ver2_kernel(\n-      query_mask,\n-      query_hash_code,\n-      query_weight,\n-      key_mask,\n-      key_hash_code,\n-      key_weight,\n-      value,\n-      hashtable_capacity,\n-      use_cuda\n-    );\n-  } else if (version == 3) {\n-    return lsh_weighted_cumulation_ver3_kernel(\n-      query_mask,\n-      query_hash_code,\n-      query_weight,\n-      key_mask,\n-      key_hash_code,\n-      key_weight,\n-      value,\n-      hashtable_capacity,\n-      use_cuda\n-    );\n-  } else if (version == 4) {\n-    return lsh_weighted_cumulation_ver4_kernel(\n-      query_mask,\n-      query_hash_code,\n-      query_weight,\n-      key_mask,\n-      key_hash_code,\n-      key_weight,\n-      value,\n-      hashtable_capacity,\n-      use_cuda\n-    );\n-  } else {\n-    return lsh_weighted_cumulation_ver3_kernel(\n-      query_mask,\n-      query_hash_code,\n-      query_weight,\n-      key_mask,\n-      key_hash_code,\n-      key_weight,\n-      value,\n-      hashtable_capacity,\n-      use_cuda\n-    );\n-  }\n-}\n-\n-PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n-  m.def(\"fast_hash\", &fast_hash, \"Fast Hash (CUDA)\");\n-  m.def(\"lsh_cumulation\", &lsh_cumulation, \"LSH Cumulation (CUDA)\");\n-  m.def(\"lsh_weighted_cumulation\", &lsh_weighted_cumulation, \"LSH Weighted Cumulation (CUDA)\");\n-}"
      },
      {
        "filename": "src/transformers/models/yoso/modeling_yoso.py",
        "status": "modified",
        "additions": 6,
        "deletions": 11,
        "changes": 17,
        "patch": "@@ -15,7 +15,6 @@\n \"\"\"PyTorch YOSO model.\"\"\"\n \n import math\n-from pathlib import Path\n from typing import Optional, Union\n \n import torch\n@@ -36,6 +35,7 @@\n from ...pytorch_utils import apply_chunking_to_forward\n from ...utils import (\n     auto_docstring,\n+    is_kernels_available,\n     is_ninja_available,\n     is_torch_cuda_available,\n     logging,\n@@ -51,17 +51,12 @@\n \n def load_cuda_kernels():\n     global lsh_cumulation\n-    from torch.utils.cpp_extension import load\n+    if not is_kernels_available():\n+        raise ImportError(\"kernels is not installed, please install it with `pip install kernels`\")\n+    from kernels import get_kernel\n \n-    def append_root(files):\n-        src_folder = Path(__file__).resolve().parent.parent.parent / \"kernels\" / \"yoso\"\n-        return [src_folder / file for file in files]\n-\n-    src_files = append_root([\"fast_lsh_cumulation_torch.cpp\", \"fast_lsh_cumulation.cu\", \"fast_lsh_cumulation_cuda.cu\"])\n-\n-    load(\"fast_lsh_cumulation\", src_files, verbose=True)\n-\n-    import fast_lsh_cumulation as lsh_cumulation\n+    yoso = get_kernel(\"kernels-community/yoso\")\n+    lsh_cumulation = yoso.lsh_cumulation\n \n \n def to_contiguous(input_tensors):"
      }
    ],
    "num_files": 9,
    "scraped_at": "2025-11-16T21:17:51.771600"
  },
  {
    "pr_number": 41493,
    "title": "[kernels] Remove RWKV kernel finally !",
    "body": "# What does this PR do?\r\n\r\nCleans the rwkv kernel, after adding the kernel to `kernels-community` : https://huggingface.co/kernels-community/rwkv",
    "html_url": "https://github.com/huggingface/transformers/pull/41493",
    "created_at": "2025-10-09T22:19:42Z",
    "merged_at": "2025-10-10T08:32:05Z",
    "merge_commit_sha": "b543679d0ec057cb51a1d0be7b86df0e78556763",
    "base_ref": "main",
    "head_sha": "8791d4e62c7d7330952e59e32e4857ff30f7897d",
    "user": "MekkCyber",
    "files": [
      {
        "filename": "src/transformers/kernels/rwkv/wkv_cuda.cu",
        "status": "removed",
        "additions": 0,
        "deletions": 187,
        "changes": 187,
        "patch": "@@ -1,187 +0,0 @@\n-#include <stdio.h>\n-#include <assert.h>\n-\n-#define MIN_VALUE (-1e38)\n-\n-template <typename F>\n-__global__ void kernel_forward(\n-    const int B, const int T, const int C, const F *__restrict__ const _w, const F *__restrict__ const _u,\n-    const F *__restrict__ const _k, const F *__restrict__ const _v, F *__restrict__ const _y\n-) {\n-    const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n-    const int _b = idx / C;\n-    const int _c = idx % C;\n-    const int _offset = _b * T * C + _c;\n-\n-    F u = _u[_c];\n-    F w = _w[_c];\n-    const F *__restrict__ const k = _k + _offset;\n-    const F *__restrict__ const v = _v + _offset;\n-    F *__restrict__ const y = _y + _offset;\n-\n-    // aa and bb are running sums divided by exp(pp) (to avoid overflow)\n-    F aa = 0, bb = 0, pp = MIN_VALUE;\n-    for (int i = 0; i < T; i++) {\n-        const int ii = i * C;\n-        const F kk = k[ii];\n-        const F vv = v[ii];\n-\n-        F ww = u + kk;\n-        F p = max(pp, ww);\n-        F e1 = exp(pp - p);\n-        F e2 = exp(ww - p);\n-        y[ii] = (e1 * aa + e2 * vv) / (e1 * bb + e2);\n-        \n-        ww = w + pp;\n-        p = max(ww, kk);\n-        e1 = exp(ww - p);\n-        e2 = exp(kk - p);\n-        aa = e1 * aa + e2 * vv;\n-        bb = e1 * bb + e2;\n-        pp = p;\n-    }\n-}\n-\n-template <typename F>\n-__global__ void kernel_forward_with_state(\n-    const int B, const int T, const int C, const F *__restrict__ const _w, const F *__restrict__ const _u,\n-    const F *__restrict__ const _k, const F *__restrict__ const _v, F *__restrict__ const _y, F *__restrict__ const _s\n-) {\n-    const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n-    const int _b = idx / C;\n-    const int _c = idx % C;\n-    const int _offset_s = _b * C * 3 + _c * 3;\n-    const int _offset = _b * T * C + _c;\n-\n-    F u = _u[_c];\n-    F w = _w[_c];\n-    const F *__restrict__ const k = _k + _offset;\n-    const F *__restrict__ const v = _v + _offset;\n-    F *__restrict__ const y = _y + _offset;\n-    F *__restrict__ const s = _s + _offset_s;\n-\n-    // aa and bb are running sums divided by exp(pp) (to avoid overflow)\n-    F aa = s[0], bb = s[1], pp = s[2];\n-    for (int i = 0; i < T; i++) {\n-        const int ii = i * C;\n-        const F kk = k[ii];\n-        const F vv = v[ii];\n-\n-        F ww = u + kk;\n-        F p = max(pp, ww);\n-        F e1 = exp(pp - p);\n-        F e2 = exp(ww - p);\n-        y[ii] = (e1 * aa + e2 * vv) / (e1 * bb + e2);\n-        \n-        ww = w + pp;\n-        p = max(ww, kk);\n-        e1 = exp(ww - p);\n-        e2 = exp(kk - p);\n-        aa = e1 * aa + e2 * vv;\n-        bb = e1 * bb + e2;\n-        pp = p;\n-    }\n-    s[0] = aa;\n-    s[1] = bb;\n-    s[2] = pp;\n-}\n-\n-template <typename F>\n-__global__ void kernel_backward(\n-    const int B, const int T, const int C, const F *__restrict__ const _w, const F *__restrict__ const _u,\n-    const F *__restrict__ const _k, const F *__restrict__ const _v, const F *__restrict__ const _y,\n-    const F *__restrict__ const _gy, F *__restrict__ const _gw, F *__restrict__ const _gu, F *__restrict__ const _gk,\n-    F *__restrict__ const _gv\n-) {\n-    const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n-    const int _b = idx / C;\n-    const int _c = idx % C;\n-    const int _offset = _b * T * C + _c;\n-\n-    F u = _u[_c];\n-    F w = _w[_c];\n-    const F *__restrict__ const k = _k + _offset;\n-    const F *__restrict__ const v = _v + _offset;\n-    const F *__restrict__ const y = _y + _offset;\n-    const F *__restrict__ const gy = _gy + _offset;\n-    F *__restrict__ const gk = _gk + _offset;\n-    F *__restrict__ const gv = _gv + _offset;\n-\n-    F q[Tmax], r[Tmax];\n-\n-    F gw = 0, gu = 0, aa = 0, bb = 0, ga = 0, gb = 0, pp = MIN_VALUE;\n-    for (int i = 0; i < T; i++) {\n-        const int ii = i * C;\n-        const F kk = k[ii];\n-        const F vv = v[ii];\n-        const F yy = y[ii];\n-\n-        F ww = u + kk;\n-        F p = max(pp, ww);\n-        F e1 = exp(pp - p);\n-        F e2 = exp(ww - p);\n-        const F qq = gy[ii] / (e1 * bb + e2);\n-        gw += (ga - gb * yy) * e1 * qq;\n-        gu += (vv - yy) * e2 * qq;\n-        q[i] = qq;\n-        r[i] = ww - p;\n-\n-        ww = w + pp;\n-        p = max(ww, kk);\n-        e1 = exp(ww - p);\n-        e2 = exp(kk - p);\n-        ga = e1 * (aa + ga);\n-        gb = e1 * (bb + gb);\n-        aa = e1 * aa + e2 * vv;\n-        bb = e1 * bb + e2;\n-        pp = p;\n-    }\n-    const int _offsetBC = _b * C + _c;\n-    _gw[_offsetBC] = gw * _w[_c]; // multiply by w because of w -> -exp(w) in python forward()\n-    _gu[_offsetBC] = gu;\n-\n-    aa = 0, bb = 0, pp = MIN_VALUE;\n-    for (int i = T - 1; i >= 0; i--) {\n-        const int ii = i * C;\n-        const F kk = k[ii];\n-        const F vv = v[ii];\n-        const F yy = y[ii];\n-        const F qq = q[i];\n-        const F rr = r[i];\n-\n-        F e1 = qq * exp(rr);\n-        F e2 = exp(kk + pp);\n-        gk[ii] = e1 * (vv - yy) + e2 * (aa * vv + bb);\n-        gv[ii] = e1 + e2 * aa;\n-\n-        const F ww = w + pp;\n-        const F www = rr - u - kk;\n-        const F p = max(ww, www);\n-        e1 = exp(ww - p);\n-        e2 = qq * exp(www - p);\n-        aa = e1 * aa + e2;\n-        bb = e1 * bb - e2 * yy;\n-        pp = p;\n-    }\n-}\n-\n-void cuda_forward(int B, int T, int C, float *w, float *u, float *k, float *v, float *y) {\n-    dim3 threadsPerBlock( min(C, 32) ); // requires --maxrregcount 60 for optimal performance\n-    assert(B * C % threadsPerBlock.x == 0);\n-    dim3 numBlocks(B * C / threadsPerBlock.x);\n-    kernel_forward<<<numBlocks, threadsPerBlock>>>(B, T, C, w, u, k, v, y);\n-}\n-\n-void cuda_forward_with_state(int B, int T, int C, float *w, float *u, float *k, float *v, float *y, float *s) {\n-    dim3 threadsPerBlock( min(C, 32) ); // requires --maxrregcount 60 for optimal performance\n-    assert(B * C % threadsPerBlock.x == 0);\n-    dim3 numBlocks(B * C / threadsPerBlock.x);\n-    kernel_forward_with_state<<<numBlocks, threadsPerBlock>>>(B, T, C, w, u, k, v, y, s);\n-}\n-\n-void cuda_backward(int B, int T, int C, float *w, float *u, float *k, float *v, float *y, float *gy, float *gw, float *gu, float *gk, float *gv) {\n-    dim3 threadsPerBlock( min(C, 32) ); // requires --maxrregcount 60 for optimal performance\n-    assert(B * C % threadsPerBlock.x == 0);\n-    dim3 numBlocks(B * C / threadsPerBlock.x);\n-    kernel_backward<<<numBlocks, threadsPerBlock>>>(B, T, C, w, u, k, v, y, gy, gw, gu, gk, gv);\n-}"
      },
      {
        "filename": "src/transformers/kernels/rwkv/wkv_cuda_bf16.cu",
        "status": "removed",
        "additions": 0,
        "deletions": 186,
        "changes": 186,
        "patch": "@@ -1,186 +0,0 @@\n-#include <stdio.h>\n-#include <assert.h>\n-#include \"ATen/ATen.h\"\n-#define MIN_VALUE (-1e38)\n-typedef at::BFloat16 bf16;\n-\n-__global__ void kernel_forward_bf16(\n-    const int B, const int T, const int C, const float *__restrict__ const _w, const bf16 *__restrict__ const _u,\n-    const bf16 *__restrict__ const _k, const bf16 *__restrict__ const _v, bf16 *__restrict__ const _y\n-) {\n-    const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n-    const int _b = idx / C;\n-    const int _c = idx % C;\n-    const int _offset = _b * T * C + _c;\n-\n-    float u = float(_u[_c]);\n-    float w = _w[_c];\n-    const bf16 *__restrict__ const k = _k + _offset;\n-    const bf16 *__restrict__ const v = _v + _offset;\n-    bf16 *__restrict__ const y = _y + _offset;\n-\n-    // aa and bb are running sums divided by exp(pp) (to avoid overflow)\n-    float aa = 0, bb = 0, pp = MIN_VALUE;\n-    for (int i = 0; i < T; i++) {\n-        const int ii = i * C;\n-        const float kk = float(k[ii]);\n-        const float vv = float(v[ii]);\n-\n-        float ww = u + kk;\n-        float p = max(pp, ww);\n-        float e1 = exp(pp - p);\n-        float e2 = exp(ww - p);\n-        y[ii] = bf16((e1 * aa + e2 * vv) / (e1 * bb + e2));\n-        \n-        ww = w + pp;\n-        p = max(ww, kk);\n-        e1 = exp(ww - p);\n-        e2 = exp(kk - p);\n-        aa = e1 * aa + e2 * vv;\n-        bb = e1 * bb + e2;\n-        pp = p;\n-    }\n-}\n-\n-__global__ void kernel_forward_with_state_bf16(\n-    const int B, const int T, const int C, const float *__restrict__ const _w, const bf16 *__restrict__ const _u,\n-    const bf16 *__restrict__ const _k, const bf16 *__restrict__ const _v, bf16 *__restrict__ const _y,\n-    float *__restrict__ const _s\n-) {\n-    const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n-    const int _b = idx / C;\n-    const int _c = idx % C;\n-    const int _offset_s = _b * C * 3 + _c * 3;\n-    const int _offset = _b * T * C + _c;\n-\n-    float u = float(_u[_c]);\n-    float w = _w[_c];\n-    const bf16 *__restrict__ const k = _k + _offset;\n-    const bf16 *__restrict__ const v = _v + _offset;\n-    bf16 *__restrict__ const y = _y + _offset;\n-    float *__restrict__ const s = _s + _offset_s;\n-\n-    // aa and bb are running sums divided by exp(pp) (to avoid overflow)\n-    float aa = s[0], bb = s[1], pp = s[2];\n-    for (int i = 0; i < T; i++) {\n-        const int ii = i * C;\n-        const float kk = float(k[ii]);\n-        const float vv = float(v[ii]);\n-\n-        float ww = u + kk;\n-        float p = max(pp, ww);\n-        float e1 = exp(pp - p);\n-        float e2 = exp(ww - p);\n-        y[ii] = bf16(e1 * aa + e2 * vv) / (e1 * bb + e2);\n-        \n-        ww = w + pp;\n-        p = max(ww, kk);\n-        e1 = exp(ww - p);\n-        e2 = exp(kk - p);\n-        aa = e1 * aa + e2 * vv;\n-        bb = e1 * bb + e2;\n-        pp = p;\n-    }\n-    s[0] = aa;\n-    s[1] = bb;\n-    s[2] = pp;\n-}\n-\n-__global__ void kernel_backward_bf16(\n-    const int B, const int T, const int C, const float *__restrict__ const _w, const bf16 *__restrict__ const _u,\n-    const bf16 *__restrict__ const _k, const bf16 *__restrict__ const _v, const bf16 *__restrict__ const _y,\n-    const bf16 *__restrict__ const _gy, bf16 *__restrict__ const _gw, bf16 *__restrict__ const _gu,\n-    bf16 *__restrict__ const _gk, bf16 *__restrict__ const _gv\n-) {\n-    const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n-    const int _b = idx / C;\n-    const int _c = idx % C;\n-    const int _offset = _b * T * C + _c;\n-\n-    float u = float(_u[_c]);\n-    float w = _w[_c];\n-    const bf16 *__restrict__ const k = _k + _offset;\n-    const bf16 *__restrict__ const v = _v + _offset;\n-    const bf16 *__restrict__ const y = _y + _offset;\n-    const bf16 *__restrict__ const gy = _gy + _offset;\n-    bf16 *__restrict__ const gk = _gk + _offset;\n-    bf16 *__restrict__ const gv = _gv + _offset;\n-\n-    float q[Tmax], r[Tmax];\n-\n-    float gw = 0, gu = 0, aa = 0, bb = 0, ga = 0, gb = 0, pp = MIN_VALUE;\n-    for (int i = 0; i < T; i++) {\n-        const int ii = i * C;\n-        const float kk = float(k[ii]);\n-        const float vv = float(v[ii]);\n-        const float yy = float(y[ii]);\n-\n-        float ww = u + kk;\n-        float p = max(pp, ww);\n-        float e1 = exp(pp - p);\n-        float e2 = exp(ww - p);\n-        const float qq = float(gy[ii]) / (e1 * bb + e2);\n-        gw += (ga - gb * yy) * e1 * qq;\n-        gu += (vv - yy) * e2 * qq;\n-        q[i] = qq;\n-        r[i] = ww - p;\n-\n-        ww = w + pp;\n-        p = max(ww, kk);\n-        e1 = exp(ww - p);\n-        e2 = exp(kk - p);\n-        ga = e1 * (aa + ga);\n-        gb = e1 * (bb + gb);\n-        aa = e1 * aa + e2 * vv;\n-        bb = e1 * bb + e2;\n-        pp = p;\n-    }\n-    const int _offsetBC = _b * C + _c;\n-    _gw[_offsetBC] = bf16(gw * _w[_c]); // multiply by w because of w -> -exp(w) in python forward()\n-    _gu[_offsetBC] = bf16(gu);\n-\n-    aa = 0, bb = 0, pp = MIN_VALUE;\n-    for (int i = T - 1; i >= 0; i--) {\n-        const int ii = i * C;\n-        const float kk = float(k[ii]);\n-        const float vv = float(v[ii]);\n-        const float yy = float(y[ii]);\n-        const float qq = q[i];\n-        const float rr = r[i];\n-\n-        float e1 = qq * exp(rr);\n-        float e2 = exp(kk + pp);\n-        gk[ii] = bf16(e1 * (vv - yy) + e2 * (aa * vv + bb));\n-        gv[ii] = bf16(e1 + e2 * aa);\n-\n-        const float ww = w + pp;\n-        const float www = rr - u - kk;\n-        const float p = max(ww, www);\n-        e1 = exp(ww - p);\n-        e2 = qq * exp(www - p);\n-        aa = e1 * aa + e2;\n-        bb = e1 * bb - e2 * yy;\n-        pp = p;\n-    }\n-}\n-\n-void cuda_forward_bf16(int B, int T, int C, float *w, bf16 *u, bf16 *k, bf16 *v, bf16 *y) {\n-    dim3 threadsPerBlock( min(C, 32) ); // requires --maxrregcount 60 for optimal performance\n-    assert(B * C % threadsPerBlock.x == 0);\n-    dim3 numBlocks(B * C / threadsPerBlock.x);\n-    kernel_forward_bf16<<<numBlocks, threadsPerBlock>>>(B, T, C, w, u, k, v, y);\n-}\n-\n-void cuda_forward_with_state_bf16(int B, int T, int C, float *w, bf16 *u, bf16 *k, bf16 *v, bf16 *y, float *s) {\n-    dim3 threadsPerBlock( min(C, 32) ); // requires --maxrregcount 60 for optimal performance\n-    assert(B * C % threadsPerBlock.x == 0);\n-    dim3 numBlocks(B * C / threadsPerBlock.x);\n-    kernel_forward_with_state_bf16<<<numBlocks, threadsPerBlock>>>(B, T, C, w, u, k, v, y, s);\n-}\n-\n-void cuda_backward_bf16(int B, int T, int C, float *w, bf16 *u, bf16 *k, bf16 *v, bf16 *y, bf16 *gy, bf16 *gw, bf16 *gu, bf16 *gk, bf16 *gv) {\n-    dim3 threadsPerBlock( min(C, 32) ); // requires --maxrregcount 60 for optimal performance\n-    assert(B * C % threadsPerBlock.x == 0);\n-    dim3 numBlocks(B * C / threadsPerBlock.x);\n-    kernel_backward_bf16<<<numBlocks, threadsPerBlock>>>(B, T, C, w, u, k, v, y, gy, gw, gu, gk, gv);\n-}"
      },
      {
        "filename": "src/transformers/kernels/rwkv/wkv_op.cpp",
        "status": "removed",
        "additions": 0,
        "deletions": 66,
        "changes": 66,
        "patch": "@@ -1,66 +0,0 @@\n-#include <torch/extension.h>\n-#include \"ATen/ATen.h\"\n-typedef at::BFloat16 bf16;\n-\n-void cuda_forward(int B, int T, int C, float *w, float *u, float *k, float *v, float *y);\n-void cuda_forward_bf16(int B, int T, int C, float *w, bf16 *u, bf16 *k, bf16 *v, bf16 *y);\n-void cuda_forward_with_state(int B, int T, int C, float *w, float *u, float *k, float *v, float *y, float *s);\n-void cuda_forward_with_state_bf16(int B, int T, int C, float *w, bf16 *u, bf16 *k, bf16 *v, bf16 *y, float *s);\n-void cuda_backward(int B, int T, int C, float *w, float *u, float *k, float *v, float *y, float *gy, float *gw, float *gu, float *gk, float *gv);\n-void cuda_backward_bf16(int B, int T, int C, float *w, bf16 *u, bf16 *k, bf16 *v, bf16 *y, bf16 *gy, bf16 *gw, bf16 *gu, bf16 *gk, bf16 *gv);\n-\n-void forward(torch::Tensor &w, torch::Tensor &u, torch::Tensor &k, torch::Tensor &v, torch::Tensor &y) {\n-    const int B = k.size(0);\n-    const int T = k.size(1);\n-    const int C = k.size(2);\n-    cuda_forward(B, T, C, w.data_ptr<float>(), u.data_ptr<float>(), k.data_ptr<float>(), v.data_ptr<float>(), y.data_ptr<float>());\n-}\n-void forward_bf16(torch::Tensor &w, torch::Tensor &u, torch::Tensor &k, torch::Tensor &v, torch::Tensor &y) {\n-    const int B = k.size(0);\n-    const int T = k.size(1);\n-    const int C = k.size(2);\n-    cuda_forward_bf16(B, T, C, w.data_ptr<float>(), u.data_ptr<bf16>(), k.data_ptr<bf16>(), v.data_ptr<bf16>(), y.data_ptr<bf16>());\n-}\n-void forward_with_state(torch::Tensor &w, torch::Tensor &u, torch::Tensor &k, torch::Tensor &v, torch::Tensor &y, torch::Tensor &s) {\n-    const int B = k.size(0);\n-    const int T = k.size(1);\n-    const int C = k.size(2);\n-    cuda_forward_with_state(B, T, C, w.data_ptr<float>(), u.data_ptr<float>(), k.data_ptr<float>(), v.data_ptr<float>(), y.data_ptr<float>(), s.data_ptr<float>());\n-}\n-void forward_with_state_bf16(torch::Tensor &w, torch::Tensor &u, torch::Tensor &k, torch::Tensor &v, torch::Tensor &y, torch::Tensor &s) {\n-    const int B = k.size(0);\n-    const int T = k.size(1);\n-    const int C = k.size(2);\n-    cuda_forward_with_state_bf16(B, T, C, w.data_ptr<float>(), u.data_ptr<bf16>(), k.data_ptr<bf16>(), v.data_ptr<bf16>(), y.data_ptr<bf16>(), s.data_ptr<float>());\n-}\n-void backward(torch::Tensor &w, torch::Tensor &u, torch::Tensor &k, torch::Tensor &v, torch::Tensor &y, torch::Tensor &gy, torch::Tensor &gw, torch::Tensor &gu, torch::Tensor &gk, torch::Tensor &gv) {\n-    const int B = k.size(0);\n-    const int T = k.size(1);\n-    const int C = k.size(2);\n-    cuda_backward(B, T, C, w.data_ptr<float>(), u.data_ptr<float>(), k.data_ptr<float>(), v.data_ptr<float>(), y.data_ptr<float>(), gy.data_ptr<float>(), gw.data_ptr<float>(), gu.data_ptr<float>(), gk.data_ptr<float>(), gv.data_ptr<float>());\n-}\n-void backward_bf16(torch::Tensor &w, torch::Tensor &u, torch::Tensor &k, torch::Tensor &v, torch::Tensor &y, torch::Tensor &gy, torch::Tensor &gw, torch::Tensor &gu, torch::Tensor &gk, torch::Tensor &gv) {\n-    const int B = k.size(0);\n-    const int T = k.size(1);\n-    const int C = k.size(2);\n-    cuda_backward_bf16(B, T, C, w.data_ptr<float>(), u.data_ptr<bf16>(), k.data_ptr<bf16>(), v.data_ptr<bf16>(), y.data_ptr<bf16>(),\n-        gy.data_ptr<bf16>(), gw.data_ptr<bf16>(), gu.data_ptr<bf16>(), gk.data_ptr<bf16>(), gv.data_ptr<bf16>());\n-}\n-\n-PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n-    m.def(\"forward\", &forward, \"wkv forward\");\n-    m.def(\"forward_bf16\", &forward_bf16, \"wkv forward bf16\");\n-    m.def(\"forward_with_state\", &forward_with_state, \"wkv forward with state\");\n-    m.def(\"forward_with_state_bf16\", &forward_with_state_bf16, \"wkv forward with state bf16\");\n-    m.def(\"backward\", &backward, \"wkv backward\");\n-    m.def(\"backward_bf16\", &backward_bf16, \"wkv backward bf16\");\n-}\n-\n-TORCH_LIBRARY(wkv, m) {\n-    m.def(\"forward\", forward);\n-    m.def(\"forward_bf16\", forward_bf16);\n-    m.def(\"forward_with_state\", forward_with_state);\n-    m.def(\"forward_with_state_bf16\", forward_with_state_bf16);\n-    m.def(\"backward\", backward);\n-    m.def(\"backward_bf16\", backward_bf16);\n-}"
      },
      {
        "filename": "src/transformers/models/rwkv/modeling_rwkv.py",
        "status": "modified",
        "additions": 6,
        "deletions": 27,
        "changes": 33,
        "patch": "@@ -17,7 +17,6 @@\n \n import math\n from dataclasses import dataclass\n-from pathlib import Path\n from typing import Optional, Union\n \n import torch\n@@ -30,6 +29,7 @@\n     ModelOutput,\n     auto_docstring,\n     is_bitsandbytes_available,\n+    is_kernels_available,\n     is_ninja_available,\n     is_torch_cuda_available,\n     logging,\n@@ -44,34 +44,13 @@\n \n \n def load_wkv_cuda_kernel(context_length):\n-    from torch.utils.cpp_extension import load as load_kernel\n-\n     global rwkv_cuda_kernel\n+    if not is_kernels_available():\n+        raise ImportError(\"kernels is not installed, please install it with `pip install kernels`\")\n+\n+    from kernels import get_kernel\n \n-    kernel_folder = Path(__file__).resolve().parent.parent.parent / \"kernels\" / \"rwkv\"\n-    cuda_kernel_files = [kernel_folder / f for f in [\"wkv_op.cpp\", \"wkv_cuda.cu\", \"wkv_cuda_bf16.cu\"]]\n-\n-    # Only load the kernel if it's not been loaded yet or if we changed the context length\n-    if rwkv_cuda_kernel is not None and rwkv_cuda_kernel.max_seq_length == context_length:\n-        return\n-\n-    logger.info(f\"Loading CUDA kernel for RWKV at context length of {context_length}.\")\n-\n-    flags = [\n-        \"-res-usage\",\n-        \"--maxrregcount 60\",\n-        \"--use_fast_math\",\n-        \"-O3\",\n-        \"-Xptxas -O3\",\n-        \"--extra-device-vectorization\",\n-        f\"-DTmax={context_length}\",\n-    ]\n-    rwkv_cuda_kernel = load_kernel(\n-        name=f\"wkv_{context_length}\",\n-        sources=cuda_kernel_files,\n-        verbose=(logging.get_verbosity() == logging.DEBUG),\n-        extra_cuda_cflags=flags,\n-    )\n+    rwkv_cuda_kernel = get_kernel(\"kernels-community/rwkv\")\n     rwkv_cuda_kernel.max_seq_length = context_length\n \n "
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T21:17:52.075473"
  },
  {
    "pr_number": 41476,
    "title": "Pickle - part 2",
    "body": "# What does this PR do?\r\n\r\nRequire \r\n```\r\n    if not strtobool(os.environ.get(\"TRUST_REMOTE_CODE\", \"False\")):\r\n        raise ValueError(\r\n            \"This part uses `pickle.load` which is insecure and will execute arbitrary code that is potentially \"\r\n            \"malicious. It's recommended to never unpickle data that could have come from an untrusted source, or \"\r\n            \"that could have been tampered with. If you already verified the pickle data and decided to use it, \"\r\n            \"you can set the environment variable `TRUST_REMOTE_CODE` to `True` to allow it.\"\r\n        )\r\n```\r\n\r\nI limit this PR to conversion scripts only. For other usage of pickle in our codebase, I will try to see if there is any alternative.\r\n",
    "html_url": "https://github.com/huggingface/transformers/pull/41476",
    "created_at": "2025-10-09T13:16:11Z",
    "merged_at": "2025-10-09T13:46:54Z",
    "merge_commit_sha": "9ef804472b25c4f69c1eb213dea6f791615538a0",
    "base_ref": "main",
    "head_sha": "67f8f0e7d16a7c297a1cbdeefb62ab19e11cd940",
    "user": "ydshieh",
    "files": [
      {
        "filename": "src/transformers/models/deprecated/mega/convert_mega_original_pytorch_checkpoint_to_pytorch.py",
        "status": "modified",
        "additions": 11,
        "deletions": 2,
        "changes": 13,
        "patch": "@@ -29,14 +29,16 @@\n \n # utilities to import the model weights and config file\n import os\n-import pickle as pkl\n+import pickle\n \n # PyTorch + new model classes\n import torch\n from torch import nn\n \n from transformers import AutoTokenizer, MegaConfig, MegaForMaskedLM\n \n+from ....utils import strtobool\n+\n \n # import the EncoderLayer class used to pretrain\n # !! NOTE !! this requires the version of fairseq that is built when you install the Mega source\n@@ -122,8 +124,15 @@ def forward(self, input_ids, attention_mask, batch_first=True, ignore_mask_value\n \n # code to convert the checkpoint located in the user-specified location\n def convert_checkpoint_to_huggingface(pretrained_checkpoint_path, output_path, includes_tokenizer):\n+    if not strtobool(os.environ.get(\"TRUST_REMOTE_CODE\", \"False\")):\n+        raise ValueError(\n+            \"This part uses `pickle.load` which is insecure and will execute arbitrary code that is potentially \"\n+            \"malicious. It's recommended to never unpickle data that could have come from an untrusted source, or \"\n+            \"that could have been tampered with. If you already verified the pickle data and decided to use it, \"\n+            \"you can set the environment variable `TRUST_REMOTE_CODE` to `True` to allow it.\"\n+        )\n     with open(os.path.join(pretrained_checkpoint_path, \"model_args.pkl\"), \"rb\") as f:\n-        mega_original_args = pkl.load(f)\n+        mega_original_args = pickle.load(f)\n \n     # load the original encoder\n     original_mlm = OriginalMegaForMaskedLM(**mega_original_args).eval()"
      },
      {
        "filename": "src/transformers/models/glm4v/convert_glm4v_mgt_weights_to_hf.py",
        "status": "modified",
        "additions": 10,
        "deletions": 0,
        "changes": 10,
        "patch": "@@ -24,6 +24,8 @@\n import torch\n from safetensors.torch import save_file\n \n+from ...utils import strtobool\n+\n \n # Avoid Using Megatron Lib\n class UnpicklerWrapper(pickle.Unpickler):\n@@ -248,6 +250,14 @@ def save_sharded_model(state_dict, output_path, max_shard_size_gb=5, num_layers=\n \n \n def merge_tp_weights(model_path, output_path, vllm_config_path=None):\n+    if not strtobool(os.environ.get(\"TRUST_REMOTE_CODE\", \"False\")):\n+        raise ValueError(\n+            \"This part uses `pickle.load` which is insecure and will execute arbitrary code that is potentially \"\n+            \"malicious. It's recommended to never unpickle data that could have come from an untrusted source, or \"\n+            \"that could have been tampered with. If you already verified the pickle data and decided to use it, \"\n+            \"you can set the environment variable `TRUST_REMOTE_CODE` to `True` to allow it.\"\n+        )\n+\n     tp_size = 0\n     for item in Path(model_path).iterdir():\n         if item.is_dir():"
      },
      {
        "filename": "src/transformers/models/maskformer/convert_maskformer_resnet_to_pytorch.py",
        "status": "modified",
        "additions": 10,
        "deletions": 0,
        "changes": 10,
        "patch": "@@ -17,6 +17,7 @@\n \n import argparse\n import json\n+import os\n import pickle\n from pathlib import Path\n \n@@ -28,6 +29,8 @@\n from transformers import MaskFormerConfig, MaskFormerForInstanceSegmentation, MaskFormerImageProcessor, ResNetConfig\n from transformers.utils import logging\n \n+from ...utils import strtobool\n+\n \n logging.set_verbosity_info()\n logger = logging.get_logger(__name__)\n@@ -266,6 +269,13 @@ def convert_maskformer_checkpoint(\n     \"\"\"\n     config = get_maskformer_config(model_name)\n \n+    if not strtobool(os.environ.get(\"TRUST_REMOTE_CODE\", \"False\")):\n+        raise ValueError(\n+            \"This part uses `pickle.load` which is insecure and will execute arbitrary code that is potentially \"\n+            \"malicious. It's recommended to never unpickle data that could have come from an untrusted source, or \"\n+            \"that could have been tampered with. If you already verified the pickle data and decided to use it, \"\n+            \"you can set the environment variable `TRUST_REMOTE_CODE` to `True` to allow it.\"\n+        )\n     # load original state_dict\n     with open(checkpoint_path, \"rb\") as f:\n         data = pickle.load(f)"
      },
      {
        "filename": "src/transformers/models/maskformer/convert_maskformer_swin_to_pytorch.py",
        "status": "modified",
        "additions": 10,
        "deletions": 0,
        "changes": 10,
        "patch": "@@ -17,6 +17,7 @@\n \n import argparse\n import json\n+import os\n import pickle\n from pathlib import Path\n \n@@ -28,6 +29,8 @@\n from transformers import MaskFormerConfig, MaskFormerForInstanceSegmentation, MaskFormerImageProcessor, SwinConfig\n from transformers.utils import logging\n \n+from ...utils import strtobool\n+\n \n logging.set_verbosity_info()\n logger = logging.get_logger(__name__)\n@@ -235,6 +238,13 @@ def convert_maskformer_checkpoint(\n     \"\"\"\n     config = get_maskformer_config(model_name)\n \n+    if not strtobool(os.environ.get(\"TRUST_REMOTE_CODE\", \"False\")):\n+        raise ValueError(\n+            \"This part uses `pickle.load` which is insecure and will execute arbitrary code that is potentially \"\n+            \"malicious. It's recommended to never unpickle data that could have come from an untrusted source, or \"\n+            \"that could have been tampered with. If you already verified the pickle data and decided to use it, \"\n+            \"you can set the environment variable `TRUST_REMOTE_CODE` to `True` to allow it.\"\n+        )\n     # load original state_dict\n     with open(checkpoint_path, \"rb\") as f:\n         data = pickle.load(f)"
      },
      {
        "filename": "src/transformers/models/olmo3/convert_olmo3_weights_to_hf.py",
        "status": "modified",
        "additions": 16,
        "deletions": 0,
        "changes": 16,
        "patch": "@@ -39,6 +39,8 @@\n \n from transformers import AutoTokenizer, Olmo3Config, Olmo3ForCausalLM\n \n+from ...utils import strtobool\n+\n \n \"\"\"\n Sample usage:\n@@ -198,6 +200,13 @@ def read_data(self, plan: dist_cp.LoadPlan, planner: dist_cp.LoadPlanner) -> Fut\n     def read_metadata(self) -> Metadata:\n         if self._metadata is None:\n             try:\n+                if not strtobool(os.environ.get(\"TRUST_REMOTE_CODE\", \"False\")):\n+                    raise ValueError(\n+                        \"This part uses `pickle.load` which is insecure and will execute arbitrary code that is potentially \"\n+                        \"malicious. It's recommended to never unpickle data that could have come from an untrusted source, or \"\n+                        \"that could have been tampered with. If you already verified the pickle data and decided to use it, \"\n+                        \"you can set the environment variable `TRUST_REMOTE_CODE` to `True` to allow it.\"\n+                    )\n                 with (Path(self.path) / \".metadata\").open(\"rb\") as metadata_file:\n                     metadata = pickle.load(metadata_file)\n             except FileNotFoundError as exc:\n@@ -256,6 +265,13 @@ def _load_unsharded_keys(\n         )\n         return state_dict\n \n+    if not strtobool(os.environ.get(\"TRUST_REMOTE_CODE\", \"False\")):\n+        raise ValueError(\n+            \"This part uses `pickle.load` which is insecure and will execute arbitrary code that is potentially \"\n+            \"malicious. It's recommended to never unpickle data that could have come from an untrusted source, or \"\n+            \"that could have been tampered with. If you already verified the pickle data and decided to use it, \"\n+            \"you can set the environment variable `TRUST_REMOTE_CODE` to `True` to allow it.\"\n+        )\n     with (Path(model_path) / \".metadata\").open(\"rb\") as metadata_file:\n         metadata = pickle.load(metadata_file)\n         keys = [key for key in metadata.state_dict_metadata.keys() if key.startswith(\"model.\")]"
      },
      {
        "filename": "src/transformers/models/perceiver/convert_perceiver_haiku_to_pytorch.py",
        "status": "modified",
        "additions": 10,
        "deletions": 0,
        "changes": 10,
        "patch": "@@ -16,6 +16,7 @@\n \n import argparse\n import json\n+import os\n import pickle\n from pathlib import Path\n \n@@ -39,6 +40,8 @@\n )\n from transformers.utils import logging\n \n+from ...utils import strtobool\n+\n \n logging.set_verbosity_info()\n logger = logging.get_logger(__name__)\n@@ -264,6 +267,13 @@ def convert_perceiver_checkpoint(pickle_file, pytorch_dump_folder_path, architec\n     \"\"\"\n     Copy/paste/tweak model's weights to our Perceiver structure.\n     \"\"\"\n+    if not strtobool(os.environ.get(\"TRUST_REMOTE_CODE\", \"False\")):\n+        raise ValueError(\n+            \"This part uses `pickle.load` which is insecure and will execute arbitrary code that is potentially \"\n+            \"malicious. It's recommended to never unpickle data that could have come from an untrusted source, or \"\n+            \"that could have been tampered with. If you already verified the pickle data and decided to use it, \"\n+            \"you can set the environment variable `TRUST_REMOTE_CODE` to `True` to allow it.\"\n+        )\n \n     # load parameters as FlatMapping data structure\n     with open(pickle_file, \"rb\") as f:"
      },
      {
        "filename": "src/transformers/models/reformer/convert_reformer_trax_checkpoint_to_pytorch.py",
        "status": "modified",
        "additions": 10,
        "deletions": 0,
        "changes": 10,
        "patch": "@@ -15,6 +15,7 @@\n \"\"\"Convert Reformer checkpoint.\"\"\"\n \n import argparse\n+import os\n import pickle\n \n import numpy as np\n@@ -24,6 +25,8 @@\n from transformers import ReformerConfig, ReformerModelWithLMHead\n from transformers.utils import logging\n \n+from ...utils import strtobool\n+\n \n logging.set_verbosity_info()\n \n@@ -188,6 +191,13 @@ def convert_trax_checkpoint_to_pytorch(trax_model_pkl_path, config_file, pytorch\n     print(f\"Building PyTorch model from configuration: {config}\")\n     model = ReformerModelWithLMHead(config)\n \n+    if not strtobool(os.environ.get(\"TRUST_REMOTE_CODE\", \"False\")):\n+        raise ValueError(\n+            \"This part uses `pickle.load` which is insecure and will execute arbitrary code that is potentially \"\n+            \"malicious. It's recommended to never unpickle data that could have come from an untrusted source, or \"\n+            \"that could have been tampered with. If you already verified the pickle data and decided to use it, \"\n+            \"you can set the environment variable `TRUST_REMOTE_CODE` to `True` to allow it.\"\n+        )\n     with open(trax_model_pkl_path, \"rb\") as f:\n         model_weights = pickle.load(f)[\"weights\"]\n "
      }
    ],
    "num_files": 7,
    "scraped_at": "2025-11-16T21:17:55.325378"
  },
  {
    "pr_number": 41470,
    "title": "[kernels] Cleanup deta kernel",
    "body": "# What does this PR do?\r\n\r\nCleanup the `deta` kernel since it's the same as `deformable_detr` kernel cleaned here : https://github.com/huggingface/transformers/pull/36853\r\nAnd do the necessary modeling changes (even though the model is deprecated)",
    "html_url": "https://github.com/huggingface/transformers/pull/41470",
    "created_at": "2025-10-09T09:38:55Z",
    "merged_at": "2025-10-09T11:17:42Z",
    "merge_commit_sha": "927aa8bef2f29296a34840b3562f9c03cc45ef81",
    "base_ref": "main",
    "head_sha": "58d47f521610e06f00e246fa269b9e437edb0136",
    "user": "MekkCyber",
    "files": [
      {
        "filename": "src/transformers/kernels/deta/cpu/ms_deform_attn_cpu.cpp",
        "status": "removed",
        "additions": 0,
        "deletions": 40,
        "changes": 40,
        "patch": "@@ -1,40 +0,0 @@\n-/*!\n-**************************************************************************************************\n-* Deformable DETR\n-* Copyright (c) 2020 SenseTime. All Rights Reserved.\n-* Licensed under the Apache License, Version 2.0 [see LICENSE for details]\n-**************************************************************************************************\n-* Modified from https://github.com/chengdazhi/Deformable-Convolution-V2-PyTorch/tree/pytorch_1.0.0\n-**************************************************************************************************\n-*/\n-\n-#include <vector>\n-\n-#include <ATen/ATen.h>\n-#include <ATen/cuda/CUDAContext.h>\n-\n-\n-at::Tensor\n-ms_deform_attn_cpu_forward(\n-    const at::Tensor &value, \n-    const at::Tensor &spatial_shapes,\n-    const at::Tensor &level_start_index,\n-    const at::Tensor &sampling_loc,\n-    const at::Tensor &attn_weight,\n-    const int im2col_step)\n-{\n-    AT_ERROR(\"Not implement on cpu\");\n-}\n-\n-std::vector<at::Tensor>\n-ms_deform_attn_cpu_backward(\n-    const at::Tensor &value, \n-    const at::Tensor &spatial_shapes,\n-    const at::Tensor &level_start_index,\n-    const at::Tensor &sampling_loc,\n-    const at::Tensor &attn_weight,\n-    const at::Tensor &grad_output,\n-    const int im2col_step)\n-{\n-    AT_ERROR(\"Not implement on cpu\");\n-}"
      },
      {
        "filename": "src/transformers/kernels/deta/cpu/ms_deform_attn_cpu.h",
        "status": "removed",
        "additions": 0,
        "deletions": 32,
        "changes": 32,
        "patch": "@@ -1,32 +0,0 @@\n-/*!\n-**************************************************************************************************\n-* Deformable DETR\n-* Copyright (c) 2020 SenseTime. All Rights Reserved.\n-* Licensed under the Apache License, Version 2.0 [see LICENSE for details]\n-**************************************************************************************************\n-* Modified from https://github.com/chengdazhi/Deformable-Convolution-V2-PyTorch/tree/pytorch_1.0.0\n-**************************************************************************************************\n-*/\n-\n-#pragma once\n-#include <torch/extension.h>\n-\n-at::Tensor\n-ms_deform_attn_cpu_forward(\n-    const at::Tensor &value, \n-    const at::Tensor &spatial_shapes,\n-    const at::Tensor &level_start_index,\n-    const at::Tensor &sampling_loc,\n-    const at::Tensor &attn_weight,\n-    const int im2col_step);\n-\n-std::vector<at::Tensor>\n-ms_deform_attn_cpu_backward(\n-    const at::Tensor &value, \n-    const at::Tensor &spatial_shapes,\n-    const at::Tensor &level_start_index,\n-    const at::Tensor &sampling_loc,\n-    const at::Tensor &attn_weight,\n-    const at::Tensor &grad_output,\n-    const int im2col_step);\n-"
      },
      {
        "filename": "src/transformers/kernels/deta/cuda/ms_deform_attn_cuda.cu",
        "status": "removed",
        "additions": 0,
        "deletions": 156,
        "changes": 156,
        "patch": "@@ -1,156 +0,0 @@\n-/*!\n-**************************************************************************************************\n-* Deformable DETR\n-* Copyright (c) 2020 SenseTime. All Rights Reserved.\n-* Licensed under the Apache License, Version 2.0 [see LICENSE for details]\n-**************************************************************************************************\n-* Modified from https://github.com/chengdazhi/Deformable-Convolution-V2-PyTorch/tree/pytorch_1.0.0\n-**************************************************************************************************\n-*/\n-\n-#include <vector>\n-#include \"cuda/ms_deform_im2col_cuda.cuh\"\n-\n-#include <ATen/ATen.h>\n-#include <ATen/cuda/CUDAContext.h>\n-#include <cuda.h>\n-#include <cuda_runtime.h>\n-\n-#pragma once\n-#include <torch/extension.h>\n-\n-\n-at::Tensor ms_deform_attn_cuda_forward(\n-    const at::Tensor &value, \n-    const at::Tensor &spatial_shapes,\n-    const at::Tensor &level_start_index,\n-    const at::Tensor &sampling_loc,\n-    const at::Tensor &attn_weight,\n-    const int im2col_step)\n-{\n-    AT_ASSERTM(value.is_contiguous(), \"value tensor has to be contiguous\");\n-    AT_ASSERTM(spatial_shapes.is_contiguous(), \"spatial_shapes tensor has to be contiguous\");\n-    AT_ASSERTM(level_start_index.is_contiguous(), \"level_start_index tensor has to be contiguous\");\n-    AT_ASSERTM(sampling_loc.is_contiguous(), \"sampling_loc tensor has to be contiguous\");\n-    AT_ASSERTM(attn_weight.is_contiguous(), \"attn_weight tensor has to be contiguous\");\n-\n-    AT_ASSERTM(value.type().is_cuda(), \"value must be a CUDA tensor\");\n-    AT_ASSERTM(spatial_shapes.type().is_cuda(), \"spatial_shapes must be a CUDA tensor\");\n-    AT_ASSERTM(level_start_index.type().is_cuda(), \"level_start_index must be a CUDA tensor\");\n-    AT_ASSERTM(sampling_loc.type().is_cuda(), \"sampling_loc must be a CUDA tensor\");\n-    AT_ASSERTM(attn_weight.type().is_cuda(), \"attn_weight must be a CUDA tensor\");\n-\n-    const int batch = value.size(0);\n-    const int spatial_size = value.size(1);\n-    const int num_heads = value.size(2);\n-    const int channels = value.size(3);\n-\n-    const int num_levels = spatial_shapes.size(0);\n-\n-    const int num_query = sampling_loc.size(1);\n-    const int num_point = sampling_loc.size(4);\n-\n-    const int im2col_step_ = std::min(batch, im2col_step);\n-\n-    AT_ASSERTM(batch % im2col_step_ == 0, \"batch(%d) must divide im2col_step(%d)\", batch, im2col_step_);\n-    \n-    auto output = at::zeros({batch, num_query, num_heads, channels}, value.options());\n-\n-    const int batch_n = im2col_step_;\n-    auto output_n = output.view({batch/im2col_step_, batch_n, num_query, num_heads, channels});\n-    auto per_value_size = spatial_size * num_heads * channels;\n-    auto per_sample_loc_size = num_query * num_heads * num_levels * num_point * 2;\n-    auto per_attn_weight_size = num_query * num_heads * num_levels * num_point;\n-    for (int n = 0; n < batch/im2col_step_; ++n)\n-    {\n-        auto columns = output_n.select(0, n);\n-        AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n-            ms_deformable_im2col_cuda(at::cuda::getCurrentCUDAStream(),\n-                value.data<scalar_t>() + n * im2col_step_ * per_value_size,\n-                spatial_shapes.data<int64_t>(),\n-                level_start_index.data<int64_t>(),\n-                sampling_loc.data<scalar_t>() + n * im2col_step_ * per_sample_loc_size,\n-                attn_weight.data<scalar_t>() + n * im2col_step_ * per_attn_weight_size,\n-                batch_n, spatial_size, num_heads, channels, num_levels, num_query, num_point,\n-                columns.data<scalar_t>());\n-\n-        }));\n-    }\n-\n-    output = output.view({batch, num_query, num_heads*channels});\n-\n-    return output;\n-}\n-\n-\n-std::vector<at::Tensor> ms_deform_attn_cuda_backward(\n-    const at::Tensor &value, \n-    const at::Tensor &spatial_shapes,\n-    const at::Tensor &level_start_index,\n-    const at::Tensor &sampling_loc,\n-    const at::Tensor &attn_weight,\n-    const at::Tensor &grad_output,\n-    const int im2col_step)\n-{\n-\n-    AT_ASSERTM(value.is_contiguous(), \"value tensor has to be contiguous\");\n-    AT_ASSERTM(spatial_shapes.is_contiguous(), \"spatial_shapes tensor has to be contiguous\");\n-    AT_ASSERTM(level_start_index.is_contiguous(), \"level_start_index tensor has to be contiguous\");\n-    AT_ASSERTM(sampling_loc.is_contiguous(), \"sampling_loc tensor has to be contiguous\");\n-    AT_ASSERTM(attn_weight.is_contiguous(), \"attn_weight tensor has to be contiguous\");\n-    AT_ASSERTM(grad_output.is_contiguous(), \"grad_output tensor has to be contiguous\");\n-\n-    AT_ASSERTM(value.type().is_cuda(), \"value must be a CUDA tensor\");\n-    AT_ASSERTM(spatial_shapes.type().is_cuda(), \"spatial_shapes must be a CUDA tensor\");\n-    AT_ASSERTM(level_start_index.type().is_cuda(), \"level_start_index must be a CUDA tensor\");\n-    AT_ASSERTM(sampling_loc.type().is_cuda(), \"sampling_loc must be a CUDA tensor\");\n-    AT_ASSERTM(attn_weight.type().is_cuda(), \"attn_weight must be a CUDA tensor\");\n-    AT_ASSERTM(grad_output.type().is_cuda(), \"grad_output must be a CUDA tensor\");\n-\n-    const int batch = value.size(0);\n-    const int spatial_size = value.size(1);\n-    const int num_heads = value.size(2);\n-    const int channels = value.size(3);\n-\n-    const int num_levels = spatial_shapes.size(0);\n-\n-    const int num_query = sampling_loc.size(1);\n-    const int num_point = sampling_loc.size(4);\n-\n-    const int im2col_step_ = std::min(batch, im2col_step);\n-\n-    AT_ASSERTM(batch % im2col_step_ == 0, \"batch(%d) must divide im2col_step(%d)\", batch, im2col_step_);\n-\n-    auto grad_value = at::zeros_like(value);\n-    auto grad_sampling_loc = at::zeros_like(sampling_loc);\n-    auto grad_attn_weight = at::zeros_like(attn_weight);\n-\n-    const int batch_n = im2col_step_;\n-    auto per_value_size = spatial_size * num_heads * channels;\n-    auto per_sample_loc_size = num_query * num_heads * num_levels * num_point * 2;\n-    auto per_attn_weight_size = num_query * num_heads * num_levels * num_point;\n-    auto grad_output_n = grad_output.view({batch/im2col_step_, batch_n, num_query, num_heads, channels});\n-    \n-    for (int n = 0; n < batch/im2col_step_; ++n)\n-    {\n-        auto grad_output_g = grad_output_n.select(0, n);\n-        AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n-            ms_deformable_col2im_cuda(at::cuda::getCurrentCUDAStream(),\n-                                    grad_output_g.data<scalar_t>(),\n-                                    value.data<scalar_t>() + n * im2col_step_ * per_value_size,\n-                                    spatial_shapes.data<int64_t>(),\n-                                    level_start_index.data<int64_t>(),\n-                                    sampling_loc.data<scalar_t>() + n * im2col_step_ * per_sample_loc_size,\n-                                    attn_weight.data<scalar_t>() + n * im2col_step_ * per_attn_weight_size,\n-                                    batch_n, spatial_size, num_heads, channels, num_levels, num_query, num_point,\n-                                    grad_value.data<scalar_t>() +  n * im2col_step_ * per_value_size,\n-                                    grad_sampling_loc.data<scalar_t>() + n * im2col_step_ * per_sample_loc_size,\n-                                    grad_attn_weight.data<scalar_t>() + n * im2col_step_ * per_attn_weight_size);\n-\n-        }));\n-    }\n-\n-    return {\n-        grad_value, grad_sampling_loc, grad_attn_weight\n-    };\n-}"
      },
      {
        "filename": "src/transformers/kernels/deta/cuda/ms_deform_attn_cuda.cuh",
        "status": "removed",
        "additions": 0,
        "deletions": 1467,
        "changes": 1467,
        "patch": "@@ -1,1467 +0,0 @@\n-/*!\n-**************************************************************************************************\n-* Deformable DETR\n-* Copyright (c) 2020 SenseTime. All Rights Reserved.\n-* Licensed under the Apache License, Version 2.0 [see LICENSE for details]\n-**************************************************************************************************\n-* Modified from https://github.com/chengdazhi/Deformable-Convolution-V2-PyTorch/tree/pytorch_1.0.0\n-**************************************************************************************************\n-*/\n-\n-#include <vector>\n-\n-#include <cuda.h>\n-#include <cuda_runtime.h>\n-\n-#include <cstdio>\n-#include <algorithm>\n-#include <cstring>\n-\n-#include <ATen/ATen.h>\n-#include <ATen/cuda/CUDAContext.h>\n-\n-#include <THC/THCAtomics.cuh>\n-\n-#define CUDA_KERNEL_LOOP(i, n)                          \\\n-  for (int i = blockIdx.x * blockDim.x + threadIdx.x;   \\\n-      i < (n);                                          \\\n-      i += blockDim.x * gridDim.x)\n-\n-\n-at::Tensor ms_deform_attn_cuda_forward(\n-    const at::Tensor &value, \n-    const at::Tensor &spatial_shapes,\n-    const at::Tensor &level_start_index,\n-    const at::Tensor &sampling_loc,\n-    const at::Tensor &attn_weight,\n-    const int im2col_step)\n-{\n-    AT_ASSERTM(value.is_contiguous(), \"value tensor has to be contiguous\");\n-    AT_ASSERTM(spatial_shapes.is_contiguous(), \"spatial_shapes tensor has to be contiguous\");\n-    AT_ASSERTM(level_start_index.is_contiguous(), \"level_start_index tensor has to be contiguous\");\n-    AT_ASSERTM(sampling_loc.is_contiguous(), \"sampling_loc tensor has to be contiguous\");\n-    AT_ASSERTM(attn_weight.is_contiguous(), \"attn_weight tensor has to be contiguous\");\n-\n-    AT_ASSERTM(value.type().is_cuda(), \"value must be a CUDA tensor\");\n-    AT_ASSERTM(spatial_shapes.type().is_cuda(), \"spatial_shapes must be a CUDA tensor\");\n-    AT_ASSERTM(level_start_index.type().is_cuda(), \"level_start_index must be a CUDA tensor\");\n-    AT_ASSERTM(sampling_loc.type().is_cuda(), \"sampling_loc must be a CUDA tensor\");\n-    AT_ASSERTM(attn_weight.type().is_cuda(), \"attn_weight must be a CUDA tensor\");\n-\n-    const int batch = value.size(0);\n-    const int spatial_size = value.size(1);\n-    const int num_heads = value.size(2);\n-    const int channels = value.size(3);\n-\n-    const int num_levels = spatial_shapes.size(0);\n-\n-    const int num_query = sampling_loc.size(1);\n-    const int num_point = sampling_loc.size(4);\n-\n-    const int im2col_step_ = std::min(batch, im2col_step);\n-\n-    AT_ASSERTM(batch % im2col_step_ == 0, \"batch(%d) must divide im2col_step(%d)\", batch, im2col_step_);\n-    \n-    auto output = at::zeros({batch, num_query, num_heads, channels}, value.options());\n-\n-    const int batch_n = im2col_step_;\n-    auto output_n = output.view({batch/im2col_step_, batch_n, num_query, num_heads, channels});\n-    auto per_value_size = spatial_size * num_heads * channels;\n-    auto per_sample_loc_size = num_query * num_heads * num_levels * num_point * 2;\n-    auto per_attn_weight_size = num_query * num_heads * num_levels * num_point;\n-    for (int n = 0; n < batch/im2col_step_; ++n)\n-    {\n-        auto columns = output_n.select(0, n);\n-        AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n-            ms_deformable_im2col_cuda(at::cuda::getCurrentCUDAStream(),\n-                value.data<scalar_t>() + n * im2col_step_ * per_value_size,\n-                spatial_shapes.data<int64_t>(),\n-                level_start_index.data<int64_t>(),\n-                sampling_loc.data<scalar_t>() + n * im2col_step_ * per_sample_loc_size,\n-                attn_weight.data<scalar_t>() + n * im2col_step_ * per_attn_weight_size,\n-                batch_n, spatial_size, num_heads, channels, num_levels, num_query, num_point,\n-                columns.data<scalar_t>());\n-\n-        }));\n-    }\n-\n-    output = output.view({batch, num_query, num_heads*channels});\n-\n-    return output;\n-}\n-\n-\n-std::vector<at::Tensor> ms_deform_attn_cuda_backward(\n-    const at::Tensor &value, \n-    const at::Tensor &spatial_shapes,\n-    const at::Tensor &level_start_index,\n-    const at::Tensor &sampling_loc,\n-    const at::Tensor &attn_weight,\n-    const at::Tensor &grad_output,\n-    const int im2col_step)\n-{\n-\n-    AT_ASSERTM(value.is_contiguous(), \"value tensor has to be contiguous\");\n-    AT_ASSERTM(spatial_shapes.is_contiguous(), \"spatial_shapes tensor has to be contiguous\");\n-    AT_ASSERTM(level_start_index.is_contiguous(), \"level_start_index tensor has to be contiguous\");\n-    AT_ASSERTM(sampling_loc.is_contiguous(), \"sampling_loc tensor has to be contiguous\");\n-    AT_ASSERTM(attn_weight.is_contiguous(), \"attn_weight tensor has to be contiguous\");\n-    AT_ASSERTM(grad_output.is_contiguous(), \"grad_output tensor has to be contiguous\");\n-\n-    AT_ASSERTM(value.type().is_cuda(), \"value must be a CUDA tensor\");\n-    AT_ASSERTM(spatial_shapes.type().is_cuda(), \"spatial_shapes must be a CUDA tensor\");\n-    AT_ASSERTM(level_start_index.type().is_cuda(), \"level_start_index must be a CUDA tensor\");\n-    AT_ASSERTM(sampling_loc.type().is_cuda(), \"sampling_loc must be a CUDA tensor\");\n-    AT_ASSERTM(attn_weight.type().is_cuda(), \"attn_weight must be a CUDA tensor\");\n-    AT_ASSERTM(grad_output.type().is_cuda(), \"grad_output must be a CUDA tensor\");\n-\n-    const int batch = value.size(0);\n-    const int spatial_size = value.size(1);\n-    const int num_heads = value.size(2);\n-    const int channels = value.size(3);\n-\n-    const int num_levels = spatial_shapes.size(0);\n-\n-    const int num_query = sampling_loc.size(1);\n-    const int num_point = sampling_loc.size(4);\n-\n-    const int im2col_step_ = std::min(batch, im2col_step);\n-\n-    AT_ASSERTM(batch % im2col_step_ == 0, \"batch(%d) must divide im2col_step(%d)\", batch, im2col_step_);\n-\n-    auto grad_value = at::zeros_like(value);\n-    auto grad_sampling_loc = at::zeros_like(sampling_loc);\n-    auto grad_attn_weight = at::zeros_like(attn_weight);\n-\n-    const int batch_n = im2col_step_;\n-    auto per_value_size = spatial_size * num_heads * channels;\n-    auto per_sample_loc_size = num_query * num_heads * num_levels * num_point * 2;\n-    auto per_attn_weight_size = num_query * num_heads * num_levels * num_point;\n-    auto grad_output_n = grad_output.view({batch/im2col_step_, batch_n, num_query, num_heads, channels});\n-    \n-    for (int n = 0; n < batch/im2col_step_; ++n)\n-    {\n-        auto grad_output_g = grad_output_n.select(0, n);\n-        AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n-            ms_deformable_col2im_cuda(at::cuda::getCurrentCUDAStream(),\n-                                    grad_output_g.data<scalar_t>(),\n-                                    value.data<scalar_t>() + n * im2col_step_ * per_value_size,\n-                                    spatial_shapes.data<int64_t>(),\n-                                    level_start_index.data<int64_t>(),\n-                                    sampling_loc.data<scalar_t>() + n * im2col_step_ * per_sample_loc_size,\n-                                    attn_weight.data<scalar_t>() + n * im2col_step_ * per_attn_weight_size,\n-                                    batch_n, spatial_size, num_heads, channels, num_levels, num_query, num_point,\n-                                    grad_value.data<scalar_t>() +  n * im2col_step_ * per_value_size,\n-                                    grad_sampling_loc.data<scalar_t>() + n * im2col_step_ * per_sample_loc_size,\n-                                    grad_attn_weight.data<scalar_t>() + n * im2col_step_ * per_attn_weight_size);\n-\n-        }));\n-    }\n-\n-    return {\n-        grad_value, grad_sampling_loc, grad_attn_weight\n-    };\n-}\n-\n-const int CUDA_NUM_THREADS = 1024;\n-inline int GET_BLOCKS(const int N, const int num_threads)\n-{\n-  return (N + num_threads - 1) / num_threads;\n-}\n-\n-\n-template <typename scalar_t>\n-__device__ scalar_t ms_deform_attn_im2col_bilinear(const scalar_t* &bottom_data, \n-                                                   const int &height, const int &width, const int &nheads, const int &channels,\n-                                                   const scalar_t &h, const scalar_t &w, const int &m, const int &c)\n-{\n-  const int h_low = floor(h);\n-  const int w_low = floor(w);\n-  const int h_high = h_low + 1;\n-  const int w_high = w_low + 1;\n-\n-  const scalar_t lh = h - h_low;\n-  const scalar_t lw = w - w_low;\n-  const scalar_t hh = 1 - lh, hw = 1 - lw;\n-\n-  const int w_stride = nheads * channels;\n-  const int h_stride = width * w_stride;\n-  const int h_low_ptr_offset = h_low * h_stride;\n-  const int h_high_ptr_offset = h_low_ptr_offset + h_stride;\n-  const int w_low_ptr_offset = w_low * w_stride;\n-  const int w_high_ptr_offset = w_low_ptr_offset + w_stride;\n-  const int base_ptr = m * channels + c;\n-\n-  scalar_t v1 = 0;\n-  if (h_low >= 0 && w_low >= 0)\n-  {\n-    const int ptr1 = h_low_ptr_offset + w_low_ptr_offset + base_ptr;\n-    v1 = bottom_data[ptr1];\n-  }\n-  scalar_t v2 = 0;\n-  if (h_low >= 0 && w_high <= width - 1)\n-  {\n-    const int ptr2 = h_low_ptr_offset + w_high_ptr_offset + base_ptr;\n-    v2 = bottom_data[ptr2];\n-  }\n-  scalar_t v3 = 0;\n-  if (h_high <= height - 1 && w_low >= 0)\n-  {\n-    const int ptr3 = h_high_ptr_offset + w_low_ptr_offset + base_ptr;\n-    v3 = bottom_data[ptr3];\n-  }\n-  scalar_t v4 = 0;\n-  if (h_high <= height - 1 && w_high <= width - 1)\n-  {\n-    const int ptr4 = h_high_ptr_offset + w_high_ptr_offset + base_ptr;\n-    v4 = bottom_data[ptr4];\n-  }\n-\n-  const scalar_t w1 = hh * hw, w2 = hh * lw, w3 = lh * hw, w4 = lh * lw;\n-\n-  const scalar_t val = (w1 * v1 + w2 * v2 + w3 * v3 + w4 * v4);\n-  return val;\n-}\n-\n-\n-template <typename scalar_t>\n-__device__ void ms_deform_attn_col2im_bilinear(const scalar_t* &bottom_data, \n-                                                   const int &height, const int &width, const int &nheads, const int &channels,\n-                                                   const scalar_t &h, const scalar_t &w, const int &m, const int &c,\n-                                                   const scalar_t &top_grad,\n-                                                   const scalar_t &attn_weight,\n-                                                   scalar_t* &grad_value, \n-                                                   scalar_t* grad_sampling_loc,\n-                                                   scalar_t* grad_attn_weight)\n-{\n-  const int h_low = floor(h);\n-  const int w_low = floor(w);\n-  const int h_high = h_low + 1;\n-  const int w_high = w_low + 1;\n-\n-  const scalar_t lh = h - h_low;\n-  const scalar_t lw = w - w_low;\n-  const scalar_t hh = 1 - lh, hw = 1 - lw;\n-\n-  const int w_stride = nheads * channels;\n-  const int h_stride = width * w_stride;\n-  const int h_low_ptr_offset = h_low * h_stride;\n-  const int h_high_ptr_offset = h_low_ptr_offset + h_stride;\n-  const int w_low_ptr_offset = w_low * w_stride;\n-  const int w_high_ptr_offset = w_low_ptr_offset + w_stride;\n-  const int base_ptr = m * channels + c;\n-\n-  const scalar_t w1 = hh * hw, w2 = hh * lw, w3 = lh * hw, w4 = lh * lw;\n-  const scalar_t top_grad_value = top_grad * attn_weight;\n-  scalar_t grad_h_weight = 0, grad_w_weight = 0;\n-\n-  scalar_t v1 = 0;\n-  if (h_low >= 0 && w_low >= 0)\n-  {\n-    const int ptr1 = h_low_ptr_offset + w_low_ptr_offset + base_ptr;\n-    v1 = bottom_data[ptr1];\n-    grad_h_weight -= hw * v1;\n-    grad_w_weight -= hh * v1;\n-    atomicAdd(grad_value+ptr1, w1*top_grad_value);\n-  }\n-  scalar_t v2 = 0;\n-  if (h_low >= 0 && w_high <= width - 1)\n-  {\n-    const int ptr2 = h_low_ptr_offset + w_high_ptr_offset + base_ptr;\n-    v2 = bottom_data[ptr2];\n-    grad_h_weight -= lw * v2;\n-    grad_w_weight += hh * v2;\n-    atomicAdd(grad_value+ptr2, w2*top_grad_value);\n-  }\n-  scalar_t v3 = 0;\n-  if (h_high <= height - 1 && w_low >= 0)\n-  {\n-    const int ptr3 = h_high_ptr_offset + w_low_ptr_offset + base_ptr;\n-    v3 = bottom_data[ptr3];\n-    grad_h_weight += hw * v3;\n-    grad_w_weight -= lh * v3;\n-    atomicAdd(grad_value+ptr3, w3*top_grad_value); \n-  }\n-  scalar_t v4 = 0;\n-  if (h_high <= height - 1 && w_high <= width - 1)\n-  {\n-    const int ptr4 = h_high_ptr_offset + w_high_ptr_offset + base_ptr;\n-    v4 = bottom_data[ptr4];\n-    grad_h_weight += lw * v4;\n-    grad_w_weight += lh * v4;\n-    atomicAdd(grad_value+ptr4, w4*top_grad_value);\n-  }\n-\n-  const scalar_t val = (w1 * v1 + w2 * v2 + w3 * v3 + w4 * v4);\n-  *grad_attn_weight = top_grad * val;\n-  *grad_sampling_loc = width * grad_w_weight * top_grad_value;\n-  *(grad_sampling_loc + 1) = height * grad_h_weight * top_grad_value;\n-}\n-\n-\n-template <typename scalar_t>\n-__device__ void ms_deform_attn_col2im_bilinear_gm(const scalar_t* &bottom_data, \n-                                                   const int &height, const int &width, const int &nheads, const int &channels,\n-                                                   const scalar_t &h, const scalar_t &w, const int &m, const int &c,\n-                                                   const scalar_t &top_grad,\n-                                                   const scalar_t &attn_weight,\n-                                                   scalar_t* &grad_value, \n-                                                   scalar_t* grad_sampling_loc,\n-                                                   scalar_t* grad_attn_weight)\n-{\n-  const int h_low = floor(h);\n-  const int w_low = floor(w);\n-  const int h_high = h_low + 1;\n-  const int w_high = w_low + 1;\n-\n-  const scalar_t lh = h - h_low;\n-  const scalar_t lw = w - w_low;\n-  const scalar_t hh = 1 - lh, hw = 1 - lw;\n-\n-  const int w_stride = nheads * channels;\n-  const int h_stride = width * w_stride;\n-  const int h_low_ptr_offset = h_low * h_stride;\n-  const int h_high_ptr_offset = h_low_ptr_offset + h_stride;\n-  const int w_low_ptr_offset = w_low * w_stride;\n-  const int w_high_ptr_offset = w_low_ptr_offset + w_stride;\n-  const int base_ptr = m * channels + c;\n-\n-  const scalar_t w1 = hh * hw, w2 = hh * lw, w3 = lh * hw, w4 = lh * lw;\n-  const scalar_t top_grad_value = top_grad * attn_weight;\n-  scalar_t grad_h_weight = 0, grad_w_weight = 0;\n-\n-  scalar_t v1 = 0;\n-  if (h_low >= 0 && w_low >= 0)\n-  {\n-    const int ptr1 = h_low_ptr_offset + w_low_ptr_offset + base_ptr;\n-    v1 = bottom_data[ptr1];\n-    grad_h_weight -= hw * v1;\n-    grad_w_weight -= hh * v1;\n-    atomicAdd(grad_value+ptr1, w1*top_grad_value);\n-  }\n-  scalar_t v2 = 0;\n-  if (h_low >= 0 && w_high <= width - 1)\n-  {\n-    const int ptr2 = h_low_ptr_offset + w_high_ptr_offset + base_ptr;\n-    v2 = bottom_data[ptr2];\n-    grad_h_weight -= lw * v2;\n-    grad_w_weight += hh * v2;\n-    atomicAdd(grad_value+ptr2, w2*top_grad_value);\n-  }\n-  scalar_t v3 = 0;\n-  if (h_high <= height - 1 && w_low >= 0)\n-  {\n-    const int ptr3 = h_high_ptr_offset + w_low_ptr_offset + base_ptr;\n-    v3 = bottom_data[ptr3];\n-    grad_h_weight += hw * v3;\n-    grad_w_weight -= lh * v3;\n-    atomicAdd(grad_value+ptr3, w3*top_grad_value); \n-  }\n-  scalar_t v4 = 0;\n-  if (h_high <= height - 1 && w_high <= width - 1)\n-  {\n-    const int ptr4 = h_high_ptr_offset + w_high_ptr_offset + base_ptr;\n-    v4 = bottom_data[ptr4];\n-    grad_h_weight += lw * v4;\n-    grad_w_weight += lh * v4;\n-    atomicAdd(grad_value+ptr4, w4*top_grad_value);\n-  }\n-\n-  const scalar_t val = (w1 * v1 + w2 * v2 + w3 * v3 + w4 * v4);\n-  atomicAdd(grad_attn_weight, top_grad * val); \n-  atomicAdd(grad_sampling_loc, width * grad_w_weight * top_grad_value);\n-  atomicAdd(grad_sampling_loc + 1, height * grad_h_weight * top_grad_value);\n-}\n-\n-\n-template <typename scalar_t>\n-__global__ void ms_deformable_im2col_gpu_kernel(const int n,\n-                                                const scalar_t *data_value, \n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *data_col)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    scalar_t *data_col_ptr = data_col + index;\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-    scalar_t col = 0;\n-    \n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const scalar_t *data_value_ptr = data_value + (data_value_ptr_init_offset + level_start_id * qid_stride);\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          col += ms_deform_attn_im2col_bilinear(data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col) * weight;\n-        }\n-\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-      }\n-    }\n-    *data_col_ptr = col;\n-  }\n-}\n-\n-template <typename scalar_t, unsigned int blockSize>\n-__global__ void ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1(const int n,\n-                                                const scalar_t *grad_col,\n-                                                const scalar_t *data_value,\n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *grad_value,\n-                                                scalar_t *grad_sampling_loc,\n-                                                scalar_t *grad_attn_weight)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    __shared__ scalar_t cache_grad_sampling_loc[blockSize * 2];\n-    __shared__ scalar_t cache_grad_attn_weight[blockSize];\n-    unsigned int tid = threadIdx.x;\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    const scalar_t top_grad = grad_col[index];\n-\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int grad_sampling_ptr = data_weight_ptr;\n-    grad_sampling_loc += grad_sampling_ptr << 1;\n-    grad_attn_weight += grad_sampling_ptr;\n-    const int grad_weight_stride = 1;\n-    const int grad_loc_stride = 2;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-\n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const int value_ptr_offset = data_value_ptr_init_offset + level_start_id * qid_stride;\n-      const scalar_t *data_value_ptr = data_value + value_ptr_offset;\n-      scalar_t *grad_value_ptr = grad_value + value_ptr_offset;\n-\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-        *(cache_grad_sampling_loc+(threadIdx.x << 1)) = 0;\n-        *(cache_grad_sampling_loc+((threadIdx.x << 1) + 1)) = 0;\n-        *(cache_grad_attn_weight+threadIdx.x)=0;\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          ms_deform_attn_col2im_bilinear(\n-            data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col,\n-            top_grad, weight, grad_value_ptr, \n-            cache_grad_sampling_loc+(threadIdx.x << 1), cache_grad_attn_weight+threadIdx.x);\n-        }\n-        \n-        __syncthreads();\n-        if (tid == 0)\n-        {\n-          scalar_t _grad_w=cache_grad_sampling_loc[0], _grad_h=cache_grad_sampling_loc[1], _grad_a=cache_grad_attn_weight[0];\n-          int sid=2;\n-          for (unsigned int tid = 1; tid < blockSize; ++tid)\n-          {\n-            _grad_w += cache_grad_sampling_loc[sid];\n-            _grad_h += cache_grad_sampling_loc[sid + 1];\n-            _grad_a += cache_grad_attn_weight[tid];\n-            sid += 2;\n-          }\n-          \n-          \n-          *grad_sampling_loc = _grad_w;\n-          *(grad_sampling_loc + 1) = _grad_h;\n-          *grad_attn_weight = _grad_a;\n-        }\n-        __syncthreads();\n-\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-        grad_attn_weight += grad_weight_stride;\n-        grad_sampling_loc += grad_loc_stride;\n-      }\n-    }\n-  }\n-}\n-\n-\n-template <typename scalar_t, unsigned int blockSize>\n-__global__ void ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v2(const int n,\n-                                                const scalar_t *grad_col,\n-                                                const scalar_t *data_value,\n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *grad_value,\n-                                                scalar_t *grad_sampling_loc,\n-                                                scalar_t *grad_attn_weight)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    __shared__ scalar_t cache_grad_sampling_loc[blockSize * 2];\n-    __shared__ scalar_t cache_grad_attn_weight[blockSize];\n-    unsigned int tid = threadIdx.x;\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    const scalar_t top_grad = grad_col[index];\n-\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int grad_sampling_ptr = data_weight_ptr;\n-    grad_sampling_loc += grad_sampling_ptr << 1;\n-    grad_attn_weight += grad_sampling_ptr;\n-    const int grad_weight_stride = 1;\n-    const int grad_loc_stride = 2;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-\n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const int value_ptr_offset = data_value_ptr_init_offset + level_start_id * qid_stride;\n-      const scalar_t *data_value_ptr = data_value + value_ptr_offset;\n-      scalar_t *grad_value_ptr = grad_value + value_ptr_offset;\n-\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-        *(cache_grad_sampling_loc+(threadIdx.x << 1)) = 0;\n-        *(cache_grad_sampling_loc+((threadIdx.x << 1) + 1)) = 0;\n-        *(cache_grad_attn_weight+threadIdx.x)=0;\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          ms_deform_attn_col2im_bilinear(\n-            data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col,\n-            top_grad, weight, grad_value_ptr, \n-            cache_grad_sampling_loc+(threadIdx.x << 1), cache_grad_attn_weight+threadIdx.x);\n-        }\n-        \n-        __syncthreads();\n-\n-        for (unsigned int s=blockSize/2; s>0; s>>=1)\n-        {\n-          if (tid < s) {\n-            const unsigned int xid1 = tid << 1;\n-            const unsigned int xid2 = (tid + s) << 1;\n-            cache_grad_attn_weight[tid] += cache_grad_attn_weight[tid + s];\n-            cache_grad_sampling_loc[xid1] += cache_grad_sampling_loc[xid2];\n-            cache_grad_sampling_loc[xid1 + 1] += cache_grad_sampling_loc[xid2 + 1];\n-          }\n-          __syncthreads();\n-        }\n-\n-        if (tid == 0)\n-        { \n-          *grad_sampling_loc = cache_grad_sampling_loc[0];\n-          *(grad_sampling_loc + 1) = cache_grad_sampling_loc[1];\n-          *grad_attn_weight = cache_grad_attn_weight[0];\n-        }\n-        __syncthreads();\n-\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-        grad_attn_weight += grad_weight_stride;\n-        grad_sampling_loc += grad_loc_stride;\n-      }\n-    }\n-  }\n-}\n-\n-\n-template <typename scalar_t>\n-__global__ void ms_deformable_col2im_gpu_kernel_shm_reduce_v1(const int n,\n-                                                const scalar_t *grad_col,\n-                                                const scalar_t *data_value,\n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *grad_value,\n-                                                scalar_t *grad_sampling_loc,\n-                                                scalar_t *grad_attn_weight)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    extern __shared__ int _s[];\n-    scalar_t* cache_grad_sampling_loc = (scalar_t*)_s;\n-    scalar_t* cache_grad_attn_weight = cache_grad_sampling_loc + 2 * blockDim.x;\n-    unsigned int tid = threadIdx.x;\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    const scalar_t top_grad = grad_col[index];\n-\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int grad_sampling_ptr = data_weight_ptr;\n-    grad_sampling_loc += grad_sampling_ptr << 1;\n-    grad_attn_weight += grad_sampling_ptr;\n-    const int grad_weight_stride = 1;\n-    const int grad_loc_stride = 2;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-\n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const int value_ptr_offset = data_value_ptr_init_offset + level_start_id * qid_stride;\n-      const scalar_t *data_value_ptr = data_value + value_ptr_offset;\n-      scalar_t *grad_value_ptr = grad_value + value_ptr_offset;\n-\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-        *(cache_grad_sampling_loc+(threadIdx.x << 1)) = 0;\n-        *(cache_grad_sampling_loc+((threadIdx.x << 1) + 1)) = 0;\n-        *(cache_grad_attn_weight+threadIdx.x)=0;\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          ms_deform_attn_col2im_bilinear(\n-            data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col,\n-            top_grad, weight, grad_value_ptr, \n-            cache_grad_sampling_loc+(threadIdx.x << 1), cache_grad_attn_weight+threadIdx.x);\n-        }\n-        \n-        __syncthreads();\n-        if (tid == 0)\n-        {\n-          scalar_t _grad_w=cache_grad_sampling_loc[0], _grad_h=cache_grad_sampling_loc[1], _grad_a=cache_grad_attn_weight[0];\n-          int sid=2;\n-          for (unsigned int tid = 1; tid < blockDim.x; ++tid)\n-          {\n-            _grad_w += cache_grad_sampling_loc[sid];\n-            _grad_h += cache_grad_sampling_loc[sid + 1];\n-            _grad_a += cache_grad_attn_weight[tid];\n-            sid += 2;\n-          }\n-          \n-          \n-          *grad_sampling_loc = _grad_w;\n-          *(grad_sampling_loc + 1) = _grad_h;\n-          *grad_attn_weight = _grad_a;\n-        }\n-        __syncthreads();\n-\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-        grad_attn_weight += grad_weight_stride;\n-        grad_sampling_loc += grad_loc_stride;\n-      }\n-    }\n-  }\n-}\n-\n-template <typename scalar_t>\n-__global__ void ms_deformable_col2im_gpu_kernel_shm_reduce_v2(const int n,\n-                                                const scalar_t *grad_col,\n-                                                const scalar_t *data_value,\n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *grad_value,\n-                                                scalar_t *grad_sampling_loc,\n-                                                scalar_t *grad_attn_weight)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    extern __shared__ int _s[];\n-    scalar_t* cache_grad_sampling_loc = (scalar_t*)_s;\n-    scalar_t* cache_grad_attn_weight = cache_grad_sampling_loc + 2 * blockDim.x;\n-    unsigned int tid = threadIdx.x;\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    const scalar_t top_grad = grad_col[index];\n-\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int grad_sampling_ptr = data_weight_ptr;\n-    grad_sampling_loc += grad_sampling_ptr << 1;\n-    grad_attn_weight += grad_sampling_ptr;\n-    const int grad_weight_stride = 1;\n-    const int grad_loc_stride = 2;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-\n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const int value_ptr_offset = data_value_ptr_init_offset + level_start_id * qid_stride;\n-      const scalar_t *data_value_ptr = data_value + value_ptr_offset;\n-      scalar_t *grad_value_ptr = grad_value + value_ptr_offset;\n-\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-        *(cache_grad_sampling_loc+(threadIdx.x << 1)) = 0;\n-        *(cache_grad_sampling_loc+((threadIdx.x << 1) + 1)) = 0;\n-        *(cache_grad_attn_weight+threadIdx.x)=0;\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          ms_deform_attn_col2im_bilinear(\n-            data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col,\n-            top_grad, weight, grad_value_ptr, \n-            cache_grad_sampling_loc+(threadIdx.x << 1), cache_grad_attn_weight+threadIdx.x);\n-        }\n-        \n-        __syncthreads();\n-\n-        for (unsigned int s=blockDim.x/2, spre=blockDim.x; s>0; s>>=1, spre>>=1)\n-        {\n-          if (tid < s) {\n-            const unsigned int xid1 = tid << 1;\n-            const unsigned int xid2 = (tid + s) << 1;\n-            cache_grad_attn_weight[tid] += cache_grad_attn_weight[tid + s];\n-            cache_grad_sampling_loc[xid1] += cache_grad_sampling_loc[xid2];\n-            cache_grad_sampling_loc[xid1 + 1] += cache_grad_sampling_loc[xid2 + 1];\n-            if (tid + (s << 1) < spre)\n-            {\n-              cache_grad_attn_weight[tid] += cache_grad_attn_weight[tid + (s << 1)];\n-              cache_grad_sampling_loc[xid1] += cache_grad_sampling_loc[xid2 + (s << 1)];\n-              cache_grad_sampling_loc[xid1 + 1] += cache_grad_sampling_loc[xid2 + 1 + (s << 1)];\n-            } \n-          }\n-          __syncthreads();\n-        }\n-\n-        if (tid == 0)\n-        {\n-          *grad_sampling_loc = cache_grad_sampling_loc[0];\n-          *(grad_sampling_loc + 1) = cache_grad_sampling_loc[1];\n-          *grad_attn_weight = cache_grad_attn_weight[0];\n-        }\n-        __syncthreads();\n-\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-        grad_attn_weight += grad_weight_stride;\n-        grad_sampling_loc += grad_loc_stride;\n-      }\n-    }\n-  }\n-}\n-\n-template <typename scalar_t>\n-__global__ void ms_deformable_col2im_gpu_kernel_shm_reduce_v2_multi_blocks(const int n,\n-                                                const scalar_t *grad_col,\n-                                                const scalar_t *data_value,\n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *grad_value,\n-                                                scalar_t *grad_sampling_loc,\n-                                                scalar_t *grad_attn_weight)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    extern __shared__ int _s[];\n-    scalar_t* cache_grad_sampling_loc = (scalar_t*)_s;\n-    scalar_t* cache_grad_attn_weight = cache_grad_sampling_loc + 2 * blockDim.x;\n-    unsigned int tid = threadIdx.x;\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    const scalar_t top_grad = grad_col[index];\n-\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int grad_sampling_ptr = data_weight_ptr;\n-    grad_sampling_loc += grad_sampling_ptr << 1;\n-    grad_attn_weight += grad_sampling_ptr;\n-    const int grad_weight_stride = 1;\n-    const int grad_loc_stride = 2;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-\n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const int value_ptr_offset = data_value_ptr_init_offset + level_start_id * qid_stride;\n-      const scalar_t *data_value_ptr = data_value + value_ptr_offset;\n-      scalar_t *grad_value_ptr = grad_value + value_ptr_offset;\n-\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-        *(cache_grad_sampling_loc+(threadIdx.x << 1)) = 0;\n-        *(cache_grad_sampling_loc+((threadIdx.x << 1) + 1)) = 0;\n-        *(cache_grad_attn_weight+threadIdx.x)=0;\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          ms_deform_attn_col2im_bilinear(\n-            data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col,\n-            top_grad, weight, grad_value_ptr, \n-            cache_grad_sampling_loc+(threadIdx.x << 1), cache_grad_attn_weight+threadIdx.x);\n-        }\n-        \n-        __syncthreads();\n-\n-        for (unsigned int s=blockDim.x/2, spre=blockDim.x; s>0; s>>=1, spre>>=1)\n-        {\n-          if (tid < s) {\n-            const unsigned int xid1 = tid << 1;\n-            const unsigned int xid2 = (tid + s) << 1;\n-            cache_grad_attn_weight[tid] += cache_grad_attn_weight[tid + s];\n-            cache_grad_sampling_loc[xid1] += cache_grad_sampling_loc[xid2];\n-            cache_grad_sampling_loc[xid1 + 1] += cache_grad_sampling_loc[xid2 + 1];\n-            if (tid + (s << 1) < spre)\n-            {\n-              cache_grad_attn_weight[tid] += cache_grad_attn_weight[tid + (s << 1)];\n-              cache_grad_sampling_loc[xid1] += cache_grad_sampling_loc[xid2 + (s << 1)];\n-              cache_grad_sampling_loc[xid1 + 1] += cache_grad_sampling_loc[xid2 + 1 + (s << 1)];\n-            }\n-          }\n-          __syncthreads();\n-        }\n-\n-        if (tid == 0)\n-        {\n-          atomicAdd(grad_sampling_loc, cache_grad_sampling_loc[0]);\n-          atomicAdd(grad_sampling_loc + 1, cache_grad_sampling_loc[1]);\n-          atomicAdd(grad_attn_weight, cache_grad_attn_weight[0]);\n-        }\n-        __syncthreads();\n-\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-        grad_attn_weight += grad_weight_stride;\n-        grad_sampling_loc += grad_loc_stride;\n-      }\n-    }\n-  }\n-}\n-\n-\n-template <typename scalar_t>\n-__global__ void ms_deformable_col2im_gpu_kernel_gm(const int n,\n-                                                const scalar_t *grad_col,\n-                                                const scalar_t *data_value,\n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *grad_value,\n-                                                scalar_t *grad_sampling_loc,\n-                                                scalar_t *grad_attn_weight)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    const scalar_t top_grad = grad_col[index];\n-\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int grad_sampling_ptr = data_weight_ptr;\n-    grad_sampling_loc += grad_sampling_ptr << 1;\n-    grad_attn_weight += grad_sampling_ptr;\n-    const int grad_weight_stride = 1;\n-    const int grad_loc_stride = 2;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-\n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const int value_ptr_offset = data_value_ptr_init_offset + level_start_id * qid_stride;\n-      const scalar_t *data_value_ptr = data_value + value_ptr_offset;\n-      scalar_t *grad_value_ptr = grad_value + value_ptr_offset;\n-\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          ms_deform_attn_col2im_bilinear_gm(\n-            data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col,\n-            top_grad, weight, grad_value_ptr, \n-            grad_sampling_loc, grad_attn_weight);\n-        }\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-        grad_attn_weight += grad_weight_stride;\n-        grad_sampling_loc += grad_loc_stride;\n-      }\n-    }\n-  }\n-}\n-\n-\n-template <typename scalar_t>\n-void ms_deformable_im2col_cuda(cudaStream_t stream,\n-                              const scalar_t* data_value,\n-                              const int64_t* data_spatial_shapes, \n-                              const int64_t* data_level_start_index, \n-                              const scalar_t* data_sampling_loc,\n-                              const scalar_t* data_attn_weight,\n-                              const int batch_size,\n-                              const int spatial_size, \n-                              const int num_heads, \n-                              const int channels, \n-                              const int num_levels, \n-                              const int num_query,\n-                              const int num_point,\n-                              scalar_t* data_col)\n-{\n-  const int num_kernels = batch_size * num_query * num_heads * channels;\n-  const int num_actual_kernels = batch_size * num_query * num_heads * channels;\n-  const int num_threads = CUDA_NUM_THREADS;\n-  ms_deformable_im2col_gpu_kernel<scalar_t>\n-      <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-          0, stream>>>(\n-      num_kernels, data_value, data_spatial_shapes, data_level_start_index, data_sampling_loc, data_attn_weight, \n-      batch_size, spatial_size, num_heads, channels, num_levels, num_query, num_point, data_col);\n-  \n-  cudaError_t err = cudaGetLastError();\n-  if (err != cudaSuccess)\n-  {\n-    printf(\"error in ms_deformable_im2col_cuda: %s\\n\", cudaGetErrorString(err));\n-  }\n-\n-}\n-\n-template <typename scalar_t>\n-void ms_deformable_col2im_cuda(cudaStream_t stream,\n-                              const scalar_t* grad_col,\n-                              const scalar_t* data_value,\n-                              const int64_t * data_spatial_shapes,\n-                              const int64_t * data_level_start_index,\n-                              const scalar_t * data_sampling_loc,\n-                              const scalar_t * data_attn_weight,\n-                              const int batch_size, \n-                              const int spatial_size, \n-                              const int num_heads,\n-                              const int channels, \n-                              const int num_levels,\n-                              const int num_query,\n-                              const int num_point, \n-                              scalar_t* grad_value,\n-                              scalar_t* grad_sampling_loc,\n-                              scalar_t* grad_attn_weight)\n-{\n-  const int num_threads = (channels > CUDA_NUM_THREADS)?CUDA_NUM_THREADS:channels;\n-  const int num_kernels = batch_size * num_query * num_heads * channels;\n-  const int num_actual_kernels = batch_size * num_query * num_heads * channels;\n-  if (channels > 1024)\n-  {\n-    if ((channels & 1023) == 0)\n-    {\n-      ms_deformable_col2im_gpu_kernel_shm_reduce_v2_multi_blocks<scalar_t>\n-          <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-              num_threads*3*sizeof(scalar_t), stream>>>(\n-                        num_kernels, \n-                        grad_col,\n-                        data_value,\n-                        data_spatial_shapes,\n-                        data_level_start_index, \n-                        data_sampling_loc,\n-                        data_attn_weight,\n-                        batch_size, \n-                        spatial_size, \n-                        num_heads,\n-                        channels, \n-                        num_levels,\n-                        num_query,\n-                        num_point,\n-                        grad_value,\n-                        grad_sampling_loc,\n-                        grad_attn_weight);\n-    }\n-    else\n-    {\n-      ms_deformable_col2im_gpu_kernel_gm<scalar_t>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-    }\n-  }\n-  else{\n-    switch(channels)\n-    {\n-      case 1:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1<scalar_t, 1>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 2:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1<scalar_t, 2>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 4:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1<scalar_t, 4>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 8:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1<scalar_t, 8>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 16:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1<scalar_t, 16>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 32:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1<scalar_t, 32>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 64:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v2<scalar_t, 64>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 128:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v2<scalar_t, 128>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 256:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v2<scalar_t, 256>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 512:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v2<scalar_t, 512>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 1024:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v2<scalar_t, 1024>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      default:\n-        if (channels < 64)\n-        {\n-          ms_deformable_col2im_gpu_kernel_shm_reduce_v1<scalar_t>\n-          <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-              num_threads*3*sizeof(scalar_t), stream>>>(\n-                        num_kernels, \n-                        grad_col,\n-                        data_value,\n-                        data_spatial_shapes,\n-                        data_level_start_index, \n-                        data_sampling_loc,\n-                        data_attn_weight,\n-                        batch_size, \n-                        spatial_size, \n-                        num_heads,\n-                        channels, \n-                        num_levels,\n-                        num_query,\n-                        num_point,\n-                        grad_value,\n-                        grad_sampling_loc,\n-                        grad_attn_weight);\n-        }\n-        else\n-        {\n-          ms_deformable_col2im_gpu_kernel_shm_reduce_v2<scalar_t>\n-          <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-              num_threads*3*sizeof(scalar_t), stream>>>(\n-                        num_kernels, \n-                        grad_col,\n-                        data_value,\n-                        data_spatial_shapes,\n-                        data_level_start_index, \n-                        data_sampling_loc,\n-                        data_attn_weight,\n-                        batch_size, \n-                        spatial_size, \n-                        num_heads,\n-                        channels, \n-                        num_levels,\n-                        num_query,\n-                        num_point,\n-                        grad_value,\n-                        grad_sampling_loc,\n-                        grad_attn_weight);\n-        }\n-    }\n-  }\n-  cudaError_t err = cudaGetLastError();\n-  if (err != cudaSuccess)\n-  {\n-    printf(\"error in ms_deformable_col2im_cuda: %s\\n\", cudaGetErrorString(err));\n-  }\n-\n-}"
      },
      {
        "filename": "src/transformers/kernels/deta/cuda/ms_deform_attn_cuda.h",
        "status": "removed",
        "additions": 0,
        "deletions": 29,
        "changes": 29,
        "patch": "@@ -1,29 +0,0 @@\n-/*!\n-**************************************************************************************************\n-* Deformable DETR\n-* Copyright (c) 2020 SenseTime. All Rights Reserved.\n-* Licensed under the Apache License, Version 2.0 [see LICENSE for details]\n-**************************************************************************************************\n-* Modified from https://github.com/chengdazhi/Deformable-Convolution-V2-PyTorch/tree/pytorch_1.0.0\n-**************************************************************************************************\n-*/\n-\n-#pragma once\n-#include <torch/extension.h>\n-\n-at::Tensor ms_deform_attn_cuda_forward(\n-    const at::Tensor &value, \n-    const at::Tensor &spatial_shapes,\n-    const at::Tensor &level_start_index,\n-    const at::Tensor &sampling_loc,\n-    const at::Tensor &attn_weight,\n-    const int im2col_step);\n-\n-std::vector<at::Tensor> ms_deform_attn_cuda_backward(\n-    const at::Tensor &value, \n-    const at::Tensor &spatial_shapes,\n-    const at::Tensor &level_start_index,\n-    const at::Tensor &sampling_loc,\n-    const at::Tensor &attn_weight,\n-    const at::Tensor &grad_output,\n-    const int im2col_step);"
      },
      {
        "filename": "src/transformers/kernels/deta/cuda/ms_deform_im2col_cuda.cuh",
        "status": "removed",
        "additions": 0,
        "deletions": 1327,
        "changes": 1327,
        "patch": "@@ -1,1327 +0,0 @@\n-/*!\n-**************************************************************************\n-* Deformable DETR\n-* Copyright (c) 2020 SenseTime. All Rights Reserved.\n-* Licensed under the Apache License, Version 2.0 [see LICENSE for details]\n-**************************************************************************\n-* Modified from DCN (https://github.com/msracver/Deformable-ConvNets)\n-* Copyright (c) 2018 Microsoft\n-**************************************************************************\n-*/\n-\n-#include <cstdio>\n-#include <algorithm>\n-#include <cstring>\n-\n-#include <ATen/ATen.h>\n-#include <ATen/cuda/CUDAContext.h>\n-\n-#include <THC/THCAtomics.cuh>\n-\n-#define CUDA_KERNEL_LOOP(i, n)                          \\\n-  for (int i = blockIdx.x * blockDim.x + threadIdx.x;   \\\n-      i < (n);                                          \\\n-      i += blockDim.x * gridDim.x)\n-\n-const int CUDA_NUM_THREADS = 1024;\n-inline int GET_BLOCKS(const int N, const int num_threads)\n-{\n-  return (N + num_threads - 1) / num_threads;\n-}\n-\n-\n-template <typename scalar_t>\n-__device__ scalar_t ms_deform_attn_im2col_bilinear(const scalar_t* &bottom_data, \n-                                                   const int &height, const int &width, const int &nheads, const int &channels,\n-                                                   const scalar_t &h, const scalar_t &w, const int &m, const int &c)\n-{\n-  const int h_low = floor(h);\n-  const int w_low = floor(w);\n-  const int h_high = h_low + 1;\n-  const int w_high = w_low + 1;\n-\n-  const scalar_t lh = h - h_low;\n-  const scalar_t lw = w - w_low;\n-  const scalar_t hh = 1 - lh, hw = 1 - lw;\n-\n-  const int w_stride = nheads * channels;\n-  const int h_stride = width * w_stride;\n-  const int h_low_ptr_offset = h_low * h_stride;\n-  const int h_high_ptr_offset = h_low_ptr_offset + h_stride;\n-  const int w_low_ptr_offset = w_low * w_stride;\n-  const int w_high_ptr_offset = w_low_ptr_offset + w_stride;\n-  const int base_ptr = m * channels + c;\n-\n-  scalar_t v1 = 0;\n-  if (h_low >= 0 && w_low >= 0)\n-  {\n-    const int ptr1 = h_low_ptr_offset + w_low_ptr_offset + base_ptr;\n-    v1 = bottom_data[ptr1];\n-  }\n-  scalar_t v2 = 0;\n-  if (h_low >= 0 && w_high <= width - 1)\n-  {\n-    const int ptr2 = h_low_ptr_offset + w_high_ptr_offset + base_ptr;\n-    v2 = bottom_data[ptr2];\n-  }\n-  scalar_t v3 = 0;\n-  if (h_high <= height - 1 && w_low >= 0)\n-  {\n-    const int ptr3 = h_high_ptr_offset + w_low_ptr_offset + base_ptr;\n-    v3 = bottom_data[ptr3];\n-  }\n-  scalar_t v4 = 0;\n-  if (h_high <= height - 1 && w_high <= width - 1)\n-  {\n-    const int ptr4 = h_high_ptr_offset + w_high_ptr_offset + base_ptr;\n-    v4 = bottom_data[ptr4];\n-  }\n-\n-  const scalar_t w1 = hh * hw, w2 = hh * lw, w3 = lh * hw, w4 = lh * lw;\n-\n-  const scalar_t val = (w1 * v1 + w2 * v2 + w3 * v3 + w4 * v4);\n-  return val;\n-}\n-\n-\n-template <typename scalar_t>\n-__device__ void ms_deform_attn_col2im_bilinear(const scalar_t* &bottom_data, \n-                                                   const int &height, const int &width, const int &nheads, const int &channels,\n-                                                   const scalar_t &h, const scalar_t &w, const int &m, const int &c,\n-                                                   const scalar_t &top_grad,\n-                                                   const scalar_t &attn_weight,\n-                                                   scalar_t* &grad_value, \n-                                                   scalar_t* grad_sampling_loc,\n-                                                   scalar_t* grad_attn_weight)\n-{\n-  const int h_low = floor(h);\n-  const int w_low = floor(w);\n-  const int h_high = h_low + 1;\n-  const int w_high = w_low + 1;\n-\n-  const scalar_t lh = h - h_low;\n-  const scalar_t lw = w - w_low;\n-  const scalar_t hh = 1 - lh, hw = 1 - lw;\n-\n-  const int w_stride = nheads * channels;\n-  const int h_stride = width * w_stride;\n-  const int h_low_ptr_offset = h_low * h_stride;\n-  const int h_high_ptr_offset = h_low_ptr_offset + h_stride;\n-  const int w_low_ptr_offset = w_low * w_stride;\n-  const int w_high_ptr_offset = w_low_ptr_offset + w_stride;\n-  const int base_ptr = m * channels + c;\n-\n-  const scalar_t w1 = hh * hw, w2 = hh * lw, w3 = lh * hw, w4 = lh * lw;\n-  const scalar_t top_grad_value = top_grad * attn_weight;\n-  scalar_t grad_h_weight = 0, grad_w_weight = 0;\n-\n-  scalar_t v1 = 0;\n-  if (h_low >= 0 && w_low >= 0)\n-  {\n-    const int ptr1 = h_low_ptr_offset + w_low_ptr_offset + base_ptr;\n-    v1 = bottom_data[ptr1];\n-    grad_h_weight -= hw * v1;\n-    grad_w_weight -= hh * v1;\n-    atomicAdd(grad_value+ptr1, w1*top_grad_value);\n-  }\n-  scalar_t v2 = 0;\n-  if (h_low >= 0 && w_high <= width - 1)\n-  {\n-    const int ptr2 = h_low_ptr_offset + w_high_ptr_offset + base_ptr;\n-    v2 = bottom_data[ptr2];\n-    grad_h_weight -= lw * v2;\n-    grad_w_weight += hh * v2;\n-    atomicAdd(grad_value+ptr2, w2*top_grad_value);\n-  }\n-  scalar_t v3 = 0;\n-  if (h_high <= height - 1 && w_low >= 0)\n-  {\n-    const int ptr3 = h_high_ptr_offset + w_low_ptr_offset + base_ptr;\n-    v3 = bottom_data[ptr3];\n-    grad_h_weight += hw * v3;\n-    grad_w_weight -= lh * v3;\n-    atomicAdd(grad_value+ptr3, w3*top_grad_value); \n-  }\n-  scalar_t v4 = 0;\n-  if (h_high <= height - 1 && w_high <= width - 1)\n-  {\n-    const int ptr4 = h_high_ptr_offset + w_high_ptr_offset + base_ptr;\n-    v4 = bottom_data[ptr4];\n-    grad_h_weight += lw * v4;\n-    grad_w_weight += lh * v4;\n-    atomicAdd(grad_value+ptr4, w4*top_grad_value);\n-  }\n-\n-  const scalar_t val = (w1 * v1 + w2 * v2 + w3 * v3 + w4 * v4);\n-  *grad_attn_weight = top_grad * val;\n-  *grad_sampling_loc = width * grad_w_weight * top_grad_value;\n-  *(grad_sampling_loc + 1) = height * grad_h_weight * top_grad_value;\n-}\n-\n-\n-template <typename scalar_t>\n-__device__ void ms_deform_attn_col2im_bilinear_gm(const scalar_t* &bottom_data, \n-                                                   const int &height, const int &width, const int &nheads, const int &channels,\n-                                                   const scalar_t &h, const scalar_t &w, const int &m, const int &c,\n-                                                   const scalar_t &top_grad,\n-                                                   const scalar_t &attn_weight,\n-                                                   scalar_t* &grad_value, \n-                                                   scalar_t* grad_sampling_loc,\n-                                                   scalar_t* grad_attn_weight)\n-{\n-  const int h_low = floor(h);\n-  const int w_low = floor(w);\n-  const int h_high = h_low + 1;\n-  const int w_high = w_low + 1;\n-\n-  const scalar_t lh = h - h_low;\n-  const scalar_t lw = w - w_low;\n-  const scalar_t hh = 1 - lh, hw = 1 - lw;\n-\n-  const int w_stride = nheads * channels;\n-  const int h_stride = width * w_stride;\n-  const int h_low_ptr_offset = h_low * h_stride;\n-  const int h_high_ptr_offset = h_low_ptr_offset + h_stride;\n-  const int w_low_ptr_offset = w_low * w_stride;\n-  const int w_high_ptr_offset = w_low_ptr_offset + w_stride;\n-  const int base_ptr = m * channels + c;\n-\n-  const scalar_t w1 = hh * hw, w2 = hh * lw, w3 = lh * hw, w4 = lh * lw;\n-  const scalar_t top_grad_value = top_grad * attn_weight;\n-  scalar_t grad_h_weight = 0, grad_w_weight = 0;\n-\n-  scalar_t v1 = 0;\n-  if (h_low >= 0 && w_low >= 0)\n-  {\n-    const int ptr1 = h_low_ptr_offset + w_low_ptr_offset + base_ptr;\n-    v1 = bottom_data[ptr1];\n-    grad_h_weight -= hw * v1;\n-    grad_w_weight -= hh * v1;\n-    atomicAdd(grad_value+ptr1, w1*top_grad_value);\n-  }\n-  scalar_t v2 = 0;\n-  if (h_low >= 0 && w_high <= width - 1)\n-  {\n-    const int ptr2 = h_low_ptr_offset + w_high_ptr_offset + base_ptr;\n-    v2 = bottom_data[ptr2];\n-    grad_h_weight -= lw * v2;\n-    grad_w_weight += hh * v2;\n-    atomicAdd(grad_value+ptr2, w2*top_grad_value);\n-  }\n-  scalar_t v3 = 0;\n-  if (h_high <= height - 1 && w_low >= 0)\n-  {\n-    const int ptr3 = h_high_ptr_offset + w_low_ptr_offset + base_ptr;\n-    v3 = bottom_data[ptr3];\n-    grad_h_weight += hw * v3;\n-    grad_w_weight -= lh * v3;\n-    atomicAdd(grad_value+ptr3, w3*top_grad_value); \n-  }\n-  scalar_t v4 = 0;\n-  if (h_high <= height - 1 && w_high <= width - 1)\n-  {\n-    const int ptr4 = h_high_ptr_offset + w_high_ptr_offset + base_ptr;\n-    v4 = bottom_data[ptr4];\n-    grad_h_weight += lw * v4;\n-    grad_w_weight += lh * v4;\n-    atomicAdd(grad_value+ptr4, w4*top_grad_value);\n-  }\n-\n-  const scalar_t val = (w1 * v1 + w2 * v2 + w3 * v3 + w4 * v4);\n-  atomicAdd(grad_attn_weight, top_grad * val); \n-  atomicAdd(grad_sampling_loc, width * grad_w_weight * top_grad_value);\n-  atomicAdd(grad_sampling_loc + 1, height * grad_h_weight * top_grad_value);\n-}\n-\n-\n-template <typename scalar_t>\n-__global__ void ms_deformable_im2col_gpu_kernel(const int n,\n-                                                const scalar_t *data_value, \n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *data_col)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    scalar_t *data_col_ptr = data_col + index;\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-    scalar_t col = 0;\n-    \n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const scalar_t *data_value_ptr = data_value + (data_value_ptr_init_offset + level_start_id * qid_stride);\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          col += ms_deform_attn_im2col_bilinear(data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col) * weight;\n-        }\n-\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-      }\n-    }\n-    *data_col_ptr = col;\n-  }\n-}\n-\n-template <typename scalar_t, unsigned int blockSize>\n-__global__ void ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1(const int n,\n-                                                const scalar_t *grad_col,\n-                                                const scalar_t *data_value,\n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *grad_value,\n-                                                scalar_t *grad_sampling_loc,\n-                                                scalar_t *grad_attn_weight)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    __shared__ scalar_t cache_grad_sampling_loc[blockSize * 2];\n-    __shared__ scalar_t cache_grad_attn_weight[blockSize];\n-    unsigned int tid = threadIdx.x;\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    const scalar_t top_grad = grad_col[index];\n-\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int grad_sampling_ptr = data_weight_ptr;\n-    grad_sampling_loc += grad_sampling_ptr << 1;\n-    grad_attn_weight += grad_sampling_ptr;\n-    const int grad_weight_stride = 1;\n-    const int grad_loc_stride = 2;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-\n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const int value_ptr_offset = data_value_ptr_init_offset + level_start_id * qid_stride;\n-      const scalar_t *data_value_ptr = data_value + value_ptr_offset;\n-      scalar_t *grad_value_ptr = grad_value + value_ptr_offset;\n-\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-        *(cache_grad_sampling_loc+(threadIdx.x << 1)) = 0;\n-        *(cache_grad_sampling_loc+((threadIdx.x << 1) + 1)) = 0;\n-        *(cache_grad_attn_weight+threadIdx.x)=0;\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          ms_deform_attn_col2im_bilinear(\n-            data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col,\n-            top_grad, weight, grad_value_ptr, \n-            cache_grad_sampling_loc+(threadIdx.x << 1), cache_grad_attn_weight+threadIdx.x);\n-        }\n-        \n-        __syncthreads();\n-        if (tid == 0)\n-        {\n-          scalar_t _grad_w=cache_grad_sampling_loc[0], _grad_h=cache_grad_sampling_loc[1], _grad_a=cache_grad_attn_weight[0];\n-          int sid=2;\n-          for (unsigned int tid = 1; tid < blockSize; ++tid)\n-          {\n-            _grad_w += cache_grad_sampling_loc[sid];\n-            _grad_h += cache_grad_sampling_loc[sid + 1];\n-            _grad_a += cache_grad_attn_weight[tid];\n-            sid += 2;\n-          }\n-          \n-          \n-          *grad_sampling_loc = _grad_w;\n-          *(grad_sampling_loc + 1) = _grad_h;\n-          *grad_attn_weight = _grad_a;\n-        }\n-        __syncthreads();\n-\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-        grad_attn_weight += grad_weight_stride;\n-        grad_sampling_loc += grad_loc_stride;\n-      }\n-    }\n-  }\n-}\n-\n-\n-template <typename scalar_t, unsigned int blockSize>\n-__global__ void ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v2(const int n,\n-                                                const scalar_t *grad_col,\n-                                                const scalar_t *data_value,\n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *grad_value,\n-                                                scalar_t *grad_sampling_loc,\n-                                                scalar_t *grad_attn_weight)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    __shared__ scalar_t cache_grad_sampling_loc[blockSize * 2];\n-    __shared__ scalar_t cache_grad_attn_weight[blockSize];\n-    unsigned int tid = threadIdx.x;\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    const scalar_t top_grad = grad_col[index];\n-\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int grad_sampling_ptr = data_weight_ptr;\n-    grad_sampling_loc += grad_sampling_ptr << 1;\n-    grad_attn_weight += grad_sampling_ptr;\n-    const int grad_weight_stride = 1;\n-    const int grad_loc_stride = 2;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-\n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const int value_ptr_offset = data_value_ptr_init_offset + level_start_id * qid_stride;\n-      const scalar_t *data_value_ptr = data_value + value_ptr_offset;\n-      scalar_t *grad_value_ptr = grad_value + value_ptr_offset;\n-\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-        *(cache_grad_sampling_loc+(threadIdx.x << 1)) = 0;\n-        *(cache_grad_sampling_loc+((threadIdx.x << 1) + 1)) = 0;\n-        *(cache_grad_attn_weight+threadIdx.x)=0;\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          ms_deform_attn_col2im_bilinear(\n-            data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col,\n-            top_grad, weight, grad_value_ptr, \n-            cache_grad_sampling_loc+(threadIdx.x << 1), cache_grad_attn_weight+threadIdx.x);\n-        }\n-        \n-        __syncthreads();\n-\n-        for (unsigned int s=blockSize/2; s>0; s>>=1)\n-        {\n-          if (tid < s) {\n-            const unsigned int xid1 = tid << 1;\n-            const unsigned int xid2 = (tid + s) << 1;\n-            cache_grad_attn_weight[tid] += cache_grad_attn_weight[tid + s];\n-            cache_grad_sampling_loc[xid1] += cache_grad_sampling_loc[xid2];\n-            cache_grad_sampling_loc[xid1 + 1] += cache_grad_sampling_loc[xid2 + 1];\n-          }\n-          __syncthreads();\n-        }\n-\n-        if (tid == 0)\n-        { \n-          *grad_sampling_loc = cache_grad_sampling_loc[0];\n-          *(grad_sampling_loc + 1) = cache_grad_sampling_loc[1];\n-          *grad_attn_weight = cache_grad_attn_weight[0];\n-        }\n-        __syncthreads();\n-\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-        grad_attn_weight += grad_weight_stride;\n-        grad_sampling_loc += grad_loc_stride;\n-      }\n-    }\n-  }\n-}\n-\n-\n-template <typename scalar_t>\n-__global__ void ms_deformable_col2im_gpu_kernel_shm_reduce_v1(const int n,\n-                                                const scalar_t *grad_col,\n-                                                const scalar_t *data_value,\n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *grad_value,\n-                                                scalar_t *grad_sampling_loc,\n-                                                scalar_t *grad_attn_weight)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    extern __shared__ int _s[];\n-    scalar_t* cache_grad_sampling_loc = (scalar_t*)_s;\n-    scalar_t* cache_grad_attn_weight = cache_grad_sampling_loc + 2 * blockDim.x;\n-    unsigned int tid = threadIdx.x;\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    const scalar_t top_grad = grad_col[index];\n-\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int grad_sampling_ptr = data_weight_ptr;\n-    grad_sampling_loc += grad_sampling_ptr << 1;\n-    grad_attn_weight += grad_sampling_ptr;\n-    const int grad_weight_stride = 1;\n-    const int grad_loc_stride = 2;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-\n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const int value_ptr_offset = data_value_ptr_init_offset + level_start_id * qid_stride;\n-      const scalar_t *data_value_ptr = data_value + value_ptr_offset;\n-      scalar_t *grad_value_ptr = grad_value + value_ptr_offset;\n-\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-        *(cache_grad_sampling_loc+(threadIdx.x << 1)) = 0;\n-        *(cache_grad_sampling_loc+((threadIdx.x << 1) + 1)) = 0;\n-        *(cache_grad_attn_weight+threadIdx.x)=0;\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          ms_deform_attn_col2im_bilinear(\n-            data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col,\n-            top_grad, weight, grad_value_ptr, \n-            cache_grad_sampling_loc+(threadIdx.x << 1), cache_grad_attn_weight+threadIdx.x);\n-        }\n-        \n-        __syncthreads();\n-        if (tid == 0)\n-        {\n-          scalar_t _grad_w=cache_grad_sampling_loc[0], _grad_h=cache_grad_sampling_loc[1], _grad_a=cache_grad_attn_weight[0];\n-          int sid=2;\n-          for (unsigned int tid = 1; tid < blockDim.x; ++tid)\n-          {\n-            _grad_w += cache_grad_sampling_loc[sid];\n-            _grad_h += cache_grad_sampling_loc[sid + 1];\n-            _grad_a += cache_grad_attn_weight[tid];\n-            sid += 2;\n-          }\n-          \n-          \n-          *grad_sampling_loc = _grad_w;\n-          *(grad_sampling_loc + 1) = _grad_h;\n-          *grad_attn_weight = _grad_a;\n-        }\n-        __syncthreads();\n-\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-        grad_attn_weight += grad_weight_stride;\n-        grad_sampling_loc += grad_loc_stride;\n-      }\n-    }\n-  }\n-}\n-\n-template <typename scalar_t>\n-__global__ void ms_deformable_col2im_gpu_kernel_shm_reduce_v2(const int n,\n-                                                const scalar_t *grad_col,\n-                                                const scalar_t *data_value,\n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *grad_value,\n-                                                scalar_t *grad_sampling_loc,\n-                                                scalar_t *grad_attn_weight)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    extern __shared__ int _s[];\n-    scalar_t* cache_grad_sampling_loc = (scalar_t*)_s;\n-    scalar_t* cache_grad_attn_weight = cache_grad_sampling_loc + 2 * blockDim.x;\n-    unsigned int tid = threadIdx.x;\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    const scalar_t top_grad = grad_col[index];\n-\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int grad_sampling_ptr = data_weight_ptr;\n-    grad_sampling_loc += grad_sampling_ptr << 1;\n-    grad_attn_weight += grad_sampling_ptr;\n-    const int grad_weight_stride = 1;\n-    const int grad_loc_stride = 2;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-\n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const int value_ptr_offset = data_value_ptr_init_offset + level_start_id * qid_stride;\n-      const scalar_t *data_value_ptr = data_value + value_ptr_offset;\n-      scalar_t *grad_value_ptr = grad_value + value_ptr_offset;\n-\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-        *(cache_grad_sampling_loc+(threadIdx.x << 1)) = 0;\n-        *(cache_grad_sampling_loc+((threadIdx.x << 1) + 1)) = 0;\n-        *(cache_grad_attn_weight+threadIdx.x)=0;\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          ms_deform_attn_col2im_bilinear(\n-            data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col,\n-            top_grad, weight, grad_value_ptr, \n-            cache_grad_sampling_loc+(threadIdx.x << 1), cache_grad_attn_weight+threadIdx.x);\n-        }\n-        \n-        __syncthreads();\n-\n-        for (unsigned int s=blockDim.x/2, spre=blockDim.x; s>0; s>>=1, spre>>=1)\n-        {\n-          if (tid < s) {\n-            const unsigned int xid1 = tid << 1;\n-            const unsigned int xid2 = (tid + s) << 1;\n-            cache_grad_attn_weight[tid] += cache_grad_attn_weight[tid + s];\n-            cache_grad_sampling_loc[xid1] += cache_grad_sampling_loc[xid2];\n-            cache_grad_sampling_loc[xid1 + 1] += cache_grad_sampling_loc[xid2 + 1];\n-            if (tid + (s << 1) < spre)\n-            {\n-              cache_grad_attn_weight[tid] += cache_grad_attn_weight[tid + (s << 1)];\n-              cache_grad_sampling_loc[xid1] += cache_grad_sampling_loc[xid2 + (s << 1)];\n-              cache_grad_sampling_loc[xid1 + 1] += cache_grad_sampling_loc[xid2 + 1 + (s << 1)];\n-            } \n-          }\n-          __syncthreads();\n-        }\n-\n-        if (tid == 0)\n-        {\n-          *grad_sampling_loc = cache_grad_sampling_loc[0];\n-          *(grad_sampling_loc + 1) = cache_grad_sampling_loc[1];\n-          *grad_attn_weight = cache_grad_attn_weight[0];\n-        }\n-        __syncthreads();\n-\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-        grad_attn_weight += grad_weight_stride;\n-        grad_sampling_loc += grad_loc_stride;\n-      }\n-    }\n-  }\n-}\n-\n-template <typename scalar_t>\n-__global__ void ms_deformable_col2im_gpu_kernel_shm_reduce_v2_multi_blocks(const int n,\n-                                                const scalar_t *grad_col,\n-                                                const scalar_t *data_value,\n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *grad_value,\n-                                                scalar_t *grad_sampling_loc,\n-                                                scalar_t *grad_attn_weight)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    extern __shared__ int _s[];\n-    scalar_t* cache_grad_sampling_loc = (scalar_t*)_s;\n-    scalar_t* cache_grad_attn_weight = cache_grad_sampling_loc + 2 * blockDim.x;\n-    unsigned int tid = threadIdx.x;\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    const scalar_t top_grad = grad_col[index];\n-\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int grad_sampling_ptr = data_weight_ptr;\n-    grad_sampling_loc += grad_sampling_ptr << 1;\n-    grad_attn_weight += grad_sampling_ptr;\n-    const int grad_weight_stride = 1;\n-    const int grad_loc_stride = 2;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-\n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const int value_ptr_offset = data_value_ptr_init_offset + level_start_id * qid_stride;\n-      const scalar_t *data_value_ptr = data_value + value_ptr_offset;\n-      scalar_t *grad_value_ptr = grad_value + value_ptr_offset;\n-\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-        *(cache_grad_sampling_loc+(threadIdx.x << 1)) = 0;\n-        *(cache_grad_sampling_loc+((threadIdx.x << 1) + 1)) = 0;\n-        *(cache_grad_attn_weight+threadIdx.x)=0;\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          ms_deform_attn_col2im_bilinear(\n-            data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col,\n-            top_grad, weight, grad_value_ptr, \n-            cache_grad_sampling_loc+(threadIdx.x << 1), cache_grad_attn_weight+threadIdx.x);\n-        }\n-        \n-        __syncthreads();\n-\n-        for (unsigned int s=blockDim.x/2, spre=blockDim.x; s>0; s>>=1, spre>>=1)\n-        {\n-          if (tid < s) {\n-            const unsigned int xid1 = tid << 1;\n-            const unsigned int xid2 = (tid + s) << 1;\n-            cache_grad_attn_weight[tid] += cache_grad_attn_weight[tid + s];\n-            cache_grad_sampling_loc[xid1] += cache_grad_sampling_loc[xid2];\n-            cache_grad_sampling_loc[xid1 + 1] += cache_grad_sampling_loc[xid2 + 1];\n-            if (tid + (s << 1) < spre)\n-            {\n-              cache_grad_attn_weight[tid] += cache_grad_attn_weight[tid + (s << 1)];\n-              cache_grad_sampling_loc[xid1] += cache_grad_sampling_loc[xid2 + (s << 1)];\n-              cache_grad_sampling_loc[xid1 + 1] += cache_grad_sampling_loc[xid2 + 1 + (s << 1)];\n-            }\n-          }\n-          __syncthreads();\n-        }\n-\n-        if (tid == 0)\n-        {\n-          atomicAdd(grad_sampling_loc, cache_grad_sampling_loc[0]);\n-          atomicAdd(grad_sampling_loc + 1, cache_grad_sampling_loc[1]);\n-          atomicAdd(grad_attn_weight, cache_grad_attn_weight[0]);\n-        }\n-        __syncthreads();\n-\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-        grad_attn_weight += grad_weight_stride;\n-        grad_sampling_loc += grad_loc_stride;\n-      }\n-    }\n-  }\n-}\n-\n-\n-template <typename scalar_t>\n-__global__ void ms_deformable_col2im_gpu_kernel_gm(const int n,\n-                                                const scalar_t *grad_col,\n-                                                const scalar_t *data_value,\n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *grad_value,\n-                                                scalar_t *grad_sampling_loc,\n-                                                scalar_t *grad_attn_weight)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    const scalar_t top_grad = grad_col[index];\n-\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int grad_sampling_ptr = data_weight_ptr;\n-    grad_sampling_loc += grad_sampling_ptr << 1;\n-    grad_attn_weight += grad_sampling_ptr;\n-    const int grad_weight_stride = 1;\n-    const int grad_loc_stride = 2;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-\n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const int value_ptr_offset = data_value_ptr_init_offset + level_start_id * qid_stride;\n-      const scalar_t *data_value_ptr = data_value + value_ptr_offset;\n-      scalar_t *grad_value_ptr = grad_value + value_ptr_offset;\n-\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          ms_deform_attn_col2im_bilinear_gm(\n-            data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col,\n-            top_grad, weight, grad_value_ptr, \n-            grad_sampling_loc, grad_attn_weight);\n-        }\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-        grad_attn_weight += grad_weight_stride;\n-        grad_sampling_loc += grad_loc_stride;\n-      }\n-    }\n-  }\n-}\n-\n-\n-template <typename scalar_t>\n-void ms_deformable_im2col_cuda(cudaStream_t stream,\n-                              const scalar_t* data_value,\n-                              const int64_t* data_spatial_shapes, \n-                              const int64_t* data_level_start_index, \n-                              const scalar_t* data_sampling_loc,\n-                              const scalar_t* data_attn_weight,\n-                              const int batch_size,\n-                              const int spatial_size, \n-                              const int num_heads, \n-                              const int channels, \n-                              const int num_levels, \n-                              const int num_query,\n-                              const int num_point,\n-                              scalar_t* data_col)\n-{\n-  const int num_kernels = batch_size * num_query * num_heads * channels;\n-  const int num_actual_kernels = batch_size * num_query * num_heads * channels;\n-  const int num_threads = CUDA_NUM_THREADS;\n-  ms_deformable_im2col_gpu_kernel<scalar_t>\n-      <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-          0, stream>>>(\n-      num_kernels, data_value, data_spatial_shapes, data_level_start_index, data_sampling_loc, data_attn_weight, \n-      batch_size, spatial_size, num_heads, channels, num_levels, num_query, num_point, data_col);\n-  \n-  cudaError_t err = cudaGetLastError();\n-  if (err != cudaSuccess)\n-  {\n-    printf(\"error in ms_deformable_im2col_cuda: %s\\n\", cudaGetErrorString(err));\n-  }\n-\n-}\n-\n-template <typename scalar_t>\n-void ms_deformable_col2im_cuda(cudaStream_t stream,\n-                              const scalar_t* grad_col,\n-                              const scalar_t* data_value,\n-                              const int64_t * data_spatial_shapes,\n-                              const int64_t * data_level_start_index,\n-                              const scalar_t * data_sampling_loc,\n-                              const scalar_t * data_attn_weight,\n-                              const int batch_size, \n-                              const int spatial_size, \n-                              const int num_heads,\n-                              const int channels, \n-                              const int num_levels,\n-                              const int num_query,\n-                              const int num_point, \n-                              scalar_t* grad_value,\n-                              scalar_t* grad_sampling_loc,\n-                              scalar_t* grad_attn_weight)\n-{\n-  const int num_threads = (channels > CUDA_NUM_THREADS)?CUDA_NUM_THREADS:channels;\n-  const int num_kernels = batch_size * num_query * num_heads * channels;\n-  const int num_actual_kernels = batch_size * num_query * num_heads * channels;\n-  if (channels > 1024)\n-  {\n-    if ((channels & 1023) == 0)\n-    {\n-      ms_deformable_col2im_gpu_kernel_shm_reduce_v2_multi_blocks<scalar_t>\n-          <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-              num_threads*3*sizeof(scalar_t), stream>>>(\n-                        num_kernels, \n-                        grad_col,\n-                        data_value,\n-                        data_spatial_shapes,\n-                        data_level_start_index, \n-                        data_sampling_loc,\n-                        data_attn_weight,\n-                        batch_size, \n-                        spatial_size, \n-                        num_heads,\n-                        channels, \n-                        num_levels,\n-                        num_query,\n-                        num_point,\n-                        grad_value,\n-                        grad_sampling_loc,\n-                        grad_attn_weight);\n-    }\n-    else\n-    {\n-      ms_deformable_col2im_gpu_kernel_gm<scalar_t>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-    }\n-  }\n-  else{\n-    switch(channels)\n-    {\n-      case 1:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1<scalar_t, 1>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 2:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1<scalar_t, 2>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 4:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1<scalar_t, 4>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 8:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1<scalar_t, 8>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 16:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1<scalar_t, 16>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 32:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1<scalar_t, 32>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 64:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v2<scalar_t, 64>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 128:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v2<scalar_t, 128>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 256:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v2<scalar_t, 256>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 512:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v2<scalar_t, 512>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 1024:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v2<scalar_t, 1024>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      default:\n-        if (channels < 64)\n-        {\n-          ms_deformable_col2im_gpu_kernel_shm_reduce_v1<scalar_t>\n-          <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-              num_threads*3*sizeof(scalar_t), stream>>>(\n-                        num_kernels, \n-                        grad_col,\n-                        data_value,\n-                        data_spatial_shapes,\n-                        data_level_start_index, \n-                        data_sampling_loc,\n-                        data_attn_weight,\n-                        batch_size, \n-                        spatial_size, \n-                        num_heads,\n-                        channels, \n-                        num_levels,\n-                        num_query,\n-                        num_point,\n-                        grad_value,\n-                        grad_sampling_loc,\n-                        grad_attn_weight);\n-        }\n-        else\n-        {\n-          ms_deformable_col2im_gpu_kernel_shm_reduce_v2<scalar_t>\n-          <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-              num_threads*3*sizeof(scalar_t), stream>>>(\n-                        num_kernels, \n-                        grad_col,\n-                        data_value,\n-                        data_spatial_shapes,\n-                        data_level_start_index, \n-                        data_sampling_loc,\n-                        data_attn_weight,\n-                        batch_size, \n-                        spatial_size, \n-                        num_heads,\n-                        channels, \n-                        num_levels,\n-                        num_query,\n-                        num_point,\n-                        grad_value,\n-                        grad_sampling_loc,\n-                        grad_attn_weight);\n-        }\n-    }\n-  }\n-  cudaError_t err = cudaGetLastError();\n-  if (err != cudaSuccess)\n-  {\n-    printf(\"error in ms_deformable_col2im_cuda: %s\\n\", cudaGetErrorString(err));\n-  }\n-\n-}"
      },
      {
        "filename": "src/transformers/kernels/deta/ms_deform_attn.h",
        "status": "removed",
        "additions": 0,
        "deletions": 61,
        "changes": 61,
        "patch": "@@ -1,61 +0,0 @@\n-/*!\n-**************************************************************************************************\n-* Deformable DETR\n-* Copyright (c) 2020 SenseTime. All Rights Reserved.\n-* Licensed under the Apache License, Version 2.0 [see LICENSE for details]\n-**************************************************************************************************\n-* Modified from https://github.com/chengdazhi/Deformable-Convolution-V2-PyTorch/tree/pytorch_1.0.0\n-**************************************************************************************************\n-*/\n-\n-#pragma once\n-\n-#include \"cpu/ms_deform_attn_cpu.h\"\n-\n-#ifdef WITH_CUDA\n-#include \"cuda/ms_deform_attn_cuda.h\"\n-#endif\n-\n-\n-at::Tensor\n-ms_deform_attn_forward(\n-    const at::Tensor &value, \n-    const at::Tensor &spatial_shapes,\n-    const at::Tensor &level_start_index,\n-    const at::Tensor &sampling_loc,\n-    const at::Tensor &attn_weight,\n-    const int im2col_step)\n-{\n-    if (value.type().is_cuda())\n-    {\n-#ifdef WITH_CUDA\n-        return ms_deform_attn_cuda_forward(\n-            value, spatial_shapes, level_start_index, sampling_loc, attn_weight, im2col_step);\n-#else\n-        AT_ERROR(\"Not compiled with GPU support\");\n-#endif\n-    }\n-    AT_ERROR(\"Not implemented on the CPU\");\n-}\n-\n-std::vector<at::Tensor>\n-ms_deform_attn_backward(\n-    const at::Tensor &value, \n-    const at::Tensor &spatial_shapes,\n-    const at::Tensor &level_start_index,\n-    const at::Tensor &sampling_loc,\n-    const at::Tensor &attn_weight,\n-    const at::Tensor &grad_output,\n-    const int im2col_step)\n-{\n-    if (value.type().is_cuda())\n-    {\n-#ifdef WITH_CUDA\n-        return ms_deform_attn_cuda_backward(\n-            value, spatial_shapes, level_start_index, sampling_loc, attn_weight, grad_output, im2col_step);\n-#else\n-        AT_ERROR(\"Not compiled with GPU support\");\n-#endif\n-    }\n-    AT_ERROR(\"Not implemented on the CPU\");\n-}"
      },
      {
        "filename": "src/transformers/kernels/deta/vision.cpp",
        "status": "removed",
        "additions": 0,
        "deletions": 16,
        "changes": 16,
        "patch": "@@ -1,16 +0,0 @@\n-/*!\n-**************************************************************************************************\n-* Deformable DETR\n-* Copyright (c) 2020 SenseTime. All Rights Reserved.\n-* Licensed under the Apache License, Version 2.0 [see LICENSE for details]\n-**************************************************************************************************\n-* Modified from https://github.com/chengdazhi/Deformable-Convolution-V2-PyTorch/tree/pytorch_1.0.0\n-**************************************************************************************************\n-*/\n-\n-#include \"ms_deform_attn.h\"\n-\n-PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n-  m.def(\"ms_deform_attn_forward\", &ms_deform_attn_forward, \"ms_deform_attn_forward\");\n-  m.def(\"ms_deform_attn_backward\", &ms_deform_attn_backward, \"ms_deform_attn_backward\");\n-}\n\\ No newline at end of file"
      },
      {
        "filename": "src/transformers/models/deprecated/deta/modeling_deta.py",
        "status": "modified",
        "additions": 59,
        "deletions": 103,
        "changes": 162,
        "patch": "@@ -16,119 +16,89 @@\n \n import copy\n import math\n-import os\n import warnings\n from dataclasses import dataclass\n-from pathlib import Path\n from typing import Optional, Union\n \n import torch\n import torch.nn.functional as F\n from torch import Tensor, nn\n-from torch.autograd import Function\n-from torch.autograd.function import once_differentiable\n \n from ....activations import ACT2FN\n from ....file_utils import (\n     ModelOutput,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     is_scipy_available,\n-    is_torch_cuda_available,\n     is_vision_available,\n     replace_return_docstrings,\n )\n+from ....integrations.hub_kernels import use_kernel_forward_from_hub\n from ....modeling_attn_mask_utils import _prepare_4d_attention_mask\n from ....modeling_layers import GradientCheckpointingLayer\n from ....modeling_outputs import BaseModelOutput\n from ....modeling_utils import PreTrainedModel\n from ....pytorch_utils import meshgrid\n-from ....utils import is_accelerate_available, is_ninja_available, is_torchvision_available, logging, requires_backends\n+from ....utils import is_accelerate_available, is_torchvision_available, logging, requires_backends\n from ....utils.backbone_utils import load_backbone\n from .configuration_deta import DetaConfig\n \n \n logger = logging.get_logger(__name__)\n \n-MultiScaleDeformableAttention = None\n \n-\n-def load_cuda_kernels():\n-    from torch.utils.cpp_extension import load\n-\n-    global MultiScaleDeformableAttention\n-\n-    root = Path(__file__).resolve().parent.parent.parent.parent / \"kernels\" / \"deta\"\n-    src_files = [\n-        root / filename\n-        for filename in [\n-            \"vision.cpp\",\n-            os.path.join(\"cpu\", \"ms_deform_attn_cpu.cpp\"),\n-            os.path.join(\"cuda\", \"ms_deform_attn_cuda.cu\"),\n-        ]\n-    ]\n-\n-    MultiScaleDeformableAttention = load(\n-        \"MultiScaleDeformableAttention\",\n-        src_files,\n-        with_cuda=True,\n-        extra_include_paths=[str(root)],\n-        extra_cflags=[\"-DWITH_CUDA=1\"],\n-        extra_cuda_cflags=[\n-            \"-DCUDA_HAS_FP16=1\",\n-            \"-D__CUDA_NO_HALF_OPERATORS__\",\n-            \"-D__CUDA_NO_HALF_CONVERSIONS__\",\n-            \"-D__CUDA_NO_HALF2_OPERATORS__\",\n-        ],\n-    )\n-\n-\n-class MultiScaleDeformableAttentionFunction(Function):\n-    @staticmethod\n+@use_kernel_forward_from_hub(\"MultiScaleDeformableAttention\")\n+class MultiScaleDeformableAttention(nn.Module):\n     def forward(\n-        context,\n-        value,\n-        value_spatial_shapes,\n-        value_level_start_index,\n-        sampling_locations,\n-        attention_weights,\n-        im2col_step,\n+        self,\n+        value: Tensor,\n+        value_spatial_shapes: Tensor,\n+        level_start_index: Tensor,\n+        sampling_locations: Tensor,\n+        attention_weights: Tensor,\n+        im2col_step: int,\n     ):\n-        context.im2col_step = im2col_step\n-        output = MultiScaleDeformableAttention.ms_deform_attn_forward(\n-            value,\n-            value_spatial_shapes,\n-            value_level_start_index,\n-            sampling_locations,\n-            attention_weights,\n-            context.im2col_step,\n-        )\n-        context.save_for_backward(\n-            value, value_spatial_shapes, value_level_start_index, sampling_locations, attention_weights\n+        batch_size, _, num_heads, hidden_dim = value.shape\n+        _, num_queries, num_heads, num_levels, num_points, _ = sampling_locations.shape\n+        value_list = value.split([height * width for height, width in value_spatial_shapes], dim=1)\n+        sampling_grids = 2 * sampling_locations - 1\n+        sampling_value_list = []\n+        for level_id, (height, width) in enumerate(value_spatial_shapes):\n+            # batch_size, height*width, num_heads, hidden_dim\n+            # -> batch_size, height*width, num_heads*hidden_dim\n+            # -> batch_size, num_heads*hidden_dim, height*width\n+            # -> batch_size*num_heads, hidden_dim, height, width\n+            value_l_ = (\n+                value_list[level_id]\n+                .flatten(2)\n+                .transpose(1, 2)\n+                .reshape(batch_size * num_heads, hidden_dim, height, width)\n+            )\n+            # batch_size, num_queries, num_heads, num_points, 2\n+            # -> batch_size, num_heads, num_queries, num_points, 2\n+            # -> batch_size*num_heads, num_queries, num_points, 2\n+            sampling_grid_l_ = sampling_grids[:, :, :, level_id].transpose(1, 2).flatten(0, 1)\n+            # batch_size*num_heads, hidden_dim, num_queries, num_points\n+            sampling_value_l_ = nn.functional.grid_sample(\n+                value_l_,\n+                sampling_grid_l_,\n+                mode=\"bilinear\",\n+                padding_mode=\"zeros\",\n+                align_corners=False,\n+            )\n+            sampling_value_list.append(sampling_value_l_)\n+        # (batch_size, num_queries, num_heads, num_levels, num_points)\n+        # -> (batch_size, num_heads, num_queries, num_levels, num_points)\n+        # -> (batch_size, num_heads, 1, num_queries, num_levels*num_points)\n+        attention_weights = attention_weights.transpose(1, 2).reshape(\n+            batch_size * num_heads, 1, num_queries, num_levels * num_points\n         )\n-        return output\n-\n-    @staticmethod\n-    @once_differentiable\n-    def backward(context, grad_output):\n-        (\n-            value,\n-            value_spatial_shapes,\n-            value_level_start_index,\n-            sampling_locations,\n-            attention_weights,\n-        ) = context.saved_tensors\n-        grad_value, grad_sampling_loc, grad_attn_weight = MultiScaleDeformableAttention.ms_deform_attn_backward(\n-            value,\n-            value_spatial_shapes,\n-            value_level_start_index,\n-            sampling_locations,\n-            attention_weights,\n-            grad_output,\n-            context.im2col_step,\n+        output = (\n+            (torch.stack(sampling_value_list, dim=-2).flatten(-2) * attention_weights)\n+            .sum(-1)\n+            .view(batch_size, num_heads * hidden_dim, num_queries)\n         )\n-\n-        return grad_value, None, None, grad_sampling_loc, grad_attn_weight, None\n+        return output.transpose(1, 2).contiguous()\n \n \n if is_accelerate_available():\n@@ -571,12 +541,7 @@ class DetaMultiscaleDeformableAttention(nn.Module):\n     def __init__(self, config: DetaConfig, num_heads: int, n_points: int):\n         super().__init__()\n \n-        kernel_loaded = MultiScaleDeformableAttention is not None\n-        if is_torch_cuda_available() and is_ninja_available() and not kernel_loaded:\n-            try:\n-                load_cuda_kernels()\n-            except Exception as e:\n-                logger.warning(f\"Could not load the custom kernel for multi-scale deformable attention: {e}\")\n+        self.attn = MultiScaleDeformableAttention()\n \n         if config.d_model % num_heads != 0:\n             raise ValueError(\n@@ -684,23 +649,14 @@ def forward(\n         else:\n             raise ValueError(f\"Last dim of reference_points must be 2 or 4, but got {reference_points.shape[-1]}\")\n \n-        if self.disable_custom_kernels:\n-            # PyTorch implementation\n-            output = multi_scale_deformable_attention(value, spatial_shapes, sampling_locations, attention_weights)\n-        else:\n-            try:\n-                # custom kernel\n-                output = MultiScaleDeformableAttentionFunction.apply(\n-                    value,\n-                    spatial_shapes,\n-                    level_start_index,\n-                    sampling_locations,\n-                    attention_weights,\n-                    self.im2col_step,\n-                )\n-            except Exception:\n-                # PyTorch implementation\n-                output = multi_scale_deformable_attention(value, spatial_shapes, sampling_locations, attention_weights)\n+        output = self.attn(\n+            value,\n+            spatial_shapes,\n+            level_start_index,\n+            sampling_locations,\n+            attention_weights,\n+            self.im2col_step,\n+        )\n         output = self.output_proj(output)\n \n         return output, attention_weights"
      }
    ],
    "num_files": 9,
    "scraped_at": "2025-11-16T21:17:56.707387"
  },
  {
    "pr_number": 41449,
    "title": "Fix trainer simple tests",
    "body": "# What does this PR do?\r\n\r\nThis PR should all simple trainer tests and some deepspeed tests. \r\nThe only remaining tests to fix are deepspeed z2 grad acc tests but this is strange why it is failing ... cc @IlyasMoutawwakil maybe you have an idea as you worked on it for `HPU` ",
    "html_url": "https://github.com/huggingface/transformers/pull/41449",
    "created_at": "2025-10-08T12:49:08Z",
    "merged_at": "2025-10-15T12:09:00Z",
    "merge_commit_sha": "70e871959c3ced65ee4804a55fb27b37876db2bf",
    "base_ref": "main",
    "head_sha": "9d556dd8a2b26cc511250fcb30442a0d2699e1a4",
    "user": "SunMarc",
    "files": [
      {
        "filename": "src/transformers/integrations/integration_utils.py",
        "status": "modified",
        "additions": 7,
        "deletions": 5,
        "changes": 12,
        "patch": "@@ -302,7 +302,7 @@ def run_hp_search_ray(trainer, n_trials: int, direction: str, **kwargs) -> BestR\n             for more options\n     \"\"\"\n     import ray\n-    import ray.train\n+    import ray.tune\n \n     def _objective(trial: dict, local_trainer):\n         try:\n@@ -315,7 +315,7 @@ def _objective(trial: dict, local_trainer):\n \n         local_trainer.objective = None\n \n-        checkpoint = ray.train.get_checkpoint()\n+        checkpoint = ray.tune.get_checkpoint()\n         if checkpoint:\n             # Upon trial resume, the local_trainer's objective gets reset to None.\n             # If `local_trainer.train` is a noop (training has already reached\n@@ -339,8 +339,8 @@ def _objective(trial: dict, local_trainer):\n \n             with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n                 local_trainer._tune_save_checkpoint(checkpoint_dir=temp_checkpoint_dir)\n-                checkpoint = ray.train.Checkpoint.from_directory(temp_checkpoint_dir)\n-                ray.train.report(metrics, checkpoint=checkpoint)\n+                checkpoint = ray.tune.Checkpoint.from_directory(temp_checkpoint_dir)\n+                ray.tune.report(metrics, checkpoint=checkpoint)\n \n     if not trainer._memory_tracker.skip_memory_metrics:\n         from ..trainer_utils import TrainerMemoryTracker\n@@ -406,7 +406,9 @@ def dynamic_modules_import_trainable(*args, **kwargs):\n \n         Assumes that `_objective`, defined above, is a function.\n         \"\"\"\n-        if is_datasets_available():\n+        if is_datasets_available() and packaging.version.parse(\n+            importlib.metadata.version(\"datasets\")\n+        ) < packaging.version.parse(\"4.0.0\"):\n             import datasets.load\n \n             dynamic_modules_path = os.path.join(datasets.load.init_dynamic_modules(), \"__init__.py\")"
      },
      {
        "filename": "src/transformers/modeling_utils.py",
        "status": "modified",
        "additions": 7,
        "deletions": 3,
        "changes": 10,
        "patch": "@@ -5140,12 +5140,15 @@ def set_is_initialized_for_modules(module):\n             # A module is already initialized if and only if all its children are also already initialized, and all\n             # its immediate `nn.Parameter` and persistent buffers are also already initialized\n             if (\n+                # All immediate children are initialized\n                 all(getattr(child, \"_is_hf_initialized\", False) for child in module.children())\n+                # All immediate parameters are initialized\n                 and all(getattr(param, \"_is_hf_initialized\", False) for param in module.parameters(recurse=False))\n+                # All immediate persistent buffers are initialized\n                 and all(\n                     getattr(buffer, \"_is_hf_initialized\", False)\n-                    for buffer in module.buffers(recurse=False)\n-                    if buffer not in module._non_persistent_buffers_set\n+                    for name, buffer in module.named_buffers(recurse=False)\n+                    if name not in module._non_persistent_buffers_set\n                 )\n             ):\n                 module._is_hf_initialized = True\n@@ -5159,8 +5162,9 @@ def set_is_initialized_for_modules(module):\n         if is_deepspeed_zero3_enabled() and not is_quantized:\n             import deepspeed\n \n+            # keep_vars=True as we need the original tensors, so that the \"_is_hf_initialized\" is present on them\n             not_initialized_parameters = list(\n-                {v for v in self.state_dict().values() if not getattr(v, \"_is_hf_initialized\", False)}\n+                {v for v in self.state_dict(keep_vars=True).values() if not getattr(v, \"_is_hf_initialized\", False)}\n             )\n             with deepspeed.zero.GatheredParameters(not_initialized_parameters, modifier_rank=0):\n                 self.initialize_weights()"
      },
      {
        "filename": "src/transformers/trainer.py",
        "status": "modified",
        "additions": 9,
        "deletions": 8,
        "changes": 17,
        "patch": "@@ -1846,15 +1846,15 @@ def _report_to_hp_search(self, trial: Union[\"optuna.Trial\", dict[str, Any]], ste\n                     self.callback_handler.on_train_end(self.args, self.state, self.control)\n                     raise optuna.TrialPruned()\n         elif self.hp_search_backend == HPSearchBackend.RAY:\n-            import ray.train\n+            import ray.tune\n \n             with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n                 checkpoint = None\n                 if self.control.should_save:\n                     self._tune_save_checkpoint(checkpoint_dir=temp_checkpoint_dir)\n-                    checkpoint = ray.train.Checkpoint.from_directory(temp_checkpoint_dir)\n+                    checkpoint = ray.tune.Checkpoint.from_directory(temp_checkpoint_dir)\n                 metrics[\"objective\"] = self.objective\n-                ray.train.report(metrics, checkpoint=checkpoint)\n+                ray.tune.report(metrics, checkpoint=checkpoint)\n \n     def _tune_save_checkpoint(self, checkpoint_dir: str):\n         output_dir = os.path.join(checkpoint_dir, f\"{PREFIX_CHECKPOINT_DIR}-{self.state.global_step}\")\n@@ -2654,9 +2654,9 @@ def _get_output_dir(self, trial):\n             if self.hp_search_backend == HPSearchBackend.OPTUNA:\n                 run_id = trial.number\n             elif self.hp_search_backend == HPSearchBackend.RAY:\n-                import ray.train\n+                import ray.tune\n \n-                run_id = ray.train.get_context().get_trial_id()\n+                run_id = ray.tune.get_context().get_trial_id()\n             elif self.hp_search_backend == HPSearchBackend.WANDB:\n                 import wandb\n \n@@ -5099,9 +5099,10 @@ def _get_num_items_in_batch(self, batch_samples: list, device: torch.device) ->\n                 pass\n \n         if num_items_in_batch is not None:\n-            if self.args.average_tokens_across_devices and self.args.world_size >= 1:\n-                num_items_in_batch = self.accelerator.gather(num_items_in_batch.to(device)).sum()\n-            elif self.args.n_gpu >= 1:\n+            if self.args.average_tokens_across_devices:\n+                if self.args.world_size > 1:\n+                    num_items_in_batch = self.accelerator.gather(num_items_in_batch.to(device)).sum()\n+            elif self.args.n_gpu > 1:\n                 # In DP case, if we don't average, we need to divide by the number of gpu. This is the simplest approximation.\n                 # Otherwise, we would have to scatter labels and calculate num_items_in_batch for each gpu.\n                 num_items_in_batch = num_items_in_batch // self.args.n_gpu"
      },
      {
        "filename": "tests/deepspeed/test_model_zoo.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -182,7 +182,7 @@ def make_task_cmds():\n             \"pegasus\",\n         ],\n         \"clm\": [\n-            \"big_bird\",\n+            # \"big_bird\", not use why there is an issue with the architecture, some modules are not ZeROOrderedDict suddenly\n             \"bigbird_pegasus\",\n             \"blenderbot\",\n             \"bloom\","
      },
      {
        "filename": "tests/trainer/test_trainer.py",
        "status": "modified",
        "additions": 55,
        "deletions": 146,
        "changes": 201,
        "patch": "@@ -46,7 +46,6 @@\n     TrainerCallback,\n     TrainingArguments,\n     default_data_collator,\n-    enable_full_determinism,\n     get_polynomial_decay_schedule_with_warmup,\n     is_datasets_available,\n     is_torch_available,\n@@ -67,10 +66,8 @@\n     backend_max_memory_allocated,\n     backend_memory_allocated,\n     backend_reset_max_memory_allocated,\n-    backend_reset_peak_memory_stats,\n     evaluate_side_effect_factory,\n     execute_subprocess_async,\n-    get_gpu_count,\n     get_steps_per_epoch,\n     get_tests_dir,\n     is_staging_test,\n@@ -97,9 +94,7 @@\n     require_torch_gpu,\n     require_torch_multi_accelerator,\n     require_torch_non_multi_accelerator,\n-    require_torch_non_multi_gpu,\n     require_torch_optimi,\n-    require_torch_tensorrt_fx,\n     require_torch_tf32,\n     require_torch_up_to_2_accelerators,\n     require_vision,\n@@ -580,7 +575,7 @@ def get_regression_trainer(\n         preprocess_logits_for_metrics = kwargs.pop(\"preprocess_logits_for_metrics\", None)\n         assert output_dir is not None, \"output_dir should be specified for testing\"\n         args = RegressionTrainingArguments(output_dir, a=a, b=b, keep_report_to=keep_report_to, **kwargs)\n-        return Trainer(\n+        trainer = Trainer(\n             model,\n             args,\n             data_collator=data_collator,\n@@ -591,6 +586,9 @@ def get_regression_trainer(\n             model_init=model_init,\n             preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n         )\n+        # TODO: loss function defined in RegressionModel doesn't accept num_item_per_batch, to fix later\n+        trainer.model_accepts_loss_kwargs = False\n+        return trainer\n \n     def get_language_model_trainer(**kwargs):\n         dataset = datasets.load_dataset(\"fka/awesome-chatgpt-prompts\")\n@@ -1948,44 +1946,54 @@ def test_use_liger_kernel_custom_config_patching(self):\n \n     @require_liger_kernel\n     @require_torch_accelerator\n+    @require_torch_non_multi_accelerator  # Don't work with DP\n     def test_use_liger_kernel_trainer(self):\n-        # Check that trainer still works with liger kernel applied\n-        config = LlamaConfig(vocab_size=100, hidden_size=32, num_hidden_layers=3, num_attention_heads=4)\n-        tiny_llama = LlamaForCausalLM(config)\n+        # Ensure any monkey patching is cleaned up for subsequent tests\n+        with patch(\"transformers.models.llama.modeling_llama\"):\n+            # Check that trainer still works with liger kernel applied\n+            config = LlamaConfig(vocab_size=100, hidden_size=32, num_hidden_layers=3, num_attention_heads=4)\n+            tiny_llama = LlamaForCausalLM(config)\n \n-        x = torch.randint(0, 100, (128,))\n-        train_dataset = RepeatDataset(x)\n+            x = torch.randint(0, 100, (128,))\n+            train_dataset = RepeatDataset(x)\n \n-        args = TrainingArguments(\n-            self.get_auto_remove_tmp_dir(), learning_rate=1e-2, logging_steps=5, max_steps=20, use_liger_kernel=True\n-        )\n-        trainer = Trainer(tiny_llama, args, train_dataset=train_dataset)\n+            args = TrainingArguments(\n+                self.get_auto_remove_tmp_dir(),\n+                learning_rate=1e-2,\n+                logging_steps=5,\n+                max_steps=20,\n+                use_liger_kernel=True,\n+            )\n+            trainer = Trainer(tiny_llama, args, train_dataset=train_dataset)\n \n-        # Check this works\n-        _ = trainer.train()\n+            # Check this works\n+            _ = trainer.train()\n \n     @require_liger_kernel\n     @require_torch_accelerator\n+    @require_torch_non_multi_accelerator  # don't work with DP\n     def test_use_liger_kernel_custom_config_trainer(self):\n-        # Check that trainer still works with liger kernel applied when using a custom config\n-        config = LlamaConfig(vocab_size=100, hidden_size=32, num_hidden_layers=3, num_attention_heads=4)\n-        tiny_llama = LlamaForCausalLM(config)\n+        # Ensure any monkey patching is cleaned up for subsequent tests\n+        with patch(\"transformers.models.llama.modeling_llama\"):\n+            # Check that trainer still works with liger kernel applied when using a custom config\n+            config = LlamaConfig(vocab_size=100, hidden_size=32, num_hidden_layers=3, num_attention_heads=4)\n+            tiny_llama = LlamaForCausalLM(config)\n \n-        x = torch.randint(0, 100, (128,))\n-        train_dataset = RepeatDataset(x)\n+            x = torch.randint(0, 100, (128,))\n+            train_dataset = RepeatDataset(x)\n \n-        args = TrainingArguments(\n-            self.get_auto_remove_tmp_dir(),\n-            learning_rate=1e-2,\n-            logging_steps=5,\n-            max_steps=20,\n-            use_liger_kernel=True,\n-            liger_kernel_config={\"rms_norm\": False, \"cross_entropy\": True, \"fused_linear_cross_entropy\": False},\n-        )\n-        trainer = Trainer(tiny_llama, args, train_dataset=train_dataset)\n+            args = TrainingArguments(\n+                self.get_auto_remove_tmp_dir(),\n+                learning_rate=1e-2,\n+                logging_steps=5,\n+                max_steps=20,\n+                use_liger_kernel=True,\n+                liger_kernel_config={\"rms_norm\": False, \"cross_entropy\": True, \"fused_linear_cross_entropy\": False},\n+            )\n+            trainer = Trainer(tiny_llama, args, train_dataset=train_dataset)\n \n-        # Check this works\n-        _ = trainer.train()\n+            # Check this works\n+            _ = trainer.train()\n \n     @require_lomo\n     @require_torch_accelerator\n@@ -3280,7 +3288,6 @@ def test_can_resume_training_lm(self):\n         training_steps = 10\n         resume_from_step = 8\n         with tempfile.TemporaryDirectory() as tmpdir:\n-            enable_full_determinism(0)\n             kwargs = {\n                 \"output_dir\": tmpdir,\n                 \"fp16\": True,\n@@ -3314,7 +3321,6 @@ def test_can_resume_training_lm(self):\n             )\n \n             # Checkpoint at intermediate step\n-            enable_full_determinism(0)\n             checkpoint = os.path.join(tmpdir, f\"checkpoint-{resume_from_step + 1}\")\n             trainer = get_language_model_trainer(**kwargs)\n             trainer.train(resume_from_checkpoint=checkpoint)\n@@ -3812,7 +3818,6 @@ def test_evaluation_iterable_dataset(self):\n             args = RegressionTrainingArguments(output_dir=tmp_dir)\n             trainer = Trainer(model=model, args=args, eval_dataset=eval_dataset, compute_metrics=AlmostAccuracy())\n             results = trainer.evaluate()\n-\n             x, y = trainer.eval_dataset.dataset.x, trainer.eval_dataset.dataset.ys[0]\n             pred = 1.5 * x + 2.5\n             expected_loss = ((pred - y) ** 2).mean()\n@@ -3839,7 +3844,6 @@ def test_predict_iterable_dataset(self):\n         with tempfile.TemporaryDirectory() as tmp_dir:\n             args = RegressionTrainingArguments(output_dir=tmp_dir)\n             trainer = Trainer(model=model, args=args, eval_dataset=eval_dataset, compute_metrics=AlmostAccuracy())\n-\n             preds = trainer.predict(trainer.eval_dataset).predictions\n             x = eval_dataset.dataset.x\n             self.assertTrue(np.allclose(preds, 1.5 * x + 2.5))\n@@ -4139,124 +4143,29 @@ def test_fp16_full_eval(self):\n             self.assertAlmostEqual(fp16_eval, fp32_init / 2, delta=5_000)\n \n     @require_torch_gpu\n-    @require_torch_non_multi_gpu\n-    @require_torch_tensorrt_fx\n-    def test_torchdynamo_full_eval(self):\n-        from torch import _dynamo as torchdynamo\n-\n-        # torchdynamo at the moment doesn't support DP/DDP, therefore require a single gpu\n-        n_gpus = get_gpu_count()\n+    @pytest.mark.torch_compile_test\n+    def test_torch_compile_train(self):\n+        with tempfile.TemporaryDirectory() as tmp_dir:\n+            trainer = get_regression_trainer(output_dir=tmp_dir)\n+            metrics = trainer.train()\n+            original_train_loss = metrics.training_loss\n \n-        bs = 8\n-        eval_len = 16 * n_gpus\n-        # make the params are somewhat big so that there will be enough RAM consumed to be able to\n-        # measure things. We should get about 64KB for a+b in fp32\n-        a = torch.ones(1000, bs) + 0.001\n-        b = torch.ones(1000, bs) - 0.001\n+            trainer = get_regression_trainer(torch_compile=True, output_dir=tmp_dir)\n+            metrics = trainer.train()\n+            self.assertAlmostEqual(metrics.training_loss, original_train_loss)\n \n+    @require_torch_gpu\n+    @pytest.mark.torch_compile_test\n+    def test_torch_compile_eval(self):\n         with tempfile.TemporaryDirectory() as tmp_dir:\n-            # 1. Default - without TorchDynamo\n-            trainer = get_regression_trainer(a=a, b=b, eval_len=eval_len, output_dir=tmp_dir)\n+            trainer = get_regression_trainer(output_dir=tmp_dir)\n             metrics = trainer.evaluate()\n             original_eval_loss = metrics[\"eval_loss\"]\n-            del trainer\n \n-            # 2. TorchDynamo eager\n-            trainer = get_regression_trainer(\n-                a=a, b=b, eval_len=eval_len, torch_compile_backend=\"eager\", output_dir=tmp_dir\n-            )\n+            trainer = get_regression_trainer(torch_compile=True, output_dir=tmp_dir)\n             metrics = trainer.evaluate()\n-            self.assertAlmostEqual(metrics[\"eval_loss\"], original_eval_loss)\n-            del trainer\n-            torchdynamo.reset()\n \n-            # 3. TorchDynamo nvfuser\n-            trainer = get_regression_trainer(\n-                a=a, b=b, eval_len=eval_len, torch_compile_backend=\"nvfuser\", output_dir=tmp_dir\n-            )\n-            metrics = trainer.evaluate()\n-            self.assertAlmostEqual(metrics[\"eval_loss\"], original_eval_loss)\n-            torchdynamo.reset()\n-\n-            # 4. TorchDynamo fx2trt\n-            trainer = get_regression_trainer(\n-                a=a, b=b, eval_len=eval_len, torch_compile_backend=\"fx2trt\", output_dir=tmp_dir\n-            )\n-            metrics = trainer.evaluate()\n             self.assertAlmostEqual(metrics[\"eval_loss\"], original_eval_loss)\n-            torchdynamo.reset()\n-\n-    @require_torch_non_multi_gpu\n-    @require_torch_gpu\n-    def test_torchdynamo_memory(self):\n-        # torchdynamo at the moment doesn't support DP/DDP, therefore require a single gpu\n-        from torch import _dynamo as torchdynamo\n-\n-        class CustomTrainer(Trainer):\n-            def compute_loss(self, model, inputs, num_items_in_batch=None, return_outputs=False):\n-                x = inputs[\"x\"]\n-                output = model(x)\n-                if self.args.n_gpu == 1:\n-                    return output.mean()\n-                return output\n-\n-        class MyModule(torch.nn.Module):\n-            \"\"\"Simple module that does aggressive fusion\"\"\"\n-\n-            def __init__(self):\n-                super().__init__()\n-\n-            def forward(self, x):\n-                for _ in range(20):\n-                    x = torch.cos(x)\n-                return x\n-\n-        mod = MyModule()\n-\n-        # 1. without TorchDynamo (eager baseline)\n-        a = torch.ones(1024, 1024, device=torch_device, requires_grad=True)\n-        a.grad = None\n-        trainer = CustomTrainer(model=mod)\n-        # warmup\n-        for _ in range(10):\n-            orig_loss = trainer.training_step(mod, {\"x\": a})\n-\n-        # resets\n-        gc.collect()\n-        backend_empty_cache(torch_device)\n-        backend_reset_peak_memory_stats(torch_device)\n-\n-        orig_loss = trainer.training_step(mod, {\"x\": a})\n-        orig_peak_mem = backend_max_memory_allocated(torch_device)\n-        torchdynamo.reset()\n-        del trainer\n-\n-        # 2. TorchDynamo nvfuser\n-        with tempfile.TemporaryDirectory() as tmp_dir:\n-            a = torch.ones(1024, 1024, device=torch_device, requires_grad=True)\n-            a.grad = None\n-            args = TrainingArguments(output_dir=tmp_dir, torch_compile_backend=\"nvfuser\")\n-            trainer = CustomTrainer(model=mod, args=args)\n-            # warmup\n-            for _ in range(10):\n-                loss = trainer.training_step(mod, {\"x\": a})\n-\n-            # resets\n-            gc.collect()\n-            backend_empty_cache(torch_device)\n-            backend_reset_peak_memory_stats(torch_device)\n-\n-            loss = trainer.training_step(mod, {\"x\": a})\n-            peak_mem = backend_max_memory_allocated(torch_device)\n-            torchdynamo.reset()\n-            del trainer\n-\n-            # Functional check\n-            self.assertAlmostEqual(loss, orig_loss)\n-\n-            # AOT Autograd recomputation and nvfuser recomputation optimization\n-            # aggressively fuses the operations and reduce the memory footprint.\n-            self.assertGreater(orig_peak_mem, peak_mem * 2)\n \n     @require_torch_accelerator\n     @require_torch_bf16"
      },
      {
        "filename": "tests/trainer/test_trainer_seq2seq.py",
        "status": "modified",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "patch": "@@ -38,8 +38,8 @@ def test_finetune_bert2bert(self):\n         tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n \n         bert2bert.config.vocab_size = bert2bert.config.encoder.vocab_size\n-        bert2bert.config.eos_token_id = tokenizer.sep_token_id\n-        bert2bert.config.decoder_start_token_id = tokenizer.cls_token_id\n+        tokenizer.eos_token_id = tokenizer.sep_token_id\n+        bert2bert.generation_config.decoder_start_token_id = tokenizer.cls_token_id\n         bert2bert.config.max_length = 128\n \n         train_dataset = datasets.load_dataset(\"abisee/cnn_dailymail\", \"3.0.0\", split=\"train[:1%]\")"
      }
    ],
    "num_files": 6,
    "scraped_at": "2025-11-16T21:18:00.090536"
  },
  {
    "pr_number": 41446,
    "title": "Enable non-streaming mode in `transformers serve`",
    "body": "Needs this to be merged first: https://github.com/huggingface/transformers/pull/41444\r\n\r\nTests and docs need to be added before undraft",
    "html_url": "https://github.com/huggingface/transformers/pull/41446",
    "created_at": "2025-10-08T11:53:19Z",
    "merged_at": "2025-10-15T07:37:26Z",
    "merge_commit_sha": "13a35a5057cbdc34b6c93a26d4e57987cbdd205c",
    "base_ref": "main",
    "head_sha": "a0c6b40d13c79614c1440b9feb4c5568626af8f3",
    "user": "LysandreJik",
    "files": [
      {
        "filename": "src/transformers/commands/serving.py",
        "status": "modified",
        "additions": 157,
        "deletions": 52,
        "changes": 209,
        "patch": "@@ -26,7 +26,7 @@\n import time\n import uuid\n from argparse import ArgumentParser, Namespace\n-from collections.abc import AsyncGenerator, Generator, Iterable\n+from collections.abc import Generator, Iterable\n from contextlib import asynccontextmanager\n from dataclasses import dataclass, field\n from io import BytesIO\n@@ -35,6 +35,7 @@\n \n from huggingface_hub import model_info\n from huggingface_hub.constants import HF_HUB_OFFLINE\n+from openai.types.chat.chat_completion import Choice\n from tokenizers.decoders import DecodeStream\n \n import transformers\n@@ -90,14 +91,16 @@\n     from fastapi.responses import JSONResponse, StreamingResponse\n     from openai.types.audio.transcription import Transcription\n     from openai.types.audio.transcription_create_params import TranscriptionCreateParamsBase\n-    from openai.types.chat import ChatCompletionMessageParam\n+    from openai.types.chat import ChatCompletion, ChatCompletionMessage, ChatCompletionMessageParam\n     from openai.types.chat.chat_completion_chunk import (\n         ChatCompletionChunk,\n-        Choice,\n         ChoiceDelta,\n         ChoiceDeltaToolCall,\n         ChoiceDeltaToolCallFunction,\n     )\n+    from openai.types.chat.chat_completion_chunk import (\n+        Choice as ChoiceChunk,\n+    )\n     from openai.types.chat.completion_create_params import CompletionCreateParamsStreaming\n     from openai.types.responses import (\n         Response,\n@@ -345,8 +348,11 @@ def delete_model(self):\n             self._timer.cancel()\n \n     def timeout_reached(self):\n-        self.delete_model()\n-        logger.info(f\"{self._name_or_path} was removed from memory after {self.timeout_seconds} seconds of inactivity\")\n+        if self.timeout_seconds > 0:\n+            self.delete_model()\n+            logger.info(\n+                f\"{self._name_or_path} was removed from memory after {self.timeout_seconds} seconds of inactivity\"\n+            )\n \n     def is_deleted(self):\n         \"\"\"Check if the instances have been deleted.\"\"\"\n@@ -412,9 +418,13 @@ class ServeArguments:\n     # Serving settings\n     host: str = field(default=\"localhost\", metadata={\"help\": \"Interface the server will listen to.\"})\n     port: int = field(default=8000, metadata={\"help\": \"Port the server will listen to.\"})\n-    model_timeout: int = field(\n-        default=300,\n-        metadata={\"help\": \"Time in seconds after which a model will be removed from memory.\"},\n+    model_timeout: Optional[int] = field(\n+        default=None,\n+        metadata={\n+            \"help\": \"Time in seconds after which a model will be removed from memory; defaults to 300 unless \"\n+            \"`force_model` is set, in which case the model will not be removed from memory unless a value\"\n+            \"is specified here.\"\n+        },\n     )\n \n     # Other settings\n@@ -512,6 +522,14 @@ def __init__(self, args: ServeArguments):\n         self.last_kv_cache = None\n         self.last_model = None\n \n+        if self.args.model_timeout is None:\n+            self.args.model_timeout = -1 if self.args.force_model else 300\n+\n+        if self.args.force_model:\n+            model_id_and_revision = self.process_model_name(self.args.force_model)\n+            self.last_model = model_id_and_revision\n+            self.load_model_and_processor(model_id_and_revision)\n+\n     def _validate_request(\n         self,\n         request: dict,\n@@ -595,7 +613,7 @@ def build_chat_completion_chunk(\n         tool_calls: Optional[list[\"ChoiceDeltaToolCall\"]] = None,\n         decode_stream: Optional[DecodeStream] = None,\n         tokenizer: Optional[PreTrainedTokenizerFast] = None,\n-    ) -> str:\n+    ) -> ChatCompletionChunk:\n         \"\"\"\n         Builds a chunk of a streaming OpenAI Chat Completion response.\n \n@@ -621,12 +639,13 @@ def build_chat_completion_chunk(\n         \"\"\"\n         if decode_stream is not None and content is not None and tokenizer is not None:\n             content = decode_stream.step(tokenizer._tokenizer, content)\n+\n         chunk = ChatCompletionChunk(\n             id=request_id,\n             created=int(time.time()),\n             model=model,\n             choices=[\n-                Choice(\n+                ChoiceChunk(\n                     delta=ChoiceDelta(\n                         content=content,\n                         role=role,\n@@ -639,23 +658,25 @@ def build_chat_completion_chunk(\n             system_fingerprint=\"\",\n             object=\"chat.completion.chunk\",\n         )\n-        return f\"data: {chunk.model_dump_json(exclude_none=True)}\\n\\n\"\n \n-    def build_response_event(self, response: \"BaseModel\") -> str:\n+        return chunk\n+\n+    @staticmethod\n+    def chunk_to_sse_element(chunk: ChatCompletionChunk | BaseModel) -> str:\n         \"\"\"\n-        Builds a event of a streaming OpenAI Response response.\n+        Builds an event of a streaming OpenAI Response model or a ChatCompletion chunk.\n \n         IMPORTANT: The serialized chunk won't contain empty fields (fields with `None`). Some downstream apps,\n         like Cursor, assume that when the field exists, it has data.\n \n         Args:\n-            response (`BaseModel`):\n+            chunk (`BaseModel` or `ChatCompletionChunk`):\n                 The response to build an event from. One of the multiple OpenAI Response output types\n \n         Returns:\n             `str`: The built chunk, a string containing a JSON string with the payload.\n         \"\"\"\n-        return f\"data: {response.model_dump_json(exclude_none=True)}\\n\\n\"\n+        return f\"data: {chunk.model_dump_json(exclude_none=True)}\\n\\n\"\n \n     def run(self):\n         \"\"\"\n@@ -668,6 +689,7 @@ def run(self):\n         - POST /v1/responses: Generates responses.\n         - POST /v1/audio/transcriptions: Generates transcriptions from audio.\n         - GET /v1/models: Lists available models for 3rd party tools.\n+        - GET /health: Health check.\n \n         Requires FastAPI and Uvicorn to be installed.\n         \"\"\"\n@@ -703,10 +725,9 @@ def chat_completion(request: Request, body: dict):\n             self.validate_chat_completion_request(request=body)\n \n             if self.use_continuous_batching:\n-                output = self.continuous_batching_chat_completion(body, request.state.request_id)\n+                return self.continuous_batching_chat_completion(body, request.state.request_id)\n             else:\n-                output = self.generate_chat_completion(body)\n-            return StreamingResponse(output, media_type=\"text/event-stream\")\n+                return self.generate_chat_completion(body)\n \n         @app.post(\"/v1/responses\")\n         def responses(request: dict):\n@@ -803,7 +824,7 @@ def get_gen_models(self) -> list[dict[str, any]]:\n                 for model in model_infos\n             ]\n \n-    def continuous_batching_chat_completion(self, req: dict, request_id: str) -> AsyncGenerator[str, None]:\n+    def continuous_batching_chat_completion(self, req: dict, request_id: str) -> StreamingResponse | JSONResponse:\n         \"\"\"\n         Generates an OpenAI Chat Completion using continuous batching.\n \n@@ -816,14 +837,16 @@ def continuous_batching_chat_completion(self, req: dict, request_id: str) -> Asy\n \n         model_id_and_revision = self.process_model_name(req[\"model\"])\n         must_discard_cache = model_id_and_revision != self.last_model\n+\n         self.last_model = model_id_and_revision\n+\n+        # When switching models, terminate a continuous batching manager if it is running.\n         if must_discard_cache:\n-            # When switching models, terminate a continuous batching manager if it is running.\n             if self.running_continuous_batching_manager is not None:\n                 self.running_continuous_batching_manager.stop(block=True, timeout=2)\n                 self.running_continuous_batching_manager = None\n-        model, processor = self.load_model_and_processor(model_id_and_revision)\n \n+        model, processor = self.load_model_and_processor(model_id_and_revision)\n         tokenizer = processor.tokenizer if hasattr(processor, \"tokenizer\") else processor\n \n         generation_config = create_generation_config_from_req(\n@@ -838,18 +861,17 @@ def continuous_batching_chat_completion(self, req: dict, request_id: str) -> Asy\n \n         if self.running_continuous_batching_manager is None:\n             self.running_continuous_batching_manager = model.init_continuous_batching(\n-                generation_config=generation_config, streaming=True\n+                generation_config=generation_config\n             )\n \n-            # TODO (Joao, Lysandre): the logits processors should be fixed in continuous batching\n-            # and correctly applied in non-cb\n+            # TODO (Joao, Lysandre): the logits processors should be fixed in continuous batching and correctly applied in non-cb\n             self.running_continuous_batching_manager.logit_processor = LogitsProcessorList()\n             self.running_continuous_batching_manager.start()\n \n         # TODO (Joao, Lysandre): this should also work with tool support\n         inputs = processor.apply_chat_template(req[\"messages\"], return_tensors=\"pt\", add_generation_prompt=True).to(\n             model.device\n-        )\n+        )[0]\n \n         def stream_chat_completion(request_id, decode_stream):\n             try:\n@@ -879,21 +901,61 @@ def stream_chat_completion(request_id, decode_stream):\n                 self.running_continuous_batching_manager.cancel_request(request_id)\n                 yield f'data: {{\"error\": \"{str(e)}\"}}'\n \n-        async def cancellation_wrapper(_inputs, request_id):\n+        def buffer_chat_completion(_request_id):\n+            result = None\n+            while self.running_continuous_batching_manager.is_running() and result is None:\n+                result = self.running_continuous_batching_manager.get_result(request_id=_request_id, timeout=1)\n+\n+            content = tokenizer.decode(result.generated_tokens)\n+\n+            chat_completion_result = ChatCompletion(\n+                id=_request_id,\n+                created=int(time.time()),\n+                object=\"chat.completion\",\n+                model=model_id_and_revision,\n+                choices=[\n+                    Choice(\n+                        # TODO check the index\n+                        index=0,\n+                        message=ChatCompletionMessage(content=content, role=\"assistant\"),\n+                        finish_reason=\"stop\",\n+                    )\n+                ],\n+                # TODO implement function calling\n+                # TODO implement usage\n+            )\n+\n+            return chat_completion_result\n+\n+        async def cancellation_wrapper_stream(_request_id):\n+            # Enables cancellation in an async context\n             try:\n-                decode_stream = DecodeStream(_inputs.tolist(), False)\n-                # XXX: using returned request_id as safety in case it is None\n-                request_id = self.running_continuous_batching_manager.add_request(\n-                    _inputs, request_id=request_id, max_new_tokens=generation_config.max_new_tokens\n-                )\n-                for chunk in stream_chat_completion(request_id, decode_stream):\n-                    yield chunk\n-                    await asyncio.sleep(0)  # Yield control to the event loop to check for cancellations\n+                decode_stream = DecodeStream(inputs.tolist(), False)\n+                for _chunk in stream_chat_completion(_request_id, decode_stream):\n+                    yield self.chunk_to_sse_element(_chunk)\n+                    await asyncio.sleep(0)\n             except asyncio.CancelledError:\n-                self.running_continuous_batching_manager.cancel_request(request_id)\n-                logger.warning(f\"Request {request_id} was cancelled.\")\n+                self.running_continuous_batching_manager.cancel_request(_request_id)\n+                logger.warning(f\"Request {_request_id} was cancelled.\")\n \n-        return cancellation_wrapper(inputs[0], request_id)\n+        def cancellation_wrapper_buffer(_request_id):\n+            # Enables cancellation in an async context\n+            try:\n+                return buffer_chat_completion(_request_id)\n+            except asyncio.CancelledError:\n+                self.running_continuous_batching_manager.cancel_request(_request_id)\n+                logger.warning(f\"Request {_request_id} was cancelled.\")\n+\n+        request_id = self.running_continuous_batching_manager.add_request(\n+            inputs, request_id=request_id, max_new_tokens=generation_config.max_new_tokens, streaming=req.get(\"stream\")\n+        )\n+\n+        if req.get(\"stream\"):\n+            return StreamingResponse(cancellation_wrapper_stream(request_id), media_type=\"text/event-stream\")\n+        else:\n+            chunk = cancellation_wrapper_buffer(request_id)\n+            json_chunk = chunk.model_dump_json(exclude_none=True)\n+            return JSONResponse(json_chunk, media_type=\"application/json\")\n \n     @staticmethod\n     def get_model_modality(model: \"PreTrainedModel\") -> Modality:\n@@ -953,7 +1015,7 @@ def get_processor_inputs_from_inbound_messages(messages, modality: Modality):\n             processor_inputs.append(parsed_message)\n         return processor_inputs\n \n-    def generate_chat_completion(self, req: dict) -> Generator[str, None, None]:\n+    def generate_chat_completion(self, req: dict) -> StreamingResponse | JSONResponse:\n         \"\"\"\n         Generates an OpenAI Chat Completion using `generate`.\n \n@@ -1132,7 +1194,10 @@ def generate_with_cache(**kwargs):\n                                 )\n \n                             yield self.build_chat_completion_chunk(\n-                                request_id=_request_id, role=None, tool_calls=[tool], model=model_id_and_revision\n+                                request_id=_request_id,\n+                                role=None,\n+                                tool_calls=[tool],\n+                                model=model_id_and_revision,\n                             )\n                             continue\n                     # ====== END OF TOOL CALL LOGIC ======\n@@ -1152,7 +1217,47 @@ def generate_with_cache(**kwargs):\n             finally:\n                 thread.join()\n \n-        return stream_chat_completion(generation_streamer, request_id)\n+        if req.get(\"stream\"):\n+            return StreamingResponse(\n+                map(self.chunk_to_sse_element, stream_chat_completion(generation_streamer, request_id)),\n+                media_type=\"text/event-stream\",\n+            )\n+        else:\n+            content = []\n+            finish_reason = \"stop\"\n+\n+            generator = stream_chat_completion(generation_streamer, request_id)\n+            usage = None\n+\n+            for chunk in generator:\n+                choice = chunk.choices[0]\n+                if getattr(choice.delta, \"content\", None):\n+                    content.append(choice.delta.content)\n+                if choice.finish_reason:\n+                    finish_reason = choice.finish_reason\n+                if getattr(chunk, \"usage\", None):\n+                    usage = chunk.usage\n+\n+            chat_completion_result = ChatCompletion(\n+                id=request_id,\n+                created=int(time.time()),\n+                object=\"chat.completion\",\n+                model=model_id_and_revision,\n+                choices=[\n+                    Choice(\n+                        # TODO check the index\n+                        index=0,\n+                        message=ChatCompletionMessage(content=\"\".join(content), role=\"assistant\"),\n+                        finish_reason=finish_reason,\n+                    )\n+                ],\n+                # TODO implement function calling\n+                usage=usage,\n+            )\n+\n+            result = chat_completion_result.model_dump(exclude_none=True)\n+\n+            return JSONResponse(result, media_type=\"application/json\")\n \n     def generate_response(self, req: dict) -> Generator[str, None, None]:\n         \"\"\"\n@@ -1263,7 +1368,7 @@ def generate_with_cache(**kwargs):\n                     ),\n                 )\n                 sequence_number += 1\n-                yield self.build_response_event(response_created)\n+                yield self.chunk_to_sse_element(response_created)\n \n                 response_in_progress = ResponseInProgressEvent(\n                     type=\"response.in_progress\",\n@@ -1284,7 +1389,7 @@ def generate_with_cache(**kwargs):\n                     ),\n                 )\n                 sequence_number += 1\n-                yield self.build_response_event(response_in_progress)\n+                yield self.chunk_to_sse_element(response_in_progress)\n \n                 # Start the output item. Emit the assistant role to start the stream. Other chunks won't have a role,\n                 # as it is implicit\n@@ -1297,7 +1402,7 @@ def generate_with_cache(**kwargs):\n                     ),\n                 )\n                 sequence_number += 1\n-                yield self.build_response_event(response_output_item_added)\n+                yield self.chunk_to_sse_element(response_output_item_added)\n \n                 # Start the content part of the event\n                 response_content_part_added = ResponseContentPartAddedEvent(\n@@ -1309,7 +1414,7 @@ def generate_with_cache(**kwargs):\n                     part=ResponseOutputText(type=\"output_text\", text=\"\", annotations=[]),\n                 )\n                 sequence_number += 1\n-                yield self.build_response_event(response_content_part_added)\n+                yield self.chunk_to_sse_element(response_content_part_added)\n \n                 # Stream the actual generated text\n                 results = \"\"\n@@ -1336,7 +1441,7 @@ def generate_with_cache(**kwargs):\n                                 logprobs=[],\n                             )\n                             sequence_number += 1\n-                            yield self.build_response_event(response_output_text_delta)\n+                            yield self.chunk_to_sse_element(response_output_text_delta)\n                     else:\n                         # Normal path: emit token deltas when not filtering CoT\n                         if result:\n@@ -1350,7 +1455,7 @@ def generate_with_cache(**kwargs):\n                                 logprobs=[],\n                             )\n                             sequence_number += 1\n-                            yield self.build_response_event(response_output_text_delta)\n+                            yield self.chunk_to_sse_element(response_output_text_delta)\n \n                 # Signal the end of the text generation\n                 response_output_text_done = ResponseTextDoneEvent(\n@@ -1363,7 +1468,7 @@ def generate_with_cache(**kwargs):\n                     logprobs=[],\n                 )\n                 sequence_number += 1\n-                yield self.build_response_event(response_output_text_done)\n+                yield self.chunk_to_sse_element(response_output_text_done)\n \n                 # Complete the content part\n                 response_content_part_done = ResponseContentPartDoneEvent(\n@@ -1376,7 +1481,7 @@ def generate_with_cache(**kwargs):\n                 )\n                 sequence_number += 1\n                 content_index += 1\n-                yield self.build_response_event(response_content_part_done)\n+                yield self.chunk_to_sse_element(response_content_part_done)\n \n                 # Complete the output item\n                 response_output_item_done = ResponseOutputItemDoneEvent(\n@@ -1394,7 +1499,7 @@ def generate_with_cache(**kwargs):\n                 )\n                 sequence_number += 1\n                 output_index += 1\n-                yield self.build_response_event(response_output_item_done)\n+                yield self.chunk_to_sse_element(response_output_item_done)\n \n                 # Finally, Complete the event\n                 response_completed = ResponseCompletedEvent(\n@@ -1416,7 +1521,7 @@ def generate_with_cache(**kwargs):\n                     ),\n                 )\n                 sequence_number += 1\n-                yield self.build_response_event(response_completed)\n+                yield self.chunk_to_sse_element(response_completed)\n \n                 thread.join()\n             except Exception as e:\n@@ -1427,7 +1532,7 @@ def generate_with_cache(**kwargs):\n                     message=str(e),\n                 )\n                 sequence_number += 1\n-                yield self.build_response_event(error_event)\n+                yield self.chunk_to_sse_element(error_event)\n \n                 response_failed = ResponseFailedEvent(\n                     type=\"response.failed\",\n@@ -1452,7 +1557,7 @@ def generate_with_cache(**kwargs):\n                     ),\n                 )\n                 sequence_number += 1\n-                yield self.build_response_event(response_failed)\n+                yield self.chunk_to_sse_element(response_failed)\n \n             finally:\n                 thread.join()"
      },
      {
        "filename": "src/transformers/generation/continuous_batching/continuous_api.py",
        "status": "modified",
        "additions": 0,
        "deletions": 1,
        "changes": 1,
        "patch": "@@ -907,7 +907,6 @@ def get_result(\n             if request_id is not None and result.request_id != request_id:\n                 self.output_queue.put(result)\n                 return None\n-            logger.debug(f\"Retrieved result for request {result.request_id}\")\n             return result\n         except queue.Empty:\n             return None"
      },
      {
        "filename": "src/transformers/modeling_utils.py",
        "status": "modified",
        "additions": 3,
        "deletions": 3,
        "changes": 6,
        "patch": "@@ -2509,14 +2509,14 @@ def _check_and_adjust_attn_implementation(\n             try:\n                 load_and_register_attn_kernel(applicable_attn_implementation)\n                 # log that we used kernel fallback if successful\n-                if attn_implementation.startswith(\"flash_attention\"):\n+                if \"flash_\" in attn_implementation:\n                     logger.warning_once(\n                         f\"You do not have `flash_attn` installed, using `{applicable_attn_implementation}` \"\n                         \"from the `kernels` library instead!\"\n                     )\n             except Exception as e:\n                 # raise the proper exception for requested flash attention\n-                if attn_implementation.startswith(\"flash_attention\"):\n+                if attn_implementation.startswith(\"flash_\"):\n                     if attn_implementation.endswith(\"2\"):\n                         self._flash_attn_2_can_dispatch()\n                     else:\n@@ -2529,7 +2529,7 @@ def _check_and_adjust_attn_implementation(\n                 applicable_attn_implementation, is_init_check\n             )\n             # preload flash attention here to allow compile with fullgraph\n-            if applicable_attn_implementation.startswith(\"flash_attention\"):\n+            if applicable_attn_implementation.startswith(\"flash_\"):\n                 lazy_import_flash_attention(applicable_attn_implementation, force_import=True)\n         return applicable_attn_implementation\n "
      },
      {
        "filename": "tests/commands/test_serving.py",
        "status": "modified",
        "additions": 51,
        "deletions": 17,
        "changes": 68,
        "patch": "@@ -85,6 +85,7 @@ def test_build_chat_completion_chunk(self):\n         chunk = ServeCommand.build_chat_completion_chunk(\n             dummy, request_id=\"req0\", content=\"hello\", finish_reason=\"stop\", role=\"user\", model=\"dummy_model@main\"\n         )\n+        chunk = ServeCommand.chunk_to_sse_element(chunk)\n         for field in MANDATORY_FIELDS:\n             self.assertIn(field, chunk)\n         self.assertIn(\n@@ -93,12 +94,14 @@ def test_build_chat_completion_chunk(self):\n \n         # Case 2: only the role is provided -- other fields in 'choices' are omitted\n         chunk = dummy.build_chat_completion_chunk(request_id=\"req0\", role=\"user\", model=\"dummy_model@main\")\n+        chunk = ServeCommand.chunk_to_sse_element(chunk)\n         for field in MANDATORY_FIELDS:\n             self.assertIn(field, chunk)\n         self.assertIn('\"choices\":[{\"delta\":{\"role\":\"user\"},\"index\":0}]', chunk)\n \n         # Case 3: only the content is provided -- other fields in 'choices' are omitted\n         chunk = dummy.build_chat_completion_chunk(request_id=\"req0\", content=\"hello\", model=\"dummy_model@main\")\n+        chunk = ServeCommand.chunk_to_sse_element(chunk)\n         for field in MANDATORY_FIELDS:\n             self.assertIn(field, chunk)\n         self.assertIn('\"choices\":[{\"delta\":{\"content\":\"hello\"},\"index\":0}]', chunk)\n@@ -110,6 +113,7 @@ def test_build_chat_completion_chunk(self):\n             type=\"function\",\n         )\n         chunk = dummy.build_chat_completion_chunk(request_id=\"req0\", tool_calls=[tool_call], model=\"dummy_model@main\")\n+        chunk = ServeCommand.chunk_to_sse_element(chunk)\n         for field in MANDATORY_FIELDS:\n             self.assertIn(field, chunk)\n         expected_choices_content = (\n@@ -147,7 +151,7 @@ def test_build_response_event(self):\n             ),\n         )\n \n-        event = dummy.build_response_event(response_created)\n+        event = dummy.chunk_to_sse_element(response_created)\n         self.assertTrue(event.startswith(\"data: \"))  # Sanity check: event formatting\n         self.assertIn('\"model\":\"dummy_model@main\"', event)  # Sanity check: set field\n         self.assertIn('\"status\":\"queued\"', event)\n@@ -411,10 +415,18 @@ def setUpClass(cls):\n         \"\"\"Starts a server for tests to connect to.\"\"\"\n         cls.port = 8001\n         args = ServeArguments(port=cls.port)\n-        serve_command = ServeCommand(args)\n-        thread = Thread(target=serve_command.run)\n-        thread.daemon = True\n-        thread.start()\n+        cls.serve_command = ServeCommand(args)\n+        cls.thread = Thread(target=cls.serve_command.run)\n+        cls.thread.daemon = True\n+        cls.thread.start()\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        cls.thread.join(timeout=1)\n+\n+    def setUp(self):\n+        \"\"\"Ensures that the healthcheck works before each test.\"\"\"\n+        _call_healthcheck(f\"http://localhost:{self.port}\")\n \n     @slow\n     def test_tool_call(self):\n@@ -548,13 +560,19 @@ class ServeCompletionsContinuousBatchingIntegrationTest(ServeCompletionsMixin, u\n     def setUpClass(cls):\n         \"\"\"Starts a server for tests to connect to.\"\"\"\n         cls.port = 8002\n-        args = ServeArguments(\n-            port=cls.port, continuous_batching=True, attn_implementation=\"sdpa_paged\", default_seed=42\n-        )\n+        args = ServeArguments(port=cls.port, continuous_batching=True, default_seed=42)\n         cls.serve_command = ServeCommand(args)\n-        thread = Thread(target=cls.serve_command.run)\n-        thread.daemon = True\n-        thread.start()\n+        cls.thread = Thread(target=cls.serve_command.run)\n+        cls.thread.daemon = True\n+        cls.thread.start()\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        cls.thread.join(timeout=1)\n+\n+    def setUp(self):\n+        \"\"\"Ensures that the healthcheck works before each test.\"\"\"\n+        _call_healthcheck(f\"http://localhost:{self.port}\")\n \n     def test_full_request(self):\n         \"\"\"Tests that an inference using the Responses API and Continuous Batching works\"\"\"\n@@ -703,9 +721,17 @@ def setUpClass(cls):\n         cls.port = 8003\n         args = ServeArguments(port=cls.port, default_seed=42)\n         serve_command = ServeCommand(args)\n-        thread = Thread(target=serve_command.run)\n-        thread.daemon = True\n-        thread.start()\n+        cls.thread = Thread(target=serve_command.run)\n+        cls.thread.daemon = True\n+        cls.thread.start()\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        cls.thread.join(timeout=1)\n+\n+    def setUp(self):\n+        \"\"\"Ensures that the healthcheck works before each test.\"\"\"\n+        _call_healthcheck(f\"http://localhost:{self.port}\")\n \n     @slow\n     def test_full_request(self):\n@@ -767,9 +793,17 @@ def setUpClass(cls):\n         cls.port = 8042\n         args = ServeArguments(port=cls.port)\n         serve_command = ServeCommand(args)\n-        thread = Thread(target=serve_command.run)\n-        thread.daemon = True\n-        thread.start()\n+        cls.thread = Thread(target=serve_command.run)\n+        cls.thread.daemon = True\n+        cls.thread.start()\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        cls.thread.join(timeout=1)\n+\n+    def setUp(self):\n+        \"\"\"Ensures that the healthcheck works before each test.\"\"\"\n+        _call_healthcheck(f\"http://localhost:{self.port}\")\n \n     def test_healthcheck(self):\n         \"\"\"Tests that the healthcheck endpoint works.\"\"\""
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T21:18:00.722357"
  },
  {
    "pr_number": 41432,
    "title": "Refactor check_auto_docstring using AST",
    "body": "# What does this PR do?\r\n\r\nUse AST instead of parsing the raw code to check for missing args in docstrings of class/functions using auto_docstring",
    "html_url": "https://github.com/huggingface/transformers/pull/41432",
    "created_at": "2025-10-08T02:57:20Z",
    "merged_at": "2025-11-14T14:57:08Z",
    "merge_commit_sha": "8976ceb0510e139282050a1b12d9e6afb21bce35",
    "base_ref": "main",
    "head_sha": "7eb3c7e4d71de7e392d00084302d7c24d808da85",
    "user": "yonigozlan",
    "files": [
      {
        "filename": "src/transformers/models/glm4v/modeling_glm4v.py",
        "status": "modified",
        "additions": 0,
        "deletions": 3,
        "changes": 3,
        "patch": "@@ -1418,14 +1418,11 @@ def forward(\n         pixel_values_videos: Optional[torch.FloatTensor] = None,\n         image_grid_thw: Optional[torch.LongTensor] = None,\n         video_grid_thw: Optional[torch.LongTensor] = None,\n-        rope_deltas: Optional[torch.LongTensor] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, Glm4vCausalLMOutputWithPast]:\n         r\"\"\"\n-        rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):\n-            The rope index difference between sequence length and multimodal rope.\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n             config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
      },
      {
        "filename": "src/transformers/models/glm4v/modular_glm4v.py",
        "status": "modified",
        "additions": 0,
        "deletions": 3,
        "changes": 3,
        "patch": "@@ -1341,14 +1341,11 @@ def forward(\n         pixel_values_videos: Optional[torch.FloatTensor] = None,\n         image_grid_thw: Optional[torch.LongTensor] = None,\n         video_grid_thw: Optional[torch.LongTensor] = None,\n-        rope_deltas: Optional[torch.LongTensor] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, Glm4vCausalLMOutputWithPast]:\n         r\"\"\"\n-        rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):\n-            The rope index difference between sequence length and multimodal rope.\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n             config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
      },
      {
        "filename": "src/transformers/models/glm4v_moe/modeling_glm4v_moe.py",
        "status": "modified",
        "additions": 0,
        "deletions": 2,
        "changes": 2,
        "patch": "@@ -1631,8 +1631,6 @@ def forward(\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, Glm4vMoeCausalLMOutputWithPast]:\n         r\"\"\"\n-        rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):\n-            The rope index difference between sequence length and multimodal rope.\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n             config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
      },
      {
        "filename": "utils/check_docstrings.py",
        "status": "modified",
        "additions": 243,
        "deletions": 206,
        "changes": 449,
        "patch": "@@ -42,8 +42,9 @@\n import os\n import re\n from collections import OrderedDict\n+from dataclasses import dataclass\n from pathlib import Path\n-from typing import Any\n+from typing import Any, Optional, Union\n \n from check_repo import ignore_undocumented\n from git import Repo\n@@ -59,6 +60,25 @@\n )\n \n \n+@dataclass\n+class DecoratedItem:\n+    \"\"\"Information about a single @auto_docstring decorated function or class.\"\"\"\n+\n+    decorator_line: int  # 1-based line number of the decorator\n+    def_line: int  # 1-based line number of the def/class statement\n+    kind: str  # 'function' or 'class'\n+    body_start_line: (\n+        int  # 1-based line number where body starts (for functions) or __init__ body start (for classes with __init__)\n+    )\n+    args: list[str]  # List of argument names (excluding self, *args, **kwargs) - for classes, these are __init__ args\n+    custom_args_text: Optional[str] = None  # custom_args string if provided in decorator\n+\n+    # Class-specific fields (only populated when kind == 'class')\n+    has_init: bool = False  # Whether the class has an __init__ method\n+    init_def_line: Optional[int] = None  # 1-based line number of __init__ def (if has_init)\n+    is_model_output: bool = False  # Whether the class inherits from ModelOutput\n+\n+\n PATH_TO_REPO = Path(__file__).parent.parent.resolve()\n PATH_TO_TRANSFORMERS = Path(\"src\").resolve() / \"transformers\"\n \n@@ -874,34 +894,35 @@ def fix_docstring(obj: Any, old_doc_args: str, new_doc_args: str):\n         f.write(\"\\n\".join(lines))\n \n \n-def _find_sig_line(lines, line_end):\n-    parenthesis_count = 0\n-    sig_line_end = line_end\n-    found_sig = False\n-    while not found_sig:\n-        for char in lines[sig_line_end]:\n-            if char == \"(\":\n-                parenthesis_count += 1\n-            elif char == \")\":\n-                parenthesis_count -= 1\n-                if parenthesis_count == 0:\n-                    found_sig = True\n-                    break\n-        sig_line_end += 1\n-    return sig_line_end\n-\n-\n def _find_docstring_end_line(lines, docstring_start_line):\n-    if '\"\"\"' not in lines[docstring_start_line]:\n+    \"\"\"Find the line number where a docstring ends. Only handles triple double quotes.\"\"\"\n+    if docstring_start_line is None or docstring_start_line < 0 or docstring_start_line >= len(lines):\n         return None\n-    docstring_end = docstring_start_line\n-    if docstring_start_line is not None:\n-        docstring_end = docstring_start_line\n-        if not lines[docstring_start_line].count('\"\"\"') >= 2:\n-            docstring_end += 1\n-            while '\"\"\"' not in lines[docstring_end]:\n-                docstring_end += 1\n-    return docstring_end\n+    start_line = lines[docstring_start_line]\n+    if '\"\"\"' not in start_line:\n+        return None\n+    # Check if docstring starts and ends on the same line\n+    if start_line.count('\"\"\"') >= 2:\n+        return docstring_start_line\n+    # Find the closing triple quotes on subsequent lines\n+    for idx in range(docstring_start_line + 1, len(lines)):\n+        if '\"\"\"' in lines[idx]:\n+            return idx\n+    return len(lines) - 1\n+\n+\n+def _is_auto_docstring_decorator(dec):\n+    \"\"\"Return True if the decorator expression corresponds to `@auto_docstring`.\"\"\"\n+    # Handle @auto_docstring(...) - unwrap the Call to get the function\n+    target = dec.func if isinstance(dec, ast.Call) else dec\n+    # Check if it's named \"auto_docstring\"\n+    return isinstance(target, ast.Name) and target.id == \"auto_docstring\"\n+\n+\n+def _extract_function_args(func_node: Union[ast.FunctionDef, ast.AsyncFunctionDef]) -> list[str]:\n+    \"\"\"Extract argument names from a function node, excluding 'self', *args, **kwargs.\"\"\"\n+    all_args = (func_node.args.posonlyargs or []) + func_node.args.args + func_node.args.kwonlyargs\n+    return [a.arg for a in all_args if a.arg != \"self\"]\n \n \n def find_matching_model_files(check_all: bool = False):\n@@ -947,64 +968,20 @@ def find_matching_model_files(check_all: bool = False):\n def find_files_with_auto_docstring(matching_files, decorator=\"@auto_docstring\"):\n     \"\"\"\n     From a list of files, return those that contain the @auto_docstring decorator.\n+    Fast path: simple substring presence check.\n     \"\"\"\n     auto_docstrings_files = []\n     for file_path in matching_files:\n-        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n-            content_base_file = f.read()\n-            if decorator in content_base_file:\n-                lines = content_base_file.split(\"\\n\")\n-                line_numbers = [i for i, line in enumerate(lines) if decorator in line]\n-                for line_number in line_numbers:\n-                    line_end = line_number\n-                    end_patterns = [\"class \", \"    def\"]\n-                    stop_condition = False\n-                    while line_end < len(lines) and not stop_condition:\n-                        line_end += 1\n-                        stop_condition = any(lines[line_end].startswith(end_pattern) for end_pattern in end_patterns)\n-                    candidate_patterns = [\"class \", \"    def\"]\n-                    candidate = any(\n-                        lines[line_end].startswith(candidate_pattern) for candidate_pattern in candidate_patterns\n-                    )\n-                    if stop_condition and candidate:\n-                        auto_docstrings_files.append(file_path)\n-                        break\n+        try:\n+            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n+                source = f.read()\n+        except OSError:\n+            continue\n+        if decorator in source:\n+            auto_docstrings_files.append(file_path)\n     return auto_docstrings_files\n \n \n-def get_auto_docstring_candidate_lines(lines):\n-    \"\"\"\n-    For a file's lines, find the start and end line indices of all @auto_docstring candidates.\n-    Returns two lists: starts and ends.\n-    \"\"\"\n-    line_numbers = [i for i, line in enumerate(lines) if \"@auto_docstring\" in line]\n-    line_starts_candidates = []\n-    line_ends_candidates = []\n-    for line_number in line_numbers:\n-        line_end = line_number\n-        end_patterns = [\"class \", \"    def\"]\n-        stop_condition = False\n-        while line_end < len(lines) and not stop_condition:\n-            line_end += 1\n-            stop_condition = any(lines[line_end].startswith(end_pattern) for end_pattern in end_patterns)\n-        candidate_patterns = [\"class \", \"    def\"]\n-        candidate = any(lines[line_end].startswith(candidate_pattern) for candidate_pattern in candidate_patterns)\n-        if stop_condition and candidate:\n-            line_ends_candidates.append(line_end)\n-            line_starts_candidates.append(line_number)\n-    return line_starts_candidates, line_ends_candidates\n-\n-\n-def get_args_in_signature(lines, signature_content):\n-    signature_content = [line.split(\"#\")[0] for line in signature_content]\n-    signature_content = \"\".join(signature_content)\n-    signature_content = \"\".join(signature_content.split(\")\")[:-1])\n-    args_in_signature = re.findall(r\"[,(]\\s*(\\w+)\\s*(?=:|=|,|\\))\", signature_content)\n-    if \"self\" in args_in_signature:\n-        args_in_signature.remove(\"self\")\n-    return args_in_signature\n-\n-\n def get_args_in_dataclass(lines, dataclass_content):\n     dataclass_content = [line.split(\"#\")[0] for line in dataclass_content]\n     dataclass_content = \"\\n\".join(dataclass_content)\n@@ -1051,6 +1028,9 @@ def generate_new_docstring_for_signature(\n     else:\n         docstring_end_line = None\n \n+    # Remove pre-existing entries for *args and untyped **kwargs from the docstring\n+    # (No longer needed since *args are excluded from args_in_signature)\n+\n     # Remove args that are the same as the ones in the source args doc\n     for arg in args_docstring_dict:\n         if arg in get_args_doc_from_source(source_args_doc) and arg not in ALWAYS_OVERRIDE:\n@@ -1132,13 +1112,16 @@ def generate_new_docstring_for_signature(\n     )\n \n \n-def generate_new_docstring_for_function(lines, current_line_end, custom_args_dict):\n+def generate_new_docstring_for_function(\n+    lines,\n+    item: DecoratedItem,\n+    custom_args_dict,\n+):\n     \"\"\"\n     Wrapper for function docstring generation using the generalized helper.\n     \"\"\"\n-    sig_end_line = _find_sig_line(lines, current_line_end)\n-    signature_content = lines[current_line_end:sig_end_line]\n-    args_in_signature = get_args_in_signature(lines, signature_content)\n+    sig_end_line = item.body_start_line - 1  # Convert to 0-based\n+    args_in_signature = item.args\n     docstring_start_line = sig_end_line if '\"\"\"' in lines[sig_end_line] else None\n     return generate_new_docstring_for_signature(\n         lines,\n@@ -1150,34 +1133,27 @@ def generate_new_docstring_for_function(lines, current_line_end, custom_args_dic\n     )\n \n \n-def generate_new_docstring_for_class(lines, current_line_end, custom_args_dict):\n+def generate_new_docstring_for_class(\n+    lines,\n+    item: DecoratedItem,\n+    custom_args_dict,\n+    source: str,\n+):\n     \"\"\"\n     Wrapper for class docstring generation (via __init__) using the generalized helper.\n     Returns the new docstring and relevant signature/docstring indices.\n     \"\"\"\n-    sig_start_line = current_line_end\n-    found_init_method = False\n-    found_model_output = False\n-    while sig_start_line < len(lines) - 1 and not found_init_method:\n-        sig_start_line += 1\n-        if \"    def __init__\" in lines[sig_start_line]:\n-            found_init_method = True\n-        elif lines[sig_start_line].startswith(\"class \") or lines[sig_start_line].startswith(\"def \"):\n-            break\n-    if not found_init_method:\n-        if \"ModelOutput\" in lines[current_line_end]:\n-            found_model_output = True\n-            sig_start_line = current_line_end\n-        else:\n-            return \"\", None, None, [], [], []\n-\n-    if found_init_method:\n-        sig_end_line = _find_sig_line(lines, sig_start_line)\n-        signature_content = lines[sig_start_line:sig_end_line]\n-        args_in_signature = get_args_in_signature(lines, signature_content)\n-    else:\n-        # we have a ModelOutput class, the class attributes are the args\n-        sig_end_line = sig_start_line + 1\n+    # Use pre-extracted information from DecoratedItem (no need to search or re-parse!)\n+    if item.has_init:\n+        # Class has an __init__ method - use its args and body start\n+        sig_end_line = item.body_start_line - 1  # Convert from body start to sig end (0-based)\n+        args_in_signature = item.args\n+        output_docstring_indent = 8\n+        source_args_doc = [ModelArgs, ImageProcessorArgs]\n+    elif item.is_model_output:\n+        # ModelOutput class - extract args from dataclass attributes\n+        current_line_end = item.def_line - 1  # Convert to 0-based\n+        sig_end_line = current_line_end + 1\n         docstring_end = _find_docstring_end_line(lines, sig_end_line)\n         model_output_class_start = docstring_end + 1 if docstring_end is not None else sig_end_line - 1\n         model_output_class_end = model_output_class_start\n@@ -1187,6 +1163,11 @@ def generate_new_docstring_for_class(lines, current_line_end, custom_args_dict):\n             model_output_class_end += 1\n         dataclass_content = lines[model_output_class_start : model_output_class_end - 1]\n         args_in_signature = get_args_in_dataclass(lines, dataclass_content)\n+        output_docstring_indent = 4\n+        source_args_doc = [ModelOutputArgs]\n+    else:\n+        # Class has no __init__ and is not a ModelOutput - nothing to document\n+        return \"\", None, None, [], [], []\n \n     docstring_start_line = sig_end_line if '\"\"\"' in lines[sig_end_line] else None\n \n@@ -1197,127 +1178,177 @@ def generate_new_docstring_for_class(lines, current_line_end, custom_args_dict):\n         docstring_start_line,\n         arg_indent=\"\",\n         custom_args_dict=custom_args_dict,\n-        output_docstring_indent=4 if found_model_output else 8,\n-        source_args_doc=[ModelArgs, ImageProcessorArgs] if not found_model_output else [ModelOutputArgs],\n+        output_docstring_indent=output_docstring_indent,\n+        source_args_doc=source_args_doc,\n     )\n \n \n-def find_custom_args_with_details(file_content: str, custom_args_var_name: str) -> list[dict]:\n-    \"\"\"\n-    Find the given custom args variable in the file content and return its content.\n+def _build_ast_indexes(source: str) -> list[DecoratedItem]:\n+    \"\"\"Parse source once and return list of all @auto_docstring decorated items.\n \n-    Args:\n-        file_content: The string content of the Python file.\n-        custom_args_var_name: The name of the custom args variable.\n+    Returns:\n+        List of DecoratedItem objects, one for each @auto_docstring decorated function or class.\n     \"\"\"\n-    # Escape the variable_name to handle any special regex characters it might contain\n-    escaped_variable_name = re.escape(custom_args_var_name)\n-\n-    # Construct the regex pattern dynamically with the specific variable name\n-    # This regex looks for:\n-    # ^\\s* : Start of a line with optional leading whitespace.\n-    # ({escaped_variable_name}) : Capture the exact variable name.\n-    # \\s*=\\s* : An equals sign, surrounded by optional whitespace.\n-    # (r?\\\"\\\"\\\")               : Capture the opening triple quotes (raw or normal string).\n-    # (.*?)                    : Capture the content (non-greedy).\n-    # (\\\"\\\"\\\")                  : Match the closing triple quotes.\n-    regex_pattern = rf\"^\\s*({escaped_variable_name})\\s*=\\s*(r?\\\"\\\"\\\")(.*?)(\\\"\\\"\\\")\"\n-\n-    flags = re.MULTILINE | re.DOTALL\n+    tree = ast.parse(source)\n+    # First pass: collect top-level string variables (for resolving custom_args variable references)\n+    var_to_string: dict[str, str] = {}\n+    for node in tree.body:\n+        # Handle: ARGS = \"some string\"\n+        if isinstance(node, ast.Assign) and isinstance(node.value, ast.Constant):\n+            if isinstance(node.value.value, str):\n+                for target in node.targets:\n+                    if isinstance(target, ast.Name):\n+                        var_to_string[target.id] = node.value.value\n+        # Handle: ARGS: str = \"some string\"\n+        elif isinstance(node, ast.AnnAssign) and isinstance(node.value, ast.Constant):\n+            if isinstance(node.value.value, str) and isinstance(node.target, ast.Name):\n+                var_to_string[node.target.id] = node.value.value\n+    # Second pass: find all @auto_docstring decorated functions/classes\n+    decorated_items: list[DecoratedItem] = []\n+    for node in ast.walk(tree):\n+        if not isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):\n+            continue\n+        # Find @auto_docstring decorator and extract custom_args if present\n+        decorator_line = None\n+        custom_args_text = None\n+        for dec in node.decorator_list:\n+            if not _is_auto_docstring_decorator(dec):\n+                continue\n+            decorator_line = dec.lineno\n+            # Extract custom_args from @auto_docstring(custom_args=...)\n+            if isinstance(dec, ast.Call):\n+                for kw in dec.keywords:\n+                    if kw.arg == \"custom_args\":\n+                        if isinstance(kw.value, ast.Constant) and isinstance(kw.value.value, str):\n+                            custom_args_text = kw.value.value.strip()\n+                        elif isinstance(kw.value, ast.Name):\n+                            custom_args_text = var_to_string.get(kw.value.id, \"\").strip()\n+            break\n+        if decorator_line is None:  # No @auto_docstring decorator found\n+            continue\n+        # Extract info for this decorated item\n+        kind = \"class\" if isinstance(node, ast.ClassDef) else \"function\"\n+        body_start_line = node.body[0].lineno if node.body else node.lineno + 1\n+        # Extract function arguments (skip self, *args, **kwargs)\n+        arg_names = []\n+        has_init = False\n+        init_def_line = None\n+        is_model_output = False\n+        if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):\n+            # For functions, extract args directly\n+            arg_names = _extract_function_args(node)\n+        elif isinstance(node, ast.ClassDef):\n+            # For classes, look for __init__ method and check if it's a ModelOutput\n+            # Check if class inherits from ModelOutput\n+            for base in node.bases:\n+                if isinstance(base, ast.Name) and \"ModelOutput\" in base.id:\n+                    is_model_output = True\n+                    break\n+            # Look for __init__ method in the class body\n+            for class_item in node.body:\n+                if isinstance(class_item, ast.FunctionDef) and class_item.name == \"__init__\":\n+                    has_init = True\n+                    init_def_line = class_item.lineno\n+                    arg_names = _extract_function_args(class_item)\n+                    # Update body_start_line to be the __init__ body start\n+                    body_start_line = class_item.body[0].lineno if class_item.body else class_item.lineno + 1\n+                    break\n \n-    # Use re.search to find the first match\n-    match = re.search(regex_pattern, file_content, flags)\n+        decorated_items.append(\n+            DecoratedItem(\n+                decorator_line=decorator_line,\n+                def_line=node.lineno,\n+                kind=kind,\n+                body_start_line=body_start_line,\n+                args=arg_names,\n+                custom_args_text=custom_args_text,\n+                has_init=has_init,\n+                init_def_line=init_def_line,\n+                is_model_output=is_model_output,\n+            )\n+        )\n \n-    if match:\n-        # match.group(1) will be the variable_name itself\n-        # match.group(3) will be the content inside the triple quotes\n-        content = match.group(3).strip()\n-        return content\n-    return None\n+    return sorted(decorated_items, key=lambda x: x.decorator_line)\n \n \n def update_file_with_new_docstrings(\n-    candidate_file, lines, line_starts_candidates, line_ends_candidates, overwrite=False\n+    candidate_file,\n+    lines,\n+    decorated_items: list[DecoratedItem],\n+    source: str,\n+    overwrite=False,\n ):\n     \"\"\"\n     For a given file, update the docstrings for all @auto_docstring candidates and write the new content.\n     \"\"\"\n-    content_base_file_new_lines = lines[: line_ends_candidates[0]]\n-    current_line_start = line_starts_candidates[0]\n-    current_line_end = line_ends_candidates[0]\n-    index = 1\n+    if not decorated_items:\n+        return [], [], []\n+\n     missing_docstring_args_warnings = []\n     fill_docstring_args_warnings = []\n     docstring_args_ro_remove_warnings = []\n \n-    while index <= len(line_starts_candidates):\n+    # Build new file content by processing decorated items and unchanged sections\n+    content_base_file_new_lines = []\n+    last_line_added = 0  # Track the last line we've already added to output (0-based)\n+\n+    for index, item in enumerate(decorated_items):\n+        def_line_0 = item.def_line - 1  # Convert to 0-based\n+\n+        # Parse custom_args if present\n         custom_args_dict = {}\n-        auto_docstring_signature_content = \"\".join(lines[current_line_start:current_line_end])\n-        match = re.findall(r\"custom_args=(\\w+)\", auto_docstring_signature_content)\n-        if match:\n-            custom_args_var_name = match[0]\n-            custom_args_var_content = find_custom_args_with_details(\"\\n\".join(lines), custom_args_var_name)\n-            if custom_args_var_content:\n-                custom_args_dict, _ = parse_docstring(custom_args_var_content)\n-        new_docstring = \"\"\n-        modify_class_docstring = False\n-        # Function\n-        if \"    def\" in lines[current_line_end]:\n+        if item.custom_args_text:\n+            custom_args_dict, _ = parse_docstring(item.custom_args_text)\n+\n+        # Generate new docstring based on kind\n+        if item.kind == \"function\":\n             (\n                 new_docstring,\n                 sig_line_end,\n                 docstring_end,\n                 missing_docstring_args,\n                 fill_docstring_args,\n                 docstring_args_ro_remove,\n-            ) = generate_new_docstring_for_function(lines, current_line_end, custom_args_dict)\n-        # Class\n-        elif \"class \" in lines[current_line_end]:\n+            ) = generate_new_docstring_for_function(lines, item, custom_args_dict)\n+        else:  # class\n             (\n                 new_docstring,\n-                class_sig_line_end,\n-                class_docstring_end_line,\n+                sig_line_end,\n+                docstring_end,\n                 missing_docstring_args,\n                 fill_docstring_args,\n                 docstring_args_ro_remove,\n-            ) = generate_new_docstring_for_class(lines, current_line_end, custom_args_dict)\n-            modify_class_docstring = class_sig_line_end is not None\n-        # Add warnings if needed\n-        if missing_docstring_args:\n-            for arg in missing_docstring_args:\n-                missing_docstring_args_warnings.append(f\"    - {arg} line {current_line_end}\")\n-        if fill_docstring_args:\n-            for arg in fill_docstring_args:\n-                fill_docstring_args_warnings.append(f\"    - {arg} line {current_line_end}\")\n-        if docstring_args_ro_remove:\n-            for arg in docstring_args_ro_remove:\n-                docstring_args_ro_remove_warnings.append(f\"    - {arg} line {current_line_end}\")\n-        # Write new lines\n-        if index >= len(line_ends_candidates) or line_ends_candidates[index] > current_line_end:\n-            if \"    def\" in lines[current_line_end]:\n-                content_base_file_new_lines += lines[current_line_end:sig_line_end]\n-                if new_docstring != \"\":\n-                    content_base_file_new_lines += new_docstring.split(\"\\n\")\n-                if index < len(line_ends_candidates):\n-                    content_base_file_new_lines += lines[docstring_end + 1 : line_ends_candidates[index]]\n-                else:\n-                    content_base_file_new_lines += lines[docstring_end + 1 :]\n-            elif modify_class_docstring:\n-                content_base_file_new_lines += lines[current_line_end:class_sig_line_end]\n-                if new_docstring != \"\":\n-                    content_base_file_new_lines += new_docstring.split(\"\\n\")\n-                if index < len(line_ends_candidates):\n-                    content_base_file_new_lines += lines[class_docstring_end_line + 1 : line_ends_candidates[index]]\n-                else:\n-                    content_base_file_new_lines += lines[class_docstring_end_line + 1 :]\n-            elif index < len(line_ends_candidates):\n-                content_base_file_new_lines += lines[current_line_end : line_ends_candidates[index]]\n-            else:\n-                content_base_file_new_lines += lines[current_line_end:]\n-            if index < len(line_ends_candidates):\n-                current_line_end = line_ends_candidates[index]\n-                current_line_start = line_starts_candidates[index]\n-        index += 1\n+            ) = generate_new_docstring_for_class(lines, item, custom_args_dict, source)\n+\n+        # If sig_line_end is None, this item couldn't be processed (e.g., class with no __init__)\n+        # In this case, we don't modify anything and just continue to the next item\n+        if sig_line_end is None:\n+            continue\n+\n+        # Add all lines from last processed line up to current def line\n+        content_base_file_new_lines += lines[last_line_added:def_line_0]\n+\n+        # Collect warnings\n+        for arg in missing_docstring_args:\n+            missing_docstring_args_warnings.append(f\"    - {arg} line {def_line_0}\")\n+        for arg in fill_docstring_args:\n+            fill_docstring_args_warnings.append(f\"    - {arg} line {def_line_0}\")\n+        for arg in docstring_args_ro_remove:\n+            docstring_args_ro_remove_warnings.append(f\"    - {arg} line {def_line_0}\")\n+\n+        # Add lines from current def through signature\n+        content_base_file_new_lines += lines[def_line_0:sig_line_end]\n+\n+        # Add new docstring if generated\n+        if new_docstring:\n+            content_base_file_new_lines += new_docstring.split(\"\\n\")\n+\n+        # Update last_line_added to skip the old docstring\n+        last_line_added = (docstring_end + 1) if docstring_end is not None else sig_line_end\n+\n+    # Add any remaining lines after the last decorated item\n+    content_base_file_new_lines += lines[last_line_added:]\n+\n     content_base_file_new = \"\\n\".join(content_base_file_new_lines)\n     if overwrite:\n         with open(candidate_file, \"w\", encoding=\"utf-8\") as f:\n@@ -1330,12 +1361,6 @@ def update_file_with_new_docstrings(\n     )\n \n \n-# TODO (Yoni): The functions in check_auto_docstrings rely on direct code parsing, which is prone to\n-# failure on edge cases and not robust to code changes. While this approach is significantly faster\n-# than using inspect (like in check_docstrings) and allows parsing any object including non-public\n-# ones, it may need to be refactored in the future to use a more robust parsing method. Note that\n-# we still need auto_docstring for some non-public objects since their docstrings are included in the\n-# docs of public objects (e.g. ModelOutput classes).\n def check_auto_docstrings(overwrite: bool = False, check_all: bool = False):\n     \"\"\"\n     Check docstrings of all public objects that are decorated with `@auto_docstrings`.\n@@ -1351,11 +1376,23 @@ def check_auto_docstrings(overwrite: bool = False, check_all: bool = False):\n     # 3. For each file, update docstrings for all candidates\n     for candidate_file in auto_docstrings_files:\n         with open(candidate_file, \"r\", encoding=\"utf-8\") as f:\n-            lines = f.read().split(\"\\n\")\n-        line_starts_candidates, line_ends_candidates = get_auto_docstring_candidate_lines(lines)\n+            content = f.read()\n+        lines = content.split(\"\\n\")\n+\n+        # Parse file once to find all @auto_docstring decorated items\n+        decorated_items = _build_ast_indexes(content)\n+\n+        if not decorated_items:\n+            continue\n+\n+        # Update docstrings for all decorated items\n         missing_docstring_args_warnings, fill_docstring_args_warnings, docstring_args_ro_remove_warnings = (\n             update_file_with_new_docstrings(\n-                candidate_file, lines, line_starts_candidates, line_ends_candidates, overwrite=overwrite\n+                candidate_file,\n+                lines,\n+                decorated_items,\n+                content,\n+                overwrite=overwrite,\n             )\n         )\n         if missing_docstring_args_warnings:"
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T21:18:04.556069"
  },
  {
    "pr_number": 41421,
    "title": "Restore cuda graphs to continuous batching",
    "body": "This PR restores cuda graphs in continuous batching. The main changes associated with this are:\r\n1. the logic of how to generate tokens have been moved to the CB processor, which also handles the cuda graphs\r\n2. the generation step automatically slices the tensors to remove all padding unless cuda graphs are activated\r\n3. cuda graphs are captured on padded shapes, which is 25%, 50%, 75% or 100% of the queries axis and 1/8, ... 8/8 of the keys values axis, to strike a balance between the amount of padding and the quantity of cuda graphs \r\n\r\nDocumentation is kind of lacking but will be added in next commits, I am opening the PR so @ArthurZucker can test stuff out\r\n\r\n- [x] Add more documentation\r\n- [x] Test it out and add performance numbers with / without on AMD / Nvidia with the three main attn implementations",
    "html_url": "https://github.com/huggingface/transformers/pull/41421",
    "created_at": "2025-10-07T16:03:25Z",
    "merged_at": "2025-10-13T09:57:57Z",
    "merge_commit_sha": "cf1e9834ec7339f4c605ba96d9c4e5cf59594cad",
    "base_ref": "main",
    "head_sha": "1443d62e28fcb43445006f1dd37a0b94d4c92188",
    "user": "remi-or",
    "files": [
      {
        "filename": "examples/pytorch/continuous_batching.py",
        "status": "modified",
        "additions": 32,
        "deletions": 24,
        "changes": 56,
        "patch": "@@ -26,22 +26,25 @@\n \n from transformers import AutoModelForCausalLM, AutoTokenizer\n from transformers.generation import GenerationConfig\n+from transformers.generation.continuous_batching.requests import logger\n \n \n # MODEL_ID = \"Qwen/Qwen3-4B-Instruct-2507\"\n SLIDING_WINDOW = 0\n-MODEL_ID = \"google/gemma-2-2b-it\" if SLIDING_WINDOW > 0 else \"Qwen/Qwen3-4B-Instruct-2507\"\n+MODEL_ID = \"google/gemma-2-2b-it\" if SLIDING_WINDOW > 0 else \"meta-llama/Meta-Llama-3-8B\"\n FORCE_MAX_LENGTH = False  # should be False unless you are debugging sliding window features\n+SKIP_SPECIAL_TOKENS = False\n \n \n def generate_simple(\n     attn_impl: str, simple_batch_inputs: list[int], generation_config: GenerationConfig\n ) -> dict[str, str]:\n     attn_impl = {\n-        \"sdpa_paged\": \"sdpa\",\n-        \"eager_paged\": \"eager\",\n+        \"sdpa\": \"sdpa\",\n+        \"eager\": \"eager\",\n         \"paged_attention\": \"eager\",  # TODO: this does not work on AMD docker\n         \"flash_paged\": \"flash_attention_2\",  # TODO: this does not work on AMD docker\n+        \"kernels-community/flash-attn\": \"eager\",\n     }[attn_impl]\n \n     model = AutoModelForCausalLM.from_pretrained(MODEL_ID, dtype=torch.bfloat16, attn_implementation=attn_impl)\n@@ -56,7 +59,7 @@ def generate_simple(\n         # attention_mask = torch.ones_like(input_ids)\n         outputs = model.generate(input_ids, generation_config=generation_config, use_model_defaults=False)\n         generated_tokens = outputs[0][input_ids.shape[1] :]\n-        decoded_output = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n+        decoded_output = tokenizer.decode(generated_tokens, skip_special_tokens=SKIP_SPECIAL_TOKENS)\n         decoded_outputs[key] = decoded_output\n     return decoded_outputs\n \n@@ -99,7 +102,6 @@ def batch_generate(\n     displayed_samples: int = 0,  # -1: no display, 0: display stats, >0: display inputs and some outputs\n     output_file: Optional[str] = None,\n     expected_outputs: Optional[list[str]] = None,\n-    slice_inputs: bool = True,\n ) -> tuple[float, float]:\n     # Actual batch generation\n     if displayed_samples >= 0:\n@@ -108,7 +110,6 @@ def batch_generate(\n     batch_outputs = model.generate_batch(\n         inputs=simple_batch_inputs,\n         generation_config=generation_config,\n-        slice_inputs=slice_inputs,  # TODO: move this to the generation config\n     )\n     end_time_simple = time.time()\n     if displayed_samples >= 0:\n@@ -118,19 +119,21 @@ def batch_generate(\n     token_count = 0\n     data = []\n     for i, request in enumerate(batch_outputs):\n-        input_text = tokenizer.decode(batch_outputs[request].prompt_ids, skip_special_tokens=True)\n+        input_text = tokenizer.decode(batch_outputs[request].prompt_ids, skip_special_tokens=SKIP_SPECIAL_TOKENS)\n         # The key is used to tie back to the output of unbatched generation\n         key = \" \".join(map(str, batch_outputs[request].prompt_ids))\n         data.append({\"input\": input_text, \"key\": key})\n \n         # Try to decode the output\n         try:\n-            output_text = tokenizer.decode(batch_outputs[request].generated_tokens, skip_special_tokens=True)\n+            output_text = tokenizer.decode(\n+                batch_outputs[request].generated_tokens, skip_special_tokens=SKIP_SPECIAL_TOKENS\n+            )\n             token_count += len(batch_outputs[request].generated_tokens[1:])\n-            data[-1][\"output\"] = output_text\n+            data[-1][\"cb_outputs\"] = output_text\n         except Exception as e:\n             print(f\"Decoding failed for request {request}: {e}\")\n-            data[-1][\"output\"] = \"__ERROR__\"\n+            data[-1][\"cb_outputs\"] = \"__ERROR__\"\n             continue\n \n         # Display sample if asked\n@@ -148,7 +151,7 @@ def batch_generate(\n         if expected_outputs is not None:\n             expected_output = expected_outputs.pop(key)\n             matches = output_text == expected_output  # TODO: rework this for a better distance metric\n-            data[-1][\"ref\"] = expected_output\n+            data[-1][\"without_cb\"] = expected_output\n             data[-1][\"matches\"] = matches\n             data[-1].pop(\"key\")\n             print(f\"Request {i} matches\" if matches else f\"Request {i} does NOT match!\")\n@@ -186,19 +189,20 @@ def batch_generate(\n \n     parser.add_argument(\"--attn\", type=str, default=\"kernels-community/flash-attn\", help=\"Attention implementation\")\n     parser.add_argument(\"--matmul-precision\", \"-mp\", type=str, default=\"high\")  # set to \"none\" to disable\n-    parser.add_argument(\"--no-slice-inputs\", action=\"store_true\")  # slicing is enabled by default because much faster\n-    parser.add_argument(\"--use-cuda-graph\", \"-cg\", action=\"store_true\")\n-    parser.add_argument(\"--compile\", action=\"store_true\")\n+    parser.add_argument(\"--cuda-graph\", \"-cg\", help=\"Use cuda graphs\", type=str, default=None)\n+    parser.add_argument(\"--compile\", action=\"store_true\", help=\"Compile the model using torch.compile\")\n \n-    parser.add_argument(\"--samples\", type=int, default=500)\n+    parser.add_argument(\"--samples\", type=int, default=500, help=\"Number of samples to generate\")\n     parser.add_argument(\"--displayed\", type=int, default=0, help=\"Number of samples to display\")\n+    parser.add_argument(\"--log-level\", type=str, default=\"INFO\")\n     parser.add_argument(\"--output-file\", type=str, default=None)\n     parser.add_argument(\"--compare\", action=\"store_true\")\n     parser.add_argument(\"--metrics\", action=\"store_true\")\n     parser.add_argument(\"--profile\", type=str, default=None)\n     args = parser.parse_args()\n \n-    args.slice_inputs = not args.no_slice_inputs\n+    # Set log level\n+    logger.setLevel(args.log_level.upper())\n \n     # If turned on, we setup metrics\n     if args.metrics:\n@@ -207,6 +211,15 @@ def batch_generate(\n     # Set matmul precision if not none\n     if args.matmul_precision != \"none\":\n         torch.set_float32_matmul_precision(args.matmul_precision)\n+    # Parse cuda graph argument\n+    if args.cuda_graph is not None:\n+        use_cuda_graph = {\n+            \"none\": None,\n+            \"yes\": True, \"y\": True, \"true\": True, \"t\": True, \"1\": True,\n+            \"no\": False, \"n\": False, \"false\": False, \"f\": False, \"0\": False,\n+        }[args.cuda_graph.lower()]  # fmt: skip\n+    else:\n+        use_cuda_graph = None\n \n     # Prepare model\n     model = AutoModelForCausalLM.from_pretrained(\n@@ -222,9 +235,6 @@ def batch_generate(\n     # If turned on, we compile the model\n     if args.compile:\n         model.forward = torch.compile(model.forward, mode=\"max-autotune-no-cudagraphs\")\n-    if args.slice_inputs:\n-        assert not args.compile, \"Slicing inputs requires is not the model to be compiled\"\n-        assert not args.use_cuda_graph, \"Slicing inputs is not compatible with cuda graphs\"\n \n     # Prepare tokenizer and dataset\n     tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, padding_side=\"left\")\n@@ -237,10 +247,10 @@ def batch_generate(\n     # Prepare generation config\n     generation_config = GenerationConfig(\n         max_new_tokens=512,\n-        use_cuda_graph=args.use_cuda_graph,\n+        use_cuda_graph=use_cuda_graph,\n         eos_token_id=tokenizer.pad_token_id if FORCE_MAX_LENGTH else tokenizer.eos_token_id,\n         pad_token_id=tokenizer.pad_token_id,\n-        do_sample=True,\n+        do_sample=not args.compare,\n         temperature=0.8,\n         top_p=0.9,\n         num_blocks=args.num_blocks,\n@@ -265,7 +275,6 @@ def batch_generate(\n         generation_config,\n         tokenizer,\n         displayed_samples=-1,\n-        slice_inputs=args.slice_inputs,\n     )\n \n     if args.profile is not None:\n@@ -282,12 +291,11 @@ def batch_generate(\n             displayed_samples=args.displayed,\n             output_file=args.output_file,\n             expected_outputs=expected_outputs,\n-            slice_inputs=args.slice_inputs,\n         )\n     if args.profile is not None:\n         filename = args.profile if args.profile.endswith(\".json\") else args.profile + \".json\"\n         prof.export_chrome_trace(filename)\n \n # Example usage:\n-# python examples/pytorch/continuous_batching.py --attn sdpa_paged -mp none --slice-inputs --samples 3 --compare\n+# python examples/pytorch/continuous_batching.py --attn sdpa_paged -mp none --samples 3 --compare\n # python examples/pytorch/continuous_batching.py --num-blocks 369 --max-batch-tokens 23 --attn sdpa_paged -mp none --samples 1 --displayed 0 --output-file sliced.json"
      },
      {
        "filename": "examples/pytorch/continuous_batching_simple.py",
        "status": "modified",
        "additions": 0,
        "deletions": 2,
        "changes": 2,
        "patch": "@@ -68,7 +68,6 @@\n     _ = model.generate_batch(\n         inputs=simple_batch_inputs[: min(5, args.samples)],\n         generation_config=generation_config,\n-        slice_inputs=True,\n     )\n \n     # Actual batch generation\n@@ -77,7 +76,6 @@\n     batch_outputs = model.generate_batch(\n         inputs=simple_batch_inputs,\n         generation_config=generation_config,\n-        slice_inputs=True,\n     )\n     end_time = time.time()\n     print(\"Done with batch generation.\")"
      },
      {
        "filename": "src/transformers/generation/continuous_batching/cache.py",
        "status": "modified",
        "additions": 5,
        "deletions": 6,
        "changes": 11,
        "patch": "@@ -204,8 +204,8 @@ def __init__(\n         # Initialize the cache\n         self.key_cache: list[torch.Tensor] = []\n         self.value_cache: list[torch.Tensor] = []\n-        # We add one extra token to the cache to handle padding and generally discard unwanted tokens\n-        self.cache_shape = (num_blocks * self.block_size + 1, self.num_key_value_heads, self.head_dim)\n+        # We add two extra tokens to the cache to handle padding and generally discard unwanted tokens\n+        self.cache_shape = (num_blocks * self.block_size + 2, self.num_key_value_heads, self.head_dim)\n         for _ in range(group_size):\n             new_layer_key_cache = torch.empty(self.cache_shape, dtype=self.dtype, device=self.device)\n             new_layer_value_cache = torch.empty(self.cache_shape, dtype=self.dtype, device=self.device)\n@@ -290,7 +290,6 @@ def update(\n         layer_idx: int,\n         read_index: list[torch.Tensor],  # shape [num_layer_groups, seqlen_kv + past_length]\n         write_index: list[torch.Tensor],  # shape [num_layer_groups, seqlen_q]\n-        **kwargs,\n     ) -> tuple[torch.Tensor, torch.Tensor]:  # shape [seqlen_kv + past_length, num_kv_heads, head_dim]\n         \"\"\"Update the cache with new key-value states for a specific layer. This method writes new KV states to the\n         appropriate cache locations. The behavior differs based on the layer's attention type:\n@@ -324,11 +323,11 @@ def update(\n         # the only case where you may write over cache you need to use\n         else:\n             # Add the cache to the key and value states\n-            mask = layer_read_index == -1  # TODO: can this can be efficiently precomputed?\n+            mask = (layer_read_index == -1).unsqueeze(-1).unsqueeze(-1)  # TODO: should this be precomputed?\n             key_states_with_cache = k_cache[layer_read_index, :, :]\n-            key_states_with_cache[mask] = key_states\n+            key_states_with_cache.masked_scatter_(mask, key_states)\n             value_states_with_cache = v_cache[layer_read_index, :, :]\n-            value_states_with_cache[mask] = value_states\n+            value_states_with_cache.masked_scatter_(mask, value_states)\n             # Write new KV values to the cache\n             k_cache[layer_write_index, :, :] = key_states\n             v_cache[layer_write_index, :, :] = value_states"
      },
      {
        "filename": "src/transformers/generation/continuous_batching/continuous_api.py",
        "status": "modified",
        "additions": 316,
        "deletions": 195,
        "changes": 511,
        "patch": "@@ -15,18 +15,21 @@\n # limitations under the License.\n import queue\n import threading\n+from collections.abc import Generator\n from dataclasses import dataclass\n from functools import partial\n from itertools import count\n+from math import ceil\n from time import perf_counter\n from typing import Optional, Union\n \n import torch\n from torch import nn\n from tqdm import tqdm\n \n-from ...configuration_utils import PreTrainedConfig\n+from ...configuration_utils import PretrainedConfig\n from ...generation.configuration_utils import GenerationConfig\n+from ...generation.logits_process import LogitsProcessor\n from ...integrations.hub_kernels import load_and_register_attn_kernel\n from ...utils.logging import logging\n from ...utils.metrics import ContinuousBatchProcessorMetrics, attach_tracer, traced\n@@ -35,10 +38,44 @@\n from .scheduler import SCHEDULER_MAPPING, FIFOScheduler, Scheduler\n \n \n+\"\"\"\n+To enable cuda graphs, we need the dimensions of all tensors to be static, which is counter-intuitive for CB. In CB, as\n+generation goes on, there are two dimensions that change:\n+- the number of queries tokens (Q), which can vary from batch to batch\n+- the number of keys/values tokens (KV), which grows as the cache does\n+\n+To solve this, we slice along those dimensions to fixed lengths. The size of the slices is controlled by the variables\n+below: NUM_X_CUDA_GRAPHS means that we create at most NUM_X_CUDA_GRAPHS graphs for the X dimension. So if the maximum\n+number of queries tokens is 1000, and NUM_Q_CUDA_GRAPHS is 4, we will slice the number of queries token by intervals of\n+1000 / 4 = 250 tokens, ie. to 250, 500, 750 or 1000 queries tokens.\n+\n+Smaller slices means more granularity and thus less padding. But since each graph takes up space on the GPU and time to\n+create, we don't want to many graphs. And since the size of the KV dimension is the number of queries tokens plus the\n+number of tokens cached, dimension of KV is usually much larger than the the dimension of Q. So we have more granularity\n+for the KV dimension than the query dimension.\n+\"\"\"\n+NUM_Q_CUDA_GRAPHS = 4\n+NUM_KV_CUDA_GRAPHS = 8\n+\n+\n+def pad_by_intervals(size: int, max_value: int, nb_intervals: int) -> int:\n+    \"\"\"Return the smallest multiple of (max_value) // (nb_intervals) greater than (size).\"\"\"\n+    interval_size = max_value // nb_intervals\n+    if interval_size == 0:\n+        return max_value\n+    padded = ceil(size / interval_size) * interval_size\n+    return min(padded, max_value)\n+\n+\n+def attn_mask_is_needed(config: PretrainedConfig) -> bool:\n+    \"\"\"Checks if attention mask is needed for the given (config).\"\"\"\n+    return config._attn_implementation in [\"paged|eager\", \"paged|sdpa\"]\n+\n+\n def build_attention_mask(\n     attention_mask: torch.Tensor,\n-    cumulative_seqlens_q: torch.Tensor,\n-    cumulative_seqlens_k: torch.Tensor,\n+    cumulative_seqlens_q: list[int],\n+    cumulative_seqlens_k: list[int],\n     sliding_window: int = 1,\n ) -> None:\n     \"\"\"Builds an attention mask inplace using the cumulative seqlens of the query and key. If given a sliding window, it\n@@ -57,7 +94,7 @@ def build_attention_mask(\n            \u2588 \u2588 \u2588 \u2588 \u2588 \u2588 \u2588 \u2588\n \n     SLIDING WINDOW MASK:\n-         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 seqlen_k - seqlen_q - sliding_window = 8 - 4 - 6 = -2 offset to the right\n+         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 seqlen_k - seqlen_q - sliding_window = 8 - 4 - 6 = -2 offset to the left\n        <\u2500\u2534\u2500>\n      \u2591 \u2588 | \u2588 \u2588 \u2588 \u2588 \u2588 \u2588 \u2588 \u2588\n      \u2591 \u2591 | \u2588 \u2588 \u2588 \u2588 \u2588 \u2588 \u2588 \u2588\n@@ -80,7 +117,7 @@ def build_attention_mask(\n            \u2588 \u2588 \u2588 \u2588 \u2588\n \n     SLIDING WINDOW MASK:\n-         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 seqlen_k - seqlen_q - sliding_window = 5 - 3 - 2 = 0 offset to the right\n+         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 seqlen_k - seqlen_q - sliding_window = 5 - 3 - 2 = 0 offset to the left\n         <\u2534>\n          | \u2591 \u2588 \u2588 \u2588 \u2588\n          | \u2591 \u2591 \u2588 \u2588 \u2588\n@@ -141,16 +178,16 @@ class ContinuousBatchProcessor:\n     def __init__(\n         self,\n         cache: PagedAttentionCache,\n-        config: PreTrainedConfig,\n+        config: PretrainedConfig,\n         generation_config: GenerationConfig,\n         input_queue: queue.Queue,\n         output_queue: queue.Queue,\n         stop_event: threading.Event,\n         model_device: torch.device,\n         model_dtype: torch.dtype,\n         scheduler: Scheduler,\n-        manual_eviction: bool = False,\n-        slice_inputs: bool = True,  # TODO: There should be an heuristic to decide on slicing, compile, cuda graphs...\n+        manual_eviction: bool,\n+        use_cuda_graph: bool,\n     ) -> None:\n         \"\"\"Initialize the continuous batch processor.\n \n@@ -165,7 +202,8 @@ def __init__(\n             model_dtype: Data type for model inputs/outputs\n             scheduler: The [`Scheduler`] to use\n             manual_eviction: Whether to manually evict blocks from the cache\n-            slice_inputs: Whether to slice the inputs to the model\n+            use_cuda_graph: Whether to use cuda graphs or not during CB. Check the docstring at the top of the file for\n+                more details.\n         \"\"\"\n         self.cache = cache\n         self.config = config\n@@ -177,36 +215,39 @@ def __init__(\n         self.model_dtype = model_dtype\n         self.scheduler = scheduler\n         self.manual_eviction = manual_eviction\n-        self.slice_inputs = slice_inputs\n \n         # Retrieve the size of the sliding window if there is one\n         self.sliding_window = 1 if getattr(config, \"sliding_window\", None) is None else config.sliding_window\n-\n+        # Accumulator for batch scheduling\n         self.requests_in_batch: list[RequestState] = []\n+        # Cuda graphs for the generation step\n+        self._graphs: Optional[dict[tuple[int, int], torch.cuda.CUDAGraph]] = {} if use_cuda_graph else None\n \n         # Set up metrics collector\n         self.max_batch_tokens = cache.max_batch_tokens\n         self.metrics = ContinuousBatchProcessorMetrics(cache.max_batch_tokens)\n \n         # Setup static tensors\n-        self.total_query_length = 0\n-        self.total_key_length = 0\n-        self.total_batch_size = 0\n+        self.actual_query_length = 0  # This is the actual number of queries tokens in the batch\n+        self.actual_key_length = 0  # This is the actual number of keys/values tokens in the batch\n+        self.actual_batch_size = 0  # This is the actual number of requests in the batch\n+        self.actual_index_sizes = [(0, 0) for _ in range(cache.num_groups)]\n         self.setup_static_tensors(cache.num_groups)\n \n     @traced(standalone=True)\n     def setup_static_tensors(self, num_groups: int) -> None:\n-        T = self.max_batch_tokens\n+        \"\"\"Setup the static tensors that are used for storage during the generation step. No other tensor will be\n+        allowed for the inputs or the outputs of the generation step.\"\"\"\n         num_pages = self.cache.num_blocks * self.cache.block_size\n         self.tensor_metadata = {\"dtype\": torch.int32, \"device\": self.model_device}\n \n         # Some tensors always have the same shape regardless of the model\n-        self.input_ids = torch.empty((1, T), **self.tensor_metadata)\n-        self.position_ids = torch.empty((1, T), **self.tensor_metadata)\n-        self.cumulative_seqlens_q = torch.empty((T + 1,), **self.tensor_metadata)\n+        self.input_ids = torch.empty((1, self.max_batch_tokens), **self.tensor_metadata)\n+        self.position_ids = torch.empty((1, self.max_batch_tokens), **self.tensor_metadata)\n+        self.cumulative_seqlens_q = torch.empty((self.max_batch_tokens + 1,), **self.tensor_metadata)\n         self.max_seqlen_q = 0\n-        self.logits_indices = torch.empty((T,), **self.tensor_metadata)\n-        self.output_ids = torch.empty((1, T), **self.tensor_metadata)\n+        self.logits_indices = torch.empty((self.max_batch_tokens,), **self.tensor_metadata)\n+        self.output_ids = torch.empty((1, self.max_batch_tokens), **self.tensor_metadata)\n \n         # For some kwargs, we have a dict of tensors with as many items as there are attention types\n         layer_types = getattr(self.config, \"layer_types\", None)\n@@ -216,13 +257,13 @@ def setup_static_tensors(self, num_groups: int) -> None:\n         layer_types = list(set(layer_types))\n \n         self.cumulative_seqlens_k = {\n-            layer_type: torch.empty((T + 1), **self.tensor_metadata) for layer_type in layer_types\n+            l_type: torch.empty((self.max_batch_tokens + 1), **self.tensor_metadata) for l_type in layer_types\n         }\n         self.max_seqlen_k = dict.fromkeys(layer_types, 0)\n \n-        if self.return_attention_mask():\n+        if attn_mask_is_needed(self.config):\n             attn_mask_kwargs = {\n-                \"size\": (1, 1, T, num_pages + T),\n+                \"size\": (1, 1, self.max_batch_tokens, num_pages + self.max_batch_tokens),\n                 \"dtype\": self.model_dtype,\n                 \"device\": self.model_device,\n             }\n@@ -231,33 +272,26 @@ def setup_static_tensors(self, num_groups: int) -> None:\n             self.attention_mask = None\n \n         # For other kwargs, we need a list of tensors with as many tensors as there are groups\n-        self.write_index_storage = [torch.empty((T,), **self.tensor_metadata) for _ in range(num_groups)]\n-        self.read_index_storage = [torch.empty((num_pages + T), **self.tensor_metadata) for _ in range(num_groups)]\n+        self.write_index_storage = [\n+            torch.empty((self.max_batch_tokens,), **self.tensor_metadata) for _ in range(num_groups)\n+        ]\n+        self.read_index_storage = [\n+            torch.empty((num_pages + self.max_batch_tokens), **self.tensor_metadata) for _ in range(num_groups)\n+        ]\n         # For read index, the +T is because there are -1 for seqlen_q when model uses a sliding window\n \n         # After allocating empty tensors, we reset them to the right value\n         self.reset_static_tensors(full_reset=True)\n \n-    def return_attention_mask(self) -> bool:\n-        return self.config._attn_implementation in [\n-            \"paged|eager\",\n-            \"paged|sdpa\",\n-        ]  # we set `is_causal` to True in paged call\n-\n     @traced\n     @torch.no_grad()\n-    def reset_static_tensors(self, full_reset: bool = False):\n+    def reset_static_tensors(self, full_reset: bool = False) -> None:\n         \"\"\"Reset static tensors for the next batch. In between batches, reset only the parts that were used in the last\n         batch, but for initialisation, we can reset everything using the (full_reset) flag.\"\"\"\n         # Compute the slice to reset\n-        if full_reset or not self.slice_inputs:\n-            q_len = self.write_index_storage[0].size(-1)\n-            k_len = self.read_index_storage[0].size(-1)\n-            b_size = self.write_index_storage[0].size(0)\n-        else:\n-            q_len = self.total_query_length\n-            k_len = self.total_key_length\n-            b_size = self.total_batch_size\n+        q_len = self.write_index_storage[0].size(-1) if full_reset else self.actual_query_length\n+        k_len = self.read_index_storage[0].size(-1) if full_reset else self.actual_key_length\n+        b_size = self.write_index_storage[0].size(0) if full_reset else self.actual_batch_size\n \n         # Reset the attributes that always have the same shape\n         self.input_ids[:, :q_len].zero_()\n@@ -276,14 +310,19 @@ def reset_static_tensors(self, full_reset: bool = False):\n \n         # Reset the attributes that are lists of tensors\n         for i in range(self.cache.num_groups):\n-            self.write_index_storage[i][:q_len].fill_(-1)\n-            self.read_index_storage[i][: q_len + k_len].fill_(-1)\n-\n-    def get_model_kwargs(self) -> PagedAttentionArgs:\n-        \"\"\"Get model keyword arguments for the current batch.\"\"\"\n-        # Compute the slice to return\n-        q_len = self.total_query_length if self.slice_inputs else self.write_index_storage[0].size(-1)\n-        b_size = self.total_batch_size if self.slice_inputs else self.cumulative_seqlens_q.size(-1) - 1\n+            self.write_index_storage[i][:q_len].fill_(-2)  # -1 is used to let the cache where new states go\n+            self.read_index_storage[i][: q_len + k_len].fill_(-2)  # same\n+\n+    def get_model_kwargs(self, padded_q_size: int = 0, padded_kv_cache_size: int = 0) -> PagedAttentionArgs:\n+        \"\"\"Get model keyword arguments for the current batch, eventually padding the query dimension to (padded_q_size)\n+        and the keys/values dimension to (padded_kv_cache_size). The padding is only useful if we want static shapes,\n+        like when using cuda graphs AND only activated if both Q and KV are padded.\"\"\"\n+        # Compute the slice to return, with the given padding if we are using cuda graphs\n+        use_padding = padded_q_size > 0 and padded_kv_cache_size > 0\n+        q_len = padded_q_size if use_padding else self.actual_query_length\n+        b_size = padded_q_size if use_padding else self.actual_batch_size\n+        # If there is padding, the size of the KV is the nb of padded Q tokens + the size padded of the padded KV cache\n+        padded_kv_size = padded_q_size + padded_kv_cache_size\n \n         # Prepare the kwargs, the attributes that are either tensors or dict of tensors are initialized to empty dicts\n         kwargs = {\n@@ -295,43 +334,57 @@ def get_model_kwargs(self) -> PagedAttentionArgs:\n             \"cu_seq_lens_k\": {},\n             \"max_seqlen_k\": {},\n             \"attention_mask\": {},\n-            \"read_index\": self.read_index,  # slicing is done during building\n-            \"write_index\": self.write_index,  # slicing is done during building\n+            \"read_index\": [],\n+            \"write_index\": [],\n             \"cache\": self.cache,\n             \"use_cache\": False,\n         }\n \n+        # If we use constant-sized slicing, there are some \"padding\" queries tokens which FA has some issues with. In\n+        # some models like Qwen3-4B-Instruct-2507, if we don't include these tokens in cumulative_seqlens_q, there are\n+        # some NaNs in the output logits even for non-padded tokens.\n+        if use_padding:\n+            self.max_seqlen_q = max(self.max_seqlen_q, q_len - self.total_seqlen_q)\n+            self.cumulative_seqlens_q[self.actual_batch_size + 1 :] = q_len\n+            # FIXME: is there another way to avoid this? It has a very slight impact on performance (~5 tok/s)\n+\n+        # For the attributes that are lists of tensors, we construct list of tensor references\n+        for i, (read_index_size, write_index_size) in enumerate(self.actual_index_sizes):\n+            read_index_size = padded_kv_size if use_padding else read_index_size\n+            write_index_size = padded_q_size if use_padding else write_index_size\n+            kwargs[\"read_index\"].append(self.read_index_storage[i][:read_index_size])\n+            kwargs[\"write_index\"].append(self.write_index_storage[i][:write_index_size])\n+\n         # For the attributes that are dict of tensors, we replace the dict with a tensor if there is only one entry\n         layer_types = list(self.cumulative_seqlens_k.keys())\n         if len(layer_types) > 1:\n             for layer_type, seqlens_k in self.cumulative_seqlens_k.items():\n                 kwargs[\"cu_seq_lens_k\"][layer_type] = seqlens_k[: b_size + 1]\n                 kwargs[\"max_seqlen_k\"][layer_type] = self.max_seqlen_k[layer_type]\n                 if self.attention_mask is not None:\n-                    k_len = seqlens_k[b_size] if self.slice_inputs else self.attention_mask[layer_type].size(-1)\n+                    k_len = padded_kv_size if use_padding else seqlens_k[b_size]\n                     kwargs[\"attention_mask\"][layer_type] = self.attention_mask[layer_type][..., :q_len, :k_len]\n         else:\n             layer_type = layer_types[0]\n             kwargs[\"cu_seq_lens_k\"] = self.cumulative_seqlens_k[layer_type][: b_size + 1]\n             kwargs[\"max_seqlen_k\"] = self.max_seqlen_k[layer_type]\n             if self.attention_mask is not None:\n-                k_len = self.cumulative_seqlens_k[layer_type][b_size]\n-                k_len = k_len if self.slice_inputs else self.attention_mask[layer_type].size(-1)\n+                k_len = padded_kv_size if use_padding else self.cumulative_seqlens_k[layer_type][b_size]\n                 kwargs[\"attention_mask\"] = self.attention_mask[layer_type][..., :q_len, :k_len]\n \n         if self.attention_mask is None:\n             kwargs[\"attention_mask\"] = None\n         return kwargs\n \n-    def __repr__(self):\n+    def __repr__(self) -> str:\n         return (\n             f\"ContinuousBatchProcessor(input_queue={self.input_queue}, output_queue={self.output_queue}, \"\n             f\"active_requests={self.scheduler.active_requests}, waiting_requests={self.scheduler.waiting_requests})\"\n             + self.get_model_kwargs().__repr__()\n         )\n \n     @traced\n-    def _get_new_requests(self):\n+    def _get_new_requests(self) -> None:\n         \"\"\"Pull new requests from the input queue and add to waiting list.\"\"\"\n         while not self.input_queue.empty():\n             try:\n@@ -349,7 +402,7 @@ def _get_new_requests(self):\n                     self._handle_request_error(e, state)\n \n     @traced\n-    def _handle_request_error(self, error, state: RequestState):\n+    def _handle_request_error(self, error: Exception, state: RequestState) -> None:\n         \"\"\"Handle general request processing error.\"\"\"\n         state.status = RequestStatus.FAILED\n         state.error = str(error)\n@@ -382,12 +435,12 @@ def prepare_next_batch(self) -> bool:\n         self.metrics.record_batch_metrics(self.requests_in_batch)\n \n         # Reset the static tensors used for storage\n-        self.reset_static_tensors()  # TODO: with slice_inputs, this might be unnecessary\n+        self.reset_static_tensors()  # TODO: this might be unnecessary\n \n         # Prepare accumulators\n-        self.total_query_length = 0\n-        self.total_key_length = 0\n-        self.total_batch_size = 0\n+        self.actual_query_length = 0\n+        self.actual_key_length = 0\n+        self.actual_batch_size = 0\n \n         input_ids = []\n         position_ids = []\n@@ -410,10 +463,10 @@ def prepare_next_batch(self) -> bool:\n             seqlens_k = self.cache.get_seqlens_k(state.request_id, past_length, query_length)\n \n             # Then we update the total lengths that are used for slicing\n-            self.total_query_length += query_length\n+            self.actual_query_length += query_length\n             # total_key_length is used to slice the keys so we need to take the max of all the key lengths\n-            self.total_key_length += max(seqlens_k.values())\n-            self.total_batch_size += 1\n+            self.actual_key_length += max(seqlens_k.values())\n+            self.actual_batch_size += 1\n             # And the attribute tracking the position in the request object\n             state.position_offset += query_length\n \n@@ -476,6 +529,7 @@ def _build_tensors(\n         self.position_ids[:, : len(position_ids)] = to_tensor(position_ids)\n         self.cumulative_seqlens_q[: len(cumulative_seqlens_q)] = to_tensor(cumulative_seqlens_q)\n         self.logits_indices[: len(logits_indices)] = to_tensor(logits_indices)\n+        self.total_seqlen_q = cumulative_seqlens_q[-1]\n \n         # Those kwargs are either dict of tensors or tensors, so we need to handle both cases\n         for layer_type, layer_type_seqlens_k in cumulative_seqlens_k.items():\n@@ -492,42 +546,32 @@ def _build_tensors(\n         self.read_index = []\n         self.write_index = []\n         for i, group_read_indices, group_write_indices in zip(count(), read_index, write_index):\n-            # Write in the actual tensors\n             self.read_index_storage[i][: len(group_read_indices)] = to_tensor(group_read_indices)\n             self.write_index_storage[i][: len(group_write_indices)] = to_tensor(group_write_indices)\n-            # Slice to the right size\n-            r = len(group_read_indices) if self.slice_inputs else self.read_index_storage[i].size(-1)\n-            w = len(group_write_indices) if self.slice_inputs else self.write_index_storage[i].size(-1)\n-            # Add to the index\n-            self.read_index.append(self.read_index_storage[i][:r])\n-            self.write_index.append(self.write_index_storage[i][:w])\n+            self.actual_index_sizes[i] = (len(group_read_indices), len(group_write_indices))\n \n     @traced\n-    def _sync(self):\n+    def _sync(self) -> list[int]:\n         if self.output_ids is not None:\n             try:\n-                out = self.output_ids.tolist()[0]  # should be the only sync we do\n+                return self.output_ids.tolist()[0]\n             except Exception:\n-                out = [0, 1]\n-        else:\n-            out = [0, 0]\n-        return out\n+                return [0, 1]\n+        return [0, 0]\n \n     @traced\n-    def _maybe_send_output(self, state: RequestState, token: int):\n+    def _maybe_send_output(self, state: RequestState) -> None:\n         \"\"\"Send output to the queue based on streaming mode and request state.\"\"\"\n         if state.streaming:\n             self.output_queue.put(state.to_generation_output())\n         elif state.status == RequestStatus.FINISHED:\n             self.output_queue.put(state.to_generation_output())\n \n     @traced\n-    def update_batch(self):\n+    def update_batch(self) -> None:\n         \"\"\"Update request states based on generated tokens.\"\"\"\n         out_tokens = self._sync()\n-        finished_request_ids = []\n         for i, state in enumerate(self.requests_in_batch):\n-            req_id = state.request_id\n             if len(state.remaining_prompt_ids) == 0:\n                 self.metrics.record_ttft_metric(state.created_time, state.request_id)\n                 state.status = RequestStatus.DECODING\n@@ -536,8 +580,7 @@ def update_batch(self):\n                 if state.update_with_token(token):\n                     self.metrics.record_request_completion(state.created_time, state.request_id)\n                     self.scheduler.finish_request(state.request_id, evict_from_cache=(not self.manual_eviction))\n-                    finished_request_ids.append(req_id)\n-                self._maybe_send_output(state, token)\n+                self._maybe_send_output(state)\n             elif state.status == RequestStatus.PREFILLING_SPLIT:\n                 state.status = RequestStatus.SPLIT_PENDING_REMAINDER\n         if self.cache.get_num_free_blocks() == 0:\n@@ -557,7 +600,7 @@ def handle_batch_error(self, error):\n             self.scheduler.finish_request(req.request_id)\n \n     @traced\n-    def fail_all_requests(self, error):\n+    def fail_all_requests(self, error: Exception) -> None:\n         \"\"\"Fail all active requests with the given error.\n \n         Args:\n@@ -577,6 +620,95 @@ def fail_all_requests(self, error):\n         # Clear the ordering queue\n         self.scheduler.waiting_requests_order.clear()\n \n+    @traced\n+    @torch.no_grad\n+    def _generation_step(self, model: nn.Module, logit_processor: LogitsProcessor, do_sample: bool) -> None:\n+        \"\"\"Perform a single generation step.\"\"\"\n+\n+        # If cuda graphs are disabled, we just use the actual size\n+        if self._graphs is None:\n+            batch_data = self.get_model_kwargs()\n+            self._forward_process_and_sample(model, batch_data, logit_processor, do_sample)\n+            return None\n+\n+        # Determine the padded size of the queries and keys/values\n+        padded_q = pad_by_intervals(self.actual_query_length, self.max_batch_tokens, NUM_Q_CUDA_GRAPHS)\n+\n+        max_read_index_size = max(self.actual_index_sizes[i][0] for i in range(self.cache.num_groups))\n+        padded_read_index_size = pad_by_intervals(\n+            max_read_index_size - self.max_batch_tokens,\n+            self.cache.num_blocks * self.cache.block_size,\n+            NUM_KV_CUDA_GRAPHS,\n+        )\n+\n+        # Get the batch data and the associated graph\n+        batch_data = self.get_model_kwargs(padded_q, padded_read_index_size)\n+\n+        graph = self._graphs.get((padded_q, padded_read_index_size))\n+\n+        # If we have a graph that fits, we replay it\n+        if graph is not None:\n+            graph.replay()\n+            return None\n+\n+        # Otherwise, we need to create it\n+        logger.info(f\"Creating graph for {(padded_q, padded_read_index_size) = }\")\n+        stream = torch.cuda.Stream(device=model.device)\n+        stream.wait_stream(torch.cuda.current_stream())\n+        # Warmup\n+        with torch.cuda.stream(stream):\n+            self._forward_process_and_sample(model, batch_data, logit_processor, do_sample)\n+        torch.cuda.current_stream().wait_stream(stream)\n+        # Catpure\n+        graph = torch.cuda.CUDAGraph()\n+        with torch.cuda.graph(graph, stream=stream):\n+            self._forward_process_and_sample(model, batch_data, logit_processor, do_sample)\n+        self._graphs[(padded_q, padded_read_index_size)] = graph\n+\n+    @traced\n+    def _forward_process_and_sample(\n+        self, model: nn.Module, batch_data: dict, logit_processor: LogitsProcessor, do_sample: bool\n+    ) -> None:\n+        \"\"\"This function performs the forward pass, logits processing, and sampling; which are broken down into smaller\n+        function to be easier to trace with OpenTelemetry.\"\"\"\n+        # with torch.no_grad():\n+        logits = self._model_forward(model, batch_data)\n+        # if self.log_prob_generation:    batch_processor.output_probs.copy_(logits)  # TODO\n+        probs = self._process_logit(batch_data, logits, logit_processor)\n+        self._sample(probs, do_sample)\n+\n+    @traced(span_name=\"model_forward\")\n+    def _model_forward(self, model: nn.Module, batch_data: dict) -> torch.Tensor:\n+        return model(**batch_data).logits\n+\n+    @traced(span_name=\"logit_processing\")\n+    def _process_logit(self, batch_data: dict, logits: torch.Tensor, logit_processor: LogitsProcessor) -> torch.Tensor:\n+        # Pass continuous batching context to logits processor if it supports it.\n+        if hasattr(logit_processor, \"set_continuous_batching_context\"):\n+            logit_processor.set_continuous_batching_context(batch_data[\"logits_indices\"], batch_data[\"cu_seq_lens_q\"])\n+        # Handle shape compatibility: logit processors expect 2D tensors [batch_size, vocab_size]\n+        # but continuous batching always produces 3D tensors [batch_size, seq_len, vocab_size]\n+        batch_size, seq_len, vocab_size = logits.shape\n+        logits_2d = logits.view(batch_size * seq_len, vocab_size)\n+        input_ids_2d = batch_data[\"input_ids\"].view(batch_size * seq_len)\n+        # Process with 2D tensors\n+        processed_logits_2d = logit_processor(input_ids_2d, logits_2d)\n+        # Reshape back to 3D\n+        return processed_logits_2d.view(batch_size, seq_len, vocab_size)\n+\n+    @traced(span_name=\"sampling\")\n+    def _sample(self, probs: torch.Tensor, do_sample: bool) -> None:\n+        if do_sample:\n+            probs = nn.functional.softmax(probs, dim=-1)\n+            # probs[0] has shape [seq_len, vocab_size], multinomial returns [seq_len, 1]\n+            next_tokens = torch.multinomial(probs[0], num_samples=1).squeeze(-1)  # Now [seq_len]\n+            # Add batch dimension back to match argmax output\n+            next_tokens = next_tokens.unsqueeze(0)  # Now [1, seq_len]\n+        else:\n+            next_tokens = torch.argmax(probs, dim=-1)  # Already [1, seq_len]\n+        tokens = next_tokens.size(1)  # Get seq_len dimension\n+        self.output_ids[:, :tokens].copy_(next_tokens)\n+\n \n # Manager Class (User Interface)\n @attach_tracer()\n@@ -589,19 +721,21 @@ class ContinuousBatchingManager:\n \n     def __init__(\n         self,\n-        model,\n+        model: nn.Module,\n         generation_config: GenerationConfig,\n         manual_eviction: bool = False,\n-        max_queue_size=0,\n-        slice_inputs: bool = True,\n-    ):\n-        \"\"\"\n-        Initialize the continuous batching manager.\n+        max_queue_size: int = 0,\n+        num_q_cuda_graphs: int = 0,\n+        num_kv_cuda_graphs: int = 0,\n+    ) -> None:\n+        \"\"\"Initialize the continuous batching manager.\n \n         Args:\n             model: The language model for generation\n             generation_config: Configuration for generation parameters\n             max_queue_size: Maximum size of the request queue (0 = unlimited)\n+            num_q_cuda_graphs: (optional) Number of CUDA graphs to use for the query dimension\n+            num_kv_cuda_graphs: (optional) Number of CUDA graphs to use for the keys/values dimension\n         \"\"\"\n         if \"paged|\" not in model.config._attn_implementation:\n             attn_implementation = f\"paged|{model.config._attn_implementation}\"\n@@ -627,17 +761,38 @@ def __init__(\n         self.model.generation_config.top_p = None\n         self.do_sample = getattr(generation_config, \"do_sample\", True)\n         self.logit_processor = self.model._get_logits_processor(generation_config)\n-        self.use_cuda_graph = getattr(generation_config, \"use_cuda_graph\", False)  # TODO: same as do_sample\n-        self.profile = getattr(generation_config, \"profile\", False)\n+        use_cuda_graph: Optional[bool] = getattr(generation_config, \"use_cuda_graph\", None)\n+        self.profile = getattr(generation_config, \"profile\", False)  # TODO: not supported yet\n         self.manual_eviction = manual_eviction\n         self.batch_processor: Optional[ContinuousBatchProcessor] = None\n-        self.slice_inputs = slice_inputs\n \n+        # If a number of cuda graphs was specified for either Q or KV, we activate cuda graphs\n+        if num_q_cuda_graphs > 0 or num_kv_cuda_graphs > 0:\n+            self.use_cuda_graph = True\n+        # If use_cuda_graph is specified, we follow the user's choice\n+        elif use_cuda_graph is not None:\n+            self.use_cuda_graph = use_cuda_graph\n+        # If the use of cuda graphs is not specified, we follow the user's choice, otherwise we have a default heuristic\n+        else:\n+            # Attention implementations where an attention mask is needed suffer a lot more from the padding associated\n+            # with cuda graphs, so default is to turn cuda graphs off for those implementations\n+            self.use_cuda_graph = not attn_mask_is_needed(self.model.config)\n+            logger.warning(\n+                f\"No behavior specified for use_cuda_graph, defaulting to {self.use_cuda_graph = } because \"\n+                f\"{self.model.config._attn_implementation = }. If you want to save memory, turn off cuda graphs, but \"\n+                \"they can improve performances.\"\n+            )\n+\n+        # If cuda graphs are activated, we set the number of cuda graphs for Q and KV if not specified\n         if self.use_cuda_graph:\n-            raise NotImplementedError(\"Cuda graphs are not supported yet\")\n+            self.num_q_cuda_graphs = num_q_cuda_graphs if num_q_cuda_graphs > 0 else NUM_Q_CUDA_GRAPHS\n+            self.num_kv_cuda_graphs = num_kv_cuda_graphs if num_kv_cuda_graphs > 0 else NUM_KV_CUDA_GRAPHS\n+\n+        if self.log_prob_generation:\n+            raise NotImplementedError(\"log_prob_generation is not supported yet\")\n \n     @traced\n-    def start(self):\n+    def start(self) -> None:\n         \"\"\"Start the background generation thread.\"\"\"\n         if self._generation_thread is not None and self._generation_thread.is_alive():\n             logger.warning(\"Manager thread is already running.\")\n@@ -647,11 +802,11 @@ def start(self):\n         self._generation_thread = threading.Thread(target=self._run_generation_loop)\n         self._generation_thread.start()\n \n-    def is_running(self):\n+    def is_running(self) -> bool:\n         \"\"\"Check if the background generation thread is running.\"\"\"\n         return self._generation_thread is not None and self._generation_thread.is_alive()\n \n-    def stop(self, block: bool = False, timeout: Optional[float] = None):\n+    def stop(self, block: bool = False, timeout: Optional[float] = None) -> None:\n         \"\"\"Signal the background thread to stop.\n \n         Args:\n@@ -669,7 +824,7 @@ def stop(self, block: bool = False, timeout: Optional[float] = None):\n         if block:\n             self.join(timeout)\n \n-    def join(self, timeout: Optional[float] = None):\n+    def join(self, timeout: Optional[float] = None) -> None:\n         \"\"\"Wait for the background thread to finish.\n \n         Args:\n@@ -719,14 +874,13 @@ def add_request(\n \n         # Use block=True with timeout to handle backpressure if queue is full\n         self.input_queue.put(state, block=True, timeout=10)  # XXX: pass timeout as fn arg?\n-        logger.debug(f\"Added request {request_id} to queue.\")\n         return request_id\n \n-    def add_requests(self, inputs: list[list[int]], **kwargs):\n+    def add_requests(self, inputs: list[list[int]], max_new_tokens: Optional[int] = None) -> None:\n         for input_ids in inputs:\n-            self.add_request(input_ids, **kwargs)\n+            self.add_request(input_ids, max_new_tokens=max_new_tokens)\n \n-    def cancel_request(self, request_id: str):\n+    def cancel_request(self, request_id: str) -> None:\n         \"\"\"Cancel a request by its ID.\n \n         Args:\n@@ -735,7 +889,9 @@ def cancel_request(self, request_id: str):\n         if self.batch_processor is not None:\n             self.batch_processor.scheduler.set_request_cancellation(request_id)\n \n-    def get_result(self, request_id=None, timeout=None) -> Optional[GenerationOutput]:\n+    def get_result(\n+        self, request_id: Optional[str] = None, timeout: Optional[float] = None\n+    ) -> Optional[GenerationOutput]:\n         \"\"\"Retrieve one result from the output queue.\n \n         Args:\n@@ -763,7 +919,7 @@ def __iter__(self):\n             if result is not None:\n                 yield result\n \n-    def request_id_iter(self, request_id):\n+    def request_id_iter(self, request_id: str) -> Generator[GenerationOutput]:\n         \"\"\"Iterate over results matching a specific request id as they become available.\"\"\"\n         request_cancelled = False\n         while self._generation_thread is not None and self._generation_thread.is_alive() and not request_cancelled:\n@@ -773,8 +929,16 @@ def request_id_iter(self, request_id):\n             if self.batch_processor is not None:\n                 request_cancelled = self.batch_processor.scheduler.request_is_cancelled(request_id)\n \n+    @staticmethod\n+    def supported_attention_implementations() -> set[str]:\n+        return {\"eager_paged\", \"sdpa_paged\", \"flash_attention_2\"}\n+\n+    @staticmethod\n+    def default_attention_implementation() -> str:\n+        return \"sdpa_paged\"\n+\n     @traced\n-    def warmup(self, batch_processor):\n+    def warmup(self, batch_processor: ContinuousBatchProcessor) -> None:\n         stream = torch.cuda.Stream(device=self.model.device)\n         stream.wait_stream(torch.cuda.current_stream())\n         with torch.cuda.stream(stream):\n@@ -788,67 +952,23 @@ def warmup(self, batch_processor):\n \n     @traced\n     # @torch.compile\n-    def _generation_step(self, batch_processor: ContinuousBatchProcessor):\n+    def _generation_step(self) -> None:\n         \"\"\"Perform a single generation step. This is cuda graphed\"\"\"\n-        batch_data = batch_processor.get_model_kwargs()\n-        with torch.no_grad():\n-            logits = self._model_forward(batch_data)\n-            if self.log_prob_generation:\n-                batch_processor.output_probs.copy_(logits)  # TODO\n-            probs = self._process_logit(batch_data, logits)\n-            self._sample(batch_processor, probs)\n-\n-    @traced(span_name=\"model_forward\")\n-    def _model_forward(self, batch_data):\n-        return self.model(**batch_data).logits\n-\n-    @traced(span_name=\"logit_processing\")\n-    def _process_logit(self, batch_data, logits):\n-        # Pass continuous batching context to logits processor if it supports it. TODO we should find a way to make this a little bit cleaner!\n-        if hasattr(self.logit_processor, \"set_continuous_batching_context\"):\n-            self.logit_processor.set_continuous_batching_context(\n-                batch_data[\"logits_indices\"], batch_data[\"cu_seq_lens_q\"]\n-            )\n-\n-        # Handle shape compatibility: logit processors expect 2D tensors [batch_size, vocab_size]\n-        # but continuous batching always produces 3D tensors [batch_size, seq_len, vocab_size]\n-        batch_size, seq_len, vocab_size = logits.shape\n-        logits_2d = logits.view(batch_size * seq_len, vocab_size)\n-        input_ids_2d = batch_data[\"input_ids\"].view(batch_size * seq_len)\n-\n-        # Process with 2D tensors\n-        processed_logits_2d = self.logit_processor(input_ids_2d, logits_2d)\n+        self.batch_processor._generation_step(self.model, self.logit_processor, self.do_sample)\n \n-        # Reshape back to 3D\n-        return processed_logits_2d.view(batch_size, seq_len, vocab_size)\n-\n-    @traced(span_name=\"sampling\")\n-    def _sample(self, batch_processor: ContinuousBatchProcessor, probs):\n-        if self.do_sample:  # sample\n-            probs = nn.functional.softmax(probs, dim=-1)\n-            # probs[0] has shape [seq_len, vocab_size], multinomial returns [seq_len, 1]\n-            next_tokens = torch.multinomial(probs[0], num_samples=1).squeeze(-1)  # Now [seq_len]\n-            # Add batch dimension back to match argmax output\n-            next_tokens = next_tokens.unsqueeze(0)  # Now [1, seq_len]\n-        else:\n-            next_tokens = torch.argmax(probs, dim=-1)  # Already [1, seq_len]\n-\n-        tokens = next_tokens.size(1)  # Get seq_len dimension\n-        batch_processor.output_ids[:, :tokens].copy_(next_tokens)\n-\n-    def _run_generation_loop(self):\n+    def _run_generation_loop(self) -> None:\n         \"\"\"Main processing loop running in the background thread.\"\"\"\n-        batch_processor = None\n+        batch_processor: Optional[ContinuousBatchProcessor] = None\n         try:\n-            ref_time = perf_counter()\n+            t0 = perf_counter()\n             paged_attention_cache = PagedAttentionCache(\n                 self.model.config,\n                 self.generation_config,\n                 self.model.device,\n                 self.model.dtype,\n                 tp_size=getattr(self.model, \"_tp_size\", None),  # Use model's actual TP setting\n             )\n-            logger.debug(f\"PagedAttentionCache created in {perf_counter() - ref_time} seconds\")\n+            logger.debug(f\"PagedAttentionCache created in {perf_counter() - t0} seconds\")\n \n             scheduler = None\n             if hasattr(self.generation_config, \"scheduler\"):\n@@ -860,23 +980,23 @@ def _run_generation_loop(self):\n                 # Default to fifo\n                 scheduler = FIFOScheduler\n \n-            ref_time = perf_counter()\n+            t1 = perf_counter()\n             batch_processor = ContinuousBatchProcessor(\n-                paged_attention_cache,\n-                self.model.config,\n-                self.generation_config,\n-                self.input_queue,\n-                self.output_queue,\n-                self.stop_event,\n-                self.model.device,\n-                self.model.dtype,\n-                scheduler(paged_attention_cache, self.manual_eviction),\n-                self.manual_eviction,\n-                slice_inputs=self.slice_inputs,\n+                cache=paged_attention_cache,\n+                config=self.model.config,\n+                generation_config=self.generation_config,\n+                input_queue=self.input_queue,\n+                output_queue=self.output_queue,\n+                stop_event=self.stop_event,\n+                model_device=self.model.device,\n+                model_dtype=self.model.dtype,\n+                scheduler=scheduler(paged_attention_cache, self.manual_eviction),\n+                manual_eviction=self.manual_eviction,\n+                use_cuda_graph=self.use_cuda_graph,\n             )\n             self.batch_processor = batch_processor\n             self.current_batch = 0\n-            logger.debug(f\"batch_processor created in {perf_counter() - ref_time} seconds\")\n+            logger.debug(f\"batch_processor created in {perf_counter() - t1} seconds\")\n             while (not self.stop_event.is_set()) or batch_processor.has_pending_requests():\n                 self._inner_generation_loop(batch_processor)\n                 self.current_batch += 1\n@@ -888,38 +1008,27 @@ def _run_generation_loop(self):\n             logger.info(\"Generation loop finished.\")\n \n     @traced(span_name=\"generation_loop\")\n-    def _inner_generation_loop(self, batch_processor: ContinuousBatchProcessor):\n+    def _inner_generation_loop(self, batch_processor: ContinuousBatchProcessor) -> None:\n+        # Pre-loop synchronization\n         if torch.cuda.is_available():\n             torch.cuda.synchronize()\n+        # Loop body ends if there is no requests in the batch\n         if not batch_processor.prepare_next_batch():\n             return\n+        # Debug logging of the current memory usage\n         if logger.level <= logging.DEBUG:\n             device, total, reserved, allocated = get_device_and_memory_breakdown()\n             logger.debug(f\"[Memory] Device: {device}, Total: {total}, Reserved: {reserved}, Allocated: {allocated}\")\n-        if torch.cuda.is_available() and self.use_cuda_graph:\n-            if self.current_batch == 0:\n-                self.warmup(batch_processor)\n-            elif hasattr(self, \"graph\"):\n-                try:\n-                    self._graph_replay()\n-                except Exception as e:\n-                    logger.error(f\"Model forward pass failed: {e}\", exc_info=True)\n-                    batch_processor.handle_batch_error(e)\n-                    return\n-            else:\n-                self._generation_step(batch_processor)\n-        else:\n-            self._generation_step(batch_processor)\n+\n+        self._generation_step()\n+\n         if torch.cuda.is_available():\n             torch.cuda.synchronize()\n+        # Processor updates the batch after generation step is truly over\n         batch_processor.update_batch()\n \n-    @traced(span_name=\"graph_replay\")\n-    def _graph_replay(self):\n-        self.graph.replay()\n-\n     @traced\n-    def _handle_critical_error(self, error, batch_processor: Optional[ContinuousBatchProcessor]):\n+    def _handle_critical_error(self, error: Exception, batch_processor: Optional[ContinuousBatchProcessor]) -> None:\n         \"\"\"Handle critical errors that terminate the generation loop.\"\"\"\n         # Signal stop\n         self.stop_event.set()\n@@ -938,7 +1047,7 @@ def _handle_critical_error(self, error, batch_processor: Optional[ContinuousBatc\n             batch_processor.fail_all_requests(error)\n \n     @traced\n-    def evict_request_from_cache(self, request_id: str):\n+    def evict_request_from_cache(self, request_id: str) -> None:\n         \"\"\"Evict a request from the cache. It is assumed that the request is already finished.\"\"\"\n         if not self.manual_eviction:\n             raise RuntimeError(\"Manual eviction is not enabled for this manager.\")\n@@ -954,13 +1063,17 @@ def init_continuous_batching(\n         generation_config: Optional[GenerationConfig] = None,\n         manual_eviction: bool = False,\n         max_queue_size: int = 0,\n-        slice_inputs: bool = True,\n+        num_q_cuda_graphs: int = 0,\n+        num_kv_cuda_graphs: int = 0,\n     ) -> ContinuousBatchingManager:\n         \"\"\"Initialize a manager for continuous batching inference.\n \n         Args:\n             generation_config: Custom generation configuration\n+            manual_eviction: Whether to manually evict requests from the cache\n             max_queue_size: Maximum size of the input request queue\n+            num_q_cuda_graphs: Number of CUDA graphs to use for the query dimension\n+            num_kv_cuda_graphs: Number of CUDA graphs to use for the keys/values dimension\n \n         Returns:\n             `ContinuousBatchingManager`: The manager instance to add requests and retrieve results.\n@@ -982,7 +1095,8 @@ def init_continuous_batching(\n             generation_config=gen_config,\n             manual_eviction=manual_eviction,\n             max_queue_size=max_queue_size,\n-            slice_inputs=slice_inputs,\n+            num_q_cuda_graphs=num_q_cuda_graphs,\n+            num_kv_cuda_graphs=num_kv_cuda_graphs,\n         )\n \n     @traced\n@@ -992,14 +1106,17 @@ def generate_batch(\n         inputs: list[list[int]],\n         generation_config: Optional[GenerationConfig] = None,\n         progress_bar: bool = True,\n-        slice_inputs: bool = True,\n+        num_q_cuda_graphs: int = 0,\n+        num_kv_cuda_graphs: int = 0,\n         **kwargs,\n-    ) -> list[list[int]]:\n+    ) -> dict[str, GenerationOutput]:\n         \"\"\"Generate sequences for a batch of prompts using continuous batching.\n \n         Args:\n             inputs: List of input token sequences (prompts)\n             generation_config: Optional generation configuration\n+            num_q_cuda_graphs: Number of CUDA graphs to use for the query dimension\n+            num_kv_cuda_graphs: Number of CUDA graphs to use for the keys/values dimension\n             **kwargs: Additional generation parameters\n \n         Returns:\n@@ -1008,13 +1125,17 @@ def generate_batch(\n                                 Returns an empty list `[]` for requests that failed.\n         \"\"\"\n         if not inputs:\n-            return []\n+            return {}\n         if logger.getEffectiveLevel() <= logging.DEBUG:\n             logger.warning(\"Progress bar is disabled when logger level is less than DEBUG\")\n             progress_bar = False\n \n         # Initialize manager with the batch inputs\n-        manager = self.init_continuous_batching(generation_config=generation_config, slice_inputs=slice_inputs)\n+        manager = self.init_continuous_batching(\n+            generation_config=generation_config,\n+            num_q_cuda_graphs=num_q_cuda_graphs,\n+            num_kv_cuda_graphs=num_kv_cuda_graphs,\n+        )\n         manager.start()\n         results = {}\n         num_requests = len(inputs)\n@@ -1028,7 +1149,7 @@ def generate_batch(\n                     desc=f\"Solving {num_requests} requests\",\n                     unit=\"request\",\n                 ) as pbar:\n-                    manager.add_requests(inputs, **kwargs)\n+                    manager.add_requests(inputs=inputs, max_new_tokens=kwargs.get(\"max_new_tokens\"))\n                     finished_count = 0\n                     while finished_count < num_requests:\n                         result = manager.get_result(timeout=1)"
      },
      {
        "filename": "src/transformers/generation/continuous_batching/requests.py",
        "status": "modified",
        "additions": 0,
        "deletions": 1,
        "changes": 1,
        "patch": "@@ -25,7 +25,6 @@\n \n # We centralize the logger here to coordinate between logging and progress bar\n logger = logging.getLogger(\"ContinuousBatchingLogger\")\n-# logger.setLevel(logging.INFO)\n \n \n @staticmethod"
      },
      {
        "filename": "src/transformers/integrations/eager_paged.py",
        "status": "modified",
        "additions": 10,
        "deletions": 2,
        "changes": 12,
        "patch": "@@ -3,6 +3,8 @@\n import torch\n from torch import nn\n \n+from ..generation.continuous_batching.cache import PagedAttentionCache\n+\n \n def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     \"\"\"\n@@ -26,10 +28,16 @@ def eager_paged_attention_forward(\n     **kwargs,\n ):\n     # Add KV cache to the key and value tensors\n-    cache = kwargs.pop(\"cache\", None)\n+    cache: Optional[PagedAttentionCache] = kwargs.pop(\"cache\", None)\n     if cache is not None:\n         # This changes the shape of k and v from [1, num_kv_heads, seqlen_kv, head_dim] to [-1, num_kv_heads, head_dim]\n-        key, value = cache.update(key, value, module.layer_idx, **kwargs)\n+        key, value = cache.update(\n+            key_states=key,\n+            value_states=value,\n+            layer_idx=module.layer_idx,\n+            read_index=kwargs[\"read_index\"],\n+            write_index=kwargs[\"write_index\"],\n+        )\n         key = key.transpose(0, 1).unsqueeze(0)\n         value = value.transpose(0, 1).unsqueeze(0)\n "
      },
      {
        "filename": "src/transformers/integrations/flash_paged.py",
        "status": "modified",
        "additions": 7,
        "deletions": 1,
        "changes": 8,
        "patch": "@@ -64,7 +64,13 @@ def paged_attention_forward(\n \n     # .update changes the shape of k and v from [1, num_kv_heads, seqlen_kv, head_dim] to [-1, num_kv_heads, head_dim]\n     if cache is not None:\n-        k, v = cache.update(k, v, module.layer_idx, **kwargs)\n+        k, v = cache.update(\n+            key_states=k,\n+            value_states=v,\n+            layer_idx=module.layer_idx,\n+            read_index=kwargs[\"read_index\"],\n+            write_index=kwargs[\"write_index\"],\n+        )\n \n     # Retrieve the cumulative sequence lengths for the current layer\n     if isinstance(cu_seq_lens_k, dict):"
      },
      {
        "filename": "src/transformers/integrations/sdpa_paged.py",
        "status": "modified",
        "additions": 10,
        "deletions": 2,
        "changes": 12,
        "patch": "@@ -2,6 +2,8 @@\n \n import torch\n \n+from ..generation.continuous_batching.cache import PagedAttentionCache\n+\n \n def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     \"\"\"\n@@ -26,10 +28,16 @@ def sdpa_attention_paged_forward(\n     **kwargs,\n ) -> tuple[torch.Tensor, None]:\n     # Add KV cache to the key and value tensors\n-    cache = kwargs.pop(\"cache\", None)\n+    cache: Optional[PagedAttentionCache] = kwargs.pop(\"cache\", None)\n     if cache is not None:\n         # This changes the shape of k and v from [1, num_kv_heads, seqlen_kv, head_dim] to [-1, num_kv_heads, head_dim]\n-        key, value = cache.update(key, value, module.layer_idx, **kwargs)\n+        key, value = cache.update(\n+            key_states=key,\n+            value_states=value,\n+            layer_idx=module.layer_idx,\n+            read_index=kwargs[\"read_index\"],\n+            write_index=kwargs[\"write_index\"],\n+        )\n         key = key.transpose(0, 1).unsqueeze(0)\n         value = value.transpose(0, 1).unsqueeze(0)\n "
      }
    ],
    "num_files": 8,
    "scraped_at": "2025-11-16T21:18:06.979739"
  },
  {
    "pr_number": 41415,
    "title": "Fix bnb fsdp loading for pre-quantized checkpoint",
    "body": "# What does this PR do?\r\n\r\nThis PR fixes bnb loading when using FSDP for pre-quantized checkpoints. This happened because we changed how we load quantized checkpoints as we need to cache all the quantized stats before creating the quantized weight. ",
    "html_url": "https://github.com/huggingface/transformers/pull/41415",
    "created_at": "2025-10-07T15:30:31Z",
    "merged_at": "2025-10-09T16:05:35Z",
    "merge_commit_sha": "823fab4860ec7a5c71d8a21f834104c6deedfaa4",
    "base_ref": "main",
    "head_sha": "3b453005e9f4dd52fcd189006cff875e2789b0e4",
    "user": "SunMarc",
    "files": [
      {
        "filename": "src/transformers/modeling_utils.py",
        "status": "modified",
        "additions": 8,
        "deletions": 12,
        "changes": 20,
        "patch": "@@ -763,21 +763,17 @@ def _load_state_dict_into_meta_model(\n                 # and then cast it to CPU to avoid excessive memory usage on each GPU\n                 # in comparison to the sharded model across GPUs.\n                 if is_fsdp_enabled() or is_deepspeed_zero3_enabled():\n-                    param_name = hf_quantizer.update_param_name(param_name)\n+                    param_name = hf_quantizer.get_param_name(param_name)\n                     module, param_type = get_module_from_name(model, param_name)\n                     value = getattr(module, param_type)\n-                    # special case for gpt_oss model, we wait for the param to be leave the meta device before casting it to cpu\n-                    if model.config.model_type == \"gpt_oss\" and value.device.type == \"meta\":\n+                    # We need to wait until the quantized value is created\n+                    if value.device.type == \"meta\":\n                         continue\n-                    param_to = \"cpu\"\n-                    if is_fsdp_enabled() and not is_local_dist_rank_0():\n-                        param_to = \"meta\"\n-                    val_kwargs = {}\n-                    if (hasattr(module, \"weight\") and module.weight.__class__.__name__ == \"Int8Params\") or (\n-                        value.dtype == torch.uint8 or value.dtype == torch.int8\n-                    ):\n+                    val_kwargs = value.__dict__\n+                    if not value.is_floating_point():\n                         val_kwargs[\"requires_grad\"] = False\n-                    value = type(value)(value.data.to(param_to), **val_kwargs, **value.__dict__)\n+                    device = \"meta\" if is_fsdp_enabled() and not is_local_dist_rank_0() else \"cpu\"\n+                    value = type(value)(value.data.to(device), **val_kwargs)\n                     setattr(module, param_type, value)\n \n         # Remove the param from the state dict if it was not loaded on the fly to avoid wasting memory\n@@ -5822,7 +5818,7 @@ def caching_allocator_warmup(model: PreTrainedModel, expanded_device_map: dict,\n         # For example in the case of MXFP4 quantization, we need to update the param name to the original param name\n         # because the checkpoint contains blocks, and scales, but since we are dequantizing, we need to use the original param name\n         if hf_quantizer is not None:\n-            param_name = hf_quantizer.update_param_name(param_name)\n+            param_name = hf_quantizer.get_param_name(param_name)\n \n         try:\n             param = model.get_parameter_or_buffer(param_name)"
      },
      {
        "filename": "src/transformers/quantizers/base.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -283,7 +283,7 @@ def _dequantize(self, model):\n             f\"{self.quantization_config.quant_method} has no implementation of `dequantize`, please raise an issue on GitHub.\"\n         )\n \n-    def update_param_name(self, param_name: str) -> str:\n+    def get_param_name(self, param_name: str) -> str:\n         \"\"\"\n         Override this method if you want to adjust the `param_name`.\n         \"\"\""
      },
      {
        "filename": "src/transformers/quantizers/quantizer_bnb_4bit.py",
        "status": "modified",
        "additions": 16,
        "deletions": 5,
        "changes": 21,
        "patch": "@@ -154,6 +154,19 @@ def param_needs_quantization(self, model: \"PreTrainedModel\", param_name: str, **\n         module, name = get_module_from_name(model, param_name)\n         return isinstance(module, bnb.nn.Linear4bit) and name != \"bias\"\n \n+    def get_param_name(self, param_name: str) -> str:\n+        \"\"\"\n+        Get the right param_name in order to get the module associated with the param.\n+        This is useful for quantized stats lile absmax or quant_map as we need to update the param_name to get the module as they are stored in ...weight.absmax.\n+        \"\"\"\n+        if self.pre_quantized:\n+            # We need to get the param name of quantized weights and not its components. Otherwise, we won't be able to get the nn.Module associated.\n+            if any(param_name.endswith(x) for x in self.bnb_keys):\n+                param_name = (\n+                    param_name.rsplit(\".\", 1)[0] if \"quant_state.\" not in param_name else param_name.rsplit(\".\", 2)[0]\n+                )\n+        return param_name\n+\n     def create_quantized_param(\n         self,\n         model: \"PreTrainedModel\",\n@@ -164,12 +177,10 @@ def create_quantized_param(\n     ):\n         import bitsandbytes as bnb\n \n-        is_quant_stat = any(param_name.endswith(x) for x in self.bnb_keys)\n         full_name = param_name\n-        if is_quant_stat:\n-            param_name = (\n-                param_name.rsplit(\".\", 1)[0] if \"quant_state.\" not in param_name else param_name.rsplit(\".\", 2)[0]\n-            )\n+\n+        # update param name to get the weights instead of the quantized stats\n+        param_name = self.get_param_name(param_name)\n         module, tensor_name = get_module_from_name(model, param_name)\n \n         # `torch.Tensor.to(<int num>)` is not supported by `torch_npu` (see this [issue](https://github.com/Ascend/pytorch/issues/16))."
      },
      {
        "filename": "src/transformers/quantizers/quantizer_mxfp4.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -365,7 +365,7 @@ def update_ep_plan(self, config):\n                 )\n         return config\n \n-    def update_param_name(self, param_name: str) -> str:\n+    def get_param_name(self, param_name: str) -> str:\n         if self.quantization_config.dequantize:\n             if \"_blocks\" in param_name:\n                 return param_name.replace(\"_blocks\", \"\")"
      },
      {
        "filename": "tests/quantization/mxfp4/test_mxfp4.py",
        "status": "modified",
        "additions": 6,
        "deletions": 6,
        "changes": 12,
        "patch": "@@ -265,7 +265,7 @@ def test_update_expected_keys(self):\n \n         self.assertEqual(set(updated_keys), set(expected_updated))\n \n-    def test_update_param_name_dequantize(self):\n+    def test_get_param_name_dequantize(self):\n         \"\"\"Test parameter name updating when dequantizing\"\"\"\n         from transformers.quantizers.quantizer_mxfp4 import Mxfp4HfQuantizer\n \n@@ -274,28 +274,28 @@ def test_update_param_name_dequantize(self):\n \n         # Should remove _blocks suffix\n         param_name = \"model.layers.0.mlp.experts.gate_up_proj_blocks\"\n-        updated_name = quantizer.update_param_name(param_name)\n+        updated_name = quantizer.get_param_name(param_name)\n         self.assertEqual(updated_name, \"model.layers.0.mlp.experts.gate_up_proj\")\n \n         # Should remove _scales suffix\n         param_name = \"model.layers.0.mlp.experts.down_proj_scales\"\n-        updated_name = quantizer.update_param_name(param_name)\n+        updated_name = quantizer.get_param_name(param_name)\n         self.assertEqual(updated_name, \"model.layers.0.mlp.experts.down_proj\")\n \n         # Should not change other names\n         param_name = \"model.embed_tokens.weight\"\n-        updated_name = quantizer.update_param_name(param_name)\n+        updated_name = quantizer.get_param_name(param_name)\n         self.assertEqual(updated_name, \"model.embed_tokens.weight\")\n \n-    def test_update_param_name_no_dequantize(self):\n+    def test_get_param_name_no_dequantize(self):\n         \"\"\"Test parameter name updating when not dequantizing\"\"\"\n         from transformers.quantizers.quantizer_mxfp4 import Mxfp4HfQuantizer\n \n         config = Mxfp4Config(dequantize=False)\n         quantizer = Mxfp4HfQuantizer(config)\n \n         param_name = \"model.layers.0.mlp.experts.gate_up_proj_blocks\"\n-        updated_name = quantizer.update_param_name(param_name)\n+        updated_name = quantizer.get_param_name(param_name)\n         self.assertEqual(updated_name, param_name)\n \n     def test_is_trainable(self):"
      }
    ],
    "num_files": 5,
    "scraped_at": "2025-11-16T21:18:08.510381"
  },
  {
    "pr_number": 41408,
    "title": "Benchmark overhaul",
    "body": "This PR overhauls the benchmarking suite that is included in transformers. \r\nThe benchmarking suite is now based around three main components:\r\n\r\n- `BenchmarkingConfig` is a dataclass-like object which contains everything needed to reproduce a benchmark on the same machine: input length, generation length, whether to use `kernels` or `compile`, attention implementation, etc. (subject to name change)\r\n- `BenchmarkRunner` is the class that runs the benchmarks defined by the configs, with a given number of measurement iterations, warmup iterations, and a model-id. The runner takes care of setting up the runs in a way that ensures no run interacts with the downstream ones: the model is reloaded, the cache is emptied and the GPU memory is flushed. It also saves the results, the config, and any additional metadata needed to reproduce the benchmark, like hardware information and package versions. \r\n- The created results files, which contain enough informations to induces (to my knowledge) most of the metrics used to evaluate a model: e2e_atency, tpot, ttft, even inter-token latency. Results also include a sample of what has been generated, which is useful to check if it was gibberish. The results files are in json format and are made to be easily created from the dataclass-like objects and vice versa. \r\n\r\nFor now, the new benchmarking suite replaces the `benchmark_v2` part of `transformers` but it could also overwrite the `benchmark` (v1) part. It would be good to make that decision in this PR. And update the CI workflows that rely on the current `benchmark_v2` (putting the PR in draft mode until then).\r\nAn example of how to use the new benchmarking suite can be found in `run_benchmarks.py`.\r\n\r\nThe format of the results file can (and may be bound to) change as we develop tools to analyze them. \r\nIf there is a metric you want to see measured in `transformers`, please leave a comment before this is merged :slightly_smiling_face: ",
    "html_url": "https://github.com/huggingface/transformers/pull/41408",
    "created_at": "2025-10-07T13:10:05Z",
    "merged_at": "2025-10-14T19:41:43Z",
    "merge_commit_sha": "94df0e65602922be2831b3faa457a2bde78b936b",
    "base_ref": "main",
    "head_sha": "400a6165037079decfa1b3710f9f4031c38bd6ca",
    "user": "remi-or",
    "files": [
      {
        "filename": ".github/workflows/benchmark.yml",
        "status": "modified",
        "additions": 1,
        "deletions": 4,
        "changes": 5,
        "patch": "@@ -1,10 +1,7 @@\n name: Self-hosted runner (benchmark)\r\n \r\n on:\r\n-  push:\r\n-    branches: [main]\r\n-  pull_request:\r\n-    types: [ opened, labeled, reopened, synchronize ]\r\n+  workflow_dispatch:\r\n \r\n concurrency:\r\n   group: ${{ github.workflow }}-${{ github.head_ref || github.run_id }}\r"
      },
      {
        "filename": ".github/workflows/benchmark_v2.yml",
        "status": "modified",
        "additions": 2,
        "deletions": 30,
        "changes": 32,
        "patch": "@@ -1,35 +1,7 @@\n name: Benchmark v2 Framework\n \n on:\n-  workflow_call:\n-    inputs:\n-      runner:\n-        description: 'GH Actions runner group to use'\n-        required: true\n-        type: string\n-      container_image:\n-        description: 'Docker image to use'\n-        required: true\n-        type: string\n-      container_options:\n-        description: 'Container options to use'\n-        required: true\n-        type: string\n-      commit_sha:\n-        description: 'Commit SHA to benchmark'\n-        required: false\n-        type: string\n-        default: ''\n-      run_id:\n-        description: 'Custom run ID for organizing results (auto-generated if not provided)'\n-        required: false\n-        type: string\n-        default: ''\n-      benchmark_repo_id:\n-        description: 'HuggingFace Dataset to upload results to (e.g., \"org/benchmark-results\")'\n-        required: false\n-        type: string\n-        default: ''\n+  workflow_dispatch:\n \n env:\n   HF_HOME: /mnt/cache\n@@ -82,4 +54,4 @@ jobs:\n           --token '${{ secrets.TRANSFORMERS_CI_RESULTS_UPLOAD_TOKEN }}' \\\n           --log-level INFO\n         env:\n-          HF_TOKEN: ${{ secrets.HF_HUB_READ_TOKEN }}\n\\ No newline at end of file\n+          HF_TOKEN: ${{ secrets.HF_HUB_READ_TOKEN }}"
      },
      {
        "filename": ".github/workflows/benchmark_v2_a10_caller.yml",
        "status": "modified",
        "additions": 2,
        "deletions": 6,
        "changes": 8,
        "patch": "@@ -1,11 +1,7 @@\n name: Benchmark v2 Scheduled Runner - A10 Single-GPU\n \n on:\n-  schedule:\n-    # Run daily at 16:30 UTC\n-    - cron: \"30 16 * * *\"\n-  pull_request:\n-    types: [ opened, labeled, reopened, synchronize ]\n+  workflow_dispatch:\n \n jobs:\n   benchmark-v2-default:\n@@ -18,4 +14,4 @@ jobs:\n       commit_sha: ${{ github.sha }}\n       run_id: ${{ github.run_id }}\n       benchmark_repo_id: hf-internal-testing/transformers-daily-benchmarks\n-    secrets: inherit\n\\ No newline at end of file\n+    secrets: inherit"
      },
      {
        "filename": ".github/workflows/benchmark_v2_mi325_caller.yml",
        "status": "modified",
        "additions": 2,
        "deletions": 6,
        "changes": 8,
        "patch": "@@ -1,11 +1,7 @@\n name: Benchmark v2 Scheduled Runner - MI325 Single-GPU\n \n on:\n-  schedule:\n-    # Run daily at 16:30 UTC\n-    - cron: \"30 16 * * *\"\n-  pull_request:\n-    types: [ opened, labeled, reopened, synchronize ]\n+  workflow_dispatch:\n \n jobs:\n   benchmark-v2-default:\n@@ -18,4 +14,4 @@ jobs:\n       commit_sha: ${{ github.sha }}\n       run_id: ${{ github.run_id }}\n       benchmark_repo_id: hf-internal-testing/transformers-daily-benchmarks\n-    secrets: inherit\n\\ No newline at end of file\n+    secrets: inherit"
      },
      {
        "filename": "benchmark_v2/.gitignore",
        "status": "modified",
        "additions": 2,
        "deletions": 1,
        "changes": 3,
        "patch": "@@ -1 +1,2 @@\n-benchmark_results/\n\\ No newline at end of file\n+benchmark_results/\n+benchmark_results_profiles/"
      },
      {
        "filename": "benchmark_v2/benches/__init__.py",
        "status": "removed",
        "additions": 0,
        "deletions": 1,
        "changes": 1,
        "patch": "@@ -1 +0,0 @@\n-# Benchmark implementations directory"
      },
      {
        "filename": "benchmark_v2/benches/llama.py",
        "status": "removed",
        "additions": 0,
        "deletions": 165,
        "changes": 165,
        "patch": "@@ -1,165 +0,0 @@\n-# Copyright 2025 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-import logging\n-import os\n-from typing import Any\n-\n-import torch\n-from benchmark_framework import ModelBenchmark\n-\n-\n-os.environ[\"TOKENIZERS_PARALLELISM\"] = \"1\"\n-torch.set_float32_matmul_precision(\"high\")\n-\n-\n-class LLaMABenchmark(ModelBenchmark):\n-    \"\"\"Simplified LLaMA model benchmark implementation using the ModelBenchmark base class.\"\"\"\n-\n-    def __init__(self, logger: logging.Logger):\n-        super().__init__(logger)\n-        self._default_prompt = \"Why dogs are so cute?\"  # Custom prompt for LLaMA\n-\n-    def get_scenario_configs(self) -> list[dict[str, Any]]:\n-        \"\"\"\n-        Get LLaMA-specific scenario configurations.\n-\n-        Returns:\n-            List of scenario configuration dictionaries\n-        \"\"\"\n-        return [\n-            # Eager variants\n-            {\"variant\": \"eager\", \"compile_mode\": None, \"use_cache\": True, \"description\": \"Eager execution with cache\"},\n-            # Compiled variants\n-            {\n-                \"variant\": \"compiled\",\n-                \"compile_mode\": \"max-autotune\",\n-                \"use_cache\": True,\n-                \"description\": \"Compiled with max autotune\",\n-            },\n-            # Kernelized variant (if available)\n-            {\n-                \"variant\": \"kernelized\",\n-                \"compile_mode\": \"max-autotune\",\n-                \"use_cache\": True,\n-                \"description\": \"Kernelized execution\",\n-            },\n-        ]\n-\n-    def _is_kernelization_available(self) -> bool:\n-        \"\"\"Check if kernelization is available for LLaMA.\"\"\"\n-        try:\n-            from kernels import Mode, kernelize  # noqa: F401\n-\n-            return True\n-        except ImportError:\n-            self.logger.debug(\"Kernelization not available: kernels module not found\")\n-            return False\n-\n-    def get_default_generation_config(self) -> dict[str, Any]:\n-        \"\"\"Get LLaMA-specific generation configuration.\"\"\"\n-        return {\n-            \"do_sample\": False,\n-            \"top_p\": 1.0,\n-            \"temperature\": 1.0,\n-            \"repetition_penalty\": 1.0,\n-            \"max_new_tokens\": None,  # Will be set per scenario\n-        }\n-\n-    def get_model_init_kwargs(self, config) -> dict[str, Any]:\n-        \"\"\"Get LLaMA-specific model initialization kwargs.\"\"\"\n-        return {\n-            \"torch_dtype\": getattr(torch, config.torch_dtype),\n-            \"attn_implementation\": config.attn_implementation,\n-            \"use_cache\": True,\n-        }\n-\n-    def get_default_torch_dtype(self) -> str:\n-        \"\"\"Get default torch dtype for LLaMA.\"\"\"\n-        return \"float16\"  # LLaMA works well with float16\n-\n-    def get_default_device(self) -> str:\n-        \"\"\"Get default device for LLaMA.\"\"\"\n-        return \"cuda\"  # LLaMA prefers CUDA\n-\n-\n-def run_llama(logger, output_dir, **kwargs):\n-    \"\"\"\n-    Run LLaMA benchmark with the given configuration.\n-\n-    Args:\n-        logger: Logger instance\n-        output_dir: Output directory for results\n-        **kwargs: Additional configuration options\n-\n-    Returns:\n-        Path to output file if successful\n-    \"\"\"\n-    from benchmark_framework import BenchmarkRunner\n-\n-    # Extract parameters with defaults\n-    model_id = kwargs.get(\"model_id\", \"meta-llama/Llama-2-7b-hf\")\n-    warmup_iterations = kwargs.get(\"warmup_iterations\", 3)\n-    measurement_iterations = kwargs.get(\"measurement_iterations\", 5)\n-    num_tokens_to_generate = kwargs.get(\"num_tokens_to_generate\", 100)\n-    include_sdpa_variants = kwargs.get(\"include_sdpa_variants\", True)\n-    device = kwargs.get(\"device\", \"cuda\")\n-    torch_dtype = kwargs.get(\"torch_dtype\", \"float16\")\n-    batch_size = kwargs.get(\"batch_size\", 1)\n-    commit_id = kwargs.get(\"commit_id\")\n-\n-    logger.info(f\"Starting LLaMA benchmark for model: {model_id}\")\n-    logger.info(\n-        f\"Configuration: warmup={warmup_iterations}, measurement={measurement_iterations}, tokens={num_tokens_to_generate}\"\n-    )\n-\n-    try:\n-        # Create benchmark instance\n-        benchmark = LLaMABenchmark(logger)\n-\n-        # Create scenarios\n-        scenarios = benchmark.create_scenarios(\n-            model_id=model_id,\n-            warmup_iterations=warmup_iterations,\n-            measurement_iterations=measurement_iterations,\n-            num_tokens_to_generate=num_tokens_to_generate,\n-            include_sdpa_variants=include_sdpa_variants,\n-            device=device,\n-            torch_dtype=torch_dtype,\n-            batch_size=batch_size,\n-        )\n-\n-        logger.info(f\"Created {len(scenarios)} benchmark scenarios\")\n-\n-        # Create runner and execute benchmarks\n-        runner = BenchmarkRunner(logger, output_dir)\n-        results = runner.run_benchmark(benchmark, scenarios, commit_id=commit_id)\n-\n-        if not results:\n-            logger.warning(\"No successful benchmark results\")\n-            return None\n-\n-        # Save results\n-        model_name = model_id.split(\"/\")[-1]  # Extract model name from ID\n-        output_file = runner.save_results(model_name, results)\n-\n-        logger.info(f\"LLaMA benchmark completed successfully. Results saved to: {output_file}\")\n-        return output_file\n-\n-    except Exception as e:\n-        logger.error(f\"LLaMA benchmark failed: {e}\")\n-        import traceback\n-\n-        logger.debug(traceback.format_exc())\n-        raise"
      },
      {
        "filename": "benchmark_v2/benchmark_framework.py",
        "status": "removed",
        "additions": 0,
        "deletions": 1199,
        "changes": 1199,
        "patch": "@@ -1,1199 +0,0 @@\n-# Copyright 2025 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-import gc\n-import json\n-import logging\n-import os\n-import statistics\n-import sys\n-import threading\n-import time\n-from abc import ABC, abstractmethod\n-from dataclasses import asdict, dataclass, field\n-from datetime import datetime\n-from typing import Any, Optional, TypedDict, Union\n-\n-import gpustat\n-import numpy as np\n-import psutil\n-import torch\n-\n-\n-class GPUMetrics(TypedDict):\n-    \"\"\"GPU monitoring result with GPU metrics.\"\"\"\n-\n-    gpu_utilization_mean: float\n-    gpu_utilization_max: float\n-    gpu_utilization_min: float\n-    gpu_memory_used_mean: float\n-    gpu_memory_used_max: float\n-    gpu_memory_used_min: float\n-    sample_count: int\n-    gpu_monitoring_status: str\n-\n-\n-class NoGPU(TypedDict):\n-    \"\"\"GPU monitoring result without GPU metrics.\"\"\"\n-\n-    gpu_monitoring_status: str\n-    gpu_monitoring_reason: str\n-\n-\n-class ArchAwareTimer:\n-    \"\"\"Architecture-aware timer for supposedly better prescision\"\"\"\n-\n-    def __init__(self, device: Optional[str] = None):\n-        \"\"\"\n-        Initialize architecture-aware timer.\n-\n-        Args:\n-            device: Device to use. If None, uses current device.\n-        \"\"\"\n-        self.device = device\n-        self.use_cuda = torch.cuda.is_available()\n-\n-        if self.use_cuda:\n-            if device and device != \"cpu\":\n-                self.device_obj = torch.device(device)\n-            else:\n-                # Fall back to CPU timing if device is CPU or CUDA not available\n-                self.use_cuda = False\n-\n-        if self.use_cuda:\n-            try:\n-                # Create CUDA events for timing\n-                self.start_event = torch.cuda.Event(enable_timing=True)\n-                self.end_event = torch.cuda.Event(enable_timing=True)\n-            except RuntimeError:\n-                # Fall back to CPU timing if CUDA events fail\n-                self.use_cuda = False\n-\n-        if not self.use_cuda:\n-            self.start_time = None\n-            self.end_time = None\n-\n-    def start(self):\n-        \"\"\"Start timing.\"\"\"\n-        if self.use_cuda:\n-            torch.cuda.synchronize(self.device_obj)\n-            self.start_event.record(stream=torch.cuda.current_stream(self.device_obj))\n-        else:\n-            self.start_time = time.perf_counter()\n-\n-    def stop(self):\n-        \"\"\"Stop timing.\"\"\"\n-        if self.use_cuda:\n-            self.end_event.record(stream=torch.cuda.current_stream(self.device_obj))\n-            torch.cuda.synchronize(self.device_obj)\n-        else:\n-            self.end_time = time.perf_counter()\n-\n-    def elapsed_time(self) -> float:\n-        \"\"\"\n-        Get elapsed time in seconds.\n-\n-        Returns:\n-            Elapsed time in seconds\n-        \"\"\"\n-        if self.use_cuda:\n-            # CUDA events return time in milliseconds, convert to seconds\n-            return self.start_event.elapsed_time(self.end_event) / 1000.0\n-        else:\n-            if self.start_time is None or self.end_time is None:\n-                raise RuntimeError(\"Timer not properly started/stopped\")\n-            return self.end_time - self.start_time\n-\n-    @property\n-    def timing_method(self) -> str:\n-        \"\"\"Get the timing method being used.\"\"\"\n-        return \"CUDA Events\" if self.use_cuda else \"CPU perf_counter\"\n-\n-    def __enter__(self):\n-        \"\"\"Context manager entry.\"\"\"\n-        self.start()\n-        return self\n-\n-    def __exit__(self, exc_type, exc_val, exc_tb):\n-        \"\"\"Context manager exit.\"\"\"\n-        self.stop()\n-\n-\n-@dataclass\n-class BenchmarkConfig:\n-    \"\"\"Configuration for a single benchmark scenario.\"\"\"\n-\n-    name: str\n-    model_id: str\n-    variant: str = \"eager\"  # \"eager\", \"compiled\", \"kernelized\"\n-    warmup_iterations: int = 3\n-    measurement_iterations: int = 10\n-    num_tokens_to_generate: int = 100\n-    device: str = \"cuda\"\n-    torch_dtype: str = \"float16\"\n-    compile_mode: Optional[str] = None  # None, \"default\", \"reduce-overhead\", \"max-autotune\"\n-    compile_options: dict[str, Any] = field(default_factory=dict)\n-    use_cache: bool = True\n-    batch_size: int = 1\n-    sequence_length: Optional[int] = None\n-    attn_implementation: str = \"sdpa\"  # \"eager\", \"sdpa\", \"flash_attention_2\"\n-    sdpa_backend: Optional[str] = None  # None, \"math\", \"flash_attention\", \"efficient_attention\", \"cudnn_attention\"\n-    custom_params: dict[str, Any] = field(default_factory=dict)\n-\n-\n-class BenchmarkScenario:\n-    \"\"\"\n-    A benchmark scenario that encapsulates both configuration and setup logic.\n-    This makes it easier to define and adapt benchmarks for different models.\n-    \"\"\"\n-\n-    def __init__(self, name: str, config: BenchmarkConfig, description: str = \"\"):\n-        self.name = name\n-        self.config = config\n-        self.description = description\n-        self._setup_callbacks = []\n-        self._teardown_callbacks = []\n-\n-    def add_setup_callback(self, callback: callable):\n-        \"\"\"Add a callback to be executed during scenario setup.\"\"\"\n-        self._setup_callbacks.append(callback)\n-\n-    def add_teardown_callback(self, callback: callable):\n-        \"\"\"Add a callback to be executed during scenario teardown.\"\"\"\n-        self._teardown_callbacks.append(callback)\n-\n-    def setup(self, model, tokenizer, logger=None):\n-        \"\"\"Execute setup callbacks for this scenario.\"\"\"\n-        for callback in self._setup_callbacks:\n-            try:\n-                callback(model, tokenizer, self.config, logger)\n-            except Exception as e:\n-                if logger:\n-                    logger.warning(f\"Setup callback failed for scenario {self.name}: {e}\")\n-\n-    def teardown(self, model, tokenizer, logger=None):\n-        \"\"\"Execute teardown callbacks for this scenario.\"\"\"\n-        for callback in self._teardown_callbacks:\n-            try:\n-                callback(model, tokenizer, self.config, logger)\n-            except Exception as e:\n-                if logger:\n-                    logger.warning(f\"Teardown callback failed for scenario {self.name}: {e}\")\n-\n-    def __repr__(self):\n-        return f\"BenchmarkScenario(name='{self.name}', variant='{self.config.variant}')\"\n-\n-\n-@dataclass\n-class TimingResult:\n-    \"\"\"Result from a timing measurement.\"\"\"\n-\n-    time_to_first_token_seconds: Optional[float] = None\n-    latency_seconds: float = 0.0\n-    tokens_per_second: Optional[float] = None\n-    time_per_output_token_seconds: Optional[float] = None\n-    total_tokens_generated: int = 0\n-    metadata: dict[str, Any] = field(default_factory=dict)\n-\n-\n-@dataclass\n-class BenchmarkStatistics:\n-    \"\"\"Statistical analysis of benchmark measurements.\"\"\"\n-\n-    name: str\n-    measurements: list[float]\n-    mean: float\n-    median: float\n-    std: float\n-    min: float\n-    max: float\n-    p25: float  # 25th percentile\n-    p75: float  # 75th percentile\n-    p90: float  # 90th percentile\n-    p95: float  # 95th percentile\n-    p99: float  # 99th percentile\n-    unit: str = \"seconds\"\n-\n-    @classmethod\n-    def from_measurements(cls, name: str, measurements: list[float], unit: str = \"seconds\") -> \"BenchmarkStatistics\":\n-        \"\"\"Create statistics from a list of measurements.\"\"\"\n-        if not measurements:\n-            raise ValueError(\"Cannot create statistics from empty measurements\")\n-\n-        measurements_array = np.array(measurements)\n-\n-        return cls(\n-            name=name,\n-            measurements=measurements,\n-            mean=float(np.mean(measurements_array)),\n-            median=float(np.median(measurements_array)),\n-            std=float(np.std(measurements_array)),\n-            min=float(np.min(measurements_array)),\n-            max=float(np.max(measurements_array)),\n-            p25=float(np.percentile(measurements_array, 25)),\n-            p75=float(np.percentile(measurements_array, 75)),\n-            p90=float(np.percentile(measurements_array, 90)),\n-            p95=float(np.percentile(measurements_array, 95)),\n-            p99=float(np.percentile(measurements_array, 99)),\n-            unit=unit,\n-        )\n-\n-\n-@dataclass\n-class HardwareInfo:\n-    \"\"\"Hardware information collected during benchmarking.\"\"\"\n-\n-    gpu_name: str\n-    gpu_memory_total_mb: int\n-    cpu_count: int\n-    memory_total_mb: int\n-    python_version: str\n-    torch_version: Optional[str] = None\n-    cuda_version: Optional[str] = None\n-\n-\n-@dataclass\n-class BenchmarkMetadata:\n-    \"\"\"Metadata collected for each benchmark run.\"\"\"\n-\n-    timestamp: str\n-    commit_id: str\n-    hardware_info: HardwareInfo\n-    config: BenchmarkConfig\n-\n-\n-class GPUMonitor:\n-    \"\"\"Monitor GPU utilization during benchmark execution.\"\"\"\n-\n-    def __init__(self, sample_interval: float = 0.1, logger: Optional[logging.Logger] = None):\n-        self.sample_interval = sample_interval\n-        self.logger = logger or logging.getLogger(__name__)\n-        self.stop_event = threading.Event()\n-        self.thread = None\n-        self.gpu_utilization = []\n-        self.gpu_memory_used = []\n-        self.timestamps = []\n-        self.gpu_available = False\n-        self.warning_logged = False\n-\n-        # Test GPU availability on initialization\n-        self._test_gpu_availability()\n-\n-    def _test_gpu_availability(self):\n-        \"\"\"Test if GPU monitoring is available.\"\"\"\n-        try:\n-            gpu_stats = gpustat.GPUStatCollection.new_query()\n-            if gpu_stats and len(gpu_stats) > 0:\n-                self.gpu_available = True\n-                self.logger.debug(f\"GPU monitoring available: {len(gpu_stats)} GPU(s) detected\")\n-            else:\n-                self.gpu_available = False\n-                self.logger.debug(\"No GPUs detected by gpustat\")\n-        except Exception as e:\n-            self.gpu_available = False\n-            self.logger.debug(f\"GPU monitoring not available: {e}\")\n-\n-    def start(self):\n-        \"\"\"Start monitoring GPU metrics.\"\"\"\n-        if not self.gpu_available:\n-            self.logger.debug(\"GPU monitoring disabled: no GPUs available\")\n-            return\n-\n-        # Clear the stop event to enable monitoring\n-        self.stop_event.clear()\n-        self.gpu_utilization = []\n-        self.gpu_memory_used = []\n-        self.timestamps = []\n-        self.warning_logged = False  # Reset warning flag for new monitoring session\n-        self.thread = threading.Thread(target=self._monitor_loop)\n-        self.thread.start()\n-        self.logger.debug(\"GPU monitoring started\")\n-\n-    def stop_and_collect(self) -> Union[GPUMetrics, NoGPU]:\n-        \"\"\"Stop monitoring and return collected metrics.\"\"\"\n-        if not self.gpu_available:\n-            return NoGPU(gpu_monitoring_status=\"disabled\", gpu_monitoring_reason=\"no_gpus_available\")\n-\n-        # Signal the monitoring thread to stop\n-        self.stop_event.set()\n-        if self.thread:\n-            self.thread.join()\n-\n-        if self.gpu_utilization:\n-            metrics = GPUMetrics(\n-                gpu_utilization_mean=statistics.mean(self.gpu_utilization),\n-                gpu_utilization_max=max(self.gpu_utilization),\n-                gpu_utilization_min=min(self.gpu_utilization),\n-                gpu_memory_used_mean=statistics.mean(self.gpu_memory_used),\n-                gpu_memory_used_max=max(self.gpu_memory_used),\n-                gpu_memory_used_min=min(self.gpu_memory_used),\n-                sample_count=len(self.gpu_utilization),\n-                gpu_monitoring_status=\"success\",\n-            )\n-            self.logger.debug(f\"GPU monitoring completed: {len(self.gpu_utilization)} samples collected\")\n-            return metrics\n-        else:\n-            return NoGPU(gpu_monitoring_status=\"failed\", gpu_monitoring_reason=\"no_samples_collected\")\n-\n-    def _monitor_loop(self):\n-        \"\"\"Background monitoring loop using threading.Event for communication.\"\"\"\n-        consecutive_failures = 0\n-        max_consecutive_failures = 5\n-\n-        # Continue monitoring until stop_event is set\n-        while not self.stop_event.is_set():\n-            try:\n-                gpu_stats = gpustat.GPUStatCollection.new_query()\n-                if gpu_stats and len(gpu_stats) > 0:\n-                    gpu = gpu_stats[0]\n-                    self.gpu_utilization.append(gpu[\"utilization.gpu\"])\n-                    self.gpu_memory_used.append(gpu[\"memory.used\"])\n-                    self.timestamps.append(time.time())\n-                    consecutive_failures = 0  # Reset failure counter on success\n-                else:\n-                    consecutive_failures += 1\n-                    if consecutive_failures >= max_consecutive_failures and not self.warning_logged:\n-                        self.logger.warning(\"GPU monitoring: No GPU data returned by gpustat\")\n-                        self.warning_logged = True\n-\n-            except Exception as e:\n-                consecutive_failures += 1\n-                if consecutive_failures >= max_consecutive_failures and not self.warning_logged:\n-                    self.logger.warning(f\"GPU monitoring failed after {max_consecutive_failures} attempts: {e}\")\n-                    self.warning_logged = True\n-\n-            # Use Event.wait() with timeout instead of time.sleep()\n-            # This allows for immediate response to stop signal while still maintaining sample interval\n-            if self.stop_event.wait(timeout=self.sample_interval):\n-                # Event was set, break out of loop immediately\n-                break\n-\n-\n-def get_hardware_info() -> HardwareInfo:\n-    \"\"\"Collect hardware information.\"\"\"\n-    gpu_name = \"unknown\"\n-    gpu_memory_total = 0\n-\n-    try:\n-        gpu_stats = gpustat.GPUStatCollection.new_query()\n-        if gpu_stats and len(gpu_stats) > 0:\n-            gpu = gpu_stats[0]\n-            gpu_name = gpu[\"name\"]\n-            gpu_memory_total = gpu[\"memory.total\"]\n-    except Exception:\n-        pass\n-\n-    torch_version = torch.__version__\n-    cuda_version = None\n-    if hasattr(torch, \"cuda\") and torch.cuda.is_available():\n-        cuda_version = torch.version.cuda\n-\n-    return HardwareInfo(\n-        gpu_name=gpu_name,\n-        gpu_memory_total_mb=gpu_memory_total,\n-        cpu_count=psutil.cpu_count(),\n-        memory_total_mb=int(psutil.virtual_memory().total / (1024 * 1024)),\n-        python_version=f\"{sys.version.split()[0]}\",\n-        torch_version=torch_version,\n-        cuda_version=cuda_version,\n-    )\n-\n-\n-def flush_memory():\n-    \"\"\"Flush GPU memory and run garbage collection.\"\"\"\n-    gc.collect()\n-    if hasattr(torch, \"cuda\") and torch.cuda.is_available():\n-        torch.cuda.empty_cache()\n-        torch.cuda.reset_max_memory_allocated()\n-        torch.cuda.reset_peak_memory_stats()\n-        torch.cuda.synchronize()\n-\n-\n-def get_sdpa_backend(backend_name: Optional[str]):\n-    \"\"\"Get the SDPA backend enum from string name.\"\"\"\n-    if backend_name is None:\n-        return None\n-\n-    try:\n-        backend_map = {\n-            \"math\": torch.nn.attention.SDPBackend.MATH,\n-            \"flash_attention\": torch.nn.attention.SDPBackend.FLASH_ATTENTION,\n-            \"efficient_attention\": torch.nn.attention.SDPBackend.EFFICIENT_ATTENTION,\n-            \"cudnn_attention\": torch.nn.attention.SDPBackend.CUDNN_ATTENTION,\n-        }\n-        return backend_map.get(backend_name.lower())\n-    except AttributeError:\n-        # torch.nn.attention.SDPBackend not available in older torch versions\n-        return None\n-\n-\n-class SDPAContext:\n-    \"\"\"Context manager for SDPA kernel selection.\"\"\"\n-\n-    def __init__(self, backend_name: Optional[str], logger: Optional[logging.Logger] = None):\n-        self.backend_name = backend_name\n-        self.logger = logger or logging.getLogger(__name__)\n-        self.backend = get_sdpa_backend(backend_name) if backend_name else None\n-        self.context = None\n-\n-    def __enter__(self):\n-        if self.backend is not None:\n-            try:\n-                self.context = torch.nn.attention.sdpa_kernel(self.backend)\n-                self.context.__enter__()\n-                if self.logger:\n-                    self.logger.debug(f\"Using SDPA backend: {self.backend_name}\")\n-            except Exception as e:\n-                if self.logger:\n-                    self.logger.warning(f\"Failed to set SDPA backend {self.backend_name}: {e}\")\n-                self.context = None\n-        elif self.backend_name and self.logger:\n-            self.logger.debug(\n-                f\"SDPA backend '{self.backend_name}' requested but not using kernel context (backend={self.backend})\"\n-            )\n-        return self\n-\n-    def __exit__(self, exc_type, exc_val, exc_tb):\n-        if self.context is not None:\n-            try:\n-                self.context.__exit__(exc_type, exc_val, exc_tb)\n-            except Exception as e:\n-                if self.logger:\n-                    self.logger.warning(f\"Error exiting SDPA context: {e}\")\n-        return False\n-\n-\n-class AbstractModelBenchmark(ABC):\n-    \"\"\"Abstract base class for model benchmarks.\"\"\"\n-\n-    def __init__(self, logger: logging.Logger):\n-        self.logger = logger\n-        self.model = None\n-        self.tokenizer = None\n-        self.device = None\n-        self.scenarios = {}  # Map of scenario_name -> BenchmarkScenario\n-\n-    @abstractmethod\n-    def create_scenarios(self, **kwargs) -> dict[str, \"BenchmarkScenario\"]:\n-        \"\"\"Create and return a dictionary of benchmark scenarios.\"\"\"\n-        pass\n-\n-    @abstractmethod\n-    def setup_model(self, config: BenchmarkConfig) -> None:\n-        \"\"\"Setup the model for benchmarking with the given configuration.\"\"\"\n-        pass\n-\n-    @abstractmethod\n-    def cleanup_model(self) -> None:\n-        \"\"\"Cleanup model resources.\"\"\"\n-        pass\n-\n-    @abstractmethod\n-    def measure_time_to_first_token(self, config: BenchmarkConfig) -> float:\n-        \"\"\"Measure time to first token generation.\"\"\"\n-        pass\n-\n-    @abstractmethod\n-    def measure_latency(self, config: BenchmarkConfig) -> TimingResult:\n-        \"\"\"Measure full generation latency and compute tokens/sec.\"\"\"\n-        pass\n-\n-    def prepare_inputs(self, config: BenchmarkConfig) -> Any:\n-        \"\"\"Prepare inputs for the model. Override if needed.\"\"\"\n-        return None\n-\n-    def get_scenarios(self, **kwargs) -> dict[str, \"BenchmarkScenario\"]:\n-        \"\"\"Get benchmark scenarios. Creates them if they don't exist.\"\"\"\n-        if not self.scenarios:\n-            self.scenarios = self.create_scenarios(**kwargs)\n-        return self.scenarios\n-\n-\n-class ModelBenchmark(AbstractModelBenchmark):\n-    \"\"\"\n-    Base class for HuggingFace Transformers model benchmarks.\n-\n-    This class provides common scenario creation logic and handles the standard\n-    patterns for eager, compiled, and kernelized execution variants with different\n-    attention implementations and SDPA backends.\n-    \"\"\"\n-\n-    def __init__(self, logger: logging.Logger):\n-        super().__init__(logger)\n-        self.inputs = None\n-        self.compiled_model = None\n-        self.past_key_values = None\n-        self.config = None\n-        self._default_prompt = \"Why dogs are so cute?\"\n-\n-    @property\n-    def default_prompt(self) -> str:\n-        \"\"\"Default prompt for text generation. Override in subclasses if needed.\"\"\"\n-        return self._default_prompt\n-\n-    def get_attention_configs(self, include_sdpa_variants: bool = True) -> list[dict[str, Any]]:\n-        \"\"\"\n-        Get attention implementation configurations.\n-\n-        Args:\n-            include_sdpa_variants: Whether to include SDPA backend variants\n-\n-        Returns:\n-            List of attention configuration dictionaries\n-        \"\"\"\n-        attention_configs = [\n-            {\"attn_implementation\": \"eager\", \"sdpa_backends\": [None], \"desc_suffix\": \" with eager attention\"},\n-        ]\n-\n-        # Add SDPA variants if requested\n-        if include_sdpa_variants:\n-            attention_configs.append(\n-                {\n-                    \"attn_implementation\": \"sdpa\",\n-                    \"sdpa_backends\": [None, \"math\", \"flash_attention\", \"efficient_attention\"],\n-                    \"desc_suffix\": \"\",\n-                }\n-            )\n-\n-        return attention_configs\n-\n-    def get_scenario_configs(self) -> list[dict[str, Any]]:\n-        \"\"\"\n-        Get base scenario configurations. Override in subclasses to customize.\n-\n-        Returns:\n-            List of scenario configuration dictionaries\n-        \"\"\"\n-        return [\n-            # Eager variants\n-            {\"variant\": \"eager\", \"compile_mode\": None, \"use_cache\": True, \"description\": \"Eager execution with cache\"},\n-            # Compiled variants\n-            {\n-                \"variant\": \"compiled\",\n-                \"compile_mode\": \"max-autotune\",\n-                \"use_cache\": True,\n-                \"description\": \"Compiled with max autotune\",\n-            },\n-            # Kernelized variant (if available)\n-            {\n-                \"variant\": \"kernelized\",\n-                \"compile_mode\": \"max-autotune\",\n-                \"use_cache\": True,\n-                \"description\": \"Kernelized execution\",\n-            },\n-        ]\n-\n-    def _is_kernelization_available(self) -> bool:\n-        \"\"\"Check if kernelization is available. Override in subclasses.\"\"\"\n-        try:\n-            from kernels import Mode, kernelize  # noqa: F401\n-\n-            return True\n-        except ImportError:\n-            return False\n-\n-    def get_default_generation_config(self) -> dict[str, Any]:\n-        \"\"\"Get default generation configuration. Override in subclasses for model-specific defaults.\"\"\"\n-        return {\"do_sample\": False, \"top_p\": 1.0, \"temperature\": 1.0}\n-\n-    def get_model_init_kwargs(self, config: BenchmarkConfig) -> dict[str, Any]:\n-        \"\"\"Get model initialization kwargs. Override in subclasses for model-specific parameters.\"\"\"\n-        return {\"torch_dtype\": getattr(torch, config.torch_dtype), \"attn_implementation\": config.attn_implementation}\n-\n-    def get_default_torch_dtype(self) -> str:\n-        \"\"\"Get default torch dtype. Override in subclasses.\"\"\"\n-        return \"float16\"\n-\n-    def get_default_device(self) -> str:\n-        \"\"\"Get default device. Override in subclasses.\"\"\"\n-        return \"cuda\"\n-\n-    def create_scenarios(self, **kwargs) -> dict[str, \"BenchmarkScenario\"]:\n-        \"\"\"Create benchmark scenarios for HuggingFace models.\"\"\"\n-        scenarios = {}\n-\n-        # Extract parameters with model-specific defaults\n-        model_id = kwargs.get(\"model_id\", \"microsoft/DialoGPT-medium\")\n-        warmup_iterations = kwargs.get(\"warmup_iterations\", 3)\n-        measurement_iterations = kwargs.get(\"measurement_iterations\", 5)\n-        num_tokens_to_generate = kwargs.get(\"num_tokens_to_generate\", 100)\n-        include_sdpa_variants = kwargs.get(\"include_sdpa_variants\", True)\n-        device = kwargs.get(\"device\", self.get_default_device())\n-        torch_dtype = kwargs.get(\"torch_dtype\", self.get_default_torch_dtype())\n-        batch_size = kwargs.get(\"batch_size\", 1)\n-\n-        # Get configurations\n-        attention_configs = self.get_attention_configs(include_sdpa_variants)\n-        scenario_configs = self.get_scenario_configs()\n-\n-        # Create scenarios for each attention config and variant combination\n-        for attn_config in attention_configs:\n-            attn_implementation = attn_config[\"attn_implementation\"]\n-            sdpa_backends = attn_config[\"sdpa_backends\"]\n-            desc_suffix = attn_config[\"desc_suffix\"]\n-\n-            for scenario_config in scenario_configs:\n-                for sdpa_backend in sdpa_backends:\n-                    # Skip kernelized if not available\n-                    if scenario_config[\"variant\"] == \"kernelized\" and not self._is_kernelization_available():\n-                        continue\n-\n-                    # Create unique config for this scenario\n-                    config = BenchmarkConfig(\n-                        name=scenario_config[\"variant\"],\n-                        model_id=model_id,\n-                        variant=scenario_config[\"variant\"],\n-                        compile_mode=scenario_config[\"compile_mode\"],\n-                        use_cache=scenario_config[\"use_cache\"],\n-                        warmup_iterations=warmup_iterations,\n-                        measurement_iterations=measurement_iterations,\n-                        num_tokens_to_generate=num_tokens_to_generate,\n-                        device=device,\n-                        torch_dtype=torch_dtype,\n-                        batch_size=batch_size,\n-                        attn_implementation=attn_implementation,\n-                        sdpa_backend=sdpa_backend if attn_implementation == \"sdpa\" else None,\n-                    )\n-\n-                    # Create scenario name\n-                    scenario_name_parts = [scenario_config[\"variant\"]]\n-                    if scenario_config[\"compile_mode\"]:\n-                        scenario_name_parts.append(f\"compile_{scenario_config['compile_mode']}\")\n-\n-                    # Add attention implementation to name\n-                    if attn_implementation == \"eager\":\n-                        scenario_name_parts.append(\"eager_attn\")\n-                    elif attn_implementation == \"sdpa\":\n-                        if sdpa_backend:\n-                            scenario_name_parts.append(f\"sdpa_{sdpa_backend}\")\n-                        else:\n-                            scenario_name_parts.append(\"sdpa_default\")\n-\n-                    scenario_name = \"_\".join(scenario_name_parts)\n-\n-                    # Create description\n-                    description = scenario_config[\"description\"]\n-                    if attn_implementation == \"sdpa\" and sdpa_backend:\n-                        description += f\" with SDPA {sdpa_backend} backend\"\n-                    elif attn_implementation == \"sdpa\":\n-                        description += \" with SDPA default backend\"\n-                    else:\n-                        description += desc_suffix\n-\n-                    # Create scenario\n-                    scenario = BenchmarkScenario(name=scenario_name, config=config, description=description)\n-\n-                    # Add setup callbacks based on variant\n-                    if scenario_config[\"variant\"] == \"compiled\":\n-                        scenario.add_setup_callback(self._setup_compilation_callback)\n-                    elif scenario_config[\"variant\"] == \"kernelized\":\n-                        scenario.add_setup_callback(self._setup_kernelization_callback)\n-\n-                    scenarios[scenario_name] = scenario\n-\n-        return scenarios\n-\n-    def _setup_compilation_callback(self, model, tokenizer, config, logger):\n-        \"\"\"Setup callback for compilation scenarios.\"\"\"\n-        if logger:\n-            logger.info(f\"Setting up compilation with mode: {config.compile_mode}\")\n-\n-        # Perform torch.compile\n-        if config.compile_mode is not None:\n-            self.compiled_model = torch.compile(model, mode=config.compile_mode, **config.compile_options)\n-        else:\n-            self.compiled_model = torch.compile(model, **config.compile_options)\n-\n-        # Setup static cache for compiled mode if needed\n-        if config.use_cache and hasattr(self, \"inputs\") and self.inputs is not None:\n-            self._setup_static_cache(config)\n-\n-    def _setup_kernelization_callback(self, model, tokenizer, config, logger):\n-        \"\"\"Setup callback for kernelization scenarios.\"\"\"\n-        if logger:\n-            logger.info(\"Setting up kernelization\")\n-\n-        try:\n-            from kernels import Mode, kernelize\n-\n-            self.compiled_model = kernelize(model, mode=Mode.INFERENCE)\n-        except Exception as e:\n-            if logger:\n-                logger.warning(f\"Failed to setup kernelized mode: {e}\")\n-                logger.warning(\"Falling back to eager mode\")\n-            config.variant = \"eager\"\n-\n-    def _setup_static_cache(self, config: BenchmarkConfig):\n-        \"\"\"Setup static cache for compiled models. Override if needed.\"\"\"\n-        if hasattr(self, \"inputs\") and self.inputs is not None:\n-            try:\n-                from transformers import StaticCache\n-\n-                seq_length = self.inputs[\"input_ids\"].shape[1]\n-\n-                # Get the actual device the model is on\n-                if hasattr(self.model, \"device\"):\n-                    cache_device = self.model.device\n-                else:\n-                    cache_device = self.device\n-\n-                self.past_key_values = StaticCache(\n-                    config=self.model.config,\n-                    max_batch_size=config.batch_size,\n-                    max_cache_len=seq_length + config.num_tokens_to_generate,\n-                    device=cache_device,\n-                    dtype=getattr(torch, config.torch_dtype),\n-                )\n-                self.logger.debug(f\"StaticCache created on device: {cache_device}\")\n-            except (ImportError, TypeError) as e:\n-                # StaticCache not available or incompatible, continue without it\n-                self.logger.debug(f\"StaticCache setup failed: {e}, continuing without cache\")\n-                self.past_key_values = None\n-\n-    def setup_model(self, config: BenchmarkConfig) -> None:\n-        \"\"\"Setup the HuggingFace model for benchmarking with the given configuration.\"\"\"\n-\n-        self.logger.info(f\"Setting up model: {config.model_id} with variant: {config.variant}\")\n-        self.device = config.device\n-        self.config = config\n-\n-        # Load model and tokenizer\n-        self._load_model_and_tokenizer(config)\n-\n-        # Prepare inputs\n-        self._prepare_model_inputs(config)\n-\n-        # Configure generation settings\n-        self._configure_generation(config)\n-\n-        self.logger.info(\"Model setup complete\")\n-\n-    def _load_model_and_tokenizer(self, config: BenchmarkConfig):\n-        \"\"\"Load the model and tokenizer. Override in subclasses for custom loading.\"\"\"\n-\n-        from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n-\n-        # Load tokenizer\n-        self.tokenizer = AutoTokenizer.from_pretrained(config.model_id)\n-        if self.tokenizer.pad_token is None:\n-            self.tokenizer.pad_token = self.tokenizer.eos_token\n-\n-        # Prepare generation config\n-        generation_config_dict = self.get_default_generation_config()\n-        gen_config = GenerationConfig(**generation_config_dict)\n-\n-        # Load model\n-        self.logger.info(\"Loading model...\")\n-\n-        target_device = config.device\n-        # Get model initialization kwargs\n-        model_init_kwargs = self.get_model_init_kwargs(config)\n-        model_init_kwargs.update({\"generation_config\": gen_config})\n-\n-        self.model = AutoModelForCausalLM.from_pretrained(config.model_id, **model_init_kwargs).eval()\n-\n-        # Move model to target device\n-        self.logger.info(f\"Moving model to device: {target_device}\")\n-        self.model.to(target_device)\n-        self.device = target_device  # Update device to match actual device used\n-\n-    def _prepare_model_inputs(self, config: BenchmarkConfig):\n-        \"\"\"Prepare model inputs. Override in subclasses for custom inputs.\"\"\"\n-        # Prepare inputs\n-        self.inputs = self.tokenizer(self.default_prompt, return_tensors=\"pt\")\n-\n-        # Move inputs to the same device as the model\n-        if hasattr(self.model, \"device\"):\n-            # Model is on a single device\n-            model_device = self.model.device\n-        else:\n-            # Model might be distributed, use self.device which was set during model loading\n-            model_device = self.device\n-\n-        self.inputs = {k: v.to(model_device) for k, v in self.inputs.items()}\n-        self.logger.debug(f\"Moved inputs to device: {model_device}\")\n-\n-    def _configure_generation(self, config: BenchmarkConfig):\n-        \"\"\"Configure generation settings.\"\"\"\n-        seq_length = self.inputs[\"input_ids\"].shape[1]\n-        self.model.generation_config.max_length = seq_length + config.num_tokens_to_generate\n-\n-    def cleanup_model(self) -> None:\n-        \"\"\"Cleanup model resources.\"\"\"\n-        if hasattr(self, \"model\") and self.model is not None:\n-            del self.model\n-            self.model = None\n-        if hasattr(self, \"compiled_model\") and self.compiled_model is not None:\n-            del self.compiled_model\n-            self.compiled_model = None\n-        if hasattr(self, \"tokenizer\") and self.tokenizer is not None:\n-            del self.tokenizer\n-            self.tokenizer = None\n-        if hasattr(self, \"past_key_values\") and self.past_key_values is not None:\n-            del self.past_key_values\n-            self.past_key_values = None\n-\n-        # Clear CUDA cache\n-        flush_memory()\n-\n-    def measure_time_to_first_token(self, config: BenchmarkConfig) -> float:\n-        \"\"\"Measure time to first token generation.\"\"\"\n-        model_to_use = self.compiled_model if self.compiled_model is not None else self.model\n-\n-        # Prepare generation kwargs\n-        generation_kwargs = self._get_generation_kwargs(config, max_new_tokens=1)\n-\n-        # Use CUDA timer for high-precision measurement\n-        with ArchAwareTimer(device=config.device) as timer:\n-            # Use SDPA context if specified\n-            with SDPAContext(config.sdpa_backend, self.logger):\n-                with torch.no_grad():\n-                    _ = model_to_use.generate(**generation_kwargs)\n-\n-        return timer.elapsed_time()\n-\n-    def measure_latency(self, config: BenchmarkConfig) -> TimingResult:\n-        \"\"\"Measure full generation latency and compute tokens/sec.\"\"\"\n-        model_to_use = self.compiled_model if self.compiled_model is not None else self.model\n-\n-        # Prepare generation kwargs\n-        generation_kwargs = self._get_generation_kwargs(config, max_new_tokens=config.num_tokens_to_generate)\n-\n-        # Use CUDA timer for high-precision measurement\n-        with ArchAwareTimer(device=config.device) as timer:\n-            # Use SDPA context if specified\n-            with SDPAContext(config.sdpa_backend, self.logger):\n-                with torch.no_grad():\n-                    outputs = model_to_use.generate(**generation_kwargs)\n-\n-        # Calculate metrics\n-        latency = timer.elapsed_time()\n-        input_length = self.inputs[\"input_ids\"].shape[1]\n-        output_length = outputs.shape[1]\n-        tokens_generated = output_length - input_length\n-\n-        tokens_per_second = tokens_generated / latency if latency > 0 else 0\n-        time_per_output_token = latency / tokens_generated if tokens_generated > 0 else None\n-\n-        return TimingResult(\n-            latency_seconds=latency,\n-            tokens_per_second=tokens_per_second,\n-            time_per_output_token_seconds=time_per_output_token,\n-            total_tokens_generated=tokens_generated,\n-            metadata={\n-                \"input_length\": input_length,\n-                \"output_length\": output_length,\n-                \"variant\": config.variant,\n-                \"compile_mode\": config.compile_mode,\n-                \"attn_implementation\": config.attn_implementation,\n-                \"sdpa_backend\": config.sdpa_backend,\n-            },\n-        )\n-\n-    def _get_generation_kwargs(self, config: BenchmarkConfig, max_new_tokens: int) -> dict[str, Any]:\n-        \"\"\"Get generation kwargs. Override in subclasses for custom generation.\"\"\"\n-        generation_config_dict = self.get_default_generation_config()\n-        generation_kwargs = {\n-            **self.inputs,\n-            \"max_new_tokens\": max_new_tokens,\n-            \"do_sample\": generation_config_dict.get(\"do_sample\", False),\n-            \"temperature\": generation_config_dict.get(\"temperature\", 1.0),\n-            \"top_p\": generation_config_dict.get(\"top_p\", 1.0),\n-            \"pad_token_id\": self.tokenizer.pad_token_id,\n-        }\n-\n-        # Handle static cache for compiled models\n-        if self.past_key_values is not None and config.variant == \"compiled\":\n-            try:\n-                from transformers import StaticCache\n-\n-                # Reset cache for each measurement\n-                seq_length = self.inputs[\"input_ids\"].shape[1]\n-\n-                # Get the actual device the model is on\n-                if hasattr(self.model, \"device\"):\n-                    cache_device = self.model.device\n-                else:\n-                    cache_device = self.device\n-\n-                fresh_cache = StaticCache(\n-                    config=self.model.config,\n-                    max_batch_size=config.batch_size,\n-                    max_cache_len=seq_length + max_new_tokens,\n-                    device=cache_device,\n-                    dtype=getattr(torch, config.torch_dtype),\n-                )\n-                generation_kwargs[\"past_key_values\"] = fresh_cache\n-            except (ImportError, TypeError) as e:\n-                self.logger.debug(f\"Fresh StaticCache creation failed: {e}\")\n-                pass\n-\n-        return generation_kwargs\n-\n-\n-class BenchmarkRunner:\n-    \"\"\"Main benchmark runner that coordinates benchmark execution.\"\"\"\n-\n-    def __init__(self, logger: logging.Logger, output_dir: str = \"benchmark_results\"):\n-        self.logger = logger\n-        self.output_dir = output_dir\n-        os.makedirs(output_dir, exist_ok=True)\n-\n-    def run_benchmark(\n-        self,\n-        benchmark: ModelBenchmark,\n-        scenarios: dict[str, BenchmarkScenario],\n-        collect_gpu_metrics: bool = True,\n-        commit_id: Optional[str] = None,\n-    ) -> dict[str, dict[str, Any]]:\n-        \"\"\"\n-        Run benchmarks using scenarios.\n-\n-        Args:\n-            benchmark: The benchmark instance to run\n-            scenarios: Dictionary mapping scenario names to BenchmarkScenario instances\n-            collect_gpu_metrics: Whether to collect GPU utilization metrics\n-            commit_id: Git commit ID for metadata (if not provided, will auto-detect from git)\n-\n-        Returns:\n-            Dictionary mapping scenario names to results with statistics\n-        \"\"\"\n-        all_results = {}\n-\n-        for scenario_name, scenario in scenarios.items():\n-            self.logger.info(f\"Running benchmark scenario: {scenario_name}\")\n-            config = scenario.config\n-\n-            try:\n-                # Setup model for this configuration\n-                benchmark.setup_model(config)\n-\n-                # Run scenario setup callbacks\n-                scenario.setup(benchmark.model, benchmark.tokenizer, self.logger)\n-\n-                # Quick validation: try one measurement first to see if this scenario works\n-                try:\n-                    flush_memory()\n-                    test_result = benchmark.measure_time_to_first_token(config)\n-                    if test_result is None or test_result <= 0:\n-                        raise ValueError(\"Invalid measurement result\")\n-                except Exception as validation_error:\n-                    self.logger.warning(f\"Skipping scenario {scenario_name}: validation failed - {validation_error}\")\n-                    # Clean up and skip this scenario\n-                    try:\n-                        scenario.teardown(benchmark.model, benchmark.tokenizer, self.logger)\n-                        benchmark.cleanup_model()\n-                    except Exception:\n-                        pass\n-                    continue\n-\n-                # Collect metadata\n-                metadata = BenchmarkMetadata(\n-                    timestamp=datetime.utcnow().isoformat(),\n-                    commit_id=commit_id,\n-                    hardware_info=get_hardware_info(),\n-                    config=config,\n-                )\n-\n-                # Initialize GPU monitor\n-                gpu_monitor = None\n-                if collect_gpu_metrics:\n-                    gpu_monitor = GPUMonitor(logger=self.logger)\n-\n-                # Warmup runs\n-                self.logger.info(f\"Warming up with {config.warmup_iterations} iterations...\")\n-                warmup_failures = 0\n-                for i in range(config.warmup_iterations):\n-                    try:\n-                        _ = benchmark.measure_latency(config)\n-                    except Exception as e:\n-                        warmup_failures += 1\n-                        self.logger.warning(f\"Warmup iteration {i + 1} failed: {e}\")\n-\n-                # If more than half the warmup iterations failed, skip this scenario\n-                if warmup_failures > config.warmup_iterations // 2:\n-                    self.logger.warning(\n-                        f\"Skipping scenario {scenario_name}: too many warmup failures ({warmup_failures}/{config.warmup_iterations})\"\n-                    )\n-                    try:\n-                        scenario.teardown(benchmark.model, benchmark.tokenizer, self.logger)\n-                        benchmark.cleanup_model()\n-                    except Exception:\n-                        pass\n-                    continue\n-\n-                # Start GPU monitoring\n-                if gpu_monitor:\n-                    gpu_monitor.start()\n-\n-                # Measurement runs for latency\n-                self.logger.info(f\"Measuring latency with {config.measurement_iterations} iterations...\")\n-                latency_measurements = []\n-                ttft_measurements = []\n-                tokens_per_sec_measurements = []\n-                itl_measurements = []  # Inter-Token Latency\n-                measurement_failures = 0\n-\n-                for i in range(config.measurement_iterations):\n-                    try:\n-                        # Measure time to first token\n-                        ttft = benchmark.measure_time_to_first_token(config)\n-                        ttft_measurements.append(ttft)\n-\n-                        # Measure full latency\n-                        timing_result = benchmark.measure_latency(config)\n-                        latency_measurements.append(timing_result.latency_seconds)\n-\n-                        if timing_result.tokens_per_second is not None:\n-                            tokens_per_sec_measurements.append(timing_result.tokens_per_second)\n-\n-                        if timing_result.time_per_output_token_seconds is not None:\n-                            itl_measurements.append(timing_result.time_per_output_token_seconds)\n-\n-                        itl_str = (\n-                            f\", itl={timing_result.time_per_output_token_seconds:.4f}s/token\"\n-                            if timing_result.time_per_output_token_seconds\n-                            else \"\"\n-                        )\n-                        self.logger.debug(\n-                            f\"Iteration {i + 1}: latency={timing_result.latency_seconds:.4f}s, ttft={ttft:.4f}s{itl_str}\"\n-                        )\n-\n-                    except Exception as e:\n-                        measurement_failures += 1\n-                        self.logger.warning(f\"Measurement iteration {i + 1} failed: {e}\")\n-\n-                # Stop GPU monitoring\n-                gpu_metrics = {}\n-                if gpu_monitor:\n-                    gpu_metrics = gpu_monitor.stop_and_collect()\n-\n-                # If we don't have enough successful measurements, skip this scenario\n-                if not latency_measurements or len(latency_measurements) < config.measurement_iterations // 2:\n-                    self.logger.warning(\n-                        f\"Skipping scenario {scenario_name}: insufficient successful measurements ({len(latency_measurements)}/{config.measurement_iterations})\"\n-                    )\n-                    try:\n-                        scenario.teardown(benchmark.model, benchmark.tokenizer, self.logger)\n-                        benchmark.cleanup_model()\n-                    except Exception:\n-                        pass\n-                    continue\n-\n-                # Calculate statistics\n-                scenario_results = {\n-                    \"metadata\": asdict(metadata),\n-                    \"measurements\": {},\n-                    \"gpu_metrics\": gpu_metrics,\n-                    \"scenario_description\": scenario.description,\n-                }\n-\n-                if latency_measurements:\n-                    latency_stats = BenchmarkStatistics.from_measurements(\"latency_seconds\", latency_measurements)\n-                    scenario_results[\"measurements\"][\"latency_seconds\"] = asdict(latency_stats)\n-\n-                if ttft_measurements:\n-                    ttft_stats = BenchmarkStatistics.from_measurements(\n-                        \"time_to_first_token_seconds\", ttft_measurements\n-                    )\n-                    scenario_results[\"measurements\"][\"time_to_first_token_seconds\"] = asdict(ttft_stats)\n-\n-                if tokens_per_sec_measurements:\n-                    tps_stats = BenchmarkStatistics.from_measurements(\n-                        \"tokens_per_second\", tokens_per_sec_measurements, \"tokens/sec\"\n-                    )\n-                    scenario_results[\"measurements\"][\"tokens_per_second\"] = asdict(tps_stats)\n-\n-                if itl_measurements:\n-                    itl_stats = BenchmarkStatistics.from_measurements(\n-                        \"time_per_output_token_seconds\", itl_measurements, \"seconds/token\"\n-                    )\n-                    scenario_results[\"measurements\"][\"time_per_output_token_seconds\"] = asdict(itl_stats)\n-\n-                # Log summary\n-                if latency_measurements:\n-                    self.logger.info(f\"Latency: {latency_stats.mean:.4f}\u00b1{latency_stats.std:.4f}s (mean\u00b1std)\")\n-                if ttft_measurements:\n-                    self.logger.info(f\"TTFT: {ttft_stats.mean:.4f}\u00b1{ttft_stats.std:.4f}s (mean\u00b1std)\")\n-                if tokens_per_sec_measurements:\n-                    self.logger.info(f\"Throughput: {tps_stats.mean:.2f}\u00b1{tps_stats.std:.2f} tokens/sec (mean\u00b1std)\")\n-                if itl_measurements:\n-                    self.logger.info(f\"ITL: {itl_stats.mean:.4f}\u00b1{itl_stats.std:.4f}s/token (mean\u00b1std)\")\n-\n-                # Add note about partial results if some measurements failed\n-                if measurement_failures > 0:\n-                    scenario_results[\"warnings\"] = [f\"Some measurements failed ({measurement_failures} failures)\"]\n-                    self.logger.info(f\"Scenario completed with {measurement_failures} measurement failures\")\n-\n-                # Run scenario teardown callbacks\n-                scenario.teardown(benchmark.model, benchmark.tokenizer, self.logger)\n-\n-                # Cleanup model\n-                benchmark.cleanup_model()\n-\n-                all_results[scenario_name] = scenario_results\n-\n-            except Exception as e:\n-                self.logger.warning(f\"Skipping scenario {scenario_name}: setup failed - {e}\")\n-                import traceback\n-\n-                self.logger.debug(traceback.format_exc())\n-\n-                # Try to clean up if possible\n-                try:\n-                    scenario.teardown(benchmark.model, benchmark.tokenizer, self.logger)\n-                    benchmark.cleanup_model()\n-                except Exception:\n-                    pass\n-                # Skip storing failed scenarios - just continue to the next one\n-            finally:\n-                try:\n-                    scenario.teardown(benchmark.model, benchmark.tokenizer, self.logger)\n-                    benchmark.cleanup_model()\n-                except Exception as cleanup_error:\n-                    self.logger.warning(f\"Cleanup failed for scenario {scenario_name}: {cleanup_error}\")\n-\n-                flush_memory()\n-\n-        return all_results\n-\n-    def save_results(self, model_name: str, results: dict[str, dict[str, Any]]) -> str:\n-        \"\"\"Save benchmark results to JSON file.\"\"\"\n-        # Create model-specific subdirectory\n-        model_dir = os.path.join(self.output_dir, model_name)\n-        os.makedirs(model_dir, exist_ok=True)\n-\n-        # Create filename with timestamp\n-        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n-        filename = f\"{model_name}_benchmark_{timestamp}.json\"\n-        filepath = os.path.join(model_dir, filename)\n-\n-        # Prepare output structure\n-        output_data = {\"model_name\": model_name, \"benchmark_scenarios\": []}\n-\n-        for config_name, config_results in results.items():\n-            scenario = {\n-                \"scenario_name\": config_name,\n-                \"metadata\": config_results[\"metadata\"],\n-                \"measurements\": config_results[\"measurements\"],\n-                \"gpu_metrics\": config_results.get(\"gpu_metrics\", {}),\n-            }\n-            output_data[\"benchmark_scenarios\"].append(scenario)\n-\n-        # Save to JSON file\n-        with open(filepath, \"w\") as f:\n-            json.dump(output_data, f, indent=2, default=str)\n-\n-        self.logger.info(f\"Results saved to {filepath}\")\n-        return filepath"
      },
      {
        "filename": "benchmark_v2/framework/benchmark_config.py",
        "status": "added",
        "additions": 218,
        "deletions": 0,
        "changes": 218,
        "patch": "@@ -0,0 +1,218 @@\n+import hashlib\n+import json\n+import logging\n+from typing import Any, Optional\n+\n+\n+KERNELIZATION_AVAILABLE = False\n+try:\n+    from kernels import Mode, kernelize  # noqa: F401\n+\n+    KERNELIZATION_AVAILABLE = True\n+except ImportError:\n+    pass\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+class BenchmarkConfig:\n+    \"\"\"Configuration for a single benchmark scenario.\"\"\"\n+\n+    def __init__(\n+        self,\n+        warmup_iterations: int = 5,\n+        measurement_iterations: int = 20,\n+        gpu_monitoring: bool = False,  # False by default because it slows down the benchmark by a lot\n+        batch_size: int = 1,\n+        sequence_length: int = 128,\n+        num_tokens_to_generate: int = 128,\n+        attn_implementation: str = \"eager\",\n+        sdpa_backend: Optional[str] = None,\n+        compile_mode: Optional[str] = None,\n+        compile_options: Optional[dict[str, Any]] = None,\n+        kernelize: bool = False,\n+        name: Optional[str] = None,\n+        skip_validity_check: bool = False,\n+    ) -> None:\n+        # Benchmark parameters\n+        self.warmup_iterations = warmup_iterations\n+        self.measurement_iterations = measurement_iterations\n+        self.gpu_monitoring = gpu_monitoring\n+        # Input parameters\n+        self.batch_size = batch_size\n+        self.sequence_length = sequence_length\n+        self.num_tokens_to_generate = num_tokens_to_generate\n+        # Generation parameters\n+        self.attn_implementation = attn_implementation\n+        self.sdpa_backend = sdpa_backend\n+        # Optimization parameters\n+        self.compile_mode = compile_mode\n+        self.compile_options = compile_options if compile_options is not None else {}\n+        self.kernelize = kernelize\n+        # Constant parameters\n+        self.dtype = \"torch.bfloat16\"\n+        self.device = \"cuda\"\n+\n+        self.check_validity(skip_validity_check)\n+        self.name = name if name is not None else self.infer_name()\n+\n+    def check_validity(self, skip_validity_check: bool = False) -> None:\n+        if skip_validity_check:\n+            return\n+        # Flash attention does not support compile mode, so we turn it off # FIXME: it would be better to support it\n+        is_fa = self.attn_implementation == \"flash_attention_2\"\n+        is_fa |= self.attn_implementation == \"sdpa\" and self.sdpa_backend == \"flash_attention\"\n+        if is_fa:\n+            logger.warning(\"Flash attention does not support compile mode. Turning off compile mode.\")\n+            self.compile_mode = None\n+\n+    @property\n+    def hash(self) -> str:\n+        return hashlib.sha256(json.dumps(self.to_dict()).encode()).hexdigest()\n+\n+    def infer_name(self, compact: bool = True) -> str:\n+        \"\"\"Infer a human-readable name for the benchmark config, either compact or verbose.\"\"\"\n+        if compact:\n+            iter_str = f\"w{self.warmup_iterations}_i{self.measurement_iterations}\"\n+            gpu_monitor_str = \"monitored\" if self.gpu_monitoring else \"unmonitored\"\n+            dimensions_str = f\"b{self.batch_size}_s{self.sequence_length}_n{self.num_tokens_to_generate}\"\n+            attn_code = self.attn_implementation\n+            attn_code += f\"_{self.sdpa_backend}\" if self.attn_implementation == \"sdpa\" else \"\"\n+            compile_str = f\"compiled_{self.compile_mode}\" if self.compile_mode is not None else \"uncompiled\"\n+            kernelize_str = \"kernelized\" if self.kernelize else \"unkernelized\"\n+            sep = \"-\"\n+        else:\n+            iter_str = f\"{self.warmup_iterations} warmup, {self.measurement_iterations} iterations\"\n+            gpu_monitor_str = (\"with\" if self.gpu_monitoring else \"no\") + \" GPU monitoring\"\n+            dimensions_str = f\"batch size {self.batch_size}, sequence length {self.sequence_length}, {self.num_tokens_to_generate} generated tokens\"\n+            attn_code = f\"{self.attn_implementation} attention\"\n+            attn_code += f\" with {self.sdpa_backend} backend\" if self.attn_implementation == \"sdpa\" else \"\"\n+            compile_str = \"compiled\" if self.compile_mode is not None else \"not compiled\"\n+            kernelize_str = \"kernelized\" if self.kernelize else \"not kernelized\"\n+            sep = \", \"\n+        return sep.join([iter_str, gpu_monitor_str, dimensions_str, attn_code, compile_str, kernelize_str])\n+\n+    def to_dict(self) -> dict[str, Any]:\n+        return {\n+            \"name\": self.name,\n+            \"warmup_iterations\": self.warmup_iterations,\n+            \"measurement_iterations\": self.measurement_iterations,\n+            \"gpu_monitoring\": self.gpu_monitoring,\n+            \"batch_size\": self.batch_size,\n+            \"sequence_length\": self.sequence_length,\n+            \"num_tokens_to_generate\": self.num_tokens_to_generate,\n+            \"attn_implementation\": self.attn_implementation,\n+            \"sdpa_backend\": self.sdpa_backend,\n+            \"compile_mode\": self.compile_mode,\n+            \"compile_options\": self.compile_options,\n+            \"kernelize\": self.kernelize,\n+        }\n+\n+    @classmethod\n+    def from_dict(cls, data: dict[str, Any], skip_validity_check: bool = False) -> \"BenchmarkConfig\":\n+        return cls(\n+            warmup_iterations=data.get(\"warmup_iterations\", 5),\n+            measurement_iterations=data.get(\"measurement_iterations\", 20),\n+            gpu_monitoring=data.get(\"gpu_monitoring\", False),\n+            batch_size=data.get(\"batch_size\", 1),\n+            sequence_length=data.get(\"sequence_length\", 128),\n+            num_tokens_to_generate=data.get(\"num_tokens_to_generate\", 128),\n+            attn_implementation=data.get(\"attn_implementation\", \"eager\"),\n+            sdpa_backend=data.get(\"sdpa_backend\"),\n+            compile_mode=data.get(\"compile_mode\"),\n+            compile_options=data.get(\"compile_options\"),\n+            kernelize=data.get(\"kernelize\", False),\n+            name=data.get(\"name\"),\n+            skip_validity_check=skip_validity_check,\n+        )\n+\n+\n+def cross_generate_configs(\n+    attn_impl_and_sdpa_backend: list[tuple[str, Optional[str]]],\n+    compiled_mode: list[Optional[str]],\n+    kernelized: list[bool],\n+    warmup_iterations: int = 5,\n+    measurement_iterations: int = 20,\n+    batch_size: int = 1,\n+    sequence_length: int = 128,\n+    num_tokens_to_generate: int = 128,\n+    gpu_monitoring: bool = False,  # this slows down the benchmark by a lot so we disable it by default\n+) -> list[BenchmarkConfig]:\n+    # Create kwargs common to all configs\n+    kwargs = {\n+        \"warmup_iterations\": warmup_iterations,\n+        \"measurement_iterations\": measurement_iterations,\n+        \"batch_size\": batch_size,\n+        \"sequence_length\": sequence_length,\n+        \"num_tokens_to_generate\": num_tokens_to_generate,\n+        \"gpu_monitoring\": gpu_monitoring,\n+    }\n+    # Cross-generate all combinations of attn_implementation, compiled_mode, and kernelized\n+    configs = []\n+    for attn_implementation, sdpa_backend in list(dict.fromkeys(attn_impl_and_sdpa_backend)):\n+        for cm in list(dict.fromkeys(compiled_mode)):\n+            for kernelize_on in list(dict.fromkeys(kernelized)):\n+                config = BenchmarkConfig(\n+                    attn_implementation=attn_implementation,\n+                    sdpa_backend=sdpa_backend,\n+                    compile_mode=cm,\n+                    kernelize=kernelize_on,\n+                    **kwargs,\n+                )\n+                configs.append(config)\n+    return configs\n+\n+\n+def generate_all_configs(\n+    warmup_iterations: int = 5,\n+    measurement_iterations: int = 20,\n+    batch_size: int = 1,\n+    sequence_length: int = 128,\n+    num_tokens_to_generate: int = 128,\n+    gpu_monitoring: bool = False,\n+) -> list[BenchmarkConfig]:\n+    all_attn_implementations = [\n+        (\"flash_attention_2\", None),\n+        (\"eager\", None),\n+        (\"sdpa\", \"math\"),\n+        (\"sdpa\", \"flash_attention\"),\n+        (\"flex_attention\", None),\n+    ]\n+    return cross_generate_configs(\n+        attn_impl_and_sdpa_backend=all_attn_implementations,\n+        compiled_mode=[None, \"default\", \"reduce-overhead\", \"max-autotune\", \"max-autotune-no-cudagraphs\"],\n+        kernelized=[False, KERNELIZATION_AVAILABLE],\n+        warmup_iterations=warmup_iterations,\n+        measurement_iterations=measurement_iterations,\n+        batch_size=batch_size,\n+        sequence_length=sequence_length,\n+        num_tokens_to_generate=num_tokens_to_generate,\n+        gpu_monitoring=gpu_monitoring,\n+    )\n+\n+\n+def generate_default_configs(\n+    warmup_iterations: int = 5,\n+    measurement_iterations: int = 20,\n+    batch_size: int = 1,\n+    sequence_length: int = 128,\n+    num_tokens_to_generate: int = 128,\n+    gpu_monitoring: bool = False,\n+) -> list[BenchmarkConfig]:\n+    all_attn_implementations = [\n+        (\"flash_attention_2\", None),\n+        (\"eager\", None),\n+        (\"sdpa\", \"math\"),\n+        (\"sdpa\", \"flash_attention\"),  # note: this one can fail with compile because of attn mask\n+    ]\n+    return cross_generate_configs(\n+        attn_impl_and_sdpa_backend=all_attn_implementations,\n+        compiled_mode=[None, \"max-autotune\"],\n+        kernelized=[False, KERNELIZATION_AVAILABLE],\n+        warmup_iterations=warmup_iterations,\n+        measurement_iterations=measurement_iterations,\n+        batch_size=batch_size,\n+        sequence_length=sequence_length,\n+        num_tokens_to_generate=num_tokens_to_generate,\n+        gpu_monitoring=gpu_monitoring,\n+    )"
      },
      {
        "filename": "benchmark_v2/framework/benchmark_runner.py",
        "status": "added",
        "additions": 388,
        "deletions": 0,
        "changes": 388,
        "patch": "@@ -0,0 +1,388 @@\n+import gc\n+import json\n+import logging\n+import os\n+import pathlib\n+import re\n+import time\n+from contextlib import nullcontext\n+from datetime import datetime\n+from queue import Queue\n+from typing import Any, Optional\n+\n+import torch\n+from tqdm import trange\n+\n+from transformers import (\n+    AutoModelForCausalLM,\n+    AutoTokenizer,\n+    CompileConfig,\n+    GenerationConfig,\n+    GenerationMixin,\n+)\n+from transformers.generation.streamers import BaseStreamer\n+\n+from .benchmark_config import BenchmarkConfig\n+from .data_classes import BenchmarkMetadata, BenchmarkResult, GPURawMetrics, pretty_print_dict\n+from .hardware_metrics import GPUMonitor\n+\n+\n+try:\n+    from kernels import Mode, kernelize  # noqa: F401\n+except ImportError:\n+    kernelize = None\n+    Mode = None\n+\n+\n+DEFAULT_PROMPT = \"\\n\".join([\n+    \"The French Revolution was a period of political and societal change in France that began with the Estates General of 1789 and ended with the Coup of 18 Brumaire on 9 November 1799.\",\n+    \"Many of the revolution's ideas are considered fundamental principles of liberal democracy, and its values remain central to modern French political discourse.\",\n+    \"It was caused by a combination of social, political, and economic factors which the existing regime proved unable to manage.\",\n+    \"Financial crisis and widespread social distress led to the convocation of the Estates General in May 1789, its first meeting since 1614.\",\n+    \"The representatives of the Third Estate broke away and re-constituted themselves as a National Assembly in June.\",\n+    \"The Storming of the Bastille in Paris on 14 July led to a series of radical measures by the Assembly, including the abolition of feudalism, state control over the Catholic Church in France, and issuing the Declaration of the Rights of Man and of the Citizen.\",\n+    \"The next three years were dominated by a struggle for political control.\",\n+    \"King Louis XVI's attempted flight to Varennes in June 1791 further discredited the monarchy, and military defeats after the outbreak of the French Revolutionary Wars in April 1792 led to the insurrection of 10 August 1792.\",\n+    \"As a result, the monarchy was replaced by the French First Republic in September, followed by the execution of Louis XVI himself in January 1793.\",\n+    \"After another revolt in June 1793, the constitution was suspended, and political power passed from the National Convention to the Committee of Public Safety, dominated by radical Jacobins led by Maximilien Robespierre.\",\n+    \"About 16,000 people were sentenced by the Revolutionary Tribunal and executed in the Reign of Terror, which ended in July 1794 with the Thermidorian Reaction.\",\n+    \"Weakened by external threats and internal opposition, the Committee of Public Safety was replaced in November 1795 by the Directory.\",\n+    \"Its instability ended in the coup of 18 Brumaire and the establishment of the Consulate, with Napoleon Bonaparte as First Consul.\",\n+])  # fmt: skip\n+\n+\n+def compact_json_numeric_arrays(data: dict):\n+    # Match arrays that contain only numbers (ints/floats), whitespace, commas, and newlines\n+    pattern = r\"\\[\\s*\\n\\s*((?:\\d+(?:\\.\\d+)?\\s*,\\s*)*\\d+(?:\\.\\d+)?)\\s*\\n\\s*\\]\"\n+\n+    def replace_numeric_array(match):\n+        # Get the array content\n+        content = match.group(1)\n+        # Remove extra whitespace but keep commas\n+        compact_content = re.sub(r\"\\s+\", \" \", content).strip()\n+        return f\"[{compact_content}]\"\n+\n+    return re.sub(pattern, replace_numeric_array, json.dumps(data, indent=4, default=str), flags=re.DOTALL)\n+\n+\n+def get_git_revision() -> str:\n+    base_path = pathlib.Path(__file__).parent.parent.parent\n+    git_dir = base_path / \".git\"\n+    with (git_dir / \"HEAD\").open(\"r\") as head:\n+        ref = head.readline().split(\" \")[-1].strip()\n+    with (git_dir / ref).open(\"r\") as git_hash:\n+        return git_hash.readline().strip()\n+\n+\n+def get_sdpa_backend(backend_name: Optional[str]) -> Optional[torch.nn.attention.SDPBackend]:\n+    \"\"\"Get the SDPA backend enum from string name.\"\"\"\n+    if backend_name is None:\n+        return None\n+\n+    try:\n+        backend_map = {\n+            \"math\": torch.nn.attention.SDPBackend.MATH,\n+            \"flash_attention\": torch.nn.attention.SDPBackend.FLASH_ATTENTION,\n+            \"efficient_attention\": torch.nn.attention.SDPBackend.EFFICIENT_ATTENTION,\n+            \"cudnn_attention\": torch.nn.attention.SDPBackend.CUDNN_ATTENTION,\n+        }\n+        return backend_map.get(backend_name.lower())\n+    except AttributeError:\n+        # torch.nn.attention.SDPBackend not available in older torch versions\n+        return None\n+\n+\n+def flush_memory():\n+    \"\"\"Flush GPU memory and run garbage collection.\"\"\"\n+    gc.collect()\n+    # Dynamo resets\n+    torch._dynamo.reset()\n+    torch._dynamo.reset_code_caches()\n+    if hasattr(torch._inductor, \"codecache\"):\n+        # Clear FX graph cache\n+        if hasattr(torch._inductor.codecache, \"FxGraphCache\"):\n+            torch._inductor.codecache.FxGraphCache.clear()\n+        # Clear PyCodeCache\n+        if hasattr(torch._inductor.codecache, \"PyCodeCache\"):\n+            torch._inductor.codecache.PyCodeCache.cache_clear()\n+        # Clear TritonFuture cache (for async compilation)\n+        if hasattr(torch._inductor.codecache, \"TritonFuture\"):\n+            if hasattr(torch._inductor.codecache.TritonFuture, \"_compile_cache\"):\n+                torch._inductor.codecache.TritonFuture._compile_cache.clear()\n+    # Clear CUDA cache\n+    if torch.cuda.is_available():\n+        torch.cuda.empty_cache()\n+        torch.cuda.reset_max_memory_allocated()\n+        torch.cuda.reset_peak_memory_stats()\n+        torch.cuda.synchronize()\n+    gc.collect()\n+\n+\n+class BenchmarkStreamer(BaseStreamer):\n+    def __init__(self, **kwargs) -> None:\n+        self.timestamps = []\n+        self.text_queue = Queue()\n+\n+    def put(self, value):\n+        \"\"\"Receives tokens and logs the timestamp of the generation.\"\"\"\n+        self.timestamps.append(time.perf_counter())\n+\n+    def end(self):\n+        self.timestamps.append(time.perf_counter())\n+\n+    def __iter__(self):\n+        return self\n+\n+    def __next__(self):\n+        value = self.text_queue.get(timeout=self.timeout)\n+        if value == self.stop_signal:\n+            raise StopIteration()\n+        else:\n+            return value\n+\n+\n+class BenchmarkRunner:\n+    \"\"\"Main benchmark runner that coordinates benchmark execution.\"\"\"\n+\n+    def __init__(\n+        self, logger: logging.Logger, output_dir: str = \"benchmark_results\", commit_id: Optional[str] = None\n+    ) -> None:\n+        # Those stay constant for the whole run\n+        self.logger = logger\n+        self.output_dir = output_dir\n+        self.commit_id = get_git_revision() if commit_id is None else commit_id\n+        os.makedirs(self.output_dir, exist_ok=True)\n+        self.profile_dir = None\n+        # Attributes that are reset for each model\n+        self._setup_for = \"\"\n+        # Attributes that are reset for each run\n+        self.model: Optional[GenerationMixin] = None\n+\n+    def cleanup(self) -> None:\n+        del self.model\n+        self.model = None\n+        flush_memory()\n+\n+    def setup_one_run(self, model_id: str, config: BenchmarkConfig) -> None:\n+        # Some attributes only need to be set once per model\n+        if self._setup_for != model_id:\n+            self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n+            # We set the EOS token to the padding token for open-ended generation\n+            self.tokenizer.eos_token = self.tokenizer.pad_token\n+            self._setup_for = model_id\n+\n+        # Prepare inputs\n+        self.inputs = self.tokenizer(\n+            [DEFAULT_PROMPT for _ in range(config.batch_size)],\n+            return_tensors=\"pt\",\n+            max_length=config.sequence_length,\n+            truncation=True,\n+            return_attention_mask=True,\n+        ).to(config.device)\n+        self.inputs[\"use_cache\"] = True\n+\n+        # Prepare generation config\n+        gen_config = GenerationConfig(\n+            do_sample=False, top_p=1.0, temperature=1.0, max_new_tokens=config.num_tokens_to_generate\n+        )\n+\n+        # Prepare compile config\n+        if config.compile_mode is not None:\n+            gen_config.compile_config = CompileConfig(mode=config.compile_mode, options=config.compile_options)\n+            gen_config.cache_implementation = \"static\"\n+\n+        # Load model\n+        self.logger.debug(f\"Loading model {model_id} on device {config.device}...\")\n+        dtype = getattr(torch, config.dtype.removeprefix(\"torch.\"))\n+        self.model = AutoModelForCausalLM.from_pretrained(\n+            model_id, dtype=dtype, attn_implementation=config.attn_implementation, generation_config=gen_config\n+        )\n+        self.model = self.model.eval().to(config.device)\n+\n+        # Kernelize the model if needed\n+        if config.kernelize:\n+            self.model = kernelize(self.model, mode=Mode.INFERENCE)\n+\n+    def run_one_benchmark(self, model_id: str, config: BenchmarkConfig, num_tokens_to_profile: int = 0) -> None:\n+        sdpa_ctx = nullcontext()\n+        if config.attn_implementation == \"sdpa\":\n+            sdpa_backend = get_sdpa_backend(config.sdpa_backend)\n+            sdpa_ctx = torch.nn.attention.sdpa_kernel(sdpa_backend)\n+\n+        with sdpa_ctx, torch.no_grad():\n+            self.logger.info(f\"Running benchmark scenario: {config.name}\")\n+\n+            # Quick validation: try one measurement first to see if this scenario works\n+            flush_memory()\n+            e2e_latency, token_generation_times, decoded_output, gpu_metrics = self.time_generate(\n+                max_new_tokens=1, gpu_monitor=None\n+            )\n+            if e2e_latency < 0:\n+                self.logger.warning(f\"Skipping config {config.name}: {e2e_latency = } (no GPU monitoring)\")\n+                return None\n+\n+            # Warmup runs\n+            self.logger.info(f\"Warming up with {config.warmup_iterations} iterations...\")\n+            for _ in trange(config.warmup_iterations):\n+                _ = self.time_generate(max_new_tokens=config.num_tokens_to_generate)\n+            self.logger.info(\"Warmup over.\")\n+\n+            # Measurement runs\n+            result = BenchmarkResult()\n+            self.logger.info(f\"Benchmarking with {config.measurement_iterations} iterations.\")\n+            for _ in trange(config.measurement_iterations):\n+                e2e_latency, token_generation_times, decoded_output, gpu_metrics = self.time_generate(\n+                    max_new_tokens=config.num_tokens_to_generate,\n+                    gpu_monitor=(GPUMonitor(logger=self.logger) if config.gpu_monitoring else None),\n+                )\n+                result.accumulate(e2e_latency, token_generation_times, decoded_output, gpu_metrics)\n+            self.logger.info(\"Benchmarking done. Cleaning up.\")\n+\n+            # Profile if needed\n+            if num_tokens_to_profile > 0:\n+                self.profile_generate(num_tokens_to_profile, config.name)\n+\n+            return {\n+                \"metadata\": BenchmarkMetadata(model_id=model_id, commit_id=self.commit_id),\n+                \"measurements\": result,\n+                \"config\": config,\n+            }\n+\n+    def time_generate(\n+        self,\n+        max_new_tokens: int,\n+        gpu_monitor: Optional[GPUMonitor] = None,\n+    ) -> tuple[float, list[float], str, Optional[GPURawMetrics]]:\n+        \"\"\"Time the latency of a call to model.generate() with the given (inputs) and (max_new_tokens).\"\"\"\n+        # Prepare gpu monitoring if needed\n+        if gpu_monitor is not None:\n+            gpu_monitor.start()\n+        # Prepare streamer\n+        streamer = BenchmarkStreamer()\n+        # Generate and time\n+        wall_time_0 = time.perf_counter()\n+        outputs = self.model.generate(\n+            **self.inputs,\n+            max_new_tokens=max_new_tokens,\n+            streamer=streamer,\n+        )\n+        wall_time_1 = time.perf_counter()\n+        # Stop gpu monitoring if needed\n+        gpu_metrics = gpu_monitor.stop_and_collect() if gpu_monitor is not None else None\n+        # Check if generation had the right number of tokens\n+        input_tokens = self.inputs[\"input_ids\"].size(-1)\n+        batch_size, output_tokens = outputs.shape\n+        new_tokens = output_tokens - input_tokens\n+        if new_tokens != max_new_tokens:\n+            raise RuntimeError(f\"Generated {new_tokens} tokens, expected {max_new_tokens}\")\n+        # Decode outputs\n+        decoded_output = self.tokenizer.decode(outputs[0, input_tokens:], skip_special_tokens=True)\n+        # Compute intermediate quantities\n+        e2e_latency = wall_time_1 - wall_time_0\n+        token_generation_times = [t - wall_time_0 for t in streamer.timestamps[1:]]\n+        return e2e_latency, token_generation_times, decoded_output, gpu_metrics\n+\n+    def profile_generate(self, num_tokens_to_profile: int, config_name: str) -> None:\n+        \"\"\"Profile the latency of a call to model.generate() with the given (inputs) and (max_new_tokens).\"\"\"\n+        profiler = torch.profiler.profile(\n+            activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],\n+            record_shapes=True,\n+        )\n+        with profiler as prof:\n+            _ = self.model.generate(\n+                **self.inputs,\n+                max_new_tokens=num_tokens_to_profile,\n+            )\n+        if self.profile_dir is None:\n+            self.profile_dir = self.output_dir + \"_profiles\"\n+            os.makedirs(self.profile_dir, exist_ok=True)\n+        prof.export_chrome_trace(f\"{self.profile_dir}/{config_name}.json\")\n+\n+    def run_benchmarks(\n+        self,\n+        model_id: str,\n+        benchmark_configs: list[BenchmarkConfig],\n+        num_tokens_to_profile: int = 0,\n+        pretty_print_summary: bool = True,\n+    ) -> dict[str, Any]:\n+        all_results = {}\n+        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n+        start_time = time.perf_counter()\n+\n+        n_configs = len(benchmark_configs)\n+        for i, config in enumerate(benchmark_configs):\n+            # Handle SDPA backend if not determined by the config (needs to be done before skipping duplicates)\n+            if config.attn_implementation == \"sdpa\" and config.sdpa_backend is None:\n+                default_backend = \"flash_attention\"  # FIXME: torch has a _cur_sdpa_kernel_backends but it fails\n+                self.logger.warning(f\"No SDPA backend provided, using {default_backend} instead.\")\n+                config.sdpa_backend = default_backend\n+\n+            # Skip if already run\n+            if config.hash in all_results:\n+                self.logger.info(f\"Skipping duplicate config {config.name} for model {model_id} ({i + 1}/{n_configs})\")\n+                continue\n+\n+            # Otherwise, run the benchmark\n+            self.setup_one_run(model_id, config)\n+            self.logger.info(\n+                f\"Running benchmark of model {model_id} with scenario: {config.name} ({i + 1}/{n_configs})\"\n+            )\n+\n+            # Launch benchmark in a try/except block to avoid stopping the whole run if one benchmark fails\n+            try:\n+                results = self.run_one_benchmark(model_id, config, num_tokens_to_profile)\n+                if results is not None:\n+                    all_results[config.hash] = results\n+\n+            except Exception as e:\n+                self.logger.error(f\"Error running with scenario: {config.name}:\\n{repr(e)}\")\n+            # Cleanup model and save results\n+            self.cleanup()\n+            self.save_results(model_id, all_results, timestamp=timestamp)\n+\n+        if pretty_print_summary:\n+            print()\n+            print(\"=\" * 100)\n+            print(f\"Finished benchmarks in {time.perf_counter() - start_time:.2f} seconds\")\n+            print(f\"Total number of benchmarks: {len(all_results)}\")\n+            if len(all_results) > 0:\n+                print(\"First run metadata:\")\n+                first_key = list(all_results.keys())[0]\n+                first_metadata = all_results[first_key][\"metadata\"].to_dict()\n+                hardware_info = first_metadata.pop(\"hardware_info\")\n+                pretty_print_dict(first_metadata | hardware_info, tabs=1)\n+            for value in all_results.values():\n+                print(\"=\" * 100)\n+                print(f\"Config: {value['config'].infer_name(compact=False)}\\n\")\n+                value[\"measurements\"].pprint(tabs=1)\n+            print(\"=\" * 100)\n+\n+        return all_results\n+\n+    def save_results(self, model_name: str, results: dict, timestamp: str = \"\") -> str:\n+        \"\"\"Save benchmark results to JSON file.\"\"\"\n+        # Create model-specific subdirectory\n+        model_name = model_name.replace(\"/\", \"_\")\n+        model_dir = os.path.join(self.output_dir, model_name)\n+        os.makedirs(model_dir, exist_ok=True)\n+\n+        # Create filename with timestamp\n+        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\") if not timestamp else timestamp\n+        filename = f\"{model_name}_benchmark_{timestamp}.json\"\n+        filepath = os.path.join(model_dir, filename)\n+\n+        # Convert results to dict\n+        converted_results = {}\n+        for cfg_hash in results.keys():\n+            converted_results[cfg_hash] = {\n+                \"metadata\": results[cfg_hash][\"metadata\"].to_dict(),\n+                \"measurements\": results[cfg_hash][\"measurements\"].to_dict(),\n+                \"config\": results[cfg_hash][\"config\"].to_dict(),\n+            }\n+\n+        # Save to JSON file\n+        with open(filepath, \"w\") as f:\n+            f.write(compact_json_numeric_arrays(converted_results))\n+\n+        self.logger.info(f\"Results saved to {filepath}\")\n+        return filepath"
      },
      {
        "filename": "benchmark_v2/framework/data_classes.py",
        "status": "added",
        "additions": 152,
        "deletions": 0,
        "changes": 152,
        "patch": "@@ -0,0 +1,152 @@\n+from dataclasses import dataclass\n+from datetime import datetime\n+from typing import Any, Optional, Union\n+\n+import numpy as np\n+\n+from .hardware_metrics import GPURawMetrics, HardwareInfo\n+\n+\n+def compute_basic_statistics(measurements: list[float]) -> dict[str, float]:\n+    return {\n+        \"avg\": np.mean(measurements),\n+        \"std\": np.std(measurements),\n+        \"min\": np.min(measurements),\n+        \"med\": np.median(measurements),\n+        \"max\": np.max(measurements),\n+        \"p95\": np.percentile(measurements, 95),\n+    }\n+\n+\n+def add_unit_to_duration(stats: dict[str, float]) -> dict[str, str]:\n+    for key in list(stats.keys()):\n+        value = stats[key]\n+        if value > 3600:\n+            stats[key] = f\"{(value / 3600):.2f}hr\"\n+        elif value > 60:\n+            stats[key] = f\"{(value / 60):.2f}min\"\n+        elif value > 1:\n+            stats[key] = f\"{value:.2f}s\"\n+        elif value > 1e-3:\n+            stats[key] = f\"{(value * 1e3):.2f}ms\"\n+        elif value > 1e-6:\n+            stats[key] = f\"{(value * 1e6):.2f}us\"\n+        else:\n+            stats[key] = f\"{(value * 1e9):.2f}ns\"\n+    return stats\n+\n+\n+def equalize_lengths_and_collate(stats: list[dict[str, str]]) -> list[str]:\n+    keys = [\"avg\", \"std\", \"min\", \"med\", \"max\", \"p95\"]\n+    for key in keys:\n+        max_length = max(len(stat[key]) for stat in stats)\n+        for stat in stats:\n+            stat[key] = stat[key].ljust(max_length, \" \")\n+    return [\" \".join([f\"{key}={stat[key]}\" for key in keys]) for stat in stats]\n+\n+\n+def pretty_print_dict(data: dict[str, Any], tabs: int = 0) -> None:\n+    max_key_length = max([len(key) for key in data.keys()])\n+    for key, value in data.items():\n+        tabs_str = \"  \" * tabs\n+        padded_key = key.ljust(max_key_length + 1, \".\")\n+        print(f\"{tabs_str}{padded_key}: {value}\")\n+\n+\n+@dataclass\n+class BenchmarkMetadata:\n+    \"\"\"Metadata collected for each benchmark run.\"\"\"\n+\n+    model_id: str\n+    timestamp: str\n+    commit_id: str\n+    hardware_info: HardwareInfo\n+\n+    def __init__(self, model_id: str, commit_id: str):\n+        self.model_id = model_id\n+        self.timestamp = datetime.utcnow().isoformat()\n+        self.commit_id = commit_id\n+        self.hardware_info = HardwareInfo()\n+\n+    def to_dict(self) -> dict[str, Any]:\n+        return {\n+            \"timestamp\": self.timestamp,\n+            \"commit_id\": self.commit_id,\n+            \"hardware_info\": self.hardware_info.to_dict(),\n+        }\n+\n+\n+class BenchmarkResult:\n+    \"\"\"Result from a series of benchmark runs.\"\"\"\n+\n+    def __init__(self) -> None:\n+        self.e2e_latency = []\n+        self.token_generation_times = []  # time at which each token was generated (relative to start of the generation)\n+        self.decoded_outputs = []\n+        self.gpu_metrics = []\n+\n+    def accumulate(\n+        self,\n+        e2e_latency: float,\n+        token_generation_times: list[float],\n+        decoded_output: str,\n+        gpu_metrics: Optional[GPURawMetrics],\n+    ) -> None:\n+        self.e2e_latency.append(e2e_latency)\n+        self.token_generation_times.append(token_generation_times)\n+        self.decoded_outputs.append(decoded_output)\n+        self.gpu_metrics.append(gpu_metrics)\n+\n+    def to_dict(self) -> dict[str, Union[None, int, float]]:\n+        # Save GPU metrics as None if it contains only None values\n+        if all(gm is None for gm in self.gpu_metrics):\n+            gpu_metrics = None\n+        else:\n+            gpu_metrics = [gm.to_dict() for gm in self.gpu_metrics]\n+        return {\n+            \"e2e_latency\": self.e2e_latency,\n+            \"token_generation_times\": self.token_generation_times,\n+            \"decoded_outputs\": self.decoded_outputs,\n+            \"gpu_metrics\": gpu_metrics,\n+        }\n+\n+    @classmethod\n+    def from_dict(cls, data: dict[str, Union[None, int, float]]) -> \"BenchmarkResult\":\n+        # Handle GPU metrics, which is saved as None if it contains only None values\n+        if data[\"gpu_metrics\"] is None:\n+            gpu_metrics = [None for _ in range(len(data[\"e2e_latency\"]))]\n+        else:\n+            gpu_metrics = [GPURawMetrics.from_dict(gm) for gm in data[\"gpu_metrics\"]]\n+        # Create a new instance and accumulate the data\n+        new_instance = cls()\n+        for i in range(len(data[\"e2e_latency\"])):\n+            new_instance.accumulate(\n+                e2e_latency=data[\"e2e_latency\"][i],\n+                token_generation_times=data[\"token_generation_times\"][i],\n+                decoded_output=data[\"decoded_output\"][i],\n+                gpu_metrics=gpu_metrics[i],\n+            )\n+        return new_instance\n+\n+    def get_measured_ttft(self) -> list[float]:\n+        return [dt[0] for dt in self.token_generation_times if len(dt) > 0]\n+\n+    def get_measured_itl(self) -> list[float]:\n+        return [(dt[-1] - dt[0]) / (len(dt) - 1) for dt in self.token_generation_times if len(dt) > 1]\n+\n+    def pprint(self, tabs: int = 0) -> None:\n+        collated_stats = equalize_lengths_and_collate(\n+            [\n+                add_unit_to_duration(compute_basic_statistics(self.e2e_latency)),\n+                add_unit_to_duration(compute_basic_statistics(self.get_measured_ttft())),\n+                add_unit_to_duration(compute_basic_statistics(self.get_measured_itl())),\n+            ]\n+        )\n+        pretty_print_dict(\n+            {\n+                \"E2E Latency\": collated_stats[0],\n+                \"Time to First Token\": collated_stats[1],\n+                \"Inter-Token Latency\": collated_stats[2],\n+            },\n+            tabs=tabs,\n+        )"
      },
      {
        "filename": "benchmark_v2/framework/hardware_metrics.py",
        "status": "added",
        "additions": 172,
        "deletions": 0,
        "changes": 172,
        "patch": "@@ -0,0 +1,172 @@\n+import json\n+import logging\n+import subprocess\n+import sys\n+import threading\n+import time\n+from dataclasses import dataclass\n+from enum import Enum\n+from logging import Logger\n+from typing import Optional, Union\n+\n+import gpustat\n+import psutil\n+import torch\n+\n+\n+# Data class to hold the hardware information\n+def get_device_name_and_memory_total() -> tuple[str, float]:\n+    \"\"\"Returns the name and memory total of GPU 0.\"\"\"\n+    device_name = torch.cuda.get_device_properties(0).name\n+    device_memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n+    return device_name, device_memory_total\n+\n+\n+class HardwareInfo:\n+    \"\"\"A class to hold information about the hardware.\"\"\"\n+\n+    def __init__(self) -> None:\n+        # Retrieve GPU stats\n+        try:\n+            self.gpu_name, self.gpu_memory_total_gb = get_device_name_and_memory_total()\n+        except Exception:\n+            self.gpu_name, self.gpu_memory_total_gb = None, None\n+        # Retrieve python, torch and CUDA version\n+        self.python_version = f\"{sys.version.split()[0]}\"\n+        self.torch_version = torch.__version__\n+        if hasattr(torch, \"cuda\") and torch.cuda.is_available():\n+            self.cuda_version = torch.version.cuda\n+        else:\n+            self.cuda_version = None\n+        # Retrieve general hardware information\n+        self.cpu_count = psutil.cpu_count()\n+        self.memory_total_mb = int(psutil.virtual_memory().total / (1024 * 1024))\n+\n+    def to_dict(self) -> dict[str, Union[None, int, float, str]]:\n+        return {\n+            \"gpu_name\": self.gpu_name,\n+            \"gpu_memory_total_gb\": self.gpu_memory_total_gb,\n+            \"python_version\": self.python_version,\n+            \"torch_version\": self.torch_version,\n+        }\n+\n+\n+# Functions to get information about the GPU\n+def get_amd_gpu_stats() -> tuple[int, float]:\n+    \"\"\"Returns the utilization and memory used of an AMD GPU, both in percent\"\"\"\n+    rocm_smi_output = subprocess.check_output([\"rocm-smi\", \"--json\", \"--showuse\", \"--showmeminfo\", \"VRAM\"])\n+    gpu_stats = json.loads(rocm_smi_output.decode(\"utf-8\"))\n+    gpu_stats = [\n+        (card_id, stats[\"GPU use (%)\"], stats[\"VRAM Total Used Memory (B)\"]) for card_id, stats in gpu_stats.items()\n+    ]\n+    gpu_stats.sort(key=lambda x: x[1], reverse=True)\n+    return int(gpu_stats[0][1]), float(gpu_stats[0][2]) / 1024**3\n+\n+\n+def get_nvidia_gpu_stats() -> tuple[int, float]:\n+    \"\"\"Returns the utilization and memory used of an NVIDIA GPU, both in percent\"\"\"\n+    gpu_stats = gpustat.GPUStatCollection.new_query()\n+    gpu_stats = gpu_stats[0]\n+    return int(gpu_stats[\"utilization.gpu\"]), float(gpu_stats[\"memory.used\"]) / 1024**3\n+\n+\n+class GPUStatsCollector:\n+    \"\"\"A class to get statistics about the GPU. It serves as a wrapper that holds the GPU total memory and its name,\n+    which is used to call the right function to get the utilization and memory used.\"\"\"\n+\n+    def __init__(self) -> None:\n+        self.device_name, self.device_memory_total = get_device_name_and_memory_total()\n+        # Monkey patch the get_utilization_and_memory_used method based on the GPU type\n+        if \"amd\" in self.device_name.lower():\n+            self.get_utilization_and_memory_used = get_amd_gpu_stats\n+        elif \"nvidia\" in self.device_name.lower():\n+            self.get_utilization_and_memory_used = get_nvidia_gpu_stats\n+        else:\n+            raise RuntimeError(f\"Unsupported GPU: {self.device_name}\")\n+\n+    def get_measurements(self) -> tuple[int, float]:\n+        \"\"\"Get the utilization and memory used of the GPU, both in percent\"\"\"\n+        raise NotImplementedError(\"This method is meant to be monkey patched during __init__\")\n+\n+\n+# Simple data classes to hold the raw GPU metrics\n+class GPUMonitoringStatus(Enum):\n+    \"\"\"Status of GPU monitoring.\"\"\"\n+\n+    SUCCESS = \"success\"\n+    FAILED = \"failed\"\n+    NO_GPUS_AVAILABLE = \"no_gpus_available\"\n+    NO_SAMPLES_COLLECTED = \"no_samples_collected\"\n+\n+\n+@dataclass\n+class GPURawMetrics:\n+    \"\"\"Raw values for GPU utilization and memory used.\"\"\"\n+\n+    utilization: list[float]  # in percent\n+    memory_used: list[float]  # in GB\n+    timestamps: list[float]  # in seconds\n+    timestamp_0: float  # in seconds\n+    monitoring_status: GPUMonitoringStatus\n+\n+    def to_dict(self) -> dict[str, Union[None, int, float, str]]:\n+        return {\n+            \"utilization\": self.utilization,\n+            \"memory_used\": self.memory_used,\n+            \"timestamps\": self.timestamps,\n+            \"timestamp_0\": self.timestamp_0,\n+            \"monitoring_status\": self.monitoring_status.value,\n+        }\n+\n+\n+# Main class, used to monitor the GPU utilization during benchmark execution\n+class GPUMonitor:\n+    \"\"\"Monitor GPU utilization during benchmark execution.\"\"\"\n+\n+    def __init__(self, sample_interval_sec: float = 0.1, logger: Optional[Logger] = None):\n+        self.sample_interval_sec = sample_interval_sec\n+        self.logger = logger if logger is not None else logging.getLogger(__name__)\n+\n+        self.num_available_gpus = torch.cuda.device_count()\n+        if self.num_available_gpus == 0:\n+            raise RuntimeError(\"No GPUs detected by torch.cuda.device_count().\")\n+        self.gpu_stats_getter = GPUStatsCollector()\n+\n+    def start(self):\n+        \"\"\"Start monitoring GPU metrics.\"\"\"\n+        # Clear the stop event to enable monitoring\n+        self.stop_event = threading.Event()\n+        self.gpu_utilization = []\n+        self.gpu_memory_used = []\n+        self.timestamps = []\n+        self.thread = threading.Thread(target=self._monitor_loop)\n+        self.thread.start()\n+        self.logger.debug(\"GPU monitoring started\")\n+\n+    def stop_and_collect(self) -> GPURawMetrics:\n+        \"\"\"Stop monitoring and return collected metrics.\"\"\"\n+        self.stop_event.set()\n+        self.thread.join()\n+        if self.gpu_utilization:\n+            timestamp_0 = self.timestamps[0]\n+            metrics = GPURawMetrics(\n+                utilization=self.gpu_utilization,\n+                memory_used=self.gpu_memory_used,\n+                timestamps=[t - timestamp_0 for t in self.timestamps],\n+                timestamp_0=timestamp_0,\n+                monitoring_status=GPUMonitoringStatus.SUCCESS,\n+            )\n+            self.logger.debug(f\"GPU monitoring completed: {len(self.gpu_utilization)} samples collected\")\n+        else:\n+            metrics = GPURawMetrics(monitoring_status=GPUMonitoringStatus.NO_SAMPLES_COLLECTED)\n+        return metrics\n+\n+    def _monitor_loop(self):\n+        \"\"\"Background monitoring loop using threading.Event for communication.\"\"\"\n+        while not self.stop_event.is_set():\n+            utilization, memory_used = self.gpu_stats_getter.get_utilization_and_memory_used()\n+            self.gpu_utilization.append(utilization)\n+            self.gpu_memory_used.append(memory_used)\n+            self.timestamps.append(time.time())\n+            if self.stop_event.wait(timeout=self.sample_interval_sec):\n+                break"
      },
      {
        "filename": "benchmark_v2/run_benchmarks.py",
        "status": "modified",
        "additions": 68,
        "deletions": 452,
        "changes": 520,
        "patch": "@@ -19,477 +19,93 @@\n \"\"\"\n \n import argparse\n-import importlib.util\n-import json\n import logging\n-import os\n+import random\n import sys\n import uuid\n-from datetime import datetime\n-from pathlib import Path\n-from typing import Any, Optional\n \n+from framework.benchmark_config import BenchmarkConfig, generate_all_configs\n+from framework.benchmark_runner import BenchmarkRunner\n \n-def setup_logging(log_level: str = \"INFO\", enable_file_logging: bool = False) -> logging.Logger:\n-    \"\"\"Setup logging configuration.\"\"\"\n-    numeric_level = getattr(logging, log_level.upper(), None)\n-    if not isinstance(numeric_level, int):\n-        raise ValueError(f\"Invalid log level: {log_level}\")\n-\n-    handlers = [logging.StreamHandler(sys.stdout)]\n-\n-    if enable_file_logging:\n-        handlers.append(logging.FileHandler(f\"benchmark_run_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"))\n-\n-    logging.basicConfig(\n-        level=numeric_level, format=\"[%(levelname)s - %(asctime)s] %(name)s: %(message)s\", handlers=handlers\n-    )\n-\n-    return logging.getLogger(__name__)\n-\n-\n-def discover_benchmarks(benches_dir: str) -> list[dict[str, Any]]:\n-    \"\"\"\n-    Discover all benchmark modules in the benches directory.\n-\n-    Returns:\n-        List of dictionaries containing benchmark module info\n-    \"\"\"\n-    benchmarks = []\n-    benches_path = Path(benches_dir)\n-\n-    if not benches_path.exists():\n-        raise FileNotFoundError(f\"Benches directory not found: {benches_dir}\")\n-\n-    for py_file in benches_path.glob(\"*.py\"):\n-        if py_file.name.startswith(\"__\"):\n-            continue\n-\n-        module_name = py_file.stem\n-\n-        try:\n-            # Import the module\n-            spec = importlib.util.spec_from_file_location(module_name, py_file)\n-            module = importlib.util.module_from_spec(spec)\n-            spec.loader.exec_module(module)\n-\n-            # Check if it has a benchmark runner function\n-            if hasattr(module, f\"run_{module_name}\"):\n-                benchmarks.append(\n-                    {\n-                        \"name\": module_name,\n-                        \"path\": str(py_file),\n-                        \"module\": module,\n-                        \"runner_function\": getattr(module, f\"run_{module_name}\"),\n-                    }\n-                )\n-            elif hasattr(module, \"run_benchmark\"):\n-                benchmarks.append(\n-                    {\n-                        \"name\": module_name,\n-                        \"path\": str(py_file),\n-                        \"module\": module,\n-                        \"runner_function\": getattr(module, \"run_benchmark\"),\n-                    }\n-                )\n-            else:\n-                logging.warning(f\"No runner function found in {py_file}\")\n-\n-        except Exception as e:\n-            logging.error(f\"Failed to import {py_file}: {e}\")\n-\n-    return benchmarks\n-\n-\n-def run_single_benchmark(\n-    benchmark_info: dict[str, Any], output_dir: str, logger: logging.Logger, **kwargs\n-) -> Optional[str]:\n-    \"\"\"\n-    Run a single benchmark and return the output file path.\n-\n-    Args:\n-        benchmark_info: Dictionary containing benchmark module info\n-        output_dir: Base output directory\n-        logger: Logger instance\n-        **kwargs: Additional arguments to pass to the benchmark\n-\n-    Returns:\n-        Path to the output file if successful, None otherwise\n-    \"\"\"\n-    benchmark_name = benchmark_info[\"name\"]\n-    runner_func = benchmark_info[\"runner_function\"]\n-\n-    logger.info(f\"Running benchmark: {benchmark_name}\")\n-\n-    try:\n-        # Check function signature to determine what arguments to pass\n-        import inspect\n-\n-        sig = inspect.signature(runner_func)\n-\n-        # Prepare arguments based on function signature\n-        func_kwargs = {\"logger\": logger, \"output_dir\": output_dir}\n-\n-        # Add other kwargs if the function accepts them\n-        for param_name in sig.parameters:\n-            if param_name in kwargs:\n-                func_kwargs[param_name] = kwargs[param_name]\n-\n-        # Filter kwargs to only include parameters the function accepts\n-        # If function has **kwargs, include all provided kwargs\n-        has_var_kwargs = any(param.kind == param.VAR_KEYWORD for param in sig.parameters.values())\n-        if has_var_kwargs:\n-            valid_kwargs = {**func_kwargs, **kwargs}\n-        else:\n-            valid_kwargs = {k: v for k, v in func_kwargs.items() if k in sig.parameters}\n-\n-        # Run the benchmark\n-        result = runner_func(**valid_kwargs)\n-\n-        if isinstance(result, str):\n-            # Function returned a file path\n-            return result\n-        else:\n-            logger.info(f\"Benchmark {benchmark_name} completed successfully\")\n-            return \"completed\"\n-\n-    except Exception as e:\n-        logger.error(f\"Benchmark {benchmark_name} failed: {e}\")\n-        import traceback\n-\n-        logger.debug(traceback.format_exc())\n-        return None\n-\n-\n-def generate_summary_report(\n-    output_dir: str,\n-    benchmark_results: dict[str, Any],\n-    logger: logging.Logger,\n-    benchmark_run_uuid: Optional[str] = None,\n-) -> str:\n-    \"\"\"Generate a summary report of all benchmark runs.\"\"\"\n-    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n-    summary_file = os.path.join(output_dir, f\"benchmark_summary_{timestamp}.json\")\n-\n-    summary_data = {\n-        \"run_metadata\": {\n-            \"timestamp\": datetime.utcnow().isoformat(),\n-            \"benchmark_run_uuid\": benchmark_run_uuid,\n-            \"total_benchmarks\": len(benchmark_results),\n-            \"successful_benchmarks\": len([r for r in benchmark_results.values() if r is not None]),\n-            \"failed_benchmarks\": len([r for r in benchmark_results.values() if r is None]),\n-        },\n-        \"benchmark_results\": benchmark_results,\n-        \"output_directory\": output_dir,\n-    }\n-\n-    with open(summary_file, \"w\") as f:\n-        json.dump(summary_data, f, indent=2, default=str)\n-\n-    logger.info(f\"Summary report saved to: {summary_file}\")\n-    return summary_file\n-\n-\n-def upload_results_to_hf_dataset(\n-    output_dir: str,\n-    summary_file: str,\n-    dataset_name: str,\n-    run_id: Optional[str] = None,\n-    token: Optional[str] = None,\n-    logger: Optional[logging.Logger] = None,\n-) -> Optional[str]:\n-    \"\"\"\n-    Upload benchmark results to a HuggingFace Dataset.\n-    Based on upload_collated_report() from utils/collated_reports.py\n-    Args:\n-        output_dir: Local output directory containing results\n-        summary_file: Path to the summary file\n-        dataset_name: Name of the HuggingFace dataset to upload to\n-        run_id: Unique run identifier (if None, will generate one)\n-        token: HuggingFace token for authentication (if None, will use environment variables)\n-        logger: Logger instance\n-    Returns:\n-        The run_id used for the upload, None if upload failed\n-    \"\"\"\n-    if logger is None:\n-        logger = logging.getLogger(__name__)\n-\n-    import os\n-\n-    from huggingface_hub import HfApi\n-\n-    api = HfApi()\n-\n-    if run_id is None:\n-        github_run_number = os.getenv(\"GITHUB_RUN_NUMBER\")\n-        github_run_id = os.getenv(\"GITHUB_RUN_ID\")\n-        if github_run_number and github_run_id:\n-            run_id = f\"{github_run_number}-{github_run_id}\"\n-\n-    date_folder = datetime.now().strftime(\"%Y-%m-%d\")\n-\n-    github_event_name = os.getenv(\"GITHUB_EVENT_NAME\")\n-    if github_event_name != \"schedule\":\n-        # Non-scheduled runs go under a runs subfolder\n-        repo_path = f\"{date_folder}/runs/{run_id}/benchmark_results\"\n-    else:\n-        # Scheduled runs go directly under the date\n-        repo_path = f\"{date_folder}/{run_id}/benchmark_results\"\n-\n-    logger.info(f\"Uploading benchmark results to dataset '{dataset_name}' at path '{repo_path}'\")\n-\n-    try:\n-        # Upload all files in the output directory\n-        from pathlib import Path\n-\n-        output_path = Path(output_dir)\n-\n-        for file_path in output_path.rglob(\"*\"):\n-            if file_path.is_file():\n-                # Calculate relative path from output_dir\n-                relative_path = file_path.relative_to(output_path)\n-                path_in_repo = f\"{repo_path}/{relative_path}\"\n-\n-                logger.debug(f\"Uploading {file_path} to {path_in_repo}\")\n-\n-                api.upload_file(\n-                    path_or_fileobj=str(file_path),\n-                    path_in_repo=path_in_repo,\n-                    repo_id=dataset_name,\n-                    repo_type=\"dataset\",\n-                    token=token,\n-                    commit_message=f\"Upload benchmark results for run {run_id}\",\n-                )\n-\n-        logger.info(\n-            f\"Successfully uploaded results to: https://huggingface.co/datasets/{dataset_name}/tree/main/{repo_path}\"\n-        )\n-\n-        return run_id\n-\n-    except Exception as upload_error:\n-        logger.error(f\"Failed to upload results: {upload_error}\")\n-        import traceback\n-\n-        logger.debug(traceback.format_exc())\n-        return None\n-\n-\n-def main():\n-    \"\"\"Main entry point for the benchmarking script.\"\"\"\n-    # Generate a unique UUID for this benchmark run\n-    benchmark_run_uuid = str(uuid.uuid4())[:8]\n-\n-    parser = argparse.ArgumentParser(\n-        description=\"Run all benchmarks in the ./benches directory\",\n-        epilog=\"\"\"\n-Examples:\n-  # Run all available benchmarks\n-  python3 run_benchmarks.py\n-  \n-  # Run with specific model and upload to HuggingFace Dataset\n-  python3 run_benchmarks.py --model-id meta-llama/Llama-2-7b-hf --upload-to-hf username/benchmark-results\n-  \n-  # Run with custom run ID and upload to HuggingFace Dataset\n-  python3 run_benchmarks.py --run-id experiment_v1 --upload-to-hf org/benchmarks\n-  \n-  # Run only specific benchmarks with file logging\n-  python3 run_benchmarks.py --include llama --enable-file-logging\n-        \"\"\",  # noqa: W293\n-        formatter_class=argparse.RawDescriptionHelpFormatter,\n-    )\n-\n-    parser.add_argument(\n-        \"--output-dir\",\n-        type=str,\n-        default=\"benchmark_results\",\n-        help=\"Base output directory for benchmark results (default: benchmark_results)\",\n-    )\n-\n-    parser.add_argument(\n-        \"--benches-dir\",\n-        type=str,\n-        default=\"./benches\",\n-        help=\"Directory containing benchmark implementations (default: ./benches)\",\n-    )\n-\n-    parser.add_argument(\n-        \"--log-level\",\n-        type=str,\n-        choices=[\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\"],\n-        default=\"INFO\",\n-        help=\"Logging level (default: INFO)\",\n-    )\n \n+if __name__ == \"__main__\":\n+    # Parse arguments\n+    parser = argparse.ArgumentParser()\n+    parser.add_argument(\"--output-dir\", type=str, default=\"benchmark_results\", help=\"Output dir for benchmark results\")\n+    parser.add_argument(\"--log-level\", type=str, choices=[\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\"], default=\"INFO\")\n     parser.add_argument(\"--model-id\", type=str, help=\"Specific model ID to benchmark (if supported by benchmarks)\")\n \n-    parser.add_argument(\"--warmup-iterations\", type=int, default=3, help=\"Number of warmup iterations (default: 3)\")\n+    parser.add_argument(\"--warmup\", type=int, default=5, help=\"Number of warmup iterations\")\n+    parser.add_argument(\"--iterations\", type=int, default=20, help=\"Number of measurement iterations\")\n \n-    parser.add_argument(\n-        \"--measurement-iterations\", type=int, default=5, help=\"Number of measurement iterations (default: 5)\"\n-    )\n-\n-    parser.add_argument(\n-        \"--num-tokens-to-generate\",\n-        type=int,\n-        default=100,\n-        help=\"Number of tokens to generate in benchmarks (default: 100)\",\n-    )\n+    parser.add_argument(\"--batch-size\", \"-b\", type=int, nargs=\"+\", help=\"Batch size\")\n+    parser.add_argument(\"--sequence-length\", \"-s\", type=int, nargs=\"+\", help=\"Sequence length\")\n+    parser.add_argument(\"--num-tokens-to-generate\", \"-n\", type=int, nargs=\"+\", help=\"Number of tokens to generate\")\n \n-    parser.add_argument(\"--include\", type=str, nargs=\"*\", help=\"Only run benchmarks matching these names\")\n-\n-    parser.add_argument(\"--exclude\", type=str, nargs=\"*\", help=\"Exclude benchmarks matching these names\")\n-\n-    parser.add_argument(\"--enable-file-logging\", action=\"store_true\", help=\"Enable file logging (disabled by default)\")\n-\n-    parser.add_argument(\n-        \"--commit-id\", type=str, help=\"Git commit ID for metadata (if not provided, will auto-detect from git)\"\n-    )\n-\n-    parser.add_argument(\n-        \"--push-to-hub\",\n-        type=str,\n-        help=\"Upload results to HuggingFace Dataset (provide dataset name, e.g., 'username/benchmark-results')\",\n-    )\n-\n-    parser.add_argument(\n-        \"--run-id\", type=str, help=\"Custom run ID for organizing results (if not provided, will generate a unique ID)\"\n-    )\n-\n-    parser.add_argument(\n-        \"--token\",\n-        type=str,\n-        help=\"HuggingFace token for dataset uploads (if not provided, will use HF_TOKEN environment variable)\",\n-    )\n+    parser.add_argument(\"--num-tokens-to-profile\", \"-p\", type=int, default=0, help=\"Number of tokens to profile\")\n \n+    parser.add_argument(\"--commit-id\", type=str, help=\"Git commit ID (if not provided, will auto-detect from git)\")\n     args = parser.parse_args()\n \n     # Setup logging\n-    logger = setup_logging(args.log_level, args.enable_file_logging)\n+    benchmark_run_uuid = str(uuid.uuid4())[:8]\n+    numeric_level = getattr(logging, args.log_level.upper())\n \n+    handlers = [logging.StreamHandler(sys.stdout)]\n+    logging.basicConfig(\n+        level=numeric_level, format=\"[%(levelname)s - %(asctime)s] %(name)s: %(message)s\", handlers=handlers\n+    )\n+\n+    logger = logging.getLogger(\"benchmark_v2\")\n     logger.info(\"Starting benchmark discovery and execution\")\n     logger.info(f\"Benchmark run UUID: {benchmark_run_uuid}\")\n     logger.info(f\"Output directory: {args.output_dir}\")\n-    logger.info(f\"Benches directory: {args.benches_dir}\")\n-\n-    # Create output directory\n-    os.makedirs(args.output_dir, exist_ok=True)\n-\n-    try:\n-        # Discover benchmarks\n-        benchmarks = discover_benchmarks(args.benches_dir)\n-        logger.info(f\"Discovered {len(benchmarks)} benchmark(s): {[b['name'] for b in benchmarks]}\")\n-\n-        if not benchmarks:\n-            logger.warning(\"No benchmarks found!\")\n-            return 1\n-\n-        # Filter benchmarks based on include/exclude\n-        filtered_benchmarks = benchmarks\n-\n-        if args.include:\n-            filtered_benchmarks = [\n-                b for b in filtered_benchmarks if any(pattern in b[\"name\"] for pattern in args.include)\n-            ]\n-            logger.info(f\"Filtered to include: {[b['name'] for b in filtered_benchmarks]}\")\n \n-        if args.exclude:\n-            filtered_benchmarks = [\n-                b for b in filtered_benchmarks if not any(pattern in b[\"name\"] for pattern in args.exclude)\n-            ]\n-            logger.info(f\"After exclusion: {[b['name'] for b in filtered_benchmarks]}\")\n+    # Error out if one of the arguments is not provided\n+    if len(args.batch_size) * len(args.sequence_length) * len(args.num_tokens_to_generate) == 0:\n+        raise ValueError(\n+            \"At least one of the arguments --batch-size, --sequence-length, or --num-tokens-to-generate is required\"\n+        )\n \n-        if not filtered_benchmarks:\n-            logger.warning(\"No benchmarks remaining after filtering!\")\n-            return 1\n+    # If there is only one (batch_size, sequence_length, num_tokens_to_generate), we benchmark across configs\n+    elif len(args.batch_size) * len(args.sequence_length) * len(args.num_tokens_to_generate) == 1:\n+        benchmark_configs = generate_all_configs(\n+            warmup_iterations=args.warmup,\n+            measurement_iterations=args.iterations,\n+            batch_size=args.batch_size[0],\n+            sequence_length=args.sequence_length[0],\n+            num_tokens_to_generate=args.num_tokens_to_generate[0],\n+        )\n+        random.shuffle(benchmark_configs)\n \n-        # Prepare common kwargs for benchmarks\n-        benchmark_kwargs = {\n-            \"warmup_iterations\": args.warmup_iterations,\n-            \"measurement_iterations\": args.measurement_iterations,\n-            \"num_tokens_to_generate\": args.num_tokens_to_generate,\n+    # Otherwise, we benchmark across all combinations of dimensions\n+    else:\n+        kwargs = {\n+            \"warmup_iterations\": args.warmup,\n+            \"measurement_iterations\": args.iterations,\n+            \"gpu_monitoring\": False,\n+            \"batch_size\": args.batch_size[0],\n+            \"sequence_length\": args.sequence_length[0],\n+            \"num_tokens_to_generate\": args.num_tokens_to_generate[0],\n+            \"attn_implementation\": \"flex_attention\",\n+            \"sdpa_backend\": None,\n+            \"compile_mode\": \"default\",\n+            \"kernelize\": False,\n         }\n-\n-        if args.model_id:\n-            benchmark_kwargs[\"model_id\"] = args.model_id\n-\n-        # Add commit_id if provided\n-        if args.commit_id:\n-            benchmark_kwargs[\"commit_id\"] = args.commit_id\n-\n-        # Run benchmarks\n-        benchmark_results = {}\n-        successful_count = 0\n-\n-        for benchmark_info in filtered_benchmarks:\n-            result = run_single_benchmark(benchmark_info, args.output_dir, logger, **benchmark_kwargs)\n-\n-            benchmark_results[benchmark_info[\"name\"]] = result\n-\n-            if result is not None:\n-                successful_count += 1\n-\n-        # Generate summary report\n-        summary_file = generate_summary_report(args.output_dir, benchmark_results, logger, benchmark_run_uuid)\n-\n-        # Upload results to HuggingFace Dataset if requested\n-        upload_run_id = None\n-        if args.push_to_hub:\n-            logger.info(\"=\" * 60)\n-            logger.info(\"UPLOADING TO HUGGINGFACE DATASET\")\n-            logger.info(\"=\" * 60)\n-            # Use provided run_id or fallback to benchmark run UUID\n-            effective_run_id = args.run_id or benchmark_run_uuid\n-            upload_run_id = upload_results_to_hf_dataset(\n-                output_dir=args.output_dir,\n-                summary_file=summary_file,\n-                dataset_name=args.push_to_hub,\n-                run_id=effective_run_id,\n-                token=args.token,\n-                logger=logger,\n-            )\n-            if upload_run_id:\n-                logger.info(f\"Upload completed with run ID: {upload_run_id}\")\n-            else:\n-                logger.warning(\"Upload failed - continuing with local results\")\n-\n-        # Final summary\n-        total_benchmarks = len(filtered_benchmarks)\n-        failed_count = total_benchmarks - successful_count\n-\n-        logger.info(\"=\" * 60)\n-        logger.info(\"BENCHMARK RUN SUMMARY\")\n-        logger.info(\"=\" * 60)\n-        logger.info(f\"Total benchmarks: {total_benchmarks}\")\n-        logger.info(f\"Successful: {successful_count}\")\n-        logger.info(f\"Failed: {failed_count}\")\n-        logger.info(f\"Output directory: {args.output_dir}\")\n-        logger.info(f\"Summary report: {summary_file}\")\n-\n-        if args.push_to_hub:\n-            if upload_run_id:\n-                logger.info(f\"HuggingFace Dataset: {args.push_to_hub}\")\n-                logger.info(f\"Run ID: {upload_run_id}\")\n-                logger.info(\n-                    f\"View results: https://huggingface.co/datasets/{args.push_to_hub}/tree/main/{datetime.now().strftime('%Y-%m-%d')}/runs/{upload_run_id}\"\n-                )\n-            else:\n-                logger.warning(\"Upload to HuggingFace Dataset failed\")\n-\n-        if failed_count > 0:\n-            logger.warning(f\"{failed_count} benchmark(s) failed. Check logs for details.\")\n-            return 1\n-        else:\n-            logger.info(\"All benchmarks completed successfully!\")\n-            return 0\n-\n-    except Exception as e:\n-        logger.error(f\"Benchmark run failed: {e}\")\n-        import traceback\n-\n-        logger.debug(traceback.format_exc())\n-        return 1\n-\n-\n-if __name__ == \"__main__\":\n-    sys.exit(main())\n+        benchmark_configs = []\n+        for num_tokens_to_generate in args.num_tokens_to_generate:\n+            for sequence_length in args.sequence_length:\n+                for batch_size in args.batch_size:\n+                    kwargs[\"batch_size\"] = batch_size\n+                    kwargs[\"sequence_length\"] = sequence_length\n+                    kwargs[\"num_tokens_to_generate\"] = num_tokens_to_generate\n+                    benchmark_configs.append(BenchmarkConfig(**kwargs))\n+\n+    runner = BenchmarkRunner(logger, args.output_dir, args.commit_id)\n+    results = runner.run_benchmarks(\n+        args.model_id,\n+        benchmark_configs[:3],\n+        args.num_tokens_to_profile,\n+        pretty_print_summary=True,\n+    )\n+    # runner.save_results(args.model_id, results)"
      }
    ],
    "num_files": 13,
    "scraped_at": "2025-11-16T21:18:10.304198"
  },
  {
    "pr_number": 41401,
    "title": "[Model] Lfm2Moe",
    "body": "# What does this PR do?\r\n\r\nThis PR implements [LFM2-8B-A1B](https://huggingface.co/LiquidAI/LFM2-8B-A1B), a hybrid Mixture-of-Experts architecture variant of [LFM2](https://huggingface.co/collections/LiquidAI/lfm2-686d721927015b2ad73eaa38). The LFM2 family is optimized for on-device inference by combining short\u2011range, input\u2011aware gated convolutions with grouped\u2011query attention (GQA). LFM2\u2011MoE keeps this fast backbone and introduces sparse MoE feed\u2011forward networks to add representational capacity without significantly increasing the active compute path.\r\n\r\n\r\n## Before submitting\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),\r\n      Pull Request section?\r\n- [x] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [x] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker @Cyrilvallez\r\n- vision models: @yonigozlan @molbap\r\n- audio models: @eustlb @ebezzam @vasqu\r\n- multimodal models: @zucchini-nlp\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- generate: @zucchini-nlp (visual-language models) or @gante (all others)\r\n- continuous batching: @remi-or @ArthurZucker @McPatate\r\n- pipelines: @Rocketknight1\r\n- tokenizers: @ArthurZucker and @itazap\r\n- trainer: @zach-huggingface @SunMarc\r\n- attention: @vasqu @ArthurZucker @CyrilVallez\r\n- model loading (from pretrained, etc): @CyrilVallez\r\n- distributed: @3outeille @ArthurZucker @S1ro1\r\n- CIs: @ydshieh\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @SunMarc @zach-huggingface\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber\r\n- kernels: @MekkCyber @drbh\r\n- peft: @BenjaminBossan @githubnemo\r\n\r\nDevices/Backends:\r\n\r\n- AMD ROCm: @ivarflakstad\r\n- Intel XPU: @IlyasMoutawwakil\r\n- Ascend NPU: @ivarflakstad \r\n\r\nDocumentation: @stevhliu\r\n\r\nResearch projects are not maintained and should be taken as is.\r\n\r\n -->\r\n",
    "html_url": "https://github.com/huggingface/transformers/pull/41401",
    "created_at": "2025-10-07T09:32:13Z",
    "merged_at": "2025-10-07T13:09:58Z",
    "merge_commit_sha": "0c9a72e4576fe4c84077f066e585129c97bfd4e6",
    "base_ref": "main",
    "head_sha": "b3a0924ec93a51b43d6d772d825ddde3b1d9f0ca",
    "user": "paulpak58",
    "files": [
      {
        "filename": "docs/source/en/_toctree.yml",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -562,6 +562,8 @@\n         title: LED\n       - local: model_doc/lfm2\n         title: LFM2\n+      - local: model_doc/lfm2_moe\n+        title: LFM2Moe\n       - local: model_doc/llama\n         title: LLaMA\n       - local: model_doc/llama2"
      },
      {
        "filename": "docs/source/en/model_doc/lfm2.md",
        "status": "modified",
        "additions": 4,
        "deletions": 4,
        "changes": 8,
        "patch": "@@ -23,15 +23,15 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-[LFM2](https://www.liquid.ai/blog/liquid-foundation-models-v2-our-second-series-of-generative-ai-models) represents a new generation of Liquid Foundation Models developed by [Liquid AI](https://liquid.ai/), specifically designed for edge AI and on-device deployment.\n+[LFM2](https://www.liquid.ai/blog/liquid-foundation-models-v2-our-second-series-of-generative-ai-models) represents a new generation of Liquid Foundation Models developed by Liquid AI, specifically designed for edge AI and on-device deployment.\n \n-The models are available in three sizes (350M, 700M, and 1.2B parameters) and are engineered to run efficiently on CPU, GPU, and NPU hardware, making them particularly well-suited for applications requiring low latency, offline operation, and privacy.\n+The models are available in four sizes (350M, 700M, 1.2B, and 2.6B parameters) and are engineered to run efficiently on CPU, GPU, and NPU hardware, making them particularly well-suited for applications requiring low latency, offline operation, and privacy.\n \n ## Architecture\n \n-The architecture consists of 16 blocks total: 10 double-gated short-range convolution blocks and 6 blocks of grouped query attention. This design stems from the concept of dynamical systems, where linear operations are modulated by input-dependent gates, allowing for \"liquid\" dynamics that can adapt in real-time. The short convolutions are particularly optimized for embedded SoC CPUs, making them ideal for devices that require fast, local inference without relying on cloud connectivity.\n+The architecture consists of blocks of gated short convolution blocks and blocks of grouped query attention with QK layernorm. This design stems from the concept of dynamical systems, where linear operations are modulated by input-dependent gates. The short convolutions are particularly optimized for embedded SoC CPUs, making them ideal for devices that require fast, local inference without relying on cloud connectivity.\n \n-The key architectural innovation of LFM2 lies in its systematic approach to balancing quality, latency, and memory efficiency through our STAR neural architecture search engine. Using STAR, Liquid AI optimized the models for real-world performance on embedded hardware, measuring actual peak memory usage and inference speed on Qualcomm Snapdragon processors. This results in models that achieve 2x faster decode and prefill performance compared to similar-sized models, while maintaining superior benchmark performance across knowledge, mathematics, instruction following, and multilingual tasks.\n+LFM2 was designed to maximize quality under strict speed and memory constraints. This was accomplished through a systematic architecture search to optimize the models for real-world performance on embedded hardware by measuring actual peak memory usage and inference speed on Qualcomm Snapdragon processors. This results in models that achieve 2x faster decode and prefill performance compared to similar-sized models, while maintaining superior benchmark performance across knowledge, mathematics, instruction following, and multilingual tasks.\n \n ## Example\n "
      },
      {
        "filename": "docs/source/en/model_doc/lfm2_moe.md",
        "status": "added",
        "additions": 83,
        "deletions": 0,
        "changes": 83,
        "patch": "@@ -0,0 +1,83 @@\n+<!--Copyright 2025 the HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+\n+\n+\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be rendered properly in your Markdown viewer.\n+\n+-->\n+\n+\n+# Lfm2Moe\n+\n+## Overview\n+\n+LFM2-MoE is a Mixture-of-Experts (MoE) variant of [LFM2](https://huggingface.co/collections/LiquidAI/lfm2-686d721927015b2ad73eaa38). The LFM2 family is optimized for on-device inference by combining short\u2011range, input\u2011aware gated convolutions with grouped\u2011query attention (GQA) in a layout tuned to maximize quality under strict speed and memory constraints.\n+\n+LFM2\u2011MoE keeps this fast backbone and introduces sparse MoE feed\u2011forward networks to add representational capacity without significantly increasing the active compute path. The first LFM2-MoE release is LFM2-8B-A1B, with 8.3B total parameters and 1.5B active parameters. The model excels in quality (comparable to 3-4B dense models) and speed (faster than other 1.5B class models). \n+\n+## Example\n+\n+The following example shows how to generate an answer using the `AutoModelForCausalLM` class.\n+\n+```python\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n+\n+# Load model and tokenizer\n+model_id = \"LiquidAI/LFM2-8B-A1B\"\n+model = AutoModelForCausalLM.from_pretrained(\n+    model_id,\n+    device_map=\"auto\",\n+    dtype=\"bfloat16\",\n+#    attn_implementation=\"flash_attention_2\" <- uncomment on compatible GPU\n+)\n+tokenizer = AutoTokenizer.from_pretrained(model_id)\n+\n+# Generate answer\n+prompt = \"What is C. elegans?\"\n+input_ids = tokenizer.apply_chat_template(\n+    [{\"role\": \"user\", \"content\": prompt}],\n+    add_generation_prompt=True,\n+    return_tensors=\"pt\",\n+    tokenize=True,\n+).to(model.device)\n+\n+output = model.generate(\n+    input_ids,\n+    do_sample=True,\n+    temperature=0.3,\n+    min_p=0.15,\n+    repetition_penalty=1.05,\n+    max_new_tokens=512,\n+)\n+\n+print(tokenizer.decode(output[0], skip_special_tokens=False))\n+```\n+\n+## Lfm2MoeConfig\n+\n+[[autodoc]] Lfm2MoeConfig\n+\n+## Lfm2MoeForCausalLM\n+\n+[[autodoc]] Lfm2MoeForCausalLM\n+\n+## Lfm2MoeModel\n+\n+[[autodoc]] Lfm2MoeModel\n+    - forward\n+\n+## Lfm2MoePreTrainedModel\n+\n+[[autodoc]] Lfm2MoePreTrainedModel\n+    - forward"
      },
      {
        "filename": "src/transformers/models/__init__.py",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -186,6 +186,7 @@\n     from .led import *\n     from .levit import *\n     from .lfm2 import *\n+    from .lfm2_moe import *\n     from .lfm2_vl import *\n     from .lightglue import *\n     from .lilt import *"
      },
      {
        "filename": "src/transformers/models/auto/configuration_auto.py",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -226,6 +226,7 @@\n         (\"led\", \"LEDConfig\"),\n         (\"levit\", \"LevitConfig\"),\n         (\"lfm2\", \"Lfm2Config\"),\n+        (\"lfm2_moe\", \"Lfm2MoeConfig\"),\n         (\"lfm2_vl\", \"Lfm2VlConfig\"),\n         (\"lightglue\", \"LightGlueConfig\"),\n         (\"lilt\", \"LiltConfig\"),\n@@ -670,6 +671,7 @@\n         (\"led\", \"LED\"),\n         (\"levit\", \"LeViT\"),\n         (\"lfm2\", \"Lfm2\"),\n+        (\"lfm2_moe\", \"Lfm2Moe\"),\n         (\"lfm2_vl\", \"Lfm2Vl\"),\n         (\"lightglue\", \"LightGlue\"),\n         (\"lilt\", \"LiLT\"),"
      },
      {
        "filename": "src/transformers/models/auto/modeling_auto.py",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -226,6 +226,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"led\", \"LEDModel\"),\n         (\"levit\", \"LevitModel\"),\n         (\"lfm2\", \"Lfm2Model\"),\n+        (\"lfm2_moe\", \"Lfm2MoeModel\"),\n         (\"lfm2_vl\", \"Lfm2VlModel\"),\n         (\"lightglue\", \"LightGlueForKeypointMatching\"),\n         (\"lilt\", \"LiltModel\"),\n@@ -694,6 +695,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"jamba\", \"JambaForCausalLM\"),\n         (\"jetmoe\", \"JetMoeForCausalLM\"),\n         (\"lfm2\", \"Lfm2ForCausalLM\"),\n+        (\"lfm2_moe\", \"Lfm2MoeForCausalLM\"),\n         (\"llama\", \"LlamaForCausalLM\"),\n         (\"llama4\", \"Llama4ForCausalLM\"),\n         (\"llama4_text\", \"Llama4ForCausalLM\"),"
      },
      {
        "filename": "src/transformers/models/lfm2/modeling_lfm2.py",
        "status": "modified",
        "additions": 0,
        "deletions": 2,
        "changes": 2,
        "patch": "@@ -163,7 +163,6 @@ def __init__(\n                 dtype=self._dtype,\n                 device=device,\n             )\n-            torch._dynamo.mark_static_address(conv_state)\n             self.conv_cache.append(conv_state)\n             self.key_cache.append(torch.tensor([]))\n             self.value_cache.append(torch.tensor([]))\n@@ -595,7 +594,6 @@ def __init__(self, config: Lfm2Config):\n         self.layers = nn.ModuleList(\n             [Lfm2DecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n         )\n-        self.rotary_emb = Lfm2RotaryEmbedding(config=config)\n         self.gradient_checkpointing = False\n         self.pos_emb = Lfm2RotaryEmbedding(config)\n         self.embedding_norm = Lfm2RMSNorm(config.hidden_size, eps=config.norm_eps)"
      },
      {
        "filename": "src/transformers/models/lfm2/modular_lfm2.py",
        "status": "modified",
        "additions": 1,
        "deletions": 2,
        "changes": 3,
        "patch": "@@ -121,7 +121,6 @@ def __init__(\n                 dtype=self._dtype,\n                 device=device,\n             )\n-            torch._dynamo.mark_static_address(conv_state)\n             self.conv_cache.append(conv_state)\n             self.key_cache.append(torch.tensor([]))\n             self.value_cache.append(torch.tensor([]))\n@@ -441,7 +440,7 @@ def __init__(self, config: Lfm2Config):\n         self.pos_emb = Lfm2RotaryEmbedding(config)\n         self.embedding_norm = Lfm2RMSNorm(config.hidden_size, eps=config.norm_eps)\n         del self.norm\n-        del self.rotary_emv\n+        del self.rotary_emb\n \n     def forward(\n         self,"
      },
      {
        "filename": "src/transformers/models/lfm2_moe/__init__.py",
        "status": "added",
        "additions": 29,
        "deletions": 0,
        "changes": 29,
        "patch": "@@ -0,0 +1,29 @@\n+# coding=utf-8\n+# Copyright 2025 the HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_lfm2_moe import *\n+    from .modeling_lfm2_moe import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
      },
      {
        "filename": "src/transformers/models/lfm2_moe/configuration_lfm2_moe.py",
        "status": "added",
        "additions": 169,
        "deletions": 0,
        "changes": 169,
        "patch": "@@ -0,0 +1,169 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import Optional\n+\n+from ...configuration_utils import PretrainedConfig\n+\n+\n+class Lfm2MoeConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Lfm2MoeModel`]. It is used to instantiate a LFM2 Moe\n+    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n+    defaults will yield a similar configuration to that of the LFM2-8B-A1B model.\n+    e.g. [LiquidAI/LFM2-8B-A1B](https://huggingface.co/LiquidAI/LFM2-8B-A1B)\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+\n+    Args:\n+        vocab_size (`int`, *optional*, defaults to 65536):\n+            Vocabulary size of the LLaMA model. Defines the number of different tokens that can be represented by the\n+            `inputs_ids` passed when calling [`Lfm2Model`]\n+        hidden_size (`int`, *optional*, defaults to 2048):\n+            Dimension of the hidden representations.\n+        intermediate_size (`int`, *optional*, defaults to 7168):\n+            Dimension of the MLP representations.\n+        moe_intermediate_size (`int`, *optional*, defaults to 1792):\n+            Intermediate size of the routed expert.\n+        num_hidden_layers (`int`, *optional*, defaults to 32):\n+            Number of hidden layers in the Transformer decoder.\n+        pad_token_id (`int`, *optional*, defaults to 0):\n+            Padding token id.\n+        bos_token_id (`int`, *optional*, defaults to 1):\n+            Beginning of stream token id.\n+        eos_token_id (`int`, *optional*, defaults to 2):\n+            End of stream token id.\n+        tie_word_embeddings (`bool`, *optional*, defaults to `True`):\n+            Whether to tie weight embeddings\n+        rope_theta (`float`, *optional*, defaults to 1000000.0):\n+            The base period of the RoPE embeddings.\n+        max_position_embeddings (`int`, *optional*, defaults to 128000):\n+            The maximum sequence length that this model might ever be used with.\n+        use_cache (`bool`, *optional*, defaults to `True`):\n+            Whether or not the model should return the last key/values attentions (not used by all models). Only\n+            relevant if `config.is_decoder=True`.\n+        norm_eps (`float`, *optional*, defaults to 1e-05):\n+            The epsilon used by the rms normalization layers.\n+        num_attention_heads (`int`, *optional*, defaults to 32):\n+            Number of attention heads for each attention layer in the Transformer decoder.\n+        num_key_value_heads (`int`, *optional*, defaults to 8):\n+            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n+            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n+            `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n+            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n+            by meanpooling all the original heads within that group. For more details, check out [this\n+            paper](https://huggingface.co/papers/2305.13245). If it is not specified, will default to\n+            `num_attention_heads`.\n+        conv_bias (`bool`, *optional*, defaults to `False`):\n+            Whether to use bias in the conv layers.\n+        conv_L_cache (`int`, *optional*, defaults to 3):\n+            L_cache dim in the conv layers.\n+        num_dense_layers (`int`, *optional*, defaults to 2):\n+            Number of dense Lfm2MoeMLP layers in shallow layers(embed->dense->dense->...->dense->moe->moe...->lm_head).\n+        num_experts_per_tok (`int`, *optional*, defaults to 4):\n+            Number of selected experts.\n+        num_experts (`int`, *optional*, defaults to 32):\n+            Number of routed experts.\n+        use_expert_bias (`bool`, *optional*, defaults to `True`):\n+            Whether to use the expert bias on the routing weights.\n+        routed_scaling_factor (`float`, *optional*, defaults to 1.0):\n+            Scaling factor for routed experts in MoE models.\n+        norm_topk_prob (`bool`, *optional*, defaults to `True`):\n+            Whether to normalize the topk probabilities.\n+        layer_types (`Optional`, *optional*):\n+            Type of each layers.\n+\n+    ```python\n+    >>> from transformers import Lfm2MoeModel, Lfm2MoeConfig\n+\n+    >>> # Initializing a LFM2 Moe model\n+    >>> configuration = Lfm2MoeConfig()\n+\n+    >>> # Initializing a model from the LFM2-8B-A1B style configuration\n+    >>> model = Lfm2MoeModel(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"lfm2_moe\"\n+    keys_to_ignore_at_inference = [\"past_key_values\"]\n+\n+    def __init__(\n+        self,\n+        vocab_size: int = 65536,\n+        hidden_size: int = 2048,\n+        intermediate_size: int = 7168,\n+        moe_intermediate_size: int = 1792,\n+        num_hidden_layers: int = 32,\n+        pad_token_id: int = 0,\n+        bos_token_id: int = 1,\n+        eos_token_id: int = 2,\n+        tie_word_embeddings: bool = True,\n+        rope_theta: float = 1000000.0,\n+        max_position_embeddings: int = 128_000,\n+        use_cache: bool = True,\n+        norm_eps: float = 0.00001,\n+        num_attention_heads: int = 32,\n+        num_key_value_heads: int = 8,\n+        conv_bias: bool = False,\n+        conv_L_cache: int = 3,\n+        num_dense_layers: int = 2,\n+        num_experts_per_tok: int = 4,\n+        num_experts: int = 32,\n+        use_expert_bias: bool = True,\n+        routed_scaling_factor: float = 1.0,\n+        norm_topk_prob: bool = True,\n+        layer_types: Optional[list[str]] = None,\n+        **kwargs,\n+    ):\n+        self.vocab_size = vocab_size\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.rope_theta = rope_theta\n+        self.max_position_embeddings = max_position_embeddings\n+        self.use_cache = use_cache\n+        self.norm_eps = norm_eps\n+\n+        # attn operator config\n+        self.num_attention_heads = num_attention_heads\n+        self.num_key_value_heads = num_key_value_heads\n+\n+        # custom operator config\n+        self.conv_bias = conv_bias\n+        self.conv_L_cache = conv_L_cache\n+\n+        # moe config\n+        self.num_dense_layers = num_dense_layers\n+        self.moe_intermediate_size = moe_intermediate_size\n+        self.num_experts_per_tok = num_experts_per_tok\n+        self.num_experts = num_experts\n+        self.use_expert_bias = use_expert_bias\n+        self.routed_scaling_factor = routed_scaling_factor\n+        self.norm_topk_prob = norm_topk_prob\n+        self.layer_types = layer_types\n+\n+        tie_word_embeddings = kwargs.get(\"tie_embedding\", tie_word_embeddings)  # to fit original config keys\n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n+\n+\n+__all__ = [\"Lfm2MoeConfig\"]"
      },
      {
        "filename": "src/transformers/models/lfm2_moe/modeling_lfm2_moe.py",
        "status": "added",
        "additions": 813,
        "deletions": 0,
        "changes": 813,
        "patch": "@@ -0,0 +1,813 @@\n+#                \ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\n+#           This file was automatically generated from src/transformers/models/lfm2_moe/modular_lfm2_moe.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_lfm2_moe.py file directly. One of our CI enforces this.\n+#                \ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import Any, Callable, Optional, Union\n+\n+import torch\n+import torch.nn.functional as F\n+from torch import nn\n+\n+from ...cache_utils import Cache\n+from ...generation import GenerationMixin\n+from ...integrations import use_kernel_forward_from_hub\n+from ...masking_utils import create_causal_mask\n+from ...modeling_layers import GradientCheckpointingLayer\n+from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast, MoeModelOutputWithPast\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils.deprecation import deprecate_kwarg\n+from ...utils.generic import check_model_inputs\n+from ...utils.import_utils import is_causal_conv1d_available\n+from .configuration_lfm2_moe import Lfm2MoeConfig\n+\n+\n+if is_causal_conv1d_available():\n+    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\n+else:\n+    causal_conv1d_fn, causal_conv1d_update = None, None\n+\n+\n+@use_kernel_forward_from_hub(\"RMSNorm\")\n+class Lfm2MoeRMSNorm(nn.Module):\n+    def __init__(self, hidden_size, eps=1e-6):\n+        \"\"\"\n+        Lfm2MoeRMSNorm is equivalent to T5LayerNorm\n+        \"\"\"\n+        super().__init__()\n+        self.weight = nn.Parameter(torch.ones(hidden_size))\n+        self.variance_epsilon = eps\n+\n+    def forward(self, hidden_states):\n+        input_dtype = hidden_states.dtype\n+        hidden_states = hidden_states.to(torch.float32)\n+        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n+        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n+        return self.weight * hidden_states.to(input_dtype)\n+\n+    def extra_repr(self):\n+        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n+\n+\n+class Lfm2MoeRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n+    def __init__(self, config: Lfm2MoeConfig, device=None):\n+        super().__init__()\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+        else:\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+    @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n+    def forward(self, x, position_ids):\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n+class Lfm2MoeMLP(nn.Module):\n+    def __init__(self, config: Lfm2MoeConfig, intermediate_size: Optional[int] = None):\n+        super().__init__()\n+        self.hidden_size = config.hidden_size\n+        self.intermediate_size = config.intermediate_size if intermediate_size is None else intermediate_size\n+        self.w1 = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.w3 = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.w2 = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n+\n+    def forward(self, x):\n+        return self.w2(F.silu(self.w1(x)) * self.w3(x))\n+\n+\n+class Lfm2MoeExperts(nn.ModuleList):\n+    \"\"\"\n+    ModuleList of experts.\n+    \"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+        self.num_experts = config.num_experts\n+        for _ in range(config.num_experts):\n+            self.append(Lfm2MoeMLP(config, intermediate_size=config.moe_intermediate_size))\n+\n+    def forward(\n+        self, hidden_states: torch.Tensor, top_k_index: torch.Tensor, top_k_weights: torch.Tensor\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        Args:\n+            hidden_states: (batch_size * sequence_length, hidden_dim)\n+            selected_experts: (batch_size * sequence_length, top_k)\n+            routing_weights: (batch_size * sequence_length, top_k)\n+        Returns:\n+            (batch_size * sequence_length, hidden_dim)\n+        \"\"\"\n+        final_hidden_states = torch.zeros_like(hidden_states)\n+        expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=self.num_experts).permute(2, 1, 0)\n+\n+        expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n+        for expert_idx in expert_hit:\n+            idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\n+            current_state = hidden_states[None, top_x].reshape(-1, hidden_states.shape[-1])\n+            current_hidden_states = self[expert_idx](current_state) * top_k_weights[top_x, idx, None]\n+            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n+        return final_hidden_states\n+\n+\n+class Lfm2MoeSparseMoeBlock(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.top_k = config.num_experts_per_tok\n+        self.routed_scaling_factor = config.routed_scaling_factor\n+        self.norm_topk_prob = config.norm_topk_prob\n+        self.use_expert_bias = config.use_expert_bias\n+\n+        self.gate = nn.Linear(config.hidden_size, config.num_experts, bias=False)\n+        self.experts = Lfm2MoeExperts(config)\n+        if self.use_expert_bias:\n+            self.register_buffer(\"expert_bias\", torch.zeros(config.num_experts, dtype=torch.float32))\n+\n+    def route_tokens_to_experts(self, router_logits):\n+        routing_weights = router_logits.sigmoid()\n+        if self.use_expert_bias:\n+            scores_for_routing = routing_weights + self.expert_bias\n+            _, selected_experts = torch.topk(scores_for_routing, k=self.top_k, dim=-1)\n+            routing_weights = torch.gather(routing_weights, dim=1, index=selected_experts).type_as(router_logits)\n+        else:\n+            routing_weights, selected_experts = torch.topk(routing_weights, k=self.top_k, dim=-1)\n+\n+        if self.norm_topk_prob:\n+            routing_weights = routing_weights / (routing_weights.sum(dim=-1, keepdim=True) + 1e-6)\n+        routing_weights = routing_weights * self.routed_scaling_factor\n+        return selected_experts, routing_weights\n+\n+    def forward(self, hidden_states: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n+        batch_size, sequence_length, hidden_dim = hidden_states.shape\n+        hidden_states_reshaped = hidden_states.view(-1, hidden_dim)\n+        router_logits = self.gate(hidden_states_reshaped)\n+        selected_experts, routing_weights = self.route_tokens_to_experts(router_logits)\n+        final_hidden_states = self.experts(hidden_states_reshaped, selected_experts, routing_weights)\n+        return final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n+\n+\n+class Lfm2MoeHybridConvCache:\n+    \"\"\"\n+    Attention and conv cache for Lfm2Moe.\n+\n+    It stores the Key and Value states as a list of tensors, one for each layer.\n+    Attention layer cache shape: `[batch_size, num_heads, seq_len, head_dim]`.\n+    Conv layer cache shape: `[batch_size, hidden_size, L_cache-1]`.\n+    \"\"\"\n+\n+    # Override @property existing in Cache\n+    max_batch_size = None\n+    is_compileable = False\n+    key_cache = None\n+    value_cache = None\n+\n+    def __init__(\n+        self,\n+        config: Lfm2MoeConfig,\n+        max_batch_size: int,\n+        dtype: torch.dtype = torch.float32,\n+        device: Union[torch.device, str, None] = None,\n+    ):\n+        self.key_cache = []\n+        self.value_cache = []\n+        self.max_batch_size = max_batch_size\n+        self.layer_types = config.layer_types\n+        self.first_attention_layer = self.layer_types.index(\"full_attention\")\n+        self.conv_L_cache = config.conv_L_cache\n+        self._dtype = dtype\n+\n+        self.conv_cache: list[torch.Tensor] = []\n+        device = torch.device(device) if device is not None else None\n+\n+        for _ in range(config.num_hidden_layers):\n+            conv_state = torch.zeros(\n+                self.max_batch_size,\n+                config.hidden_size,\n+                self.conv_L_cache,\n+                dtype=self._dtype,\n+                device=device,\n+            )\n+            self.conv_cache.append(conv_state)\n+            self.key_cache.append(torch.tensor([]))\n+            self.value_cache.append(torch.tensor([]))\n+\n+    def update(\n+        self,\n+        key_states: torch.Tensor,\n+        value_states: torch.Tensor,\n+        layer_idx: int,\n+        cache_kwargs: Optional[dict[str, Any]] = None,\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n+        \"\"\"\n+        Updates the cache with the new `key_states` and `value_states` for the layer `layer_idx`.\n+\n+        Parameters:\n+            key_states (`torch.Tensor`):\n+                The new key states to cache.\n+            value_states (`torch.Tensor`):\n+                The new value states to cache.\n+            layer_idx (`int`):\n+                The index of the layer to cache the states for.\n+            cache_kwargs (`Dict[str, Any]`, `optional`):\n+                Additional arguments for the cache subclass. No additional arguments are used in `DynamicCache`.\n+\n+        Return:\n+            A tuple containing the updated key and value states.\n+        \"\"\"\n+        # Update the cache\n+        if self.key_cache[layer_idx].numel() == 0:\n+            self.key_cache[layer_idx] = key_states\n+            self.value_cache[layer_idx] = value_states\n+        else:\n+            self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2)\n+            self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2)\n+\n+        return self.key_cache[layer_idx], self.value_cache[layer_idx]\n+\n+    def reorder_cache(self, beam_idx: torch.LongTensor):\n+        \"\"\"Reorders the cache for beam search, given the selected beam indices.\"\"\"\n+        for layer_idx in range(len(self.key_cache)):\n+            if self.key_cache[layer_idx].numel():\n+                device = self.key_cache[layer_idx].device\n+                self.key_cache[layer_idx] = self.key_cache[layer_idx].index_select(0, beam_idx.to(device))\n+                device = self.value_cache[layer_idx].device\n+                self.value_cache[layer_idx] = self.value_cache[layer_idx].index_select(0, beam_idx.to(device))\n+\n+            if self.conv_cache[layer_idx].numel():\n+                device = self.conv_cache[layer_idx].device\n+                self.conv_cache[layer_idx] = self.conv_cache[layer_idx].index_select(0, beam_idx.to(device))\n+\n+    def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n+        \"\"\"Returns the sequence length of the cached states. A layer index can be optionally passed.\"\"\"\n+        # take any layer that contains cache and not empty tensor\n+        layer_idx = self.first_attention_layer if self.layer_types[layer_idx] != \"full_attention\" else layer_idx\n+        if len(self.key_cache) <= layer_idx or self.key_cache[layer_idx].numel() == 0:\n+            return 0\n+        return self.key_cache[layer_idx].shape[-2]\n+\n+    def get_mask_sizes(self, cache_position: torch.Tensor, layer_idx: int) -> tuple[int, int]:\n+        \"\"\"\n+        Return a tuple (kv_length, kv_offset) corresponding to the length and offset that will be returned for\n+        the given layer at `layer_idx`.\n+        The masks are then prepared according to the given lengths (kv_length, kv_offset) and patterns (i.e. sliding_window, chunk_size),\n+        for each layer.\n+        \"\"\"\n+        full_mask_kv_offset = 0\n+        query_length = cache_position.shape[0]\n+        past_seen_tokens = self.get_seq_length()\n+        kv_length = query_length + past_seen_tokens\n+        return kv_length, full_mask_kv_offset\n+\n+    def crop(self, max_length: int):\n+        \"\"\"Crop the cache to the given length\"\"\"\n+        if max_length < 0:\n+            max_length = self.get_seq_length() - abs(max_length)\n+\n+        if self.get_seq_length() <= max_length:\n+            return\n+\n+        for idx in range(len(self.key_cache)):\n+            if self.key_cache[idx].numel():\n+                self.key_cache[idx] = self.key_cache[idx][..., :max_length, :]\n+                self.value_cache[idx] = self.value_cache[idx][..., :max_length, :]\n+\n+    def __len__(self) -> int:\n+        return len(self.key_cache)\n+\n+    def __getitem__(self, layer_idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n+        return self.key_cache[layer_idx], self.value_cache[layer_idx]\n+\n+    def reset(self):\n+        for layer_idx in range(len(self.conv_cache)):\n+            # In-place ops prevent breaking the static address\n+            self.conv_cache[layer_idx].zero_()\n+\n+\n+def rotate_half(x):\n+    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n+    x1 = x[..., : x.shape[-1] // 2]\n+    x2 = x[..., x.shape[-1] // 2 :]\n+    return torch.cat((-x2, x1), dim=-1)\n+\n+\n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n+    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n+\n+    Args:\n+        q (`torch.Tensor`): The query tensor.\n+        k (`torch.Tensor`): The key tensor.\n+        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n+        sin (`torch.Tensor`): The sine part of the rotary embedding.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n+        unsqueeze_dim (`int`, *optional*, defaults to 1):\n+            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n+            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n+            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n+            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n+            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n+            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n+    Returns:\n+        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n+    \"\"\"\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n+    q_embed = (q * cos) + (rotate_half(q) * sin)\n+    k_embed = (k * cos) + (rotate_half(k) * sin)\n+    return q_embed, k_embed\n+\n+\n+def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n+    \"\"\"\n+    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n+    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n+    \"\"\"\n+    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n+    if n_rep == 1:\n+        return hidden_states\n+    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n+    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n+\n+\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs: Unpack[TransformersKwargs],\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n+class Lfm2MoeAttention(nn.Module):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    def __init__(self, config: Lfm2MoeConfig, layer_idx: int):\n+        super().__init__()\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n+        self.scaling = self.head_dim**-0.5\n+        self.is_causal = True\n+        self.q_proj = nn.Linear(config.hidden_size, config.num_attention_heads * self.head_dim, bias=False)\n+        self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n+        self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n+        self.out_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n+        self.q_layernorm = Lfm2MoeRMSNorm(self.head_dim, eps=config.norm_eps)\n+        self.k_layernorm = Lfm2MoeRMSNorm(self.head_dim, eps=config.norm_eps)\n+\n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n+        past_key_values: Optional[Lfm2MoeHybridConvCache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+\n+        query_states = self.q_layernorm(self.q_proj(hidden_states).view(*hidden_shape)).transpose(1, 2)\n+        key_states = self.k_layernorm(self.k_proj(hidden_states).view(*hidden_shape)).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(*hidden_shape).transpose(1, 2)\n+\n+        cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        if past_key_values is not None:\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0,\n+            scaling=self.scaling,\n+            **kwargs,\n+        )\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        output = self.out_proj(attn_output)\n+        return output, attn_weights\n+\n+\n+def apply_mask_to_padding_states(hidden_states, attention_mask):\n+    \"\"\"\n+    Tunes out the hidden states for padding tokens, see https://github.com/state-spaces/mamba/issues/66\n+    \"\"\"\n+    if attention_mask is not None and attention_mask.shape[1] > 1 and attention_mask.shape[0] > 1:\n+        dtype = hidden_states.dtype\n+        hidden_states = (hidden_states * attention_mask[:, :, None]).to(dtype)\n+\n+    return hidden_states\n+\n+\n+kernel_modules = (causal_conv1d_fn, causal_conv1d_update)\n+is_fast_path_available = all(kernel_modules)\n+\n+\n+class Lfm2MoeShortConv(nn.Module):\n+    def __init__(\n+        self,\n+        config: Lfm2MoeConfig,\n+        layer_idx: int,\n+    ):\n+        super().__init__()\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        self.L_cache = config.conv_L_cache\n+        self.bias = config.conv_bias\n+\n+        self.conv = nn.Conv1d(\n+            in_channels=config.hidden_size,\n+            out_channels=config.hidden_size,\n+            kernel_size=self.L_cache,\n+            groups=config.hidden_size,\n+            bias=self.bias,\n+            padding=self.L_cache - 1,\n+        )\n+        self.in_proj = nn.Linear(config.hidden_size, 3 * config.hidden_size, bias=self.bias)\n+        self.out_proj = nn.Linear(config.hidden_size, config.hidden_size, bias=self.bias)\n+\n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n+    def cuda_kernels_forward(\n+        self,\n+        x: torch.Tensor,\n+        past_key_values: Optional[Lfm2MoeHybridConvCache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+    ):\n+        x = apply_mask_to_padding_states(x, attention_mask)\n+        BCx = self.in_proj(x).transpose(-1, -2)\n+        B, C, x = BCx.chunk(3, dim=-2)\n+\n+        Bx = B * x\n+\n+        conv_weights = self.conv.weight.view(self.conv.weight.size(0), self.conv.weight.size(2))\n+        if past_key_values is not None and cache_position[0] > 0:\n+            conv_out = causal_conv1d_update(\n+                Bx.squeeze(-1),\n+                past_key_values.conv_cache[self.layer_idx],\n+                conv_weights,\n+                self.conv.bias,\n+                None,\n+            )\n+            conv_out = conv_out.unsqueeze(-1)\n+        else:\n+            if past_key_values is not None:\n+                conv_state = nn.functional.pad(Bx, (self.L_cache - Bx.shape[-1], 0))\n+                past_key_values.conv_cache[self.layer_idx].copy_(conv_state)\n+\n+            conv_out = causal_conv1d_fn(Bx, conv_weights, self.conv.bias, activation=None)\n+\n+        y = C * conv_out\n+        y = self.out_proj(y.transpose(-1, -2).contiguous())\n+        return y\n+\n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n+    def slow_forward(\n+        self,\n+        x: torch.Tensor,\n+        past_key_values: Optional[Lfm2MoeHybridConvCache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+    ):\n+        seqlen = x.shape[1]\n+\n+        x = apply_mask_to_padding_states(x, attention_mask)\n+        BCx = self.in_proj(x).transpose(-1, -2)\n+        B, C, x = BCx.chunk(3, dim=-2)\n+\n+        Bx = B * x\n+\n+        if past_key_values is not None and cache_position[0] > 0:\n+            conv_state = past_key_values.conv_cache[self.layer_idx]\n+            cache_position = cache_position.clamp(0, self.L_cache - 1)\n+            conv_state = conv_state.roll(shifts=-1, dims=-1)\n+            conv_state[:, :, cache_position] = Bx.to(device=conv_state.device, dtype=conv_state.dtype)\n+            past_key_values.conv_cache[self.layer_idx].copy_(conv_state)\n+            conv_out = torch.sum(conv_state.to(Bx.device) * self.conv.weight[:, 0, :], dim=-1)\n+            if self.bias:\n+                conv_out += self.conv.bias\n+\n+            conv_out = conv_out.unsqueeze(-1)\n+        else:\n+            if past_key_values is not None:\n+                conv_state = nn.functional.pad(Bx, (self.L_cache - Bx.shape[-1], 0))\n+                past_key_values.conv_cache[self.layer_idx].copy_(conv_state)\n+\n+            conv_out = self.conv(Bx)[..., :seqlen]\n+\n+        y = C * conv_out\n+        y = y.transpose(-1, -2).contiguous()\n+        y = self.out_proj(y)\n+        return y\n+\n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        past_key_values: Optional[Lfm2MoeHybridConvCache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+    ):\n+        if is_fast_path_available and \"cuda\" in hidden_states.device.type and not torch._dynamo.is_compiling():\n+            return self.cuda_kernels_forward(hidden_states, past_key_values, cache_position, attention_mask)\n+        return self.slow_forward(hidden_states, past_key_values, cache_position, attention_mask)\n+\n+\n+class Lfm2MoeDecoderLayer(GradientCheckpointingLayer):\n+    def __init__(self, config: Lfm2MoeConfig, layer_idx: int):\n+        super().__init__()\n+        self.is_attention_layer = config.layer_types[layer_idx] == \"full_attention\"\n+\n+        if self.is_attention_layer:\n+            self.self_attn = Lfm2MoeAttention(config, layer_idx)\n+        else:\n+            self.conv = Lfm2MoeShortConv(config, layer_idx)\n+        self.feed_forward = (\n+            Lfm2MoeMLP(config, intermediate_size=config.intermediate_size)\n+            if layer_idx < config.num_dense_layers\n+            else Lfm2MoeSparseMoeBlock(config)\n+        )\n+        self.operator_norm = Lfm2MoeRMSNorm(config.hidden_size, eps=config.norm_eps)\n+        self.ffn_norm = Lfm2MoeRMSNorm(config.hidden_size, eps=config.norm_eps)\n+\n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Lfm2MoeHybridConvCache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n+    ) -> torch.Tensor:\n+        residual = hidden_states\n+        if self.is_attention_layer:\n+            hidden_states, _ = self.self_attn(\n+                hidden_states=self.operator_norm(hidden_states),\n+                position_embeddings=position_embeddings,\n+                attention_mask=attention_mask,\n+                position_ids=position_ids,\n+                past_key_values=past_key_values,\n+                cache_position=cache_position,\n+                **kwargs,\n+            )\n+        else:\n+            hidden_states = self.conv(\n+                hidden_states=self.operator_norm(hidden_states),\n+                past_key_values=past_key_values,\n+                cache_position=cache_position,\n+                attention_mask=attention_mask,\n+            )\n+        hidden_states = hidden_states + residual\n+        hidden_states = hidden_states + self.feed_forward(self.ffn_norm(hidden_states))\n+\n+        return hidden_states\n+\n+\n+@auto_docstring\n+class Lfm2MoePreTrainedModel(PreTrainedModel):\n+    config: Lfm2MoeConfig\n+    base_model_prefix = \"model\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"Lfm2MoeDecoderLayer\"]\n+    _skip_keys_device_placement = [\"past_key_values\"]\n+    _supports_flash_attn = True\n+    _supports_sdpa = True\n+    _supports_flex_attn = True\n+    _can_compile_fullgraph = False\n+    _supports_attention_backend = True\n+    _can_record_outputs = {\n+        \"hidden_states\": Lfm2MoeDecoderLayer,\n+        \"attentions\": Lfm2MoeAttention,\n+    }\n+\n+\n+@auto_docstring\n+class Lfm2MoeModel(Lfm2MoePreTrainedModel):\n+    def __init__(self, config: Lfm2MoeConfig):\n+        super().__init__(config)\n+        self.padding_idx = config.pad_token_id\n+        self.vocab_size = config.vocab_size\n+\n+        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n+        self.layers = nn.ModuleList(\n+            [Lfm2MoeDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+        self.gradient_checkpointing = False\n+        self.pos_emb = Lfm2MoeRotaryEmbedding(config)\n+        self.embedding_norm = Lfm2MoeRMSNorm(config.hidden_size, eps=config.norm_eps)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @check_model_inputs()\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Lfm2MoeHybridConvCache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> MoeModelOutputWithPast:\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.embed_tokens(input_ids)\n+\n+        if use_cache and past_key_values is None:\n+            batch_size = inputs_embeds.shape[0]\n+            past_key_values = Lfm2MoeHybridConvCache(\n+                config=self.config, max_batch_size=batch_size, dtype=self.dtype, device=self.device\n+            )\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            position_ids=position_ids,\n+        )\n+\n+        hidden_states = inputs_embeds\n+        position_embeddings = self.pos_emb(hidden_states, position_ids)\n+\n+        # decoder layers\n+        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n+            hidden_states = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_values=past_key_values,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                **kwargs,\n+            )\n+\n+        hidden_states = self.embedding_norm(hidden_states)\n+\n+        return MoeModelOutputWithPast(\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values,\n+        )\n+\n+\n+@auto_docstring\n+class Lfm2MoeForCausalLM(Lfm2MoePreTrainedModel, GenerationMixin):\n+    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tp_plan = {\"lm_head\": \"colwise_rep\"}\n+    _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.model = Lfm2MoeModel(config)\n+        self.vocab_size = config.vocab_size\n+        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> CausalLMOutputWithPast:\n+        r\"\"\"\n+        Example:\n+\n+        ```python\n+        >>> from transformers import AutoTokenizer, Lfm2MoeForCausalLM\n+\n+        >>> model = Lfm2MoeForCausalLM.from_pretrained(\"meta-lfm2_moe/Lfm2Moe-2-7b-hf\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"meta-lfm2_moe/Lfm2Moe-2-7b-hf\")\n+\n+        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n+        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n+\n+        >>> # Generate\n+        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n+        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n+        ```\"\"\"\n+        outputs: BaseModelOutputWithPast = self.model(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        hidden_states = outputs.last_hidden_state\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n+\n+        return CausalLMOutputWithPast(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+\n+__all__ = [\"Lfm2MoeForCausalLM\", \"Lfm2MoeModel\", \"Lfm2MoePreTrainedModel\"]"
      },
      {
        "filename": "src/transformers/models/lfm2_moe/modular_lfm2_moe.py",
        "status": "added",
        "additions": 204,
        "deletions": 0,
        "changes": 204,
        "patch": "@@ -0,0 +1,204 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import Optional\n+\n+import torch\n+from torch import nn\n+\n+from ...masking_utils import create_causal_mask\n+from ...modeling_outputs import MoeModelOutputWithPast\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, logging\n+from ...utils.import_utils import is_causal_conv1d_available\n+from ..lfm2.modeling_lfm2 import Lfm2Attention, Lfm2DecoderLayer, Lfm2HybridConvCache, Lfm2MLP, Lfm2ShortConv\n+from ..llama.modeling_llama import LlamaForCausalLM, LlamaPreTrainedModel, LlamaRMSNorm, LlamaRotaryEmbedding\n+from ..mixtral.modeling_mixtral import MixtralModel\n+from ..qwen2_moe.modeling_qwen2_moe import Qwen2MoeExperts\n+from .configuration_lfm2_moe import Lfm2MoeConfig\n+\n+\n+if is_causal_conv1d_available():\n+    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\n+else:\n+    causal_conv1d_fn, causal_conv1d_update = None, None\n+\n+\n+kernel_modules = (causal_conv1d_fn, causal_conv1d_update)\n+is_fast_path_available = all(kernel_modules)\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class Lfm2MoeRMSNorm(LlamaRMSNorm):\n+    pass\n+\n+\n+class Lfm2MoeRotaryEmbedding(LlamaRotaryEmbedding):\n+    pass\n+\n+\n+class Lfm2MoeMLP(Lfm2MLP):\n+    def __init__(self, config: Lfm2MoeConfig, intermediate_size: Optional[int] = None):\n+        nn.Module.__init__(self)\n+        self.hidden_size = config.hidden_size\n+        self.intermediate_size = config.intermediate_size if intermediate_size is None else intermediate_size\n+        self.w1 = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.w3 = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.w2 = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n+\n+\n+class Lfm2MoeExperts(Qwen2MoeExperts):\n+    pass\n+\n+\n+class Lfm2MoeSparseMoeBlock(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.top_k = config.num_experts_per_tok\n+        self.routed_scaling_factor = config.routed_scaling_factor\n+        self.norm_topk_prob = config.norm_topk_prob\n+        self.use_expert_bias = config.use_expert_bias\n+\n+        self.gate = nn.Linear(config.hidden_size, config.num_experts, bias=False)\n+        self.experts = Lfm2MoeExperts(config)\n+        if self.use_expert_bias:\n+            self.register_buffer(\"expert_bias\", torch.zeros(config.num_experts, dtype=torch.float32))\n+\n+    def route_tokens_to_experts(self, router_logits):\n+        routing_weights = router_logits.sigmoid()\n+        if self.use_expert_bias:\n+            scores_for_routing = routing_weights + self.expert_bias\n+            _, selected_experts = torch.topk(scores_for_routing, k=self.top_k, dim=-1)\n+            routing_weights = torch.gather(routing_weights, dim=1, index=selected_experts).type_as(router_logits)\n+        else:\n+            routing_weights, selected_experts = torch.topk(routing_weights, k=self.top_k, dim=-1)\n+\n+        if self.norm_topk_prob:\n+            routing_weights = routing_weights / (routing_weights.sum(dim=-1, keepdim=True) + 1e-6)\n+        routing_weights = routing_weights * self.routed_scaling_factor\n+        return selected_experts, routing_weights\n+\n+    def forward(self, hidden_states: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n+        batch_size, sequence_length, hidden_dim = hidden_states.shape\n+        hidden_states_reshaped = hidden_states.view(-1, hidden_dim)\n+        router_logits = self.gate(hidden_states_reshaped)\n+        selected_experts, routing_weights = self.route_tokens_to_experts(router_logits)\n+        final_hidden_states = self.experts(hidden_states_reshaped, selected_experts, routing_weights)\n+        return final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n+\n+\n+class Lfm2MoeHybridConvCache(Lfm2HybridConvCache):\n+    pass\n+\n+\n+class Lfm2MoeAttention(Lfm2Attention):\n+    pass\n+\n+\n+class Lfm2MoeShortConv(Lfm2ShortConv):\n+    pass\n+\n+\n+class Lfm2MoeDecoderLayer(Lfm2DecoderLayer):\n+    def __init__(self, config: Lfm2MoeConfig, layer_idx: int):\n+        super().__init__(config, layer_idx)\n+        self.feed_forward = (\n+            Lfm2MoeMLP(config, intermediate_size=config.intermediate_size)\n+            if layer_idx < config.num_dense_layers\n+            else Lfm2MoeSparseMoeBlock(config)\n+        )\n+\n+\n+class Lfm2MoePreTrainedModel(LlamaPreTrainedModel):\n+    _can_compile_fullgraph = False\n+\n+\n+class Lfm2MoeModel(MixtralModel):\n+    def __init__(self, config: Lfm2MoeConfig):\n+        super().__init__(config)\n+        self.pos_emb = Lfm2MoeRotaryEmbedding(config)\n+        self.embedding_norm = Lfm2MoeRMSNorm(config.hidden_size, eps=config.norm_eps)\n+        del self.norm\n+        del self.rotary_emb\n+\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Lfm2MoeHybridConvCache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> MoeModelOutputWithPast:\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.embed_tokens(input_ids)\n+\n+        if use_cache and past_key_values is None:\n+            batch_size = inputs_embeds.shape[0]\n+            past_key_values = Lfm2MoeHybridConvCache(\n+                config=self.config, max_batch_size=batch_size, dtype=self.dtype, device=self.device\n+            )\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            position_ids=position_ids,\n+        )\n+\n+        hidden_states = inputs_embeds\n+        position_embeddings = self.pos_emb(hidden_states, position_ids)\n+\n+        # decoder layers\n+        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n+            hidden_states = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_values=past_key_values,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                **kwargs,\n+            )\n+\n+        hidden_states = self.embedding_norm(hidden_states)\n+\n+        return MoeModelOutputWithPast(\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values,\n+        )\n+\n+\n+class Lfm2MoeForCausalLM(LlamaForCausalLM):\n+    pass\n+\n+\n+__all__ = [\"Lfm2MoeForCausalLM\", \"Lfm2MoeModel\", \"Lfm2MoePreTrainedModel\"]"
      },
      {
        "filename": "tests/causal_lm_tester.py",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -448,6 +448,7 @@ def test_model_rope_scaling_frequencies(self):\n         # named location of the RoPE layer class.\n         base_model = self.model_tester.base_model_class(config)\n         possible_rope_attributes = [\n+            \"pos_emb\",\n             \"rotary_emb\",  # most common case\n             \"global_rotary_emb\",\n             \"local_rotary_emb\","
      },
      {
        "filename": "tests/models/lfm2/test_modeling_lfm2.py",
        "status": "modified",
        "additions": 77,
        "deletions": 14,
        "changes": 91,
        "patch": "@@ -23,12 +23,15 @@\n     require_torch,\n     require_torch_accelerator,\n     slow,\n+    torch_device,\n )\n \n from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n \n \n if is_torch_available():\n+    import torch\n+\n     from transformers import Lfm2ForCausalLM, Lfm2Model\n \n \n@@ -60,22 +63,82 @@ class Lfm2ModelTest(CausalLMModelTest, unittest.TestCase):\n     # used in `test_torch_compile_for_training`\n     _torch_compile_train_cls = Lfm2ForCausalLM if is_torch_available() else None\n \n-    @unittest.skip(\n-        \"Lfm2 alternates between attention and conv layers, so attention are only returned for attention layers\"\n-    )\n     def test_attention_outputs(self):\n-        pass\n-\n-    @unittest.skip(\"Lfm2 has a special cache format as it alternates between attention and conv layers\")\n+        \"\"\"Lfm2Moe alternates between attention and short-conv layers.\"\"\"\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        config.return_dict = True\n+        # force eager attention to support output attentions\n+        config._attn_implementation = \"eager\"\n+        seq_len = getattr(self.model_tester, \"seq_length\", None)\n+\n+        for model_class in self.all_model_classes:\n+            inputs_dict[\"output_attentions\"] = True\n+            inputs_dict[\"output_hidden_states\"] = False\n+            config.return_dict = True\n+            model = model_class._from_config(config, attn_implementation=\"eager\").to(torch_device).eval()\n+            config = model.config\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+            attentions = outputs.attentions\n+            self.assertEqual(len(attentions), sum(layer == \"full_attention\" for layer in config.layer_types))\n+\n+            # check that output_attentions also work using config\n+            del inputs_dict[\"output_attentions\"]\n+            config.output_attentions = True\n+            model = model_class(config).to(torch_device).eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+            attentions = outputs.attentions\n+            self.assertEqual(len(attentions), sum(layer == \"full_attention\" for layer in config.layer_types))\n+            self.assertListEqual(list(attentions[0].shape[-3:]), [config.num_attention_heads, seq_len, seq_len])\n+            out_len = len(outputs)\n+\n+            # Check attention is always last and order is fine\n+            inputs_dict[\"output_attentions\"] = True\n+            inputs_dict[\"output_hidden_states\"] = True\n+            model = model_class(config).to(torch_device).eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+                self_attentions = outputs.attentions\n+\n+            self.assertEqual(out_len + 1, len(outputs))\n+            self.assertEqual(len(self_attentions), sum(layer == \"full_attention\" for layer in config.layer_types))\n+            self.assertListEqual(list(self_attentions[0].shape[-3:]), [config.num_attention_heads, seq_len, seq_len])\n+\n+    @pytest.mark.generate\n     def test_past_key_values_format(self):\n-        pass\n-\n-    @unittest.skip(\n-        \"Lfm2 has a special cache format which is not compatible with compile as it has static address for conv cache\"\n-    )\n-    @pytest.mark.torch_compile_test\n-    def test_sdpa_can_compile_dynamic(self):\n-        pass\n+        \"\"\"Lfm2Moe has a special cache format as it alternates between attention and conv layers\"\"\"\n+        for model_class in self.all_generative_model_classes:\n+            config, inputs = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+            model = model_class(config).to(torch_device).eval()\n+            if \"use_cache\" not in inputs:\n+                inputs[\"use_cache\"] = True\n+            outputs = model(**inputs)\n+\n+            past_kv = outputs[\"past_key_values\"]\n+\n+            num_query_attention_heads = config.num_attention_heads\n+            embed_dim = config.hidden_size\n+            per_head_embed_dim = embed_dim // num_query_attention_heads\n+            num_key_value_heads = getattr(config, \"num_key_value_heads\", num_query_attention_heads)\n+\n+            batch_size, seq_length = inputs[\"input_ids\"].shape[:2]\n+            default_self_attention_shape = (batch_size, num_key_value_heads, seq_length, per_head_embed_dim)\n+            default_conv_shape = (batch_size, config.hidden_size, config.conv_L_cache)\n+\n+            num_cache_decoder_layers = len(past_kv)\n+            self.assertEqual(num_cache_decoder_layers, config.num_hidden_layers)\n+\n+            for i in range(config.num_hidden_layers):\n+                if config.layer_types[i] == \"full_attention\":\n+                    self_attention_layer_keys = past_kv.key_cache[i]\n+                    self_attention_layer_values = past_kv.value_cache[i]\n+                    self.assertEqual(self_attention_layer_keys.shape, default_self_attention_shape)\n+                    self.assertEqual(self_attention_layer_values.shape, default_self_attention_shape)\n+                else:\n+                    conv_layer = past_kv.conv_cache[i]\n+                    self.assertEqual(conv_layer.shape, default_conv_shape)\n \n \n @require_torch_accelerator"
      },
      {
        "filename": "tests/models/lfm2_moe/__init__.py",
        "status": "added",
        "additions": 0,
        "deletions": 0,
        "changes": 0,
        "patch": ""
      },
      {
        "filename": "tests/models/lfm2_moe/test_modeling_lfm2_moe.py",
        "status": "added",
        "additions": 246,
        "deletions": 0,
        "changes": 246,
        "patch": "@@ -0,0 +1,246 @@\n+# coding=utf-8\n+# Copyright 2025 the HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch LLaMA model.\"\"\"\n+\n+import unittest\n+\n+import pytest\n+\n+from transformers import AutoTokenizer, is_torch_available, set_seed\n+from transformers.testing_utils import (\n+    cleanup,\n+    require_read_token,\n+    require_torch,\n+    require_torch_accelerator,\n+    slow,\n+    torch_device,\n+)\n+\n+from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+    from transformers import Lfm2MoeConfig, Lfm2MoeForCausalLM, Lfm2MoeModel\n+\n+\n+class Lfm2MoeModelTester(CausalLMModelTester):\n+    if is_torch_available():\n+        config_class = Lfm2MoeConfig\n+        base_model_class = Lfm2MoeModel\n+        causal_lm_class = Lfm2MoeForCausalLM\n+\n+    def __init__(\n+        self,\n+        parent,\n+        layer_types=[\"full_attention\", \"conv\"],\n+    ):\n+        super().__init__(parent)\n+        self.layer_types = layer_types\n+\n+\n+@require_torch\n+class Lfm2MoeModelTest(CausalLMModelTest, unittest.TestCase):\n+    all_model_classes = (Lfm2MoeModel, Lfm2MoeForCausalLM) if is_torch_available() else ()\n+    pipeline_model_mapping = (\n+        {\n+            \"feature-extraction\": Lfm2MoeModel,\n+            \"text-generation\": Lfm2MoeForCausalLM,\n+        }\n+        if is_torch_available()\n+        else {}\n+    )\n+    test_headmasking = False\n+    test_pruning = False\n+    fx_compatible = False\n+    model_tester_class = Lfm2MoeModelTester\n+    # used in `test_torch_compile_for_training`\n+    _torch_compile_train_cls = Lfm2MoeForCausalLM if is_torch_available() else None\n+\n+    def test_attention_outputs(self):\n+        \"\"\"Lfm2Moe alternates between attention and short-conv layers.\"\"\"\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        config.return_dict = True\n+        # force eager attention to support output attentions\n+        config._attn_implementation = \"eager\"\n+        seq_len = getattr(self.model_tester, \"seq_length\", None)\n+\n+        for model_class in self.all_model_classes:\n+            inputs_dict[\"output_attentions\"] = True\n+            inputs_dict[\"output_hidden_states\"] = False\n+            config.return_dict = True\n+            model = model_class._from_config(config, attn_implementation=\"eager\").to(torch_device).eval()\n+            config = model.config\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+            attentions = outputs.attentions\n+            self.assertEqual(len(attentions), sum(layer == \"full_attention\" for layer in config.layer_types))\n+\n+            # check that output_attentions also work using config\n+            del inputs_dict[\"output_attentions\"]\n+            config.output_attentions = True\n+            model = model_class(config).to(torch_device).eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+            attentions = outputs.attentions\n+            self.assertEqual(len(attentions), sum(layer == \"full_attention\" for layer in config.layer_types))\n+            self.assertListEqual(list(attentions[0].shape[-3:]), [config.num_attention_heads, seq_len, seq_len])\n+            out_len = len(outputs)\n+\n+            # Check attention is always last and order is fine\n+            inputs_dict[\"output_attentions\"] = True\n+            inputs_dict[\"output_hidden_states\"] = True\n+            model = model_class(config).to(torch_device).eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+                self_attentions = outputs.attentions\n+\n+            self.assertEqual(out_len + 1, len(outputs))\n+            self.assertEqual(len(self_attentions), sum(layer == \"full_attention\" for layer in config.layer_types))\n+            self.assertListEqual(list(self_attentions[0].shape[-3:]), [config.num_attention_heads, seq_len, seq_len])\n+\n+    @pytest.mark.generate\n+    def test_past_key_values_format(self):\n+        \"\"\"Lfm2Moe has a special cache format as it alternates between attention and conv layers\"\"\"\n+        for model_class in self.all_generative_model_classes:\n+            config, inputs = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+            model = model_class(config).to(torch_device).eval()\n+            if \"use_cache\" not in inputs:\n+                inputs[\"use_cache\"] = True\n+            outputs = model(**inputs)\n+\n+            past_kv = outputs[\"past_key_values\"]\n+\n+            num_query_attention_heads = config.num_attention_heads\n+            embed_dim = config.hidden_size\n+            per_head_embed_dim = embed_dim // num_query_attention_heads\n+            num_key_value_heads = getattr(config, \"num_key_value_heads\", num_query_attention_heads)\n+\n+            batch_size, seq_length = inputs[\"input_ids\"].shape[:2]\n+            default_self_attention_shape = (batch_size, num_key_value_heads, seq_length, per_head_embed_dim)\n+            default_conv_shape = (batch_size, config.hidden_size, config.conv_L_cache)\n+\n+            num_cache_decoder_layers = len(past_kv)\n+            self.assertEqual(num_cache_decoder_layers, config.num_hidden_layers)\n+\n+            for i in range(config.num_hidden_layers):\n+                if config.layer_types[i] == \"full_attention\":\n+                    self_attention_layer_keys = past_kv.key_cache[i]\n+                    self_attention_layer_values = past_kv.value_cache[i]\n+                    self.assertEqual(self_attention_layer_keys.shape, default_self_attention_shape)\n+                    self.assertEqual(self_attention_layer_values.shape, default_self_attention_shape)\n+                else:\n+                    conv_layer = past_kv.conv_cache[i]\n+                    self.assertEqual(conv_layer.shape, default_conv_shape)\n+\n+\n+@require_torch_accelerator\n+@require_read_token\n+@slow\n+class Lfm2MoeIntegrationTest(unittest.TestCase):\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.model = None\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        del cls.model\n+        cleanup(torch_device, gc_collect=True)\n+\n+    def tearDown(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n+    @classmethod\n+    def get_model(cls):\n+        if cls.model is None:\n+            cls.model = Lfm2MoeForCausalLM.from_pretrained(\n+                \"LiquidAI/LFM2-8B-A1B\", device_map=\"auto\", dtype=torch.bfloat16\n+            )\n+        return cls.model\n+\n+    @slow\n+    def test_model_1a8b_logits(self):\n+        set_seed(1789)\n+        input_ids = [1, 22998, 768, 1947, 797, 22017, 811, 6332, 928, 5743, 797, 779, 48123, 772, 33551, 60996, 523]\n+        model = self.get_model()\n+        input_ids = torch.tensor([input_ids]).to(model.model.embed_tokens.weight.device)\n+        with torch.no_grad():\n+            out = model(input_ids).logits.float().cpu()\n+        # Expected mean on dim = -1\n+        EXPECTED_MEAN = torch.tensor(\n+            [\n+                [\n+                    -1.3855,\n+                    -0.5123,\n+                    -1.3143,\n+                    -1.2144,\n+                    -1.0791,\n+                    -1.2117,\n+                    -1.4704,\n+                    -0.7648,\n+                    -0.6175,\n+                    -1.2402,\n+                    -1.1459,\n+                    -1.0083,\n+                    -1.0247,\n+                    -0.8830,\n+                    -1.5643,\n+                    -1.7266,\n+                    -1.6254,\n+                ]\n+            ]\n+        )\n+        torch.testing.assert_close(out.mean(-1), EXPECTED_MEAN, rtol=1e-2, atol=1e-2)\n+        # Expected portion of the logits\n+        EXPECTED_SLICE = torch.tensor(\n+            [-1.2656, 2.4844, 5.5000, -1.3359, -1.3203, -1.3438, 1.9375, 5.8438, -0.6523, -1.2891]\n+        )\n+        torch.testing.assert_close(out[0, 0, :10], EXPECTED_SLICE, rtol=1e-4, atol=1e-4)\n+\n+    @slow\n+    def test_model_1a8b_generation(self):\n+        EXPECTED_TEXT_COMPLETION = \"\"\"In 1st century A.D., the Roman Empire controlled much of Europe, North Africa, and parts of the Middle East.\"\"\"\n+        set_seed(1789)\n+        prompt = \"In 1st century A.D., the Roman Empire\"\n+        tokenizer = AutoTokenizer.from_pretrained(\"LiquidAI/LFM2-8B-A1B\", use_fast=False)\n+        model = self.get_model()\n+        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=True).to(\n+            model.model.embed_tokens.weight.device\n+        )\n+        with torch.no_grad():\n+            generated_ids = model.generate(input_ids, max_new_tokens=15, do_sample=False)\n+        text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n+        self.assertEqual(EXPECTED_TEXT_COMPLETION, text)\n+\n+    @slow\n+    def test_model_1a8b_batched_chat_generation(self):\n+        prompts = [\"Who are you?\", \"Complete the text: Lorem ipsum dolor \", \"The Meji Restoration in Japan ended\"]\n+        EXPECTED_TEXT_COMPLETIONS = [\n+            \"Who are you??  \\nI am an artificial intelligence assistant designed to provide information, answer questions\",\n+            \"Complete the text: Lorem ipsum dolor ipsum dolor ipsum dolor ipsum dolor ipsum dolor\",\n+            \"The Meji Restoration in Japan ended (1868) marked the:  \\nA) Establishment of a constitutional\",\n+        ]\n+        set_seed(1789)\n+        tokenizer = AutoTokenizer.from_pretrained(\"LiquidAI/LFM2-8B-A1B\", use_fast=False)\n+        model = self.get_model()\n+        batched_input_ids = tokenizer(prompts, return_tensors=\"pt\", padding=True).to(\n+            model.model.embed_tokens.weight.device\n+        )\n+        with torch.no_grad():\n+            generated_ids = model.generate(**batched_input_ids, max_new_tokens=15, do_sample=False)\n+        text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n+        self.assertEqual(EXPECTED_TEXT_COMPLETIONS, text)"
      },
      {
        "filename": "utils/check_config_attributes.py",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -36,6 +36,7 @@\n     \"Ernie4_5Config\": [\"tie_word_embeddings\"],\n     \"Ernie4_5_MoeConfig\": [\"tie_word_embeddings\"],\n     \"Lfm2Config\": [\"full_attn_idxs\", \"tie_word_embeddings\"],\n+    \"Lfm2MoeConfig\": [\"tie_word_embeddings\"],\n     # used internally during generation to provide the custom logit processors with their necessary information\n     \"DiaConfig\": [\n         \"delay_pattern\","
      }
    ],
    "num_files": 17,
    "scraped_at": "2025-11-16T21:18:12.158761"
  },
  {
    "pr_number": 41394,
    "title": "Adding superglue fast image processing",
    "body": "# What does this PR do?\r\n\r\nTLDR :\r\n- Implement fast processor for SuperGlue\r\n- About 3 times faster\r\n\r\nThis PR aims to translate the features of the class `SuperGlueImageProcessor` in the fast equivalent class `SuperGlueImageProcessorFast`.\r\nThe implementation heavily follows the standard implementation but reduces memory consumption and about 3 times the execution speed on my hardware.\r\nThe implementation mostly refactor the image formatting in the `preprocessing` step, notably by using torch tensors instead of PIL or Numpy.\r\n\r\n\r\n## Test Performed\r\nRUN_SLOW=1 python -m pytest tests/models/superglue/test_image_processing_superglue.py\r\n\r\nWith an additional test based on the default processor tester (this test has not to be included in the repo) :\r\n```python\r\n@require_vision\r\n@require_torch\r\ndef test_fast_is_faster_than_slow(self):\r\n    if not self.test_slow_image_processor or not self.test_fast_image_processor:\r\n        self.skipTest(reason=\"Skipping speed test\")\r\n\r\n    if self.image_processing_class is None or self.fast_image_processing_class is None:\r\n        self.skipTest(reason=\"Skipping speed test as one of the image processors is not defined\")\r\n\r\n    def measure_time(image_processor, image):\r\n        # Warmup\r\n        for _ in range(5):\r\n            _ = image_processor(image, return_tensors=\"pt\")\r\n        all_times = []\r\n        for _ in range(10):\r\n            start = time.time()\r\n            _ = image_processor(image, return_tensors=\"pt\")\r\n            all_times.append(time.time() - start)\r\n        # Take the average of the fastest 3 runs\r\n        avg_time = sum(sorted(all_times[:3])) / 3.0\r\n        return avg_time\r\n\r\n    dummy_images = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\r\n    image_processor_slow = self.image_processing_class(**self.image_processor_dict)\r\n    image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\r\n\r\n    fast_time = measure_time(image_processor_fast, dummy_images)\r\n    slow_time = measure_time(image_processor_slow, dummy_images)\r\n\r\n    self.assertLessEqual(fast_time, slow_time)\r\n```\r\nBy reviewing the flame graph, I noticed the improvement in every `__calls__` made to the fast version.\r\n\r\nCallers of the old processor, and the full execution time of the method:\r\n<img width=\"1239\" height=\"391\" alt=\"image\" src=\"https://github.com/user-attachments/assets/0fedbb2f-991c-481e-9425-b041b4f31767\" />\r\n\r\nThe equivalent but with the fast processor: \r\n<img width=\"1255\" height=\"385\" alt=\"image\" src=\"https://github.com/user-attachments/assets/ff658450-0c8c-459d-b58c-6da727555558\" />\r\n\r\nSome calls made during the test passes directly to the preprocess function, without passing by the `__call__` one, I am including them as well:\r\nSlow\r\n<img width=\"1150\" height=\"209\" alt=\"image\" src=\"https://github.com/user-attachments/assets/b763e4c8-0336-42fe-a1de-3611dc2b0f66\" />\r\nFast\r\n<img width=\"1230\" height=\"176\" alt=\"image\" src=\"https://github.com/user-attachments/assets/9979f600-e03b-4732-85eb-91a9ded018ee\" />\r\n\r\n## Before submitting\r\n- [    ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [ X ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),\r\n      Pull Request section?\r\n- [ X ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n           link :  [Contributions Welcome] Add Fast Image Processors https://github.com/huggingface/transformers/issues/36978#issue-2947632853\r\n- [ X ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [    ] Did you write any new necessary tests? \r\n\r\n\r\n## Who can review?\r\nThank you for reviewing my PR @yonigozlan (or anyone else :) )\r\n",
    "html_url": "https://github.com/huggingface/transformers/pull/41394",
    "created_at": "2025-10-06T22:29:36Z",
    "merged_at": "2025-10-16T19:34:10Z",
    "merge_commit_sha": "354567d955fbc5fbd70fc841b7a7bcc654bea3f1",
    "base_ref": "main",
    "head_sha": "4774877ad594a80466aab69c8938e5e96fb5ea55",
    "user": "AlphaOrOmega",
    "files": [
      {
        "filename": "docs/source/en/model_doc/superglue.md",
        "status": "modified",
        "additions": 11,
        "deletions": 4,
        "changes": 15,
        "patch": "@@ -88,16 +88,16 @@ processed_outputs = processor.post_process_keypoint_matching(outputs, image_size\n     import torch\n     from PIL import Image\n     import requests\n-    \n+\n     processor = AutoImageProcessor.from_pretrained(\"magic-leap-community/superglue_outdoor\")\n     model = AutoModel.from_pretrained(\"magic-leap-community/superglue_outdoor\")\n-    \n+\n     # SuperGlue requires pairs of images\n     images = [image1, image2]\n     inputs = processor(images, return_tensors=\"pt\")\n     with torch.inference_mode():\n         outputs = model(**inputs)\n-    \n+\n     # Extract matching information\n     keypoints0 = outputs.keypoints0  # Keypoints in first image\n     keypoints1 = outputs.keypoints1  # Keypoints in second image\n@@ -112,7 +112,7 @@ processed_outputs = processor.post_process_keypoint_matching(outputs, image_size\n     # Process outputs for visualization\n     image_sizes = [[(image.height, image.width) for image in images]]\n     processed_outputs = processor.post_process_keypoint_matching(outputs, image_sizes, threshold=0.2)\n-    \n+\n     for i, output in enumerate(processed_outputs):\n         print(f\"For the image pair {i}\")\n         for keypoint0, keypoint1, matching_score in zip(\n@@ -147,6 +147,13 @@ processed_outputs = processor.post_process_keypoint_matching(outputs, image_size\n     - post_process_keypoint_matching\n     - visualize_keypoint_matching\n \n+## SuperGlueImageProcessorFast\n+\n+[[autodoc]] SuperGlueImageProcessorFast\n+    - preprocess\n+    - post_process_keypoint_matching\n+    - visualize_keypoint_matching\n+\n ## SuperGlueForKeypointMatching\n \n [[autodoc]] SuperGlueForKeypointMatching"
      },
      {
        "filename": "src/transformers/models/auto/image_processing_auto.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -171,7 +171,7 @@\n             (\"siglip\", (\"SiglipImageProcessor\", \"SiglipImageProcessorFast\")),\n             (\"siglip2\", (\"Siglip2ImageProcessor\", \"Siglip2ImageProcessorFast\")),\n             (\"smolvlm\", (\"SmolVLMImageProcessor\", \"SmolVLMImageProcessorFast\")),\n-            (\"superglue\", (\"SuperGlueImageProcessor\", None)),\n+            (\"superglue\", (\"SuperGlueImageProcessor\", \"SuperGlueImageProcessorFast\")),\n             (\"superpoint\", (\"SuperPointImageProcessor\", \"SuperPointImageProcessorFast\")),\n             (\"swiftformer\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n             (\"swin\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),"
      },
      {
        "filename": "src/transformers/models/efficientloftr/image_processing_efficientloftr_fast.py",
        "status": "modified",
        "additions": 13,
        "deletions": 34,
        "changes": 47,
        "patch": "@@ -1,30 +1,17 @@\n-# coding=utf-8\n-# Copyright 2025 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Fast Image processor class for EfficientLoFTR.\"\"\"\n-\n-from typing import TYPE_CHECKING, Optional, Union\n+#                \ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\n+#           This file was automatically generated from src/transformers/models/efficientloftr/modular_efficientloftr.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_efficientloftr.py file directly. One of our CI enforces this.\n+#                \ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\n+from typing import Optional, Union\n \n import torch\n from PIL import Image, ImageDraw\n+from torchvision.transforms.v2 import functional as F\n \n-from ...image_processing_utils import BatchFeature\n-from ...image_processing_utils_fast import (\n-    BaseImageProcessorFast,\n-    group_images_by_shape,\n-    reorder_images,\n-)\n+from ...image_processing_utils_fast import BaseImageProcessorFast, BatchFeature\n+from ...image_transforms import group_images_by_shape, reorder_images\n from ...image_utils import (\n     ImageInput,\n     ImageType,\n@@ -35,17 +22,9 @@\n     is_valid_image,\n )\n from ...processing_utils import Unpack\n-from ...utils import (\n-    TensorType,\n-    auto_docstring,\n-)\n+from ...utils import TensorType, auto_docstring\n from .image_processing_efficientloftr import EfficientLoFTRImageProcessorKwargs\n-\n-\n-if TYPE_CHECKING:\n-    from .modeling_efficientloftr import KeypointMatchingOutput\n-\n-import torchvision.transforms.v2.functional as F\n+from .modeling_efficientloftr import KeypointMatchingOutput\n \n \n def _is_valid_image(image):\n@@ -299,7 +278,7 @@ def _get_color(self, score):\n         r = int(255 * (1 - score))\n         g = int(255 * score)\n         b = 0\n-        return (r, g, b)\n+        return r, g, b\n \n \n __all__ = [\"EfficientLoFTRImageProcessorFast\"]"
      },
      {
        "filename": "src/transformers/models/efficientloftr/modular_efficientloftr.py",
        "status": "added",
        "additions": 8,
        "deletions": 0,
        "changes": 8,
        "patch": "@@ -0,0 +1,8 @@\n+from ..superglue.image_processing_superglue_fast import SuperGlueImageProcessorFast\n+\n+\n+class EfficientLoFTRImageProcessorFast(SuperGlueImageProcessorFast):\n+    pass\n+\n+\n+__all__ = [\"EfficientLoFTRImageProcessorFast\"]"
      },
      {
        "filename": "src/transformers/models/superglue/__init__.py",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -20,6 +20,7 @@\n if TYPE_CHECKING:\n     from .configuration_superglue import *\n     from .image_processing_superglue import *\n+    from .image_processing_superglue_fast import *\n     from .modeling_superglue import *\n else:\n     import sys"
      },
      {
        "filename": "src/transformers/models/superglue/image_processing_superglue.py",
        "status": "modified",
        "additions": 10,
        "deletions": 0,
        "changes": 10,
        "patch": "@@ -35,6 +35,7 @@\n     valid_images,\n     validate_preprocess_arguments,\n )\n+from ...processing_utils import ImagesKwargs\n from ...utils import TensorType, logging, requires_backends\n from ...utils.import_utils import requires\n \n@@ -133,6 +134,15 @@ def _is_valid_image(image):\n     raise ValueError(error_message)\n \n \n+class SuperGlueImageProcessorKwargs(ImagesKwargs, total=False):\n+    r\"\"\"\n+    do_grayscale (`bool`, *optional*, defaults to `True`):\n+        Whether to convert the image to grayscale. Can be overridden by `do_grayscale` in the `preprocess` method.\n+    \"\"\"\n+\n+    do_grayscale: bool\n+\n+\n @requires(backends=(\"torch\",))\n class SuperGlueImageProcessor(BaseImageProcessor):\n     r\"\"\""
      },
      {
        "filename": "src/transformers/models/superglue/image_processing_superglue_fast.py",
        "status": "added",
        "additions": 292,
        "deletions": 0,
        "changes": 292,
        "patch": "@@ -0,0 +1,292 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import Optional, Union\n+\n+import torch\n+from PIL import Image, ImageDraw\n+from torchvision.transforms.v2 import functional as F\n+\n+from ...image_processing_utils_fast import BaseImageProcessorFast, BatchFeature\n+from ...image_transforms import group_images_by_shape, reorder_images\n+from ...image_utils import (\n+    ImageInput,\n+    ImageType,\n+    PILImageResampling,\n+    SizeDict,\n+    get_image_type,\n+    is_pil_image,\n+    is_valid_image,\n+)\n+from ...processing_utils import Unpack\n+from ...utils import TensorType, auto_docstring\n+from .image_processing_superglue import SuperGlueImageProcessorKwargs\n+from .modeling_superglue import KeypointMatchingOutput\n+\n+\n+def _is_valid_image(image):\n+    return is_pil_image(image) or (\n+        is_valid_image(image) and get_image_type(image) != ImageType.PIL and len(image.shape) == 3\n+    )\n+\n+\n+def flatten_pair_images(images):\n+    # Handle the pair validation and flattening similar to slow processor\n+    if isinstance(images, list):\n+        if len(images) == 2 and all((_is_valid_image(image) or isinstance(image, torch.Tensor)) for image in images):\n+            # Single pair of images - keep as is, they'll be processed by the base class\n+            return images\n+        elif all(\n+            isinstance(image_pair, list)\n+            and len(image_pair) == 2\n+            and all(_is_valid_image(image) or isinstance(image, torch.Tensor) for image in image_pair)\n+            for image_pair in images\n+        ):\n+            # Multiple pairs - flatten them\n+            images = [image for image_pair in images for image in image_pair]\n+            return images\n+    raise ValueError(\n+        \"Input images must be a one of the following :\",\n+        \" - A pair of PIL images.\",\n+        \" - A pair of 3D arrays.\",\n+        \" - A list of pairs of PIL images.\",\n+        \" - A list of pairs of 3D arrays.\",\n+    )\n+\n+\n+def is_grayscale(\n+    image: \"torch.Tensor\",\n+):\n+    \"\"\"Checks if an image is grayscale (all RGB channels are identical).\"\"\"\n+    if image.ndim < 3 or image.shape[0 if image.ndim == 3 else 1] == 1:\n+        return True\n+    return torch.all(image[..., 0, :, :] == image[..., 1, :, :]) and torch.all(\n+        image[..., 1, :, :] == image[..., 2, :, :]\n+    )\n+\n+\n+def convert_to_grayscale(\n+    image: \"torch.Tensor\",\n+) -> \"torch.Tensor\":\n+    \"\"\"\n+    Converts an image to grayscale format using the NTSC formula. Only support torch.Tensor.\n+\n+    This function is supposed to return a 1-channel image, but it returns a 3-channel image with the same value in each\n+    channel, because of an issue that is discussed in :\n+    https://github.com/huggingface/transformers/pull/25786#issuecomment-1730176446\n+\n+    Args:\n+        image (torch.Tensor):\n+            The image to convert.\n+    \"\"\"\n+    if is_grayscale(image):\n+        return image\n+    return F.rgb_to_grayscale(image, num_output_channels=3)\n+\n+\n+@auto_docstring\n+class SuperGlueImageProcessorFast(BaseImageProcessorFast):\n+    resample = PILImageResampling.BILINEAR\n+    size = {\"height\": 480, \"width\": 640}\n+    default_to_square = False\n+    do_resize = True\n+    do_rescale = True\n+    rescale_factor = 1 / 255\n+    do_normalize = None\n+    valid_kwargs = SuperGlueImageProcessorKwargs\n+\n+    def __init__(self, **kwargs: Unpack[SuperGlueImageProcessorKwargs]):\n+        super().__init__(**kwargs)\n+\n+    @auto_docstring\n+    def preprocess(self, images: ImageInput, **kwargs: Unpack[SuperGlueImageProcessorKwargs]) -> BatchFeature:\n+        return super().preprocess(images, **kwargs)\n+\n+    def _prepare_images_structure(\n+        self,\n+        images: ImageInput,\n+        **kwargs,\n+    ) -> ImageInput:\n+        # we need to handle image pairs validation and flattening\n+        return flatten_pair_images(images)\n+\n+    def _preprocess(\n+        self,\n+        images: list[\"torch.Tensor\"],\n+        size: Union[dict[str, int], SizeDict],\n+        rescale_factor: float,\n+        do_rescale: bool,\n+        do_resize: bool,\n+        interpolation: Optional[\"F.InterpolationMode\"],\n+        do_grayscale: bool,\n+        disable_grouping: bool,\n+        return_tensors: Union[str, TensorType],\n+        **kwargs,\n+    ) -> BatchFeature:\n+        grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n+        processed_images_grouped = {}\n+\n+        for shape, stacked_images in grouped_images.items():\n+            if do_resize:\n+                stacked_images = self.resize(stacked_images, size=size, interpolation=interpolation)\n+            processed_images_grouped[shape] = stacked_images\n+        resized_images = reorder_images(processed_images_grouped, grouped_images_index)\n+\n+        grouped_images, grouped_images_index = group_images_by_shape(resized_images, disable_grouping=disable_grouping)\n+        processed_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            if do_rescale:\n+                stacked_images = self.rescale(stacked_images, rescale_factor)\n+            if do_grayscale:\n+                stacked_images = convert_to_grayscale(stacked_images)\n+            processed_images_grouped[shape] = stacked_images\n+\n+        processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n+\n+        # Convert back to pairs format\n+        image_pairs = [processed_images[i : i + 2] for i in range(0, len(processed_images), 2)]\n+\n+        # Stack each pair into a single tensor to match slow processor format\n+        stacked_pairs = [torch.stack(pair, dim=0) for pair in image_pairs]\n+\n+        # Return in same format as slow processor\n+        image_pairs = torch.stack(stacked_pairs, dim=0) if return_tensors else stacked_pairs\n+\n+        return BatchFeature(data={\"pixel_values\": image_pairs})\n+\n+    def post_process_keypoint_matching(\n+        self,\n+        outputs: \"KeypointMatchingOutput\",\n+        target_sizes: Union[TensorType, list[tuple]],\n+        threshold: float = 0.0,\n+    ) -> list[dict[str, torch.Tensor]]:\n+        \"\"\"\n+        Converts the raw output of [`KeypointMatchingOutput`] into lists of keypoints, scores and descriptors\n+        with coordinates absolute to the original image sizes.\n+        Args:\n+            outputs ([`KeypointMatchingOutput`]):\n+                Raw outputs of the model.\n+            target_sizes (`torch.Tensor` or `List[Tuple[Tuple[int, int]]]`, *optional*):\n+                Tensor of shape `(batch_size, 2, 2)` or list of tuples of tuples (`Tuple[int, int]`) containing the\n+                target size `(height, width)` of each image in the batch. This must be the original image size (before\n+                any processing).\n+            threshold (`float`, *optional*, defaults to 0.0):\n+                Threshold to filter out the matches with low scores.\n+        Returns:\n+            `List[Dict]`: A list of dictionaries, each dictionary containing the keypoints in the first and second image\n+            of the pair, the matching scores and the matching indices.\n+        \"\"\"\n+        if outputs.matches.shape[0] != len(target_sizes):\n+            raise ValueError(\"Make sure that you pass in as many target sizes as the batch dimension of the mask\")\n+        if not all(len(target_size) == 2 for target_size in target_sizes):\n+            raise ValueError(\"Each element of target_sizes must contain the size (h, w) of each image of the batch\")\n+\n+        if isinstance(target_sizes, list):\n+            image_pair_sizes = torch.tensor(target_sizes, device=outputs.matches.device)\n+        else:\n+            if target_sizes.shape[1] != 2 or target_sizes.shape[2] != 2:\n+                raise ValueError(\n+                    \"Each element of target_sizes must contain the size (h, w) of each image of the batch\"\n+                )\n+            image_pair_sizes = target_sizes\n+\n+        keypoints = outputs.keypoints.clone()\n+        keypoints = keypoints * image_pair_sizes.flip(-1).reshape(-1, 2, 1, 2)\n+        keypoints = keypoints.to(torch.int32)\n+\n+        results = []\n+        for keypoints_pair, matches, scores in zip(keypoints, outputs.matches, outputs.matching_scores):\n+            # Filter out matches with low scores\n+            valid_matches = torch.logical_and(scores > threshold, matches > -1)\n+\n+            matched_keypoints0 = keypoints_pair[0][valid_matches[0]]\n+            matched_keypoints1 = keypoints_pair[1][valid_matches[1]]\n+            matching_scores = scores[0][valid_matches[0]]\n+\n+            results.append(\n+                {\n+                    \"keypoints0\": matched_keypoints0,\n+                    \"keypoints1\": matched_keypoints1,\n+                    \"matching_scores\": matching_scores,\n+                }\n+            )\n+\n+        return results\n+\n+    def visualize_keypoint_matching(\n+        self,\n+        images,\n+        keypoint_matching_output: list[dict[str, torch.Tensor]],\n+    ) -> list[\"Image.Image\"]:\n+        \"\"\"\n+        Plots the image pairs side by side with the detected keypoints as well as the matching between them.\n+\n+        Args:\n+            images:\n+                Image pairs to plot. Same as `EfficientLoFTRImageProcessor.preprocess`. Expects either a list of 2\n+                images or a list of list of 2 images list with pixel values ranging from 0 to 255.\n+            keypoint_matching_output (List[Dict[str, torch.Tensor]]]):\n+                A post processed keypoint matching output\n+\n+        Returns:\n+            `List[PIL.Image.Image]`: A list of PIL images, each containing the image pairs side by side with the detected\n+            keypoints as well as the matching between them.\n+        \"\"\"\n+        from ...image_utils import to_numpy_array\n+        from .image_processing_superglue import validate_and_format_image_pairs\n+\n+        images = validate_and_format_image_pairs(images)\n+        images = [to_numpy_array(image) for image in images]\n+        image_pairs = [images[i : i + 2] for i in range(0, len(images), 2)]\n+\n+        results = []\n+        for image_pair, pair_output in zip(image_pairs, keypoint_matching_output):\n+            height0, width0 = image_pair[0].shape[:2]\n+            height1, width1 = image_pair[1].shape[:2]\n+            plot_image = torch.zeros((max(height0, height1), width0 + width1, 3), dtype=torch.uint8)\n+            plot_image[:height0, :width0] = torch.from_numpy(image_pair[0])\n+            plot_image[:height1, width0:] = torch.from_numpy(image_pair[1])\n+\n+            plot_image_pil = Image.fromarray(plot_image.numpy())\n+            draw = ImageDraw.Draw(plot_image_pil)\n+\n+            keypoints0_x, keypoints0_y = pair_output[\"keypoints0\"].unbind(1)\n+            keypoints1_x, keypoints1_y = pair_output[\"keypoints1\"].unbind(1)\n+            for keypoint0_x, keypoint0_y, keypoint1_x, keypoint1_y, matching_score in zip(\n+                keypoints0_x, keypoints0_y, keypoints1_x, keypoints1_y, pair_output[\"matching_scores\"]\n+            ):\n+                color = self._get_color(matching_score)\n+                draw.line(\n+                    (keypoint0_x, keypoint0_y, keypoint1_x + width0, keypoint1_y),\n+                    fill=color,\n+                    width=3,\n+                )\n+                draw.ellipse((keypoint0_x - 2, keypoint0_y - 2, keypoint0_x + 2, keypoint0_y + 2), fill=\"black\")\n+                draw.ellipse(\n+                    (keypoint1_x + width0 - 2, keypoint1_y - 2, keypoint1_x + width0 + 2, keypoint1_y + 2),\n+                    fill=\"black\",\n+                )\n+\n+            results.append(plot_image_pil)\n+        return results\n+\n+    def _get_color(self, score):\n+        \"\"\"Maps a score to a color.\"\"\"\n+        r = int(255 * (1 - score))\n+        g = int(255 * score)\n+        b = 0\n+        return r, g, b\n+\n+\n+__all__ = [\"SuperGlueImageProcessorFast\"]"
      },
      {
        "filename": "tests/models/efficientloftr/test_image_processing_efficientloftr.py",
        "status": "modified",
        "additions": 0,
        "deletions": 67,
        "changes": 67,
        "patch": "@@ -15,19 +15,14 @@\n import unittest\n \n import numpy as np\n-import pytest\n-from packaging import version\n \n from tests.models.superglue.test_image_processing_superglue import (\n     SuperGlueImageProcessingTest,\n     SuperGlueImageProcessingTester,\n )\n from transformers.testing_utils import (\n     require_torch,\n-    require_torch_accelerator,\n     require_vision,\n-    slow,\n-    torch_device,\n )\n from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n@@ -103,46 +98,6 @@ def setUp(self) -> None:\n         super().setUp()\n         self.image_processor_tester = EfficientLoFTRImageProcessingTester(self)\n \n-    def test_slow_fast_equivalence(self):\n-        \"\"\"Override the generic test since EfficientLoFTR requires image pairs.\"\"\"\n-        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n-            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n-\n-        if self.image_processing_class is None or self.fast_image_processing_class is None:\n-            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n-\n-        # Create image pairs instead of single images\n-        dummy_images = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=False)\n-        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n-        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n-\n-        encoding_slow = image_processor_slow(dummy_images, return_tensors=\"pt\")\n-        encoding_fast = image_processor_fast(dummy_images, return_tensors=\"pt\")\n-        self._assert_slow_fast_tensors_equivalence(encoding_slow.pixel_values, encoding_fast.pixel_values)\n-\n-    def test_slow_fast_equivalence_batched(self):\n-        \"\"\"Override the generic test since EfficientLoFTR requires image pairs.\"\"\"\n-        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n-            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n-\n-        if self.image_processing_class is None or self.fast_image_processing_class is None:\n-            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n-\n-        if hasattr(self.image_processor_tester, \"do_center_crop\") and self.image_processor_tester.do_center_crop:\n-            self.skipTest(\n-                reason=\"Skipping as do_center_crop is True and center_crop functions are not equivalent for fast and slow processors\"\n-            )\n-\n-        # Create image pairs instead of single images\n-        dummy_images = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n-        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n-        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n-\n-        encoding_slow = image_processor_slow(dummy_images, return_tensors=\"pt\")\n-        encoding_fast = image_processor_fast(dummy_images, return_tensors=\"pt\")\n-\n-        self._assert_slow_fast_tensors_equivalence(encoding_slow.pixel_values, encoding_fast.pixel_values)\n-\n     @unittest.skip(reason=\"Many failing cases. This test needs a more deep investigation.\")\n     def test_fast_is_faster_than_slow(self):\n         \"\"\"Override the generic test since EfficientLoFTR requires image pairs.\"\"\"\n@@ -173,25 +128,3 @@ def test_fast_is_faster_than_slow(self):\n         self.assertLessEqual(\n             fast_time, slow_time * 1.2, \"Fast processor should not be significantly slower than slow processor\"\n         )\n-\n-    @slow\n-    @require_torch_accelerator\n-    @require_vision\n-    @pytest.mark.torch_compile_test\n-    def test_can_compile_fast_image_processor(self):\n-        \"\"\"Override the generic test since EfficientLoFTR requires image pairs.\"\"\"\n-        if self.fast_image_processing_class is None:\n-            self.skipTest(\"Skipping compilation test as fast image processor is not defined\")\n-        if version.parse(torch.__version__) < version.parse(\"2.3\"):\n-            self.skipTest(reason=\"This test requires torch >= 2.3 to run.\")\n-\n-        torch.compiler.reset()\n-        input_image = self.image_processor_tester.prepare_image_inputs(equal_resolution=True, torchify=False)\n-        image_processor = self.fast_image_processing_class(**self.image_processor_dict)\n-        output_eager = image_processor(input_image, device=torch_device, return_tensors=\"pt\")\n-\n-        image_processor = torch.compile(image_processor, mode=\"reduce-overhead\")\n-        output_compiled = image_processor(input_image, device=torch_device, return_tensors=\"pt\")\n-        self._assert_slow_fast_tensors_equivalence(\n-            output_eager.pixel_values, output_compiled.pixel_values, atol=1e-4, rtol=1e-4, mean_atol=1e-5\n-        )"
      },
      {
        "filename": "tests/models/lightglue/test_image_processing_lightglue.py",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -90,6 +90,7 @@ def prepare_keypoint_matching_output(self, pixel_values):\n @require_vision\n class LightGlueImageProcessingTest(SuperGlueImageProcessingTest, unittest.TestCase):\n     image_processing_class = LightGlueImageProcessor if is_vision_available() else None\n+    fast_image_processing_class = None\n \n     def setUp(self) -> None:\n         super().setUp()"
      },
      {
        "filename": "tests/models/superglue/test_image_processing_superglue.py",
        "status": "modified",
        "additions": 89,
        "deletions": 2,
        "changes": 91,
        "patch": "@@ -11,12 +11,22 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n+import time\n import unittest\n \n+import numpy as np\n+import pytest\n+from packaging import version\n from parameterized import parameterized\n \n-from transformers.testing_utils import require_torch, require_vision\n-from transformers.utils import is_torch_available, is_vision_available\n+from transformers.testing_utils import (\n+    require_torch,\n+    require_torch_accelerator,\n+    require_vision,\n+    slow,\n+    torch_device,\n+)\n+from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import (\n     ImageProcessingTestMixin,\n@@ -33,6 +43,9 @@\n if is_vision_available():\n     from transformers import SuperGlueImageProcessor\n \n+    if is_torchvision_available():\n+        from transformers import SuperGlueImageProcessorFast\n+\n \n def random_array(size):\n     return np.random.randint(255, size=size)\n@@ -119,6 +132,7 @@ def prepare_keypoint_matching_output(self, pixel_values):\n @require_vision\n class SuperGlueImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n     image_processing_class = SuperGlueImageProcessor if is_vision_available() else None\n+    fast_image_processing_class = SuperGlueImageProcessorFast if is_torchvision_available() else None\n \n     def setUp(self) -> None:\n         super().setUp()\n@@ -397,3 +411,76 @@ def check_post_processed_output(post_processed_output, image_pair_size):\n             tensor_post_processed_outputs = image_processor.post_process_keypoint_matching(outputs, tensor_image_sizes)\n \n             check_post_processed_output(tensor_post_processed_outputs, tensor_image_sizes)\n+\n+    @unittest.skip(reason=\"Many failing cases. This test needs a more deep investigation.\")\n+    def test_fast_is_faster_than_slow(self):\n+        \"\"\"Override the generic test since EfficientLoFTR requires image pairs.\"\"\"\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast speed test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast speed test as one of the image processors is not defined\")\n+\n+        # Create image pairs for speed test\n+        dummy_images = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=False)\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        # Time slow processor\n+        start_time = time.time()\n+        for _ in range(10):\n+            _ = image_processor_slow(dummy_images, return_tensors=\"pt\")\n+        slow_time = time.time() - start_time\n+\n+        # Time fast processor\n+        start_time = time.time()\n+        for _ in range(10):\n+            _ = image_processor_fast(dummy_images, return_tensors=\"pt\")\n+        fast_time = time.time() - start_time\n+\n+        # Fast should be faster (or at least not significantly slower)\n+        self.assertLessEqual(\n+            fast_time, slow_time * 1.2, \"Fast processor should not be significantly slower than slow processor\"\n+        )\n+\n+    @require_vision\n+    @require_torch\n+    def test_slow_fast_equivalence(self):\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n+\n+        dummy_image = self.image_processor_tester.prepare_image_inputs(\n+            equal_resolution=False, numpify=True, batch_size=2, pairs=False\n+        )\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        encoding_slow = image_processor_slow(dummy_image, return_tensors=\"pt\")\n+        encoding_fast = image_processor_fast(dummy_image, return_tensors=\"pt\")\n+\n+        self._assert_slow_fast_tensors_equivalence(encoding_slow.pixel_values, encoding_fast.pixel_values)\n+\n+    @slow\n+    @require_torch_accelerator\n+    @require_vision\n+    @pytest.mark.torch_compile_test\n+    def test_can_compile_fast_image_processor(self):\n+        \"\"\"Override the generic test since EfficientLoFTR requires image pairs.\"\"\"\n+        if self.fast_image_processing_class is None:\n+            self.skipTest(\"Skipping compilation test as fast image processor is not defined\")\n+        if version.parse(torch.__version__) < version.parse(\"2.3\"):\n+            self.skipTest(reason=\"This test requires torch >= 2.3 to run.\")\n+\n+        torch.compiler.reset()\n+        input_image = self.image_processor_tester.prepare_image_inputs(equal_resolution=True, torchify=False)\n+        image_processor = self.fast_image_processing_class(**self.image_processor_dict)\n+        output_eager = image_processor(input_image, device=torch_device, return_tensors=\"pt\")\n+\n+        image_processor = torch.compile(image_processor, mode=\"reduce-overhead\")\n+        output_compiled = image_processor(input_image, device=torch_device, return_tensors=\"pt\")\n+        self._assert_slow_fast_tensors_equivalence(\n+            output_eager.pixel_values, output_compiled.pixel_values, atol=1e-4, rtol=1e-4, mean_atol=1e-5\n+        )"
      }
    ],
    "num_files": 10,
    "scraped_at": "2025-11-16T21:18:13.456674"
  }
]