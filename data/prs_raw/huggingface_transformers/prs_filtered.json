[
  {
    "pr_number": 41997,
    "title": "Fix issue with from pretrained and kwargs in image processors",
    "body": "# What does this PR do?\r\nFixes https://github.com/huggingface/transformers/issues/41955.\r\nFixes an issue raised in https://github.com/huggingface/transformers/pull/41954. Instead of setting attributes from kwargs after instantiating the image processor in `from_pretrained`, we update the image processor dict with the kwargs before instantiating the object. This allows custom logic in the init to take into account the custom kwargs passed to `from_pretrained`.\r\n\r\nIn the PR linked, the issue was that `max_pixels` is supposed to overwrite `size[\"longest_edge\"] `when passed to the init, but in `from_pretrained`, `max_pixels` was never passed to the init and only set as an attribute after instantiating the image processor.",
    "html_url": "https://github.com/huggingface/transformers/pull/41997",
    "created_at": "2025-11-03T15:57:28Z",
    "merged_at": "2025-11-04T15:35:39Z",
    "merge_commit_sha": "900cf9d33bc091f3e47f8e598cba464f8b93bdd7",
    "base_ref": "main",
    "head_sha": "96a2a7085096b928c72a8d07652180c1c913fef3",
    "user": "yonigozlan",
    "files": [
      {
        "filename": "src/transformers/image_processing_base.py",
        "status": "modified",
        "additions": 4,
        "deletions": 16,
        "changes": 20,
        "patch": "@@ -362,25 +362,13 @@ def from_dict(cls, image_processor_dict: dict[str, Any], **kwargs):\n         \"\"\"\n         image_processor_dict = image_processor_dict.copy()\n         return_unused_kwargs = kwargs.pop(\"return_unused_kwargs\", False)\n-\n-        # The `size` parameter is a dict and was previously an int or tuple in feature extractors.\n-        # We set `size` here directly to the `image_processor_dict` so that it is converted to the appropriate\n-        # dict within the image processor and isn't overwritten if `size` is passed in as a kwarg.\n-        if \"size\" in kwargs and \"size\" in image_processor_dict:\n-            image_processor_dict[\"size\"] = kwargs.pop(\"size\")\n-        if \"crop_size\" in kwargs and \"crop_size\" in image_processor_dict:\n-            image_processor_dict[\"crop_size\"] = kwargs.pop(\"crop_size\")\n-\n+        image_processor_dict.update({k: v for k, v in kwargs.items() if k in cls.valid_kwargs.__annotations__})\n         image_processor = cls(**image_processor_dict)\n \n-        # Update image_processor with kwargs if needed\n-        to_remove = []\n-        for key, value in kwargs.items():\n+        # Remove kwargs that are used to initialize the image processor attributes\n+        for key in list(kwargs):\n             if hasattr(image_processor, key):\n-                setattr(image_processor, key, value)\n-                to_remove.append(key)\n-        for key in to_remove:\n-            kwargs.pop(key, None)\n+                kwargs.pop(key)\n \n         logger.info(f\"Image processor {image_processor}\")\n         if return_unused_kwargs:"
      },
      {
        "filename": "src/transformers/image_processing_utils_fast.py",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -185,6 +185,7 @@ class BaseImageProcessorFast(BaseImageProcessor):\n     input_data_format = None\n     device = None\n     model_input_names = [\"pixel_values\"]\n+    image_seq_length = None\n     valid_kwargs = ImagesKwargs\n     unused_kwargs = None\n "
      },
      {
        "filename": "src/transformers/models/pix2struct/image_processing_pix2struct.py",
        "status": "modified",
        "additions": 7,
        "deletions": 0,
        "changes": 7,
        "patch": "@@ -53,11 +53,18 @@ class Pix2StructImageProcessorKwargs(ImagesKwargs, total=False):\n     \"\"\"\n     max_patches (`int`, *optional*):\n         Maximum number of patches to extract.\n+    patch_size (`dict[str, int]`, *optional*, defaults to `{\"height\": 16, \"width\": 16}`):\n+        The patch size to use for the image. According to Pix2Struct paper and code, the patch size is 16x16.\n+    is_vqa (`bool`, *optional*, defaults to `False`):\n+        Whether or not the image processor is for the VQA task. If `True` and `header_text` is passed in, text is\n+        rendered onto the input images.\n     header_text (`Union[list[str], str]`, *optional*):\n         Text to render as a header. Only has an effect if `image_processor.is_vqa` is `True`.\n     \"\"\"\n \n     max_patches: int\n+    patch_size: dict[str, int]\n+    is_vqa: bool\n     header_text: Optional[Union[list[str], str]]\n \n "
      },
      {
        "filename": "src/transformers/processing_utils.py",
        "status": "modified",
        "additions": 4,
        "deletions": 0,
        "changes": 4,
        "patch": "@@ -219,6 +219,9 @@ class methods and docstrings.\n             - `'np'`: Return NumPy `np.ndarray` objects.\n         disable_grouping (`bool`, *optional*):\n             Whether to group images by shapes when processing or not, only relevant for fast image processing.\n+        image_seq_length (`int`, *optional*):\n+            The number of image tokens to be used for each image in the input.\n+            Added for backward compatibility but this should be set as a processor attribute in future models.\n     \"\"\"\n \n     do_convert_rgb: Optional[bool]\n@@ -239,6 +242,7 @@ class methods and docstrings.\n     device: Annotated[Optional[str], device_validator()]\n     return_tensors: Annotated[Optional[Union[str, TensorType]], tensor_type_validator()]\n     disable_grouping: Optional[bool]\n+    image_seq_length: Optional[int]\n \n \n class VideosKwargs(TypedDict, total=False):"
      },
      {
        "filename": "tests/models/pix2struct/test_processing_pix2struct.py",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -172,6 +172,7 @@ def test_image_processor_defaults_preserved_by_image_kwargs(self):\n         if \"image_processor\" not in self.processor_class.attributes:\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         image_processor = self.get_component(\"image_processor\", max_patches=1024, patch_size={\"height\": 8, \"width\": 8})\n+        print(\"image_processor\", image_processor)\n         tokenizer = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n \n         processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)"
      }
    ],
    "num_files": 5,
    "scraped_at": "2025-11-16T21:16:18.178797",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR contains non-trivial logic changes to how kwargs are handled in the image processor initialization flow. It addresses a real bug where custom kwargs weren't being passed to the `__init__` method, affecting parameter override behavior. The fix involves understanding the interaction between `from_pretrained`, `from_dict`, kwargs validation, and object initialization\u2014all substantive architectural concerns that would benefit from technical questions.",
      "substance_level": "medium"
    }
  },
  {
    "pr_number": 41969,
    "title": "add support for saving encoder only so any parakeet model can be loaded for inference",
    "body": "# What does this PR do?\r\n\r\n\r\n\r\nAdds support for conversion of any parakeet model encoder for both ctc and tdt decoders. This will enable researchers to use encoder only for foundation model training experiments. \r\n\r\n\r\n\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes # (issue)\r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker @Cyrilvallez\r\n- vision models: @yonigozlan @molbap\r\n- audio models: @eustlb @ebezzam @vasqu\r\n- multimodal models: @zucchini-nlp\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- generate: @zucchini-nlp (visual-language models) or @gante (all others)\r\n- continuous batching: @remi-or @ArthurZucker @McPatate\r\n- pipelines: @Rocketknight1\r\n- tokenizers: @ArthurZucker and @itazap\r\n- trainer: @SunMarc\r\n- attention: @vasqu @ArthurZucker @CyrilVallez\r\n- model loading (from pretrained, etc): @CyrilVallez\r\n- distributed: @3outeille @ArthurZucker\r\n- CIs: @ydshieh\r\n\r\nIntegrations:\r\n\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization: @SunMarc @MekkCyber\r\n- kernels: @MekkCyber @drbh\r\n- peft: @BenjaminBossan @githubnemo\r\n\r\nDevices/Backends:\r\n\r\n- AMD ROCm: @ivarflakstad\r\n- Intel XPU: @IlyasMoutawwakil\r\n- Ascend NPU: @ivarflakstad \r\n\r\nDocumentation: @stevhliu\r\n\r\nResearch projects are not maintained and should be taken as is.\r\n\r\n -->\r\n",
    "html_url": "https://github.com/huggingface/transformers/pull/41969",
    "created_at": "2025-10-31T19:28:32Z",
    "merged_at": "2025-11-02T18:21:41Z",
    "merge_commit_sha": "b9f90dc388fd415a2ba2a6a31a372f451d4a4eed",
    "base_ref": "main",
    "head_sha": "959de987484cd7a686c3c92a42bce511fc9e3f78",
    "user": "nithinraok",
    "files": [
      {
        "filename": "src/transformers/models/fastspeech2_conformer/configuration_fastspeech2_conformer.py",
        "status": "modified",
        "additions": 4,
        "deletions": 0,
        "changes": 4,
        "patch": "@@ -147,6 +147,8 @@ class FastSpeech2ConformerConfig(PreTrainedConfig):\n             Speaker embedding dimension. If set to > 0, assume that speaker_embedding will be provided as the input.\n         is_encoder_decoder (`bool`, *optional*, defaults to `True`):\n             Specifies whether the model is an encoder-decoder.\n+        convolution_bias (`bool`, *optional*, defaults to `True`):\n+            Specifies whether to use bias in convolutions of the conformer's convolution module.\n \n     Example:\n \n@@ -224,6 +226,7 @@ def __init__(\n         num_languages=None,\n         speaker_embed_dim=None,\n         is_encoder_decoder=True,\n+        convolution_bias=True,\n         **kwargs,\n     ):\n         if positionwise_conv_kernel_size % 2 == 0:\n@@ -318,6 +321,7 @@ def __init__(\n         self.speaker_embed_dim = speaker_embed_dim\n         self.duration_predictor_dropout_rate = duration_predictor_dropout_rate\n         self.is_encoder_decoder = is_encoder_decoder\n+        self.convolution_bias = convolution_bias\n \n         super().__init__(\n             is_encoder_decoder=is_encoder_decoder,"
      },
      {
        "filename": "src/transformers/models/fastspeech2_conformer/modeling_fastspeech2_conformer.py",
        "status": "modified",
        "additions": 13,
        "deletions": 3,
        "changes": 16,
        "patch": "@@ -490,12 +490,22 @@ def __init__(self, config: FastSpeech2ConformerConfig, module_config=None):\n             kernel_size = module_config[\"kernel_size\"]\n             self.activation = ACT2FN[module_config.get(\"activation\", \"silu\")]\n         self.padding = (kernel_size - 1) // 2\n-        self.pointwise_conv1 = nn.Conv1d(channels, 2 * channels, kernel_size=1, stride=1, padding=0, bias=True)\n+        self.pointwise_conv1 = nn.Conv1d(\n+            channels, 2 * channels, kernel_size=1, stride=1, padding=0, bias=config.convolution_bias\n+        )\n         self.depthwise_conv = nn.Conv1d(\n-            channels, channels, kernel_size, stride=1, padding=self.padding, groups=channels, bias=True\n+            channels,\n+            channels,\n+            kernel_size,\n+            stride=1,\n+            padding=self.padding,\n+            groups=channels,\n+            bias=config.convolution_bias,\n         )\n         self.norm = nn.BatchNorm1d(channels)\n-        self.pointwise_conv2 = nn.Conv1d(channels, channels, kernel_size=1, stride=1, padding=0, bias=True)\n+        self.pointwise_conv2 = nn.Conv1d(\n+            channels, channels, kernel_size=1, stride=1, padding=0, bias=config.convolution_bias\n+        )\n \n     def forward(self, hidden_states, attention_mask=None):\n         \"\"\""
      },
      {
        "filename": "src/transformers/models/parakeet/configuration_parakeet.py",
        "status": "modified",
        "additions": 4,
        "deletions": 0,
        "changes": 4,
        "patch": "@@ -44,6 +44,8 @@ class ParakeetEncoderConfig(PreTrainedConfig):\n             The non-linear activation function (function or string) in the encoder and pooler.\n         attention_bias (`bool`, *optional*, defaults to `True`):\n             Whether to use bias in the attention layers.\n+        convolution_bias (`bool`, *optional*, defaults to `True`):\n+            Whether to use bias in convolutions of the conformer's convolution module.\n         conv_kernel_size (`int`, *optional*, defaults to 9):\n             The kernel size of the convolution layers in the Conformer block.\n         subsampling_factor (`int`, *optional*, defaults to 8):\n@@ -102,6 +104,7 @@ def __init__(\n         intermediate_size=4096,\n         hidden_act=\"silu\",\n         attention_bias=True,\n+        convolution_bias=True,\n         conv_kernel_size=9,\n         subsampling_factor=8,\n         subsampling_conv_channels=256,\n@@ -128,6 +131,7 @@ def __init__(\n         self.intermediate_size = intermediate_size\n         self.hidden_act = hidden_act\n         self.attention_bias = attention_bias\n+        self.convolution_bias = convolution_bias\n \n         if (conv_kernel_size - 1) % 2 != 0:\n             raise ValueError(f\"conv_kernel_size must be odd, got {conv_kernel_size}\")"
      },
      {
        "filename": "src/transformers/models/parakeet/convert_nemo_to_hf.py",
        "status": "modified",
        "additions": 92,
        "deletions": 26,
        "changes": 118,
        "patch": "@@ -25,6 +25,8 @@\n \n from transformers import (\n     ParakeetCTCConfig,\n+    ParakeetEncoder,\n+    ParakeetEncoderConfig,\n     ParakeetFeatureExtractor,\n     ParakeetForCTC,\n     ParakeetProcessor,\n@@ -203,7 +205,8 @@ def write_processor(nemo_config: dict, model_files, output_dir, push_to_repo_id=\n         processor.push_to_hub(push_to_repo_id)\n \n \n-def write_model(nemo_config, model_files, model_type, output_dir, push_to_repo_id=None):\n+def convert_encoder_config(nemo_config):\n+    \"\"\"Convert NeMo encoder config to HF encoder config.\"\"\"\n     encoder_keys_to_ignore = [\n         \"att_context_size\",\n         \"causal_downsampling\",\n@@ -220,8 +223,11 @@ def write_model(nemo_config, model_files, model_type, output_dir, push_to_repo_i\n         \"stochastic_depth_mode\",\n         \"conv_context_size\",\n         \"dropout_pre_encoder\",\n+        \"reduction\",\n+        \"reduction_factor\",\n+        \"reduction_position\",\n     ]\n-    enocder_config_keys_mapping = {\n+    encoder_config_keys_mapping = {\n         \"d_model\": \"hidden_size\",\n         \"n_heads\": \"num_attention_heads\",\n         \"n_layers\": \"num_hidden_layers\",\n@@ -234,17 +240,26 @@ def write_model(nemo_config, model_files, model_type, output_dir, push_to_repo_i\n         \"dropout_emb\": \"dropout_positions\",\n         \"dropout_att\": \"attention_dropout\",\n         \"xscaling\": \"scale_input\",\n+        \"use_bias\": \"attention_bias\",\n     }\n     converted_encoder_config = {}\n \n     for key, value in nemo_config[\"encoder\"].items():\n         if key in encoder_keys_to_ignore:\n             continue\n-        if key in enocder_config_keys_mapping:\n-            converted_encoder_config[enocder_config_keys_mapping[key]] = value\n+        if key in encoder_config_keys_mapping:\n+            converted_encoder_config[encoder_config_keys_mapping[key]] = value\n+            # NeMo uses 'use_bias' for both attention and convolution bias, but HF separates them\n+            if key == \"use_bias\":\n+                converted_encoder_config[\"convolution_bias\"] = value\n         else:\n-            raise ValueError(f\"Key {key} not found in enocder_config_keys_mapping\")\n+            raise ValueError(f\"Key {key} not found in encoder_config_keys_mapping\")\n+\n+    return ParakeetEncoderConfig(**converted_encoder_config)\n+\n \n+def load_and_convert_state_dict(model_files):\n+    \"\"\"Load NeMo state dict and convert keys to HF format.\"\"\"\n     state_dict = torch.load(model_files[\"model_weights\"], map_location=\"cpu\", weights_only=True)\n     converted_state_dict = {}\n     for key, value in state_dict.items():\n@@ -255,31 +270,80 @@ def write_model(nemo_config, model_files, model_type, output_dir, push_to_repo_i\n         converted_key = convert_key(key, NEMO_TO_HF_WEIGHT_MAPPING)\n         converted_state_dict[converted_key] = value\n \n-    if model_type == \"ctc\":\n-        model_config = ParakeetCTCConfig(\n-            encoder_config=converted_encoder_config,\n-        )\n-        print(\"Loading the checkpoint in a Parakeet CTC model.\")\n-        with torch.device(\"meta\"):\n-            model = ParakeetForCTC(model_config)\n-        model.load_state_dict(converted_state_dict, strict=True, assign=True)\n-        print(\"Checkpoint loaded successfully.\")\n-        del model.config._name_or_path\n+    return converted_state_dict\n+\n+\n+def write_ctc_model(encoder_config, converted_state_dict, output_dir, push_to_repo_id=None):\n+    \"\"\"Write CTC model using encoder config and converted state dict.\"\"\"\n+    model_config = ParakeetCTCConfig.from_encoder_config(encoder_config)\n+\n+    print(\"Loading the checkpoint in a Parakeet CTC model.\")\n+    with torch.device(\"meta\"):\n+        model = ParakeetForCTC(model_config)\n+    model.load_state_dict(converted_state_dict, strict=True, assign=True)\n+    print(\"Checkpoint loaded successfully.\")\n+    del model.config._name_or_path\n+\n+    print(\"Saving the model.\")\n+    model.save_pretrained(output_dir)\n+\n+    if push_to_repo_id:\n+        model.push_to_hub(push_to_repo_id)\n \n-        print(\"Saving the model.\")\n-        model.save_pretrained(output_dir)\n+    del model\n \n-        if push_to_repo_id:\n-            model.push_to_hub(push_to_repo_id)\n+    # Safety check: reload the converted model\n+    gc.collect()\n+    print(\"Reloading the model to check if it's saved correctly.\")\n+    ParakeetForCTC.from_pretrained(output_dir, dtype=torch.bfloat16, device_map=\"auto\")\n+    print(\"Model reloaded successfully.\")\n \n-        del converted_state_dict, model\n \n-        # Safety check: reload the converted model\n-        gc.collect()\n-        print(\"Reloading the model to check if it's saved correctly.\")\n-        ParakeetForCTC.from_pretrained(output_dir, dtype=torch.bfloat16, device_map=\"auto\")\n-        print(\"Model reloaded successfully.\")\n+def write_encoder_model(encoder_config, converted_state_dict, output_dir, push_to_repo_id=None):\n+    \"\"\"Write encoder model using encoder config and converted state dict.\"\"\"\n+    # Filter to only encoder weights (exclude CTC head if present)\n+    encoder_state_dict = {\n+        k.replace(\"encoder.\", \"\", 1) if k.startswith(\"encoder.\") else k: v\n+        for k, v in converted_state_dict.items()\n+        if k.startswith(\"encoder.\")\n+    }\n+\n+    print(\"Loading the checkpoint in a Parakeet Encoder model (for TDT).\")\n+    with torch.device(\"meta\"):\n+        model = ParakeetEncoder(encoder_config)\n+\n+    model.load_state_dict(encoder_state_dict, strict=True, assign=True)\n+    print(\"Checkpoint loaded successfully.\")\n+    del model.config._name_or_path\n+\n+    print(\"Saving the model.\")\n+    model.save_pretrained(output_dir)\n+\n+    if push_to_repo_id:\n+        model.push_to_hub(push_to_repo_id)\n+    del model\n+\n+    # Safety check: reload the converted model\n+    gc.collect()\n+    print(\"Reloading the model to check if it's saved correctly.\")\n+    ParakeetEncoder.from_pretrained(output_dir, dtype=torch.bfloat16, device_map=\"auto\")\n+    print(\"Model reloaded successfully.\")\n+\n \n+def write_model(nemo_config, model_files, model_type, output_dir, push_to_repo_id=None):\n+    \"\"\"Main model conversion function.\"\"\"\n+    # Step 1: Convert encoder config (shared across all model types)\n+    encoder_config = convert_encoder_config(nemo_config)\n+    print(f\"Converted encoder config: {encoder_config}\")\n+\n+    # Step 2: Load and convert state dict (shared across all model types)\n+    converted_state_dict = load_and_convert_state_dict(model_files)\n+\n+    # Step 3: Write model based on type\n+    if model_type == \"encoder\":\n+        write_encoder_model(encoder_config, converted_state_dict, output_dir, push_to_repo_id)\n+    elif model_type == \"ctc\":\n+        write_ctc_model(encoder_config, converted_state_dict, output_dir, push_to_repo_id)\n     else:\n         raise ValueError(f\"Model type {model_type} not supported.\")\n \n@@ -303,7 +367,9 @@ def main(\n if __name__ == \"__main__\":\n     parser = argparse.ArgumentParser()\n     parser.add_argument(\"--hf_repo_id\", required=True, help=\"Model repo on huggingface.co\")\n-    parser.add_argument(\"--model_type\", required=True, choices=[\"ctc\"], help=\"Model type (`ctc`, `tdt`)\")\n+    parser.add_argument(\n+        \"--model_type\", required=True, choices=[\"encoder\", \"ctc\"], help=\"Model type (`encoder`, `ctc`)\"\n+    )\n     parser.add_argument(\"--output_dir\", required=True, help=\"Output directory for HuggingFace model\")\n     parser.add_argument(\"--push_to_repo_id\", help=\"Repository ID to push the model to on the Hub\")\n     args = parser.parse_args()"
      },
      {
        "filename": "src/transformers/models/parakeet/modeling_parakeet.py",
        "status": "modified",
        "additions": 13,
        "deletions": 3,
        "changes": 16,
        "patch": "@@ -130,12 +130,22 @@ def __init__(self, config: ParakeetEncoderConfig, module_config=None):\n             kernel_size = module_config[\"kernel_size\"]\n             self.activation = ACT2FN[module_config.get(\"activation\", \"silu\")]\n         self.padding = (kernel_size - 1) // 2\n-        self.pointwise_conv1 = nn.Conv1d(channels, 2 * channels, kernel_size=1, stride=1, padding=0, bias=True)\n+        self.pointwise_conv1 = nn.Conv1d(\n+            channels, 2 * channels, kernel_size=1, stride=1, padding=0, bias=config.convolution_bias\n+        )\n         self.depthwise_conv = nn.Conv1d(\n-            channels, channels, kernel_size, stride=1, padding=self.padding, groups=channels, bias=True\n+            channels,\n+            channels,\n+            kernel_size,\n+            stride=1,\n+            padding=self.padding,\n+            groups=channels,\n+            bias=config.convolution_bias,\n         )\n         self.norm = nn.BatchNorm1d(channels)\n-        self.pointwise_conv2 = nn.Conv1d(channels, channels, kernel_size=1, stride=1, padding=0, bias=True)\n+        self.pointwise_conv2 = nn.Conv1d(\n+            channels, channels, kernel_size=1, stride=1, padding=0, bias=config.convolution_bias\n+        )\n \n     def forward(self, hidden_states, attention_mask=None):\n         \"\"\""
      }
    ],
    "num_files": 5,
    "scraped_at": "2025-11-16T21:16:23.965275",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR adds meaningful functionality to support encoder-only model conversion for the Parakeet model, involving configuration changes, model architecture modifications, and conversion logic updates. The changes demonstrate non-trivial refactoring of the NeMo-to-HF conversion process and introduce a new `convolution_bias` parameter across multiple model components, providing sufficient substance for generating technical questions about model architecture, configuration management, and conversion workflows.",
      "substance_level": "medium"
    }
  },
  {
    "pr_number": 41930,
    "title": "handle inputs from Siglip/Siglip2 non-automapped encoder layers",
    "body": "# What does this PR do?\r\n\r\nShould fix #41929 . The `check_model_inputs` / `can_record_outputs` interaction is not always trivial and models with several entrypoints such as `VisionModel` vs `VisionTransformer` are missing some, adding it here. Also added a modification in `generic` to make sure the flag was captured, not 100% sure it's needed. ",
    "html_url": "https://github.com/huggingface/transformers/pull/41930",
    "created_at": "2025-10-29T09:51:32Z",
    "merged_at": "2025-11-12T13:58:44Z",
    "merge_commit_sha": "fd36275be2f3e56bc20da01f1f320b623b413957",
    "base_ref": "main",
    "head_sha": "f74fde9c3aef78d41ec7cf83b72980d920d7b27d",
    "user": "molbap",
    "files": [
      {
        "filename": "src/transformers/models/siglip/modeling_siglip.py",
        "status": "modified",
        "additions": 8,
        "deletions": 2,
        "changes": 10,
        "patch": "@@ -678,9 +678,14 @@ def forward(\n         )\n \n \n-class SiglipVisionTransformer(nn.Module):\n+class SiglipVisionTransformer(SiglipPreTrainedModel):\n+    _can_record_outputs = {\n+        \"hidden_states\": SiglipEncoderLayer,\n+        \"attentions\": SiglipAttention,\n+    }\n+\n     def __init__(self, config: SiglipVisionConfig):\n-        super().__init__()\n+        super().__init__(config)\n         self.config = config\n         embed_dim = config.hidden_size\n \n@@ -691,6 +696,7 @@ def __init__(self, config: SiglipVisionConfig):\n         if self.use_head:\n             self.head = SiglipMultiheadAttentionPoolingHead(config)\n \n+    @check_model_inputs(tie_last_hidden_states=False)\n     @auto_docstring\n     def forward(\n         self,"
      },
      {
        "filename": "src/transformers/models/siglip2/modeling_siglip2.py",
        "status": "modified",
        "additions": 99,
        "deletions": 93,
        "changes": 192,
        "patch": "@@ -349,99 +349,6 @@ def forward(\n         return hidden_states\n \n \n-class Siglip2Encoder(nn.Module):\n-    \"\"\"\n-    Transformer encoder consisting of `config.num_hidden_layers` self attention layers. Each layer is a\n-    [`Siglip2EncoderLayer`].\n-\n-    Args:\n-        config: Siglip2Config\n-    \"\"\"\n-\n-    def __init__(self, config: Siglip2Config):\n-        super().__init__()\n-        self.config = config\n-        self.layers = nn.ModuleList([Siglip2EncoderLayer(config) for _ in range(config.num_hidden_layers)])\n-        self.gradient_checkpointing = False\n-\n-    # Ignore copy\n-    @auto_docstring\n-    def forward(\n-        self,\n-        inputs_embeds,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        **kwargs: Unpack[TransformersKwargs],\n-    ) -> BaseModelOutput:\n-        hidden_states = inputs_embeds\n-        for encoder_layer in self.layers:\n-            hidden_states = encoder_layer(\n-                hidden_states,\n-                attention_mask,\n-                **kwargs,\n-            )\n-\n-        return BaseModelOutput(last_hidden_state=hidden_states)\n-\n-\n-class Siglip2VisionTransformer(nn.Module):\n-    def __init__(self, config: Siglip2VisionConfig):\n-        super().__init__()\n-        self.config = config\n-        embed_dim = config.hidden_size\n-\n-        self.embeddings = Siglip2VisionEmbeddings(config)\n-        self.encoder = Siglip2Encoder(config)\n-        self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n-        self.use_head = True if not hasattr(config, \"vision_use_head\") else config.vision_use_head\n-        if self.use_head:\n-            self.head = Siglip2MultiheadAttentionPoolingHead(config)\n-\n-    @auto_docstring\n-    def forward(\n-        self,\n-        pixel_values: torch.FloatTensor,\n-        attention_mask: torch.Tensor,\n-        spatial_shapes: torch.LongTensor,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-    ) -> BaseModelOutputWithPooling:\n-        r\"\"\"\n-        spatial_shapes (`torch.LongTensor` of shape `(batch_size, 2)`):\n-            Tensor containing the spatial dimensions (height, width) of the input images.\n-        \"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-\n-        hidden_states = self.embeddings(pixel_values, spatial_shapes)\n-\n-        if attention_mask is not None and self.config._attn_implementation != \"flash_attention_2\":\n-            # [batch_size, seq_len] -> [batch_size, 1, tgt_seq_len, src_seq_len]\n-            encoder_attention_mask = _prepare_4d_attention_mask(attention_mask, hidden_states.dtype)\n-        else:\n-            encoder_attention_mask = attention_mask\n-\n-        encoder_outputs: BaseModelOutput = self.encoder(\n-            inputs_embeds=hidden_states,\n-            attention_mask=encoder_attention_mask,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-        )\n-\n-        last_hidden_state = encoder_outputs.last_hidden_state\n-        last_hidden_state = self.post_layernorm(last_hidden_state)\n-\n-        pooler_output = self.head(last_hidden_state, attention_mask) if self.use_head else None\n-\n-        return BaseModelOutputWithPooling(\n-            last_hidden_state=last_hidden_state,\n-            pooler_output=pooler_output,\n-            hidden_states=encoder_outputs.hidden_states,\n-            attentions=encoder_outputs.attentions,\n-        )\n-\n-\n def _trunc_normal_(tensor, mean, std, a, b):\n     # Cut & paste from PyTorch official master until it's in a few official releases - RW\n     # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n@@ -607,6 +514,105 @@ def _init_weights(self, module):\n             module.weight.data.fill_(1.0)\n \n \n+class Siglip2Encoder(nn.Module):\n+    \"\"\"\n+    Transformer encoder consisting of `config.num_hidden_layers` self attention layers. Each layer is a\n+    [`Siglip2EncoderLayer`].\n+\n+    Args:\n+        config: Siglip2Config\n+    \"\"\"\n+\n+    def __init__(self, config: Siglip2Config):\n+        super().__init__()\n+        self.config = config\n+        self.layers = nn.ModuleList([Siglip2EncoderLayer(config) for _ in range(config.num_hidden_layers)])\n+        self.gradient_checkpointing = False\n+\n+    # Ignore copy\n+    @auto_docstring\n+    def forward(\n+        self,\n+        inputs_embeds,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> BaseModelOutput:\n+        hidden_states = inputs_embeds\n+        for encoder_layer in self.layers:\n+            hidden_states = encoder_layer(\n+                hidden_states,\n+                attention_mask,\n+                **kwargs,\n+            )\n+\n+        return BaseModelOutput(last_hidden_state=hidden_states)\n+\n+\n+class Siglip2VisionTransformer(Siglip2PreTrainedModel):\n+    _can_record_outputs = {\n+        \"hidden_states\": Siglip2EncoderLayer,\n+        \"attentions\": Siglip2Attention,\n+    }\n+\n+    def __init__(self, config: Siglip2VisionConfig):\n+        super().__init__(config)\n+        self.config = config\n+        embed_dim = config.hidden_size\n+\n+        self.embeddings = Siglip2VisionEmbeddings(config)\n+        self.encoder = Siglip2Encoder(config)\n+        self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n+        self.use_head = True if not hasattr(config, \"vision_use_head\") else config.vision_use_head\n+        if self.use_head:\n+            self.head = Siglip2MultiheadAttentionPoolingHead(config)\n+\n+    @check_model_inputs(tie_last_hidden_states=False)\n+    @auto_docstring\n+    def forward(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        attention_mask: torch.Tensor,\n+        spatial_shapes: torch.LongTensor,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+    ) -> BaseModelOutputWithPooling:\n+        r\"\"\"\n+        spatial_shapes (`torch.LongTensor` of shape `(batch_size, 2)`):\n+            Tensor containing the spatial dimensions (height, width) of the input images.\n+        \"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+\n+        hidden_states = self.embeddings(pixel_values, spatial_shapes)\n+\n+        if attention_mask is not None and self.config._attn_implementation != \"flash_attention_2\":\n+            # [batch_size, seq_len] -> [batch_size, 1, tgt_seq_len, src_seq_len]\n+            encoder_attention_mask = _prepare_4d_attention_mask(attention_mask, hidden_states.dtype)\n+        else:\n+            encoder_attention_mask = attention_mask\n+\n+        encoder_outputs: BaseModelOutput = self.encoder(\n+            inputs_embeds=hidden_states,\n+            attention_mask=encoder_attention_mask,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+        )\n+\n+        last_hidden_state = encoder_outputs.last_hidden_state\n+        last_hidden_state = self.post_layernorm(last_hidden_state)\n+\n+        pooler_output = self.head(last_hidden_state, attention_mask) if self.use_head else None\n+\n+        return BaseModelOutputWithPooling(\n+            last_hidden_state=last_hidden_state,\n+            pooler_output=pooler_output,\n+            hidden_states=encoder_outputs.hidden_states,\n+            attentions=encoder_outputs.attentions,\n+        )\n+\n+\n class Siglip2TextEmbeddings(nn.Module):\n     def __init__(self, config: Siglip2TextConfig):\n         super().__init__()"
      },
      {
        "filename": "src/transformers/models/siglip2/modular_siglip2.py",
        "status": "modified",
        "additions": 7,
        "deletions": 4,
        "changes": 11,
        "patch": "@@ -37,6 +37,7 @@\n \n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n from ...utils import auto_docstring, filter_out_non_signature_kwargs\n+from ...utils.generic import check_model_inputs\n \n \n class Siglip2TextConfig(SiglipTextConfig):\n@@ -230,6 +231,10 @@ def forward(self, pixel_values: torch.FloatTensor, spatial_shapes: torch.LongTen\n         return embeddings\n \n \n+class Siglip2PreTrainedModel(SiglipPreTrainedModel):\n+    pass\n+\n+\n class Siglip2VisionTransformer(SiglipVisionTransformer):\n     def __init__(self, config: Siglip2VisionConfig):\n         super().__init__(config)\n@@ -280,10 +285,6 @@ def forward(\n         )\n \n \n-class Siglip2PreTrainedModel(SiglipPreTrainedModel):\n-    pass\n-\n-\n class Siglip2TextModel(SiglipTextModel):\n     pass\n \n@@ -314,6 +315,8 @@ def forward(self, hidden_state: torch.Tensor, attention_mask: Optional[torch.Ten\n \n class Siglip2VisionModel(SiglipVisionModel):\n     # Update: add `spatial_shapes` and `pixel_attention_mask`\n+    @check_model_inputs(tie_last_hidden_states=False)\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: torch.FloatTensor,"
      },
      {
        "filename": "utils/check_repo.py",
        "status": "modified",
        "additions": 4,
        "deletions": 0,
        "changes": 4,
        "patch": "@@ -90,6 +90,8 @@\n     \"Kosmos2_5TextForCausalLM\",\n     \"Kosmos2_5VisionModel\",\n     \"SmolVLMVisionTransformer\",\n+    \"SiglipVisionTransformer\",\n+    \"Siglip2VisionTransformer\",\n     \"AriaTextForCausalLM\",\n     \"AriaTextModel\",\n     \"Phi4MultimodalAudioModel\",\n@@ -358,7 +360,9 @@\n     \"SegGptForImageSegmentation\",\n     \"SiglipVisionModel\",\n     \"SiglipTextModel\",\n+    \"SiglipVisionTransformer\",\n     \"Siglip2VisionModel\",\n+    \"Siglip2VisionTransformer\",\n     \"Siglip2TextModel\",\n     \"ChameleonVQVAE\",  # no autoclass for VQ-VAE models\n     \"VitPoseForPoseEstimation\","
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T21:16:31.379977",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR involves non-trivial architectural changes to model classes, including inheritance hierarchy modifications, addition of metadata attributes for output tracking, and decorator application to control model input handling. The changes demonstrate meaningful interactions between components (check_model_inputs, can_record_outputs, PreTrainedModel inheritance) that would require developers to understand the framework's design patterns.",
      "substance_level": "medium"
    }
  },
  {
    "pr_number": 41914,
    "title": "Run slow v2",
    "body": "# What does this PR do?\r\n\r\nRun slow v2!\r\n\r\n- Send feedback as a comment.\r\n- Include verified failed tests specific to a PR.\r\n\r\nDiscussed offline in the office. merge now \ud83d\udd25 ",
    "html_url": "https://github.com/huggingface/transformers/pull/41914",
    "created_at": "2025-10-28T13:13:51Z",
    "merged_at": "2025-11-01T18:40:40Z",
    "merge_commit_sha": "8fb854cac869b42c87a7bd15d9298985c5aea96e",
    "base_ref": "main",
    "head_sha": "cfea0a5035c6995d3cded16684734a0483ee5c2f",
    "user": "ydshieh",
    "files": [
      {
        "filename": ".github/workflows/check_failed_tests.yml",
        "status": "modified",
        "additions": 75,
        "deletions": 28,
        "changes": 103,
        "patch": "@@ -6,9 +6,6 @@ on:\n       docker:\n         required: true\n         type: string\n-      start_sha:\n-        required: true\n-        type: string\n       job:\n         required: true\n         type: string\n@@ -24,7 +21,13 @@ on:\n       commit_sha:\n         required: false\n         type: string\n-\n+      pr_number:\n+        required: false\n+        type: string\n+    outputs:\n+      report:\n+        description: \"Content of the report of new failures\"\n+        value: ${{ jobs.process_new_failures_with_commit_info.outputs.report }}\n \n env:\n   HF_HOME: /mnt/cache\n@@ -88,27 +91,55 @@ jobs:\n             echo \"PREV_WORKFLOW_RUN_ID=\" >> $GITHUB_ENV\n           fi\n \n-          if [ -f setup_values/other_workflow_run_id.txt ]; then\n-            echo \"OTHER_WORKFLOW_RUN_ID=$(cat setup_values/other_workflow_run_id.txt)\" >> $GITHUB_ENV\n-          else\n-            echo \"OTHER_WORKFLOW_RUN_ID=\" >> $GITHUB_ENV\n-          fi\n-\n       - name: Update clone\n         working-directory: /transformers\n         if: ${{ env.process == 'true' }}\n-        run: git fetch && git checkout ${{ inputs.commit_sha || github.sha }}\n+        run: |\n+          git fetch origin ${{ inputs.commit_sha || github.sha }}\n+          git fetch && git checkout ${{ inputs.commit_sha || github.sha }}\n \n-      - name: Get target commit\n+      - name: Get `START_SHA`\n         working-directory: /transformers/utils\n         if: ${{ env.process == 'true' }}\n+        run: |\n+          echo \"START_SHA=${{ inputs.commit_sha || github.sha }}\" >> $GITHUB_ENV\n+\n+      # This is used if the CI is triggered from a pull request `self-comment-ci.yml` (after security check is verified)\n+      - name: Extract the base commit on `main` (of the merge commit created by Github) if it is a PR\n+        id: pr_info\n+        if: ${{ env.process == 'true' && inputs.pr_number != '' }}\n+        uses: actions/github-script@v6\n+        with:\n+          script: |            \n+            const { data: pr } = await github.rest.pulls.get({\n+              owner: context.repo.owner,\n+              repo: context.repo.repo,\n+              pull_number: ${{ inputs.pr_number }}\n+            });\n+\n+            const { data: merge_commit }  = await github.rest.repos.getCommit({\n+              owner: pr.base.repo.owner.login,\n+              repo: pr.base.repo.name,\n+              ref: pr.merge_commit_sha,\n+            });\n+\n+            core.setOutput('merge_commit_base_sha', merge_commit.parents[0].sha);\n+\n+      # Usually, `END_SHA` should be the commit of the last previous workflow run of the **SAME** (scheduled) workflow.\n+      # (This is why we don't need to specify `workflow_id` which would be fetched automatically in the python script.)\n+      - name: Get `END_SHA` from previous CI runs of the same workflow\n+        working-directory: /transformers/utils\n+        if: ${{ env.process == 'true' && inputs.pr_number == '' }}\n         run: |\n           echo \"END_SHA=$(TOKEN=${{ secrets.ACCESS_REPO_INFO_TOKEN }} python3 -c 'import os; from get_previous_daily_ci import get_last_daily_ci_run_commit; commit=get_last_daily_ci_run_commit(token=os.environ[\"TOKEN\"], workflow_run_id=os.environ[\"PREV_WORKFLOW_RUN_ID\"]); print(commit)')\" >> $GITHUB_ENV\n \n-      - name: Checkout to `start_sha`\n-        working-directory: /transformers\n-        if: ${{ env.process == 'true' }}\n-        run: git fetch && git checkout ${{ inputs.start_sha }}\n+      # However, for workflow runs triggered by `issue_comment` (for pull requests), we want to check against the\n+      # parent commit (on `main`) of the `merge_commit` (dynamically created by GitHub). In this case, the goal is to\n+      # see if a reported failing test is actually ONLY failing on the `merge_commit`.\n+      - name: Set `END_SHA`\n+        if: ${{ env.process == 'true' && inputs.pr_number != '' }}\n+        run: |\n+          echo \"END_SHA=${{ steps.pr_info.outputs.merge_commit_base_sha }}\" >> $GITHUB_ENV\n \n       - name: Reinstall transformers in edit mode (remove the one installed during docker image build)\n         working-directory: /transformers\n@@ -138,7 +169,7 @@ jobs:\n       - name: Check failed tests\n         working-directory: /transformers\n         if: ${{ env.process == 'true' }}\n-        run: python3 utils/check_bad_commit.py --start_commit ${{ inputs.start_sha }} --end_commit ${{ env.END_SHA }} --file ci_results_${{ inputs.job }}/new_failures.json --output_file new_failures_with_bad_commit_${{ inputs.job }}_${{ matrix.run_idx }}.json\n+        run: python3 utils/check_bad_commit.py --start_commit ${{ env.START_SHA }} --end_commit ${{ env.END_SHA }} --file ci_results_${{ inputs.job }}/new_failures.json --output_file new_failures_with_bad_commit_${{ inputs.job }}_${{ matrix.run_idx }}.json\n \n       - name: Show results\n         working-directory: /transformers\n@@ -159,6 +190,8 @@ jobs:\n     if: needs.check_new_failures.outputs.process == 'true'\n     runs-on:\n       group: aws-g5-4xlarge-cache\n+    outputs:\n+      report: ${{ steps.set_output.outputs.report }}\n     container:\n       image: ${{ inputs.docker }}\n       options: --gpus all --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n@@ -190,18 +223,9 @@ jobs:\n \n       - name: Update clone\n         working-directory: /transformers\n-        run: git fetch && git checkout ${{ inputs.commit_sha || github.sha }}\n-\n-      - name: Process report\n-        shell: bash\n-        working-directory: /transformers\n-        env:\n-          ACCESS_REPO_INFO_TOKEN: ${{ secrets.ACCESS_REPO_INFO_TOKEN }}\n-          TRANSFORMERS_CI_RESULTS_UPLOAD_TOKEN: ${{ secrets.TRANSFORMERS_CI_RESULTS_UPLOAD_TOKEN }}\n-          JOB_NAME: ${{ inputs.job }}\n-          REPORT_REPO_ID: ${{ inputs.report_repo_id }}\n         run: |\n-          python3 utils/process_bad_commit_report.py\n+          git fetch origin ${{ inputs.commit_sha || github.sha }}\n+          git fetch && git checkout ${{ inputs.commit_sha || github.sha }}\n \n       - name: Process report\n         shell: bash\n@@ -218,6 +242,29 @@ jobs:\n             echo EOF\n           } >> \"$GITHUB_ENV\"\n \n+      # The output is useful if a caller needs more processing, for example, we have a chain\n+      # self-comment-ci.yml -> self-scheduled.yml -> this one (check_failed_tests.yml),\n+      # and `self-comment-ci.yml` needs further processing before sending a GitHub comment to the pull request page.\n+      - name: Show results & Set outputs\n+        id: set_output\n+        working-directory: /transformers\n+        run: |\n+          ls -l new_failures_with_bad_commit.json\n+          cat new_failures_with_bad_commit.json\n+\n+          {\n+            echo 'report<<EOF'\n+            cat new_failures_with_bad_commit.json\n+            echo ''  # Force a newline\n+            echo EOF\n+          } >> \"$GITHUB_OUTPUT\"\n+\n+      - name: Upload artifacts\n+        uses: actions/upload-artifact@v4\n+        with:\n+          name: new_failures_with_bad_commit_${{ inputs.job }}\n+          path: /transformers/new_failures_with_bad_commit.json\n+\n       - name: Prepare Slack report title\n         working-directory: /transformers\n         run: |"
      },
      {
        "filename": ".github/workflows/get-pr-info.yml",
        "status": "modified",
        "additions": 9,
        "deletions": 0,
        "changes": 9,
        "patch": "@@ -39,6 +39,9 @@ on:\n       PR_MERGE_COMMIT_SHA:\n         description: \"The sha of the merge commit for the pull request (created by GitHub) in the base repository\"\n         value: ${{ jobs.get-pr-info.outputs.PR_MERGE_COMMIT_SHA }}\n+      PR_MERGE_COMMIT_BASE_SHA:\n+        description: \"The sha of the parent commit of the the merge commit on the target branch in the base repository\"\n+        value: ${{ jobs.get-pr-info.outputs.PR_MERGE_COMMIT_BASE_SHA }}\n       PR_HEAD_COMMIT_DATE:\n         description: \"The date of the head sha of the pull request branch in the head repository\"\n         value: ${{ jobs.get-pr-info.outputs.PR_HEAD_COMMIT_DATE }}\n@@ -74,6 +77,7 @@ jobs:\n       PR_BASE_REF: ${{ steps.pr_info.outputs.base_ref }}\n       PR_HEAD_SHA: ${{ steps.pr_info.outputs.head_sha }}\n       PR_BASE_SHA: ${{ steps.pr_info.outputs.base_sha }}\n+      PR_MERGE_COMMIT_BASE_SHA: ${{ steps.pr_info.outputs.merge_commit_base_sha }}\n       PR_MERGE_COMMIT_SHA: ${{ steps.pr_info.outputs.merge_commit_sha }}\n       PR_HEAD_COMMIT_DATE: ${{ steps.pr_info.outputs.head_commit_date }}\n       PR_MERGE_COMMIT_DATE: ${{ steps.pr_info.outputs.merge_commit_date }}\n@@ -122,6 +126,7 @@ jobs:\n             core.setOutput('base_ref', pr.base.ref);\n             core.setOutput('head_sha', pr.head.sha);\n             core.setOutput('base_sha', pr.base.sha);\n+            core.setOutput('merge_commit_base_sha', merge_commit.parents[0].sha);\n             core.setOutput('merge_commit_sha', pr.merge_commit_sha);\n             core.setOutput('pr', pr);\n \n@@ -142,6 +147,10 @@ jobs:\n               date: merge_commit.commit.committer.date\n             });\n \n+            console.log('PR Info:', {\n+              pr_info: pr\n+            });\n+\n       - name: Convert dates to timestamps\n         id: get_timestamps\n         run: |"
      },
      {
        "filename": ".github/workflows/model_jobs.yml",
        "status": "modified",
        "additions": 4,
        "deletions": 2,
        "changes": 6,
        "patch": "@@ -80,7 +80,9 @@ jobs:\n \n       - name: Update clone\n         working-directory: /transformers\n-        run: git fetch && git checkout ${{ inputs.commit_sha || github.sha }}\n+        run: |\n+          git fetch origin ${{ inputs.commit_sha || github.sha }}\n+          git fetch && git checkout ${{ inputs.commit_sha || github.sha }}\n \n       - name: Reinstall transformers in edit mode (remove the one installed during docker image build)\n         working-directory: /transformers\n@@ -174,7 +176,7 @@ jobs:\n \n   collated_reports:\n     name: Collated Reports\n-    if: ${{ always() }}\n+    if: ${{ always() && inputs.runner_type != '' }}\n     needs: run_models_gpu\n     uses: huggingface/transformers/.github/workflows/collated-reports.yml@main\n     with:"
      },
      {
        "filename": ".github/workflows/push-important-models.yml",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -153,5 +153,5 @@ jobs:\n       ci_event: push\n       report_repo_id: hf-internal-testing/transformers_ci_push\n       commit_sha: ${{ github.sha }}\n-      models: ${{ needs.get_modified_models.outputs.matrix }}\n+      subdirs: ${{ needs.get_modified_models.outputs.matrix }}\n     secrets: inherit"
      },
      {
        "filename": ".github/workflows/self-comment-ci.yml",
        "status": "modified",
        "additions": 203,
        "deletions": 280,
        "changes": 483,
        "patch": "@@ -23,62 +23,34 @@ env:\n   TF_FORCE_GPU_ALLOW_GROWTH: true\n   CUDA_VISIBLE_DEVICES: 0,1\n \n+\n jobs:\n   get-pr-number:\n-    runs-on: ubuntu-22.04\n     name: Get PR number\n-    # For security: only allow team members to run\n     if: ${{ github.event.issue.state == 'open' && contains(fromJSON('[\"ydshieh\", \"ArthurZucker\", \"zucchini-nlp\", \"molbap\", \"gante\", \"LysandreJik\", \"Cyrilvallez\", \"Rocketknight1\", \"SunMarc\", \"eustlb\", \"MekkCyber\", \"vasqu\", \"ivarflakstad\", \"stevhliu\", \"ebezzam\", \"remi-or\", \"itazap\"]'), github.actor) && (startsWith(github.event.comment.body, 'run-slow') || startsWith(github.event.comment.body, 'run slow') || startsWith(github.event.comment.body, 'run_slow')) }}\n-    outputs:\n-      PR_NUMBER: ${{ steps.set_pr_number.outputs.PR_NUMBER }}\n-    steps:\n-      - name: Get PR number\n-        shell: bash\n-        run: |\n-          if [[ \"${{ github.event.issue.number }}\" != \"\" && \"${{ github.event.issue.pull_request }}\" != \"\" ]]; then\n-            echo \"PR_NUMBER=${{ github.event.issue.number }}\" >> $GITHUB_ENV\n-          else\n-            echo \"PR_NUMBER=\" >> $GITHUB_ENV\n-          fi\n-\n-      - name: Check PR number\n-        shell: bash\n-        run: |\n-          echo \"${{ env.PR_NUMBER }}\"\n-\n-      - name: Set PR number\n-        id: set_pr_number\n-        run: echo \"PR_NUMBER=${{ env.PR_NUMBER }}\" >> \"$GITHUB_OUTPUT\"\n+    uses: ./.github/workflows/get-pr-number.yml\n \n-  get-sha:\n-    runs-on: ubuntu-22.04\n+  get-pr-info:\n+    name: Get PR commit SHA\n     needs: get-pr-number\n     if: ${{ needs.get-pr-number.outputs.PR_NUMBER != ''}}\n+    uses: ./.github/workflows/get-pr-info.yml\n+    with:\n+      pr_number: ${{ needs.get-pr-number.outputs.PR_NUMBER }}\n+\n+  check-timestamps:\n+    name: Check timestamps (security check)\n+    runs-on: ubuntu-22.04\n+    needs: get-pr-info\n     outputs:\n-      PR_HEAD_SHA: ${{ steps.get_sha.outputs.PR_HEAD_SHA }}\n-      PR_MERGE_SHA: ${{ steps.get_sha.outputs.PR_MERGE_SHA }}\n+      PR_HEAD_SHA: ${{ needs.get-pr-info.outputs.PR_HEAD_SHA }}\n+      PR_MERGE_SHA: ${{ needs.get-pr-info.outputs.PR_MERGE_COMMIT_SHA }}\n     steps:\n-      - uses: actions/checkout@v4\n-        with:\n-          fetch-depth: \"0\"\n-          ref: \"refs/pull/${{needs.get-pr-number.outputs.PR_NUMBER}}/merge\"\n-\n-      - name: Get SHA (and verify timestamps against the issue comment date)\n-        id: get_sha\n+      - name: Verify `merge_commit` timestamp is older than the issue comment timestamp\n         env:\n-          PR_NUMBER: ${{ needs.get-pr-number.outputs.PR_NUMBER }}\n           COMMENT_DATE: ${{ github.event.comment.created_at }}\n+          PR_MERGE_COMMIT_TIMESTAMP: ${{ needs.get-pr-info.outputs.PR_MERGE_COMMIT_TIMESTAMP }}\n         run: |\n-            git fetch origin refs/pull/$PR_NUMBER/head:refs/remotes/pull/$PR_NUMBER/head\n-            git checkout refs/remotes/pull/$PR_NUMBER/head\n-            echo \"PR_HEAD_SHA: $(git log -1 --format=%H)\"\n-            echo \"PR_HEAD_SHA=$(git log -1 --format=%H)\" >> \"$GITHUB_OUTPUT\"\n-            git fetch origin refs/pull/$PR_NUMBER/merge:refs/remotes/pull/$PR_NUMBER/merge\n-            git checkout refs/remotes/pull/$PR_NUMBER/merge\n-            echo \"PR_MERGE_SHA: $(git log -1 --format=%H)\"\n-            echo \"PR_MERGE_SHA=$(git log -1 --format=%H)\" >> \"$GITHUB_OUTPUT\"\n-            PR_MERGE_COMMIT_TIMESTAMP=$(git log -1 --date=unix --format=%cd)\n-            echo \"PR_MERGE_COMMIT_TIMESTAMP: $PR_MERGE_COMMIT_TIMESTAMP\"\n             COMMENT_TIMESTAMP=$(date -d \"${COMMENT_DATE}\" +\"%s\")\n             echo \"COMMENT_DATE: $COMMENT_DATE\"\n             echo \"COMMENT_TIMESTAMP: $COMMENT_TIMESTAMP\"\n@@ -87,25 +59,22 @@ jobs:\n               exit -1;\n             fi\n \n-  # use a python script to handle this complex logic\n-  # case 1: `run-slow` (auto. infer with limited number of models, but in particular, new model)\n-  # case 2: `run-slow model_1, model_2`\n+  # use a python script to handle this complex logic.\n   get-tests:\n     runs-on: ubuntu-22.04\n-    needs: [get-pr-number, get-sha]\n-    if: ${{ needs.get-pr-number.outputs.PR_NUMBER != ''}}\n+    needs: [get-pr-number, check-timestamps]\n     outputs:\n       models: ${{ steps.models_to_run.outputs.models }}\n       quantizations: ${{ steps.models_to_run.outputs.quantizations }}\n     steps:\n       - uses: actions/checkout@v4\n         with:\n           fetch-depth: \"0\"\n-          ref: \"refs/pull/${{needs.get-pr-number.outputs.PR_NUMBER}}/merge\"\n+          ref: \"refs/pull/${{ needs.get-pr-number.outputs.PR_NUMBER }}/merge\"\n \n       - name: Verify merge commit SHA\n         env:\n-          VERIFIED_PR_MERGE_SHA: ${{ needs.get-sha.outputs.PR_MERGE_SHA }}\n+          VERIFIED_PR_MERGE_SHA: ${{ needs.check-timestamps.outputs.PR_MERGE_SHA }}\n         run: |\n             PR_MERGE_SHA=$(git log -1 --format=%H)\n             if [ $PR_MERGE_SHA != $VERIFIED_PR_MERGE_SHA ]; then\n@@ -119,19 +88,39 @@ jobs:\n         run: |\n           python -m pip install GitPython\n           python utils/pr_slow_ci_models.py --message \"$PR_COMMENT\" | tee output.txt\n-          echo \"models=$(tail -n 1 output.txt)\" >> $GITHUB_ENV\n+          echo 'models=$(tail -n 1 output.txt)' >> $GITHUB_ENV\n           python utils/pr_slow_ci_models.py --message \"$PR_COMMENT\" --quantization | tee output2.txt\n-          echo \"quantizations=$(tail -n 1 output2.txt)\" >> $GITHUB_ENV\n+          echo 'quantizations=$(tail -n 1 output2.txt)' >> $GITHUB_ENV\n \n       - name: Show models to test\n         id: models_to_run\n         run: |\n           echo \"${{ env.models }}\"\n-          echo \"models=${{ env.models }}\" >> $GITHUB_ENV\n           echo \"models=${{ env.models }}\" >> $GITHUB_OUTPUT\n           echo \"${{ env.quantizations }}\"\n           echo \"quantizations=${{ env.quantizations }}\" >> $GITHUB_OUTPUT\n \n+  # Report back if we are not able to get the tests (for example, security check is failing)\n+  report_error_earlier:\n+    name: Report error earlier\n+    if: ${{ always() && needs.get-pr-info.result == 'success' && needs.get-tests.result != 'success' }}\n+    needs: [get-pr-number, get-pr-info, get-tests]\n+    permissions:\n+      pull-requests: write\n+    runs-on: ubuntu-22.04\n+    steps:\n+      - name: Reply to the comment\n+        env:\n+          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n+          GITHUB_RUN_URL: https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}\n+        run: |\n+          gh api \\\n+            --method POST \\\n+            -H \"Accept: application/vnd.github+json\" \\\n+            -H \"X-GitHub-Api-Version: 2022-11-28\" \\\n+            repos/${{ github.repository }}/issues/${{ needs.get-pr-number.outputs.PR_NUMBER }}/comments \\\n+            -f body=\"\ud83d\udc94 This comment contains \\`run-slow\\`, but unknown error occurred and [the workflow run]($GITHUB_RUN_URL) aborted!\"\n+\n   reply_to_comment:\n     name: Reply to the comment\n     if: ${{ needs.get-tests.outputs.models != '[]'  || needs.get-tests.outputs.quantizations != '[]' }}\n@@ -143,20 +132,18 @@ jobs:\n       - name: Reply to the comment\n         env:\n           GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n-          MODELS: ${{ needs.get-tests.outputs.models }}\n-          BODY: \"\\n\\nmodels: ${{ needs.get-tests.outputs.models }}\\nquantizations: ${{ needs.get-tests.outputs.quantizations }}\"\n+          BODY: '\\n\\nmodels: ${{ needs.get-tests.outputs.models }}\\nquantizations: ${{ needs.get-tests.outputs.quantizations }}'\n         run: |\n           gh api \\\n             --method POST \\\n             -H \"Accept: application/vnd.github+json\" \\\n             -H \"X-GitHub-Api-Version: 2022-11-28\" \\\n             repos/${{ github.repository }}/issues/${{ needs.get-pr-number.outputs.PR_NUMBER }}/comments \\\n-            -f \"body=This comment contains run-slow, running the specified jobs: ${{ env.BODY }} ...\"\n+            -f body=\"This comment contains \\`run-slow\\`, running the specified jobs: $(echo -e '${{ env.BODY }}')\"\n \n   create_run:\n     name: Create run\n-    if: ${{ needs.get-tests.outputs.models != '[]' || needs.get-tests.outputs.quantizations != '[]' }}\n-    needs: [get-sha, get-tests, reply_to_comment]\n+    needs: [check-timestamps, reply_to_comment]\n     permissions:\n       statuses: write\n     runs-on: ubuntu-22.04\n@@ -173,243 +160,179 @@ jobs:\n             --method POST \\\n             -H \"Accept: application/vnd.github+json\" \\\n             -H \"X-GitHub-Api-Version: 2022-11-28\" \\\n-            repos/${{ github.repository }}/statuses/${{ needs.get-sha.outputs.PR_HEAD_SHA }} \\\n+            repos/${{ github.repository }}/statuses/${{ needs.check-timestamps.outputs.PR_HEAD_SHA }} \\\n             -f \"target_url=$GITHUB_RUN_URL\" -f \"state=pending\" -f \"description=Slow CI job\" -f \"context=pytest/custom-tests\"\n \n-  run_models_gpu:\n-    name: Run all tests for the model\n+  model-ci:\n+    name: Model CI\n     if: ${{ needs.get-tests.outputs.models != '[]' }}\n-    needs: [get-pr-number, get-sha, get-tests, create_run]\n-    strategy:\n-      fail-fast: false\n-      matrix:\n-        folders: ${{ fromJson(needs.get-tests.outputs.models) }}\n-        machine_type: [aws-g5-4xlarge-cache, aws-g5-12xlarge-cache]\n-    runs-on:\n-       group: '${{ matrix.machine_type }}'\n-    container:\n-      image: huggingface/transformers-all-latest-gpu\n-      options: --gpus all --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n-    steps:\n-      - name: Echo input and matrix info\n-        shell: bash\n-        run: |\n-          echo \"${{ matrix.folders }}\"\n-\n-      - name: Echo folder ${{ matrix.folders }}\n-        shell: bash\n-        # For folders like `models/bert`, set an env. var. (`matrix_folders`) to `models_bert`, which will be used to\n-        # set the artifact folder names (because the character `/` is not allowed).\n-        run: |\n-          echo \"${{ matrix.folders }}\"\n-          matrix_folders=${{ matrix.folders }}\n-          matrix_folders=${matrix_folders/'models/'/'models_'}\n-          echo \"$matrix_folders\"\n-          echo \"matrix_folders=$matrix_folders\" >> $GITHUB_ENV\n-\n-      - name: Checkout to PR merge commit\n-        working-directory: /transformers\n-        run: |\n-          git fetch origin refs/pull/${{ needs.get-pr-number.outputs.PR_NUMBER }}/merge:refs/remotes/pull/${{ needs.get-pr-number.outputs.PR_NUMBER }}/merge\n-          git checkout refs/remotes/pull/${{ needs.get-pr-number.outputs.PR_NUMBER }}/merge\n-          git log -1 --format=%H\n-\n-      - name: Verify merge commit SHA\n-        env:\n-          VERIFIED_PR_MERGE_SHA: ${{ needs.get-sha.outputs.PR_MERGE_SHA }}\n-        working-directory: /transformers\n-        run: |\n-          PR_MERGE_SHA=$(git log -1 --format=%H)\n-          if [ $PR_MERGE_SHA != $VERIFIED_PR_MERGE_SHA ]; then\n-            echo \"The merged commit SHA is not the same as the verified one! Security issue detected, abort the workflow!\";\n-            exit -1;\n-          fi\n-\n-      - name: Reinstall transformers in edit mode (remove the one installed during docker image build)\n-        working-directory: /transformers\n-        run: python3 -m pip uninstall -y transformers && python3 -m pip install -e .\n-\n-      - name: NVIDIA-SMI\n-        run: |\n-          nvidia-smi\n-\n-      - name: Set `machine_type` for report and artifact names\n-        working-directory: /transformers\n-        shell: bash\n-        run: |\n-          echo \"${{ matrix.machine_type }}\"\n-          if [ \"${{ matrix.machine_type }}\" = \"aws-g5-4xlarge-cache\" ]; then\n-            machine_type=single-gpu\n-          elif [ \"${{ matrix.machine_type }}\" = \"aws-g5-12xlarge-cache\" ]; then\n-            machine_type=multi-gpu\n-          else\n-            machine_type=${{ matrix.machine_type }}\n-          fi\n-          echo \"$machine_type\"\n-          echo \"machine_type=$machine_type\" >> $GITHUB_ENV\n-\n-      - name: Environment\n-        working-directory: /transformers\n-        run: |\n-          python3 utils/print_env.py\n-\n-      - name: Show installed libraries and their versions\n-        working-directory: /transformers\n-        run: pip freeze\n-\n-      - name: Run all tests on GPU\n-        working-directory: /transformers\n-        run: |\n-          export CUDA_VISIBLE_DEVICES=\"$(python3 utils/set_cuda_devices_for_ci.py --test_folder ${{ matrix.folders }})\"\n-          echo $CUDA_VISIBLE_DEVICES\n-          python3 -m pytest -v -rsfE --make-reports=${{ env.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports tests/${{ matrix.folders }}\n-\n-      - name: Failure short reports\n-        if: ${{ failure() }}\n-        continue-on-error: true\n-        run: cat /transformers/reports/${{ env.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports/failures_short.txt\n-\n-      - name: Make sure report directory exists\n-        shell: bash\n-        run: |\n-          mkdir -p /transformers/reports/${{ env.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports\n-          echo \"hello\" > /transformers/reports/${{ env.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports/hello.txt\n-          echo \"${{ env.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports\"\n-\n-      - name: \"Test suite reports artifacts: ${{ env.machine_type }}_run_models_gpu_${{ env.matrix_folders }}_test_reports\"\n-        if: ${{ always() }}\n-        uses: actions/upload-artifact@v4\n-        with:\n-          name: ${{ env.machine_type }}_run_models_gpu_${{ env.matrix_folders }}_test_reports\n-          path: /transformers/reports/${{ env.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports\n-\n-  run_quantization_torch_gpu:\n-    name: Run all tests for a quantization\n+    uses: ./.github/workflows/self-scheduled.yml\n+    needs: [get-pr-number, check-timestamps, get-tests, create_run]\n+    with:\n+      job: run_models_gpu\n+      slack_report_channel: \"#transformers-ci-pr\"\n+      docker: huggingface/transformers-all-latest-gpu\n+      ci_event: PR Comment CI\n+      report_repo_id: hf-internal-testing/transformers_pr_ci\n+      commit_sha: ${{ needs.check-timestamps.outputs.PR_MERGE_SHA }}\n+      subdirs: ${{ needs.get-tests.outputs.models }}\n+      pr_number: ${{ needs.get-pr-number.outputs.PR_NUMBER }}\n+    secrets: inherit\n+\n+  quantization-ci:\n+    name: Quantization CI\n     if: ${{ needs.get-tests.outputs.quantizations != '[]' }}\n-    needs: [get-pr-number, get-sha, get-tests, create_run]\n-    strategy:\n-      fail-fast: false\n-      matrix:\n-        folders: ${{ fromJson(needs.get-tests.outputs.quantizations) }}\n-        machine_type: [aws-g5-4xlarge-cache, aws-g5-12xlarge-cache]\n-    runs-on:\n-      group: '${{ matrix.machine_type }}'\n-    container:\n-      image: huggingface/transformers-quantization-latest-gpu\n-      options: --gpus all --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n+    uses: ./.github/workflows/self-scheduled.yml\n+    needs: [get-pr-number, check-timestamps, get-tests, create_run]\n+    with:\n+      job: run_quantization_torch_gpu\n+      slack_report_channel: \"#transformers-ci-pr\"\n+      docker: huggingface/transformers-quantization-latest-gpu\n+      ci_event: PR Comment CI\n+      report_repo_id: hf-internal-testing/transformers_pr_ci\n+      commit_sha: ${{ needs.check-timestamps.outputs.PR_MERGE_SHA }}\n+      subdirs: ${{ needs.get-tests.outputs.quantizations }}\n+      pr_number: ${{ needs.get-pr-number.outputs.PR_NUMBER }}\n+    secrets: inherit\n+\n+  report:\n+    name: Check & Report\n+    needs: [get-pr-number, check-timestamps, create_run, model-ci, quantization-ci]\n+    permissions:\n+      pull-requests: write\n+      statuses: write\n+    if: ${{ always() && needs.create_run.result == 'success' }}\n+    runs-on: ubuntu-22.04\n     steps:\n-      - name: Echo folder ${{ matrix.folders }}\n-        shell: bash\n-        run: |\n-          echo \"${{ matrix.folders }}\"\n-          matrix_folders=${{ matrix.folders }}\n-          matrix_folders=${matrix_folders/'quantization/'/'quantization_'}\n-          echo \"$matrix_folders\"\n-          echo \"matrix_folders=$matrix_folders\" >> $GITHUB_ENV\n-\n-      - name: Checkout to PR merge commit\n-        working-directory: /transformers\n+      - name: Show reports from jobs\n         run: |\n-          git fetch origin refs/pull/${{ needs.get-pr-number.outputs.PR_NUMBER }}/merge:refs/remotes/pull/${{ needs.get-pr-number.outputs.PR_NUMBER }}/merge\n-          git checkout refs/remotes/pull/${{ needs.get-pr-number.outputs.PR_NUMBER }}/merge\n-          git log -1 --format=%H\n+          echo \"${{ needs.model-ci.outputs.report }}\"\n+          echo \"${{ needs.quantization-ci.outputs.report }}\"\n \n-      - name: Verify merge commit SHA\n+      - name: Process and filter reports\n         env:\n-          VERIFIED_PR_MERGE_SHA: ${{ needs.get-sha.outputs.PR_MERGE_SHA }}\n-        working-directory: /transformers\n-        run: |\n-          PR_MERGE_SHA=$(git log -1 --format=%H)\n-          if [ $PR_MERGE_SHA != $VERIFIED_PR_MERGE_SHA ]; then\n-            echo \"The merged commit SHA is not the same as the verified one! Security issue detected, abort the workflow!\";\n-            exit -1;\n-          fi\n-\n-      - name: Reinstall transformers in edit mode (remove the one installed during docker image build)\n-        working-directory: /transformers\n-        run: python3 -m pip uninstall -y transformers && python3 -m pip install -e .\n-      - name: NVIDIA-SMI\n-        run: |\n-          nvidia-smi\n-\n-      - name: Set `machine_type` for report and artifact names\n-        working-directory: /transformers\n-        shell: bash\n-        run: |\n-          echo \"${{ matrix.machine_type }}\"\n-          if [ \"${{ matrix.machine_type }}\" = \"aws-g5-4xlarge-cache\" ]; then\n-            machine_type=single-gpu\n-          elif [ \"${{ matrix.machine_type }}\" = \"aws-g5-12xlarge-cache\" ]; then\n-            machine_type=multi-gpu\n-          else\n-            machine_type=${{ matrix.machine_type }}\n-          fi\n-          echo \"$machine_type\"\n-          echo \"machine_type=$machine_type\" >> $GITHUB_ENV\n-\n-      - name: Environment\n-        working-directory: /transformers\n-        run: |\n-          python3 utils/print_env.py\n-\n-      - name: Show installed libraries and their versions\n-        working-directory: /transformers\n-        run: pip freeze\n-\n-      - name: Run quantization tests on GPU\n-        working-directory: /transformers\n+          MODEL_REPORT: ${{ needs.model-ci.outputs.report }}\n+          QUANT_REPORT: ${{ needs.quantization-ci.outputs.report }}\n         run: |\n-          python3 -m pytest -v --make-reports=${{ env.machine_type }}_run_quantization_torch_gpu_${{ matrix.folders }}_test_reports tests/${{ matrix.folders }}\n-\n-      - name: Failure short reports\n-        if: ${{ failure() }}\n-        continue-on-error: true\n-        run: cat /transformers/reports/${{ env.machine_type }}_run_quantization_torch_gpu_${{ matrix.folders }}_test_reports/failures_short.txt\n-\n-      - name: Make sure report directory exists\n-        shell: bash\n+          # Preprocess with Python\n+          python3 << 'PYTHON_SCRIPT'\n+          import json\n+          import os\n+          \n+          def filter_and_format_report(data):\n+            \"\"\"\n+            Filter out entries where commit is `None` (failing tests who status is not certain) and format as text\n+            \"\"\"\n+            lines = []\n+            \n+            for model, model_result in data.items():\n+                model_lines = []\n+                for device, failures in model_result.items():\n+                    \n+                    # Filter out None commits and extract just the test names\n+                    test_names = [\n+                        failure['test'] \n+                        for failure in failures \n+                        if isinstance(failure, dict) and failure.get('commit') is not None\n+                    ]\n+\n+                    # Add tests to model lines\n+                    for idx, test_name in enumerate(test_names):\n+                        if idx == 0:\n+                            job_link = failures[idx]['job_link']\n+                            model_lines.append(f\"- [{model}]({job_link}):\")\n+          \n+                        model_lines.append(f\"    {test_name}\")\n+\n+                # Only add model section if it has tests\n+                if len(model_lines) > 0:\n+                    lines.extend(model_lines)\n+                    lines.append(\"\")  # Empty line between models\n+            \n+            return \"\\n\".join(lines).strip()\n+          \n+          # Load and filter reports\n+          model_report_str = os.environ.get('MODEL_REPORT', '{}')\n+          quant_report_str = os.environ.get('QUANT_REPORT', '{}')\n+          \n+          model_report = json.loads(model_report_str) if model_report_str else {}\n+          quant_report = json.loads(quant_report_str) if quant_report_str else {}\n+          \n+          formatted_model = filter_and_format_report(model_report)\n+          formatted_quant = filter_and_format_report(quant_report)\n+          \n+          # Write to files\n+          with open('model_ci.txt', 'w') as f:\n+              f.write(formatted_model)\n+              if formatted_model:\n+                  f.write('\\n')\n+          \n+          with open('quantization_ci.txt', 'w') as f:\n+              f.write(formatted_quant)\n+              if formatted_quant:\n+                  f.write('\\n')\n+          PYTHON_SCRIPT\n+\n+      - name: Post results as PR comment\n+        env:\n+          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n+          GITHUB_RUN_URL: https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}\n         run: |\n-          mkdir -p /transformers/reports/${{ env.machine_type }}_run_quantization_gpu_${{ matrix.folders }}_test_reports\n-          echo \"hello\" > /transformers/reports/${{ env.machine_type }}_run_quantization_gpu_${{ matrix.folders }}_test_reports/hello.txt\n-          echo \"${{ env.machine_type }}_run_quantization_gpu_${{ matrix.folders }}_test_reports\"\n-\n-      - name: \"Test suite reports artifacts: ${{ env.machine_type }}_run_quantization_torch_gpu_${{ env.matrix_folders }}_test_reports\"\n-        if: ${{ always() }}\n-        uses: actions/upload-artifact@v4\n-        with:\n-          name: ${{ env.machine_type }}_run_quantization_torch_gpu_${{ env.matrix_folders }}_test_reports\n-          path: /transformers/reports/${{ env.machine_type }}_run_quantization_torch_gpu_${{ matrix.folders }}_test_reports\n+          {\n+            echo '## CI Results'\n+            echo \"[Workflow Run \u2699\ufe0f]($GITHUB_RUN_URL)\"\n+            echo ''\n+\n+            # Check if both jobs were skipped or cancelled\n+            if [[ \"${{ needs.model-ci.result }}\" == \"skipped\" || \"${{ needs.model-ci.result }}\" == \"cancelled\" ]] && \\\n+               [[ \"${{ needs.quantization-ci.result }}\" == \"skipped\" || \"${{ needs.quantization-ci.result }}\" == \"cancelled\" ]]; then\n+              echo '\u26a0\ufe0f No test being reported (jobs are skipped or cancelled)!'\n+              echo \"STATUS=error\" >> $GITHUB_ENV\n+\n+            # Check if either file has content\n+            elif [ -s model_ci.txt ] || [ -s quantization_ci.txt ]; then\n+              echo \"STATUS=failure\" >> $GITHUB_ENV\n+\n+              # Check if model_ci.txt has content\n+              if [ -s model_ci.txt ]; then\n+                echo '### Model CI Report'\n+                echo ''\n+                echo '#### \u274c Failed tests'\n+                echo ''\n+                cat model_ci.txt\n+                echo ''\n+              fi\n+              \n+              # Check if quantization_ci.txt has content\n+              if [ -s quantization_ci.txt ]; then\n+                echo '### Quantization CI Report'\n+                echo ''\n+                echo '#### \u274c Failed tests'\n+                echo ''\n+                cat quantization_ci.txt\n+                echo ''\n+              fi\n+            else\n+              echo \"STATUS=success\" >> $GITHUB_ENV\n+              echo '\u2705 No failing test specific to this PR \ud83c\udf89 !'\n+            fi\n+          } > comment_body.txt\n \n-  update_run_status:\n-    name: Update Check Run Status\n-    needs: [get-sha, create_run, run_models_gpu, run_quantization_torch_gpu]\n-    permissions:\n-      statuses: write\n-    if: ${{ always() && needs.create_run.result == 'success' }}\n-    runs-on: ubuntu-22.04\n-    env:\n-      GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n-      GITHUB_RUN_URL: https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}\n-      STATUS_OK: ${{ contains(fromJSON('[\"skipped\", \"success\"]'), needs.run_models_gpu.result) && contains(fromJSON('[\"skipped\", \"success\"]'), needs.run_quantization_torch_gpu.result) }}\n-    steps:\n-      - name: Get `run_models_gpu` job status\n-        run: |\n-          echo \"${{ needs.run_models_gpu.result }}\"\n-          echo \"${{ needs.run_quantization_torch_gpu.result }}\"\n-          echo $STATUS_OK\n-          if [ \"$STATUS_OK\" = \"true\" ]; then\n-            echo \"STATUS=success\" >> $GITHUB_ENV\n-          else\n-            echo \"STATUS=failure\" >> $GITHUB_ENV\n-          fi\n+          gh api \\\n+            --method POST \\\n+            -H \"Accept: application/vnd.github+json\" \\\n+            -H \"X-GitHub-Api-Version: 2022-11-28\" \\\n+            repos/${{ github.repository }}/issues/${{ needs.get-pr-number.outputs.PR_NUMBER }}/comments \\\n+            -F body=@comment_body.txt\n \n       - name: Update PR commit statuses\n+        env:\n+          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n+          GITHUB_RUN_URL: https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}\n         run: |\n-          echo \"${{ needs.run_models_gpu.result }}\"\n-          echo \"${{ env.STATUS }}\"\n           gh api \\\n             --method POST \\\n             -H \"Accept: application/vnd.github+json\" \\\n             -H \"X-GitHub-Api-Version: 2022-11-28\" \\\n-            repos/${{ github.repository }}/statuses/${{ needs.get-sha.outputs.PR_HEAD_SHA }} \\\n+            repos/${{ github.repository }}/statuses/${{ needs.check-timestamps.outputs.PR_HEAD_SHA }} \\\n             -f \"target_url=$GITHUB_RUN_URL\" -f \"state=${{ env.STATUS }}\" -f \"description=Slow CI job\" -f \"context=pytest/custom-tests\""
      },
      {
        "filename": ".github/workflows/self-nightly-caller.yml",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -51,6 +51,7 @@ jobs:\n       slack_report_channel: \"#transformers-ci-past-future\"\n       docker: huggingface/transformers-all-latest-torch-nightly-gpu\n       ci_event: Nightly CI\n+      runner_type: \"a10\"\n       report_repo_id: hf-internal-testing/transformers_daily_ci_with_torch_nightly\n       commit_sha: ${{ github.event.workflow_run.head_sha || github.sha }}\n     secrets: inherit"
      },
      {
        "filename": ".github/workflows/self-scheduled.yml",
        "status": "modified",
        "additions": 14,
        "deletions": 6,
        "changes": 20,
        "patch": "@@ -34,14 +34,20 @@ on:\n       runner_type:\n         required: false\n         type: string\n-      models:\n+      subdirs:\n         default: \"\"\n         required: false\n         type: string\n       pytest_marker:\n         required: false\n         type: string\n-\n+      pr_number:\n+        required: false\n+        type: string\n+    outputs:\n+      report:\n+        description: \"Content of the report of new failures\"\n+        value: ${{ jobs.check_new_failures.outputs.report }}\n \n env:\n   HF_HOME: /mnt/cache\n@@ -76,6 +82,7 @@ jobs:\n       - name: Update clone\n         working-directory: /transformers\n         run: |\n+          git fetch origin ${{ inputs.commit_sha || github.sha }}\n           git fetch && git checkout ${{ inputs.commit_sha || github.sha }}\n \n       - name: Cleanup\n@@ -95,7 +102,7 @@ jobs:\n         working-directory: /transformers/tests\n         run: |\n           if [ \"${{ inputs.job }}\" = \"run_models_gpu\" ]; then\n-            echo \"folder_slices=$(python3 ../utils/split_model_tests.py --models '${{ inputs.models }}' --num_splits ${{ env.NUM_SLICES }})\" >> $GITHUB_OUTPUT\n+            echo \"folder_slices=$(python3 ../utils/split_model_tests.py --subdirs '${{ inputs.subdirs }}' --num_splits ${{ env.NUM_SLICES }})\" >> $GITHUB_OUTPUT\n             echo \"slice_ids=$(python3 -c 'd = list(range(${{ env.NUM_SLICES }})); print(d)')\" >> $GITHUB_OUTPUT\n           elif [ \"${{ inputs.job }}\" = \"run_trainer_and_fsdp_gpu\" ]; then\n             echo \"folder_slices=[['trainer'], ['fsdp']]\" >> $GITHUB_OUTPUT\n@@ -107,7 +114,7 @@ jobs:\n         name: Identify quantization method to test\n         working-directory: /transformers/tests\n         run: |\n-          echo \"quantization_matrix=$(python3 -c 'import os; tests = os.getcwd(); quantization_tests = os.listdir(os.path.join(tests, \"quantization\")); d = sorted(list(filter(os.path.isdir, [f\"quantization/{x}\" for x in quantization_tests]))) ;  print(d)')\" >> $GITHUB_OUTPUT\n+          echo \"quantization_matrix=$(python3 -c 'import ast; import os; tests = os.getcwd(); quantization_tests = os.listdir(os.path.join(tests, \"quantization\")); subdirs = ast.literal_eval(${{ inputs.subdirs || '\"None\"' }}); quantization_tests = [x.removeprefix(\"quantization/\") for x in subdirs] if subdirs is not None else quantization_tests; d = sorted(list(filter(os.path.isdir, [f\"quantization/{x}\" for x in quantization_tests]))) ;  print(d)')\" >> $GITHUB_OUTPUT\n \n       - name: NVIDIA-SMI\n         run: |\n@@ -539,16 +546,17 @@ jobs:\n     secrets: inherit\n \n   check_new_failures:\n-    if: ${{ always() && inputs.ci_event == 'Daily CI' && needs.send_results.result == 'success' }}\n+    if: ${{ always() && needs.send_results.result == 'success' }}\n     name: Check new failures\n     needs: send_results\n     uses: ./.github/workflows/check_failed_tests.yml\n     with:\n       docker: ${{ inputs.docker }}\n-      start_sha: ${{ inputs.commit_sha || github.sha }}\n+      commit_sha: ${{ inputs.commit_sha || github.sha }}\n       job: ${{ inputs.job }}\n       slack_report_channel: ${{ inputs.slack_report_channel }}\n       ci_event: ${{ inputs.ci_event }}\n       report_repo_id: ${{ inputs.report_repo_id }}\n+      pr_number: ${{ inputs.pr_number }}\n \n     secrets: inherit"
      },
      {
        "filename": "utils/check_bad_commit.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -151,7 +151,7 @@ def find_bad_commit(target_test, start_commit, end_commit):\n \n     bash = f\"\"\"\n git bisect reset\n-git bisect start {start_commit} {end_commit}\n+git bisect start --first-parent {start_commit} {end_commit}\n git bisect run python3 target_script.py\n \"\"\"\n "
      },
      {
        "filename": "utils/notification_service.py",
        "status": "modified",
        "additions": 10,
        "deletions": 0,
        "changes": 10,
        "patch": "@@ -1521,6 +1521,16 @@ def pop_default(l: list[Any], i: int, default: Any) -> Any:\n                 token=os.environ[\"ACCESS_REPO_INFO_TOKEN\"], workflow_id=other_workflow_id, commit_sha=ci_sha\n             )\n             other_workflow_run_ids.append(other_workflow_run_id)\n+    # triggered via `issue_comment` for CI on pull requests (e.g. using the comment `run-slow:`)\n+    elif os.environ.get(\"GITHUB_EVENT_NAME\") in [\"issue_comment\"]:\n+        # TODO (ydshieh): Make this flexible once we implement `run-slow` for AMD CI and others.\n+        # The id of the workflow `.github/workflows/self-scheduled-caller.yml` (not of a workflow run of it).\n+        prev_workflow_id = \"90575235\"\n+        # TODO (ydshieh): It's better to make sure using the last completed scheduled workflow run with the commit being a parent\n+        #  of the PR's `merge_commit`.\n+        prev_workflow_run_id = get_last_daily_ci_workflow_run_id(\n+            token=os.environ[\"ACCESS_REPO_INFO_TOKEN\"], workflow_id=prev_workflow_id\n+        )\n     else:\n         prev_workflow_run_id = os.environ[\"PREV_WORKFLOW_RUN_ID\"]\n         other_workflow_run_id = os.environ[\"OTHER_WORKFLOW_RUN_ID\"]"
      },
      {
        "filename": "utils/pr_slow_ci_models.py",
        "status": "modified",
        "additions": 4,
        "deletions": 1,
        "changes": 5,
        "patch": "@@ -27,6 +27,7 @@\n \"\"\"\n \n import argparse\n+import json\n import os.path\n import re\n import string\n@@ -169,4 +170,6 @@ def check_model_names(model_name: str):\n         elif os.path.isdir(f\"tests/quantization/{model}\"):\n             final_list.append(f\"quantization/{model}\")\n \n-    print(sorted(set(final_list)))\n+    # Use `json.dumps` to get the double quotes instead of single quote, e.g. `[\"model/vit\"]`.\n+    # (to avoid some shell expansion issues when this script is called from a Github Actions workflow)\n+    print(json.dumps(sorted(set(final_list))))"
      },
      {
        "filename": "utils/process_bad_commit_report.py",
        "status": "modified",
        "additions": 19,
        "deletions": 15,
        "changes": 34,
        "patch": "@@ -45,6 +45,25 @@\n \n     report_repo_id = os.getenv(\"REPORT_REPO_ID\")\n \n+    with open(\"new_failures_with_bad_commit.json\") as fp:\n+        data = json.load(fp)\n+\n+    with open(f\"ci_results_{job_name}/job_links.json\") as fp:\n+        job_links = json.load(fp)\n+\n+    # Update `new_failures_with_bad_commit.json` with job links information before uploading to Hub repository\n+    #   - need to change `single-gpu` to `single` and same for `multi-gpu` to match the keys in `job_link`.\n+    for model, model_result in data.items():\n+        for device, failed_tests in model_result.items():\n+            for failed_test in failed_tests:\n+                key = model\n+                if list(job_links.keys()) == [job_name]:\n+                    key = job_name\n+                failed_test[\"job_link\"] = job_links[key][device.replace(\"-gpu\", \"\")]\n+\n+    with open(\"new_failures_with_bad_commit.json\", \"w\") as fp:\n+        json.dump(data, fp, indent=4, ensure_ascii=False)\n+\n     commit_info = api.upload_file(\n         path_or_fileobj=\"new_failures_with_bad_commit.json\",\n         path_in_repo=f\"{report_repo_folder}/ci_results_{job_name}/new_failures_with_bad_commit.json\",\n@@ -53,12 +72,6 @@\n         token=os.environ.get(\"TRANSFORMERS_CI_RESULTS_UPLOAD_TOKEN\", None),\n     )\n \n-    with open(\"new_failures_with_bad_commit.json\") as fp:\n-        data = json.load(fp)\n-\n-    with open(f\"ci_results_{job_name}/job_links.json\") as fp:\n-        job_links = json.load(fp)\n-\n     # TODO: extend\n     team_members = [\n         \"ArthurZucker\",\n@@ -101,16 +114,7 @@\n     for author, _data in new_data_full.items():\n         for model, model_result in _data.items():\n             for device, failed_tests in model_result.items():\n-                # prepare job_link and add it to each entry of new failed test information.\n-                # need to change from `single-gpu` to `single` and same for `multi-gpu` to match `job_link`.\n-                key = model\n-                if list(job_links.keys()) == [job_name]:\n-                    key = job_name\n-                job_link = job_links[key][device.replace(\"-gpu\", \"\")]\n-\n                 failed_tests = [x for x in failed_tests if x[\"author\"] == author or x[\"merged_by\"] == author]\n-                for x in failed_tests:\n-                    x.update({\"job_link\": job_link})\n                 model_result[device] = failed_tests\n             _data[model] = {k: v for k, v in model_result.items() if len(v) > 0}\n         new_data_full[author] = {k: v for k, v in _data.items() if len(v) > 0}"
      },
      {
        "filename": "utils/split_model_tests.py",
        "status": "modified",
        "additions": 14,
        "deletions": 5,
        "changes": 19,
        "patch": "@@ -40,10 +40,10 @@\n if __name__ == \"__main__\":\n     parser = argparse.ArgumentParser()\n     parser.add_argument(\n-        \"--models\",\n+        \"--subdirs\",\n         type=str,\n         default=\"\",\n-        help=\"the list of pre-computed model names.\",\n+        help=\"the list of pre-computed model names (directory names under `tests/models`) or directory names under `tests` (except `models`).\",\n     )\n     parser.add_argument(\n         \"--num_splits\",\n@@ -60,9 +60,18 @@\n     d1.remove(\"models\")\n     d = d2 + d1\n \n-    if args.models != \"\":\n-        model_tests = ast.literal_eval(args.models)\n-        d = sorted(filter(os.path.isdir, [f\"models/{x}\" for x in model_tests]))\n+    if args.subdirs != \"\":\n+        model_tests = ast.literal_eval(args.subdirs)\n+        # We handle both cases with and without prefix because `push-important-models.yml` returns the list without\n+        # the prefix (i.e. `models`) but `utils/pr_slow_ci_models.py` (called by `self-comment-ci.yml`) returns the\n+        # list with the prefix (`models`) and some directory names under `tests`.\n+        d = []\n+        for x in model_tests:\n+            if os.path.isdir(x):\n+                d.append(x)\n+            if os.path.isdir(f\"models/{x}\"):\n+                d.append(f\"models/{x}\")\n+        d = sorted(d)\n \n     num_jobs = len(d)\n     num_jobs_per_splits = num_jobs // args.num_splits"
      }
    ],
    "num_files": 12,
    "scraped_at": "2025-11-16T21:16:34.488800",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR contains substantial changes to CI/CD workflows and utility scripts that refactor how PR testing is triggered and reported. It involves architectural decisions about workflow composition, parameter passing, commit tracking logic, and integration patterns across multiple GitHub Actions workflows\u2014all of which provide meaningful context for technical questions about CI infrastructure.",
      "substance_level": "medium"
    }
  },
  {
    "pr_number": 41897,
    "title": "[FPQuant] MXFP8 and MXFP4 backwards support",
    "body": "# What does this PR do?\r\n\r\nThis PR adds MXFP4 and MXFP8 backwards support in combination with MXFP4 forward, allowing for lightning-fast QAT on Blackwell GPUs.\r\n\r\nIt's blocked by `QuTLASS v0.2.0` and `fp_quant v0.3.0` release we're hoping to release within a day or two.\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker @Cyrilvallez\r\n- vision models: @yonigozlan @molbap\r\n- audio models: @eustlb @ebezzam @vasqu\r\n- multimodal models: @zucchini-nlp\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- generate: @zucchini-nlp (visual-language models) or @gante (all others)\r\n- continuous batching: @remi-or @ArthurZucker @McPatate\r\n- pipelines: @Rocketknight1\r\n- tokenizers: @ArthurZucker and @itazap\r\n- trainer: @SunMarc\r\n- attention: @vasqu @ArthurZucker @CyrilVallez\r\n- model loading (from pretrained, etc): @CyrilVallez\r\n- distributed: @3outeille @ArthurZucker\r\n- CIs: @ydshieh\r\n\r\nIntegrations:\r\n\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization: @SunMarc @MekkCyber\r\n- kernels: @MekkCyber @drbh\r\n- peft: @BenjaminBossan @githubnemo\r\n\r\nDevices/Backends:\r\n\r\n- AMD ROCm: @ivarflakstad\r\n- Intel XPU: @IlyasMoutawwakil\r\n- Ascend NPU: @ivarflakstad \r\n\r\nDocumentation: @stevhliu\r\n\r\nResearch projects are not maintained and should be taken as is.\r\n\r\n -->\r\n\r\n@SunMarc ",
    "html_url": "https://github.com/huggingface/transformers/pull/41897",
    "created_at": "2025-10-27T16:29:33Z",
    "merged_at": "2025-11-04T16:52:48Z",
    "merge_commit_sha": "020e713ac8e70bd2e72bcd12dc6bd1ada6162562",
    "base_ref": "main",
    "head_sha": "e3718018e1f600354bd92bbff751851cb7ce3e03",
    "user": "BlackSamorez",
    "files": [
      {
        "filename": "docker/transformers-quantization-latest-gpu/Dockerfile",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -81,7 +81,7 @@ RUN python3 -m pip uninstall -y flash-attn\n RUN cd transformers && python3 setup.py develop\n \n # Add fp-quant for quantization testing\n-RUN python3 -m pip install --no-cache-dir \"fp-quant>=0.2.0\"\n+RUN python3 -m pip install --no-cache-dir \"fp-quant>=0.3.2\"\n \n # Low usage or incompatible lib, will enable later on\n "
      },
      {
        "filename": "src/transformers/integrations/fp_quant.py",
        "status": "modified",
        "additions": 4,
        "deletions": 0,
        "changes": 4,
        "patch": "@@ -35,6 +35,10 @@ def adapt_fp_quant_config(config: FPQuantConfig):\n \n     if config.backward_dtype == \"bf16\":\n         backward_dtype = FPQuantDtype.BF16\n+    elif config.backward_dtype == \"mxfp8\":\n+        backward_dtype = FPQuantDtype.MXFP8\n+    elif config.backward_dtype == \"mxfp4\":\n+        backward_dtype = FPQuantDtype.MXFP4\n     else:\n         raise ValueError(f\"Unsupported backward dtype: {config.backward_dtype}\")\n "
      },
      {
        "filename": "src/transformers/utils/import_utils.py",
        "status": "modified",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "patch": "@@ -971,13 +971,13 @@ def is_quark_available() -> bool:\n @lru_cache\n def is_fp_quant_available():\n     is_available, fp_quant_version = _is_package_available(\"fp_quant\", return_version=True)\n-    return is_available and version.parse(fp_quant_version) >= version.parse(\"0.2.0\")\n+    return is_available and version.parse(fp_quant_version) >= version.parse(\"0.3.2\")\n \n \n @lru_cache\n def is_qutlass_available():\n     is_available, qutlass_version = _is_package_available(\"qutlass\", return_version=True)\n-    return is_available and version.parse(qutlass_version) >= version.parse(\"0.1.0\")\n+    return is_available and version.parse(qutlass_version) >= version.parse(\"0.2.0\")\n \n \n @lru_cache"
      },
      {
        "filename": "src/transformers/utils/quantization_config.py",
        "status": "modified",
        "additions": 6,
        "deletions": 2,
        "changes": 8,
        "patch": "@@ -1601,8 +1601,12 @@ def post_init(self):\n         else:\n             raise ValueError(\"Only 'mxfp4' and 'nvfp4' are supported for forward_dtype for now.\")\n \n-        if self.backward_dtype != \"bf16\":\n-            raise ValueError(\"Only 'bf16' is supported for backward_dtype for now.\")\n+        if self.backward_dtype not in [\"bf16\", \"mxfp8\", \"mxfp4\"]:\n+            raise ValueError(\"Only 'bf16', 'mxfp8' and 'mxfp4' are supported for backward_dtype for now.\")\n+\n+        if self.backward_dtype != \"bf16\" and self.forward_dtype != \"mxfp4\":\n+            raise ValueError(\"Only 'mxfp4' forward is compatible with non-bf16 backwards for now.\")\n+\n         if self.transform_init not in [\"hadamard\", \"identity\", \"gsr\"]:\n             raise ValueError(\"Only 'hadamard', 'identity' and 'gsr' are supported for transform_init.\")\n "
      },
      {
        "filename": "tests/quantization/fp_quant_integration/test_fp_quant.py",
        "status": "modified",
        "additions": 7,
        "deletions": 0,
        "changes": 7,
        "patch": "@@ -163,6 +163,13 @@ def getQuantizationConfig(cls):\n         return FPQuantConfig(forward_dtype=\"mxfp4\", pseudoquantization=False)\n \n \n+@require_qutlass\n+class FPQuantNVFP4Test(FPQuantBaseTest):\n+    @classmethod\n+    def getQuantizationConfig(cls):\n+        return FPQuantConfig(forward_dtype=\"nvfp4\", pseudoquantization=False)\n+\n+\n @require_qutlass\n class FPQuantMXFP4GS128Test(FPQuantBaseTest):\n     @classmethod"
      }
    ],
    "num_files": 5,
    "scraped_at": "2025-11-16T21:16:35.971124",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR adds meaningful feature support for MXFP4 and MXFP8 backwards compatibility in quantization, involving logic changes across multiple files (dtype validation, config adaptation, version requirements, and tests). The changes require understanding of quantization frameworks, dtype handling, and architectural constraints (e.g., the requirement that mxfp4 forward must be used with non-bf16 backwards).",
      "substance_level": "medium"
    }
  },
  {
    "pr_number": 41818,
    "title": ":rotating_light: Fix gradient checkpointing for several models and improve test robustness  ",
    "body": "Support for gradient checkpointing was lost in the major refactoring in PR #38635 and this is the attempt to re-add it.\r\n\r\nI extended the tests to\r\n- test `use_reentrant=True` and `False`\r\n- make sure `model.train` is called so that gradient checkpointing works; this is a limiation of the tests currently used by GPTBigCode\r\n- make sure that one (the first) gradient checkpointing layer is called\r\n- make sure that the same non-zero grads are there for normal and checkpointing runs - this is something we tripped over before in PEFT due to the possibly incompletely stored runtime environment in the checkpointed forward step, see also peft#2826\r\n\r\nNote that the invocation of `GPTBigCodeBlock.forward` has changed:\r\n\r\n- `layer_past` is now passed as a keyword argument so that `GradientCheckpointingLayer.__call__` can see and filter this parameter (`use_reentrant=False` fails otherwise)\r\n- `{encoder_}hidden_states` are still passed as positional arguments so that `torch.utils.checkpoint.checkpoint` receives them as pos. args and computes gradients for these (kwargs would be filtered by `GradientCheckpointingLayer`).\r\n\r\n:rotating_light: Note that this is breaking compatibility by changing the forward signature in `GPTBigCodeBlock.forward`!",
    "html_url": "https://github.com/huggingface/transformers/pull/41818",
    "created_at": "2025-10-23T15:17:23Z",
    "merged_at": "2025-11-11T17:13:38Z",
    "merge_commit_sha": "fa22b569038540d31eacbf5d333a1e9aa0787131",
    "base_ref": "main",
    "head_sha": "195fbc846d9e8047ae0712bfc6517c8f5577118d",
    "user": "githubnemo",
    "files": [
      {
        "filename": "src/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py",
        "status": "modified",
        "additions": 6,
        "deletions": 5,
        "changes": 11,
        "patch": "@@ -26,6 +26,7 @@\n from ...generation import GenerationMixin\n from ...masking_utils import create_causal_mask\n from ...modeling_flash_attention_utils import is_flash_attn_available\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n     CausalLMOutputWithCrossAttentions,\n@@ -266,7 +267,7 @@ def forward(self, hidden_states: Optional[tuple[torch.FloatTensor]]) -> torch.Fl\n         return hidden_states\n \n \n-class GPTBigCodeBlock(nn.Module):\n+class GPTBigCodeBlock(GradientCheckpointingLayer):\n     def __init__(self, config, layer_idx=None):\n         super().__init__()\n         hidden_size = config.hidden_size\n@@ -291,9 +292,9 @@ def __init__(self, config, layer_idx=None):\n     def forward(\n         self,\n         hidden_states: Optional[tuple[torch.Tensor]],\n+        encoder_hidden_states: Optional[torch.Tensor] = None,\n         layer_past: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n-        encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         use_cache: Optional[bool] = False,\n         output_attentions: Optional[bool] = False,\n@@ -536,10 +537,10 @@ def forward(\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n             outputs = block(\n-                hidden_states,\n-                past_key_values,\n-                causal_mask,\n+                hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n+                layer_past=past_key_values,  # as keyword argument so it can be removed by GradientCheckpointingLayer\n+                attention_mask=causal_mask,\n                 encoder_attention_mask=encoder_attention_mask,\n                 use_cache=use_cache,\n                 output_attentions=output_attentions,"
      },
      {
        "filename": "src/transformers/models/swiftformer/modeling_swiftformer.py",
        "status": "modified",
        "additions": 2,
        "deletions": 1,
        "changes": 3,
        "patch": "@@ -21,6 +21,7 @@\n from torch import nn\n \n from ...activations import ACT2CLS\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithNoAttention, ImageClassifierOutputWithNoAttention\n from ...modeling_utils import PreTrainedModel\n from ...utils import auto_docstring, logging\n@@ -295,7 +296,7 @@ def forward(self, x):\n         return x\n \n \n-class SwiftFormerStage(nn.Module):\n+class SwiftFormerStage(GradientCheckpointingLayer):\n     \"\"\"\n     A Swiftformer stage consisting of a series of `SwiftFormerConvEncoder` blocks and a final\n     `SwiftFormerEncoderBlock`."
      },
      {
        "filename": "src/transformers/models/xlstm/modeling_xlstm.py",
        "status": "modified",
        "additions": 12,
        "deletions": 14,
        "changes": 26,
        "patch": "@@ -22,17 +22,21 @@\n from torch.nn import CrossEntropyLoss\n \n from ...generation import GenerationMixin\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_utils import PreTrainedModel\n from ...utils import ModelOutput, auto_docstring, can_return_tuple, is_xlstm_available\n from .configuration_xlstm import xLSTMConfig\n \n \n if is_xlstm_available():\n     from xlstm.xlstm_large.model import RMSNorm as xLSTMRMSNorm\n-    from xlstm.xlstm_large.model import mLSTMBlock as xLSTMBlock\n-    from xlstm.xlstm_large.model import mLSTMStateType, soft_cap\n+    from xlstm.xlstm_large.model import mLSTMBlock, mLSTMStateType, soft_cap\n \n     external_xlstm = True\n+\n+    class xLSTMBlock(GradientCheckpointingLayer, mLSTMBlock):\n+        pass\n+\n else:\n     from collections.abc import Callable\n     from functools import partial\n@@ -1164,7 +1168,7 @@ def forward(\n             y = self.out_proj(h_out)\n             return y, state\n \n-    class xLSTMBlock(nn.Module):\n+    class xLSTMBlock(GradientCheckpointingLayer):\n         def __init__(self, config: xLSTMConfig):\n             super().__init__()\n             self.config = config\n@@ -1457,17 +1461,11 @@ def forward(\n         else:\n             all_hidden_states = () if output_hidden_states else None\n             for layer_idx, xlstm_block in enumerate(self.blocks):\n-                if self.gradient_checkpointing and self.training:\n-                    hidden_states, rnn_state = self._gradient_checkpointing_func(\n-                        xlstm_block.__call__,\n-                        hidden_states,\n-                        cache_params.rnn_state[layer_idx] if cache_params is not None else None,\n-                    )\n-                else:\n-                    hidden_states, rnn_state = xlstm_block(\n-                        hidden_states,\n-                        state=cache_params.rnn_state[layer_idx] if cache_params is not None else None,\n-                    )\n+                hidden_states, rnn_state = xlstm_block(\n+                    hidden_states,\n+                    cache_params.rnn_state[layer_idx] if cache_params is not None else None,\n+                )\n+\n                 if cache_params:\n                     for state_idx in range(len(cache_params.rnn_state[layer_idx])):\n                         local_rnn_state = rnn_state[state_idx]"
      },
      {
        "filename": "src/transformers/models/zamba/modeling_zamba.py",
        "status": "modified",
        "additions": 15,
        "deletions": 27,
        "changes": 42,
        "patch": "@@ -32,6 +32,7 @@\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast, SequenceClassifierOutputWithPast\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n@@ -639,7 +640,7 @@ def forward(\n         return outputs\n \n \n-class ZambaMambaDecoderLayer(nn.Module):\n+class ZambaMambaDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: ZambaConfig, layer_idx: int):\n         super().__init__()\n         self.mamba = ZambaMambaMixer(config=config, layer_idx=layer_idx)\n@@ -708,7 +709,7 @@ def forward(\n         return outputs\n \n \n-class ZambaHybridLayer(nn.Module):\n+class ZambaHybridLayer(GradientCheckpointingLayer):\n     def __init__(self, shared_transf: ZambaAttentionDecoderLayer, linear: nn.Linear, mamba: ZambaMambaDecoderLayer):\n         super().__init__()\n         self.shared_transf = shared_transf\n@@ -942,31 +943,18 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer.__call__,\n-                    hidden_states,\n-                    original_hidden_states,\n-                    layer_idx,\n-                    attention_mask,\n-                    causal_mask,\n-                    past_key_values,\n-                    output_attentions,\n-                    use_cache,\n-                    cache_position,\n-                )\n-            else:\n-                layer_outputs = layer(\n-                    hidden_states,\n-                    original_hidden_states=original_hidden_states,\n-                    layer_idx=layer_idx,\n-                    attention_mask=attention_mask,\n-                    causal_mask=causal_mask,\n-                    past_key_values=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                )\n+            layer_outputs = layer(\n+                hidden_states,\n+                original_hidden_states,\n+                layer_idx,\n+                attention_mask,\n+                causal_mask,\n+                past_key_values=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+            )\n+\n             hidden_states = layer_outputs[0]\n \n             if output_attentions:"
      },
      {
        "filename": "src/transformers/models/zamba2/modeling_zamba2.py",
        "status": "modified",
        "additions": 16,
        "deletions": 29,
        "changes": 45,
        "patch": "@@ -34,6 +34,7 @@\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast, SequenceClassifierOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n@@ -1058,7 +1059,7 @@ def forward(\n         return outputs\n \n \n-class Zamba2MambaDecoderLayer(nn.Module):\n+class Zamba2MambaDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: Zamba2Config, layer_idx: int):\n         super().__init__()\n         self.mamba = Zamba2MambaMixer(config=config, layer_idx=layer_idx)\n@@ -1127,7 +1128,7 @@ def forward(\n         return outputs\n \n \n-class Zamba2HybridLayer(nn.Module):\n+class Zamba2HybridLayer(GradientCheckpointingLayer):\n     def __init__(\n         self, shared_transformer: Zamba2AttentionDecoderLayer, linear: nn.Linear, mamba: Zamba2MambaDecoderLayer\n     ):\n@@ -1344,33 +1345,19 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer.__call__,\n-                    hidden_states,\n-                    original_hidden_states,\n-                    layer_idx,\n-                    attention_mask,\n-                    causal_mask,\n-                    past_key_values,\n-                    output_attentions,\n-                    use_cache,\n-                    position_embeddings,\n-                    position_ids,\n-                )\n-            else:\n-                layer_outputs = layer(\n-                    hidden_states,\n-                    original_hidden_states=original_hidden_states,\n-                    layer_idx=layer_idx,\n-                    attention_mask=attention_mask,\n-                    causal_mask=causal_mask,\n-                    past_key_values=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    position_embeddings=position_embeddings,\n-                    position_ids=position_ids,\n-                )\n+            layer_outputs = layer(\n+                hidden_states,\n+                original_hidden_states,\n+                layer_idx,\n+                attention_mask,\n+                causal_mask,\n+                past_key_values=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                position_embeddings=position_embeddings,\n+                position_ids=position_ids,\n+            )\n+\n             hidden_states = layer_outputs[0]\n \n             if output_attentions:"
      },
      {
        "filename": "src/transformers/models/zamba2/modular_zamba2.py",
        "status": "modified",
        "additions": 13,
        "deletions": 27,
        "changes": 40,
        "patch": "@@ -1079,33 +1079,19 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer.__call__,\n-                    hidden_states,\n-                    original_hidden_states,\n-                    layer_idx,\n-                    attention_mask,\n-                    causal_mask,\n-                    past_key_values,\n-                    output_attentions,\n-                    use_cache,\n-                    position_embeddings,\n-                    position_ids,\n-                )\n-            else:\n-                layer_outputs = layer(\n-                    hidden_states,\n-                    original_hidden_states=original_hidden_states,\n-                    layer_idx=layer_idx,\n-                    attention_mask=attention_mask,\n-                    causal_mask=causal_mask,\n-                    past_key_values=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    position_embeddings=position_embeddings,\n-                    position_ids=position_ids,\n-                )\n+            layer_outputs = layer(\n+                hidden_states,\n+                original_hidden_states,\n+                layer_idx,\n+                attention_mask,\n+                causal_mask,\n+                past_key_values=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                position_embeddings=position_embeddings,\n+                position_ids=position_ids,\n+            )\n+\n             hidden_states = layer_outputs[0]\n \n             if output_attentions:"
      },
      {
        "filename": "tests/models/clvp/test_modeling_clvp.py",
        "status": "modified",
        "additions": 4,
        "deletions": 15,
        "changes": 19,
        "patch": "@@ -186,6 +186,10 @@ def test_training(self):\n     def test_training_gradient_checkpointing(self):\n         pass\n \n+    @unittest.skip(reason=\"ClvpEncoder does not output loss\")\n+    def test_gradient_checkpointing_enable_disable(self):\n+        pass\n+\n \n class ClvpDecoderTester:\n     def __init__(\n@@ -311,21 +315,6 @@ def test_training(self):\n         loss = model(**inputs).loss\n         loss.backward()\n \n-    def test_training_gradient_checkpointing(self):\n-        # we will only test the ClvpForCausalLM since it outputs loss\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.use_cache = False\n-        config.return_dict = True\n-\n-        model = ClvpForCausalLM(config)\n-        model.to(torch_device)\n-        model.gradient_checkpointing_enable()\n-        model.train()\n-        inputs = self._prepare_for_class(inputs_dict, ClvpForCausalLM, return_labels=True)\n-\n-        loss = model(**inputs).loss\n-        loss.backward()\n-\n     @unittest.skip(reason=\"Clvp `prepare_inputs_for_generation` function doesn't have cache position.\")\n     def test_generate_continue_from_inputs_embeds(self):\n         pass"
      },
      {
        "filename": "tests/models/gpt_bigcode/test_modeling_gpt_bigcode.py",
        "status": "modified",
        "additions": 7,
        "deletions": 7,
        "changes": 14,
        "patch": "@@ -307,12 +307,16 @@ def create_and_check_lm_head_model(self, config, input_ids, input_mask, token_ty\n         self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n \n     def create_and_check_forward_and_backwards(\n-        self, config, input_ids, input_mask, token_type_ids, *args, gradient_checkpointing=False\n+        self,\n+        config,\n+        input_ids,\n+        input_mask,\n+        token_type_ids,\n+        *args,\n     ):\n         model = GPTBigCodeForCausalLM(config)\n+        model.train()\n         model.to(torch_device)\n-        if gradient_checkpointing:\n-            model.gradient_checkpointing_enable()\n \n         result = model(input_ids, token_type_ids=token_type_ids, labels=input_ids)\n         self.parent.assertEqual(result.loss.shape, ())\n@@ -463,10 +467,6 @@ def test_gpt_bigcode_token_classification_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_gpt_bigcode_for_token_classification(*config_and_inputs)\n \n-    def test_gpt_bigcode_gradient_checkpointing(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_forward_and_backwards(*config_and_inputs, gradient_checkpointing=True)\n-\n     def test_gpt_bigcode_scale_attn_by_inverse_layer_idx(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs(scale_attn_by_inverse_layer_idx=True)\n         self.model_tester.create_and_check_forward_and_backwards(*config_and_inputs)"
      },
      {
        "filename": "tests/models/janus/test_modeling_janus.py",
        "status": "modified",
        "additions": 4,
        "deletions": 0,
        "changes": 4,
        "patch": "@@ -396,6 +396,10 @@ def test_model_get_set_embeddings(self):\n     def test_retain_grad_hidden_states_attentions(self):\n         pass\n \n+    @unittest.skip(\"Janus VQ module has no gradient checkpointing layers\")\n+    def test_gradient_checkpointing_enable_disable(self):\n+        pass\n+\n \n class JanusIntegrationTest(unittest.TestCase):\n     def setUp(self):"
      },
      {
        "filename": "tests/test_modeling_common.py",
        "status": "modified",
        "additions": 54,
        "deletions": 5,
        "changes": 59,
        "patch": "@@ -20,6 +20,7 @@\n import random\n import re\n import tempfile\n+import unittest.mock\n import warnings\n from collections import defaultdict\n from contextlib import contextmanager\n@@ -46,6 +47,7 @@\n     is_deepspeed_zero3_enabled,\n     unset_hf_deepspeed_config,\n )\n+from transformers.modeling_layers import GradientCheckpointingLayer\n from transformers.modeling_utils import _get_tied_weight_keys\n from transformers.models.auto import get_values\n from transformers.models.auto.modeling_auto import (\n@@ -827,6 +829,12 @@ def test_gradient_checkpointing_enable_disable(self):\n             model = model_class(copy.deepcopy(config))\n             self.assertFalse(model.is_gradient_checkpointing)\n \n+            # Gradient checkpointing is implemented via GradientCheckpointingLayer, if none is present this is likely\n+            # an implementation issue. Note we exclude clvp for now since they are still not using\n+            # GradientCheckpointingLayer.\n+            if config.model_type not in [\"clvp\", \"clvp_decoder\"]:\n+                self.assertTrue([m for m in model.modules() if isinstance(m, GradientCheckpointingLayer)])\n+\n             # check enable works\n             model.gradient_checkpointing_enable()\n             self.assertTrue(model.is_gradient_checkpointing)\n@@ -1151,22 +1159,63 @@ def check_training_gradient_checkpointing(self, gradient_checkpointing_kwargs=No\n                 config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n                 config.use_cache = False\n                 config.return_dict = True\n-                model = model_class(config)\n \n+                # make sure that test runs are consistent by disabling dropout\n+                #\n+                # Note: attention_probs_dropout_prob seem to influence classifier.bias in BertForMultipleChoice\n+                # (and other Bert derived models). Sometimes classifier.bias is None when\n+                # attention_probs_dropout_prob > 0. This might indicate a bug somewhere.\n+                if hasattr(config, \"hidden_dropout_prob\"):\n+                    config.hidden_dropout_prob = 0.0\n+                if hasattr(config, \"attention_probs_dropout_prob\"):\n+                    config.attention_probs_dropout_prob = 0.0\n+\n+                inputs = self._prepare_for_class(inputs_dict, model_class, return_labels=True)\n+\n+                torch.manual_seed(0)\n+                model = model_class(config)\n                 model.to(torch_device)\n-                model.gradient_checkpointing_enable(gradient_checkpointing_kwargs=gradient_checkpointing_kwargs)\n                 model.train()\n \n                 # unfreeze additional layers\n                 for p in model.parameters():\n                     p.requires_grad_(True)\n \n+                # do a non-checkpointing run, so we can compare the set of non-zero gradients later. we skip None\n+                # grads here to collect a reference set of modules that have non-zero gradients (to filter layers like\n+                # MoE that drop out parts of the model).\n                 optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n-\n-                inputs = self._prepare_for_class(inputs_dict, model_class, return_labels=True)\n+                torch.manual_seed(0)\n                 loss = model(**inputs).loss\n                 loss.backward()\n-                optimizer.step()\n+                grad_expected_params = [(n, p) for n, p in model.named_parameters() if p.grad is not None]\n+                non_zero_grads_normal = {n for n, p in grad_expected_params if p.grad.abs().sum() > 0}\n+\n+                # reset all gradients to zero for the comparison with the gradient checkpointing run\n+                optimizer.zero_grad()\n+\n+                # now enable gradient checkpointing and compare the gradients\n+                model.gradient_checkpointing_enable(gradient_checkpointing_kwargs=gradient_checkpointing_kwargs)\n+\n+                checkpointing_layer = next(m for m in model.modules() if isinstance(m, GradientCheckpointingLayer))\n+\n+                optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n+                with unittest.mock.patch.object(\n+                    checkpointing_layer, \"forward\", wraps=checkpointing_layer.forward\n+                ) as forward_mock:\n+                    torch.manual_seed(0)\n+                    loss = model(**inputs).loss\n+                    loss.backward()\n+                    optimizer.step()\n+\n+                    # test that gradient checkpointing is active as it would call the gradient checkpointing layer's\n+                    # forward more than once.\n+                    self.assertGreater(forward_mock.call_count, 1)\n+\n+                # check that all the parameters that had non-zero gradients before, have non-zero grads with gradient\n+                # checkpointing. divergence indicates a different forward-pass environment that needs special handling.\n+                non_zero_grads_gradcp = {n for n, p in grad_expected_params if p.grad.abs().sum() > 0}\n+                self.assertEqual(non_zero_grads_gradcp, non_zero_grads_normal)\n \n                 if self.test_all_params_have_gradient:\n                     for k, v in model.named_parameters():"
      }
    ],
    "num_files": 10,
    "scraped_at": "2025-11-16T21:16:50.040590",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR contains substantial non-trivial changes that restore and improve gradient checkpointing functionality across multiple models by leveraging a new GradientCheckpointingLayer abstraction. The changes involve architectural decisions (inheritance patterns, parameter ordering), logic modifications (conditional removal of gradient checkpointing boilerplate), and comprehensive test improvements that would help developers understand how gradient checkpointing integrates with the codebase.",
      "substance_level": "high"
    }
  },
  {
    "pr_number": 41817,
    "title": "add fuyu fast image processors",
    "body": "# What does this PR do?\r\n\r\nThis PR introduces FuyuImageProcessorFast, providing a faster alternative to the original FuyuImageProcessor by leveraging torchvision for image transformations.\r\n\r\nKey changes include:\r\n\r\n* Implementation of FuyuImageProcessorFast inheriting from BaseImageProcessorFast.\r\n* Updates to tests/models/fuyu/test_image_processing_fuyu.py to include the fast processor, override save/load tests and fixed the image height and width in test_preprocess_with_tokenizer_info have been updated to values divisible by 30 (180x300), ensuring compatibility with FuyuImageProcessorFast and avoiding ValueError: image_height must be divisible by 30. All Fuyu image processing tests now pass.\r\n* Addition of documentation for FuyuImageProcessorFast \r\n\r\nFixes #36978\r\n\r\n\r\n## Before submitting\r\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),\r\n      Pull Request section?\r\n- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case. Was this discussed/approved via a Github issue or the forum? [Contributions Welcome] Add Fast Image Processors #36978](https://github.com/huggingface/transformers/issues/36978)\r\n\r\n- [x] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [x] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n@yonigozlan \r\n\r\n",
    "html_url": "https://github.com/huggingface/transformers/pull/41817",
    "created_at": "2025-10-23T14:43:41Z",
    "merged_at": "2025-11-04T15:45:03Z",
    "merge_commit_sha": "325810e7fccf8273599c58a525ae0011ea8ba3e6",
    "base_ref": "main",
    "head_sha": "de29121c99e94337dc1e1392352e2304aa0c6e35",
    "user": "DeXtAr47-oss",
    "files": [
      {
        "filename": "docs/source/en/model_doc/fuyu.md",
        "status": "modified",
        "additions": 7,
        "deletions": 2,
        "changes": 9,
        "patch": "@@ -75,11 +75,11 @@ A processor requires an image_processor and a tokenizer. Hence, inputs can be lo\n from PIL import Image\n from transformers import AutoTokenizer\n from transformers.models.fuyu.processing_fuyu import FuyuProcessor\n-from transformers.models.fuyu.image_processing_fuyu import FuyuImageProcessor\n+from transformers.models.fuyu.image_processing_fuyu_fast import FuyuImageProcessorFast\n \n \n tokenizer = AutoTokenizer.from_pretrained('adept-hf-collab/fuyu-8b')\n-image_processor = FuyuImageProcessor()\n+image_processor = FuyuImageProcessorFast()\n \n \n processor = FuyuProcessor(image_processor=image_processor, tokenizer=tokenizer)\n@@ -118,6 +118,11 @@ The `LlamaTokenizer` is used as it is a standard wrapper around sentencepiece.\n [[autodoc]] FuyuImageProcessor\n     - __call__\n \n+## FuyuImageProcessor\n+\n+[[autodoc]] FuyuImageProcessorFast\n+    - __call__\n+\n ## FuyuProcessor\n \n [[autodoc]] FuyuProcessor"
      },
      {
        "filename": "src/transformers/image_processing_utils_fast.py",
        "status": "modified",
        "additions": 6,
        "deletions": 3,
        "changes": 9,
        "patch": "@@ -227,6 +227,7 @@ def pad(\n         padding_mode: Optional[str] = \"constant\",\n         return_mask: bool = False,\n         disable_grouping: Optional[bool] = False,\n+        is_nested: Optional[bool] = False,\n         **kwargs,\n     ) -> Union[tuple[\"torch.Tensor\", \"torch.Tensor\"], \"torch.Tensor\"]:\n         \"\"\"\n@@ -257,7 +258,9 @@ def pad(\n         else:\n             pad_size = get_max_height_width(images)\n \n-        grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n+        grouped_images, grouped_images_index = group_images_by_shape(\n+            images, disable_grouping=disable_grouping, is_nested=is_nested\n+        )\n         processed_images_grouped = {}\n         processed_masks_grouped = {}\n         for shape, stacked_images in grouped_images.items():\n@@ -280,9 +283,9 @@ def pad(\n                 stacked_masks[..., : image_size[0], : image_size[1]] = 1\n                 processed_masks_grouped[shape] = stacked_masks\n \n-        processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n+        processed_images = reorder_images(processed_images_grouped, grouped_images_index, is_nested=is_nested)\n         if return_mask:\n-            processed_masks = reorder_images(processed_masks_grouped, grouped_images_index)\n+            processed_masks = reorder_images(processed_masks_grouped, grouped_images_index, is_nested=is_nested)\n             return processed_images, processed_masks\n \n         return processed_images"
      },
      {
        "filename": "src/transformers/models/auto/image_processing_auto.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -98,7 +98,7 @@\n             (\"eomt\", (\"EomtImageProcessor\", \"EomtImageProcessorFast\")),\n             (\"flava\", (\"FlavaImageProcessor\", \"FlavaImageProcessorFast\")),\n             (\"focalnet\", (\"BitImageProcessor\", \"BitImageProcessorFast\")),\n-            (\"fuyu\", (\"FuyuImageProcessor\", None)),\n+            (\"fuyu\", (\"FuyuImageProcessor\", \"FuyuImageProcessorFast\")),\n             (\"gemma3\", (\"Gemma3ImageProcessor\", \"Gemma3ImageProcessorFast\")),\n             (\"gemma3n\", (\"SiglipImageProcessor\", \"SiglipImageProcessorFast\")),\n             (\"git\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),"
      },
      {
        "filename": "src/transformers/models/fuyu/__init__.py",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -20,6 +20,7 @@\n if TYPE_CHECKING:\n     from .configuration_fuyu import *\n     from .image_processing_fuyu import *\n+    from .image_processing_fuyu_fast import *\n     from .modeling_fuyu import *\n     from .processing_fuyu import *\n else:"
      },
      {
        "filename": "src/transformers/models/fuyu/image_processing_fuyu.py",
        "status": "modified",
        "additions": 18,
        "deletions": 0,
        "changes": 18,
        "patch": "@@ -29,6 +29,7 @@\n     ChannelDimension,\n     ImageInput,\n     PILImageResampling,\n+    SizeDict,\n     get_image_size,\n     infer_channel_dimension_format,\n     is_scaled_image,\n@@ -37,6 +38,7 @@\n     to_numpy_array,\n     validate_preprocess_arguments,\n )\n+from ...processing_utils import ImagesKwargs\n from ...utils import (\n     TensorType,\n     filter_out_non_signature_kwargs,\n@@ -70,6 +72,21 @@ def make_list_of_list_of_images(\n     raise ValueError(\"images must be a list of list of images or a list of images or an image.\")\n \n \n+class FuyuImagesKwargs(ImagesKwargs, total=False):\n+    r\"\"\"\n+    patch_size (`dict[str, int]`, *optional*, defaults to `{\"height\": 30, \"width\": 30}`):\n+        Dictionary in the format `{\"height\": int, \"width\": int}` specifying the size of the patches.\n+    padding_value (`float`, *optional*, defaults to 1.0):\n+        The value to pad the image with.\n+    padding_mode (`str`, *optional*, defaults to \"constant\"):\n+        The padding mode to use when padding the image.\n+    \"\"\"\n+\n+    patch_size: Optional[SizeDict]\n+    padding_value: float\n+    padding_mode: str\n+\n+\n class FuyuBatchFeature(BatchFeature):\n     \"\"\"\n     BatchFeature class for Fuyu image processor and processor.\n@@ -232,6 +249,7 @@ class FuyuImageProcessor(BaseImageProcessor):\n         \"image_patch_indices_per_batch\",\n         \"image_patch_indices_per_subsequence\",\n     ]\n+    valid_kwargs = FuyuImagesKwargs\n \n     def __init__(\n         self,"
      },
      {
        "filename": "src/transformers/models/fuyu/image_processing_fuyu_fast.py",
        "status": "added",
        "additions": 382,
        "deletions": 0,
        "changes": 382,
        "patch": "@@ -0,0 +1,382 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Fast Image processor class for Fuyu.\"\"\"\n+\n+import math\n+from typing import Optional, Union\n+\n+import torch\n+\n+from ...image_processing_utils import get_size_dict\n+from ...image_processing_utils_fast import (\n+    BaseImageProcessorFast,\n+    group_images_by_shape,\n+    reorder_images,\n+)\n+from ...image_utils import (\n+    ImageInput,\n+    PILImageResampling,\n+    SizeDict,\n+)\n+from ...utils import (\n+    TensorType,\n+    auto_docstring,\n+    is_torchvision_available,\n+    logging,\n+    requires_backends,\n+)\n+from .image_processing_fuyu import FuyuBatchFeature, FuyuImagesKwargs, make_list_of_list_of_images\n+\n+\n+if is_torchvision_available():\n+    from torchvision.transforms.v2 import functional as F\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+@auto_docstring\n+class FuyuImageProcessorFast(BaseImageProcessorFast):\n+    do_resize = True\n+    size = {\"height\": 1080, \"width\": 1920}\n+    resample = PILImageResampling.BILINEAR\n+    do_pad = True\n+    padding_value = 1.0\n+    padding_mode = \"constant\"\n+    do_normalize = True\n+    image_mean = 0.5\n+    image_std = 0.5\n+    do_rescale = True\n+    rescale_factor = 1 / 255\n+    model_input_names = [\n+        \"images\",\n+        \"image_input_ids\",\n+        \"image_patches\",\n+        \"image_patch_indices_per_batch\",\n+        \"image_patch_indices_per_subsequence\",\n+    ]\n+    valid_kwargs = FuyuImagesKwargs\n+\n+    def _prepare_images_structure(\n+        self,\n+        images: ImageInput,\n+        expected_ndims: int = 3,\n+    ) -> ImageInput:\n+        images = self.fetch_images(images)\n+        return make_list_of_list_of_images(images)\n+\n+    def resize(\n+        self,\n+        image: torch.Tensor,\n+        size: SizeDict,\n+        interpolation: Optional[\"F.InterpolationMode\"] = None,\n+        antialias: bool = True,\n+        **kwargs,\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        Resize an image to fit within `(size[\"height\"], size[\"width\"])` while maintaining aspect ratio.\n+        Only resizes if the image is larger than the target size.\n+        Args:\n+            image (`torch.Tensor`):\n+                Image to resize.\n+            size (`SizeDict`):\n+                Dictionary in the format `{\"height\": int, \"width\": int}` specifying the max size of the output image.\n+            interpolation (`InterpolationMode`, *optional*, defaults to `InterpolationMode.BILINEAR`):\n+                `InterpolationMode` filter to use when resizing the image e.g. `InterpolationMode.BILINEAR`.\n+            antialias (`bool`, *optional*, defaults to `True`):\n+                Whether to apply antialiasing when resizing.\n+        \"\"\"\n+        interpolation = interpolation if interpolation is not None else F.InterpolationMode.BILINEAR\n+        image_height, image_width = image.shape[-2:]\n+        target_height, target_width = size.height, size.width\n+        # Only resize if image is larger than target\n+        if image_width <= target_width and image_height <= target_height:\n+            return image\n+        # Calculate optimal scale factor to fit within target size\n+        height_scale_factor = target_height / image_height\n+        width_scale_factor = target_width / image_width\n+        optimal_scale_factor = min(height_scale_factor, width_scale_factor)\n+\n+        new_height = int(image_height * optimal_scale_factor)\n+        new_width = int(image_width * optimal_scale_factor)\n+\n+        return super().resize(\n+            image, SizeDict(height=new_height, width=new_width), interpolation=interpolation, antialias=antialias\n+        )\n+\n+    def _preprocess(\n+        self,\n+        images: list[\"torch.Tensor\"],\n+        do_resize: bool,\n+        size: SizeDict,\n+        interpolation: Optional[\"F.InterpolationMode\"],\n+        do_rescale: bool,\n+        rescale_factor: float,\n+        do_normalize: bool,\n+        image_mean: Optional[Union[float, list[float]]],\n+        image_std: Optional[Union[float, list[float]]],\n+        do_pad: Optional[bool],\n+        padding_value: Optional[float],\n+        padding_mode: Optional[str],\n+        disable_grouping: Optional[bool],\n+        return_tensors: Optional[Union[str, TensorType]],\n+        **kwargs,\n+    ) -> FuyuBatchFeature:\n+        # Group images by size for batched resizing\n+        original_image_sizes = [batch_image[0].shape[-2:] for batch_image in images if batch_image]\n+        grouped_images, grouped_images_index = group_images_by_shape(\n+            images, disable_grouping=disable_grouping, is_nested=True\n+        )\n+        resized_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            if do_resize:\n+                stacked_images = self.resize(image=stacked_images, size=size, interpolation=interpolation)\n+            resized_images_grouped[shape] = stacked_images\n+        resized_images = reorder_images(resized_images_grouped, grouped_images_index, is_nested=True)\n+\n+        image_sizes = [batch_image[0].shape[-2:] for batch_image in resized_images if batch_image]\n+        image_unpadded_heights = [[image_size[0]] for image_size in image_sizes]\n+        image_unpadded_widths = [[image_size[1]] for image_size in image_sizes]\n+        image_scale_factors = [\n+            [resized_size[0] / original_size[0]]\n+            for original_size, resized_size in zip(original_image_sizes, image_sizes)\n+        ]\n+        if do_pad:\n+            resized_images = self.pad(\n+                resized_images,\n+                pad_size=size,\n+                fill_value=padding_value,\n+                padding_mode=padding_mode,\n+                disable_grouping=disable_grouping,\n+                is_nested=True,\n+            )\n+        # Group images by size for further processing\n+        # Needed in case do_resize is False, or resize returns images with different sizes\n+        grouped_images, grouped_images_index = group_images_by_shape(\n+            resized_images, disable_grouping=disable_grouping, is_nested=True\n+        )\n+        processed_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            # Fused rescale and normalize\n+            stacked_images = self.rescale_and_normalize(\n+                stacked_images, do_rescale, rescale_factor, do_normalize, image_mean, image_std\n+            )\n+            processed_images_grouped[shape] = stacked_images\n+        processed_images = reorder_images(processed_images_grouped, grouped_images_index, is_nested=True)\n+\n+        return FuyuBatchFeature(\n+            data={\n+                \"images\": processed_images,\n+                \"image_unpadded_heights\": image_unpadded_heights,\n+                \"image_unpadded_widths\": image_unpadded_widths,\n+                \"image_scale_factors\": image_scale_factors,\n+            },\n+            tensor_type=return_tensors,\n+        )\n+\n+    def get_num_patches(self, image_height: int, image_width: int, patch_size: Optional[SizeDict] = None) -> int:\n+        \"\"\"\n+        Calculate number of patches required to encode an image.\n+        Args:\n+            image_height (`int`):\n+                Height of the image.\n+            image_width (`int`):\n+                Width of the image.\n+            patch_size (`SizeDict`, *optional*):\n+                Dictionary in the format `{\"height\": int, \"width\": int}` specifying the size of the patches.\n+        \"\"\"\n+        if patch_size is None:\n+            patch_size = SizeDict(**self.patch_size)\n+        patch_height, patch_width = patch_size.height, patch_size.width\n+        if image_height % patch_height != 0:\n+            raise ValueError(f\"{image_height=} must be divisible by {patch_height}\")\n+        if image_width % patch_width != 0:\n+            raise ValueError(f\"{image_width=} must be divisible by {patch_width}\")\n+        num_patches_per_dim_h = image_height // patch_height\n+        num_patches_per_dim_w = image_width // patch_width\n+        num_patches = num_patches_per_dim_h * num_patches_per_dim_w\n+        return num_patches\n+\n+    def patchify_image(self, image: torch.Tensor, patch_size: Optional[SizeDict] = None) -> torch.Tensor:\n+        \"\"\"\n+        Convert an image into a tensor of patches using PyTorch's unfold operation.\n+        Args:\n+            image (`torch.Tensor`):\n+                Image to convert. Shape: [batch, channels, height, width]\n+            patch_size (`SizeDict`, *optional*):\n+                Dictionary in the format `{\"height\": int, \"width\": int}` specifying the size of the patches.\n+        \"\"\"\n+        requires_backends(self, [\"torch\"])\n+        if patch_size is None:\n+            patch_size = SizeDict(**self.patch_size)\n+        patch_height, patch_width = patch_size.height, patch_size.width\n+        batch_size, channels, _, _ = image.shape\n+        # Use unfold to extract patches\n+        unfolded_along_height = image.unfold(2, patch_height, patch_height)\n+        patches = unfolded_along_height.unfold(3, patch_width, patch_width)\n+        patches = patches.contiguous()\n+        # Reshape to [batch, num_patches, channels * patch_h * patch_w]\n+        patches = patches.view(batch_size, channels, -1, patch_height, patch_width)\n+        patches = patches.permute(0, 2, 3, 4, 1)\n+        patches = patches.reshape(batch_size, -1, channels * patch_height * patch_width)\n+        return patches\n+\n+    def preprocess_with_tokenizer_info(\n+        self,\n+        image_input: torch.Tensor,\n+        image_present: torch.Tensor,\n+        image_unpadded_h: torch.Tensor,\n+        image_unpadded_w: torch.Tensor,\n+        image_placeholder_id: int,\n+        image_newline_id: int,\n+        variable_sized: bool,\n+        patch_size: Optional[dict[str, int]] = None,\n+    ) -> FuyuBatchFeature:\n+        \"\"\"\n+        Process images for model input. In particular, variable-sized images are handled here.\n+\n+        Args:\n+            image_input (`torch.Tensor` of shape [batch_size, subsequence_size, num_channels, height, width]):\n+                Tensor of images padded to model input size.\n+            image_present (`torch.Tensor` of shape [batch_size, subsequence_size, num_images]):\n+                Tensor of 1s and 0s indicating whether an image is present.\n+            image_unpadded_h (`torch.Tensor` of shape [batch_size, subsequence_size]):\n+                Tensor of unpadded image heights.\n+            image_unpadded_w (`torch.Tensor` of shape [batch_size, subsequence_size]):\n+                Tensor of unpadded image widths.\n+            image_placeholder_id (int):\n+                The id of the image placeholder token. Comes from an associated tokenizer.\n+            image_newline_id (int):\n+                The id of the image newline token. Comes from an associated tokenizer.\n+            variable_sized (bool):\n+                Whether to process images as variable-sized.\n+            patch_size (`dict[str, int]`, *optional*):\n+                Size of the patches.\n+        \"\"\"\n+        requires_backends(self, [\"torch\"])\n+\n+        if patch_size is None:\n+            patch_size = SizeDict(**self.patch_size)\n+        else:\n+            patch_size = SizeDict(**patch_size)\n+        patch_height, patch_width = patch_size.height, patch_size.width\n+        # Only images that are present\n+        images: list[list[torch.Tensor]] = []\n+        batch_image_patches: list[list[torch.Tensor]] = []\n+        # Image input ids for every subsequence, including ones with no image present\n+        batch_image_input_ids: list[list[torch.Tensor]] = []\n+        for batch_index in range(image_input.shape[0]):\n+            image_input_ids = []\n+            image_patches = []\n+            for subseq_index in range(image_input.shape[1]):\n+                if image_present[batch_index, subseq_index]:\n+                    image = image_input[batch_index, subseq_index]\n+                    image_height, image_width = image.shape[1], image.shape[2]\n+                    if variable_sized:\n+                        # Calculate new dimensions based on unpadded size\n+                        # The min() is required here due to floating point issues\n+                        new_h = min(\n+                            image_height,\n+                            math.ceil(image_unpadded_h[batch_index, subseq_index] / patch_height) * patch_height,\n+                        )\n+                        new_w = min(\n+                            image_width,\n+                            math.ceil(image_unpadded_w[batch_index, subseq_index] / patch_width) * patch_width,\n+                        )\n+                        image = image[:, :new_h, :new_w]\n+                        image_height, image_width = new_h, new_w\n+                    num_patches = self.get_num_patches(\n+                        image_height=image_height, image_width=image_width, patch_size=patch_size\n+                    )\n+                    # Create tensor of placeholder IDs\n+                    tensor_of_image_ids = torch.full(\n+                        [num_patches], image_placeholder_id, dtype=torch.int32, device=image_input.device\n+                    )\n+                    # Patchify the image\n+                    patches = self.patchify_image(image=image.unsqueeze(0), patch_size=patch_size).squeeze(0)\n+                    assert num_patches == patches.shape[0]\n+                    if variable_sized:\n+                        # Terminate each line with newline ID\n+                        tensor_of_image_ids = tensor_of_image_ids.reshape(-1, image_width // patch_width)\n+                        newline_ids = torch.full(\n+                            [tensor_of_image_ids.shape[0], 1],\n+                            image_newline_id,\n+                            dtype=torch.int32,\n+                            device=image_input.device,\n+                        )\n+                        tensor_of_image_ids = torch.cat([tensor_of_image_ids, newline_ids], dim=1)\n+                        tensor_of_image_ids = tensor_of_image_ids.reshape(-1)\n+                    images.append([image])\n+                    image_input_ids.append(tensor_of_image_ids)\n+                    image_patches.append(patches)\n+                else:\n+                    image_input_ids.append(torch.tensor([], dtype=torch.int32, device=image_input.device))\n+            batch_image_input_ids.append(image_input_ids)\n+            batch_image_patches.append(image_patches)\n+        # Create image patch indices\n+        image_patch_indices_per_batch: list[list[torch.Tensor]] = []\n+        image_patch_indices_per_subsequence: list[list[torch.Tensor]] = []\n+\n+        for sample_image_input_ids in batch_image_input_ids:\n+            index_offset = 0\n+            per_batch_indices = []\n+            per_subsequence_indices = []\n+            for subseq_image_input_ids in sample_image_input_ids:\n+                # Indices of image patches\n+                patches_mask = subseq_image_input_ids == image_placeholder_id\n+                num_patches = torch.count_nonzero(patches_mask)\n+                indices = torch.arange(num_patches, dtype=torch.int64, device=subseq_image_input_ids.device).type_as(\n+                    subseq_image_input_ids\n+                )\n+                # Place those indices in the image input ids token stream, with -1 representing non-index tokens\n+                indices_in_stream_per_batch = torch.full_like(subseq_image_input_ids, -1)\n+                indices_in_stream_per_subsequence = torch.full_like(subseq_image_input_ids, -1)\n+                patches_inds = torch.nonzero(patches_mask, as_tuple=True)[0]\n+\n+                indices_in_stream_per_batch[patches_inds] = indices + index_offset\n+                indices_in_stream_per_subsequence[patches_inds] = indices\n+\n+                per_batch_indices.append(indices_in_stream_per_batch)\n+                per_subsequence_indices.append(indices_in_stream_per_subsequence)\n+                index_offset += num_patches\n+\n+            image_patch_indices_per_batch.append(per_batch_indices)\n+            image_patch_indices_per_subsequence.append(per_subsequence_indices)\n+        return FuyuBatchFeature(\n+            data={\n+                \"images\": images,\n+                \"image_input_ids\": batch_image_input_ids,\n+                \"image_patches\": batch_image_patches,\n+                \"image_patch_indices_per_batch\": image_patch_indices_per_batch,\n+                \"image_patch_indices_per_subsequence\": image_patch_indices_per_subsequence,\n+            }\n+        )\n+\n+    def _further_process_kwargs(\n+        self,\n+        patch_size: Optional[dict[str, int]] = None,\n+        **kwargs,\n+    ) -> dict:\n+        \"\"\"\n+        Process Fuyu-specific kwargs before validation.\n+        \"\"\"\n+        kwargs = super()._further_process_kwargs(**kwargs)\n+        if patch_size is not None:\n+            patch_size = SizeDict(**get_size_dict(patch_size, param_name=\"patch_size\"))\n+        kwargs[\"patch_size\"] = patch_size\n+        return kwargs\n+\n+\n+__all__ = [\"FuyuImageProcessorFast\"]"
      },
      {
        "filename": "tests/models/fuyu/test_image_processing_fuyu.py",
        "status": "modified",
        "additions": 432,
        "deletions": 29,
        "changes": 461,
        "patch": "@@ -1,63 +1,466 @@\n+import io\n import unittest\n \n+import httpx\n import numpy as np\n+import pytest\n+from packaging import version\n \n-from transformers import is_torch_available, is_vision_available\n+from transformers.image_utils import SizeDict\n from transformers.testing_utils import (\n     require_torch,\n+    require_torch_accelerator,\n     require_torchvision,\n     require_vision,\n+    slow,\n+    torch_device,\n )\n+from transformers.utils import is_torch_available, is_vision_available\n+\n+from ...test_image_processing_common import ImageProcessingTestMixin\n \n \n if is_torch_available() and is_vision_available():\n     import torch\n \n-    from transformers import FuyuImageProcessor\n+    from transformers import FuyuImageProcessor, FuyuImageProcessorFast\n \n if is_vision_available():\n     from PIL import Image\n \n \n+class FuyuImageProcessingTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=3,\n+        num_channels=3,\n+        image_size=18,\n+        min_resolution=30,\n+        max_resolution=400,\n+        do_resize=True,\n+        size=None,\n+        do_pad=True,\n+        do_normalize=True,\n+        image_mean=[0.5, 0.5, 0.5],\n+        image_std=[0.5, 0.5, 0.5],\n+        do_rescale=True,\n+        rescale_factor=1 / 255,\n+        patch_size=None,\n+    ):\n+        size = size if size is not None else {\"height\": 180, \"width\": 360}\n+        patch_size = patch_size if patch_size is not None else {\"height\": 30, \"width\": 30}\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.num_channels = num_channels\n+        self.image_size = image_size\n+        self.min_resolution = 30\n+        self.max_resolution = 360\n+        self.do_resize = do_resize\n+        self.size = size\n+        self.do_pad = do_pad\n+        self.do_normalize = do_normalize\n+        self.image_mean = image_mean\n+        self.image_std = image_std\n+        self.do_rescale = do_rescale\n+        self.rescale_factor = rescale_factor\n+        self.patch_size = patch_size\n+\n+    def prepare_image_processor_dict(self):\n+        return {\n+            \"do_resize\": self.do_resize,\n+            \"size\": self.size,\n+            \"do_pad\": self.do_pad,\n+            \"do_normalize\": self.do_normalize,\n+            \"image_mean\": self.image_mean,\n+            \"image_std\": self.image_std,\n+            \"do_rescale\": self.do_rescale,\n+            \"rescale_factor\": self.rescale_factor,\n+            \"patch_size\": self.patch_size,\n+        }\n+\n+    def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=False):\n+        \"\"\"Prepares a batch of images for testing\"\"\"\n+        if equal_resolution:\n+            image_inputs = [\n+                np.random.randint(\n+                    0, 256, (self.num_channels, self.max_resolution, self.max_resolution), dtype=np.uint8\n+                )\n+                for _ in range(self.batch_size)\n+            ]\n+        else:\n+            heights = [\n+                h - (h % 30) for h in np.random.randint(self.min_resolution, self.max_resolution, self.batch_size)\n+            ]\n+            widths = [\n+                w - (w % 30) for w in np.random.randint(self.min_resolution, self.max_resolution, self.batch_size)\n+            ]\n+\n+            image_inputs = [\n+                np.random.randint(0, 256, (self.num_channels, height, width), dtype=np.uint8)\n+                for height, width in zip(heights, widths)\n+            ]\n+\n+        if not numpify and not torchify:\n+            image_inputs = [Image.fromarray(np.moveaxis(img, 0, -1)) for img in image_inputs]\n+\n+        if torchify:\n+            image_inputs = [torch.from_numpy(img) for img in image_inputs]\n+\n+        return image_inputs\n+\n+    def expected_output_image_shape(self, images):\n+        return self.num_channels, self.size[\"height\"], self.size[\"width\"]\n+\n+\n @require_torch\n @require_vision\n @require_torchvision\n-class TestFuyuImageProcessor(unittest.TestCase):\n+class FuyuImageProcessorTest(ImageProcessingTestMixin, unittest.TestCase):\n+    image_processing_class = FuyuImageProcessor\n+    fast_image_processing_class = FuyuImageProcessorFast\n+\n+    # Skip tests that expect pixel_values output\n+    test_cast_dtype = None\n+\n     def setUp(self):\n-        self.size = {\"height\": 160, \"width\": 320}\n-        self.processor = FuyuImageProcessor(size=self.size, padding_value=1.0)\n-        self.batch_size = 3\n-        self.channels = 3\n-        self.height = 300\n-        self.width = 300\n+        self.image_processor_tester = FuyuImageProcessingTester(self)\n+        self.image_processor_dict = self.image_processor_tester.prepare_image_processor_dict()\n \n-        self.image_input = torch.rand(self.batch_size, self.channels, self.height, self.width)\n+        # Initialize image_processor_list (from ImageProcessingTestMixin)\n+        image_processor_list = []\n+        if self.test_slow_image_processor and self.image_processing_class:\n+            image_processor_list.append(self.image_processing_class)\n+        if self.test_fast_image_processor and self.fast_image_processing_class:\n+            image_processor_list.append(self.fast_image_processing_class)\n+        self.image_processor_list = image_processor_list\n \n-        self.image_patch_dim_h = 30\n-        self.image_patch_dim_w = 30\n-        self.sample_image = np.zeros((450, 210, 3), dtype=np.uint8)\n-        self.sample_image_pil = Image.fromarray(self.sample_image)\n+    def test_call_pil(self):\n+        \"\"\"Override to handle Fuyu's custom output structure\"\"\"\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False)\n+            for image in image_inputs:\n+                self.assertIsInstance(image, Image.Image)\n \n-    def test_patches(self):\n-        expected_num_patches = self.processor.get_num_patches(image_height=self.height, image_width=self.width)\n+            encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\")\n+            self.assertIn(\"images\", encoded_images)\n+            self.assertEqual(len(encoded_images.images), 1)\n+\n+            encoded_images = image_processing(image_inputs, return_tensors=\"pt\")\n+            self.assertIn(\"images\", encoded_images)\n+            self.assertEqual(len(encoded_images.images), self.image_processor_tester.batch_size)\n+\n+    def test_call_numpy(self):\n+        \"\"\"Override to handle Fuyu's custom output structure\"\"\"\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, numpify=True)\n+            for image in image_inputs:\n+                self.assertIsInstance(image, np.ndarray)\n+\n+            encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\")\n+            self.assertIn(\"images\", encoded_images)\n+            self.assertEqual(len(encoded_images.images), 1)\n+\n+            encoded_images = image_processing(image_inputs, return_tensors=\"pt\")\n+            self.assertIn(\"images\", encoded_images)\n+            self.assertEqual(len(encoded_images.images), self.image_processor_tester.batch_size)\n+\n+    def test_call_pytorch(self):\n+        \"\"\"Override to handle Fuyu's custom output structure\"\"\"\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n+\n+            for image in image_inputs:\n+                self.assertIsInstance(image, torch.Tensor)\n+\n+            encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\")\n+            self.assertIn(\"images\", encoded_images)\n+            self.assertEqual(len(encoded_images.images), 1)\n \n-        patches_final = self.processor.patchify_image(image=self.image_input)\n-        assert patches_final.shape[1] == expected_num_patches, (\n-            f\"Expected {expected_num_patches} patches, got {patches_final.shape[1]}.\"\n+            encoded_images = image_processing(image_inputs, return_tensors=\"pt\")\n+            self.assertIn(\"images\", encoded_images)\n+            self.assertEqual(len(encoded_images.images), self.image_processor_tester.batch_size)\n+\n+    def test_call_numpy_4_channels(self):\n+        \"\"\"Skip this test as Fuyu doesn't support arbitrary channels\"\"\"\n+        self.skipTest(\"Fuyu processor is designed for 3-channel RGB images\")\n+\n+    def test_slow_fast_equivalence(self):\n+        \"\"\"Override to handle Fuyu's custom output structure\"\"\"\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n+        dummy_image = Image.open(\n+            io.BytesIO(\n+                httpx.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", follow_redirects=True).content\n+            )\n         )\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        encoding_slow = image_processor_slow(dummy_image, return_tensors=\"pt\")\n+        encoding_fast = image_processor_fast(dummy_image, return_tensors=\"pt\")\n+\n+        self._assert_slow_fast_tensors_equivalence(encoding_slow.images[0][0], encoding_fast.images[0][0])\n+\n+    def test_slow_fast_equivalence_batched(self):\n+        \"\"\"Override to handle Fuyu's custom output structure\"\"\"\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n+\n+        dummy_images = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        encoding_slow = image_processor_slow(dummy_images, return_tensors=\"pt\")\n+        encoding_fast = image_processor_fast(dummy_images, return_tensors=\"pt\")\n+\n+        # Compare each image tensor\n+        for slow_img, fast_img in zip(encoding_slow.images, encoding_fast.images):\n+            self._assert_slow_fast_tensors_equivalence(slow_img[0], fast_img[0])\n+\n+    @slow\n+    @require_torch_accelerator\n+    @require_vision\n+    @pytest.mark.torch_compile_test\n+    def test_can_compile_fast_image_processor(self):\n+        if self.fast_image_processing_class is None:\n+            self.skipTest(\"Skipping compilation test as fast image processor is not defined\")\n+        if version.parse(torch.__version__) < version.parse(\"2.3\"):\n+            self.skipTest(reason=\"This test requires torch >= 2.3 to run.\")\n+\n+        torch.compiler.reset()\n+        input_image = torch.randint(0, 255, (3, 224, 224), dtype=torch.uint8)\n+        image_processor = self.fast_image_processing_class(**self.image_processor_dict)\n+        output_eager = image_processor(input_image, device=torch_device, return_tensors=\"pt\")\n+\n+        image_processor = torch.compile(image_processor, mode=\"reduce-overhead\")\n+        output_compiled = image_processor(input_image, device=torch_device, return_tensors=\"pt\")\n+        self._assert_slow_fast_tensors_equivalence(\n+            output_eager.images[0][0], output_compiled.images[0][0], atol=1e-4, rtol=1e-4, mean_atol=1e-5\n+        )\n+\n+    def test_image_processor_properties(self):\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class(**self.image_processor_dict)\n+            self.assertTrue(hasattr(image_processor, \"do_resize\"))\n+            self.assertTrue(hasattr(image_processor, \"size\"))\n+            self.assertTrue(hasattr(image_processor, \"do_pad\"))\n+            self.assertTrue(hasattr(image_processor, \"do_normalize\"))\n+            self.assertTrue(hasattr(image_processor, \"image_mean\"))\n+            self.assertTrue(hasattr(image_processor, \"image_std\"))\n+            self.assertTrue(hasattr(image_processor, \"do_rescale\"))\n+            self.assertTrue(hasattr(image_processor, \"rescale_factor\"))\n+            self.assertTrue(hasattr(image_processor, \"patch_size\"))\n+\n+    def test_patches(self):\n+        \"\"\"Test that patchify_image produces the expected number of patches.\"\"\"\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class(**self.image_processor_dict)\n+            batch_size = 3\n+            channels = 3\n+            height = 300\n+            width = 300\n+            image_input = torch.rand(batch_size, channels, height, width)\n+\n+            expected_num_patches = image_processor.get_num_patches(image_height=height, image_width=width)\n+            patches_final = image_processor.patchify_image(image=image_input)\n+\n+            self.assertEqual(patches_final.shape[1], expected_num_patches)\n+\n+    def test_patches_match_slow_fast(self):\n+        \"\"\"Test that fast processor produces same patches as slow processor.\"\"\"\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast patch equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(\n+                reason=\"Skipping slow/fast patch equivalence test as one of the image processors is not defined\"\n+            )\n+\n+        batch_size = 3\n+        channels = 3\n+        height = 300\n+        width = 300\n+        image_input = torch.rand(batch_size, channels, height, width)\n+\n+        processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        patches_fast = processor_fast.patchify_image(image=image_input)\n+        patches_slow = processor_slow.patchify_image(image=image_input)\n+\n+        self.assertEqual(patches_fast.shape, patches_slow.shape)\n+        torch.testing.assert_close(patches_fast, patches_slow, rtol=1e-4, atol=1e-4)\n \n     def test_scale_to_target_aspect_ratio(self):\n-        # (h:450, w:210) fitting (160, 320) -> (160, 210*160/450)\n-        scaled_image = self.processor.resize(self.sample_image, size=self.size)\n-        self.assertEqual(scaled_image.shape[0], 160)\n-        self.assertEqual(scaled_image.shape[1], 74)\n+        \"\"\"Test that resize maintains aspect ratio correctly.\"\"\"\n+        sample_image = np.zeros((450, 210, 3), dtype=np.uint8)\n+\n+        if self.test_slow_image_processor and self.image_processing_class:\n+            image_processor = self.image_processing_class(**self.image_processor_dict)\n+            scaled_image = image_processor.resize(sample_image, size=self.image_processor_dict[\"size\"])\n+            self.assertEqual(scaled_image.shape[0], 180)\n+            self.assertEqual(scaled_image.shape[1], 84)\n+\n+        if self.test_fast_image_processor and self.fast_image_processing_class:\n+            image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+            sample_tensor = torch.from_numpy(sample_image).permute(2, 0, 1).float()\n+\n+            size_dict = SizeDict(\n+                height=self.image_processor_dict[\"size\"][\"height\"], width=self.image_processor_dict[\"size\"][\"width\"]\n+            )\n+            scaled_image = image_processor_fast.resize(sample_tensor, size=size_dict)\n+\n+            self.assertEqual(scaled_image.shape[1], 180)\n+            self.assertEqual(scaled_image.shape[2], 84)\n \n     def test_apply_transformation_numpy(self):\n-        transformed_image = self.processor.preprocess(self.sample_image).images[0][0]\n-        self.assertEqual(transformed_image.shape[1], 160)\n-        self.assertEqual(transformed_image.shape[2], 320)\n+        \"\"\"Test preprocessing with numpy input.\"\"\"\n+        sample_image = np.zeros((450, 210, 3), dtype=np.uint8)\n+\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class(**self.image_processor_dict)\n+            transformed_image = image_processor.preprocess(sample_image).images[0][0]\n+            self.assertEqual(transformed_image.shape[1], 180)\n+            self.assertEqual(transformed_image.shape[2], 360)\n \n     def test_apply_transformation_pil(self):\n-        transformed_image = self.processor.preprocess(self.sample_image_pil).images[0][0]\n-        self.assertEqual(transformed_image.shape[1], 160)\n-        self.assertEqual(transformed_image.shape[2], 320)\n+        \"\"\"Test preprocessing with PIL input.\"\"\"\n+        sample_image = np.zeros((450, 210, 3), dtype=np.uint8)\n+        sample_image_pil = Image.fromarray(sample_image)\n+\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class(**self.image_processor_dict)\n+            transformed_image = image_processor.preprocess(sample_image_pil).images[0][0]\n+            self.assertEqual(transformed_image.shape[1], 180)\n+            self.assertEqual(transformed_image.shape[2], 360)\n+\n+    def test_preprocess_output_structure(self):\n+        \"\"\"Test that preprocess returns correct output structure.\"\"\"\n+        sample_image = np.zeros((450, 210, 3), dtype=np.uint8)\n+\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class(**self.image_processor_dict)\n+            result = image_processor.preprocess(sample_image)\n+\n+            self.assertIn(\"images\", result)\n+            self.assertIn(\"image_unpadded_heights\", result)\n+            self.assertIn(\"image_unpadded_widths\", result)\n+            self.assertIn(\"image_scale_factors\", result)\n+\n+            self.assertEqual(len(result.images), 1)\n+            self.assertEqual(len(result.images[0]), 1)\n+            self.assertEqual(len(result.image_unpadded_heights), 1)\n+            self.assertEqual(len(result.image_unpadded_widths), 1)\n+            self.assertEqual(len(result.image_scale_factors), 1)\n+\n+    def test_batch_processing(self):\n+        \"\"\"Test processing multiple images.\"\"\"\n+        sample_image = np.zeros((450, 210, 3), dtype=np.uint8)\n+        sample_image_pil = Image.fromarray(sample_image)\n+        images = [sample_image, sample_image_pil]\n+\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class(**self.image_processor_dict)\n+            result = image_processor.preprocess(images)\n+\n+            self.assertEqual(len(result.images), 2)\n+            for img in result.images:\n+                self.assertEqual(len(img), 1)\n+                if hasattr(img[0], \"shape\"):\n+                    if len(img[0].shape) == 3:\n+                        self.assertEqual(img[0].shape[1], 180)\n+                        self.assertEqual(img[0].shape[2], 360)\n+\n+    def test_pad_image_fast(self):\n+        \"\"\"Test that padding works correctly for fast processor.\"\"\"\n+        if not self.test_fast_image_processor or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Fast processor not available\")\n+\n+        from transformers.image_utils import SizeDict\n+\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        small_image = torch.rand(3, 100, 100)\n+        size_dict = SizeDict(height=180, width=360)\n+\n+        padded = image_processor_fast.pad([small_image], pad_size=size_dict, fill_value=1.0)[0]\n+        self.assertEqual(padded.shape[1], 180)\n+        self.assertEqual(padded.shape[2], 360)\n+\n+        self.assertTrue(torch.allclose(padded[:, 100:, :], torch.ones_like(padded[:, 100:, :])))\n+        self.assertTrue(torch.allclose(padded[:, :, 100:], torch.ones_like(padded[:, :, 100:])))\n+\n+    def test_preprocess_with_tokenizer_info(self):\n+        \"\"\"Test preprocess_with_tokenizer_info functionality.\"\"\"\n+        batch_size = 2\n+        subseq_size = 1\n+        channels = 3\n+        image_input = torch.rand(batch_size, subseq_size, channels, 180, 360)\n+        image_present = torch.ones(batch_size, subseq_size, dtype=torch.bool)\n+        image_unpadded_h = torch.tensor([[180], [180]])\n+        image_unpadded_w = torch.tensor([[360], [360]])\n+\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class(**self.image_processor_dict)\n+\n+            result = image_processor.preprocess_with_tokenizer_info(\n+                image_input=image_input,\n+                image_present=image_present,\n+                image_unpadded_h=image_unpadded_h,\n+                image_unpadded_w=image_unpadded_w,\n+                image_placeholder_id=100,\n+                image_newline_id=101,\n+                variable_sized=True,\n+            )\n+\n+            # Check output structure\n+            self.assertIn(\"images\", result)\n+            self.assertIn(\"image_input_ids\", result)\n+            self.assertIn(\"image_patches\", result)\n+            self.assertIn(\"image_patch_indices_per_batch\", result)\n+            self.assertIn(\"image_patch_indices_per_subsequence\", result)\n+\n+            # Check batch structure\n+            self.assertEqual(len(result.images), batch_size)\n+            self.assertEqual(len(result.image_input_ids), batch_size)\n+            self.assertEqual(len(result.image_patches), batch_size)\n+\n+    def test_device_handling_fast(self):\n+        \"\"\"Test that fast processor can handle device placement.\"\"\"\n+        if not self.test_fast_image_processor or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Fast processor not available\")\n+\n+        sample_image = np.zeros((450, 210, 3), dtype=np.uint8)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        if torch.cuda.is_available():\n+            result_cuda = image_processor_fast.preprocess(sample_image, device=\"cuda\")\n+            self.assertEqual(result_cuda.images[0][0].device.type, \"cuda\")\n+\n+        result_cpu = image_processor_fast.preprocess(sample_image, device=\"cpu\")\n+        self.assertEqual(result_cpu.images[0][0].device.type, \"cpu\")\n+\n+    def test_do_not_resize_if_smaller(self):\n+        \"\"\"Test that images smaller than target size are not resized.\"\"\"\n+        if not self.test_fast_image_processor or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Fast processor not available\")\n+\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        small_image = torch.rand(3, 100, 150)\n+        size_dict = SizeDict(height=180, width=360)\n+\n+        resized = image_processor_fast.resize(small_image, size=size_dict)\n+\n+        self.assertEqual(resized.shape[1], 100)\n+        self.assertEqual(resized.shape[2], 150)"
      }
    ],
    "num_files": 7,
    "scraped_at": "2025-11-16T21:16:50.377457",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR introduces a new FuyuImageProcessorFast class with non-trivial implementation (~382 lines), inherits from BaseImageProcessorFast, leverages torchvision for optimizations, and includes substantial test updates. The changes involve architectural decisions about performance optimization and image processing logic that would be valuable for developers to understand.",
      "substance_level": "high"
    }
  },
  {
    "pr_number": 41790,
    "title": "Fix attention mask in mamba layers",
    "body": "# What does this PR do?\r\n\r\nAs per title. It was reported by LFM-VL team that the batched generation is outputting garbage with one of the checkpoints. I found that the masking is not being applied at all for mamba layers\r\n\r\nFirstly, mamba layer do not have a 4D attention weight and thus need a normal attention in 2D. Also we do not need to check if the attention has a certain shape, instead we only make sure it is applied in prefill stage\r\n\r\nThis fixes LFM-VL but Ig all mamba models are affected. I'm still surprised we didn't get issues before and that bigger checkpoints of LFM-VL generated normal text even without proper masking. I'm going to fix other mamba models and tag for review when ready",
    "html_url": "https://github.com/huggingface/transformers/pull/41790",
    "created_at": "2025-10-22T13:29:41Z",
    "merged_at": "2025-10-22T16:15:38Z",
    "merge_commit_sha": "87be5595081364ef99393feeaa60d71db3652679",
    "base_ref": "main",
    "head_sha": "a8b35a0273467ac687ae075a86c3b6fb8f57910d",
    "user": "zucchini-nlp",
    "files": [
      {
        "filename": "src/transformers/models/bamba/modeling_bamba.py",
        "status": "modified",
        "additions": 4,
        "deletions": 3,
        "changes": 7,
        "patch": "@@ -486,20 +486,21 @@ def segment_sum(input_tensor):\n     return tensor_segsum\n \n \n-is_fast_path_available = all((selective_state_update, causal_conv1d_fn, causal_conv1d_update))\n-\n-\n def apply_mask_to_padding_states(hidden_states, attention_mask):\n     \"\"\"\n     Tunes out the hidden states for padding tokens, see https://github.com/state-spaces/mamba/issues/66\n     \"\"\"\n+    # NOTE: attention mask is a 2D boolean tensor\n     if attention_mask is not None and attention_mask.shape[1] > 1 and attention_mask.shape[0] > 1:\n         dtype = hidden_states.dtype\n         hidden_states = (hidden_states * attention_mask[:, :, None]).to(dtype)\n \n     return hidden_states\n \n \n+is_fast_path_available = all((selective_state_update, causal_conv1d_fn, causal_conv1d_update))\n+\n+\n # Adapted from transformers.models.mamba2.modeling_mamba2.Mamba2Mixer\n class BambaMixer(nn.Module):\n     \"\"\""
      },
      {
        "filename": "src/transformers/models/bamba/modular_bamba.py",
        "status": "modified",
        "additions": 1,
        "deletions": 11,
        "changes": 12,
        "patch": "@@ -36,6 +36,7 @@\n )\n from transformers.models.mamba2.modeling_mamba2 import (\n     MambaRMSNormGated,\n+    apply_mask_to_padding_states,\n     pad_tensor_by_size,\n     reshape_into_chunks,\n     segment_sum,\n@@ -203,17 +204,6 @@ class BambaRMSNormGated(MambaRMSNormGated):\n     pass\n \n \n-def apply_mask_to_padding_states(hidden_states, attention_mask):\n-    \"\"\"\n-    Tunes out the hidden states for padding tokens, see https://github.com/state-spaces/mamba/issues/66\n-    \"\"\"\n-    if attention_mask is not None and attention_mask.shape[1] > 1 and attention_mask.shape[0] > 1:\n-        dtype = hidden_states.dtype\n-        hidden_states = (hidden_states * attention_mask[:, :, None]).to(dtype)\n-\n-    return hidden_states\n-\n-\n # Adapted from transformers.models.mamba2.modeling_mamba2.Mamba2Mixer\n class BambaMixer(nn.Module):\n     \"\"\""
      },
      {
        "filename": "src/transformers/models/falcon_h1/modeling_falcon_h1.py",
        "status": "modified",
        "additions": 4,
        "deletions": 3,
        "changes": 7,
        "patch": "@@ -521,20 +521,21 @@ def segment_sum(input_tensor):\n     return tensor_segsum\n \n \n-is_fast_path_available = all((selective_state_update, causal_conv1d_fn, causal_conv1d_update))\n-\n-\n def apply_mask_to_padding_states(hidden_states, attention_mask):\n     \"\"\"\n     Tunes out the hidden states for padding tokens, see https://github.com/state-spaces/mamba/issues/66\n     \"\"\"\n+    # NOTE: attention mask is a 2D boolean tensor\n     if attention_mask is not None and attention_mask.shape[1] > 1 and attention_mask.shape[0] > 1:\n         dtype = hidden_states.dtype\n         hidden_states = (hidden_states * attention_mask[:, :, None]).to(dtype)\n \n     return hidden_states\n \n \n+is_fast_path_available = all((selective_state_update, causal_conv1d_fn, causal_conv1d_update))\n+\n+\n # Adapted from transformers.models.mamba2.modeling_mamba2.Mamba2Mixer\n class FalconH1Mixer(nn.Module):\n     \"\"\""
      },
      {
        "filename": "src/transformers/models/falcon_h1/modular_falcon_h1.py",
        "status": "modified",
        "additions": 1,
        "deletions": 11,
        "changes": 12,
        "patch": "@@ -39,6 +39,7 @@\n )\n from transformers.models.mamba2.modeling_mamba2 import (\n     MambaRMSNormGated,\n+    apply_mask_to_padding_states,\n     pad_tensor_by_size,\n     reshape_into_chunks,\n     segment_sum,\n@@ -285,17 +286,6 @@ def forward(self, hidden_states, gate=None):\n         return hidden_states.to(input_dtype)\n \n \n-def apply_mask_to_padding_states(hidden_states, attention_mask):\n-    \"\"\"\n-    Tunes out the hidden states for padding tokens, see https://github.com/state-spaces/mamba/issues/66\n-    \"\"\"\n-    if attention_mask is not None and attention_mask.shape[1] > 1 and attention_mask.shape[0] > 1:\n-        dtype = hidden_states.dtype\n-        hidden_states = (hidden_states * attention_mask[:, :, None]).to(dtype)\n-\n-    return hidden_states\n-\n-\n # Adapted from transformers.models.mamba2.modeling_mamba2.Mamba2Mixer\n class FalconH1Mixer(nn.Module):\n     \"\"\""
      },
      {
        "filename": "src/transformers/models/granitemoehybrid/modeling_granitemoehybrid.py",
        "status": "modified",
        "additions": 4,
        "deletions": 3,
        "changes": 7,
        "patch": "@@ -322,20 +322,21 @@ def segment_sum(input_tensor):\n     return tensor_segsum\n \n \n-is_fast_path_available = all((selective_state_update, causal_conv1d_fn, causal_conv1d_update))\n-\n-\n def apply_mask_to_padding_states(hidden_states, attention_mask):\n     \"\"\"\n     Tunes out the hidden states for padding tokens, see https://github.com/state-spaces/mamba/issues/66\n     \"\"\"\n+    # NOTE: attention mask is a 2D boolean tensor\n     if attention_mask is not None and attention_mask.shape[1] > 1 and attention_mask.shape[0] > 1:\n         dtype = hidden_states.dtype\n         hidden_states = (hidden_states * attention_mask[:, :, None]).to(dtype)\n \n     return hidden_states\n \n \n+is_fast_path_available = all((selective_state_update, causal_conv1d_fn, causal_conv1d_update))\n+\n+\n # Adapted from transformers.models.mamba2.modeling_mamba2.Mamba2Mixer\n class GraniteMoeHybridMambaLayer(nn.Module):\n     \"\"\""
      },
      {
        "filename": "src/transformers/models/lfm2/modeling_lfm2.py",
        "status": "modified",
        "additions": 5,
        "deletions": 1,
        "changes": 6,
        "patch": "@@ -422,6 +422,7 @@ def apply_mask_to_padding_states(hidden_states, attention_mask):\n     \"\"\"\n     Tunes out the hidden states for padding tokens, see https://github.com/state-spaces/mamba/issues/66\n     \"\"\"\n+    # NOTE: attention mask is a 2D boolean tensor\n     if attention_mask is not None and attention_mask.shape[1] > 1 and attention_mask.shape[0] > 1:\n         dtype = hidden_states.dtype\n         hidden_states = (hidden_states * attention_mask[:, :, None]).to(dtype)\n@@ -665,15 +666,18 @@ def forward(\n             past_key_values=past_key_values,\n             position_ids=position_ids,\n         )\n+        # Skip masking for decoding stage. We check shape here to be compile-friendly\n+        linear_attention = attention_mask if inputs_embeds.shape[1] != 1 else None\n \n         hidden_states = inputs_embeds\n         position_embeddings = self.rotary_emb(hidden_states, position_ids=position_ids)\n \n         # decoder layers\n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n+            layer_mask = causal_mask if decoder_layer.is_attention_layer else linear_attention\n             hidden_states = decoder_layer(\n                 hidden_states,\n-                attention_mask=causal_mask,\n+                attention_mask=layer_mask,\n                 position_embeddings=position_embeddings,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,"
      },
      {
        "filename": "src/transformers/models/lfm2/modular_lfm2.py",
        "status": "modified",
        "additions": 4,
        "deletions": 1,
        "changes": 5,
        "patch": "@@ -473,15 +473,18 @@ def forward(\n             past_key_values=past_key_values,\n             position_ids=position_ids,\n         )\n+        # Skip masking for decoding stage. We check shape here to be compile-friendly\n+        linear_attention = attention_mask if inputs_embeds.shape[1] != 1 else None\n \n         hidden_states = inputs_embeds\n         position_embeddings = self.rotary_emb(hidden_states, position_ids=position_ids)\n \n         # decoder layers\n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n+            layer_mask = causal_mask if decoder_layer.is_attention_layer else linear_attention\n             hidden_states = decoder_layer(\n                 hidden_states,\n-                attention_mask=causal_mask,\n+                attention_mask=layer_mask,\n                 position_embeddings=position_embeddings,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,"
      },
      {
        "filename": "src/transformers/models/lfm2_moe/modeling_lfm2_moe.py",
        "status": "modified",
        "additions": 5,
        "deletions": 1,
        "changes": 6,
        "patch": "@@ -485,6 +485,7 @@ def apply_mask_to_padding_states(hidden_states, attention_mask):\n     \"\"\"\n     Tunes out the hidden states for padding tokens, see https://github.com/state-spaces/mamba/issues/66\n     \"\"\"\n+    # NOTE: attention mask is a 2D boolean tensor\n     if attention_mask is not None and attention_mask.shape[1] > 1 and attention_mask.shape[0] > 1:\n         dtype = hidden_states.dtype\n         hidden_states = (hidden_states * attention_mask[:, :, None]).to(dtype)\n@@ -732,15 +733,18 @@ def forward(\n             past_key_values=past_key_values,\n             position_ids=position_ids,\n         )\n+        # Skip masking for decoding stage. We check shape here to be compile-friendly\n+        linear_attention = attention_mask if inputs_embeds.shape[1] != 1 else None\n \n         hidden_states = inputs_embeds\n         position_embeddings = self.pos_emb(hidden_states, position_ids=position_ids)\n \n         # decoder layers\n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n+            layer_mask = causal_mask if decoder_layer.is_attention_layer else linear_attention\n             hidden_states = decoder_layer(\n                 hidden_states,\n-                attention_mask=causal_mask,\n+                attention_mask=layer_mask,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n                 cache_position=cache_position,"
      },
      {
        "filename": "src/transformers/models/lfm2_moe/modular_lfm2_moe.py",
        "status": "modified",
        "additions": 4,
        "deletions": 1,
        "changes": 5,
        "patch": "@@ -180,15 +180,18 @@ def forward(\n             past_key_values=past_key_values,\n             position_ids=position_ids,\n         )\n+        # Skip masking for decoding stage. We check shape here to be compile-friendly\n+        linear_attention = attention_mask if inputs_embeds.shape[1] != 1 else None\n \n         hidden_states = inputs_embeds\n         position_embeddings = self.pos_emb(hidden_states, position_ids=position_ids)\n \n         # decoder layers\n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n+            layer_mask = causal_mask if decoder_layer.is_attention_layer else linear_attention\n             hidden_states = decoder_layer(\n                 hidden_states,\n-                attention_mask=causal_mask,\n+                attention_mask=layer_mask,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n                 cache_position=cache_position,"
      },
      {
        "filename": "src/transformers/models/mamba2/modeling_mamba2.py",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -117,6 +117,7 @@ def apply_mask_to_padding_states(hidden_states, attention_mask):\n     \"\"\"\n     Tunes out the hidden states for padding tokens, see https://github.com/state-spaces/mamba/issues/66\n     \"\"\"\n+    # NOTE: attention mask is a 2D boolean tensor\n     if attention_mask is not None and attention_mask.shape[1] > 1 and attention_mask.shape[0] > 1:\n         dtype = hidden_states.dtype\n         hidden_states = (hidden_states * attention_mask[:, :, None]).to(dtype)"
      },
      {
        "filename": "src/transformers/models/qwen3_next/modeling_qwen3_next.py",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -430,6 +430,7 @@ def apply_mask_to_padding_states(hidden_states, attention_mask):\n     \"\"\"\n     Tunes out the hidden states for padding tokens, see https://github.com/state-spaces/mamba/issues/66\n     \"\"\"\n+    # NOTE: attention mask is a 2D boolean tensor\n     if attention_mask is not None and attention_mask.shape[1] > 1 and attention_mask.shape[0] > 1:\n         dtype = hidden_states.dtype\n         hidden_states = (hidden_states * attention_mask[:, :, None]).to(dtype)"
      },
      {
        "filename": "tests/models/lfm2_moe/test_modeling_lfm2_moe.py",
        "status": "modified",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "patch": "@@ -220,9 +220,9 @@ def test_model_1a8b_generation(self):\n     def test_model_1a8b_batched_chat_generation(self):\n         prompts = [\"Who are you?\", \"Complete the text: Lorem ipsum dolor \", \"The Meji Restoration in Japan ended\"]\n         EXPECTED_TEXT_COMPLETIONS = [\n-            \"Who are you??  \\nI am an artificial intelligence assistant designed to provide information, answer questions\",\n+            \"Who are you?, a language model designed to assist with information and tasks?  \\nI am\",\n             \"Complete the text: Lorem ipsum dolor ipsum dolor ipsum dolor ipsum dolor ipsum dolor\",\n-            \"The Meji Restoration in Japan ended (1868) marked the:  \\nA) Establishment of a constitutional\",\n+            \"The Meji Restoration in Japan ended or the Meiji Restoration (1868\u20131912) marked a pivotal\",\n         ]\n         set_seed(1789)\n         tokenizer = AutoTokenizer.from_pretrained(\"LiquidAI/LFM2-8B-A1B\", use_fast=False)"
      },
      {
        "filename": "tests/models/lfm2_vl/test_modeling_lfm2_vl.py",
        "status": "modified",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "patch": "@@ -300,7 +300,7 @@ def test_integration_test_batched(self):\n         )\n \n         # Create inputs\n-        text = [\"<image>In this image, we see\", \"<image>In this image, we see\"]\n+        text = [\"<image>In this image, we see\", \"<image>In this image, we see a cat\"]\n         images = [[self.image2], [self.image]]\n         inputs = self.processor(text=text, images=images, return_tensors=\"pt\", padding=True)\n         inputs.to(device=torch_device, dtype=torch.bfloat16)\n@@ -310,6 +310,6 @@ def test_integration_test_batched(self):\n \n         expected_generated_text = [\n             \"In this image, we see a panoramic view of the New York City skyline. The iconic Statics and the New York\",\n-            \"In this image, there is a cat on a bed with a cat on a bed with a cat on a bed with a cat on a bed\",\n+            \"In this image, we see a cat that is lying on its side on a cat bed.\",\n         ]\n         self.assertListEqual(generated_texts, expected_generated_text)"
      }
    ],
    "num_files": 13,
    "scraped_at": "2025-11-16T21:16:54.624374",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR fixes a meaningful bug in attention mask handling across multiple mamba-based model implementations. It addresses a real issue where batched generation was producing garbage output due to improper masking application, involving logic changes around when and how masks are applied to different layer types (attention vs. linear/mamba layers). The changes demonstrate understanding of model architecture differences and have enough substance to generate questions about mask handling, layer differentiation, and batched generation.",
      "substance_level": "medium"
    }
  },
  {
    "pr_number": 41778,
    "title": "Fix Qwen3-Omni RoPE",
    "body": "# What does this PR do?\r\n\r\nAs per title, after the last PR with RoPE refactoring, Qwen3-Omni model has issues when loading the model. One of the many sub-configs doesn't call standardization on RoPE which causes issues\r\n\r\nI also updated slow tests with correct checkpoint, right now they use Omni-2 checkpoints and thus do not test anything\r\n\r\ncc @BakerBunker ",
    "html_url": "https://github.com/huggingface/transformers/pull/41778",
    "created_at": "2025-10-22T09:28:17Z",
    "merged_at": "2025-11-06T08:30:40Z",
    "merge_commit_sha": "85c50557b97590538229f99a321ea88d03d6eaa7",
    "base_ref": "main",
    "head_sha": "ff3ae1aee7535114a933097f31b1193f3c2e3793",
    "user": "zucchini-nlp",
    "files": [
      {
        "filename": "src/transformers/models/qwen3_omni_moe/configuration_qwen3_omni_moe.py",
        "status": "modified",
        "additions": 15,
        "deletions": 4,
        "changes": 19,
        "patch": "@@ -347,6 +347,7 @@ def __init__(\n             tie_word_embeddings=tie_word_embeddings,\n             **kwargs,\n         )\n+        rope_config_validation(self, ignore_keys={\"mrope_section\", \"interleaved\", \"mrope_interleaved\"})\n \n \n class Qwen3OmniMoeThinkerConfig(PreTrainedConfig):\n@@ -947,8 +948,10 @@ class Qwen3OmniMoeCode2WavConfig(PreTrainedConfig):\n             Dimensionality of the hidden states and embeddings in the autoregressive transformer decoder.\n         max_position_embeddings (`int`, *optional*, defaults to 8000):\n             Maximum sequence length that the autoregressive decoder can handle. Determines positional embedding size.\n-        rope_theta (`float`, *optional*, defaults to 10000.0):\n-            The base period for rotary position embeddings (RoPE) applied to attention layers.\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         num_attention_heads (`int`, *optional*, defaults to 16):\n             Number of attention heads for each attention layer in the decoder.\n         num_key_value_heads (`int`, *optional*, defaults to 16):\n@@ -998,7 +1001,7 @@ def __init__(\n         codebook_size=2048,\n         hidden_size=1024,\n         max_position_embeddings=8000,\n-        rope_theta=10000,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n         num_attention_heads=16,\n         num_key_value_heads=16,\n         attention_bias=False,\n@@ -1019,7 +1022,6 @@ def __init__(\n         self.codebook_size = codebook_size\n         self.hidden_size = hidden_size\n         self.max_position_embeddings = max_position_embeddings\n-        self.rope_theta = rope_theta\n         self.num_attention_heads = num_attention_heads\n         self.num_key_value_heads = num_key_value_heads\n         self.attention_bias = attention_bias\n@@ -1035,6 +1037,15 @@ def __init__(\n         self.decoder_dim = decoder_dim\n         self.attention_dropout = attention_dropout\n \n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n+\n+        # Validate the correctness of rotary position embeddings parameters\n+        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n+        rope_config_validation(self)\n+\n     @property\n     def layer_types(self):\n         \"\"\""
      },
      {
        "filename": "src/transformers/models/qwen3_omni_moe/modeling_qwen3_omni_moe.py",
        "status": "modified",
        "additions": 18,
        "deletions": 79,
        "changes": 97,
        "patch": "@@ -2687,69 +2687,8 @@ class Qwen3OmniMoeTalkerOutputWithPast(MoeCausalLMOutputWithPast):\n     generation_step: Optional[int] = None\n \n \n-class Qwen3OmniMoeTalkerRotaryEmbedding(nn.Module):\n-    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n-\n-    def __init__(self, config: Qwen3OmniMoeConfig, device=None):\n-        super().__init__()\n-        self.max_seq_len_cached = config.max_position_embeddings\n-        self.original_max_seq_len = config.max_position_embeddings\n-\n-        self.config = config\n-\n-        self.rope_type = self.config.rope_parameters[\"rope_type\"]\n-        rope_init_fn: Callable = self.compute_default_rope_parameters\n-        if self.rope_type != \"default\":\n-            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n-        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n-\n-        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n-\n-    @staticmethod\n-    def compute_default_rope_parameters(\n-        config: Optional[Qwen3OmniMoeConfig] = None,\n-        device: Optional[\"torch.device\"] = None,\n-        seq_len: Optional[int] = None,\n-    ) -> tuple[\"torch.Tensor\", float]:\n-        \"\"\"\n-        Computes the inverse frequencies according to the original RoPE implementation\n-        Args:\n-            config ([`~transformers.PreTrainedConfig`]):\n-                The model configuration.\n-            device (`torch.device`):\n-                The device to use for initialization of the inverse frequencies.\n-            seq_len (`int`, *optional*):\n-                The current sequence length. Unused for this type of RoPE.\n-        Returns:\n-            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n-            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n-        \"\"\"\n-        base = config.rope_parameters[\"rope_theta\"]\n-        dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n-\n-        attention_factor = 1.0  # Unused in this type of RoPE\n-\n-        # Compute the inverse frequencies\n-        inv_freq = 1.0 / (\n-            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n-        )\n-        return inv_freq, attention_factor\n-\n-    @torch.no_grad()\n-    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n-    def forward(self, x, position_ids):\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n-        position_ids_expanded = position_ids[:, None, :].float()\n-\n-        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n-            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos() * self.attention_scaling\n-            sin = emb.sin() * self.attention_scaling\n-\n-        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+class Qwen3OmniMoeTalkerRotaryEmbedding(Qwen3OmniMoeThinkerTextRotaryEmbedding):\n+    pass\n \n \n class Qwen3OmniMoeTalkerTextMLP(nn.Module):\n@@ -3823,7 +3762,7 @@ def _get_talker_user_parts(\n     ):\n         user_talker_part = torch.empty(\n             (1, segment_end_index - im_start_index, self.config.talker_config.text_config.hidden_size),\n-            device=self.talker.device,\n+            device=thinker_hidden.device,\n             dtype=self.talker.dtype,\n         )\n \n@@ -3832,18 +3771,18 @@ def _get_talker_user_parts(\n         # Multimodal data exists\n         if user_mm_mask.any():\n             user_thinker_hidden_mm = thinker_hidden[:, im_start_index:segment_end_index][user_mm_mask]\n-            mm_hidden = self.talker.hidden_projection(user_thinker_hidden_mm).to(self.talker.device)\n+            mm_hidden = self.talker.hidden_projection(user_thinker_hidden_mm).to(thinker_hidden.device)\n             user_talker_part[user_mm_mask] = mm_hidden\n         user_thinker_embed = thinker_embed[:, im_start_index:segment_end_index][~user_mm_mask]\n-        user_text_hidden = self.talker.text_projection(user_thinker_embed).to(self.talker.device)\n+        user_text_hidden = self.talker.text_projection(user_thinker_embed).to(thinker_hidden.device)\n         user_talker_part[~user_mm_mask] = user_text_hidden\n         return user_talker_part\n \n     def _get_talker_assistant_parts(\n         self, im_start_index, segment_end_index, speaker_id, thinker_embed, tts_pad_embed, tts_bos_embed, tts_eos_embed\n     ):\n         assistant_hidden = self.talker.text_projection(thinker_embed[:, im_start_index:segment_end_index]).to(\n-            self.talker.device\n+            tts_pad_embed.device\n         )  # [1 t d]\n         assistant_text_hidden = torch.cat(\n             (\n@@ -3865,17 +3804,17 @@ def _get_talker_assistant_parts(\n                     self.config.talker_config.codec_bos_id,\n                 ]\n             ],\n-            device=self.talker.device,\n+            device=tts_pad_embed.device,\n             dtype=torch.long,\n         )\n         assistant_codec_hidden = torch.cat(\n             (\n                 torch.zeros(\n                     (1, 3, self.config.talker_config.text_config.hidden_size),\n-                    device=self.talker.device,\n+                    device=tts_pad_embed.device,\n                     dtype=self.talker.dtype,\n                 ),\n-                self.talker.get_input_embeddings()(codec_special_tokens).to(self.talker.device),\n+                self.talker.get_input_embeddings()(codec_special_tokens).to(tts_pad_embed.device),\n             ),\n             dim=1,\n         )\n@@ -3991,31 +3930,31 @@ def generate(\n         thinker_result = self.thinker.generate(input_ids=input_ids, **thinker_kwargs)\n \n         if not generate_audio:\n-            return thinker_result, None\n+            return thinker_result\n \n         # 2. Prepare talker input\n         thinker_embed = torch.cat([hidden_states[0] for hidden_states in thinker_result.hidden_states], dim=1).to(\n-            self.talker.device\n+            input_ids.device\n         )  # [1 t d]\n         thinker_hidden = torch.cat(\n             [\n                 hidden_states[self.config.talker_config.accept_hidden_layer]\n                 for hidden_states in thinker_result.hidden_states\n             ],\n             dim=1,\n-        ).to(self.talker.device)  # [1 t d]\n+        ).to(input_ids.device)  # [1 t d]\n         im_start_indexes = torch.cat(\n             (\n                 torch.nonzero(input_ids[0] == self.config.im_start_token_id).squeeze(),\n                 torch.tensor([thinker_result.sequences.shape[-1]], device=input_ids.device, dtype=input_ids.dtype),\n             ),\n             dim=-1,\n-        ).to(self.talker.device)  # Shape [n_starts + 1]; Take batch 0 since batched inference is not supported here.\n+        )  # Shape [n_starts + 1]; Take batch 0 since batched inference is not supported here.\n         multimodal_mask = (\n             (thinker_result.sequences == self.config.thinker_config.audio_token_id) |\n             (thinker_result.sequences == self.config.thinker_config.image_token_id) |\n             (thinker_result.sequences == self.config.thinker_config.video_token_id)\n-        ).to(self.talker.device)  # [1 t] # fmt: skip\n+        ).to(input_ids.device)  # [1 t] # fmt: skip\n \n         talker_special_tokens = torch.tensor(\n             [[self.config.tts_bos_token_id, self.config.tts_eos_token_id, self.config.tts_pad_token_id]],\n@@ -4024,7 +3963,7 @@ def generate(\n         )\n         tts_bos_embed, tts_eos_embed, tts_pad_embed = (\n             self.talker.text_projection(self.thinker.get_input_embeddings()(talker_special_tokens))\n-            .to(self.talker.device)\n+            .to(input_ids.device)\n             .chunk(3, dim=1)\n         )  # 3 * [1 1 d]\n \n@@ -4063,8 +4002,8 @@ def generate(\n                 continue\n             else:\n                 raise AssertionError(\"Expect role id after <|im_start|> (assistant, user, system)\")\n-        talker_input_embed = torch.cat([embed.to(self.talker.device) for embed in talker_input_embeds], dim=1)\n-        talker_input_id = torch.cat([embed.to(self.talker.device) for embed in talker_input_ids], dim=1)\n+        talker_input_embed = torch.cat([embed.to(input_ids.device) for embed in talker_input_embeds], dim=1)\n+        talker_input_id = torch.cat([embed.to(input_ids.device) for embed in talker_input_ids], dim=1)\n         talker_result = self.talker.generate(\n             inputs_embeds=talker_input_embed,\n             trailing_text_hidden=trailing_text_hidden,\n@@ -4079,7 +4018,7 @@ def generate(\n         )\n         talker_wavs = self.code2wav.chunked_decode(talker_codes, chunk_size=300, left_context_size=25)\n \n-        return thinker_result, talker_wavs.float()\n+        return thinker_result.sequences, talker_wavs.float()\n \n \n __all__ = ["
      },
      {
        "filename": "src/transformers/models/qwen3_omni_moe/modular_qwen3_omni_moe.py",
        "status": "modified",
        "additions": 32,
        "deletions": 22,
        "changes": 54,
        "patch": "@@ -217,7 +217,7 @@ def __init__(\n         # Validate the correctness of rotary position embeddings parameters\n         rope_theta = kwargs.get(\"rope_theta\", 1000000.0)\n         standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        rope_config_validation(self, ignore_keys={\"mrope_section\", \"interleaved\", \"mrope_interleaved\"})\n \n \n class Qwen3OmniMoeThinkerConfig(Qwen2_5OmniThinkerConfig):\n@@ -581,8 +581,10 @@ class Qwen3OmniMoeCode2WavConfig(PreTrainedConfig):\n             Dimensionality of the hidden states and embeddings in the autoregressive transformer decoder.\n         max_position_embeddings (`int`, *optional*, defaults to 8000):\n             Maximum sequence length that the autoregressive decoder can handle. Determines positional embedding size.\n-        rope_theta (`float`, *optional*, defaults to 10000.0):\n-            The base period for rotary position embeddings (RoPE) applied to attention layers.\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         num_attention_heads (`int`, *optional*, defaults to 16):\n             Number of attention heads for each attention layer in the decoder.\n         num_key_value_heads (`int`, *optional*, defaults to 16):\n@@ -632,7 +634,7 @@ def __init__(\n         codebook_size=2048,\n         hidden_size=1024,\n         max_position_embeddings=8000,\n-        rope_theta=10000,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n         num_attention_heads=16,\n         num_key_value_heads=16,\n         attention_bias=False,\n@@ -653,7 +655,6 @@ def __init__(\n         self.codebook_size = codebook_size\n         self.hidden_size = hidden_size\n         self.max_position_embeddings = max_position_embeddings\n-        self.rope_theta = rope_theta\n         self.num_attention_heads = num_attention_heads\n         self.num_key_value_heads = num_key_value_heads\n         self.attention_bias = attention_bias\n@@ -669,6 +670,15 @@ def __init__(\n         self.decoder_dim = decoder_dim\n         self.attention_dropout = attention_dropout\n \n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n+\n+        # Validate the correctness of rotary position embeddings parameters\n+        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n+        rope_config_validation(self)\n+\n     @property\n     def layer_types(self):\n         \"\"\"\n@@ -1681,7 +1691,7 @@ class Qwen3OmniMoeTalkerOutputWithPast(MoeCausalLMOutputWithPast):\n     generation_step: Optional[int] = None\n \n \n-class Qwen3OmniMoeTalkerRotaryEmbedding(Qwen3RotaryEmbedding):\n+class Qwen3OmniMoeTalkerRotaryEmbedding(Qwen3OmniMoeThinkerTextRotaryEmbedding):\n     pass\n \n \n@@ -2312,7 +2322,7 @@ def _get_talker_user_parts(\n     ):\n         user_talker_part = torch.empty(\n             (1, segment_end_index - im_start_index, self.config.talker_config.text_config.hidden_size),\n-            device=self.talker.device,\n+            device=thinker_hidden.device,\n             dtype=self.talker.dtype,\n         )\n \n@@ -2321,18 +2331,18 @@ def _get_talker_user_parts(\n         # Multimodal data exists\n         if user_mm_mask.any():\n             user_thinker_hidden_mm = thinker_hidden[:, im_start_index:segment_end_index][user_mm_mask]\n-            mm_hidden = self.talker.hidden_projection(user_thinker_hidden_mm).to(self.talker.device)\n+            mm_hidden = self.talker.hidden_projection(user_thinker_hidden_mm).to(thinker_hidden.device)\n             user_talker_part[user_mm_mask] = mm_hidden\n         user_thinker_embed = thinker_embed[:, im_start_index:segment_end_index][~user_mm_mask]\n-        user_text_hidden = self.talker.text_projection(user_thinker_embed).to(self.talker.device)\n+        user_text_hidden = self.talker.text_projection(user_thinker_embed).to(thinker_hidden.device)\n         user_talker_part[~user_mm_mask] = user_text_hidden\n         return user_talker_part\n \n     def _get_talker_assistant_parts(\n         self, im_start_index, segment_end_index, speaker_id, thinker_embed, tts_pad_embed, tts_bos_embed, tts_eos_embed\n     ):\n         assistant_hidden = self.talker.text_projection(thinker_embed[:, im_start_index:segment_end_index]).to(\n-            self.talker.device\n+            tts_pad_embed.device\n         )  # [1 t d]\n         assistant_text_hidden = torch.cat(\n             (\n@@ -2354,17 +2364,17 @@ def _get_talker_assistant_parts(\n                     self.config.talker_config.codec_bos_id,\n                 ]\n             ],\n-            device=self.talker.device,\n+            device=tts_pad_embed.device,\n             dtype=torch.long,\n         )\n         assistant_codec_hidden = torch.cat(\n             (\n                 torch.zeros(\n                     (1, 3, self.config.talker_config.text_config.hidden_size),\n-                    device=self.talker.device,\n+                    device=tts_pad_embed.device,\n                     dtype=self.talker.dtype,\n                 ),\n-                self.talker.get_input_embeddings()(codec_special_tokens).to(self.talker.device),\n+                self.talker.get_input_embeddings()(codec_special_tokens).to(tts_pad_embed.device),\n             ),\n             dim=1,\n         )\n@@ -2480,31 +2490,31 @@ def generate(\n         thinker_result = self.thinker.generate(input_ids=input_ids, **thinker_kwargs)\n \n         if not generate_audio:\n-            return thinker_result, None\n+            return thinker_result\n \n         # 2. Prepare talker input\n         thinker_embed = torch.cat([hidden_states[0] for hidden_states in thinker_result.hidden_states], dim=1).to(\n-            self.talker.device\n+            input_ids.device\n         )  # [1 t d]\n         thinker_hidden = torch.cat(\n             [\n                 hidden_states[self.config.talker_config.accept_hidden_layer]\n                 for hidden_states in thinker_result.hidden_states\n             ],\n             dim=1,\n-        ).to(self.talker.device)  # [1 t d]\n+        ).to(input_ids.device)  # [1 t d]\n         im_start_indexes = torch.cat(\n             (\n                 torch.nonzero(input_ids[0] == self.config.im_start_token_id).squeeze(),\n                 torch.tensor([thinker_result.sequences.shape[-1]], device=input_ids.device, dtype=input_ids.dtype),\n             ),\n             dim=-1,\n-        ).to(self.talker.device)  # Shape [n_starts + 1]; Take batch 0 since batched inference is not supported here.\n+        )  # Shape [n_starts + 1]; Take batch 0 since batched inference is not supported here.\n         multimodal_mask = (\n             (thinker_result.sequences == self.config.thinker_config.audio_token_id) |\n             (thinker_result.sequences == self.config.thinker_config.image_token_id) |\n             (thinker_result.sequences == self.config.thinker_config.video_token_id)\n-        ).to(self.talker.device)  # [1 t] # fmt: skip\n+        ).to(input_ids.device)  # [1 t] # fmt: skip\n \n         talker_special_tokens = torch.tensor(\n             [[self.config.tts_bos_token_id, self.config.tts_eos_token_id, self.config.tts_pad_token_id]],\n@@ -2513,7 +2523,7 @@ def generate(\n         )\n         tts_bos_embed, tts_eos_embed, tts_pad_embed = (\n             self.talker.text_projection(self.thinker.get_input_embeddings()(talker_special_tokens))\n-            .to(self.talker.device)\n+            .to(input_ids.device)\n             .chunk(3, dim=1)\n         )  # 3 * [1 1 d]\n \n@@ -2552,8 +2562,8 @@ def generate(\n                 continue\n             else:\n                 raise AssertionError(\"Expect role id after <|im_start|> (assistant, user, system)\")\n-        talker_input_embed = torch.cat([embed.to(self.talker.device) for embed in talker_input_embeds], dim=1)\n-        talker_input_id = torch.cat([embed.to(self.talker.device) for embed in talker_input_ids], dim=1)\n+        talker_input_embed = torch.cat([embed.to(input_ids.device) for embed in talker_input_embeds], dim=1)\n+        talker_input_id = torch.cat([embed.to(input_ids.device) for embed in talker_input_ids], dim=1)\n         talker_result = self.talker.generate(\n             inputs_embeds=talker_input_embed,\n             trailing_text_hidden=trailing_text_hidden,\n@@ -2568,7 +2578,7 @@ def generate(\n         )\n         talker_wavs = self.code2wav.chunked_decode(talker_codes, chunk_size=300, left_context_size=25)\n \n-        return thinker_result, talker_wavs.float()\n+        return thinker_result.sequences, talker_wavs.float()\n \n \n class Qwen3OmniMoeProcessorKwargs(Qwen2_5OmniProcessorKwargs):"
      },
      {
        "filename": "tests/models/qwen3_omni_moe/test_modeling_qwen3_omni_moe.py",
        "status": "modified",
        "additions": 34,
        "deletions": 32,
        "changes": 66,
        "patch": "@@ -619,7 +619,9 @@ def test_get_rope_index_video_with_audio(self):\n @require_torch\n class Qwen2_5OmniModelIntegrationTest(unittest.TestCase):\n     def setUp(self):\n-        self.processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-Omni-7B\")\n+        self.processor = AutoProcessor.from_pretrained(\n+            \"Qwen/Qwen3-Omni-30B-A3B-Instruct\", min_pixels=28 * 28, max_pixels=56 * 56\n+        )\n         self.audio_url = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/glass-breaking-151256.mp3\"\n         self.audio_url_additional = (\n             \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/f2641_0_throatclearing.wav\"\n@@ -650,7 +652,7 @@ def tearDown(self):\n     @slow\n     def test_small_model_integration_test(self):\n         model = Qwen3OmniMoeForConditionalGeneration.from_pretrained(\n-            \"Qwen/Qwen2.5-Omni-7B\", dtype=torch.bfloat16, device_map=\"auto\"\n+            \"Qwen/Qwen3-Omni-30B-A3B-Instruct\", dtype=torch.bfloat16, device_map=\"auto\"\n         )\n \n         text = self.processor.apply_chat_template(self.messages, tokenize=False, add_generation_prompt=True)\n@@ -660,35 +662,35 @@ def test_small_model_integration_test(self):\n \n         expected_input_ids = torch.tensor(\n             [\n-                151644,\n-                8948,\n-                198,\n-                2610,\n-                525,\n-                264,\n-                10950,\n-                17847,\n-                13,\n-                151645,\n-                198,\n                 151644,\n                 872,\n                 198,\n-                151647,\n-                151646,\n-                151646,\n+                151669,\n+                151675,\n+                151675,\n+                151675,\n+                151675,\n+                151675,\n+                151675,\n+                151675,\n+                151675,\n+                151675,\n+                151675,\n+                151675,\n+                151675,\n+                151675,\n             ]\n         )\n-        assert torch.allclose(expected_input_ids, inputs.input_ids[0][:17], atol=3e-3)\n+        torch.allclose(expected_input_ids, inputs.input_ids[0][:17], atol=3e-3)\n \n         expected_pixel_slice = torch.tensor(\n             [\n-                [0.8792, 0.8792, 0.9084],\n-                [1.1858, 1.1858, 1.2296],\n-                [1.2004, 1.2004, 1.2150],\n-                [1.4340, 1.4340, 1.4194],\n-                [1.3902, 1.4048, 1.4194],\n-                [1.5216, 1.5362, 1.5362],\n+                [0.5234, 0.6016, 0.6562],\n+                [0.9297, 0.9375, 0.9453],\n+                [0.4902, 0.5078, 0.4902],\n+                [0.8438, 0.8438, 0.8359],\n+                [0.9688, 0.9688, 0.9688],\n+                [0.9609, 0.9531, 0.9531],\n             ],\n             dtype=torch.bfloat16,\n             device=\"cpu\",\n@@ -703,7 +705,7 @@ def test_small_model_integration_test(self):\n         )\n \n         EXPECTED_DECODED_TEXT = Expectations({\n-            (\"cuda\", (8, 6)): \"system\\nYou are a helpful assistant.\\nuser\\nWhat's that sound and what kind of dog is this?\\nassistant\\nThe sound is glass shattering, and the dog is a Labrador Retriever.\",\n+            (\"cuda\", (8, 6)): \"user\\nWhat's that sound and what kind of dog is this?\\nassistant\\nBased on the audio and visual information, here is a breakdown of what you're hearing and seeing:-\",\n             (\"rocm\", (9, 4)): \"system\\nYou are a helpful assistant.\\nuser\\nWhat's that sound and what kind of dog is this?\\nassistant\\nThe sound is glass shattering, and the dog is a Labrador Retriever.\",\n         }).get_expectation()  # fmt: skip\n \n@@ -713,7 +715,7 @@ def test_small_model_integration_test(self):\n     @slow\n     def test_small_model_integration_test_batch(self):\n         model = Qwen3OmniMoeForConditionalGeneration.from_pretrained(\n-            \"Qwen/Qwen2.5-Omni-7B\", dtype=torch.bfloat16, device_map=\"auto\"\n+            \"Qwen/Qwen3-Omni-30B-A3B-Instruct\", dtype=torch.bfloat16, device_map=\"auto\"\n         )\n         text = self.processor.apply_chat_template(self.messages, tokenize=False, add_generation_prompt=True)\n         inputs = self.processor(\n@@ -735,8 +737,8 @@ def test_small_model_integration_test_batch(self):\n                     \"system\\nYou are a helpful assistant.\\nuser\\nWhat's that sound and what kind of dog is this?\\nassistant\\nThe sound is of glass shattering, and the dog in the picture is a Labrador Retriever\",\n                 ],\n                 (\"cuda\", 8): [\n-                    \"system\\nYou are a helpful assistant.\\nuser\\nWhat's that sound and what kind of dog is this?\\nassistant\\nThe sound is glass shattering, and the dog is a Labrador Retriever.\",\n-                    \"system\\nYou are a helpful assistant.\\nuser\\nWhat's that sound and what kind of dog is this?\\nassistant\\nThe sound is glass shattering, and the dog is a Labrador Retriever.\",\n+                    \"user\\nWhat's that sound and what kind of dog is this?\\nassistant\\nBased on the audio and visual information, here is a breakdown of what you're hearing and seeing:\\n\\n\",\n+                    \"user\\nWhat's that sound and what kind of dog is this?\\nassistant\\nBased on the audio and visual information, here is a breakdown of what you're hearing and seeing:\\n\\n\"\n                 ],\n                 (\"rocm\", (9, 4)): [\n                     \"system\\nYou are a helpful assistant.\\nuser\\nWhat's that sound and what kind of dog is this?\\nassistant\\nThe sound is glass shattering, and the dog is a Labrador Retriever.\",\n@@ -751,7 +753,7 @@ def test_small_model_integration_test_batch(self):\n     @slow\n     def test_small_model_integration_test_multiturn(self):\n         model = Qwen3OmniMoeForConditionalGeneration.from_pretrained(\n-            \"Qwen/Qwen2.5-Omni-7B\", dtype=torch.bfloat16, device_map=\"auto\"\n+            \"Qwen/Qwen3-Omni-30B-A3B-Instruct\", dtype=torch.bfloat16, device_map=\"auto\"\n         )\n \n         messages = [\n@@ -787,7 +789,7 @@ def test_small_model_integration_test_multiturn(self):\n             **inputs, thinker_temperature=0, thinker_do_sample=False, return_audio=False, thinker_max_new_tokens=20\n         )\n \n-        EXPECTED_DECODED_TEXT = \"system\\nYou are a helpful assistant.\\nuser\\nWhat's that sound and what kind of dog is this?\\nassistant\\nThe sound is glass shattering, and the dog appears to be a Labrador Retriever.\\nuser\\nHow about this one?\\nassistant\\nThe sound is a cough.\"\n+        EXPECTED_DECODED_TEXT = \"user\\nWhat's that sound and what kind of dog is this?\\nassistant\\nThe sound is glass shattering, and the dog appears to be a Labrador Retriever.\\nuser\\nHow about this one?\\nassistant\\nThe sound is a person coughing.\"\n \n         self.assertEqual(\n             self.processor.decode(output[0], skip_special_tokens=True),\n@@ -797,7 +799,7 @@ def test_small_model_integration_test_multiturn(self):\n     @slow\n     def test_small_model_integration_test_w_audio(self):\n         model = Qwen3OmniMoeForConditionalGeneration.from_pretrained(\n-            \"Qwen/Qwen2.5-Omni-7B\", dtype=torch.bfloat16, device_map=\"auto\"\n+            \"Qwen/Qwen3-Omni-30B-A3B-Instruct\", dtype=torch.bfloat16, device_map=\"auto\"\n         )\n         audio_url = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/guess_age_gender.wav\"\n \n@@ -834,7 +836,7 @@ def test_small_model_integration_test_w_audio(self):\n         EXPECTED_DECODED_TEXTS = Expectations(\n             {\n                 (\"cuda\", 7): \"system\\nYou are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\\nuser\\n\\nassistant\\nWell, I can try. But it's not always that accurate. I might be able to make\",\n-                (\"cuda\", 8): \"system\\nYou are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\\nuser\\n\\nassistant\\nWell, I can't really guess your age and gender just from your voice. There are so many\",\n+                (\"cuda\", 8): \"'system\\nYou are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\\nuser\\n\\nassistant\\nYes, I can analyze audio inputs to understand spoken content, and I can also make inferences about'\",\n             }\n         )  # fmt: skip\n         EXPECTED_DECODED_TEXT = EXPECTED_DECODED_TEXTS.get_expectation()\n@@ -850,7 +852,7 @@ def test_small_model_integration_test_w_audio(self):\n     @require_torch_gpu\n     def test_small_model_integration_test_batch_flashatt2(self):\n         model = Qwen3OmniMoeForConditionalGeneration.from_pretrained(\n-            \"Qwen/Qwen2.5-Omni-7B\",\n+            \"Qwen/Qwen3-Omni-30B-A3B-Instruct\",\n             dtype=torch.bfloat16,\n             attn_implementation=\"flash_attention_2\",\n             device_map=\"auto\","
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T21:16:57.130942",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR addresses a non-trivial bug in RoPE (Rotary Position Embeddings) configuration for the Qwen3-Omni model by adding missing validation calls and refactoring rope parameter handling. The changes involve architectural decisions about how RoPE parameters are standardized and validated across multiple config classes, with sufficient context to generate substantive questions about the codebase.",
      "substance_level": "medium"
    }
  },
  {
    "pr_number": 41758,
    "title": "Fixed incorrect model_type for qwen2vl and qwen2.5vl when config is saved and loaded again",
    "body": "# What does this PR do?\r\nFixes the issue where if you save the config and load it again it would return the incorrect model_type . \r\nMinor fix in __getattribute__ method of the config class for both models .\r\n\r\n\r\nFixes # 41746\r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),\r\n      Pull Request section?\r\n- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.  - https://github.com/huggingface/transformers/issues/41746\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n@zucchini-nlp \r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n",
    "html_url": "https://github.com/huggingface/transformers/pull/41758",
    "created_at": "2025-10-21T08:27:38Z",
    "merged_at": "2025-10-21T10:54:58Z",
    "merge_commit_sha": "ede7976cd2462ce868a0058c339c6b21baf7fc04",
    "base_ref": "main",
    "head_sha": "4ac562c79c686cac933ba899b4d04071fdd8ebc8",
    "user": "i3hz",
    "files": [
      {
        "filename": "src/transformers/models/qwen2_5_vl/configuration_qwen2_5_vl.py",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -313,6 +313,8 @@ def __setattr__(self, key, value):\n \n     def __getattribute__(self, key):\n         if \"text_config\" in super().__getattribute__(\"__dict__\") and key not in [\n+            \"_name_or_path\",\n+            \"model_type\",\n             \"dtype\",\n             \"_attn_implementation_internal\",\n         ]:"
      },
      {
        "filename": "src/transformers/models/qwen2_vl/configuration_qwen2_vl.py",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -301,6 +301,8 @@ def __setattr__(self, key, value):\n \n     def __getattribute__(self, key):\n         if \"text_config\" in super().__getattribute__(\"__dict__\") and key not in [\n+            \"_name_or_path\",\n+            \"model_type\",\n             \"dtype\",\n             \"_attn_implementation_internal\",\n         ]:"
      },
      {
        "filename": "tests/models/qwen2_5_vl/test_modeling_qwen2_5_vl.py",
        "status": "modified",
        "additions": 11,
        "deletions": 0,
        "changes": 11,
        "patch": "@@ -235,6 +235,17 @@ def test_text_config(self):\n         self.assertEqual(base_config.patch_size, 8)\n         self.assertNotEqual(base_config.vision_config.patch_size, 8)\n \n+        # Test for making sure config save and load preserves correct model type\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        self.assertEqual(config.model_type, \"qwen2_5_vl\")\n+\n+        with tempfile.TemporaryDirectory() as tmp_dir:\n+            config.save_pretrained(tmp_dir)\n+\n+            loaded_config = Qwen2_5_VLConfig.from_pretrained(tmp_dir)\n+            self.assertEqual(loaded_config.model_type, \"qwen2_5_vl\")\n+\n     def test_mismatching_num_image_tokens(self):\n         \"\"\"\n         Tests that VLMs through an error with explicit message saying what is wrong"
      },
      {
        "filename": "tests/models/qwen2_vl/test_modeling_qwen2_vl.py",
        "status": "modified",
        "additions": 11,
        "deletions": 0,
        "changes": 11,
        "patch": "@@ -215,6 +215,17 @@ def test_text_config(self):\n         self.assertEqual(base_config.patch_size, 8)\n         self.assertNotEqual(base_config.vision_config.patch_size, 8)\n \n+        # Test for making sure config save and load preserves correct model type\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        self.assertEqual(config.model_type, \"qwen2_vl\")\n+\n+        with tempfile.TemporaryDirectory() as tmp_dir:\n+            config.save_pretrained(tmp_dir)\n+\n+            loaded_config = Qwen2VLConfig.from_pretrained(tmp_dir)\n+            self.assertEqual(loaded_config.model_type, \"qwen2_vl\")\n+\n     def test_mismatching_num_image_tokens(self):\n         \"\"\"\n         Tests that VLMs through an error with explicit message saying what is wrong"
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T21:17:00.797535",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR fixes a non-trivial bug in the `__getattribute__` method of configuration classes that affects how model_type and _name_or_path are preserved during config serialization/deserialization. The fix addresses a real issue (referenced in #41746), includes proper test coverage demonstrating the problem, and involves understanding how Python's attribute access mechanism interacts with config loading\u2014enough substance to generate meaningful questions about the codebase's configuration system.",
      "substance_level": "medium"
    }
  },
  {
    "pr_number": 41750,
    "title": ":rotating_light: [`Clip`] Fix masking and enable flash attention on all model types",
    "body": "Clip used old mask APIs leading to a confused usage:\r\n- A causal mask (normal triu mask)\r\n- A padding mask (encoder mask == only accounting for padding)\r\n- Add both of above == final mask --> causal mask with padding\r\n\r\n^ works only for interfaces with support for 4D masks which disabled FA usage in general.\r\n\r\nThis PR now correctly changes this to the new API which handles padding automatically. We have to additionally pass the `is_causal` kwarg to dynamically switch between modality types (text == causal, image == full). This is only enabled through recent PRs (fa #39707, sdpa #41692).\r\n\r\nCloses #41673\r\nFixes #41668",
    "html_url": "https://github.com/huggingface/transformers/pull/41750",
    "created_at": "2025-10-20T14:53:33Z",
    "merged_at": "2025-10-24T18:44:10Z",
    "merge_commit_sha": "7a833d1ccd41673030c85107f65f454c0c3222f5",
    "base_ref": "main",
    "head_sha": "eccf8c7793e5302c8d23ccb133d5d933d1e35a16",
    "user": "vasqu",
    "files": [
      {
        "filename": "src/transformers/models/clip/modeling_clip.py",
        "status": "modified",
        "additions": 14,
        "deletions": 42,
        "changes": 56,
        "patch": "@@ -22,7 +22,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...modeling_attn_mask_utils import _create_4d_causal_attention_mask, _prepare_4d_attention_mask\n+from ...masking_utils import create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling, ImageClassifierOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n@@ -310,7 +310,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n-        causal_attention_mask: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n@@ -324,15 +323,6 @@ def forward(\n         queries = queries.view(batch_size, seq_length, -1, self.head_dim).transpose(1, 2)\n         keys = keys.view(batch_size, seq_length, -1, self.head_dim).transpose(1, 2)\n         values = values.view(batch_size, seq_length, -1, self.head_dim).transpose(1, 2)\n-        # CLIP text model uses both `causal_attention_mask` and `attention_mask`\n-        # in case FA2 kernel is called, `is_causal` should be inferred from `causal_attention_mask`\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            self.is_causal = causal_attention_mask is not None\n-        else:\n-            if attention_mask is not None and causal_attention_mask is not None:\n-                attention_mask = attention_mask + causal_attention_mask\n-            elif causal_attention_mask is not None:\n-                attention_mask = causal_attention_mask\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -344,13 +334,12 @@ def forward(\n             keys,\n             values,\n             attention_mask,\n-            is_causal=self.is_causal,\n             scaling=self.scale,\n             dropout=0.0 if not self.training else self.dropout,\n             **kwargs,\n         )\n \n-        attn_output = attn_output.reshape(batch_size, seq_length, embed_dim).contiguous()\n+        attn_output = attn_output.reshape(batch_size, seq_length, -1).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n         return attn_output, attn_weights\n@@ -384,16 +373,14 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: torch.Tensor,\n-        causal_attention_mask: torch.Tensor,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> torch.FloatTensor:\n         residual = hidden_states\n \n         hidden_states = self.layer_norm1(hidden_states)\n-        hidden_states, attn_weights = self.self_attn(\n+        hidden_states, _ = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n-            causal_attention_mask=causal_attention_mask,\n             **kwargs,\n         )\n         hidden_states = residual + hidden_states\n@@ -497,7 +484,6 @@ def forward(\n         self,\n         inputs_embeds,\n         attention_mask: Optional[torch.Tensor] = None,\n-        causal_attention_mask: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutput:\n         r\"\"\"\n@@ -512,21 +498,13 @@ def forward(\n                 - 1 for tokens that are **not masked**,\n                 - 0 for tokens that are **masked**.\n \n-                [What are attention masks?](../glossary#attention-mask)\n-            causal_attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Causal mask for the text model. Mask values selected in `[0, 1]`:\n-\n-                - 1 for tokens that are **not masked**,\n-                - 0 for tokens that are **masked**.\n-\n                 [What are attention masks?](../glossary#attention-mask)\n         \"\"\"\n         hidden_states = inputs_embeds\n         for encoder_layer in self.layers:\n             hidden_states = encoder_layer(\n                 hidden_states,\n                 attention_mask,\n-                causal_attention_mask,\n                 **kwargs,\n             )\n \n@@ -563,17 +541,19 @@ def forward(\n \n         hidden_states = self.embeddings(input_ids=input_ids, position_ids=position_ids)\n \n-        causal_attention_mask = _create_4d_causal_attention_mask(\n-            input_shape, hidden_states.dtype, device=hidden_states.device\n+        attention_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=hidden_states,\n+            attention_mask=attention_mask,\n+            cache_position=torch.arange(hidden_states.shape[1], device=hidden_states.device),\n+            past_key_values=None,\n         )\n \n-        if attention_mask is not None and self.config._attn_implementation != \"flash_attention_2\":\n-            attention_mask = _prepare_4d_attention_mask(attention_mask, hidden_states.dtype)\n-\n+        kwargs.pop(\"is_causal\", None)\n         encoder_outputs: BaseModelOutput = self.encoder(\n             inputs_embeds=hidden_states,\n             attention_mask=attention_mask,\n-            causal_attention_mask=causal_attention_mask,\n+            is_causal=True,\n             **kwargs,\n         )\n \n@@ -618,7 +598,6 @@ class CLIPTextModel(CLIPPreTrainedModel):\n     input_modalities = \"text\"\n \n     _no_split_modules = [\"CLIPTextEmbeddings\", \"CLIPEncoderLayer\"]\n-    _supports_flash_attn = False  # mask creation only accounts for sdpa/eager\n \n     def __init__(self, config: CLIPTextConfig):\n         super().__init__(config)\n@@ -632,8 +611,7 @@ def get_input_embeddings(self) -> nn.Module:\n     def set_input_embeddings(self, value):\n         self.text_model.embeddings.token_embedding = value\n \n-    @check_model_inputs()\n-    @can_return_tuple\n+    @check_model_inputs(tie_last_hidden_states=False)\n     @auto_docstring\n     def forward(\n         self,\n@@ -726,7 +704,6 @@ def get_input_embeddings(self) -> nn.Module:\n         return self.vision_model.embeddings.patch_embedding\n \n     @check_model_inputs(tie_last_hidden_states=False)\n-    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -766,7 +743,6 @@ def forward(\n class CLIPModel(CLIPPreTrainedModel):\n     config: CLIPConfig\n     _no_split_modules = [\"CLIPTextEmbeddings\", \"CLIPEncoderLayer\", \"CLIPVisionEmbeddings\"]\n-    _supports_flash_attn = False  # mask creation only accounts for sdpa/eager\n \n     def __init__(self, config: CLIPConfig):\n         super().__init__(config)\n@@ -966,7 +942,6 @@ class CLIPTextModelWithProjection(CLIPPreTrainedModel):\n     config: CLIPTextConfig\n     input_modalities = \"text\"\n \n-    _supports_flash_attn = False\n     _no_split_modules = [\"CLIPTextEmbeddings\", \"CLIPEncoderLayer\"]\n \n     def __init__(self, config: CLIPTextConfig):\n@@ -986,8 +961,7 @@ def get_input_embeddings(self) -> nn.Module:\n     def set_input_embeddings(self, value):\n         self.text_model.embeddings.token_embedding = value\n \n-    @check_model_inputs()\n-    @can_return_tuple\n+    @check_model_inputs(tie_last_hidden_states=False)\n     @auto_docstring\n     def forward(\n         self,\n@@ -1049,7 +1023,6 @@ def get_input_embeddings(self) -> nn.Module:\n         return self.vision_model.embeddings.patch_embedding\n \n     @check_model_inputs(tie_last_hidden_states=False)\n-    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -1117,8 +1090,7 @@ def __init__(self, config: CLIPConfig) -> None:\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @check_model_inputs()\n-    @can_return_tuple\n+    @check_model_inputs(tie_last_hidden_states=False)\n     @auto_docstring\n     def forward(\n         self,"
      },
      {
        "filename": "src/transformers/models/metaclip_2/modeling_metaclip_2.py",
        "status": "modified",
        "additions": 15,
        "deletions": 52,
        "changes": 67,
        "patch": "@@ -12,7 +12,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...modeling_attn_mask_utils import _create_4d_causal_attention_mask, _prepare_4d_attention_mask\n+from ...masking_utils import create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling, ImageClassifierOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n@@ -200,7 +200,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n-        causal_attention_mask: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n@@ -214,15 +213,6 @@ def forward(\n         queries = queries.view(batch_size, seq_length, -1, self.head_dim).transpose(1, 2)\n         keys = keys.view(batch_size, seq_length, -1, self.head_dim).transpose(1, 2)\n         values = values.view(batch_size, seq_length, -1, self.head_dim).transpose(1, 2)\n-        # METACLIP_2 text model uses both `causal_attention_mask` and `attention_mask`\n-        # in case FA2 kernel is called, `is_causal` should be inferred from `causal_attention_mask`\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            self.is_causal = causal_attention_mask is not None\n-        else:\n-            if attention_mask is not None and causal_attention_mask is not None:\n-                attention_mask = attention_mask + causal_attention_mask\n-            elif causal_attention_mask is not None:\n-                attention_mask = causal_attention_mask\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -234,13 +224,12 @@ def forward(\n             keys,\n             values,\n             attention_mask,\n-            is_causal=self.is_causal,\n             scaling=self.scale,\n             dropout=0.0 if not self.training else self.dropout,\n             **kwargs,\n         )\n \n-        attn_output = attn_output.reshape(batch_size, seq_length, embed_dim).contiguous()\n+        attn_output = attn_output.reshape(batch_size, seq_length, -1).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n         return attn_output, attn_weights\n@@ -274,16 +263,14 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: torch.Tensor,\n-        causal_attention_mask: torch.Tensor,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> torch.FloatTensor:\n         residual = hidden_states\n \n         hidden_states = self.layer_norm1(hidden_states)\n-        hidden_states, attn_weights = self.self_attn(\n+        hidden_states, _ = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n-            causal_attention_mask=causal_attention_mask,\n             **kwargs,\n         )\n         hidden_states = residual + hidden_states\n@@ -387,7 +374,6 @@ def forward(\n         self,\n         inputs_embeds,\n         attention_mask: Optional[torch.Tensor] = None,\n-        causal_attention_mask: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutput:\n         r\"\"\"\n@@ -402,21 +388,13 @@ def forward(\n                 - 1 for tokens that are **not masked**,\n                 - 0 for tokens that are **masked**.\n \n-                [What are attention masks?](../glossary#attention-mask)\n-            causal_attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Causal mask for the text model. Mask values selected in `[0, 1]`:\n-\n-                - 1 for tokens that are **not masked**,\n-                - 0 for tokens that are **masked**.\n-\n                 [What are attention masks?](../glossary#attention-mask)\n         \"\"\"\n         hidden_states = inputs_embeds\n         for encoder_layer in self.layers:\n             hidden_states = encoder_layer(\n                 hidden_states,\n                 attention_mask,\n-                causal_attention_mask,\n                 **kwargs,\n             )\n \n@@ -437,36 +415,32 @@ def __init__(self, config: MetaClip2TextConfig):\n         # For `pooled_output` computation\n         self.eos_token_id = config.eos_token_id\n \n-    @check_model_inputs(tie_last_hidden_states=False)\n     @auto_docstring\n     def forward(\n         self,\n         input_ids,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        use_cache: Optional[bool] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutputWithPooling:\n         input_shape = input_ids.size()\n         input_ids = input_ids.view(-1, input_shape[-1])\n \n         hidden_states = self.embeddings(input_ids=input_ids, position_ids=position_ids)\n \n-        # CLIP's text model uses causal mask, prepare it here.\n-        # https://github.com/openai/CLIP/blob/cfcffb90e69f37bf2ff1e988237a0fbe41f33c04/clip/model.py#L324\n-        causal_attention_mask = _create_4d_causal_attention_mask(\n-            input_shape, hidden_states.dtype, device=hidden_states.device\n+        attention_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=hidden_states,\n+            attention_mask=attention_mask,\n+            cache_position=torch.arange(hidden_states.shape[1], device=hidden_states.device),\n+            past_key_values=None,\n         )\n \n-        # expand attention_mask\n-        if attention_mask is not None and self.config._attn_implementation != \"flash_attention_2\":\n-            # [batch_size, seq_len] -> [batch_size, 1, tgt_seq_len, src_seq_len]\n-            attention_mask = _prepare_4d_attention_mask(attention_mask, hidden_states.dtype)\n-\n+        kwargs.pop(\"is_causal\", None)\n         encoder_outputs: BaseModelOutput = self.encoder(\n             inputs_embeds=hidden_states,\n             attention_mask=attention_mask,\n-            causal_attention_mask=causal_attention_mask,\n+            is_causal=True,\n             **kwargs,\n         )\n \n@@ -527,7 +501,6 @@ class MetaClip2TextModel(MetaClip2PreTrainedModel):\n     input_modalities = \"text\"\n \n     _no_split_modules = [\"MetaClip2TextEmbeddings\", \"MetaClip2EncoderLayer\"]\n-    _supports_flash_attn = False  # mask creation only accounts for sdpa/eager\n \n     def __init__(self, config: MetaClip2TextConfig):\n         super().__init__(config)\n@@ -541,16 +514,13 @@ def get_input_embeddings(self) -> nn.Module:\n     def set_input_embeddings(self, value):\n         self.text_model.embeddings.token_embedding = value\n \n-    @check_model_inputs()\n-    @can_return_tuple\n+    @check_model_inputs(tie_last_hidden_states=False)\n     @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutputWithPooling:\n         r\"\"\"\n@@ -630,7 +600,6 @@ class MetaClip2TextModelWithProjection(MetaClip2PreTrainedModel):\n     config: MetaClip2TextConfig\n     input_modalities = \"text\"\n \n-    _supports_flash_attn = False\n     _no_split_modules = [\"MetaClip2TextEmbeddings\", \"MetaClip2EncoderLayer\"]\n \n     def __init__(self, config: MetaClip2TextConfig):\n@@ -650,16 +619,13 @@ def get_input_embeddings(self) -> nn.Module:\n     def set_input_embeddings(self, value):\n         self.text_model.embeddings.token_embedding = value\n \n-    @check_model_inputs()\n-    @can_return_tuple\n+    @check_model_inputs(tie_last_hidden_states=False)\n     @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> MetaClip2TextModelOutput:\n         r\"\"\"\n@@ -792,7 +758,6 @@ class MetaClip2Model(MetaClip2PreTrainedModel):\n \n     config: MetaClip2Config\n     _no_split_modules = [\"MetaClip2TextEmbeddings\", \"MetaClip2EncoderLayer\", \"MetaClip2VisionEmbeddings\"]\n-    _supports_flash_attn = False  # mask creation only accounts for sdpa/eager\n \n     def __init__(self, config: MetaClip2Config):\n         super().__init__(config)\n@@ -1078,7 +1043,7 @@ def get_input_embeddings(self) -> nn.Module:\n         return self.vision_model.embeddings.patch_embedding\n \n     @check_model_inputs(tie_last_hidden_states=False)\n-    @can_return_tuple\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n@@ -1187,7 +1152,6 @@ def get_input_embeddings(self) -> nn.Module:\n         return self.vision_model.embeddings.patch_embedding\n \n     @check_model_inputs(tie_last_hidden_states=False)\n-    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -1254,8 +1218,7 @@ def __init__(self, config: MetaClip2Config) -> None:\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @check_model_inputs()\n-    @can_return_tuple\n+    @check_model_inputs(tie_last_hidden_states=False)\n     @auto_docstring\n     def forward(\n         self,"
      },
      {
        "filename": "src/transformers/models/metaclip_2/modular_metaclip_2.py",
        "status": "modified",
        "additions": 19,
        "deletions": 59,
        "changes": 78,
        "patch": "@@ -3,19 +3,18 @@\n import torch\n from torch import nn\n \n-from ...modeling_attn_mask_utils import _create_4d_causal_attention_mask, _prepare_4d_attention_mask\n+from ...masking_utils import create_causal_mask\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n-from ...modeling_utils import PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n from ...utils.generic import check_model_inputs\n from ..clip.configuration_clip import CLIPConfig, CLIPTextConfig, CLIPVisionConfig\n from ..clip.modeling_clip import (\n     CLIPMLP,\n     CLIPAttention,\n-    CLIPEncoderLayer,\n     CLIPForImageClassification,\n     CLIPModel,\n+    CLIPPreTrainedModel,\n     CLIPTextEmbeddings,\n     CLIPTextModel,\n     CLIPTextModelWithProjection,\n@@ -214,24 +213,9 @@ class MetaClip2MLP(CLIPMLP):\n     pass\n \n \n-class MetaClip2EncoderLayer(CLIPEncoderLayer):\n-    pass\n-\n-\n @auto_docstring\n-class MetaClip2PreTrainedModel(PreTrainedModel):\n-    config: MetaClip2Config\n+class MetaClip2PreTrainedModel(CLIPPreTrainedModel):\n     base_model_prefix = \"metaclip_2\"\n-    input_modalities = [\"image\", \"text\"]\n-    supports_gradient_checkpointing = True\n-    _supports_sdpa = True\n-    _supports_flash_attn = True\n-    _supports_flex_attn = True\n-    _supports_attention_backend = True\n-    _can_record_outputs = {\n-        \"hidden_states\": MetaClip2EncoderLayer,\n-        \"attentions\": MetaClip2Attention,\n-    }\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n@@ -291,36 +275,32 @@ def _init_weights(self, module):\n \n \n class MetaClip2TextTransformer(CLIPTextTransformer):\n-    @check_model_inputs(tie_last_hidden_states=False)\n     @auto_docstring\n     def forward(\n         self,\n         input_ids,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        use_cache: Optional[bool] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutputWithPooling:\n         input_shape = input_ids.size()\n         input_ids = input_ids.view(-1, input_shape[-1])\n \n         hidden_states = self.embeddings(input_ids=input_ids, position_ids=position_ids)\n \n-        # CLIP's text model uses causal mask, prepare it here.\n-        # https://github.com/openai/CLIP/blob/cfcffb90e69f37bf2ff1e988237a0fbe41f33c04/clip/model.py#L324\n-        causal_attention_mask = _create_4d_causal_attention_mask(\n-            input_shape, hidden_states.dtype, device=hidden_states.device\n+        attention_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=hidden_states,\n+            attention_mask=attention_mask,\n+            cache_position=torch.arange(hidden_states.shape[1], device=hidden_states.device),\n+            past_key_values=None,\n         )\n \n-        # expand attention_mask\n-        if attention_mask is not None and self.config._attn_implementation != \"flash_attention_2\":\n-            # [batch_size, seq_len] -> [batch_size, 1, tgt_seq_len, src_seq_len]\n-            attention_mask = _prepare_4d_attention_mask(attention_mask, hidden_states.dtype)\n-\n+        kwargs.pop(\"is_causal\", None)\n         encoder_outputs: BaseModelOutput = self.encoder(\n             inputs_embeds=hidden_states,\n             attention_mask=attention_mask,\n-            causal_attention_mask=causal_attention_mask,\n+            is_causal=True,\n             **kwargs,\n         )\n \n@@ -372,22 +352,13 @@ class MetaClip2TextModel(CLIPTextModel):\n     >>> pooled_output = outputs.pooler_output  # pooled (EOS token) states\n     ```\"\"\"\n \n-    def __init__(self, config: MetaClip2TextConfig):\n-        super().__init__(config)\n-        self.text_model = MetaClip2TextTransformer(config)\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    @check_model_inputs()\n-    @can_return_tuple\n+    @check_model_inputs(tie_last_hidden_states=False)\n     @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ):\n         r\"\"\"\n@@ -409,8 +380,6 @@ def forward(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n             **kwargs,\n         )\n \n@@ -446,24 +415,13 @@ class MetaClip2TextModelWithProjection(CLIPTextModelWithProjection):\n     >>> text_embeds = outputs.text_embeds\n     ```\"\"\"\n \n-    def __init__(self, config: MetaClip2TextConfig):\n-        super().__init__(config)\n-\n-        text_model = MetaClip2TextModel._from_config(config)\n-        self.text_model = text_model.text_model\n-\n-        self.text_projection = nn.Linear(config.hidden_size, config.projection_dim, bias=False)\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n+    @check_model_inputs(tie_last_hidden_states=False)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ):\n         r\"\"\"\n@@ -484,8 +442,6 @@ def forward(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n             **kwargs,\n         )\n \n@@ -550,6 +506,8 @@ def __init__(self, config: MetaClip2Config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @can_return_tuple\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -694,7 +652,7 @@ class MetaClip2VisionModel(CLIPVisionModel):\n     ```\"\"\"\n \n     @check_model_inputs(tie_last_hidden_states=False)\n-    @can_return_tuple\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n@@ -764,6 +722,8 @@ class MetaClip2VisionModelWithProjection(CLIPVisionModelWithProjection):\n     >>> image_embeds = outputs.image_embeds\n     ```\"\"\"\n \n+    @check_model_inputs(tie_last_hidden_states=False)\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,"
      },
      {
        "filename": "src/transformers/models/mlcd/modeling_mlcd.py",
        "status": "modified",
        "additions": 65,
        "deletions": 124,
        "changes": 189,
        "patch": "@@ -25,12 +25,12 @@\n import torch.nn as nn\n \n from ...activations import ACT2FN\n-from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, torch_int\n+from ...utils.generic import check_model_inputs\n from .configuration_mlcd import MLCDVisionConfig\n \n \n@@ -259,7 +259,7 @@ def forward(\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n-        **kwargs: Unpack[FlashAttentionKwargs],\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n         batch_size, seq_length = hidden_states.shape[:-1]\n@@ -316,7 +316,7 @@ def forward(\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = False,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.FloatTensor]:\n         \"\"\"\n         Args:\n@@ -328,18 +328,15 @@ def forward(\n                 Represents absolute positional embeddings for the query and key in the attention mechanism.\n             attention_mask (`torch.FloatTensor`):\n                 Attention mask of shape `(batch, 1, q_len, k_v_seq_len)` where padding elements are indicated by very large negative values.\n-            output_attentions (`bool`, *optional*, defaults to `False`):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n         \"\"\"\n         residual = hidden_states\n \n         hidden_states = self.layer_norm1(hidden_states)\n-        hidden_states, attn_weights = self.self_attn(\n+        hidden_states, _ = self.self_attn(\n             hidden_states=hidden_states,\n             position_embeddings=position_embeddings,\n             attention_mask=attention_mask,\n-            output_attentions=output_attentions,\n+            **kwargs,\n         )\n         hidden_states = residual + hidden_states\n \n@@ -348,12 +345,7 @@ def forward(\n         hidden_states = self.mlp(hidden_states)\n         hidden_states = residual + hidden_states\n \n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (attn_weights,)\n-\n-        return outputs\n+        return hidden_states\n \n \n class MLCDEncoder(nn.Module):\n@@ -377,9 +369,7 @@ def forward(\n         inputs_embeds: torch.FloatTensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, BaseModelOutput]:\n         r\"\"\"\n         Args:\n@@ -395,53 +385,68 @@ def forward(\n                 - 1 for tokens that are **not masked**,\n                 - 0 for tokens that are **masked**.\n                 [What are attention masks?](../glossary#attention-mask)\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-            output_hidden_states (`bool`, *optional*):\n-                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n-                for more detail.\n-            return_dict (`bool`, *optional*):\n-                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n         \"\"\"\n-\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-\n-        encoder_states = () if output_hidden_states else None\n-        all_attentions = () if output_attentions else None\n-\n         hidden_states = inputs_embeds\n-        for idx, encoder_layer in enumerate(self.layers):\n-            if output_hidden_states:\n-                encoder_states = encoder_states + (hidden_states,)\n-            layer_outputs = encoder_layer(\n-                hidden_states=hidden_states,\n-                position_embeddings=position_embeddings,\n-                attention_mask=attention_mask,\n-                output_attentions=output_attentions,\n+        for encoder_layer in self.layers:\n+            hidden_states = encoder_layer(\n+                hidden_states,\n+                position_embeddings,\n+                attention_mask,\n+                **kwargs,\n             )\n \n-            hidden_states = layer_outputs[0]\n-\n-            if output_attentions:\n-                all_attentions = all_attentions + (layer_outputs[1],)\n-\n-        if output_hidden_states:\n-            encoder_states = encoder_states + (hidden_states,)\n-\n-        if not return_dict:\n-            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n         return BaseModelOutput(\n             last_hidden_state=hidden_states,\n-            hidden_states=encoder_states,\n-            attentions=all_attentions,\n         )\n \n \n+@auto_docstring\n+class MLCDPreTrainedModel(PreTrainedModel):\n+    config: MLCDVisionConfig\n+    base_model_prefix = \"mlcd\"\n+    supports_gradient_checkpointing = True\n+    accepts_loss_kwargs = False\n+    _supports_flash_attn = True\n+    _supports_sdpa = True\n+    _supports_flex_attn = True\n+    _supports_attention_backend = True\n+    _can_record_outputs = {\n+        \"hidden_states\": MLCDEncoderLayer,\n+        \"attentions\": MLCDAttention,\n+    }\n+\n+    def _init_weights(self, module):\n+        \"\"\"Initialize the weights\"\"\"\n+        factor = self.config.initializer_factor\n+        if isinstance(module, MLCDVisionEmbeddings):\n+            factor = self.config.initializer_factor\n+            nn.init.normal_(module.class_embedding, mean=0.0, std=module.embed_dim**-0.5 * factor)\n+            nn.init.normal_(module.patch_embedding.weight, std=module.config.initializer_range * factor)\n+        elif isinstance(module, MLCDAttention):\n+            factor = self.config.initializer_factor\n+            in_proj_std = (module.embed_dim**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n+            out_proj_std = (module.embed_dim**-0.5) * factor\n+            nn.init.normal_(module.q_proj.weight, std=in_proj_std)\n+            nn.init.normal_(module.k_proj.weight, std=in_proj_std)\n+            nn.init.normal_(module.v_proj.weight, std=in_proj_std)\n+            nn.init.normal_(module.out_proj.weight, std=out_proj_std)\n+        elif isinstance(module, MLCDMLP):\n+            factor = self.config.initializer_factor\n+            in_proj_std = (module.config.hidden_size**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n+            fc_std = (2 * module.config.hidden_size) ** -0.5 * factor\n+            nn.init.normal_(module.fc1.weight, std=fc_std)\n+            nn.init.normal_(module.fc2.weight, std=in_proj_std)\n+        elif isinstance(module, MLCDVisionTransformer):\n+            factor = self.config.initializer_factor\n+            pos_emb_std = (module.config.hidden_size // module.config.num_attention_heads // 2) ** -0.5 * factor\n+            nn.init.normal_(module.class_pos_emb, mean=0.0, std=pos_emb_std)\n+        elif isinstance(module, nn.LayerNorm):\n+            module.bias.data.zero_()\n+            module.weight.data.fill_(1.0)\n+        elif isinstance(module, nn.Linear) and module.bias is not None:\n+            module.bias.data.zero_()\n+\n+\n class MLCDVisionTransformer(nn.Module):\n     def __init__(self, config: MLCDVisionConfig):\n         super().__init__()\n@@ -459,16 +464,8 @@ def __init__(self, config: MLCDVisionConfig):\n     def forward(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, BaseModelOutputWithPooling]:\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-\n         if pixel_values is None:\n             raise ValueError(\"You have to specify pixel_values\")\n \n@@ -486,66 +483,19 @@ def forward(\n         encoder_outputs = self.encoder(\n             inputs_embeds=hidden_states,\n             position_embeddings=position_embeddings,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            **kwargs,\n         )\n \n         last_hidden_state = encoder_outputs[0]\n         pooled_output = last_hidden_state[:, 0, :]\n         pooled_output = self.post_layernorm(pooled_output)\n \n-        if not return_dict:\n-            return (last_hidden_state, pooled_output) + encoder_outputs[1:]\n-\n         return BaseModelOutputWithPooling(\n             last_hidden_state=last_hidden_state,\n             pooler_output=pooled_output,\n-            hidden_states=encoder_outputs.hidden_states,\n-            attentions=encoder_outputs.attentions,\n         )\n \n \n-@auto_docstring\n-class MLCDPreTrainedModel(PreTrainedModel):\n-    config: MLCDVisionConfig\n-    base_model_prefix = \"mlcd\"\n-    supports_gradient_checkpointing = True\n-    _supports_flash_attn = True\n-    _supports_sdpa = True\n-\n-    def _init_weights(self, module):\n-        \"\"\"Initialize the weights\"\"\"\n-        factor = self.config.initializer_factor\n-        if isinstance(module, MLCDVisionEmbeddings):\n-            factor = self.config.initializer_factor\n-            nn.init.normal_(module.class_embedding, mean=0.0, std=module.embed_dim**-0.5 * factor)\n-            nn.init.normal_(module.patch_embedding.weight, std=module.config.initializer_range * factor)\n-        elif isinstance(module, MLCDAttention):\n-            factor = self.config.initializer_factor\n-            in_proj_std = (module.embed_dim**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n-            out_proj_std = (module.embed_dim**-0.5) * factor\n-            nn.init.normal_(module.q_proj.weight, std=in_proj_std)\n-            nn.init.normal_(module.k_proj.weight, std=in_proj_std)\n-            nn.init.normal_(module.v_proj.weight, std=in_proj_std)\n-            nn.init.normal_(module.out_proj.weight, std=out_proj_std)\n-        elif isinstance(module, MLCDMLP):\n-            factor = self.config.initializer_factor\n-            in_proj_std = (module.config.hidden_size**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n-            fc_std = (2 * module.config.hidden_size) ** -0.5 * factor\n-            nn.init.normal_(module.fc1.weight, std=fc_std)\n-            nn.init.normal_(module.fc2.weight, std=in_proj_std)\n-        elif isinstance(module, MLCDVisionTransformer):\n-            factor = self.config.initializer_factor\n-            pos_emb_std = (module.config.hidden_size // module.config.num_attention_heads // 2) ** -0.5 * factor\n-            nn.init.normal_(module.class_pos_emb, mean=0.0, std=pos_emb_std)\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n-        elif isinstance(module, nn.Linear) and module.bias is not None:\n-            module.bias.data.zero_()\n-\n-\n @auto_docstring(\n     custom_intro=\"\"\"\n     The vision model from M_L_C_D without any head or projection on top.\n@@ -566,13 +516,12 @@ def __init__(self, config: MLCDVisionConfig):\n     def get_input_embeddings(self) -> nn.Module:\n         return self.vision_model.embeddings.patch_embedding\n \n+    @check_model_inputs(tie_last_hidden_states=False)\n     @auto_docstring\n     def forward(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, BaseModelOutputWithPooling]:\n         r\"\"\"\n         Example:\n@@ -596,17 +545,9 @@ def forward(\n         >>> print(f\"Number of attention layers: {len(outputs.attentions)}\")\n         >>> print(f\"Attention shape: {outputs.attentions[0].shape}\")\n         ```\"\"\"\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-\n         return self.vision_model(\n             pixel_values=pixel_values,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            **kwargs,\n         )\n \n "
      },
      {
        "filename": "src/transformers/models/mlcd/modular_mlcd.py",
        "status": "modified",
        "additions": 66,
        "deletions": 125,
        "changes": 191,
        "patch": "@@ -19,11 +19,11 @@\n import torch.nn as nn\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import auto_docstring, logging\n+from ...utils import TransformersKwargs, auto_docstring, logging\n+from ...utils.generic import check_model_inputs\n from ..clip.modeling_clip import (\n     CLIPMLP,\n     CLIPAttention,\n@@ -206,7 +206,7 @@ def forward(\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n-        **kwargs: Unpack[FlashAttentionKwargs],\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         batch_size, seq_length = hidden_states.shape[:-1]\n \n@@ -258,7 +258,7 @@ def forward(\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = False,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.FloatTensor]:\n         \"\"\"\n         Args:\n@@ -270,18 +270,15 @@ def forward(\n                 Represents absolute positional embeddings for the query and key in the attention mechanism.\n             attention_mask (`torch.FloatTensor`):\n                 Attention mask of shape `(batch, 1, q_len, k_v_seq_len)` where padding elements are indicated by very large negative values.\n-            output_attentions (`bool`, *optional*, defaults to `False`):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n         \"\"\"\n         residual = hidden_states\n \n         hidden_states = self.layer_norm1(hidden_states)\n-        hidden_states, attn_weights = self.self_attn(\n+        hidden_states, _ = self.self_attn(\n             hidden_states=hidden_states,\n             position_embeddings=position_embeddings,\n             attention_mask=attention_mask,\n-            output_attentions=output_attentions,\n+            **kwargs,\n         )\n         hidden_states = residual + hidden_states\n \n@@ -290,12 +287,7 @@ def forward(\n         hidden_states = self.mlp(hidden_states)\n         hidden_states = residual + hidden_states\n \n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (attn_weights,)\n-\n-        return outputs\n+        return hidden_states\n \n \n class MLCDEncoder(CLIPEncoder):\n@@ -316,9 +308,7 @@ def forward(\n         inputs_embeds: torch.FloatTensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, BaseModelOutput]:\n         r\"\"\"\n         Args:\n@@ -334,107 +324,18 @@ def forward(\n                 - 1 for tokens that are **not masked**,\n                 - 0 for tokens that are **masked**.\n                 [What are attention masks?](../glossary#attention-mask)\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-            output_hidden_states (`bool`, *optional*):\n-                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n-                for more detail.\n-            return_dict (`bool`, *optional*):\n-                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n         \"\"\"\n-\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-\n-        encoder_states = () if output_hidden_states else None\n-        all_attentions = () if output_attentions else None\n-\n         hidden_states = inputs_embeds\n-        for idx, encoder_layer in enumerate(self.layers):\n-            if output_hidden_states:\n-                encoder_states = encoder_states + (hidden_states,)\n-            layer_outputs = encoder_layer(\n-                hidden_states=hidden_states,\n-                position_embeddings=position_embeddings,\n-                attention_mask=attention_mask,\n-                output_attentions=output_attentions,\n+        for encoder_layer in self.layers:\n+            hidden_states = encoder_layer(\n+                hidden_states,\n+                position_embeddings,\n+                attention_mask,\n+                **kwargs,\n             )\n \n-            hidden_states = layer_outputs[0]\n-\n-            if output_attentions:\n-                all_attentions = all_attentions + (layer_outputs[1],)\n-\n-        if output_hidden_states:\n-            encoder_states = encoder_states + (hidden_states,)\n-\n-        if not return_dict:\n-            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n         return BaseModelOutput(\n             last_hidden_state=hidden_states,\n-            hidden_states=encoder_states,\n-            attentions=all_attentions,\n-        )\n-\n-\n-class MLCDVisionTransformer(CLIPVisionTransformer):\n-    def __init__(self, config: MLCDVisionConfig):\n-        super().__init__(config)\n-        self.vision_rotary_embedding = MLCDRotaryEmbedding(config.hidden_size // config.num_attention_heads // 2)\n-        self.class_pos_emb = nn.Parameter(torch.randn(1, config.hidden_size // config.num_attention_heads // 2))\n-\n-    @auto_docstring\n-    def forward(\n-        self,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[tuple, BaseModelOutputWithPooling]:\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-\n-        if pixel_values is None:\n-            raise ValueError(\"You have to specify pixel_values\")\n-\n-        num_patches_height = pixel_values.shape[-2] // self.config.patch_size\n-        num_patches_width = pixel_values.shape[-1] // self.config.patch_size\n-        rotary_pos_emb = self.vision_rotary_embedding(num_patches_height, num_patches_width)\n-        rotary_pos_emb = rotary_pos_emb.to(self.class_pos_emb.device)\n-        rotary_pos_emb = torch.cat([self.class_pos_emb, rotary_pos_emb], dim=0)\n-        emb = torch.cat((rotary_pos_emb, rotary_pos_emb), dim=-1)\n-        position_embeddings = (emb.cos(), emb.sin())\n-\n-        hidden_states = self.embeddings(pixel_values)\n-        hidden_states = self.pre_layrnorm(hidden_states)\n-\n-        encoder_outputs = self.encoder(\n-            inputs_embeds=hidden_states,\n-            position_embeddings=position_embeddings,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-        )\n-\n-        last_hidden_state = encoder_outputs[0]\n-        pooled_output = last_hidden_state[:, 0, :]\n-        pooled_output = self.post_layernorm(pooled_output)\n-\n-        if not return_dict:\n-            return (last_hidden_state, pooled_output) + encoder_outputs[1:]\n-\n-        return BaseModelOutputWithPooling(\n-            last_hidden_state=last_hidden_state,\n-            pooler_output=pooled_output,\n-            hidden_states=encoder_outputs.hidden_states,\n-            attentions=encoder_outputs.attentions,\n         )\n \n \n@@ -443,8 +344,15 @@ class MLCDPreTrainedModel(PreTrainedModel):\n     config: MLCDVisionConfig\n     base_model_prefix = \"mlcd\"\n     supports_gradient_checkpointing = True\n+    accepts_loss_kwargs = False\n     _supports_flash_attn = True\n     _supports_sdpa = True\n+    _supports_flex_attn = True\n+    _supports_attention_backend = True\n+    _can_record_outputs = {\n+        \"hidden_states\": MLCDEncoderLayer,\n+        \"attentions\": MLCDAttention,\n+    }\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n@@ -478,14 +386,55 @@ def _init_weights(self, module):\n             module.bias.data.zero_()\n \n \n+class MLCDVisionTransformer(CLIPVisionTransformer):\n+    def __init__(self, config: MLCDVisionConfig):\n+        super().__init__(config)\n+        self.vision_rotary_embedding = MLCDRotaryEmbedding(config.hidden_size // config.num_attention_heads // 2)\n+        self.class_pos_emb = nn.Parameter(torch.randn(1, config.hidden_size // config.num_attention_heads // 2))\n+\n+    @auto_docstring\n+    def forward(\n+        self,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[tuple, BaseModelOutputWithPooling]:\n+        if pixel_values is None:\n+            raise ValueError(\"You have to specify pixel_values\")\n+\n+        num_patches_height = pixel_values.shape[-2] // self.config.patch_size\n+        num_patches_width = pixel_values.shape[-1] // self.config.patch_size\n+        rotary_pos_emb = self.vision_rotary_embedding(num_patches_height, num_patches_width)\n+        rotary_pos_emb = rotary_pos_emb.to(self.class_pos_emb.device)\n+        rotary_pos_emb = torch.cat([self.class_pos_emb, rotary_pos_emb], dim=0)\n+        emb = torch.cat((rotary_pos_emb, rotary_pos_emb), dim=-1)\n+        position_embeddings = (emb.cos(), emb.sin())\n+\n+        hidden_states = self.embeddings(pixel_values)\n+        hidden_states = self.pre_layrnorm(hidden_states)\n+\n+        encoder_outputs = self.encoder(\n+            inputs_embeds=hidden_states,\n+            position_embeddings=position_embeddings,\n+            **kwargs,\n+        )\n+\n+        last_hidden_state = encoder_outputs[0]\n+        pooled_output = last_hidden_state[:, 0, :]\n+        pooled_output = self.post_layernorm(pooled_output)\n+\n+        return BaseModelOutputWithPooling(\n+            last_hidden_state=last_hidden_state,\n+            pooler_output=pooled_output,\n+        )\n+\n+\n class MLCDVisionModel(CLIPVisionModel):\n+    @check_model_inputs(tie_last_hidden_states=False)\n     @auto_docstring\n     def forward(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, BaseModelOutputWithPooling]:\n         r\"\"\"\n         Example:\n@@ -509,17 +458,9 @@ def forward(\n         >>> print(f\"Number of attention layers: {len(outputs.attentions)}\")\n         >>> print(f\"Attention shape: {outputs.attentions[0].shape}\")\n         ```\"\"\"\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-\n         return self.vision_model(\n             pixel_values=pixel_values,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            **kwargs,\n         )\n \n "
      },
      {
        "filename": "tests/models/mlcd/test_modeling_mlcd.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -146,7 +146,7 @@ class MLCDVisionModelIntegrationTest(unittest.TestCase):\n     @slow\n     def test_inference(self):\n         model_name = \"DeepGlint-AI/mlcd-vit-bigG-patch14-448\"\n-        model = MLCDVisionModel.from_pretrained(model_name).to(torch_device)\n+        model = MLCDVisionModel.from_pretrained(model_name, attn_implementation=\"eager\").to(torch_device)\n         processor = AutoProcessor.from_pretrained(model_name)\n \n         # process single image"
      }
    ],
    "num_files": 6,
    "scraped_at": "2025-11-16T21:17:02.173611",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR contains substantial architectural changes to attention mask handling across multiple CLIP-based models, migrating from old mask APIs to new ones and enabling flash attention support. The changes involve non-trivial logic modifications around masking strategies, causal attention handling, and attention implementation selection\u2014providing ample material for meaningful questions about how these components interact.",
      "substance_level": "high"
    }
  },
  {
    "pr_number": 41725,
    "title": "Add GLPNImageProcessorFast ",
    "body": "# What does this PR do?\r\n\r\nThis PR adds a **fast image processor for the GLPN model**, implemented as `GLPNImageProcessorFast`.  \r\n\r\nFixes # (issue)\r\n\r\n## Before submitting\r\n- Implements `GLPNImageProcessorFast` using `BaseImageProcessorFast`.\r\n- Adds tests and documentation updates.\r\n\r\n### \ud83e\uddea Testing\r\n- All tests pass except for the (`test_slow_fast_equivalence_batched`). I would like some help here. \r\n\r\n### \ud83d\udcc4 Files updated\r\n- `src/transformers/models/glpn/image_processing_glpn_fast.py`\r\n- `src/transformers/models/glpn/__init__.py`\r\n- `src/transformers/models/auto/image_processing_auto.py`\r\n- `tests/models/glpn/test_image_processing_glpn.py`\r\n- `docs/source/en/model_doc/glpn.md`\r\n\r\n## Before submitting\r\n- [x] Read the [contributor guidelines](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request).\r\n- [x] Updated documentation and tests.\r\n- [x] Verified style and quality with `make style` and `make quality`.\r\n\r\n## Who can review?\r\n\r\n@yonigozlan @molbap\r\n",
    "html_url": "https://github.com/huggingface/transformers/pull/41725",
    "created_at": "2025-10-19T01:23:05Z",
    "merged_at": "2025-11-04T15:44:52Z",
    "merge_commit_sha": "9a19171fad3025f57fae72d8f3598f44b68102e5",
    "base_ref": "main",
    "head_sha": "60df2c6ce408cc623aef0551f3968e9fe3d1295b",
    "user": "Aravind-11",
    "files": [
      {
        "filename": "docs/source/en/model_doc/glpn.md",
        "status": "modified",
        "additions": 5,
        "deletions": 0,
        "changes": 5,
        "patch": "@@ -61,6 +61,11 @@ A list of official Hugging Face and community (indicated by \ud83c\udf0e) resources to h\n [[autodoc]] GLPNImageProcessor\n     - preprocess\n \n+## GLPNImageProcessorFast\n+\n+[[autodoc]] GLPNImageProcessorFast\n+    - preprocess\n+\n ## GLPNModel\n \n [[autodoc]] GLPNModel"
      },
      {
        "filename": "src/transformers/image_processing_utils_fast.py",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -305,6 +305,8 @@ def resize(\n                 Dictionary in the format `{\"height\": int, \"width\": int}` specifying the size of the output image.\n             interpolation (`InterpolationMode`, *optional*, defaults to `InterpolationMode.BILINEAR`):\n                 `InterpolationMode` filter to use when resizing the image e.g. `InterpolationMode.BICUBIC`.\n+            antialias (`bool`, *optional*, defaults to `True`):\n+                Whether to use antialiasing.\n \n         Returns:\n             `torch.Tensor`: The resized image."
      },
      {
        "filename": "src/transformers/models/auto/image_processing_auto.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -103,7 +103,7 @@\n             (\"gemma3n\", (\"SiglipImageProcessor\", \"SiglipImageProcessorFast\")),\n             (\"git\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),\n             (\"glm4v\", (\"Glm4vImageProcessor\", \"Glm4vImageProcessorFast\")),\n-            (\"glpn\", (\"GLPNImageProcessor\", None)),\n+            (\"glpn\", (\"GLPNImageProcessor\", \"GLPNImageProcessorFast\")),\n             (\"got_ocr2\", (\"GotOcr2ImageProcessor\", \"GotOcr2ImageProcessorFast\")),\n             (\"grounding-dino\", (\"GroundingDinoImageProcessor\", \"GroundingDinoImageProcessorFast\")),\n             (\"groupvit\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),"
      },
      {
        "filename": "src/transformers/models/glpn/__init__.py",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -21,6 +21,7 @@\n     from .configuration_glpn import *\n     from .feature_extraction_glpn import *\n     from .image_processing_glpn import *\n+    from .image_processing_glpn_fast import *\n     from .modeling_glpn import *\n else:\n     import sys"
      },
      {
        "filename": "src/transformers/models/glpn/image_processing_glpn.py",
        "status": "modified",
        "additions": 22,
        "deletions": 1,
        "changes": 23,
        "patch": "@@ -39,6 +39,7 @@\n     valid_images,\n     validate_preprocess_arguments,\n )\n+from ...processing_utils import ImagesKwargs\n from ...utils import TensorType, filter_out_non_signature_kwargs, logging, requires_backends\n \n \n@@ -49,6 +50,17 @@\n logger = logging.get_logger(__name__)\n \n \n+class GLPNImageProcessorKwargs(ImagesKwargs, total=False):\n+    \"\"\"\n+    size_divisor (`int`, *optional*, defaults to 32):\n+        When `do_resize` is `True`, images are resized so their height and width are rounded down to the closest\n+        multiple of `size_divisor`.\n+    \"\"\"\n+\n+    size_divisor: int\n+    resample: PILImageResampling\n+\n+\n @requires(backends=(\"vision\",))\n class GLPNImageProcessor(BaseImageProcessor):\n     r\"\"\"\n@@ -66,22 +78,27 @@ class GLPNImageProcessor(BaseImageProcessor):\n         do_rescale (`bool`, *optional*, defaults to `True`):\n             Whether or not to apply the scaling factor (to make pixel values floats between 0. and 1.). Can be\n             overridden by `do_rescale` in `preprocess`.\n+        rescale_factor (`float`, *optional*, defaults to `1 / 255`):\n+            The scaling factor to apply to the pixel values. Can be overridden by `rescale_factor` in `preprocess`.\n     \"\"\"\n \n     model_input_names = [\"pixel_values\"]\n+    valid_kwargs = GLPNImageProcessorKwargs\n \n     def __init__(\n         self,\n         do_resize: bool = True,\n         size_divisor: int = 32,\n         resample=PILImageResampling.BILINEAR,\n         do_rescale: bool = True,\n+        rescale_factor: Optional[float] = 1 / 255,\n         **kwargs,\n     ) -> None:\n         self.do_resize = do_resize\n         self.do_rescale = do_rescale\n         self.size_divisor = size_divisor\n         self.resample = resample\n+        self.rescale_factor = rescale_factor\n         super().__init__(**kwargs)\n \n     def resize(\n@@ -142,6 +159,7 @@ def preprocess(\n         size_divisor: Optional[int] = None,\n         resample=None,\n         do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n         return_tensors: Optional[Union[TensorType, str]] = None,\n         data_format: ChannelDimension = ChannelDimension.FIRST,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n@@ -181,6 +199,7 @@ def preprocess(\n         \"\"\"\n         do_resize = do_resize if do_resize is not None else self.do_resize\n         do_rescale = do_rescale if do_rescale is not None else self.do_rescale\n+        rescale_factor = rescale_factor if rescale_factor is not None else self.rescale_factor\n         size_divisor = size_divisor if size_divisor is not None else self.size_divisor\n         resample = resample if resample is not None else self.resample\n \n@@ -217,7 +236,9 @@ def preprocess(\n             ]\n \n         if do_rescale:\n-            images = [self.rescale(image, scale=1 / 255, input_data_format=input_data_format) for image in images]\n+            images = [\n+                self.rescale(image, scale=rescale_factor, input_data_format=input_data_format) for image in images\n+            ]\n \n         images = [\n             to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format) for image in images"
      },
      {
        "filename": "src/transformers/models/glpn/image_processing_glpn_fast.py",
        "status": "added",
        "additions": 136,
        "deletions": 0,
        "changes": 136,
        "patch": "@@ -0,0 +1,136 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Fast Image processor class for GLPN.\"\"\"\n+\n+from typing import Optional, Union\n+\n+import torch\n+from torchvision.transforms.v2 import functional as F\n+\n+from ...image_processing_utils import BatchFeature\n+from ...image_processing_utils_fast import BaseImageProcessorFast, group_images_by_shape, reorder_images\n+from ...image_utils import (\n+    PILImageResampling,\n+    SizeDict,\n+)\n+from ...utils import (\n+    TensorType,\n+    auto_docstring,\n+    requires_backends,\n+)\n+from .image_processing_glpn import GLPNImageProcessorKwargs\n+\n+\n+@auto_docstring\n+class GLPNImageProcessorFast(BaseImageProcessorFast):\n+    do_resize = True\n+    do_rescale = True\n+    rescale_factor = 1 / 255\n+    resample = PILImageResampling.BILINEAR\n+    size_divisor = 32\n+    valid_kwargs = GLPNImageProcessorKwargs\n+\n+    def _validate_preprocess_kwargs(self, **kwargs):\n+        # pop `do_resize` to not raise an error as `size` is not None\n+        kwargs.pop(\"do_resize\", None)\n+        return super()._validate_preprocess_kwargs(**kwargs)\n+\n+    def resize(\n+        self,\n+        image: \"torch.Tensor\",\n+        size_divisor: int,\n+        interpolation: Optional[\"F.InterpolationMode\"] = None,\n+        antialias: bool = True,\n+        **kwargs,\n+    ) -> \"torch.Tensor\":\n+        \"\"\"\n+        Resize an image to `(size[\"height\"], size[\"width\"])`.\n+\n+        Args:\n+            image (`torch.Tensor`):\n+                Image to resize.\n+            size (`SizeDict`):\n+                Dictionary in the format `{\"height\": int, \"width\": int}` specifying the size of the output image.\n+            interpolation (`InterpolationMode`, *optional*, defaults to `InterpolationMode.BILINEAR`):\n+                `InterpolationMode` filter to use when resizing the image e.g. `InterpolationMode.BICUBIC`.\n+            antialias (`bool`, *optional*, defaults to `True`):\n+                Whether to use antialiasing.\n+\n+        Returns:\n+            `torch.Tensor`: The resized image.\n+        \"\"\"\n+        height, width = image.shape[-2:]\n+        # Rounds the height and width down to the closest multiple of size_divisor\n+        new_h = height // size_divisor * size_divisor\n+        new_w = width // size_divisor * size_divisor\n+        return super().resize(\n+            image, SizeDict(height=new_h, width=new_w), interpolation=interpolation, antialias=antialias\n+        )\n+\n+    def _preprocess(\n+        self,\n+        images: list[\"torch.Tensor\"],\n+        do_resize: bool,\n+        size_divisor: Optional[int] = None,\n+        interpolation: Optional[\"F.InterpolationMode\"] = None,\n+        do_rescale: bool = True,\n+        rescale_factor: Optional[float] = 1 / 255,\n+        do_normalize: bool = False,\n+        image_mean: Optional[Union[float, list[float]]] = None,\n+        image_std: Optional[Union[float, list[float]]] = None,\n+        disable_grouping: Optional[bool] = None,\n+        return_tensors: Optional[Union[str, TensorType]] = None,\n+        resample: Optional[PILImageResampling] = None,\n+        **kwargs,\n+    ) -> BatchFeature:\n+        grouped_images, grouped_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n+        processed_groups = {}\n+\n+        for shape, stacked_images in grouped_images.items():\n+            if do_resize:\n+                stacked_images = self.resize(stacked_images, size_divisor=size_divisor, interpolation=interpolation)\n+            stacked_images = self.rescale_and_normalize(\n+                stacked_images, do_rescale, rescale_factor, do_normalize, image_mean, image_std\n+            )\n+            processed_groups[shape] = stacked_images\n+\n+        processed_images = reorder_images(processed_groups, grouped_index)\n+        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n+        return BatchFeature(data={\"pixel_values\": processed_images}, tensor_type=return_tensors)\n+\n+    def post_process_depth_estimation(self, outputs, target_sizes=None):\n+        \"\"\"\n+        Convert raw model outputs to final depth predictions.\n+        Mirrors slow GLPN: PyTorch interpolate w/ bicubic, align_corners=False.\n+        \"\"\"\n+        requires_backends(self, \"torch\")\n+        predicted_depth = outputs.predicted_depth\n+\n+        results = []\n+        target_sizes = target_sizes or [None] * predicted_depth.shape[0]\n+        for depth, target_size in zip(predicted_depth, target_sizes):\n+            if target_size is not None:\n+                # Add batch and channel dimensions for interpolation\n+                depth_4d = depth[None, None, ...]\n+                resized = torch.nn.functional.interpolate(\n+                    depth_4d, size=target_size, mode=\"bicubic\", align_corners=False\n+                )\n+                depth = resized.squeeze(0).squeeze(0)\n+            results.append({\"predicted_depth\": depth})\n+\n+        return results\n+\n+\n+__all__ = [\"GLPNImageProcessorFast\"]"
      },
      {
        "filename": "tests/models/glpn/test_image_processing_glpn.py",
        "status": "modified",
        "additions": 61,
        "deletions": 6,
        "changes": 67,
        "patch": "@@ -18,7 +18,7 @@\n import numpy as np\n \n from transformers.testing_utils import require_torch, require_vision\n-from transformers.utils import is_torch_available, is_vision_available\n+from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n \n@@ -31,6 +31,9 @@\n \n     from transformers import GLPNImageProcessor\n \n+    if is_torchvision_available():\n+        from transformers import GLPNImageProcessorFast\n+\n \n class GLPNImageProcessingTester:\n     def __init__(\n@@ -87,19 +90,32 @@ def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=F\n             torchify=torchify,\n         )\n \n+    def prepare_depth_outputs(self):\n+        if not is_torch_available():\n+            return None\n+        depth_tensors = prepare_image_inputs(\n+            batch_size=self.batch_size,\n+            num_channels=1,\n+            min_resolution=self.min_resolution,\n+            max_resolution=self.max_resolution,\n+            equal_resolution=True,\n+            torchify=True,\n+        )\n+        depth_tensors = [depth_tensor.squeeze(0) for depth_tensor in depth_tensors]\n+        stacked_depth_tensors = torch.stack(depth_tensors, dim=0)\n+        return type(\"DepthOutput\", (), {\"predicted_depth\": stacked_depth_tensors})\n+\n \n @require_torch\n @require_vision\n class GLPNImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n     image_processing_class = GLPNImageProcessor if is_vision_available() else None\n+    fast_image_processing_class = GLPNImageProcessorFast if is_torchvision_available() else None\n \n     def setUp(self):\n         super().setUp()\n         self.image_processor_tester = GLPNImageProcessingTester(self)\n-\n-    @property\n-    def image_processor_dict(self):\n-        return self.image_processor_tester.prepare_image_processor_dict()\n+        self.image_processor_dict = self.image_processor_tester.prepare_image_processor_dict()\n \n     def test_image_processor_properties(self):\n         image_processing = self.image_processing_class(**self.image_processor_dict)\n@@ -115,7 +131,6 @@ def test_call_pil(self):\n         image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False)\n         for image in image_inputs:\n             self.assertIsInstance(image, Image.Image)\n-\n         # Test not batched input (GLPNImageProcessor doesn't support batching)\n         encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").pixel_values\n         expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs)\n@@ -161,3 +176,43 @@ def test_call_numpy_4_channels(self):\n         expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs)\n         self.assertTrue(tuple(encoded_images.shape) == (1, *expected_output_image_shape))\n         self.image_processing_class.num_channels = 3\n+\n+    # override as glpn image processors don't support heterogeneous batching\n+    @require_vision\n+    @require_torch\n+    def test_slow_fast_equivalence_batched(self):\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n+\n+        dummy_images = self.image_processor_tester.prepare_image_inputs(equal_resolution=True, torchify=True)\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        encoding_slow = image_processor_slow(dummy_images, return_tensors=\"pt\")\n+        encoding_fast = image_processor_fast(dummy_images, return_tensors=\"pt\")\n+\n+        self._assert_slow_fast_tensors_equivalence(encoding_slow.pixel_values, encoding_fast.pixel_values)\n+\n+    def test_post_process_depth_equivalence(self):\n+        # Check that both processors produce equivalent post-processed depth maps\n+        if self.fast_image_processing_class is None:\n+            self.skipTest(\"TorchVision not available\")\n+\n+        outputs = self.image_processor_tester.prepare_depth_outputs()\n+        slow = self.image_processing_class(**self.image_processor_dict)\n+        fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        # target_sizes simulate resized inference outputs\n+        target_sizes = [(240, 320)] * self.image_processor_tester.batch_size\n+        processed_slow = slow.post_process_depth_estimation(outputs, target_sizes=target_sizes)\n+        processed_fast = fast.post_process_depth_estimation(outputs, target_sizes=target_sizes)\n+\n+        # Compare per-sample predicted depth tensors\n+        for pred_slow, pred_fast in zip(processed_slow, processed_fast):\n+            depth_slow = pred_slow[\"predicted_depth\"]\n+            depth_fast = pred_fast[\"predicted_depth\"]\n+            torch.testing.assert_close(depth_fast, depth_slow, atol=1e-1, rtol=1e-3)\n+            self.assertLessEqual(torch.mean(torch.abs(depth_fast.float() - depth_slow.float())).item(), 5e-3)"
      }
    ],
    "num_files": 7,
    "scraped_at": "2025-11-16T21:17:05.746058",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR adds a complete new fast image processor implementation for GLPN with meaningful architectural decisions (inheriting from BaseImageProcessorFast, batching optimizations, torch backend integration), accompanying tests, and documentation. The changes involve non-trivial logic around image processing pipelines, batching strategies, and framework-specific optimizations that would help developers understand both the GLPN model integration and the fast processor pattern used across the codebase.",
      "substance_level": "high"
    }
  },
  {
    "pr_number": 41672,
    "title": "feat: add benchmark v2 ci with results pushed to dataset",
    "body": "Reusing the old `benchmark.yml` workflow for the benchmark v2, we'll eventually rename and clean up everything once we're confident it works nicely.\r\n\r\nBenchmark results will be pushed to a dataset, for now located [here](https://huggingface.co/datasets/hf-benchmarks/transformers).\r\n\r\nI chose JSONL for simplicity of parsing in the frontend space, and file name have the following format: `f\"benchmark_run_{timestamp}.jsonl\"`.\r\n\r\nAlso did a little naming refactoring and added extra metadata",
    "html_url": "https://github.com/huggingface/transformers/pull/41672",
    "created_at": "2025-10-16T20:21:06Z",
    "merged_at": "2025-10-20T07:56:58Z",
    "merge_commit_sha": "71db0d49e99884566026c140f8b12b61056fa8dc",
    "base_ref": "main",
    "head_sha": "213f4f92086981c65d65edcb0002217ad8900241",
    "user": "McPatate",
    "files": [
      {
        "filename": ".github/workflows/benchmark.yml",
        "status": "modified",
        "additions": 10,
        "deletions": 21,
        "changes": 31,
        "patch": "@@ -1,14 +1,19 @@\n name: Self-hosted runner (benchmark)\r\n \r\n on:\r\n-  workflow_dispatch:\r\n+  push:\r\n+    branches: [main]\r\n+  pull_request:\r\n+    types: [ opened, labeled, reopened, synchronize ]\r\n \r\n concurrency:\r\n   group: ${{ github.workflow }}-${{ github.head_ref || github.run_id }}\r\n   cancel-in-progress: true\r\n \r\n env:\r\n   HF_HOME: /mnt/cache\r\n+  DATASET_ID: hf-benchmarks/transformers\r\n+  MODEL_ID: meta-llama/Llama-3.1-8B-Instruct\r\n \r\n jobs:\r\n   benchmark:\r\n@@ -31,26 +36,12 @@ jobs:\n         with:\r\n           ref: ${{ github.event.pull_request.head.sha || github.sha }}\r\n \r\n-      - name: Install libpq-dev & psql\r\n-        run: |\r\n-          apt update\r\n-          apt install -y libpq-dev postgresql-client\r\n-\r\n       - name: Install benchmark script dependencies\r\n-        run: python3 -m pip install -r benchmark/requirements.txt\r\n+        run: python3 -m pip install -r benchmark_v2/requirements.txt kernels\r\n \r\n       - name: Reinstall transformers in edit mode (remove the one installed during docker image build)\r\n         working-directory: /transformers\r\n-        run: python3 -m pip uninstall -y transformers && python3 -m pip install -e \".[torch]\"\r\n-\r\n-      - name: Run database init script\r\n-        run: |\r\n-          psql -f benchmark/utils/init_db.sql\r\n-        env:\r\n-          PGDATABASE: metrics\r\n-          PGHOST: ${{ secrets.TRANSFORMERS_BENCHMARKS_PGHOST }}\r\n-          PGUSER: transformers_benchmarks\r\n-          PGPASSWORD: ${{ secrets.TRANSFORMERS_BENCHMARKS_PGPASSWORD }}\r\n+        run: python3 -m pip uninstall -y transformers && python3 -m pip install -e \".[torch]\" && python3 -m pip uninstall -y torchvision # temp fix\r\n \r\n       - name: Run benchmark\r\n         run: |\r\n@@ -61,13 +52,11 @@ jobs:\n             commit_id=$GITHUB_SHA\r\n           fi\r\n           commit_msg=$(git show -s --format=%s | cut -c1-70)\r\n-          python3 benchmark/benchmarks_entrypoint.py \"huggingface/transformers\" \"$BRANCH_NAME\" \"$commit_id\" \"$commit_msg\"\r\n+          python3 benchmark_v2/run_benchmarks.py -b 32 -s 128 -n 256 --branch-name \"$BRANCH_NAME\" --commit-id \"$commit_id\" --commit-message \"$commit_msg\" --model-id \"$MODEL_ID\" --log-level INFO --push-result-to-dataset \"$DATASET_ID\"\r\n         env:\r\n           HF_TOKEN: ${{ secrets.HF_HUB_READ_TOKEN }}\r\n+          PUSH_TO_HUB_TOKEN: ${{ secrets.PUSH_TO_HUB_TOKEN }}\r\n           # Enable this to see debug logs\r\n           # HF_HUB_VERBOSITY: debug\r\n           # TRANSFORMERS_VERBOSITY: debug\r\n-          PGHOST: ${{ secrets.TRANSFORMERS_BENCHMARKS_PGHOST }}\r\n-          PGUSER: transformers_benchmarks\r\n-          PGPASSWORD: ${{ secrets.TRANSFORMERS_BENCHMARKS_PGPASSWORD }}\r\n           BRANCH_NAME: ${{ github.head_ref || github.ref_name }}\r"
      },
      {
        "filename": "benchmark_v2/framework/benchmark_config.py",
        "status": "modified",
        "additions": 7,
        "deletions": 8,
        "changes": 15,
        "patch": "@@ -22,7 +22,7 @@ def __init__(\n         self,\n         warmup_iterations: int = 5,\n         measurement_iterations: int = 20,\n-        gpu_monitoring: bool = False,  # False by default because it slows down the benchmark by a lot\n+        gpu_monitoring: bool = True,  # NOTE: you may want to disable this at times as we have obsvered it could heavily slow down benchmarks on AMD\n         batch_size: int = 1,\n         sequence_length: int = 128,\n         num_tokens_to_generate: int = 128,\n@@ -136,7 +136,7 @@ def cross_generate_configs(\n     batch_size: int = 1,\n     sequence_length: int = 128,\n     num_tokens_to_generate: int = 128,\n-    gpu_monitoring: bool = False,  # this slows down the benchmark by a lot so we disable it by default\n+    gpu_monitoring: bool = True,\n ) -> list[BenchmarkConfig]:\n     # Create kwargs common to all configs\n     kwargs = {\n@@ -169,7 +169,7 @@ def generate_all_configs(\n     batch_size: int = 1,\n     sequence_length: int = 128,\n     num_tokens_to_generate: int = 128,\n-    gpu_monitoring: bool = False,\n+    gpu_monitoring: bool = True,\n ) -> list[BenchmarkConfig]:\n     all_attn_implementations = [\n         (\"flash_attention_2\", None),\n@@ -197,7 +197,6 @@ def generate_main_configs(\n     batch_size: int = 1,\n     sequence_length: int = 128,\n     num_tokens_to_generate: int = 128,\n-    gpu_monitoring: bool = False,\n ) -> list[BenchmarkConfig]:\n     # Create kwargs common to all configs\n     kwargs = {\n@@ -206,10 +205,10 @@ def generate_main_configs(\n         \"batch_size\": batch_size,\n         \"sequence_length\": sequence_length,\n         \"num_tokens_to_generate\": num_tokens_to_generate,\n-        \"gpu_monitoring\": gpu_monitoring,\n     }\n     return [  # TODO: test max-autotune instead of default\n-        BenchmarkConfig(attn_implementation=\"flex_attention\", compile_mode=\"default\", **kwargs),\n-        BenchmarkConfig(attn_implementation=\"eager\", compile_mode=\"default\", **kwargs),\n-        BenchmarkConfig(attn_implementation=\"flash_attention_2\", **kwargs),\n+        BenchmarkConfig(attn_implementation=\"flex_attention\", compile_mode=\"default\", gpu_monitoring=False, **kwargs),\n+        BenchmarkConfig(attn_implementation=\"flex_attention\", compile_mode=\"default\", gpu_monitoring=True, **kwargs),\n+        BenchmarkConfig(attn_implementation=\"eager\", compile_mode=\"default\", gpu_monitoring=True, **kwargs),\n+        BenchmarkConfig(attn_implementation=\"flash_attention_2\", gpu_monitoring=True, **kwargs),\n     ]"
      },
      {
        "filename": "benchmark_v2/framework/benchmark_runner.py",
        "status": "modified",
        "additions": 76,
        "deletions": 9,
        "changes": 85,
        "patch": "@@ -4,13 +4,16 @@\n import os\n import pathlib\n import re\n+import tempfile\n import time\n from contextlib import nullcontext\n from datetime import datetime\n from queue import Queue\n from typing import Any\n \n import torch\n+from datasets import Dataset\n+from huggingface_hub import HfApi\n from tqdm import trange\n \n from transformers import (\n@@ -50,6 +53,8 @@\n     \"Its instability ended in the coup of 18 Brumaire and the establishment of the Consulate, with Napoleon Bonaparte as First Consul.\",\n ])  # fmt: skip\n \n+PUSH_TO_HUB_TOKEN = os.getenv(\"PUSH_TO_HUB_TOKEN\", None)\n+\n \n def compact_json_numeric_arrays(data: dict):\n     # Match arrays that contain only numbers (ints/floats), whitespace, commas, and newlines\n@@ -120,15 +125,19 @@ def flush_memory():\n \n class BenchmarkStreamer(BaseStreamer):\n     def __init__(self, **kwargs) -> None:\n+        self.timeout = kwargs.pop(\"timeout\", 10)\n         self.timestamps = []\n         self.text_queue = Queue()\n+        self.stop_signal = None\n \n     def put(self, value):\n         \"\"\"Receives tokens and logs the timestamp of the generation.\"\"\"\n         self.timestamps.append(time.perf_counter())\n+        self.text_queue.put(value)\n \n     def end(self):\n         self.timestamps.append(time.perf_counter())\n+        self.text_queue.put(self.stop_signal)\n \n     def __iter__(self):\n         return self\n@@ -144,13 +153,22 @@ def __next__(self):\n class BenchmarkRunner:\n     \"\"\"Main benchmark runner that coordinates benchmark execution.\"\"\"\n \n-    def __init__(self, logger: logging.Logger, output_dir: str | None = None, commit_id: str | None = None) -> None:\n+    def __init__(\n+        self,\n+        logger: logging.Logger,\n+        output_dir: str | None = None,\n+        branch_name: str | None = None,\n+        commit_id: str | None = None,\n+        commit_message: str | None = None,\n+    ) -> None:\n         # Those stay constant for the whole run\n         self.logger = logger\n         if output_dir is None:\n             output_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), \"benchmark_results\")\n         self.output_dir = output_dir\n+        self.branch_name = branch_name\n         self.commit_id = get_git_revision() if commit_id is None else commit_id\n+        self.commit_message = commit_message\n         os.makedirs(self.output_dir, exist_ok=True)\n         self.profile_dir = None\n         # Attributes that are reset for each model\n@@ -163,7 +181,7 @@ def cleanup(self) -> None:\n         self.model = None\n         flush_memory()\n \n-    def setup_one_run(self, model_id: str, config: BenchmarkConfig) -> None:\n+    def setup_benchmark(self, model_id: str, config: BenchmarkConfig) -> None:\n         # Some attributes only need to be set once per model\n         if self._setup_for != model_id:\n             self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n@@ -200,10 +218,13 @@ def setup_one_run(self, model_id: str, config: BenchmarkConfig) -> None:\n         self.model = self.model.eval().to(config.device)\n \n         # Kernelize the model if needed\n-        if config.kernelize:\n+        if config.kernelize and kernelize is not None and Mode is not None:\n             self.model = kernelize(self.model, mode=Mode.INFERENCE)\n \n-    def run_one_benchmark(self, model_id: str, config: BenchmarkConfig, num_tokens_to_profile: int = 0) -> None:\n+    def run_benchmark(\n+        self, model_id: str, config: BenchmarkConfig, num_tokens_to_profile: int = 0\n+    ) -> dict[str, Any] | None:\n+        \"\"\"Run a single benchmark with the given model ID and config.\"\"\"\n         sdpa_ctx = nullcontext()\n         if config.attn_implementation == \"sdpa\":\n             sdpa_backend = get_sdpa_backend(config.sdpa_backend)\n@@ -243,7 +264,12 @@ def run_one_benchmark(self, model_id: str, config: BenchmarkConfig, num_tokens_t\n                 self.profile_generate(num_tokens_to_profile, config.name)\n \n             return {\n-                \"metadata\": BenchmarkMetadata(model_id=model_id, commit_id=self.commit_id),\n+                \"metadata\": BenchmarkMetadata(\n+                    model_id=model_id,\n+                    branch_name=self.branch_name,\n+                    commit_id=self.commit_id,\n+                    commit_message=self.commit_message,\n+                ),\n                 \"measurements\": result,\n                 \"config\": config,\n             }\n@@ -305,7 +331,8 @@ def run_benchmarks(\n         benchmark_configs: list[BenchmarkConfig],\n         num_tokens_to_profile: int = 0,\n         pretty_print_summary: bool = True,\n-    ) -> dict[str, Any]:\n+    ) -> tuple[str, dict[str, Any]]:\n+        \"\"\"Run multiple benchmarks for the given model ID and list of benchmark configs.\"\"\"\n         all_results = {}\n         timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n         start_time = time.perf_counter()\n@@ -324,14 +351,14 @@ def run_benchmarks(\n                 continue\n \n             # Otherwise, run the benchmark\n-            self.setup_one_run(model_id, config)\n+            self.setup_benchmark(model_id, config)\n             self.logger.info(\n                 f\"Running benchmark of model {model_id} with scenario: {config.name} ({i + 1}/{n_configs})\"\n             )\n \n             # Launch benchmark in a try/except block to avoid stopping the whole run if one benchmark fails\n             try:\n-                results = self.run_one_benchmark(model_id, config, num_tokens_to_profile)\n+                results = self.run_benchmark(model_id, config, num_tokens_to_profile)\n                 if results is not None:\n                     all_results[config.hash] = results\n \n@@ -358,7 +385,7 @@ def run_benchmarks(\n                 result[\"measurements\"].pprint(batch_size=result[\"config\"].batch_size, tabs=1)\n             print(\"=\" * 100)\n \n-        return all_results\n+        return (timestamp, all_results)\n \n     def save_results(self, model_name: str, results: dict, timestamp: str = \"\") -> str:\n         \"\"\"Save benchmark results to JSON file.\"\"\"\n@@ -387,3 +414,43 @@ def save_results(self, model_name: str, results: dict, timestamp: str = \"\") -> s\n \n         self.logger.info(f\"Results saved to {filepath}\")\n         return filepath\n+\n+    def push_results_to_hub(self, dataset_id: str, results: dict[Any, Any], timestamp: str) -> None:\n+        if PUSH_TO_HUB_TOKEN is None:\n+            raise ValueError(\n+                \"PUSH_TO_HUB_TOKEN is not set, cannot push results to the Hub. When setting dataset_id, please also set the PUSH_TO_HUB_TOKEN environment variable.\"\n+            )\n+\n+        n_results = len(results)\n+        self.logger.info(f\"Pushing {n_results} results to: {dataset_id}\")\n+        rows = []\n+        for cfg_hash, entry in results.items():\n+            row = {\n+                \"benchmark_config_hash\": cfg_hash,\n+                \"config\": entry[\"config\"].to_dict(),\n+                \"measurements\": entry[\"measurements\"].to_dict(),\n+                \"metadata\": entry[\"metadata\"].to_dict(),\n+            }\n+            rows.append(row)\n+\n+        ds = Dataset.from_list(rows)\n+        with tempfile.TemporaryDirectory() as tmp:\n+            jsonl_path = os.path.join(tmp, \"data.jsonl\")\n+            with open(jsonl_path, \"w\") as f:\n+                json_lines = []\n+                for ex in ds:\n+                    json_lines.append(json.dumps(ex, ensure_ascii=False))\n+                f.write(\"\\n\".join(json_lines))\n+\n+            api = HfApi()\n+            # NOTE: we expect the repository to already exist\n+            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\") if not timestamp else timestamp\n+            file_name = f\"benchmark_run_{timestamp}.jsonl\"\n+            api.upload_file(\n+                path_or_fileobj=jsonl_path,\n+                path_in_repo=file_name,\n+                repo_id=dataset_id,\n+                repo_type=\"dataset\",\n+                token=PUSH_TO_HUB_TOKEN,\n+            )\n+        self.logger.info(f\"Succesfully uploaded results to: {dataset_id}\")"
      },
      {
        "filename": "benchmark_v2/framework/data_classes.py",
        "status": "modified",
        "additions": 10,
        "deletions": 3,
        "changes": 13,
        "patch": "@@ -1,5 +1,5 @@\n from dataclasses import dataclass\n-from datetime import datetime\n+from datetime import datetime, timezone\n from typing import Any\n \n import numpy as np\n@@ -59,19 +59,26 @@ class BenchmarkMetadata:\n \n     model_id: str\n     timestamp: str\n+    branch_name: str\n     commit_id: str\n+    commit_message: str\n     hardware_info: HardwareInfo\n \n-    def __init__(self, model_id: str, commit_id: str):\n+    def __init__(self, model_id: str, commit_id: str, branch_name: str = \"main\", commit_message: str = \"\") -> None:\n         self.model_id = model_id\n-        self.timestamp = datetime.utcnow().isoformat()\n+        self.timestamp = datetime.now(timezone.utc).isoformat()\n+        self.branch_name = branch_name\n         self.commit_id = commit_id\n+        self.commit_message = commit_message\n         self.hardware_info = HardwareInfo()\n \n     def to_dict(self) -> dict[str, Any]:\n         return {\n+            \"model_id\": self.model_id,\n             \"timestamp\": self.timestamp,\n+            \"branch_name\": self.branch_name,\n             \"commit_id\": self.commit_id,\n+            \"commit_message\": self.commit_message,\n             \"hardware_info\": self.hardware_info.to_dict(),\n         }\n "
      },
      {
        "filename": "benchmark_v2/requirements.txt",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -4,4 +4,4 @@ gpustat>=1.0.0\n torch>=2.0.0\n transformers>=4.30.0\n datasets>=2.10.0\n-huggingface_hub>=0.16.0 \n\\ No newline at end of file\n+huggingface_hub>=0.16.0"
      },
      {
        "filename": "benchmark_v2/run_benchmarks.py",
        "status": "modified",
        "additions": 32,
        "deletions": 6,
        "changes": 38,
        "patch": "@@ -33,9 +33,8 @@\n     parser.add_argument(\"--output-dir\", type=str, default=None, help=\"Output dir for benchmark results\")\n     parser.add_argument(\"--log-level\", type=str, choices=[\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\"], default=\"INFO\")\n     parser.add_argument(\"--model-id\", type=str, help=\"Specific model ID to benchmark (if supported by benchmarks)\")\n-\n-    parser.add_argument(\"--warmup\", type=int, default=3, help=\"Number of warmup iterations\")\n-    parser.add_argument(\"--iterations\", type=int, default=10, help=\"Number of measurement iterations\")\n+    parser.add_argument(\"--warmup\", \"-w\", type=int, default=3, help=\"Number of warmup iterations\")\n+    parser.add_argument(\"--iterations\", \"-i\", type=int, default=10, help=\"Number of measurement iterations\")\n \n     parser.add_argument(\"--batch-size\", \"-b\", type=int, nargs=\"+\", help=\"Batch size\")\n     parser.add_argument(\"--sequence-length\", \"-s\", type=int, nargs=\"+\", help=\"Sequence length\")\n@@ -44,7 +43,20 @@\n     parser.add_argument(\"--cross-generate\", action=\"store_true\", help=\"Cross-generate all combinations of configs\")\n     parser.add_argument(\"--num-tokens-to-profile\", \"-p\", type=int, default=0, help=\"Number of tokens to profile\")\n \n+    parser.add_argument(\"--branch-name\", type=str, help=\"Git branch name\")\n     parser.add_argument(\"--commit-id\", type=str, help=\"Git commit ID (if not provided, will auto-detect from git)\")\n+    parser.add_argument(\"--commit-message\", type=str, help=\"Git commit message\")\n+\n+    parser.add_argument(\n+        \"--no-gpu-monitoring\", action=\"store_true\", help=\"Disables GPU monitoring during benchmark runs\"\n+    )\n+\n+    parser.add_argument(\n+        \"--push-result-to-dataset\",\n+        type=str,\n+        default=None,\n+        help=\"Name of the dataset to push results to. If not provided, results are not pushed to the Hub.\",\n+    )\n     args = parser.parse_args()\n \n     # Setup logging\n@@ -76,6 +88,7 @@\n                 batch_size=args.batch_size[0],\n                 sequence_length=args.sequence_length[0],\n                 num_tokens_to_generate=args.num_tokens_to_generate[0],\n+                gpu_monitoring=not args.no_gpu_monitoring,\n             )\n         else:\n             benchmark_configs = generate_main_configs(\n@@ -106,11 +119,24 @@\n                     cfg_dict.pop(\"name\")\n                     benchmark_configs.append(BenchmarkConfig.from_dict(cfg_dict))\n \n-    runner = BenchmarkRunner(logger, args.output_dir, args.commit_id)\n-    results = runner.run_benchmarks(\n+    runner = BenchmarkRunner(\n+        logger,\n+        args.output_dir,\n+        args.branch_name,\n+        args.commit_id,\n+        args.commit_message,\n+    )\n+    timestamp, results = runner.run_benchmarks(\n         args.model_id,\n         benchmark_configs,\n         args.num_tokens_to_profile,\n         pretty_print_summary=True,\n     )\n-    # runner.save_results(args.model_id, results)\n+\n+    dataset_id = args.push_result_to_dataset\n+    if dataset_id is not None and len(results) > 0:\n+        runner.push_results_to_hub(\n+            dataset_id,\n+            results,\n+            timestamp,\n+        )"
      }
    ],
    "num_files": 6,
    "scraped_at": "2025-11-16T21:17:15.995964",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR contains substantial non-trivial changes that migrate benchmark infrastructure from v1 (database-backed) to v2 (dataset-backed), with meaningful modifications to CI workflows, data structures, and the benchmark runner logic. The PR description provides clear context about the architectural shift and design decisions (JSONL format, dataset storage, metadata tracking), and developers would need to understand this infrastructure change to work on related benchmarking features.",
      "substance_level": "high"
    }
  },
  {
    "pr_number": 41647,
    "title": "[Fix] Deepseek V3 expert bias routing",
    "body": "# What does this PR do?\r\n\r\nBy chance we noticed that https://github.com/huggingface/transformers/pull/40132 seems to have introduced a bug in the Deepseek V3 routing implementation: The Deepseek-V3 technical report explicitly states\r\n> Note that the bias term is only used for routing. The gating value, which will be multiplied with\r\nthe FFN output, is still derived from the original affinity score\r\n\r\nThis was the case in transformers until https://github.com/huggingface/transformers/pull/40132 which changed the routing code such that the gating values are now derived from the tensor with the added bias term. I wrote a quick fix for the Deepseek-V3 model in this PR, not sure if other models are also affected. Can you please have a look @ArthurZucker and confirm that this is indeed a bug?\r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker @Cyrilvallez\r\n- vision models: @yonigozlan @molbap\r\n- audio models: @eustlb @ebezzam @vasqu\r\n- multimodal models: @zucchini-nlp\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- generate: @zucchini-nlp (visual-language models) or @gante (all others)\r\n- continuous batching: @remi-or @ArthurZucker @McPatate\r\n- pipelines: @Rocketknight1\r\n- tokenizers: @ArthurZucker and @itazap\r\n- trainer: @SunMarc\r\n- attention: @vasqu @ArthurZucker @CyrilVallez\r\n- model loading (from pretrained, etc): @CyrilVallez\r\n- distributed: @3outeille @ArthurZucker\r\n- CIs: @ydshieh\r\n\r\nIntegrations:\r\n\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization: @SunMarc @MekkCyber\r\n- kernels: @MekkCyber @drbh\r\n- peft: @BenjaminBossan @githubnemo\r\n\r\nDevices/Backends:\r\n\r\n- AMD ROCm: @ivarflakstad\r\n- Intel XPU: @IlyasMoutawwakil\r\n- Ascend NPU: @ivarflakstad \r\n\r\nDocumentation: @stevhliu\r\n\r\nResearch projects are not maintained and should be taken as is.\r\n\r\n -->\r\n",
    "html_url": "https://github.com/huggingface/transformers/pull/41647",
    "created_at": "2025-10-16T08:00:13Z",
    "merged_at": "2025-10-16T14:04:48Z",
    "merge_commit_sha": "8725ce10edb29771fb9a1aa108e6a04859efe973",
    "base_ref": "main",
    "head_sha": "c57e8b38dbb408c7ea96b3d1990a0e18654c810f",
    "user": "fjosw",
    "files": [
      {
        "filename": "src/transformers/models/deepseek_v3/modeling_deepseek_v3.py",
        "status": "modified",
        "additions": 5,
        "deletions": 3,
        "changes": 8,
        "patch": "@@ -176,9 +176,11 @@ def __init__(self, config):\n \n     def route_tokens_to_experts(self, router_logits):\n         router_logits = router_logits.sigmoid()\n-        router_logits = router_logits + self.gate.e_score_correction_bias\n+        router_logits_for_choice = router_logits + self.gate.e_score_correction_bias\n         group_scores = (\n-            router_logits.view(-1, self.n_group, self.n_routed_experts // self.n_group).topk(2, dim=-1)[0].sum(dim=-1)\n+            router_logits_for_choice.view(-1, self.n_group, self.n_routed_experts // self.n_group)\n+            .topk(2, dim=-1)[0]\n+            .sum(dim=-1)\n         )\n         group_idx = torch.topk(group_scores, k=self.topk_group, dim=-1, sorted=False)[1]\n         group_mask = torch.zeros_like(group_scores)\n@@ -188,7 +190,7 @@ def route_tokens_to_experts(self, router_logits):\n             .expand(-1, self.n_group, self.n_routed_experts // self.n_group)\n             .reshape(-1, self.n_routed_experts)\n         )\n-        scores_for_choice = router_logits.masked_fill(~score_mask.bool(), 0.0)\n+        scores_for_choice = router_logits_for_choice.masked_fill(~score_mask.bool(), 0.0)\n         topk_indices = torch.topk(scores_for_choice, k=self.top_k, dim=-1, sorted=False)[1]\n         topk_weights = router_logits.gather(1, topk_indices)\n         if self.norm_topk_prob:"
      },
      {
        "filename": "src/transformers/models/deepseek_v3/modular_deepseek_v3.py",
        "status": "modified",
        "additions": 5,
        "deletions": 3,
        "changes": 8,
        "patch": "@@ -132,9 +132,11 @@ def __init__(self, config):\n \n     def route_tokens_to_experts(self, router_logits):\n         router_logits = router_logits.sigmoid()\n-        router_logits = router_logits + self.gate.e_score_correction_bias\n+        router_logits_for_choice = router_logits + self.gate.e_score_correction_bias\n         group_scores = (\n-            router_logits.view(-1, self.n_group, self.n_routed_experts // self.n_group).topk(2, dim=-1)[0].sum(dim=-1)\n+            router_logits_for_choice.view(-1, self.n_group, self.n_routed_experts // self.n_group)\n+            .topk(2, dim=-1)[0]\n+            .sum(dim=-1)\n         )\n         group_idx = torch.topk(group_scores, k=self.topk_group, dim=-1, sorted=False)[1]\n         group_mask = torch.zeros_like(group_scores)\n@@ -144,7 +146,7 @@ def route_tokens_to_experts(self, router_logits):\n             .expand(-1, self.n_group, self.n_routed_experts // self.n_group)\n             .reshape(-1, self.n_routed_experts)\n         )\n-        scores_for_choice = router_logits.masked_fill(~score_mask.bool(), 0.0)\n+        scores_for_choice = router_logits_for_choice.masked_fill(~score_mask.bool(), 0.0)\n         topk_indices = torch.topk(scores_for_choice, k=self.top_k, dim=-1, sorted=False)[1]\n         topk_weights = router_logits.gather(1, topk_indices)\n         if self.norm_topk_prob:"
      },
      {
        "filename": "src/transformers/models/glm4_moe/modeling_glm4_moe.py",
        "status": "modified",
        "additions": 5,
        "deletions": 3,
        "changes": 8,
        "patch": "@@ -319,9 +319,11 @@ def __init__(self, config):\n \n     def route_tokens_to_experts(self, router_logits):\n         router_logits = router_logits.sigmoid()\n-        router_logits = router_logits + self.gate.e_score_correction_bias\n+        router_logits_for_choice = router_logits + self.gate.e_score_correction_bias\n         group_scores = (\n-            router_logits.view(-1, self.n_group, self.n_routed_experts // self.n_group).topk(2, dim=-1)[0].sum(dim=-1)\n+            router_logits_for_choice.view(-1, self.n_group, self.n_routed_experts // self.n_group)\n+            .topk(2, dim=-1)[0]\n+            .sum(dim=-1)\n         )\n         group_idx = torch.topk(group_scores, k=self.topk_group, dim=-1, sorted=False)[1]\n         group_mask = torch.zeros_like(group_scores)\n@@ -331,7 +333,7 @@ def route_tokens_to_experts(self, router_logits):\n             .expand(-1, self.n_group, self.n_routed_experts // self.n_group)\n             .reshape(-1, self.n_routed_experts)\n         )\n-        scores_for_choice = router_logits.masked_fill(~score_mask.bool(), 0.0)\n+        scores_for_choice = router_logits_for_choice.masked_fill(~score_mask.bool(), 0.0)\n         topk_indices = torch.topk(scores_for_choice, k=self.top_k, dim=-1, sorted=False)[1]\n         topk_weights = router_logits.gather(1, topk_indices)\n         if self.norm_topk_prob:"
      },
      {
        "filename": "src/transformers/models/glm4v_moe/modeling_glm4v_moe.py",
        "status": "modified",
        "additions": 5,
        "deletions": 3,
        "changes": 8,
        "patch": "@@ -314,9 +314,11 @@ def __init__(self, config: Glm4vMoeTextConfig):\n \n     def route_tokens_to_experts(self, router_logits):\n         router_logits = router_logits.sigmoid()\n-        router_logits = router_logits + self.gate.e_score_correction_bias\n+        router_logits_for_choice = router_logits + self.gate.e_score_correction_bias\n         group_scores = (\n-            router_logits.view(-1, self.n_group, self.n_routed_experts // self.n_group).topk(2, dim=-1)[0].sum(dim=-1)\n+            router_logits_for_choice.view(-1, self.n_group, self.n_routed_experts // self.n_group)\n+            .topk(2, dim=-1)[0]\n+            .sum(dim=-1)\n         )\n         group_idx = torch.topk(group_scores, k=self.topk_group, dim=-1, sorted=False)[1]\n         group_mask = torch.zeros_like(group_scores)\n@@ -326,7 +328,7 @@ def route_tokens_to_experts(self, router_logits):\n             .expand(-1, self.n_group, self.n_routed_experts // self.n_group)\n             .reshape(-1, self.n_routed_experts)\n         )\n-        scores_for_choice = router_logits.masked_fill(~score_mask.bool(), 0.0)\n+        scores_for_choice = router_logits_for_choice.masked_fill(~score_mask.bool(), 0.0)\n         topk_indices = torch.topk(scores_for_choice, k=self.top_k, dim=-1, sorted=False)[1]\n         topk_weights = router_logits.gather(1, topk_indices)\n         if self.norm_topk_prob:"
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T21:17:21.679826",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR fixes a non-trivial bug in the expert routing logic of multiple MOE (Mixture of Experts) models. The fix involves understanding the distinction between using biased vs. unbiased logits for different stages of the routing algorithm\u2014a nuanced algorithmic detail grounded in the Deepseek-V3 technical report. The bug affects core model behavior and requires developers to understand how expert selection and gating work in MOE architectures.",
      "substance_level": "high"
    }
  },
  {
    "pr_number": 41635,
    "title": "Enforce check_auto_docstring",
    "body": "# What does this PR do?\r\n\r\nFix some small issues with auto_docstring and raise an error instead of just warning if something is wrong when running check_auto_docstring",
    "html_url": "https://github.com/huggingface/transformers/pull/41635",
    "created_at": "2025-10-15T18:07:22Z",
    "merged_at": "2025-11-11T16:05:55Z",
    "merge_commit_sha": "df45a92cea0385970fabb21f8c329150cc3d5a9c",
    "base_ref": "main",
    "head_sha": "f2ed1da694d955a14f66d6ab7bcc16d09cdb9d4e",
    "user": "yonigozlan",
    "files": [
      {
        "filename": "src/transformers/models/deprecated/gptsan_japanese/modeling_gptsan_japanese.py",
        "status": "modified",
        "additions": 1,
        "deletions": 2,
        "changes": 3,
        "patch": "@@ -22,7 +22,7 @@\n from ....cache_utils import Cache\n from ....modeling_outputs import MoECausalLMOutputWithPast, MoEModelOutputWithPastAndCrossAttentions\n from ....modeling_utils import PreTrainedModel\n-from ....utils import DUMMY_INPUTS, DUMMY_MASK, auto_docstring\n+from ....utils import DUMMY_INPUTS, DUMMY_MASK\n from .configuration_gptsan_japanese import GPTSanJapaneseConfig\n \n \n@@ -635,7 +635,6 @@ def __init__(self, config: GPTSanJapaneseConfig):\n     def set_input_embeddings(self, new_embeddings):\n         self.embed_tokens = new_embeddings\n \n-    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,"
      },
      {
        "filename": "src/transformers/models/nougat/tokenization_nougat_fast.py",
        "status": "modified",
        "additions": 0,
        "deletions": 13,
        "changes": 13,
        "patch": "@@ -23,9 +23,7 @@\n \n import numpy as np\n \n-from transformers.tokenization_utils_base import INIT_TOKENIZER_DOCSTRING\n from transformers.tokenization_utils_fast import PreTrainedTokenizerFast\n-from transformers.utils import add_end_docstrings\n \n from ...utils import is_levenshtein_available, is_nltk_available, logging, requires_backends\n \n@@ -40,16 +38,6 @@\n logger = logging.get_logger(__name__)\n \n \n-INIT_TOKENIZER_DOCSTRING += \"\"\"\n-        tokenizer_object ([`tokenizers.Tokenizer`]):\n-            A [`tokenizers.Tokenizer`] object from \ud83e\udd17 tokenizers to instantiate from. See [Using tokenizers from \ud83e\udd17\n-            tokenizers](../fast_tokenizers) for more information.\n-        tokenizer_file ([`str`]):\n-            A path to a local JSON file representing a previously serialized [`tokenizers.Tokenizer`] object from \ud83e\udd17\n-            tokenizers.\n-\"\"\"\n-\n-\n VOCAB_FILES_NAMES = {\"tokenizer_file\": \"tokenizer.json\"}\n \n \n@@ -358,7 +346,6 @@ def remove_slice_from_lines(lines, clean_text, slice) -> str:\n     return to_delete.strip()\n \n \n-@add_end_docstrings(INIT_TOKENIZER_DOCSTRING)\n class NougatTokenizerFast(PreTrainedTokenizerFast):\n     \"\"\"\n     Fast tokenizer for Nougat (backed by HuggingFace tokenizers library)."
      },
      {
        "filename": "src/transformers/utils/auto_docstring.py",
        "status": "modified",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "patch": "@@ -1729,9 +1729,9 @@ def auto_method_docstring(\n     model_name_lowercase, class_name, config_class = _get_model_info(func, parent_class)\n     func_documentation = func.__doc__\n     if custom_args is not None and func_documentation is not None:\n-        func_documentation = set_min_indent(custom_args, indent_level + 4) + \"\\n\" + func_documentation\n+        func_documentation = \"\\n\" + set_min_indent(custom_args.strip(\"\\n\"), 0) + \"\\n\" + func_documentation\n     elif custom_args is not None:\n-        func_documentation = custom_args\n+        func_documentation = \"\\n\" + set_min_indent(custom_args.strip(\"\\n\"), 0)\n \n     # Add intro to the docstring before args description if needed\n     if custom_intro is not None:"
      },
      {
        "filename": "utils/check_docstrings.py",
        "status": "modified",
        "additions": 4,
        "deletions": 0,
        "changes": 4,
        "patch": "@@ -1378,6 +1378,10 @@ def check_auto_docstrings(overwrite: bool = False, check_all: bool = False):\n             print(f\"[ERROR] Docstring needs to be filled for the following arguments in {candidate_file}:\")\n             for warning in fill_docstring_args_warnings:\n                 print(warning)\n+        if missing_docstring_args_warnings or docstring_args_ro_remove_warnings or fill_docstring_args_warnings:\n+            raise ValueError(\n+                \"There was at least one problem when checking docstrings of objects decorated with @auto_docstring.\"\n+            )\n \n \n def check_docstrings(overwrite: bool = False, check_all: bool = False):"
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T21:17:24.183638",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR contains meaningful code changes to enforce auto_docstring validation with actual logic modifications: it removes deprecated decorator usage, fixes docstring formatting logic, and adds error-raising behavior instead of silent warnings. The changes involve understanding how the auto_docstring system works and why enforcement matters, providing enough substance for technical questions about the codebase.",
      "substance_level": "medium"
    }
  },
  {
    "pr_number": 41626,
    "title": "[v5] Return a BatchEncoding dict from apply_chat_template by default",
    "body": "Tokenizers return a BatchEncoding dict by default, but `apply_chat_template` doesn't. This is just an accident of how I wrote it originally, which we were stuck with for backward compatibility reasons. Ideally, I think `apply_chat_template` should return exactly the same format as tokenizers, since it also performs tokenization most of the time. It's now `v5` time, so we can start making that happen :sweat_smile:\r\n\r\nThis PR also updates tests, and removes very old `test_tokenization_for_chat` tests. These model-specific tests don't do anything useful anymore, since the `apply_chat_template` functionality is unified across tokenizers; they're mostly a legacy leftover from when model classes used to need custom chat tokenization functions.",
    "html_url": "https://github.com/huggingface/transformers/pull/41626",
    "created_at": "2025-10-15T15:00:46Z",
    "merged_at": "2025-10-31T13:50:26Z",
    "merge_commit_sha": "5f8d02f2f12e771d59d473702bc61a7e7c4a6255",
    "base_ref": "main",
    "head_sha": "137015ec595cd3627825ea0e84201ad3b3c5a153",
    "user": "Rocketknight1",
    "files": [
      {
        "filename": "src/transformers/models/voxtral/processing_voxtral.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -206,7 +206,7 @@ def apply_chat_template(\n         tokenizer_kwargs = {**processed_kwargs[\"template_kwargs\"], **text_kwargs}\n         tokenizer_kwargs[\"return_tensors\"] = None  # let's not return tensors here\n         tokenize = tokenizer_kwargs.pop(\"tokenize\", False)\n-        return_dict = tokenizer_kwargs.pop(\"return_dict\", False)\n+        return_dict = tokenizer_kwargs.pop(\"return_dict\", True)\n \n         encoded_instruct_inputs = self.tokenizer.apply_chat_template(\n             conversations,"
      },
      {
        "filename": "src/transformers/processing_utils.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -1603,7 +1603,7 @@ def apply_chat_template(\n             conversations = [conversation]\n \n         tokenize = processed_kwargs[\"template_kwargs\"].pop(\"tokenize\", False)\n-        return_dict = processed_kwargs[\"template_kwargs\"].pop(\"return_dict\", False)\n+        return_dict = processed_kwargs[\"template_kwargs\"].pop(\"return_dict\", True)\n         mm_load_kwargs = processed_kwargs[\"mm_load_kwargs\"]\n \n         if tokenize:"
      },
      {
        "filename": "src/transformers/tokenization_mistral_common.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -1378,7 +1378,7 @@ def apply_chat_template(\n         truncation: bool = False,\n         max_length: Optional[int] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n-        return_dict: bool = False,\n+        return_dict: bool = True,\n         **kwargs,\n     ) -> Union[str, list[int], list[str], list[list[int]], BatchEncoding]:\n         \"\"\""
      },
      {
        "filename": "src/transformers/tokenization_utils_base.py",
        "status": "modified",
        "additions": 12,
        "deletions": 11,
        "changes": 23,
        "patch": "@@ -1588,7 +1588,7 @@ def apply_chat_template(\n         truncation: bool = False,\n         max_length: Optional[int] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n-        return_dict: bool = False,\n+        return_dict: bool = True,\n         return_assistant_tokens_mask: bool = False,\n         tokenizer_kwargs: Optional[dict[str, Any]] = None,\n         **kwargs,\n@@ -1661,14 +1661,11 @@ def apply_chat_template(\n             set, will return a dict of tokenizer outputs instead.\n         \"\"\"\n \n-        if return_dict and not tokenize:\n-            raise ValueError(\n-                \"`return_dict=True` is incompatible with `tokenize=False`, because there is no dict \"\n-                \"of tokenizer outputs to return.\"\n-            )\n+        if not tokenize:\n+            return_dict = False  # dicts are only returned by the tokenizer anyway\n \n-        if return_assistant_tokens_mask and not return_dict:\n-            raise ValueError(\"`return_assistant_tokens_mask=True` is incompatible with `return_dict=False`\")\n+        if return_assistant_tokens_mask and not (return_dict and tokenize):\n+            raise ValueError(\"`return_assistant_tokens_mask=True` requires `return_dict=True` and `tokenize=True`\")\n \n         if tokenizer_kwargs is None:\n             tokenizer_kwargs = {}\n@@ -1783,13 +1780,17 @@ def encode_message_with_chat_template(\n             )\n \n         if conversation_history is None or len(conversation_history) == 0:\n-            return self.apply_chat_template([message], add_generation_prompt=False, tokenize=True, **kwargs)\n+            return self.apply_chat_template(\n+                [message], add_generation_prompt=False, tokenize=True, return_dict=False, **kwargs\n+            )\n \n         conversation = conversation_history + [message]\n-        tokens = self.apply_chat_template(conversation, add_generation_prompt=False, tokenize=True, **kwargs)\n+        tokens = self.apply_chat_template(\n+            conversation, add_generation_prompt=False, tokenize=True, return_dict=False, **kwargs\n+        )\n \n         prefix_tokens = self.apply_chat_template(\n-            conversation_history, add_generation_prompt=False, tokenize=True, **kwargs\n+            conversation_history, add_generation_prompt=False, tokenize=True, return_dict=False, **kwargs\n         )\n         # It's possible that the prefix tokens are not a prefix of the full list of tokens.\n         # For example, if the prefix is `<s>User: Hi` and the full conversation is `<s>User: Hi</s><s>Assistant: Hello`."
      },
      {
        "filename": "tests/models/blenderbot/test_tokenization_blenderbot.py",
        "status": "modified",
        "additions": 0,
        "deletions": 22,
        "changes": 22,
        "patch": "@@ -18,7 +18,6 @@\n from functools import cached_property\n \n from transformers import BlenderbotTokenizer, BlenderbotTokenizerFast\n-from transformers.testing_utils import require_jinja\n \n \n class Blenderbot3BTokenizerTests(unittest.TestCase):\n@@ -51,24 +50,3 @@ def test_3B_tokenization_same_as_parlai(self):\n     def test_3B_tokenization_same_as_parlai_rust_tokenizer(self):\n         assert self.rust_tokenizer_3b.add_prefix_space\n         assert self.rust_tokenizer_3b([\" Sam\", \"Sam\"]).input_ids == [[5502, 2], [5502, 2]]\n-\n-    @require_jinja\n-    def test_tokenization_for_chat(self):\n-        tok = self.tokenizer_3b\n-        test_chats = [\n-            [{\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"}, {\"role\": \"user\", \"content\": \"Hello!\"}],\n-            [\n-                {\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"},\n-                {\"role\": \"user\", \"content\": \"Hello!\"},\n-                {\"role\": \"assistant\", \"content\": \"Nice to meet you.\"},\n-            ],\n-            [{\"role\": \"assistant\", \"content\": \"Nice to meet you.\"}, {\"role\": \"user\", \"content\": \"Hello!\"}],\n-        ]\n-        tokenized_chats = [tok.apply_chat_template(test_chat) for test_chat in test_chats]\n-        expected_tokens = [\n-            [553, 366, 265, 4792, 3879, 73, 311, 21, 228, 228, 6950, 8, 2],\n-            [553, 366, 265, 4792, 3879, 73, 311, 21, 228, 228, 6950, 8, 228, 3490, 287, 2273, 304, 21, 2],\n-            [3490, 287, 2273, 304, 21, 228, 228, 6950, 8, 2],\n-        ]\n-        for tokenized_chat, expected_tokens in zip(tokenized_chats, expected_tokens):\n-            self.assertListEqual(tokenized_chat, expected_tokens)"
      },
      {
        "filename": "tests/models/bloom/test_tokenization_bloom.py",
        "status": "modified",
        "additions": 1,
        "deletions": 23,
        "changes": 24,
        "patch": "@@ -18,7 +18,7 @@\n from datasets import load_dataset\n \n from transformers import BloomTokenizerFast\n-from transformers.testing_utils import require_jinja, require_tokenizers\n+from transformers.testing_utils import require_tokenizers\n \n from ...test_tokenization_common import TokenizerTesterMixin\n \n@@ -137,28 +137,6 @@ def test_encodings_from_xnli_dataset(self):\n         predicted_text = [tokenizer.decode(x, clean_up_tokenization_spaces=False) for x in output_tokens]\n         self.assertListEqual(predicted_text, input_text)\n \n-    @require_jinja\n-    def test_tokenization_for_chat(self):\n-        tokenizer = self.get_rust_tokenizer()\n-        tokenizer.chat_template = \"{% for message in messages %}{{ message.content }}{{ eos_token }}{% endfor %}\"\n-        test_chats = [\n-            [{\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"}, {\"role\": \"user\", \"content\": \"Hello!\"}],\n-            [\n-                {\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"},\n-                {\"role\": \"user\", \"content\": \"Hello!\"},\n-                {\"role\": \"assistant\", \"content\": \"Nice to meet you.\"},\n-            ],\n-            [{\"role\": \"assistant\", \"content\": \"Nice to meet you.\"}, {\"role\": \"user\", \"content\": \"Hello!\"}],\n-        ]\n-        tokenized_chats = [tokenizer.apply_chat_template(test_chat) for test_chat in test_chats]\n-        expected_tokens = [\n-            [5448, 1306, 267, 66799, 44799, 37143, 17, 2, 59414, 4, 2],\n-            [5448, 1306, 267, 66799, 44799, 37143, 17, 2, 59414, 4, 2, 229126, 427, 11890, 1152, 17, 2],\n-            [229126, 427, 11890, 1152, 17, 2, 59414, 4, 2],\n-        ]\n-        for tokenized_chat, expected_tokens in zip(tokenized_chats, expected_tokens):\n-            self.assertListEqual(tokenized_chat, expected_tokens)\n-\n     def test_add_prefix_space_fast(self):\n         tokenizer_w_prefix = self.get_rust_tokenizer(add_prefix_space=True)\n         tokenizer_wo_prefix = self.get_rust_tokenizer(add_prefix_space=False)"
      },
      {
        "filename": "tests/models/cohere/test_tokenization_cohere.py",
        "status": "modified",
        "additions": 0,
        "deletions": 26,
        "changes": 26,
        "patch": "@@ -146,32 +146,6 @@ def test_pretrained_model_lists(self):\n         self.assertGreaterEqual(len(self.tokenizer_class.pretrained_vocab_files_map), 1)\n         self.assertGreaterEqual(len(list(self.tokenizer_class.pretrained_vocab_files_map.values())[0]), 1)\n \n-    @require_jinja\n-    def test_tokenization_for_chat(self):\n-        tokenizer = self.get_rust_tokenizer()\n-        test_chats = [\n-            [{\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"}, {\"role\": \"user\", \"content\": \"Hello!\"}],\n-            [\n-                {\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"},\n-                {\"role\": \"user\", \"content\": \"Hello!\"},\n-                {\"role\": \"assistant\", \"content\": \"Nice to meet you.\"},\n-            ],\n-        ]\n-        tokenized_chats = [tokenizer.apply_chat_template(test_chat) for test_chat in test_chats]\n-        # fmt: off\n-        expected_tokens = [\n-            [5, 36, 99, 59, 60, 41, 58, 60, 71, 55, 46, 71, 60, 61, 58, 54, 71, 60, 55, 51, 45, 54, 99, 38, 36, 99, 59, 65, 59, 60, 45, 53, 71, 60, 55, 51, 45, 54, 99, 38, 65, 243, 394, 204, 336, 84, 88, 887, 374, 216, 74, 286, 22, 8, 36, 99, 59, 60, 41, 58, 60, 71, 55, 46, 71, 60, 61, 58, 54, 71, 60, 55, 51, 45, 54, 99, 38, 36, 99, 61, 59, 45, 58, 71, 60, 55, 51, 45, 54, 99, 38, 48, 420, 87, 9, 8],\n-            [5, 36, 99, 59, 60, 41, 58, 60, 71, 55, 46, 71, 60, 61, 58, 54, 71, 60, 55, 51, 45, 54, 99, 38, 36, 99, 59, 65,\n-            59, 60, 45, 53, 71, 60, 55, 51, 45, 54, 99, 38, 65, 243, 394, 204, 336, 84, 88, 887, 374, 216, 74, 286, 22, 8,\n-            36, 99, 59, 60, 41, 58, 60, 71, 55, 46, 71, 60, 61, 58, 54, 71, 60, 55, 51, 45, 54, 99, 38, 36, 99, 61, 59,\n-            45, 58, 71, 60, 55, 51, 45, 54, 99, 38, 48, 420, 87, 9, 8, 36, 99, 59, 60, 41, 58, 60, 71, 55, 46, 71, 60, 61,\n-            58, 54, 71, 60, 55, 51, 45, 54, 99, 38, 36, 99, 43, 48, 41, 60, 42, 55, 60, 71, 60, 55, 51, 45, 54, 99, 38,\n-            54, 567, 235, 693, 276, 411, 243, 22, 8]\n-        ]\n-        # fmt: on\n-        for tokenized_chat, expected_tokens in zip(tokenized_chats, expected_tokens):\n-            self.assertListEqual(tokenized_chat, expected_tokens)\n-\n     @require_jinja\n     def test_tokenization_for_tool_use(self):\n         tokenizer = self.get_rust_tokenizer()"
      },
      {
        "filename": "tests/models/gemma/test_tokenization_gemma.py",
        "status": "modified",
        "additions": 0,
        "deletions": 20,
        "changes": 20,
        "patch": "@@ -27,7 +27,6 @@\n from transformers.testing_utils import (\n     get_tests_dir,\n     nested_simplify,\n-    require_jinja,\n     require_read_token,\n     require_sentencepiece,\n     require_tokenizers,\n@@ -428,25 +427,6 @@ def test_some_edge_cases(self):\n         # a dummy prefix space is not added by the sp_model as it was de-activated\n         self.assertEqual(tokens, tokenizer.sp_model.encode(\"\u2581\u2581\", out_type=str))\n \n-    @require_jinja\n-    def test_tokenization_for_chat(self):\n-        tokenizer = GemmaTokenizer.from_pretrained(\"hf-internal-testing/dummy-gemma\")\n-\n-        test_chats = [\n-            [{\"role\": \"user\", \"content\": \"Hello!\"}],\n-            [\n-                {\"role\": \"user\", \"content\": \"Hello!\"},\n-                {\"role\": \"assistant\", \"content\": \"Nice to meet you.\"},\n-            ],\n-            [{\"role\": \"user\", \"content\": \"Hello!\"}],\n-        ]\n-        # Matt: The third test case tests the default system message, but if this is ever changed in the\n-        #       class/repo code then that test will fail, and the case will need to be updated.\n-        tokenized_chats = [tokenizer.apply_chat_template(test_chat) for test_chat in test_chats]\n-        expected_tokens = [[235322, 235371, 571, 235298, 2997, 73786, 1645, 108, 4521, 149907, 235371, 571, 235298, 615, 73786, 108], [235322, 235371, 571, 235298, 2997, 73786, 1645, 108, 4521, 149907, 235371, 571, 235298, 615, 73786, 108, 235322, 235371, 571, 235298, 2997, 73786, 105776, 108, 7731, 577, 4664, 692, 35606, 235371, 571, 235298, 615, 73786, 108], [235322, 235371, 571, 235298, 2997, 73786, 1645, 108, 4521, 149907, 235371, 571, 235298, 615, 73786, 108]]  # fmt: skip\n-        for tokenized_chat, expected_tokens in zip(tokenized_chats, expected_tokens):\n-            self.assertListEqual(tokenized_chat, expected_tokens)\n-\n     def test_save_fast_load_slow(self):\n         # Ensure that we can save a fast tokenizer and load it as a slow tokenizer\n         slow_tokenizer = self.tokenizer"
      },
      {
        "filename": "tests/models/gpt2/test_tokenization_gpt2.py",
        "status": "modified",
        "additions": 1,
        "deletions": 23,
        "changes": 24,
        "patch": "@@ -19,7 +19,7 @@\n \n from transformers import AutoTokenizer, GPT2Tokenizer, GPT2TokenizerFast\n from transformers.models.gpt2.tokenization_gpt2 import VOCAB_FILES_NAMES\n-from transformers.testing_utils import require_jinja, require_tiktoken, require_tokenizers\n+from transformers.testing_utils import require_tiktoken, require_tokenizers\n \n from ...test_tokenization_common import TokenizerTesterMixin\n \n@@ -281,28 +281,6 @@ def test_special_tokens_mask_input_pairs_and_bos_token(self):\n                 filtered_sequence = [x for x in filtered_sequence if x is not None]\n                 self.assertEqual(encoded_sequence, filtered_sequence)\n \n-    @require_jinja\n-    def test_tokenization_for_chat(self):\n-        tokenizer = GPT2Tokenizer.from_pretrained(self.tmpdirname)\n-        tokenizer.chat_template = \"{% for message in messages %}{{ message.content }}{{ eos_token }}{% endfor %}\"\n-        test_chats = [\n-            [{\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"}, {\"role\": \"user\", \"content\": \"Hello!\"}],\n-            [\n-                {\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"},\n-                {\"role\": \"user\", \"content\": \"Hello!\"},\n-                {\"role\": \"assistant\", \"content\": \"Nice to meet you.\"},\n-            ],\n-            [{\"role\": \"assistant\", \"content\": \"Nice to meet you.\"}, {\"role\": \"user\", \"content\": \"Hello!\"}],\n-        ]\n-        tokenized_chats = [tokenizer.apply_chat_template(test_chat) for test_chat in test_chats]\n-        # fmt: off\n-        expected_tokens = [[20, 1, 20, 10, 20, 4, 3, 10, 20, 10, 20, 3, 0, 20, 20, 20, 0, 10, 20, 20, 20, 6, 20, 1, 6, 20, 20, 20, 3, 0, 0, 1, 20, 20],\n-                          [20, 1, 20, 10, 20, 4, 3, 10, 20, 10, 20, 3, 0, 20, 20, 20, 0, 10, 20, 20, 20, 6, 20, 1, 6, 20, 20, 20, 3, 0, 0, 1, 20, 20, 20, 7, 20, 3, 10, 6, 1, 10, 20, 3, 3, 6, 10, 20, 1, 20, 20, 20],\n-                          [20, 7, 20, 3, 10, 6, 1, 10, 20, 3, 3, 6, 10, 20, 1, 20, 20, 20, 20, 3, 0, 0, 1, 20, 20]]\n-        # fmt: on\n-        for tokenized_chat, expected_tokens in zip(tokenized_chats, expected_tokens):\n-            self.assertListEqual(tokenized_chat, expected_tokens)\n-\n     @require_tiktoken\n     def test_tokenization_tiktoken(self):\n         from tiktoken import encoding_name_for_model"
      },
      {
        "filename": "tests/models/gpt_sw3/test_tokenization_gpt_sw3.py",
        "status": "modified",
        "additions": 1,
        "deletions": 34,
        "changes": 35,
        "patch": "@@ -15,7 +15,7 @@\n import unittest\n \n from transformers import GPTSw3Tokenizer\n-from transformers.testing_utils import get_tests_dir, require_jinja, require_sentencepiece, require_tokenizers, slow\n+from transformers.testing_utils import get_tests_dir, require_sentencepiece, require_tokenizers, slow\n \n from ...test_tokenization_common import TokenizerTesterMixin\n \n@@ -127,36 +127,3 @@ def test_tokenizer_integration(self):\n             model_name=\"AI-Sweden-Models/gpt-sw3-126m\",\n             sequences=sequences,\n         )\n-\n-    @require_jinja\n-    def test_tokenization_for_chat(self):\n-        tokenizer = GPTSw3Tokenizer(SAMPLE_VOCAB)\n-        tokenizer.chat_template = (\n-            \"{{ eos_token }}{{ bos_token }}\"\n-            \"{% for message in messages %}\"\n-            \"{% if message['role'] == 'user' %}{{ 'User: ' + message['content']}}\"\n-            \"{% else %}{{ 'Bot: ' + message['content']}}{% endif %}\"\n-            \"{{ message['text'] }}{{ bos_token }}\"\n-            \"{% endfor %}\"\n-            \"Bot:\"\n-        )\n-        # This is in English, but it's just here to make sure the chat control tokens are being added properly\n-        test_chats = [\n-            [{\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"}, {\"role\": \"user\", \"content\": \"Hello!\"}],\n-            [\n-                {\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"},\n-                {\"role\": \"user\", \"content\": \"Hello!\"},\n-                {\"role\": \"assistant\", \"content\": \"Nice to meet you.\"},\n-            ],\n-            [{\"role\": \"assistant\", \"content\": \"Nice to meet you.\"}, {\"role\": \"user\", \"content\": \"Hello!\"}],\n-        ]\n-        tokenized_chats = [tokenizer.apply_chat_template(test_chat) for test_chat in test_chats]\n-        # fmt: off\n-        expected_tokens = [\n-            [2000, 1, 575, 541, 419, 530, 339, 265, 878, 708, 727, 275, 347, 541, 260, 1, 968, 263, 314, 419, 366, 354, 294, 360, 1, 575, 541, 419],\n-            [2000, 1, 575, 541, 419, 530, 339, 265, 878, 708, 727, 275, 347, 541, 260, 1, 968, 263, 314, 419, 366, 354, 294, 360, 1, 575, 541, 419, 984, 429, 281, 264, 1261, 291, 260, 1, 575, 541, 419],\n-            [2000, 1, 575, 541, 419, 984, 429, 281, 264, 1261, 291, 260, 1, 968, 263, 314, 419, 366, 354, 294, 360, 1, 575, 541, 419]\n-            ]\n-        # fmt: on\n-        for tokenized_chat, expected_tokens in zip(tokenized_chats, expected_tokens):\n-            self.assertListEqual(tokenized_chat, expected_tokens)"
      },
      {
        "filename": "tests/models/llama/test_tokenization_llama.py",
        "status": "modified",
        "additions": 0,
        "deletions": 27,
        "changes": 27,
        "patch": "@@ -32,7 +32,6 @@\n from transformers.testing_utils import (\n     get_tests_dir,\n     nested_simplify,\n-    require_jinja,\n     require_read_token,\n     require_sentencepiece,\n     require_tiktoken,\n@@ -702,32 +701,6 @@ def test_fast_post_processor(self):\n         with self.assertRaises(ValueError):\n             tokenizer = LlamaTokenizerFast(SAMPLE_VOCAB, eos_token=None, add_bos_token=True, add_eos_token=True)\n \n-    @require_jinja\n-    def test_tokenization_for_chat(self):\n-        tokenizer = LlamaTokenizer.from_pretrained(\"huggyllama/llama-7b\", legacy=False)\n-\n-        test_chats = [\n-            [{\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"}, {\"role\": \"user\", \"content\": \"Hello!\"}],\n-            [\n-                {\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"},\n-                {\"role\": \"user\", \"content\": \"Hello!\"},\n-                {\"role\": \"assistant\", \"content\": \"Nice to meet you.\"},\n-            ],\n-            [{\"role\": \"user\", \"content\": \"Hello!\"}],\n-        ]\n-        # Matt: The third test case tests the default system message, but if this is ever changed in the\n-        #       class/repo code then that test will fail, and the case will need to be updated.\n-        tokenized_chats = [tokenizer.apply_chat_template(test_chat) for test_chat in test_chats]\n-        # fmt: off\n-        expected_tokens = [\n-            [1, 29961, 25580, 29962, 3532, 14816, 29903, 6778, 13, 3492, 526, 263, 8444, 13563, 7451, 29889, 13, 29966, 829, 14816, 29903, 6778, 13, 13, 10994, 29991, 518, 29914, 25580, 29962],\n-            [1, 29961, 25580, 29962, 3532, 14816, 29903, 6778, 13, 3492, 526, 263, 8444, 13563, 7451, 29889, 13, 29966, 829, 14816, 29903, 6778, 13, 13, 10994, 29991, 518, 29914, 25580, 29962, 20103, 304, 5870, 366, 29889, 29871, 2],\n-            [1, 29961, 25580, 29962, 15043, 29991, 518, 29914, 25580, 29962]\n-        ]\n-        # fmt: on\n-        for tokenized_chat, expected_tokens in zip(tokenized_chats, expected_tokens):\n-            self.assertListEqual(tokenized_chat, expected_tokens)\n-\n \n @require_sentencepiece\n @require_tokenizers"
      },
      {
        "filename": "tests/test_tokenization_mistral_common.py",
        "status": "modified",
        "additions": 38,
        "deletions": 24,
        "changes": 62,
        "patch": "@@ -799,7 +799,9 @@ def test_apply_chat_template_basic(self):\n \n         # Test 2:\n         # without tokenize\n-        self.assertEqual(self.tokenizer.apply_chat_template(conversation, tokenize=True), expected_tokenized.tokens)\n+        self.assertEqual(\n+            self.tokenizer.apply_chat_template(conversation, tokenize=True).input_ids, expected_tokenized.tokens\n+        )\n \n         with self.assertRaises(\n             ValueError, msg=\"Kwargs [unk_args] are not supported by `MistralCommonTokenizer.apply_chat_template`.\"\n@@ -824,7 +826,7 @@ def test_apply_chat_template_continue_final_message(self):\n             expected_tokenized.text,\n         )\n         self.assertEqual(\n-            self.tokenizer.apply_chat_template(conversation, tokenize=True, continue_final_message=True),\n+            self.tokenizer.apply_chat_template(conversation, tokenize=True, continue_final_message=True).input_ids,\n             expected_tokenized.tokens,\n         )\n \n@@ -846,7 +848,7 @@ def test_apply_chat_template_with_add_generation_prompt(self):\n             token_outputs = self.tokenizer.apply_chat_template(\n                 conversation, tokenize=True, add_generation_prompt=add_generation_prompt\n             )\n-            self.assertEqual(token_outputs, expected_tokenized.tokens)\n+            self.assertEqual(token_outputs.input_ids, expected_tokenized.tokens)\n \n         # Test 2:\n         # with continue_final_message\n@@ -958,18 +960,16 @@ def test_apply_chat_template_with_image(self):\n                 },\n             ]\n \n-            output = self.tokenizer.apply_chat_template(conversation, tokenize=True)\n+            output = self.tokenizer.apply_chat_template(conversation).input_ids\n             self.assertEqual(output, expected_tokenized.tokens)\n \n-        output_dict = self.tokenizer.apply_chat_template(conversation, tokenize=True, return_dict=True)\n+        output_dict = self.tokenizer.apply_chat_template(conversation, tokenize=True)\n         self.assertEqual(output_dict[\"input_ids\"], expected_tokenized.tokens)\n         self.assertEqual(len(output_dict[\"pixel_values\"]), len(expected_tokenized.images))\n         for o, e in zip(output_dict[\"pixel_values\"], expected_tokenized.images):\n             self.assertTrue(np.allclose(o, e))\n \n-        output_dict = self.tokenizer.apply_chat_template(\n-            conversation, tokenize=True, return_dict=True, return_tensors=\"pt\"\n-        )\n+        output_dict = self.tokenizer.apply_chat_template(conversation, tokenize=True, return_tensors=\"pt\")\n         self.assertEqual(output_dict[\"input_ids\"].tolist()[0], expected_tokenized.tokens)\n         expected_images_pt_tensor = torch.from_numpy(np.stack(expected_tokenized.images))\n         self.assertTrue(torch.allclose(output_dict[\"pixel_values\"], expected_images_pt_tensor))\n@@ -1013,7 +1013,7 @@ def test_apply_chat_template_with_audio(self):\n                 },\n             ]\n \n-            output = self.tokenizer_audio.apply_chat_template(conversation, tokenize=True)\n+            output = self.tokenizer_audio.apply_chat_template(conversation, tokenize=True).input_ids\n             self.assertEqual(output, expected_tokenized.tokens)\n \n         output_dict = self.tokenizer_audio.apply_chat_template(conversation, tokenize=True, return_dict=True)\n@@ -1041,14 +1041,14 @@ def test_apply_chat_template_with_truncation(self):\n         # Test 1:\n         # with truncation\n         self.assertEqual(\n-            self.tokenizer.apply_chat_template(conversation, tokenize=True, truncation=True, max_length=20),\n+            self.tokenizer.apply_chat_template(conversation, tokenize=True, truncation=True, max_length=20).input_ids,\n             expected_tokenized.tokens[:20],\n         )\n \n         # Test 2:\n         # without truncation\n         self.assertEqual(\n-            self.tokenizer.apply_chat_template(conversation, tokenize=True, truncation=False, max_length=20),\n+            self.tokenizer.apply_chat_template(conversation, tokenize=True, truncation=False, max_length=20).input_ids,\n             expected_tokenized.tokens,\n         )\n \n@@ -1130,7 +1130,7 @@ def test_batch_apply_chat_template(self):\n         ]\n \n         text_outputs = self.tokenizer.apply_chat_template(conversations, tools=tools, tokenize=False)\n-        token_outputs = self.tokenizer.apply_chat_template(conversations, tools=tools, tokenize=True)\n+        token_outputs = self.tokenizer.apply_chat_template(conversations, tools=tools, tokenize=True).input_ids\n \n         self.assertEqual(len(text_outputs), len(token_outputs))\n         self.assertEqual(len(text_outputs), len(expected_tokenized))\n@@ -1202,7 +1202,7 @@ def test_batch_apply_chat_template_images(self):\n             ChatCompletionRequest.from_openai(ref_conversation)\n         )\n \n-        output = self.tokenizer.apply_chat_template(conversations, tokenize=True)\n+        output = self.tokenizer.apply_chat_template(conversations, tokenize=True).input_ids\n         self.assertEqual(output, [expected_tokenized.tokens] * 3)\n \n         output = self.tokenizer.apply_chat_template(conversations, tokenize=True, return_dict=True)\n@@ -1248,7 +1248,9 @@ def test_batch_apply_chat_template_with_continue_final_message(self):\n             for conversation in conversations\n         ]\n \n-        token_outputs = self.tokenizer.apply_chat_template(conversations, tokenize=True, continue_final_message=True)\n+        token_outputs = self.tokenizer.apply_chat_template(\n+            conversations, tokenize=True, continue_final_message=True\n+        ).input_ids\n \n         for output, expected in zip(token_outputs, expected_tokenized):\n             self.assertEqual(output, expected.tokens)\n@@ -1297,7 +1299,7 @@ def test_batch_apply_chat_template_with_add_generation_prompt(self):\n             ]\n             token_outputs = self.tokenizer.apply_chat_template(\n                 conversations, tokenize=True, add_generation_prompt=add_generation_prompt\n-            )\n+            ).input_ids\n             for output, expected in zip(token_outputs, expected_tokenized):\n                 self.assertEqual(output, expected.tokens)\n \n@@ -1331,7 +1333,7 @@ def test_batch_apply_chat_template_with_truncation(\n         # with truncation\n         token_outputs = self.tokenizer.apply_chat_template(\n             self.fixture_conversations, tokenize=True, truncation=True, max_length=20\n-        )\n+        ).input_ids\n \n         for output, expected in zip(token_outputs, self.tokenized_fixture_conversations):\n             self.assertEqual(output, expected.tokens[:20])\n@@ -1340,7 +1342,7 @@ def test_batch_apply_chat_template_with_truncation(\n         # without truncation\n         token_outputs = self.tokenizer.apply_chat_template(\n             self.fixture_conversations, tokenize=True, truncation=False, max_length=20\n-        )\n+        ).input_ids\n         self.assertEqual(len(token_outputs), len(self.tokenized_fixture_conversations))\n         for output, expected in zip(token_outputs, self.tokenized_fixture_conversations):\n             self.assertEqual(output, expected.tokens)\n@@ -1358,15 +1360,17 @@ def test_batch_apply_chat_template_with_padding(\n         for padding in [True, \"max_length\", PaddingStrategy.LONGEST, PaddingStrategy.MAX_LENGTH]:\n             if padding == PaddingStrategy.MAX_LENGTH:\n                 # No padding if no max length is provided\n-                token_outputs = self.tokenizer.apply_chat_template(self.fixture_conversations, padding=padding)\n+                token_outputs = self.tokenizer.apply_chat_template(\n+                    self.fixture_conversations, padding=padding, return_dict=False\n+                )\n                 self.assertEqual(len(token_outputs), len(self.tokenized_fixture_conversations))\n                 for output, expected in zip(token_outputs, self.tokenized_fixture_conversations):\n                     self.assertEqual(output, expected.tokens)\n \n             max_length = 20 if padding == PaddingStrategy.MAX_LENGTH else None\n \n             token_outputs = self.tokenizer.apply_chat_template(\n-                self.fixture_conversations, tokenize=True, padding=padding, max_length=max_length\n+                self.fixture_conversations, tokenize=True, padding=padding, max_length=max_length, return_dict=False\n             )\n \n             if padding != PaddingStrategy.MAX_LENGTH:\n@@ -1390,7 +1394,7 @@ def test_batch_apply_chat_template_with_padding(\n \n         for padding in [False, \"do_not_pad\", PaddingStrategy.DO_NOT_PAD]:\n             token_outputs = self.tokenizer.apply_chat_template(\n-                self.fixture_conversations, tokenize=True, padding=padding\n+                self.fixture_conversations, tokenize=True, padding=padding, return_dict=False\n             )\n             self.assertEqual(len(token_outputs), len(self.tokenized_fixture_conversations))\n             for output, expected in zip(token_outputs, self.tokenized_fixture_conversations):\n@@ -1402,7 +1406,12 @@ def test_batch_apply_chat_template_with_padding_and_truncation(\n         max_length = 20\n         for padding in [True, \"max_length\", PaddingStrategy.LONGEST, PaddingStrategy.MAX_LENGTH]:\n             token_outputs = self.tokenizer.apply_chat_template(\n-                self.fixture_conversations, tokenize=True, truncation=True, padding=padding, max_length=max_length\n+                self.fixture_conversations,\n+                tokenize=True,\n+                truncation=True,\n+                padding=padding,\n+                max_length=max_length,\n+                return_dict=False,\n             )\n             self.assertEqual(len(token_outputs), len(self.tokenized_fixture_conversations))\n             for output, expected in zip(token_outputs, self.tokenized_fixture_conversations):\n@@ -1411,7 +1420,12 @@ def test_batch_apply_chat_template_with_padding_and_truncation(\n                 )\n         for padding in [False, \"do_not_pad\", PaddingStrategy.DO_NOT_PAD]:\n             token_outputs = self.tokenizer.apply_chat_template(\n-                self.fixture_conversations, tokenize=True, truncation=True, padding=padding, max_length=max_length\n+                self.fixture_conversations,\n+                tokenize=True,\n+                truncation=True,\n+                padding=padding,\n+                max_length=max_length,\n+                return_dict=False,\n             )\n             self.assertEqual(len(token_outputs), len(self.tokenized_fixture_conversations))\n             for output, expected in zip(token_outputs, self.tokenized_fixture_conversations):\n@@ -1421,7 +1435,7 @@ def test_batch_apply_chat_template_return_tensors(self):\n         # Test 1:\n         # with tokenize\n         token_outputs = self.tokenizer.apply_chat_template(\n-            self.fixture_conversations, tokenize=True, return_tensors=\"pt\", padding=True\n+            self.fixture_conversations, tokenize=True, return_tensors=\"pt\", padding=True, return_dict=False\n         )\n         self.assertIsInstance(token_outputs, torch.Tensor)\n         self.assertEqual(\n@@ -1432,7 +1446,7 @@ def test_batch_apply_chat_template_return_tensors(self):\n         # Test 2:\n         # without tokenize, should ignore return_tensors\n         token_outputs = self.tokenizer.apply_chat_template(\n-            self.fixture_conversations, tokenize=False, return_tensors=\"pt\", padding=True\n+            self.fixture_conversations, tokenize=False, return_tensors=\"pt\", padding=True, return_dict=False\n         )\n         self.assertEqual(token_outputs, [t.text for t in self.tokenized_fixture_conversations])\n "
      },
      {
        "filename": "tests/tokenization/test_tokenization_utils.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -323,7 +323,7 @@ def test_encode_message(self):\n         ]\n \n         # First, test the default case, where we encode the whole conversation at once\n-        whole_conversation_tokens = tokenizer.apply_chat_template(conversation, tokenize=True)\n+        whole_conversation_tokens = tokenizer.apply_chat_template(conversation, tokenize=True, return_dict=False)\n \n         # Now, test the message-by-message encoding\n         tokens = []"
      }
    ],
    "num_files": 13,
    "scraped_at": "2025-11-16T21:17:25.496723",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR contains meaningful architectural changes to the `apply_chat_template` API that affects how tokenizer outputs are returned across multiple tokenizer classes. The change involves modifying default behavior, updating validation logic for parameter combinations, and removing legacy test code. Developers would need to understand the implications of this API change, backward compatibility considerations, and how BatchEncoding relates to tokenization workflows.",
      "substance_level": "medium"
    }
  },
  {
    "pr_number": 41625,
    "title": "[`Masks`] Fix mask handling in eager for vision models",
    "body": "As per title, some vision models use masks, let's sync with bert this time and reduce errors that could be introduced",
    "html_url": "https://github.com/huggingface/transformers/pull/41625",
    "created_at": "2025-10-15T15:00:17Z",
    "merged_at": "2025-10-16T14:27:26Z",
    "merge_commit_sha": "bf815e9b5ea076f758cc58f73f2be2d36237f9ec",
    "base_ref": "main",
    "head_sha": "b0a0ae01889ae4790058b103b2a1654baf314cd5",
    "user": "vasqu",
    "files": [
      {
        "filename": "src/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py",
        "status": "modified",
        "additions": 11,
        "deletions": 8,
        "changes": 19,
        "patch": "@@ -96,25 +96,28 @@ def forward(self, input_values: torch.Tensor) -> torch.Tensor:\n         return embeddings\n \n \n-# Copied from transformers.models.vit.modeling_vit.eager_attention_forward\n+# Copied from transformers.models.bert.modeling_bert.eager_attention_forward\n def eager_attention_forward(\n     module: nn.Module,\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n     attention_mask: Optional[torch.Tensor],\n-    scaling: float,\n+    scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n-    # Normalize the attention scores to probabilities.\n-    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n+        attn_weights = attn_weights + attention_mask\n \n-    # This is actually dropping out entire tokens to attend to, which might\n-    # seem a bit unusual, but is taken from the original Transformer paper.\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n     attn_output = torch.matmul(attn_weights, value)"
      },
      {
        "filename": "src/transformers/models/deit/modeling_deit.py",
        "status": "modified",
        "additions": 11,
        "deletions": 8,
        "changes": 19,
        "patch": "@@ -161,25 +161,28 @@ def forward(self, pixel_values: torch.Tensor) -> torch.Tensor:\n         return x\n \n \n-# Copied from transformers.models.vit.modeling_vit.eager_attention_forward\n+# Copied from transformers.models.bert.modeling_bert.eager_attention_forward\n def eager_attention_forward(\n     module: nn.Module,\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n     attention_mask: Optional[torch.Tensor],\n-    scaling: float,\n+    scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n-    # Normalize the attention scores to probabilities.\n-    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n+        attn_weights = attn_weights + attention_mask\n \n-    # This is actually dropping out entire tokens to attend to, which might\n-    # seem a bit unusual, but is taken from the original Transformer paper.\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n     attn_output = torch.matmul(attn_weights, value)"
      },
      {
        "filename": "src/transformers/models/dinov2/modeling_dinov2.py",
        "status": "modified",
        "additions": 11,
        "deletions": 8,
        "changes": 19,
        "patch": "@@ -149,25 +149,28 @@ def forward(self, pixel_values: torch.Tensor) -> torch.Tensor:\n         return embeddings\n \n \n-# Copied from transformers.models.vit.modeling_vit.eager_attention_forward\n+# Copied from transformers.models.bert.modeling_bert.eager_attention_forward\n def eager_attention_forward(\n     module: nn.Module,\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n     attention_mask: Optional[torch.Tensor],\n-    scaling: float,\n+    scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n-    # Normalize the attention scores to probabilities.\n-    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n+        attn_weights = attn_weights + attention_mask\n \n-    # This is actually dropping out entire tokens to attend to, which might\n-    # seem a bit unusual, but is taken from the original Transformer paper.\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n     attn_output = torch.matmul(attn_weights, value)"
      },
      {
        "filename": "src/transformers/models/dinov2_with_registers/modeling_dinov2_with_registers.py",
        "status": "modified",
        "additions": 10,
        "deletions": 7,
        "changes": 17,
        "patch": "@@ -176,18 +176,21 @@ def eager_attention_forward(\n     key: torch.Tensor,\n     value: torch.Tensor,\n     attention_mask: Optional[torch.Tensor],\n-    scaling: float,\n+    scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n-    # Normalize the attention scores to probabilities.\n-    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n+        attn_weights = attn_weights + attention_mask\n \n-    # This is actually dropping out entire tokens to attend to, which might\n-    # seem a bit unusual, but is taken from the original Transformer paper.\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n     attn_output = torch.matmul(attn_weights, value)"
      },
      {
        "filename": "src/transformers/models/dinov3_vit/modeling_dinov3_vit.py",
        "status": "modified",
        "additions": 10,
        "deletions": 7,
        "changes": 17,
        "patch": "@@ -194,18 +194,21 @@ def eager_attention_forward(\n     key: torch.Tensor,\n     value: torch.Tensor,\n     attention_mask: Optional[torch.Tensor],\n-    scaling: float,\n+    scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n-    # Normalize the attention scores to probabilities.\n-    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n+        attn_weights = attn_weights + attention_mask\n \n-    # This is actually dropping out entire tokens to attend to, which might\n-    # seem a bit unusual, but is taken from the original Transformer paper.\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n     attn_output = torch.matmul(attn_weights, value)"
      },
      {
        "filename": "src/transformers/models/dpt/modeling_dpt.py",
        "status": "modified",
        "additions": 13,
        "deletions": 9,
        "changes": 22,
        "patch": "@@ -32,7 +32,8 @@\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, DepthEstimatorOutput, SemanticSegmenterOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n-from ...utils import ModelOutput, auto_docstring, logging, torch_int\n+from ...processing_utils import Unpack\n+from ...utils import ModelOutput, TransformersKwargs, auto_docstring, logging, torch_int\n from ...utils.backbone_utils import load_backbone\n from ...utils.generic import can_return_tuple, check_model_inputs\n from .configuration_dpt import DPTConfig\n@@ -267,25 +268,28 @@ def forward(self, pixel_values: torch.Tensor) -> torch.Tensor:\n         return embeddings\n \n \n-# Copied from transformers.models.vit.modeling_vit.eager_attention_forward\n+# Copied from transformers.models.bert.modeling_bert.eager_attention_forward\n def eager_attention_forward(\n     module: nn.Module,\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n     attention_mask: Optional[torch.Tensor],\n-    scaling: float,\n+    scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n-    # Normalize the attention scores to probabilities.\n-    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n+        attn_weights = attn_weights + attention_mask\n \n-    # This is actually dropping out entire tokens to attend to, which might\n-    # seem a bit unusual, but is taken from the original Transformer paper.\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n     attn_output = torch.matmul(attn_weights, value)"
      },
      {
        "filename": "src/transformers/models/ijepa/modeling_ijepa.py",
        "status": "modified",
        "additions": 10,
        "deletions": 7,
        "changes": 17,
        "patch": "@@ -147,18 +147,21 @@ def eager_attention_forward(\n     key: torch.Tensor,\n     value: torch.Tensor,\n     attention_mask: Optional[torch.Tensor],\n-    scaling: float,\n+    scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n-    # Normalize the attention scores to probabilities.\n-    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n+        attn_weights = attn_weights + attention_mask\n \n-    # This is actually dropping out entire tokens to attend to, which might\n-    # seem a bit unusual, but is taken from the original Transformer paper.\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n     attn_output = torch.matmul(attn_weights, value)"
      },
      {
        "filename": "src/transformers/models/videomae/modeling_videomae.py",
        "status": "modified",
        "additions": 11,
        "deletions": 8,
        "changes": 19,
        "patch": "@@ -178,25 +178,28 @@ def forward(self, pixel_values):\n         return embeddings\n \n \n-# Copied from transformers.models.vit.modeling_vit.eager_attention_forward\n+# Copied from transformers.models.bert.modeling_bert.eager_attention_forward\n def eager_attention_forward(\n     module: nn.Module,\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n     attention_mask: Optional[torch.Tensor],\n-    scaling: float,\n+    scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n-    # Normalize the attention scores to probabilities.\n-    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n+        attn_weights = attn_weights + attention_mask\n \n-    # This is actually dropping out entire tokens to attend to, which might\n-    # seem a bit unusual, but is taken from the original Transformer paper.\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n     attn_output = torch.matmul(attn_weights, value)"
      },
      {
        "filename": "src/transformers/models/vit/modeling_vit.py",
        "status": "modified",
        "additions": 11,
        "deletions": 7,
        "changes": 18,
        "patch": "@@ -167,24 +167,28 @@ def forward(self, pixel_values: torch.Tensor, interpolate_pos_encoding: bool = F\n         return embeddings\n \n \n+# Copied from transformers.models.bert.modeling_bert.eager_attention_forward\n def eager_attention_forward(\n     module: nn.Module,\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n     attention_mask: Optional[torch.Tensor],\n-    scaling: float,\n+    scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n-    # Normalize the attention scores to probabilities.\n-    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n+        attn_weights = attn_weights + attention_mask\n \n-    # This is actually dropping out entire tokens to attend to, which might\n-    # seem a bit unusual, but is taken from the original Transformer paper.\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n     attn_output = torch.matmul(attn_weights, value)"
      },
      {
        "filename": "src/transformers/models/vit_mae/modeling_vit_mae.py",
        "status": "modified",
        "additions": 11,
        "deletions": 8,
        "changes": 19,
        "patch": "@@ -326,25 +326,28 @@ def forward(self, pixel_values, interpolate_pos_encoding: bool = False):\n         return x\n \n \n-# Copied from transformers.models.vit.modeling_vit.eager_attention_forward\n+# Copied from transformers.models.bert.modeling_bert.eager_attention_forward\n def eager_attention_forward(\n     module: nn.Module,\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n     attention_mask: Optional[torch.Tensor],\n-    scaling: float,\n+    scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n-    # Normalize the attention scores to probabilities.\n-    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n+        attn_weights = attn_weights + attention_mask\n \n-    # This is actually dropping out entire tokens to attend to, which might\n-    # seem a bit unusual, but is taken from the original Transformer paper.\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n     attn_output = torch.matmul(attn_weights, value)"
      },
      {
        "filename": "src/transformers/models/vit_msn/modeling_vit_msn.py",
        "status": "modified",
        "additions": 11,
        "deletions": 8,
        "changes": 19,
        "patch": "@@ -163,25 +163,28 @@ def forward(self, pixel_values: torch.Tensor, interpolate_pos_encoding: bool = F\n         return embeddings\n \n \n-# Copied from transformers.models.vit.modeling_vit.eager_attention_forward\n+# Copied from transformers.models.bert.modeling_bert.eager_attention_forward\n def eager_attention_forward(\n     module: nn.Module,\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n     attention_mask: Optional[torch.Tensor],\n-    scaling: float,\n+    scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n-    # Normalize the attention scores to probabilities.\n-    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n+        attn_weights = attn_weights + attention_mask\n \n-    # This is actually dropping out entire tokens to attend to, which might\n-    # seem a bit unusual, but is taken from the original Transformer paper.\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n     attn_output = torch.matmul(attn_weights, value)"
      },
      {
        "filename": "src/transformers/models/vitpose_backbone/modeling_vitpose_backbone.py",
        "status": "modified",
        "additions": 13,
        "deletions": 9,
        "changes": 22,
        "patch": "@@ -30,7 +30,8 @@\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BackboneOutput, BaseModelOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n-from ...utils import auto_docstring, logging\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, auto_docstring, logging\n from ...utils.backbone_utils import BackboneMixin\n from ...utils.generic import check_model_inputs\n from .configuration_vitpose_backbone import VitPoseBackboneConfig\n@@ -95,25 +96,28 @@ def forward(self, pixel_values: torch.Tensor) -> torch.Tensor:\n         return embeddings\n \n \n-# Copied from transformers.models.vit.modeling_vit.eager_attention_forward\n+# Copied from transformers.models.bert.modeling_bert.eager_attention_forward\n def eager_attention_forward(\n     module: nn.Module,\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n     attention_mask: Optional[torch.Tensor],\n-    scaling: float,\n+    scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n-    # Normalize the attention scores to probabilities.\n-    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n+        attn_weights = attn_weights + attention_mask\n \n-    # This is actually dropping out entire tokens to attend to, which might\n-    # seem a bit unusual, but is taken from the original Transformer paper.\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n     attn_output = torch.matmul(attn_weights, value)"
      },
      {
        "filename": "src/transformers/models/vivit/modeling_vivit.py",
        "status": "modified",
        "additions": 11,
        "deletions": 8,
        "changes": 19,
        "patch": "@@ -156,25 +156,28 @@ def forward(self, pixel_values: torch.Tensor, interpolate_pos_encoding: bool = F\n         return embeddings\n \n \n-# Copied from transformers.models.vit.modeling_vit.eager_attention_forward\n+# Copied from transformers.models.bert.modeling_bert.eager_attention_forward\n def eager_attention_forward(\n     module: nn.Module,\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n     attention_mask: Optional[torch.Tensor],\n-    scaling: float,\n+    scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n-    # Normalize the attention scores to probabilities.\n-    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n+        attn_weights = attn_weights + attention_mask\n \n-    # This is actually dropping out entire tokens to attend to, which might\n-    # seem a bit unusual, but is taken from the original Transformer paper.\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n     attn_output = torch.matmul(attn_weights, value)"
      },
      {
        "filename": "src/transformers/models/yolos/modeling_yolos.py",
        "status": "modified",
        "additions": 11,
        "deletions": 8,
        "changes": 19,
        "patch": "@@ -211,25 +211,28 @@ def forward(self, pixel_values: torch.Tensor) -> torch.Tensor:\n         return embeddings\n \n \n-# Copied from transformers.models.vit.modeling_vit.eager_attention_forward\n+# Copied from transformers.models.bert.modeling_bert.eager_attention_forward\n def eager_attention_forward(\n     module: nn.Module,\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n     attention_mask: Optional[torch.Tensor],\n-    scaling: float,\n+    scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n-    # Normalize the attention scores to probabilities.\n-    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n+        attn_weights = attn_weights + attention_mask\n \n-    # This is actually dropping out entire tokens to attend to, which might\n-    # seem a bit unusual, but is taken from the original Transformer paper.\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n     attn_output = torch.matmul(attn_weights, value)"
      }
    ],
    "num_files": 14,
    "scraped_at": "2025-11-16T21:17:25.820794",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR makes meaningful logic changes to attention mechanisms across multiple vision models, syncing mask handling from BERT's implementation. The changes involve actual algorithmic decisions (transpose indices, mask application timing, scaling computation) and architectural alignment that would help developers understand attention implementation patterns in the codebase.",
      "substance_level": "medium"
    }
  },
  {
    "pr_number": 41612,
    "title": "Fix EncoderDecoder cache",
    "body": "In #41569 we restored thr `__iter__` method to `DynamicCache` but I missed the fact that it was also removed from `EncoderDecoderCache`. This PR fixes that and modifies the `__init__` of `EncoderDecoderCache` in the case of DDP, so it is compatible with the new system.",
    "html_url": "https://github.com/huggingface/transformers/pull/41612",
    "created_at": "2025-10-15T10:39:31Z",
    "merged_at": "2025-10-16T12:55:42Z",
    "merge_commit_sha": "eef9fb2af3db888cf93f81b425f9db453336726c",
    "base_ref": "main",
    "head_sha": "c3f2af117695ca9146c0bbaa9d374bc7c073b94a",
    "user": "remi-or",
    "files": [
      {
        "filename": "src/transformers/cache_utils.py",
        "status": "modified",
        "additions": 27,
        "deletions": 14,
        "changes": 41,
        "patch": "@@ -937,7 +937,7 @@ class DynamicCache(Cache):\n \n     def __init__(\n         self,\n-        ddp_cache_data: Optional[Iterable[tuple[Optional[torch.Tensor], torch.Tensor, torch.Tensor]]] = None,\n+        ddp_cache_data: Optional[Iterable[tuple[Optional[torch.Tensor], ...]]] = None,\n         config: Optional[PreTrainedConfig] = None,\n         offloading: bool = False,\n         offload_only_non_sliding: bool = False,\n@@ -970,17 +970,21 @@ def __init__(\n         # In this case, use the passed data to already fill in the Cache\n         if ddp_cache_data is not None:\n             # Init all the layers with the data\n-            for layer_idx, (sliding_window_tensor, key_states, value_states) in enumerate(ddp_cache_data):\n+            for layer_idx, kv_and_optional_sliding in enumerate(ddp_cache_data):\n                 # If the config was not passed above, initialize a new cache layer for each entry of the ddp_data\n                 if config is None:\n+                    # kv_and_optional_sliding contains at least two elements: the key and value states. It can also\n+                    # contain a third element, which is an optional sliding window tensor.\n+                    sliding_window_tensor = kv_and_optional_sliding[2] if len(kv_and_optional_sliding) == 3 else None\n+                    # If there is a sliding window tensor, use it to initialize the layer\n                     if sliding_window_tensor is not None:\n                         # Since the same layer is dispatched across replicas, sliding_window is the same for all\n                         sliding_window = sliding_window_tensor[0].item()\n                         layers.append(DynamicSlidingWindowLayer(sliding_window=sliding_window))\n                     else:\n                         layers.append(DynamicLayer())\n                 # Update the layer with the data\n-                _, _ = layers[layer_idx].update(key_states, value_states)\n+                _, _ = layers[layer_idx].update(kv_and_optional_sliding[0], kv_and_optional_sliding[1])\n \n         # If neither of config nor ddp_data was passed, then simply lazy init a full cache of DynamicLayer\n         if len(layers) == 0:\n@@ -994,7 +998,7 @@ def __init__(\n \n     def __iter__(self):\n         for layer in self.layers:\n-            yield getattr(layer, \"_sliding_window_tensor\", None), layer.keys, layer.values\n+            yield layer.keys, layer.values, getattr(layer, \"_sliding_window_tensor\", None)\n \n \n class StaticCache(Cache):\n@@ -1166,17 +1170,21 @@ class EncoderDecoderCache(Cache):\n     \"\"\"\n \n     def __init__(self, *caches) -> None:\n-        # For dp and ddp support, if only one argument is passed, it should be an iterable of tuples of tensors\n+        # For dp and ddp support, if only one argument is passed, it should be an iterable of DynamicCache ddp data\n         if len(caches) == 1:\n-            self.self_attention_cache = DynamicCache()\n-            self.cross_attention_cache = DynamicCache()\n-            # Populate cache from the iterable\n-            for layer_idx, key_value_states in enumerate(caches[0]):\n-                key_states, value_states = key_value_states[:2]\n-                self.self_attention_cache.update(key_states, value_states, layer_idx)\n-                if len(key_value_states) > 2:\n-                    key_states, value_states = key_value_states[2:]\n-                    self.cross_attention_cache.update(key_states, value_states, layer_idx)\n+            self_attention_cache_data, cross_attention_cache_data = [], []\n+            for combined_cache_data in caches[0]:\n+                if len(combined_cache_data) == 6:  # two tuple of style (self_attn_k, self_attn_v, self_attn_sliding)\n+                    self_attention_cache_data.append(combined_cache_data[:3])\n+                    cross_attention_cache_data.append(combined_cache_data[3:])\n+                # To support old DDP-style init, we handle the case where the tuple has no sliding window tensor\n+                elif len(combined_cache_data) == 4:  # two tuple of style (self_attn_k, self_attn_v)\n+                    self_attention_cache_data.append(combined_cache_data[:2])\n+                    cross_attention_cache_data.append(combined_cache_data[2:])\n+                else:\n+                    raise ValueError(f\"Expected {len(combined_cache_data) = } to be 4 or 6.\\n{combined_cache_data = }\")\n+            self.self_attention_cache = DynamicCache(self_attention_cache_data)\n+            self.cross_attention_cache = DynamicCache(cross_attention_cache_data)\n         # Otherwise, we should get two arguments, a self-attention cache and a cross-attention cache\n         elif len(caches) == 2:\n             if not isinstance(caches[0], Cache) or not isinstance(caches[1], Cache):\n@@ -1191,6 +1199,11 @@ def __init__(self, *caches) -> None:\n         for layer_idx in range(len(self.cross_attention_cache)):\n             self.is_updated[layer_idx] = bool(self.cross_attention_cache.get_seq_length(layer_idx) > 0)\n \n+    def __iter__(self):\n+        \"\"\"Returns tuples of style (self_attn_k, self_attn_v, self_attn_sliding, cross_attn_k, cross_attn_v, cross_attn_sliding)\"\"\"\n+        for self_attention_layer, cross_attention_layer in zip(self.self_attention_cache, self.cross_attention_cache):\n+            yield self_attention_layer + cross_attention_layer\n+\n     def __repr__(self) -> str:\n         return (\n             f\"{self.__class__.__name__}(self_attention_cache={self.self_attention_cache}, cross_attention_cache=\""
      },
      {
        "filename": "src/transformers/models/rag/modeling_rag.py",
        "status": "modified",
        "additions": 16,
        "deletions": 14,
        "changes": 30,
        "patch": "@@ -1187,22 +1187,24 @@ def _reorder_stacked(hidden_states, new_order):\n         reordered_past = ()\n         for idx in range(len(past_key_values)):\n             if isinstance(past_key_values, EncoderDecoderCache):\n-                layer_past = (\n-                    past_key_values.self_attention_cache.layers[idx].keys,\n-                    past_key_values.self_attention_cache.layers[idx].values,\n-                    past_key_values.cross_attention_cache.layers[idx].keys,\n-                    past_key_values.cross_attention_cache.layers[idx].values,\n+                self_attention_k, self_attention_v, cross_attention_k, cross_attention_v = (\n+                    _reorder_stacked(x, beam_idx.to(x.device))\n+                    for x in (\n+                        past_key_values.self_attention_cache.layers[idx].keys,\n+                        past_key_values.self_attention_cache.layers[idx].values,\n+                        past_key_values.cross_attention_cache.layers[idx].keys,\n+                        past_key_values.cross_attention_cache.layers[idx].values,\n+                    )\n                 )\n+                new_tuple = (self_attention_k, self_attention_v, cross_attention_k, cross_attention_v)\n             else:\n-                layer_past = (past_key_values.layers[idx].keys, past_key_values.layers[idx].values)\n-            # get the correct batch idx from decoder layer's batch dim for cross and self-attn\n-            reordered_past += (\n-                tuple(_reorder_stacked(past_state, beam_idx.to(past_state.device)) for past_state in layer_past),\n-            )\n-\n-        # Cast back to the correct cache class\n-        reordered_cache = type(past_key_values)(reordered_past)\n-        return reordered_cache\n+                self_attention_k, self_attention_v = (\n+                    _reorder_stacked(x, beam_idx.to(x.device))\n+                    for x in (past_key_values.layers[idx].keys, past_key_values.layers[idx].values)\n+                )\n+                new_tuple = (self_attention_k, self_attention_v)\n+            reordered_past += (new_tuple,)\n+        return type(past_key_values)(reordered_past)\n \n     def marginalize(self, seq_logits, doc_scores, n_docs=None):\n         n_docs = n_docs if n_docs is not None else self.config.n_docs"
      },
      {
        "filename": "src/transformers/models/whisper/generation_whisper.py",
        "status": "modified",
        "additions": 12,
        "deletions": 8,
        "changes": 20,
        "patch": "@@ -1180,12 +1180,14 @@ def split_by_batch_index(values, key, batch_idx, is_shortform, beam_indices=None\n                     return None\n                 all_past_key_values = []\n                 for layer_idx in range(self.config.decoder_layers):\n-                    layer_past_key_values = []\n-                    for cache_cls in [values.self_attention_cache, values.cross_attention_cache]:\n-                        for v in [cache_cls.layers[layer_idx].keys, cache_cls.layers[layer_idx].values]:\n-                            layer_past_key_values.append(v[batch_idx][None].cpu())\n-                    all_past_key_values.append(tuple(layer_past_key_values))\n-                return EncoderDecoderCache(tuple(all_past_key_values))\n+                    layer_cache = (\n+                        values.self_attention_cache.layers[layer_idx].keys[batch_idx][None].cpu(),\n+                        values.self_attention_cache.layers[layer_idx].values[batch_idx][None].cpu(),\n+                        values.cross_attention_cache.layers[layer_idx].keys[batch_idx][None].cpu(),\n+                        values.cross_attention_cache.layers[layer_idx].values[batch_idx][None].cpu(),\n+                    )\n+                    all_past_key_values.append(layer_cache)\n+                return EncoderDecoderCache(all_past_key_values)\n \n             return values[batch_idx].cpu()\n \n@@ -1224,7 +1226,7 @@ def _stack_split_outputs(self, seek_outputs, model_output_type, device, kwargs):\n                 if seek_outputs[0][key] is not None:\n                     all_past_key_values = []\n                     for layer_idx in range(len(seek_outputs[0][key])):\n-                        layer_past_key_values = tuple(\n+                        self_attention_k, self_attention_v, cross_attention_k, cross_attention_v = (\n                             torch.stack(\n                                 [\n                                     getattr(getattr(sub_output[key], sub_cache).layers[layer_idx], sub_key)\n@@ -1236,7 +1238,9 @@ def _stack_split_outputs(self, seek_outputs, model_output_type, device, kwargs):\n                             for sub_cache in [\"self_attention_cache\", \"cross_attention_cache\"]\n                             for sub_key in [\"keys\", \"values\"]\n                         )\n-                        all_past_key_values.append(layer_past_key_values)\n+                        all_past_key_values.append(\n+                            (self_attention_k, self_attention_v, cross_attention_k, cross_attention_v)\n+                        )\n                     outputs[key] = EncoderDecoderCache(tuple(all_past_key_values))\n                 else:\n                     outputs[key] = None"
      },
      {
        "filename": "tests/utils/test_modeling_utils.py",
        "status": "modified",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "patch": "@@ -1807,8 +1807,8 @@ def test_cache_when_needed_at_train_time(self):\n         # simulate injecting virtual tokens like in prefix tuning\n         num_virtual_tokens = 3\n         past_key_values = [\n-            (None, torch.randn(1, 2, num_virtual_tokens, 8), torch.randn(1, 2, num_virtual_tokens, 8)),\n-            (None, torch.randn(1, 2, num_virtual_tokens, 8), torch.randn(1, 2, num_virtual_tokens, 8)),\n+            (torch.randn(1, 2, num_virtual_tokens, 8), torch.randn(1, 2, num_virtual_tokens, 8)),\n+            (torch.randn(1, 2, num_virtual_tokens, 8), torch.randn(1, 2, num_virtual_tokens, 8)),\n         ]\n         past_key_values = DynamicCache(past_key_values)\n         model_inputs[\"attention_mask\"] = torch.cat("
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T21:17:28.321872",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR contains non-trivial logic changes to cache handling systems, specifically fixing how `EncoderDecoderCache` and `DynamicCache` handle distributed data parallel (DDP) scenarios and the `__iter__` method. The changes involve understanding cache layer architecture, optional tensor handling, and how tuple structures are passed through multiple systems. A developer would need to understand the cache abstraction and DDP considerations to work on related features.",
      "substance_level": "medium"
    }
  },
  {
    "pr_number": 41605,
    "title": "Fix fp32_ln for various models",
    "body": "This PR fixes the test `test_flash_attn_2_fp32_ln` for several models:\r\n- `bark` was failing the test because it call `_flash_attention_forward` directly without checking the `queries` dtype, and so the test could fail if the dtype was `torch.float32`. To fix this we re-factored out a code block into a function  `get_target_dtype`  that takes care of infering whether to cast the fp32 tesnor to fp16 or bf16, and added a called to it before the call to FA\r\n- same for `stablelm`\r\n- `mllama` was failing the test because `MllamaTextSelfAttention` lacks the `is_causal`attribute, which was added and set to True (it's a text attention so it's causal, as discussed in #39182)\r\n- same for `kosmos2` but the test still fails for many many other reasons\r\n\r\nThe list of fixed test is here:\r\n```\r\nFAILED tests/models/bark/test_modeling_bark.py::BarkSemanticModelTest::test_flash_attn_2_fp32_ln - RuntimeError: FlashAttention only support fp16 and bf16 data type\r\nFAILED tests/models/bark/test_modeling_bark.py::BarkCoarseModelTest::test_flash_attn_2_fp32_ln - RuntimeError: FlashAttention only support fp16 and bf16 data type\r\nFAILED tests/models/mllama/test_modeling_mllama.py::MllamaForCausalLMModelTest::test_flash_attn_2_fp32_ln - AttributeError: 'MllamaTextSelfAttention' object has no attribute 'is_causal'\r\nFAILED tests/models/mllama/test_modeling_mllama.py::MllamaForConditionalGenerationModelTest::test_flash_attn_2_fp32_ln - AttributeError: 'MllamaTextSelfAttention' object has no attribute 'is_causal'\r\nFAILED tests/models/stablelm/test_modeling_stablelm.py::StableLmModelTest::test_flash_attn_2_fp32_ln - RuntimeError: FlashAttention only support fp16 and bf16 data type\r\n```",
    "html_url": "https://github.com/huggingface/transformers/pull/41605",
    "created_at": "2025-10-15T09:44:32Z",
    "merged_at": "2025-10-16T12:18:50Z",
    "merge_commit_sha": "2935a1be19f12176c455cb65d67dc5a3bb84cd77",
    "base_ref": "main",
    "head_sha": "1bb5bcef2c49aa77d8380189feae8418bbd83296",
    "user": "remi-or",
    "files": [
      {
        "filename": "src/transformers/integrations/flash_attention.py",
        "status": "modified",
        "additions": 14,
        "deletions": 9,
        "changes": 23,
        "patch": "@@ -11,6 +11,19 @@\n _use_top_left_mask = flash_attn_supports_top_left_mask()\n \n \n+def get_target_dtype(query: torch.Tensor, module: torch.nn.Module) -> torch.dtype:\n+    \"\"\"If the query is in float32, return a target dtype compatible with flash attention. Return None otherwise.\"\"\"\n+    if query.dtype == torch.float32:\n+        if torch.is_autocast_enabled():\n+            return torch.get_autocast_gpu_dtype()\n+        # Handle the case where the model is quantized\n+        elif hasattr(module.config, \"_pre_quantization_dtype\"):\n+            return module.config._pre_quantization_dtype\n+        else:\n+            return next(layer for layer in module.modules() if isinstance(layer, torch.nn.Linear)).weight.dtype\n+    return None\n+\n+\n def flash_attention_forward(\n     module: torch.nn.Module,\n     query: torch.Tensor,\n@@ -48,15 +61,7 @@ def flash_attention_forward(\n     # cast them back in the correct dtype just to be sure everything works as expected.\n     # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n     # in fp32. (usually our RMSNorm modules handle it correctly)\n-    target_dtype = None\n-    if query.dtype == torch.float32:\n-        if torch.is_autocast_enabled():\n-            target_dtype = torch.get_autocast_gpu_dtype()\n-        # Handle the case where the model is quantized\n-        elif hasattr(module.config, \"_pre_quantization_dtype\"):\n-            target_dtype = module.config._pre_quantization_dtype\n-        else:\n-            target_dtype = next(layer for layer in module.modules() if isinstance(layer, torch.nn.Linear)).weight.dtype\n+    target_dtype = get_target_dtype(query, module)\n \n     # Instead of relying on the value set in the module directly, we use the is_causal passed in kwargs if it is presented\n     is_causal = kwargs.pop(\"is_causal\", None)"
      },
      {
        "filename": "src/transformers/models/bark/modeling_bark.py",
        "status": "modified",
        "additions": 5,
        "deletions": 0,
        "changes": 5,
        "patch": "@@ -57,6 +57,7 @@\n \n \n if is_flash_attn_available():\n+    from ...integrations.flash_attention import get_target_dtype\n     from ...modeling_flash_attention_utils import _flash_attention_forward\n \n \n@@ -78,6 +79,7 @@ def __init__(self, config, is_causal=False, layer_idx=None):\n         self.embed_dim = config.hidden_size\n         self.num_heads = config.num_heads\n         self.head_dim = self.embed_dim // self.num_heads\n+        self.config = config\n \n         if config.hidden_size % config.num_heads != 0:\n             raise ValueError(\n@@ -228,6 +230,8 @@ def forward(\n         if past_key_values is not None:\n             key, value = past_key_values.update(key, value, self.layer_idx, {\"cache_position\": cache_position})\n \n+        target_dtype = get_target_dtype(query, self)  # if the query is in float32, this is the dtype to cast to for FA\n+\n         attn_output = _flash_attention_forward(\n             query,\n             key,\n@@ -237,6 +241,7 @@ def forward(\n             dropout=self.dropout if self.training else 0.0,\n             use_top_left_mask=self._flash_attn_uses_top_left_mask,\n             is_causal=self.is_causal,\n+            target_dtype=target_dtype,\n         )\n \n         attn_output = self._merge_heads(attn_output, self.num_heads, self.head_dim)"
      },
      {
        "filename": "src/transformers/models/blt/modeling_blt.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -280,12 +280,12 @@ def __init__(self, config: BltConfig, layer_idx: int):\n         self.scaling = self.head_dim**-0.5\n         self.rope_theta = config.rope_theta\n         self.layer_idx = layer_idx\n+        self.is_causal = True\n \n         self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n         self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n         self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n         self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n-        self.is_causal = True\n \n     def forward(\n         self,"
      },
      {
        "filename": "src/transformers/models/kosmos2/modeling_kosmos2.py",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -680,6 +680,7 @@ def __init__(\n         self.num_heads = num_heads\n         self.dropout = dropout\n         self.head_dim = embed_dim // num_heads\n+        self.is_causal = True\n \n         if (self.head_dim * num_heads) != self.embed_dim:\n             raise ValueError("
      },
      {
        "filename": "src/transformers/models/mllama/modeling_mllama.py",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -519,6 +519,7 @@ def __init__(self, config: MllamaTextConfig, layer_idx: int):\n         self.scaling = self.head_dim**-0.5\n         self.rope_theta = config.rope_theta\n         self.layer_idx = layer_idx\n+        self.is_causal = True\n \n         self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n         self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)"
      },
      {
        "filename": "src/transformers/models/stablelm/modeling_stablelm.py",
        "status": "modified",
        "additions": 4,
        "deletions": 0,
        "changes": 4,
        "patch": "@@ -52,6 +52,7 @@\n \n \n if is_flash_attn_available():\n+    from ...integrations.flash_attention import get_target_dtype\n     from ...modeling_flash_attention_utils import _flash_attention_forward\n \n \n@@ -495,6 +496,8 @@ def forward(\n \n         dropout_rate = self.attention_dropout.p if self.training else 0.0\n \n+        target_dtype = get_target_dtype(query_states, self)\n+\n         attn_output = _flash_attention_forward(\n             query_states,\n             key_states,\n@@ -505,6 +508,7 @@ def forward(\n             dropout=dropout_rate,\n             use_top_left_mask=self._flash_attn_uses_top_left_mask,\n             is_causal=self.is_causal,\n+            target_dtype=target_dtype,\n         )\n \n         attn_output = attn_output.reshape(bsz, q_len, self.hidden_size).contiguous()"
      }
    ],
    "num_files": 6,
    "scraped_at": "2025-11-16T21:17:29.373689",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR contains non-trivial logic changes across multiple model implementations to fix Flash Attention compatibility issues. It introduces a new utility function `get_target_dtype` that handles dtype casting logic, applies it to multiple models (bark, stablelm, mllama, kosmos2), and adds missing `is_causal` attributes. The changes involve meaningful decisions about dtype inference and Flash Attention integration that developers would need to understand when working on similar model implementations.",
      "substance_level": "medium"
    }
  },
  {
    "pr_number": 41577,
    "title": "[kernels] refactor function kernel calling",
    "body": "# What does this PR do?\r\n\r\nThis should simplify lazy kernel loading in Transformers.  \r\nWe simply define a mapping between each kernel name and the repository it should be pulled from, then load it using the `lazy_load_kernel` function. This function adds the kernel to a global cache shared across all models.  \r\nIf the kernel isn\u2019t available, we check whether it\u2019s installed as a module for backward compatibility; otherwise, we return `None`.\r\n",
    "html_url": "https://github.com/huggingface/transformers/pull/41577",
    "created_at": "2025-10-14T12:58:37Z",
    "merged_at": "2025-10-16T13:43:03Z",
    "merge_commit_sha": "1fb3fc4db0e87fd7c2f57a36b6b32ee6fa69c50c",
    "base_ref": "main",
    "head_sha": "374040458947013699454f267606233374be6303",
    "user": "MekkCyber",
    "files": [
      {
        "filename": "src/transformers/integrations/hub_kernels.py",
        "status": "modified",
        "additions": 55,
        "deletions": 0,
        "changes": 55,
        "patch": "@@ -14,12 +14,16 @@\n import re\n from collections.abc import Callable\n from functools import partial\n+from types import ModuleType\n from typing import Optional, Union\n \n from ..modeling_flash_attention_utils import lazy_import_flash_attention\n+from ..utils import logging\n from .flash_attention import flash_attention_forward\n \n \n+logger = logging.get_logger(__name__)\n+\n try:\n     from kernels import (\n         Device,\n@@ -158,6 +162,13 @@ def register_kernel_mapping(*args, **kwargs):\n         raise RuntimeError(\"register_kernel_mapping requires `kernels` to be installed. Run `pip install kernels`.\")\n \n \n+_HUB_KERNEL_MAPPING: dict[str, str] = {\n+    \"causal-conv1d\": \"kernels-community/causal-conv1d\",\n+}\n+\n+_KERNEL_MODULE_MAPPING: dict[str, Optional[ModuleType]] = {}\n+\n+\n def is_kernel(attn_implementation: Optional[str]) -> bool:\n     \"\"\"Check whether `attn_implementation` matches a kernel pattern from the hub.\"\"\"\n     return (\n@@ -220,9 +231,53 @@ def load_and_register_attn_kernel(attn_implementation: str, attention_wrapper: O\n     ALL_MASK_ATTENTION_FUNCTIONS.register(attn_implementation, ALL_MASK_ATTENTION_FUNCTIONS[\"flash_attention_2\"])\n \n \n+def lazy_load_kernel(kernel_name: str, mapping: dict[str, Optional[ModuleType]] = _KERNEL_MODULE_MAPPING):\n+    if kernel_name in mapping and isinstance(mapping[kernel_name], ModuleType):\n+        return mapping[kernel_name]\n+    if kernel_name not in _HUB_KERNEL_MAPPING:\n+        logger.warning(f\"Kernel {kernel_name} not found in _HUB_KERNEL_MAPPING\")\n+        mapping[kernel_name] = None\n+        return None\n+    if _kernels_available:\n+        from kernels import get_kernel\n+\n+        try:\n+            kernel = get_kernel(_HUB_KERNEL_MAPPING[kernel_name])\n+            mapping[kernel_name] = kernel\n+        except FileNotFoundError:\n+            mapping[kernel_name] = None\n+\n+    else:\n+        # Try to import is_{kernel_name}_available from ..utils\n+        import importlib\n+\n+        new_kernel_name = kernel_name.replace(\"-\", \"_\")\n+        func_name = f\"is_{new_kernel_name}_available\"\n+\n+        try:\n+            utils_mod = importlib.import_module(\"..utils.import_utils\", __package__)\n+            is_kernel_available = getattr(utils_mod, func_name, None)\n+        except Exception:\n+            is_kernel_available = None\n+\n+        if callable(is_kernel_available) and is_kernel_available():\n+            # Try to import the module \"{kernel_name}\" from parent package level\n+            try:\n+                module = importlib.import_module(f\"{kernel_name}\")\n+                mapping[kernel_name] = module\n+                return module\n+            except Exception:\n+                mapping[kernel_name] = None\n+        else:\n+            mapping[kernel_name] = None\n+\n+    return mapping[kernel_name]\n+\n+\n __all__ = [\n     \"LayerRepository\",\n     \"use_kernel_forward_from_hub\",\n     \"register_kernel_mapping\",\n     \"replace_kernel_forward_from_hub\",\n+    \"lazy_load_kernel\",\n ]"
      },
      {
        "filename": "src/transformers/models/falcon_mamba/modeling_falcon_mamba.py",
        "status": "modified",
        "additions": 19,
        "deletions": 32,
        "changes": 51,
        "patch": "@@ -30,12 +30,11 @@\n from ...activations import ACT2FN\n from ...configuration_utils import PreTrainedConfig\n from ...generation import GenerationMixin\n+from ...integrations.hub_kernels import lazy_load_kernel\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_utils import PreTrainedModel\n from ...utils import ModelOutput, auto_docstring, logging\n from ...utils.import_utils import (\n-    is_causal_conv1d_available,\n-    is_kernels_available,\n     is_mamba_ssm_available,\n     is_mambapy_available,\n )\n@@ -162,33 +161,6 @@ def reset(self):\n             self.ssm_states[layer_idx].zero_()\n \n \n-def _lazy_load_causal_conv1d():\n-    global _causal_conv1d_cache\n-    if _causal_conv1d_cache is not None:\n-        return _causal_conv1d_cache\n-\n-    if is_kernels_available():\n-        from kernels import get_kernel\n-\n-        try:\n-            _causal_conv1d_kernel = get_kernel(\"kernels-community/causal-conv1d\")\n-        except FileNotFoundError:\n-            # no kernel binary match, fallback to slow path\n-            _causal_conv1d_cache = (None, None)\n-        else:\n-            _causal_conv1d_cache = (_causal_conv1d_kernel.causal_conv1d_update, _causal_conv1d_kernel.causal_conv1d_fn)\n-    elif is_causal_conv1d_available():\n-        from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\n-\n-        _causal_conv1d_cache = (causal_conv1d_update, causal_conv1d_fn)\n-    else:\n-        _causal_conv1d_cache = (None, None)\n-    return _causal_conv1d_cache\n-\n-\n-_causal_conv1d_cache = None\n-\n-\n def rms_forward(hidden_states, variance_epsilon=1e-6):\n     \"\"\"\n     Calculates simple RMSNorm with no learnable weights. `MambaRMSNorm` will\n@@ -268,7 +240,12 @@ def __init__(self, config: FalconMambaConfig, layer_idx: int):\n         self.rms_eps = config.mixer_rms_eps\n \n     def warn_slow_implementation(self):\n-        causal_conv1d_update, causal_conv1d_fn = _lazy_load_causal_conv1d()\n+        causal_conv1d = lazy_load_kernel(\"causal-conv1d\")\n+        causal_conv1d_update, causal_conv1d_fn = (\n+            (causal_conv1d.causal_conv1d_update, causal_conv1d.causal_conv1d_fn)\n+            if causal_conv1d is not None\n+            else (None, None)\n+        )\n         is_fast_path_available = all(\n             (selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)\n         )\n@@ -323,7 +300,12 @@ def cuda_kernels_forward(\n             )\n \n         else:\n-            causal_conv1d_update, causal_conv1d_fn = _lazy_load_causal_conv1d()\n+            causal_conv1d = lazy_load_kernel(\"causal-conv1d\")\n+            causal_conv1d_update, causal_conv1d_fn = (\n+                (causal_conv1d.causal_conv1d_update, causal_conv1d.causal_conv1d_fn)\n+                if causal_conv1d is not None\n+                else (None, None)\n+            )\n             hidden_states, gate = projected_states.chunk(2, dim=1)\n \n             if attention_mask is not None:\n@@ -518,7 +500,12 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.LongTensor] = None,\n     ):\n-        causal_conv1d_update, causal_conv1d_fn = _lazy_load_causal_conv1d()\n+        causal_conv1d = lazy_load_kernel(\"causal-conv1d\")\n+        causal_conv1d_update, causal_conv1d_fn = (\n+            (causal_conv1d.causal_conv1d_update, causal_conv1d.causal_conv1d_fn)\n+            if causal_conv1d is not None\n+            else (None, None)\n+        )\n         is_fast_path_available = all(\n             (selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)\n         )"
      },
      {
        "filename": "src/transformers/models/falcon_mamba/modular_falcon_mamba.py",
        "status": "modified",
        "additions": 19,
        "deletions": 6,
        "changes": 25,
        "patch": "@@ -19,6 +19,7 @@\n import torch\n from torch import nn\n \n+from ...integrations.hub_kernels import lazy_load_kernel\n from ...utils import auto_docstring, logging\n from ...utils.import_utils import (\n     is_mamba_ssm_available,\n@@ -35,7 +36,6 @@\n     MambaOutput,\n     MambaPreTrainedModel,\n     MambaRMSNorm,\n-    _lazy_load_causal_conv1d,\n )\n \n \n@@ -54,8 +54,6 @@\n else:\n     selective_state_update, selective_scan_fn, mamba_inner_fn = None, None, None\n \n-_causal_conv1d_cache = None\n-\n \n class FalconMambaConfig(MambaConfig):\n     \"\"\"\n@@ -258,7 +256,12 @@ def rms_forward(hidden_states, variance_epsilon=1e-6):\n \n class FalconMambaMixer(MambaMixer):\n     def warn_slow_implementation(self):\n-        causal_conv1d_update, causal_conv1d_fn = _lazy_load_causal_conv1d()\n+        causal_conv1d = lazy_load_kernel(\"causal-conv1d\")\n+        causal_conv1d_update, causal_conv1d_fn = (\n+            (causal_conv1d.causal_conv1d_update, causal_conv1d.causal_conv1d_fn)\n+            if causal_conv1d is not None\n+            else (None, None)\n+        )\n         is_fast_path_available = all(\n             (selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)\n         )\n@@ -324,7 +327,12 @@ def cuda_kernels_forward(\n             )\n \n         else:\n-            causal_conv1d_update, causal_conv1d_fn = _lazy_load_causal_conv1d()\n+            causal_conv1d = lazy_load_kernel(\"causal-conv1d\")\n+            causal_conv1d_update, causal_conv1d_fn = (\n+                (causal_conv1d.causal_conv1d_update, causal_conv1d.causal_conv1d_fn)\n+                if causal_conv1d is not None\n+                else (None, None)\n+            )\n             hidden_states, gate = projected_states.chunk(2, dim=1)\n \n             if attention_mask is not None:\n@@ -518,7 +526,12 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.LongTensor] = None,\n     ):\n-        causal_conv1d_update, causal_conv1d_fn = _lazy_load_causal_conv1d()\n+        causal_conv1d = lazy_load_kernel(\"causal-conv1d\")\n+        causal_conv1d_update, causal_conv1d_fn = (\n+            (causal_conv1d.causal_conv1d_update, causal_conv1d.causal_conv1d_fn)\n+            if causal_conv1d is not None\n+            else (None, None)\n+        )\n         is_fast_path_available = all(\n             (selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)\n         )"
      },
      {
        "filename": "src/transformers/models/mamba/modeling_mamba.py",
        "status": "modified",
        "additions": 19,
        "deletions": 31,
        "changes": 50,
        "patch": "@@ -25,6 +25,7 @@\n from ...activations import ACT2FN\n from ...configuration_utils import PreTrainedConfig\n from ...generation import GenerationMixin\n+from ...integrations.hub_kernels import lazy_load_kernel\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n@@ -33,8 +34,6 @@\n     logging,\n )\n from ...utils.import_utils import (\n-    is_causal_conv1d_available,\n-    is_kernels_available,\n     is_mamba_ssm_available,\n     is_mambapy_available,\n )\n@@ -54,32 +53,6 @@\n else:\n     selective_state_update, selective_scan_fn, mamba_inner_fn = None, None, None\n \n-_causal_conv1d_cache = None\n-\n-\n-def _lazy_load_causal_conv1d():\n-    global _causal_conv1d_cache\n-    if _causal_conv1d_cache is not None:\n-        return _causal_conv1d_cache\n-\n-    if is_kernels_available():\n-        from kernels import get_kernel\n-\n-        try:\n-            _causal_conv1d_kernel = get_kernel(\"kernels-community/causal-conv1d\")\n-        except FileNotFoundError:\n-            # no kernel binary match, fallback to slow path\n-            _causal_conv1d_cache = (None, None)\n-        else:\n-            _causal_conv1d_cache = (_causal_conv1d_kernel.causal_conv1d_update, _causal_conv1d_kernel.causal_conv1d_fn)\n-    elif is_causal_conv1d_available():\n-        from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\n-\n-        _causal_conv1d_cache = (causal_conv1d_update, causal_conv1d_fn)\n-    else:\n-        _causal_conv1d_cache = (None, None)\n-    return _causal_conv1d_cache\n-\n \n class MambaCache:\n     \"\"\"\n@@ -236,7 +209,12 @@ def __init__(self, config: MambaConfig, layer_idx: int):\n         self.warn_slow_implementation()\n \n     def warn_slow_implementation(self):\n-        causal_conv1d_update, causal_conv1d_fn = _lazy_load_causal_conv1d()\n+        causal_conv1d = lazy_load_kernel(\"causal-conv1d\")\n+        causal_conv1d_update, causal_conv1d_fn = (\n+            (causal_conv1d.causal_conv1d_update, causal_conv1d.causal_conv1d_fn)\n+            if causal_conv1d is not None\n+            else (None, None)\n+        )\n         is_fast_path_available = all(\n             (selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)\n         )\n@@ -287,7 +265,12 @@ def cuda_kernels_forward(\n             )\n \n         else:\n-            causal_conv1d_update, causal_conv1d_fn = _lazy_load_causal_conv1d()\n+            causal_conv1d = lazy_load_kernel(\"causal-conv1d\")\n+            causal_conv1d_update, causal_conv1d_fn = (\n+                (causal_conv1d.causal_conv1d_update, causal_conv1d.causal_conv1d_fn)\n+                if causal_conv1d is not None\n+                else (None, None)\n+            )\n             hidden_states, gate = projected_states.chunk(2, dim=1)\n \n             if attention_mask is not None:\n@@ -451,7 +434,12 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.LongTensor] = None,\n     ):\n-        causal_conv1d_update, causal_conv1d_fn = _lazy_load_causal_conv1d()\n+        causal_conv1d = lazy_load_kernel(\"causal-conv1d\")\n+        causal_conv1d_update, causal_conv1d_fn = (\n+            (causal_conv1d.causal_conv1d_update, causal_conv1d.causal_conv1d_fn)\n+            if causal_conv1d is not None\n+            else (None, None)\n+        )\n         is_fast_path_available = all(\n             (selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)\n         )"
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T21:17:36.120795",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR contains substantial refactoring with meaningful architectural changes: it centralizes kernel loading logic into a reusable `lazy_load_kernel` function with a global cache mapping system, eliminating duplicated code across multiple model files. The changes involve non-trivial logic for lazy loading, caching, fallback handling, and backward compatibility - enough substance to generate questions about kernel loading patterns, caching strategies, and module integration.",
      "substance_level": "high"
    }
  },
  {
    "pr_number": 41572,
    "title": "Gemma3 fixes",
    "body": "This PR fixes three things in `gemma3`:\r\n- a multiple-device error where `torch.where` takes some of its coefficients from a tensor that is not on the right device and is a full_like, so we just replace it with the filling element\r\n- an error in the `flash_attn_inference_equivalence` which is due to the model needing more parameters than are generated by defualt. To avoid this, we add a flag that specifies if we need to check the forward pass with training or not, and make this check default for both and left padding (cc. @vasqu )\r\n- the test `flash_attn_from_config` was failing for the same reasons (`token_type_ids` is required as a model input when training) so I added a `.eval()` to avoid this. It does not seem the model needs to be in train mode for this test, but I can also add an option to the test to only call `.eval()` if a flag is passed",
    "html_url": "https://github.com/huggingface/transformers/pull/41572",
    "created_at": "2025-10-14T11:02:36Z",
    "merged_at": "2025-10-14T16:33:27Z",
    "merge_commit_sha": "9e4199ede396f136b3dff1e918816fcc3a65f0a0",
    "base_ref": "main",
    "head_sha": "673cecc4db388bdaeaed9b001d20e13f1c65a0ff",
    "user": "remi-or",
    "files": [
      {
        "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -798,7 +798,7 @@ def create_causal_mask_mapping(\n         is_previous_image = nn.functional.pad(is_image, (1, 0), value=0)[:, :-1]\n         new_image_start = is_image & ~is_previous_image\n         image_group_ids = torch.cumsum(new_image_start.int(), dim=1) - 1\n-        image_group_ids = torch.where(is_image, image_group_ids, torch.full_like(token_type_ids, -1))\n+        image_group_ids = torch.where(is_image, image_group_ids, -1)\n         mask_kwargs[\"or_mask_function\"] = token_type_ids_mask_function(\n             token_type_ids.to(cache_position.device), image_group_ids\n         )"
      },
      {
        "filename": "src/transformers/models/gemma3/modular_gemma3.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -764,7 +764,7 @@ def create_causal_mask_mapping(\n         is_previous_image = nn.functional.pad(is_image, (1, 0), value=0)[:, :-1]\n         new_image_start = is_image & ~is_previous_image\n         image_group_ids = torch.cumsum(new_image_start.int(), dim=1) - 1\n-        image_group_ids = torch.where(is_image, image_group_ids, torch.full_like(token_type_ids, -1))\n+        image_group_ids = torch.where(is_image, image_group_ids, -1)\n         mask_kwargs[\"or_mask_function\"] = token_type_ids_mask_function(\n             token_type_ids.to(cache_position.device), image_group_ids\n         )"
      },
      {
        "filename": "tests/models/gemma3/test_modeling_gemma3.py",
        "status": "modified",
        "additions": 17,
        "deletions": 0,
        "changes": 17,
        "patch": "@@ -19,6 +19,7 @@\n \n import pytest\n from parameterized import parameterized\n+from pytest import mark\n \n from transformers import (\n     AutoModelForCausalLM,\n@@ -33,9 +34,11 @@\n     is_flash_attn_2_available,\n     require_deterministic_for_xpu,\n     require_flash_attn,\n+    require_flash_attn_3,\n     require_read_token,\n     require_torch,\n     require_torch_accelerator,\n+    require_torch_gpu,\n     require_torch_large_accelerator,\n     slow,\n     torch_device,\n@@ -342,6 +345,20 @@ def test_automodelforcausallm(self):\n             for_causal_lm = AutoModelForCausalLM.from_pretrained(tmp_dir)\n             self.assertIsInstance(for_causal_lm, Gemma3ForConditionalGeneration)\n \n+    @require_flash_attn\n+    @require_torch_gpu\n+    @mark.flash_attn_test\n+    @slow\n+    def test_flash_attn_2_from_config(self):\n+        self.flash_attn_from_config(attn_implementation=\"flash_attention_2\", test_fwd_in_train=False)\n+\n+    @require_flash_attn_3\n+    @require_torch_gpu\n+    @mark.flash_attn_3_test\n+    @slow\n+    def test_flash_attn_3_from_config(self):\n+        self.flash_attn_from_config(attn_implementation=\"flash_attention_3\", test_fwd_in_train=False)\n+\n \n @slow\n @require_torch_accelerator"
      },
      {
        "filename": "tests/test_modeling_common.py",
        "status": "modified",
        "additions": 10,
        "deletions": 5,
        "changes": 15,
        "patch": "@@ -2976,7 +2976,7 @@ def test_model_is_small(self):\n \n     def flash_attn_inference_equivalence(\n         self, attn_implementation: str, padding_side: str, atol: float = 4e-2, rtol: float = 4e-2\n-    ):\n+    ) -> None:\n         r\"\"\"\n         Tests the equivalence between the eager and flash attention implementations.\n         This test is only for inference and runs with `dtype=torch.bfloat16`.\n@@ -3114,9 +3114,6 @@ def flash_attn_inference_equivalence(\n                 torch.testing.assert_close(logits_1_eager, logits_1_fa, atol=atol, rtol=rtol)\n                 if padding_side == \"left\":\n                     torch.testing.assert_close(logits_2_eager[1:], logits_2_fa[1:], atol=atol, rtol=rtol)\n-                    # Check it can run in training mode\n-                    model.train()\n-                    _ = model(**second_inputs)\n                 else:\n                     torch.testing.assert_close(logits_2_eager[:-1], logits_2_fa[:-1], atol=atol, rtol=rtol)\n \n@@ -3651,7 +3648,7 @@ def test_flash_attn_2_can_compile_with_attention_mask_None_without_graph_break(s\n \n         assert not loss.isnan().any()\n \n-    def flash_attn_from_config(self, attn_implementation: str):\n+    def flash_attn_from_config(self, attn_implementation: str, test_fwd_in_train: bool = True):\n         r\"\"\"\n         Tests if the model can be loaded with `attn_implementation` from the config and if the\n         weights are not randomly initialized.\n@@ -3669,6 +3666,14 @@ def flash_attn_from_config(self, attn_implementation: str):\n                 config, attn_implementation=attn_implementation, dtype=torch.bfloat16\n             ).to(torch_device)\n \n+            # By default, we perform the forward pass in train mode, because it's more sctrict than eval mode. If the\n+            # forward pass is successful in train mode, it will also be successful in eval mode. But since some models\n+            # (eg. gemma3) need different inputs in train mode we have the option to test the forward pass in eval mode.\n+            if test_fwd_in_train:\n+                fa_model = fa_model.train()\n+            else:\n+                fa_model = fa_model.eval()\n+\n             dummy_input = inputs_dict[fa_model.main_input_name]\n             if dummy_input.dtype in [torch.float32, torch.float16]:\n                 dummy_input = dummy_input.to(torch.bfloat16)"
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T21:17:37.599993",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR contains non-trivial bug fixes addressing real issues in the Gemma3 model: a multi-device tensor placement error, a training/inference mode incompatibility in flash attention equivalence testing, and model input requirements. The changes involve understanding device management in PyTorch, attention mechanism implementations, and model state transitions - all substantive topics that would help developers understand the codebase.",
      "substance_level": "medium"
    }
  },
  {
    "pr_number": 41536,
    "title": "[Qwen3VL] fix device mismatch error for FSDP2 training",
    "body": "# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFor FSDP2, parameters might be on a meta device, and the weight.device attribute may not accurately reflect where the actual computation will happen during forward passes.\r\n\r\n```log\r\n  File \"transformers/models/qwen3_vl_moe/modeling_qwen3_vl_moe.py\", line 776, in forward\r\n    pos_embeds = self.fast_pos_embed_interpolate(grid_thw)\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"transformers/models/qwen3_vl_moe/modeling_qwen3_vl_moe.py\", line 745, in fast_pos_embed_interpolate\r\n    pos_embeds = self.pos_embed(idx_tensor) * weight_tensor[:, :, None]\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"torch/nn/modules/module.py\", line 1879, in _call_impl\r\n    return inner()\r\n           ^^^^^^^\r\n  File \"torch/nn/modules/module.py\", line 1827, in inner\r\n    result = forward_call(*args, **kwargs)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"torch/nn/modules/sparse.py\", line 192, in forward\r\n    return F.embedding(\r\n           ^^^^^^^^^^^^\r\n  File \"torch/nn/functional.py\", line 2546, in embedding\r\n    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nRuntimeError: Expected all tensors to be on the same device, but got index is on cpu, different from other tensors on cuda:0 (when checking argument in method wrapper_CUDA__index_select)\r\n```\r\nhttps://github.com/volcengine/verl/pull/3686#issuecomment-3380981817\r\n\r\nSince the device for grid_thw is pretty much dependent on the user-side implementation (passed as a parameter for the forward method), I think it's better to take the device of grid_thw for unifying the device of idx_tensor and weight_tensor, so that user-side implementation can have more control over this and to guarantee nothing can go wrong here. User-side code can ensure the input grid is on the same device as the positional embedding weight, as the size of grid_thw is small, so there shouldn't be too much overhead. This is tested on [verl](https://github.com/volcengine/verl) and worked fine.\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [X] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker @Cyrilvallez\r\n- vision models: @yonigozlan @molbap\r\n- audio models: @eustlb @ebezzam @vasqu\r\n- multimodal models: @zucchini-nlp\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- generate: @zucchini-nlp (visual-language models) or @gante (all others)\r\n- continuous batching: @remi-or @ArthurZucker @McPatate\r\n- pipelines: @Rocketknight1\r\n- tokenizers: @ArthurZucker and @itazap\r\n- trainer: @zach-huggingface @SunMarc\r\n- attention: @vasqu @ArthurZucker @CyrilVallez\r\n- model loading (from pretrained, etc): @CyrilVallez\r\n- distributed: @3outeille @ArthurZucker @S1ro1\r\n- CIs: @ydshieh\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @SunMarc @zach-huggingface\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber\r\n- kernels: @MekkCyber @drbh\r\n- peft: @BenjaminBossan @githubnemo\r\n\r\nDevices/Backends:\r\n\r\n- AMD ROCm: @ivarflakstad\r\n- Intel XPU: @IlyasMoutawwakil\r\n- Ascend NPU: @ivarflakstad \r\n\r\nDocumentation: @stevhliu\r\n\r\nResearch projects are not maintained and should be taken as is.\r\n\r\n -->\r\n\r\n@yonigozlan @molbap @ArthurZucker @Cyrilvallez @zucchini-nlp",
    "html_url": "https://github.com/huggingface/transformers/pull/41536",
    "created_at": "2025-10-12T18:46:38Z",
    "merged_at": "2025-10-14T10:28:25Z",
    "merge_commit_sha": "b3e3c3dc93f29770a768d6943c9fb9d377e5edce",
    "base_ref": "main",
    "head_sha": "90abc00f3183111d03d87929753817e8cef44861",
    "user": "HollowMan6",
    "files": [
      {
        "filename": "src/transformers/models/qwen3_omni_moe/modeling_qwen3_omni_moe.py",
        "status": "modified",
        "additions": 4,
        "deletions": 5,
        "changes": 9,
        "patch": "@@ -1099,6 +1099,7 @@ def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:\n \n     def fast_pos_embed_interpolate(self, grid_thw):\n         grid_ts, grid_hs, grid_ws = grid_thw[:, 0], grid_thw[:, 1], grid_thw[:, 2]\n+        device = grid_thw.device\n \n         idx_list = [[] for _ in range(4)]\n         weight_list = [[] for _ in range(4)]\n@@ -1136,11 +1137,9 @@ def fast_pos_embed_interpolate(self, grid_thw):\n                 idx_list[i].extend(indices[i].tolist())\n                 weight_list[i].extend(weights[i].tolist())\n \n-        idx_tensor = torch.tensor(idx_list, dtype=torch.long, device=self.pos_embed.weight.device)\n-        weight_tensor = torch.tensor(\n-            weight_list, dtype=self.pos_embed.weight.dtype, device=self.pos_embed.weight.device\n-        )\n-        pos_embeds = self.pos_embed(idx_tensor) * weight_tensor[:, :, None]\n+        idx_tensor = torch.tensor(idx_list, dtype=torch.long, device=device)\n+        weight_tensor = torch.tensor(weight_list, dtype=self.pos_embed.weight.dtype, device=device)\n+        pos_embeds = self.pos_embed(idx_tensor).to(device) * weight_tensor[:, :, None]\n         patch_pos_embeds = pos_embeds[0] + pos_embeds[1] + pos_embeds[2] + pos_embeds[3]\n \n         patch_pos_embeds = patch_pos_embeds.split([h * w for h, w in zip(grid_hs, grid_ws)])"
      },
      {
        "filename": "src/transformers/models/qwen3_vl/modeling_qwen3_vl.py",
        "status": "modified",
        "additions": 4,
        "deletions": 5,
        "changes": 9,
        "patch": "@@ -639,6 +639,7 @@ def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:\n \n     def fast_pos_embed_interpolate(self, grid_thw):\n         grid_ts, grid_hs, grid_ws = grid_thw[:, 0], grid_thw[:, 1], grid_thw[:, 2]\n+        device = grid_thw.device\n \n         idx_list = [[] for _ in range(4)]\n         weight_list = [[] for _ in range(4)]\n@@ -676,11 +677,9 @@ def fast_pos_embed_interpolate(self, grid_thw):\n                 idx_list[i].extend(indices[i].tolist())\n                 weight_list[i].extend(weights[i].tolist())\n \n-        idx_tensor = torch.tensor(idx_list, dtype=torch.long, device=self.pos_embed.weight.device)\n-        weight_tensor = torch.tensor(\n-            weight_list, dtype=self.pos_embed.weight.dtype, device=self.pos_embed.weight.device\n-        )\n-        pos_embeds = self.pos_embed(idx_tensor) * weight_tensor[:, :, None]\n+        idx_tensor = torch.tensor(idx_list, dtype=torch.long, device=device)\n+        weight_tensor = torch.tensor(weight_list, dtype=self.pos_embed.weight.dtype, device=device)\n+        pos_embeds = self.pos_embed(idx_tensor).to(device) * weight_tensor[:, :, None]\n         patch_pos_embeds = pos_embeds[0] + pos_embeds[1] + pos_embeds[2] + pos_embeds[3]\n \n         patch_pos_embeds = patch_pos_embeds.split([h * w for h, w in zip(grid_hs, grid_ws)])"
      },
      {
        "filename": "src/transformers/models/qwen3_vl/modular_qwen3_vl.py",
        "status": "modified",
        "additions": 4,
        "deletions": 5,
        "changes": 9,
        "patch": "@@ -615,6 +615,7 @@ def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:\n \n     def fast_pos_embed_interpolate(self, grid_thw):\n         grid_ts, grid_hs, grid_ws = grid_thw[:, 0], grid_thw[:, 1], grid_thw[:, 2]\n+        device = grid_thw.device\n \n         idx_list = [[] for _ in range(4)]\n         weight_list = [[] for _ in range(4)]\n@@ -652,11 +653,9 @@ def fast_pos_embed_interpolate(self, grid_thw):\n                 idx_list[i].extend(indices[i].tolist())\n                 weight_list[i].extend(weights[i].tolist())\n \n-        idx_tensor = torch.tensor(idx_list, dtype=torch.long, device=self.pos_embed.weight.device)\n-        weight_tensor = torch.tensor(\n-            weight_list, dtype=self.pos_embed.weight.dtype, device=self.pos_embed.weight.device\n-        )\n-        pos_embeds = self.pos_embed(idx_tensor) * weight_tensor[:, :, None]\n+        idx_tensor = torch.tensor(idx_list, dtype=torch.long, device=device)\n+        weight_tensor = torch.tensor(weight_list, dtype=self.pos_embed.weight.dtype, device=device)\n+        pos_embeds = self.pos_embed(idx_tensor).to(device) * weight_tensor[:, :, None]\n         patch_pos_embeds = pos_embeds[0] + pos_embeds[1] + pos_embeds[2] + pos_embeds[3]\n \n         patch_pos_embeds = patch_pos_embeds.split([h * w for h, w in zip(grid_hs, grid_ws)])"
      },
      {
        "filename": "src/transformers/models/qwen3_vl_moe/modeling_qwen3_vl_moe.py",
        "status": "modified",
        "additions": 4,
        "deletions": 5,
        "changes": 9,
        "patch": "@@ -661,6 +661,7 @@ def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:\n \n     def fast_pos_embed_interpolate(self, grid_thw):\n         grid_ts, grid_hs, grid_ws = grid_thw[:, 0], grid_thw[:, 1], grid_thw[:, 2]\n+        device = grid_thw.device\n \n         idx_list = [[] for _ in range(4)]\n         weight_list = [[] for _ in range(4)]\n@@ -698,11 +699,9 @@ def fast_pos_embed_interpolate(self, grid_thw):\n                 idx_list[i].extend(indices[i].tolist())\n                 weight_list[i].extend(weights[i].tolist())\n \n-        idx_tensor = torch.tensor(idx_list, dtype=torch.long, device=self.pos_embed.weight.device)\n-        weight_tensor = torch.tensor(\n-            weight_list, dtype=self.pos_embed.weight.dtype, device=self.pos_embed.weight.device\n-        )\n-        pos_embeds = self.pos_embed(idx_tensor) * weight_tensor[:, :, None]\n+        idx_tensor = torch.tensor(idx_list, dtype=torch.long, device=device)\n+        weight_tensor = torch.tensor(weight_list, dtype=self.pos_embed.weight.dtype, device=device)\n+        pos_embeds = self.pos_embed(idx_tensor).to(device) * weight_tensor[:, :, None]\n         patch_pos_embeds = pos_embeds[0] + pos_embeds[1] + pos_embeds[2] + pos_embeds[3]\n \n         patch_pos_embeds = patch_pos_embeds.split([h * w for h, w in zip(grid_hs, grid_ws)])"
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T21:17:44.165355",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR addresses a non-trivial device placement issue in FSDP2 training by changing how tensors are allocated to devices based on input parameters rather than model weights. The fix involves meaningful logic changes across multiple model files that demonstrate understanding of distributed training constraints and device management in PyTorch, providing sufficient substance for questions about distributed training, device handling, and positional embeddings.",
      "substance_level": "medium"
    }
  },
  {
    "pr_number": 41534,
    "title": "Add VideoMAE video processor ",
    "body": "## What does this PR do?\r\n\r\n- add a dedicated `VideoMAEVideoProcessor` that decodes/samples videos via TorchCodec and emits `pixel_values` ready for VideoMAE models\r\n- document the new processor alongside the existing image processor so users can discover the GPU-friendly path\r\n- cover the processor with torchvision-gated regression tests to ensure serialization, sampling, and output naming stay stable\r\n\r\nFixes #41520 \r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),\r\n      Pull Request section?\r\n- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case. (follow-up to the discussion with @zucchini-nlp on video processors)\r\n- [x] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [x] Did you write any new necessary tests?\r\n",
    "html_url": "https://github.com/huggingface/transformers/pull/41534",
    "created_at": "2025-10-12T14:05:59Z",
    "merged_at": "2025-10-13T13:42:27Z",
    "merge_commit_sha": "3813a8e3a1663993b3ec44c455cab8af1beca2b5",
    "base_ref": "main",
    "head_sha": "47e175c3ebcad2553581fe87d6d2f6aaee7b645a",
    "user": "Aki-07",
    "files": [
      {
        "filename": "docs/source/en/model_doc/videomae.md",
        "status": "modified",
        "additions": 5,
        "deletions": 0,
        "changes": 5,
        "patch": "@@ -90,6 +90,11 @@ to fine-tune a VideoMAE model on a custom dataset.\n [[autodoc]] VideoMAEImageProcessor\n     - preprocess\n \n+## VideoMAEVideoProcessor\n+\n+[[autodoc]] VideoMAEVideoProcessor\n+    - preprocess\n+\n ## VideoMAEModel\n \n [[autodoc]] VideoMAEModel"
      },
      {
        "filename": "src/transformers/models/auto/video_processing_auto.py",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -62,6 +62,7 @@\n             (\"sam2_video\", \"Sam2VideoVideoProcessor\"),\n             (\"smolvlm\", \"SmolVLMVideoProcessor\"),\n             (\"video_llava\", \"VideoLlavaVideoProcessor\"),\n+            (\"videomae\", \"VideoMAEVideoProcessor\"),\n             (\"vjepa2\", \"VJEPA2VideoProcessor\"),\n         ]\n     )"
      },
      {
        "filename": "src/transformers/models/videomae/__init__.py",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -22,6 +22,7 @@\n     from .feature_extraction_videomae import *\n     from .image_processing_videomae import *\n     from .modeling_videomae import *\n+    from .video_processing_videomae import *\n else:\n     import sys\n "
      },
      {
        "filename": "src/transformers/models/videomae/modeling_videomae.py",
        "status": "modified",
        "additions": 14,
        "deletions": 111,
        "changes": 125,
        "patch": "@@ -440,72 +440,25 @@ def forward(\n         Examples:\n \n         ```python\n-        >>> import av\n-        >>> import numpy as np\n-\n-        >>> from transformers import AutoImageProcessor, VideoMAEModel\n+        >>> import torch\n+        >>> from transformers import VideoMAEVideoProcessor, VideoMAEModel\n         >>> from huggingface_hub import hf_hub_download\n \n-        >>> np.random.seed(0)\n-\n-\n-        >>> def read_video_pyav(container, indices):\n-        ...     '''\n-        ...     Decode the video with PyAV decoder.\n-        ...     Args:\n-        ...         container (`av.container.input.InputContainer`): PyAV container.\n-        ...         indices (`list[int]`): List of frame indices to decode.\n-        ...     Returns:\n-        ...         result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n-        ...     '''\n-        ...     frames = []\n-        ...     container.seek(0)\n-        ...     start_index = indices[0]\n-        ...     end_index = indices[-1]\n-        ...     for i, frame in enumerate(container.decode(video=0)):\n-        ...         if i > end_index:\n-        ...             break\n-        ...         if i >= start_index and i in indices:\n-        ...             frames.append(frame)\n-        ...     return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n-\n-\n-        >>> def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n-        ...     '''\n-        ...     Sample a given number of frame indices from the video.\n-        ...     Args:\n-        ...         clip_len (`int`): Total number of frames to sample.\n-        ...         frame_sample_rate (`int`): Sample every n-th frame.\n-        ...         seg_len (`int`): Maximum allowed index of sample's last frame.\n-        ...     Returns:\n-        ...         indices (`list[int]`): List of sampled frame indices\n-        ...     '''\n-        ...     converted_len = int(clip_len * frame_sample_rate)\n-        ...     end_idx = np.random.randint(converted_len, seg_len)\n-        ...     start_idx = end_idx - converted_len\n-        ...     indices = np.linspace(start_idx, end_idx, num=clip_len)\n-        ...     indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n-        ...     return indices\n-\n-\n-        >>> # video clip consists of 300 frames (10 seconds at 30 FPS)\n-        >>> file_path = hf_hub_download(\n+        >>> # replace this with your own video file\n+        >>> video_path = hf_hub_download(\n         ...     repo_id=\"nielsr/video-demo\", filename=\"eating_spaghetti.mp4\", repo_type=\"dataset\"\n         ... )\n-        >>> container = av.open(file_path)\n \n-        >>> # sample 16 frames\n-        >>> indices = sample_frame_indices(clip_len=16, frame_sample_rate=1, seg_len=container.streams.video[0].frames)\n-        >>> video = read_video_pyav(container, indices)\n-\n-        >>> image_processor = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n+        >>> video_processor = VideoMAEVideoProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n         >>> model = VideoMAEModel.from_pretrained(\"MCG-NJU/videomae-base\")\n \n         >>> # prepare video for the model\n-        >>> inputs = image_processor(list(video), return_tensors=\"pt\")\n+        >>> inputs = video_processor(video_path, return_tensors=\"pt\")\n \n         >>> # forward pass\n-        >>> outputs = model(**inputs)\n+        >>> with torch.no_grad():\n+        ...     outputs = model(**inputs)\n+\n         >>> last_hidden_states = outputs.last_hidden_state\n         >>> list(last_hidden_states.shape)\n         [1, 1568, 768]\n@@ -764,69 +717,19 @@ def forward(\n         Examples:\n \n         ```python\n-        >>> import av\n         >>> import torch\n-        >>> import numpy as np\n-\n-        >>> from transformers import AutoImageProcessor, VideoMAEForVideoClassification\n+        >>> from transformers import VideoMAEVideoProcessor, VideoMAEForVideoClassification\n         >>> from huggingface_hub import hf_hub_download\n \n-        >>> np.random.seed(0)\n-\n-\n-        >>> def read_video_pyav(container, indices):\n-        ...     '''\n-        ...     Decode the video with PyAV decoder.\n-        ...     Args:\n-        ...         container (`av.container.input.InputContainer`): PyAV container.\n-        ...         indices (`list[int]`): List of frame indices to decode.\n-        ...     Returns:\n-        ...         result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n-        ...     '''\n-        ...     frames = []\n-        ...     container.seek(0)\n-        ...     start_index = indices[0]\n-        ...     end_index = indices[-1]\n-        ...     for i, frame in enumerate(container.decode(video=0)):\n-        ...         if i > end_index:\n-        ...             break\n-        ...         if i >= start_index and i in indices:\n-        ...             frames.append(frame)\n-        ...     return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n-\n-\n-        >>> def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n-        ...     '''\n-        ...     Sample a given number of frame indices from the video.\n-        ...     Args:\n-        ...         clip_len (`int`): Total number of frames to sample.\n-        ...         frame_sample_rate (`int`): Sample every n-th frame.\n-        ...         seg_len (`int`): Maximum allowed index of sample's last frame.\n-        ...     Returns:\n-        ...         indices (`list[int]`): List of sampled frame indices\n-        ...     '''\n-        ...     converted_len = int(clip_len * frame_sample_rate)\n-        ...     end_idx = np.random.randint(converted_len, seg_len)\n-        ...     start_idx = end_idx - converted_len\n-        ...     indices = np.linspace(start_idx, end_idx, num=clip_len)\n-        ...     indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n-        ...     return indices\n-\n-\n-        >>> # video clip consists of 300 frames (10 seconds at 30 FPS)\n-        >>> file_path = hf_hub_download(\n+        >>> # replace this with your own video file\n+        >>> video_path = hf_hub_download(\n         ...     repo_id=\"nielsr/video-demo\", filename=\"eating_spaghetti.mp4\", repo_type=\"dataset\"\n         ... )\n-        >>> container = av.open(file_path)\n-\n-        >>> # sample 16 frames\n-        >>> indices = sample_frame_indices(clip_len=16, frame_sample_rate=1, seg_len=container.streams.video[0].frames)\n-        >>> video = read_video_pyav(container, indices)\n \n-        >>> image_processor = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base-finetuned-kinetics\")\n+        >>> video_processor = VideoMAEVideoProcessor.from_pretrained(\"MCG-NJU/videomae-base-finetuned-kinetics\")\n         >>> model = VideoMAEForVideoClassification.from_pretrained(\"MCG-NJU/videomae-base-finetuned-kinetics\")\n \n-        >>> inputs = image_processor(list(video), return_tensors=\"pt\")\n+        >>> inputs = video_processor(video_path, return_tensors=\"pt\")\n \n         >>> with torch.no_grad():\n         ...     outputs = model(**inputs)"
      },
      {
        "filename": "src/transformers/models/videomae/video_processing_videomae.py",
        "status": "added",
        "additions": 43,
        "deletions": 0,
        "changes": 43,
        "patch": "@@ -0,0 +1,43 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Video processor class for VideoMAE.\"\"\"\n+\n+from ...image_utils import IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD, PILImageResampling\n+from ...video_processing_utils import BaseVideoProcessor\n+\n+\n+class VideoMAEVideoProcessor(BaseVideoProcessor):\n+    resample = PILImageResampling.BILINEAR\n+    image_mean = IMAGENET_STANDARD_MEAN\n+    image_std = IMAGENET_STANDARD_STD\n+    size = {\"shortest_edge\": 224}\n+    default_to_square = False\n+    crop_size = {\"height\": 224, \"width\": 224}\n+    do_resize = True\n+    do_center_crop = True\n+    do_rescale = True\n+    rescale_factor = 1 / 255\n+    do_normalize = True\n+    do_convert_rgb = True\n+    do_sample_frames = False  # Set to False for backward compatibility with image processor workflows.\n+    model_input_names = [\"pixel_values\"]\n+\n+    def preprocess(self, videos, **kwargs):\n+        batch = super().preprocess(videos, **kwargs)\n+        batch[\"pixel_values\"] = batch.pop(\"pixel_values_videos\")\n+        return batch\n+\n+\n+__all__ = [\"VideoMAEVideoProcessor\"]"
      },
      {
        "filename": "tests/models/videomae/test_video_processing_videomae.py",
        "status": "added",
        "additions": 160,
        "deletions": 0,
        "changes": 160,
        "patch": "@@ -0,0 +1,160 @@\n+# coding=utf-8\n+# Copyright 2025 HuggingFace Inc.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import unittest\n+\n+import torch\n+from PIL import Image\n+\n+from transformers.image_utils import IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD\n+from transformers.testing_utils import require_torch, require_torchvision, require_vision\n+from transformers.utils import is_torchvision_available, is_vision_available\n+\n+from ...test_video_processing_common import VideoProcessingTestMixin, prepare_video_inputs\n+\n+\n+if is_vision_available():\n+    if is_torchvision_available():\n+        from transformers import VideoMAEImageProcessor, VideoMAEVideoProcessor\n+\n+\n+class VideoMAEVideoProcessingTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=5,\n+        num_frames=8,\n+        num_channels=3,\n+        image_size=18,\n+        min_resolution=30,\n+        max_resolution=80,\n+        do_resize=True,\n+        size=None,\n+        do_center_crop=True,\n+        crop_size=None,\n+        do_rescale=True,\n+        rescale_factor=1 / 255,\n+        do_normalize=True,\n+        image_mean=IMAGENET_STANDARD_MEAN,\n+        image_std=IMAGENET_STANDARD_STD,\n+        do_convert_rgb=True,\n+    ):\n+        super().__init__()\n+        size = size if size is not None else {\"shortest_edge\": 20}\n+        crop_size = crop_size if crop_size is not None else {\"height\": 18, \"width\": 18}\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.num_frames = num_frames\n+        self.num_channels = num_channels\n+        self.image_size = image_size\n+        self.min_resolution = min_resolution\n+        self.max_resolution = max_resolution\n+        self.do_resize = do_resize\n+        self.size = size\n+        self.do_center_crop = do_center_crop\n+        self.crop_size = crop_size\n+        self.do_rescale = do_rescale\n+        self.rescale_factor = rescale_factor\n+        self.do_normalize = do_normalize\n+        self.image_mean = image_mean\n+        self.image_std = image_std\n+        self.do_convert_rgb = do_convert_rgb\n+\n+    def prepare_video_processor_dict(self):\n+        return {\n+            \"do_resize\": self.do_resize,\n+            \"size\": self.size,\n+            \"do_center_crop\": self.do_center_crop,\n+            \"crop_size\": self.crop_size,\n+            \"do_rescale\": self.do_rescale,\n+            \"rescale_factor\": self.rescale_factor,\n+            \"do_normalize\": self.do_normalize,\n+            \"image_mean\": self.image_mean,\n+            \"image_std\": self.image_std,\n+            \"do_convert_rgb\": self.do_convert_rgb,\n+        }\n+\n+    def expected_output_video_shape(self, videos):\n+        return self.num_frames, self.num_channels, self.crop_size[\"height\"], self.crop_size[\"width\"]\n+\n+    def prepare_video_inputs(self, equal_resolution=False, return_tensors=\"pil\"):\n+        videos = prepare_video_inputs(\n+            batch_size=self.batch_size,\n+            num_frames=self.num_frames,\n+            num_channels=self.num_channels,\n+            min_resolution=self.min_resolution,\n+            max_resolution=self.max_resolution,\n+            equal_resolution=equal_resolution,\n+            return_tensors=return_tensors,\n+        )\n+\n+        return videos\n+\n+\n+@require_torch\n+@require_vision\n+@require_torchvision\n+class VideoMAEVideoProcessingTest(VideoProcessingTestMixin, unittest.TestCase):\n+    fast_video_processing_class = VideoMAEVideoProcessor if is_torchvision_available() else None\n+    input_name = \"pixel_values\"\n+\n+    def setUp(self):\n+        super().setUp()\n+        self.video_processor_tester = VideoMAEVideoProcessingTester(self)\n+\n+    @property\n+    def video_processor_dict(self):\n+        return self.video_processor_tester.prepare_video_processor_dict()\n+\n+    def test_video_processor_properties(self):\n+        video_processing = self.fast_video_processing_class(**self.video_processor_dict)\n+        self.assertTrue(hasattr(video_processing, \"do_resize\"))\n+        self.assertTrue(hasattr(video_processing, \"size\"))\n+        self.assertTrue(hasattr(video_processing, \"do_center_crop\"))\n+        self.assertTrue(hasattr(video_processing, \"center_crop\"))\n+        self.assertTrue(hasattr(video_processing, \"do_normalize\"))\n+        self.assertTrue(hasattr(video_processing, \"image_mean\"))\n+        self.assertTrue(hasattr(video_processing, \"image_std\"))\n+        self.assertTrue(hasattr(video_processing, \"do_convert_rgb\"))\n+        self.assertTrue(hasattr(video_processing, \"model_input_names\"))\n+        self.assertIn(\"pixel_values\", video_processing.model_input_names)\n+\n+    def test_pixel_value_identity(self):\n+        \"\"\"\n+        Verify that VideoMAEVideoProcessor (TorchCodec-based) produces pixel tensors\n+        numerically similar to those from VideoMAEImageProcessor (PIL-based).\n+        Minor (<1%) differences are expected due to color conversion and interpolation.\n+        \"\"\"\n+        video = self.video_processor_tester.prepare_video_inputs(return_tensors=\"np\")\n+        video_processor = VideoMAEVideoProcessor(**self.video_processor_dict)\n+        image_processor = VideoMAEImageProcessor(**self.video_processor_dict)\n+\n+        video_frames_np = video[0]\n+        video_frames_pil = [Image.fromarray(frame.astype(\"uint8\")) for frame in video_frames_np]\n+        video_out = video_processor(video_frames_pil, return_tensors=\"pt\")\n+        image_out = image_processor(video_frames_pil, return_tensors=\"pt\")\n+\n+        torch.testing.assert_close(\n+            video_out[\"pixel_values\"],\n+            image_out[\"pixel_values\"],\n+            rtol=5e-2,\n+            atol=1e-2,\n+            msg=(\n+                \"Pixel values differ slightly between VideoMAEVideoProcessor \"\n+                \"and VideoMAEImageProcessor. \"\n+                \"Differences \u22641% are expected due to YUV\u2192RGB conversion and \"\n+                \"interpolation behavior in different decoders.\"\n+            ),\n+        )"
      }
    ],
    "num_files": 6,
    "scraped_at": "2025-11-16T21:17:44.791440",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR introduces a new specialized video processor component (VideoMAEVideoProcessor) with meaningful architectural decisions, including inheritance from BaseVideoProcessor, frame sampling logic, and output normalization. The PR includes comprehensive tests, documentation updates, and auto-registration, providing substantial material for questions about video processing workflows, processor design patterns, and integration with the transformers ecosystem.",
      "substance_level": "high"
    }
  },
  {
    "pr_number": 41449,
    "title": "Fix trainer simple tests",
    "body": "# What does this PR do?\r\n\r\nThis PR should all simple trainer tests and some deepspeed tests. \r\nThe only remaining tests to fix are deepspeed z2 grad acc tests but this is strange why it is failing ... cc @IlyasMoutawwakil maybe you have an idea as you worked on it for `HPU` ",
    "html_url": "https://github.com/huggingface/transformers/pull/41449",
    "created_at": "2025-10-08T12:49:08Z",
    "merged_at": "2025-10-15T12:09:00Z",
    "merge_commit_sha": "70e871959c3ced65ee4804a55fb27b37876db2bf",
    "base_ref": "main",
    "head_sha": "9d556dd8a2b26cc511250fcb30442a0d2699e1a4",
    "user": "SunMarc",
    "files": [
      {
        "filename": "src/transformers/integrations/integration_utils.py",
        "status": "modified",
        "additions": 7,
        "deletions": 5,
        "changes": 12,
        "patch": "@@ -302,7 +302,7 @@ def run_hp_search_ray(trainer, n_trials: int, direction: str, **kwargs) -> BestR\n             for more options\n     \"\"\"\n     import ray\n-    import ray.train\n+    import ray.tune\n \n     def _objective(trial: dict, local_trainer):\n         try:\n@@ -315,7 +315,7 @@ def _objective(trial: dict, local_trainer):\n \n         local_trainer.objective = None\n \n-        checkpoint = ray.train.get_checkpoint()\n+        checkpoint = ray.tune.get_checkpoint()\n         if checkpoint:\n             # Upon trial resume, the local_trainer's objective gets reset to None.\n             # If `local_trainer.train` is a noop (training has already reached\n@@ -339,8 +339,8 @@ def _objective(trial: dict, local_trainer):\n \n             with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n                 local_trainer._tune_save_checkpoint(checkpoint_dir=temp_checkpoint_dir)\n-                checkpoint = ray.train.Checkpoint.from_directory(temp_checkpoint_dir)\n-                ray.train.report(metrics, checkpoint=checkpoint)\n+                checkpoint = ray.tune.Checkpoint.from_directory(temp_checkpoint_dir)\n+                ray.tune.report(metrics, checkpoint=checkpoint)\n \n     if not trainer._memory_tracker.skip_memory_metrics:\n         from ..trainer_utils import TrainerMemoryTracker\n@@ -406,7 +406,9 @@ def dynamic_modules_import_trainable(*args, **kwargs):\n \n         Assumes that `_objective`, defined above, is a function.\n         \"\"\"\n-        if is_datasets_available():\n+        if is_datasets_available() and packaging.version.parse(\n+            importlib.metadata.version(\"datasets\")\n+        ) < packaging.version.parse(\"4.0.0\"):\n             import datasets.load\n \n             dynamic_modules_path = os.path.join(datasets.load.init_dynamic_modules(), \"__init__.py\")"
      },
      {
        "filename": "src/transformers/modeling_utils.py",
        "status": "modified",
        "additions": 7,
        "deletions": 3,
        "changes": 10,
        "patch": "@@ -5140,12 +5140,15 @@ def set_is_initialized_for_modules(module):\n             # A module is already initialized if and only if all its children are also already initialized, and all\n             # its immediate `nn.Parameter` and persistent buffers are also already initialized\n             if (\n+                # All immediate children are initialized\n                 all(getattr(child, \"_is_hf_initialized\", False) for child in module.children())\n+                # All immediate parameters are initialized\n                 and all(getattr(param, \"_is_hf_initialized\", False) for param in module.parameters(recurse=False))\n+                # All immediate persistent buffers are initialized\n                 and all(\n                     getattr(buffer, \"_is_hf_initialized\", False)\n-                    for buffer in module.buffers(recurse=False)\n-                    if buffer not in module._non_persistent_buffers_set\n+                    for name, buffer in module.named_buffers(recurse=False)\n+                    if name not in module._non_persistent_buffers_set\n                 )\n             ):\n                 module._is_hf_initialized = True\n@@ -5159,8 +5162,9 @@ def set_is_initialized_for_modules(module):\n         if is_deepspeed_zero3_enabled() and not is_quantized:\n             import deepspeed\n \n+            # keep_vars=True as we need the original tensors, so that the \"_is_hf_initialized\" is present on them\n             not_initialized_parameters = list(\n-                {v for v in self.state_dict().values() if not getattr(v, \"_is_hf_initialized\", False)}\n+                {v for v in self.state_dict(keep_vars=True).values() if not getattr(v, \"_is_hf_initialized\", False)}\n             )\n             with deepspeed.zero.GatheredParameters(not_initialized_parameters, modifier_rank=0):\n                 self.initialize_weights()"
      },
      {
        "filename": "src/transformers/trainer.py",
        "status": "modified",
        "additions": 9,
        "deletions": 8,
        "changes": 17,
        "patch": "@@ -1846,15 +1846,15 @@ def _report_to_hp_search(self, trial: Union[\"optuna.Trial\", dict[str, Any]], ste\n                     self.callback_handler.on_train_end(self.args, self.state, self.control)\n                     raise optuna.TrialPruned()\n         elif self.hp_search_backend == HPSearchBackend.RAY:\n-            import ray.train\n+            import ray.tune\n \n             with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n                 checkpoint = None\n                 if self.control.should_save:\n                     self._tune_save_checkpoint(checkpoint_dir=temp_checkpoint_dir)\n-                    checkpoint = ray.train.Checkpoint.from_directory(temp_checkpoint_dir)\n+                    checkpoint = ray.tune.Checkpoint.from_directory(temp_checkpoint_dir)\n                 metrics[\"objective\"] = self.objective\n-                ray.train.report(metrics, checkpoint=checkpoint)\n+                ray.tune.report(metrics, checkpoint=checkpoint)\n \n     def _tune_save_checkpoint(self, checkpoint_dir: str):\n         output_dir = os.path.join(checkpoint_dir, f\"{PREFIX_CHECKPOINT_DIR}-{self.state.global_step}\")\n@@ -2654,9 +2654,9 @@ def _get_output_dir(self, trial):\n             if self.hp_search_backend == HPSearchBackend.OPTUNA:\n                 run_id = trial.number\n             elif self.hp_search_backend == HPSearchBackend.RAY:\n-                import ray.train\n+                import ray.tune\n \n-                run_id = ray.train.get_context().get_trial_id()\n+                run_id = ray.tune.get_context().get_trial_id()\n             elif self.hp_search_backend == HPSearchBackend.WANDB:\n                 import wandb\n \n@@ -5099,9 +5099,10 @@ def _get_num_items_in_batch(self, batch_samples: list, device: torch.device) ->\n                 pass\n \n         if num_items_in_batch is not None:\n-            if self.args.average_tokens_across_devices and self.args.world_size >= 1:\n-                num_items_in_batch = self.accelerator.gather(num_items_in_batch.to(device)).sum()\n-            elif self.args.n_gpu >= 1:\n+            if self.args.average_tokens_across_devices:\n+                if self.args.world_size > 1:\n+                    num_items_in_batch = self.accelerator.gather(num_items_in_batch.to(device)).sum()\n+            elif self.args.n_gpu > 1:\n                 # In DP case, if we don't average, we need to divide by the number of gpu. This is the simplest approximation.\n                 # Otherwise, we would have to scatter labels and calculate num_items_in_batch for each gpu.\n                 num_items_in_batch = num_items_in_batch // self.args.n_gpu"
      },
      {
        "filename": "tests/deepspeed/test_model_zoo.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -182,7 +182,7 @@ def make_task_cmds():\n             \"pegasus\",\n         ],\n         \"clm\": [\n-            \"big_bird\",\n+            # \"big_bird\", not use why there is an issue with the architecture, some modules are not ZeROOrderedDict suddenly\n             \"bigbird_pegasus\",\n             \"blenderbot\",\n             \"bloom\","
      },
      {
        "filename": "tests/trainer/test_trainer.py",
        "status": "modified",
        "additions": 55,
        "deletions": 146,
        "changes": 201,
        "patch": "@@ -46,7 +46,6 @@\n     TrainerCallback,\n     TrainingArguments,\n     default_data_collator,\n-    enable_full_determinism,\n     get_polynomial_decay_schedule_with_warmup,\n     is_datasets_available,\n     is_torch_available,\n@@ -67,10 +66,8 @@\n     backend_max_memory_allocated,\n     backend_memory_allocated,\n     backend_reset_max_memory_allocated,\n-    backend_reset_peak_memory_stats,\n     evaluate_side_effect_factory,\n     execute_subprocess_async,\n-    get_gpu_count,\n     get_steps_per_epoch,\n     get_tests_dir,\n     is_staging_test,\n@@ -97,9 +94,7 @@\n     require_torch_gpu,\n     require_torch_multi_accelerator,\n     require_torch_non_multi_accelerator,\n-    require_torch_non_multi_gpu,\n     require_torch_optimi,\n-    require_torch_tensorrt_fx,\n     require_torch_tf32,\n     require_torch_up_to_2_accelerators,\n     require_vision,\n@@ -580,7 +575,7 @@ def get_regression_trainer(\n         preprocess_logits_for_metrics = kwargs.pop(\"preprocess_logits_for_metrics\", None)\n         assert output_dir is not None, \"output_dir should be specified for testing\"\n         args = RegressionTrainingArguments(output_dir, a=a, b=b, keep_report_to=keep_report_to, **kwargs)\n-        return Trainer(\n+        trainer = Trainer(\n             model,\n             args,\n             data_collator=data_collator,\n@@ -591,6 +586,9 @@ def get_regression_trainer(\n             model_init=model_init,\n             preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n         )\n+        # TODO: loss function defined in RegressionModel doesn't accept num_item_per_batch, to fix later\n+        trainer.model_accepts_loss_kwargs = False\n+        return trainer\n \n     def get_language_model_trainer(**kwargs):\n         dataset = datasets.load_dataset(\"fka/awesome-chatgpt-prompts\")\n@@ -1948,44 +1946,54 @@ def test_use_liger_kernel_custom_config_patching(self):\n \n     @require_liger_kernel\n     @require_torch_accelerator\n+    @require_torch_non_multi_accelerator  # Don't work with DP\n     def test_use_liger_kernel_trainer(self):\n-        # Check that trainer still works with liger kernel applied\n-        config = LlamaConfig(vocab_size=100, hidden_size=32, num_hidden_layers=3, num_attention_heads=4)\n-        tiny_llama = LlamaForCausalLM(config)\n+        # Ensure any monkey patching is cleaned up for subsequent tests\n+        with patch(\"transformers.models.llama.modeling_llama\"):\n+            # Check that trainer still works with liger kernel applied\n+            config = LlamaConfig(vocab_size=100, hidden_size=32, num_hidden_layers=3, num_attention_heads=4)\n+            tiny_llama = LlamaForCausalLM(config)\n \n-        x = torch.randint(0, 100, (128,))\n-        train_dataset = RepeatDataset(x)\n+            x = torch.randint(0, 100, (128,))\n+            train_dataset = RepeatDataset(x)\n \n-        args = TrainingArguments(\n-            self.get_auto_remove_tmp_dir(), learning_rate=1e-2, logging_steps=5, max_steps=20, use_liger_kernel=True\n-        )\n-        trainer = Trainer(tiny_llama, args, train_dataset=train_dataset)\n+            args = TrainingArguments(\n+                self.get_auto_remove_tmp_dir(),\n+                learning_rate=1e-2,\n+                logging_steps=5,\n+                max_steps=20,\n+                use_liger_kernel=True,\n+            )\n+            trainer = Trainer(tiny_llama, args, train_dataset=train_dataset)\n \n-        # Check this works\n-        _ = trainer.train()\n+            # Check this works\n+            _ = trainer.train()\n \n     @require_liger_kernel\n     @require_torch_accelerator\n+    @require_torch_non_multi_accelerator  # don't work with DP\n     def test_use_liger_kernel_custom_config_trainer(self):\n-        # Check that trainer still works with liger kernel applied when using a custom config\n-        config = LlamaConfig(vocab_size=100, hidden_size=32, num_hidden_layers=3, num_attention_heads=4)\n-        tiny_llama = LlamaForCausalLM(config)\n+        # Ensure any monkey patching is cleaned up for subsequent tests\n+        with patch(\"transformers.models.llama.modeling_llama\"):\n+            # Check that trainer still works with liger kernel applied when using a custom config\n+            config = LlamaConfig(vocab_size=100, hidden_size=32, num_hidden_layers=3, num_attention_heads=4)\n+            tiny_llama = LlamaForCausalLM(config)\n \n-        x = torch.randint(0, 100, (128,))\n-        train_dataset = RepeatDataset(x)\n+            x = torch.randint(0, 100, (128,))\n+            train_dataset = RepeatDataset(x)\n \n-        args = TrainingArguments(\n-            self.get_auto_remove_tmp_dir(),\n-            learning_rate=1e-2,\n-            logging_steps=5,\n-            max_steps=20,\n-            use_liger_kernel=True,\n-            liger_kernel_config={\"rms_norm\": False, \"cross_entropy\": True, \"fused_linear_cross_entropy\": False},\n-        )\n-        trainer = Trainer(tiny_llama, args, train_dataset=train_dataset)\n+            args = TrainingArguments(\n+                self.get_auto_remove_tmp_dir(),\n+                learning_rate=1e-2,\n+                logging_steps=5,\n+                max_steps=20,\n+                use_liger_kernel=True,\n+                liger_kernel_config={\"rms_norm\": False, \"cross_entropy\": True, \"fused_linear_cross_entropy\": False},\n+            )\n+            trainer = Trainer(tiny_llama, args, train_dataset=train_dataset)\n \n-        # Check this works\n-        _ = trainer.train()\n+            # Check this works\n+            _ = trainer.train()\n \n     @require_lomo\n     @require_torch_accelerator\n@@ -3280,7 +3288,6 @@ def test_can_resume_training_lm(self):\n         training_steps = 10\n         resume_from_step = 8\n         with tempfile.TemporaryDirectory() as tmpdir:\n-            enable_full_determinism(0)\n             kwargs = {\n                 \"output_dir\": tmpdir,\n                 \"fp16\": True,\n@@ -3314,7 +3321,6 @@ def test_can_resume_training_lm(self):\n             )\n \n             # Checkpoint at intermediate step\n-            enable_full_determinism(0)\n             checkpoint = os.path.join(tmpdir, f\"checkpoint-{resume_from_step + 1}\")\n             trainer = get_language_model_trainer(**kwargs)\n             trainer.train(resume_from_checkpoint=checkpoint)\n@@ -3812,7 +3818,6 @@ def test_evaluation_iterable_dataset(self):\n             args = RegressionTrainingArguments(output_dir=tmp_dir)\n             trainer = Trainer(model=model, args=args, eval_dataset=eval_dataset, compute_metrics=AlmostAccuracy())\n             results = trainer.evaluate()\n-\n             x, y = trainer.eval_dataset.dataset.x, trainer.eval_dataset.dataset.ys[0]\n             pred = 1.5 * x + 2.5\n             expected_loss = ((pred - y) ** 2).mean()\n@@ -3839,7 +3844,6 @@ def test_predict_iterable_dataset(self):\n         with tempfile.TemporaryDirectory() as tmp_dir:\n             args = RegressionTrainingArguments(output_dir=tmp_dir)\n             trainer = Trainer(model=model, args=args, eval_dataset=eval_dataset, compute_metrics=AlmostAccuracy())\n-\n             preds = trainer.predict(trainer.eval_dataset).predictions\n             x = eval_dataset.dataset.x\n             self.assertTrue(np.allclose(preds, 1.5 * x + 2.5))\n@@ -4139,124 +4143,29 @@ def test_fp16_full_eval(self):\n             self.assertAlmostEqual(fp16_eval, fp32_init / 2, delta=5_000)\n \n     @require_torch_gpu\n-    @require_torch_non_multi_gpu\n-    @require_torch_tensorrt_fx\n-    def test_torchdynamo_full_eval(self):\n-        from torch import _dynamo as torchdynamo\n-\n-        # torchdynamo at the moment doesn't support DP/DDP, therefore require a single gpu\n-        n_gpus = get_gpu_count()\n+    @pytest.mark.torch_compile_test\n+    def test_torch_compile_train(self):\n+        with tempfile.TemporaryDirectory() as tmp_dir:\n+            trainer = get_regression_trainer(output_dir=tmp_dir)\n+            metrics = trainer.train()\n+            original_train_loss = metrics.training_loss\n \n-        bs = 8\n-        eval_len = 16 * n_gpus\n-        # make the params are somewhat big so that there will be enough RAM consumed to be able to\n-        # measure things. We should get about 64KB for a+b in fp32\n-        a = torch.ones(1000, bs) + 0.001\n-        b = torch.ones(1000, bs) - 0.001\n+            trainer = get_regression_trainer(torch_compile=True, output_dir=tmp_dir)\n+            metrics = trainer.train()\n+            self.assertAlmostEqual(metrics.training_loss, original_train_loss)\n \n+    @require_torch_gpu\n+    @pytest.mark.torch_compile_test\n+    def test_torch_compile_eval(self):\n         with tempfile.TemporaryDirectory() as tmp_dir:\n-            # 1. Default - without TorchDynamo\n-            trainer = get_regression_trainer(a=a, b=b, eval_len=eval_len, output_dir=tmp_dir)\n+            trainer = get_regression_trainer(output_dir=tmp_dir)\n             metrics = trainer.evaluate()\n             original_eval_loss = metrics[\"eval_loss\"]\n-            del trainer\n \n-            # 2. TorchDynamo eager\n-            trainer = get_regression_trainer(\n-                a=a, b=b, eval_len=eval_len, torch_compile_backend=\"eager\", output_dir=tmp_dir\n-            )\n+            trainer = get_regression_trainer(torch_compile=True, output_dir=tmp_dir)\n             metrics = trainer.evaluate()\n-            self.assertAlmostEqual(metrics[\"eval_loss\"], original_eval_loss)\n-            del trainer\n-            torchdynamo.reset()\n \n-            # 3. TorchDynamo nvfuser\n-            trainer = get_regression_trainer(\n-                a=a, b=b, eval_len=eval_len, torch_compile_backend=\"nvfuser\", output_dir=tmp_dir\n-            )\n-            metrics = trainer.evaluate()\n-            self.assertAlmostEqual(metrics[\"eval_loss\"], original_eval_loss)\n-            torchdynamo.reset()\n-\n-            # 4. TorchDynamo fx2trt\n-            trainer = get_regression_trainer(\n-                a=a, b=b, eval_len=eval_len, torch_compile_backend=\"fx2trt\", output_dir=tmp_dir\n-            )\n-            metrics = trainer.evaluate()\n             self.assertAlmostEqual(metrics[\"eval_loss\"], original_eval_loss)\n-            torchdynamo.reset()\n-\n-    @require_torch_non_multi_gpu\n-    @require_torch_gpu\n-    def test_torchdynamo_memory(self):\n-        # torchdynamo at the moment doesn't support DP/DDP, therefore require a single gpu\n-        from torch import _dynamo as torchdynamo\n-\n-        class CustomTrainer(Trainer):\n-            def compute_loss(self, model, inputs, num_items_in_batch=None, return_outputs=False):\n-                x = inputs[\"x\"]\n-                output = model(x)\n-                if self.args.n_gpu == 1:\n-                    return output.mean()\n-                return output\n-\n-        class MyModule(torch.nn.Module):\n-            \"\"\"Simple module that does aggressive fusion\"\"\"\n-\n-            def __init__(self):\n-                super().__init__()\n-\n-            def forward(self, x):\n-                for _ in range(20):\n-                    x = torch.cos(x)\n-                return x\n-\n-        mod = MyModule()\n-\n-        # 1. without TorchDynamo (eager baseline)\n-        a = torch.ones(1024, 1024, device=torch_device, requires_grad=True)\n-        a.grad = None\n-        trainer = CustomTrainer(model=mod)\n-        # warmup\n-        for _ in range(10):\n-            orig_loss = trainer.training_step(mod, {\"x\": a})\n-\n-        # resets\n-        gc.collect()\n-        backend_empty_cache(torch_device)\n-        backend_reset_peak_memory_stats(torch_device)\n-\n-        orig_loss = trainer.training_step(mod, {\"x\": a})\n-        orig_peak_mem = backend_max_memory_allocated(torch_device)\n-        torchdynamo.reset()\n-        del trainer\n-\n-        # 2. TorchDynamo nvfuser\n-        with tempfile.TemporaryDirectory() as tmp_dir:\n-            a = torch.ones(1024, 1024, device=torch_device, requires_grad=True)\n-            a.grad = None\n-            args = TrainingArguments(output_dir=tmp_dir, torch_compile_backend=\"nvfuser\")\n-            trainer = CustomTrainer(model=mod, args=args)\n-            # warmup\n-            for _ in range(10):\n-                loss = trainer.training_step(mod, {\"x\": a})\n-\n-            # resets\n-            gc.collect()\n-            backend_empty_cache(torch_device)\n-            backend_reset_peak_memory_stats(torch_device)\n-\n-            loss = trainer.training_step(mod, {\"x\": a})\n-            peak_mem = backend_max_memory_allocated(torch_device)\n-            torchdynamo.reset()\n-            del trainer\n-\n-            # Functional check\n-            self.assertAlmostEqual(loss, orig_loss)\n-\n-            # AOT Autograd recomputation and nvfuser recomputation optimization\n-            # aggressively fuses the operations and reduce the memory footprint.\n-            self.assertGreater(orig_peak_mem, peak_mem * 2)\n \n     @require_torch_accelerator\n     @require_torch_bf16"
      },
      {
        "filename": "tests/trainer/test_trainer_seq2seq.py",
        "status": "modified",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "patch": "@@ -38,8 +38,8 @@ def test_finetune_bert2bert(self):\n         tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n \n         bert2bert.config.vocab_size = bert2bert.config.encoder.vocab_size\n-        bert2bert.config.eos_token_id = tokenizer.sep_token_id\n-        bert2bert.config.decoder_start_token_id = tokenizer.cls_token_id\n+        tokenizer.eos_token_id = tokenizer.sep_token_id\n+        bert2bert.generation_config.decoder_start_token_id = tokenizer.cls_token_id\n         bert2bert.config.max_length = 128\n \n         train_dataset = datasets.load_dataset(\"abisee/cnn_dailymail\", \"3.0.0\", split=\"train[:1%]\")"
      }
    ],
    "num_files": 6,
    "scraped_at": "2025-11-16T21:18:00.090536",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR contains non-trivial bug fixes across multiple components including Ray integration changes (ray.train \u2192 ray.tune API updates), model initialization logic fixes for DeepSpeed zero3, trainer hyperparameter search fixes, and token averaging logic corrections. The changes involve actual logic modifications and architectural decisions that affect how the trainer handles model initialization, checkpoint management, and distributed training scenarios. The PR description provides meaningful context about fixing trainer tests and mentions specific issues like deepspeed z2 grad accumulation problems.",
      "substance_level": "medium"
    }
  },
  {
    "pr_number": 41446,
    "title": "Enable non-streaming mode in `transformers serve`",
    "body": "Needs this to be merged first: https://github.com/huggingface/transformers/pull/41444\r\n\r\nTests and docs need to be added before undraft",
    "html_url": "https://github.com/huggingface/transformers/pull/41446",
    "created_at": "2025-10-08T11:53:19Z",
    "merged_at": "2025-10-15T07:37:26Z",
    "merge_commit_sha": "13a35a5057cbdc34b6c93a26d4e57987cbdd205c",
    "base_ref": "main",
    "head_sha": "a0c6b40d13c79614c1440b9feb4c5568626af8f3",
    "user": "LysandreJik",
    "files": [
      {
        "filename": "src/transformers/commands/serving.py",
        "status": "modified",
        "additions": 157,
        "deletions": 52,
        "changes": 209,
        "patch": "@@ -26,7 +26,7 @@\n import time\n import uuid\n from argparse import ArgumentParser, Namespace\n-from collections.abc import AsyncGenerator, Generator, Iterable\n+from collections.abc import Generator, Iterable\n from contextlib import asynccontextmanager\n from dataclasses import dataclass, field\n from io import BytesIO\n@@ -35,6 +35,7 @@\n \n from huggingface_hub import model_info\n from huggingface_hub.constants import HF_HUB_OFFLINE\n+from openai.types.chat.chat_completion import Choice\n from tokenizers.decoders import DecodeStream\n \n import transformers\n@@ -90,14 +91,16 @@\n     from fastapi.responses import JSONResponse, StreamingResponse\n     from openai.types.audio.transcription import Transcription\n     from openai.types.audio.transcription_create_params import TranscriptionCreateParamsBase\n-    from openai.types.chat import ChatCompletionMessageParam\n+    from openai.types.chat import ChatCompletion, ChatCompletionMessage, ChatCompletionMessageParam\n     from openai.types.chat.chat_completion_chunk import (\n         ChatCompletionChunk,\n-        Choice,\n         ChoiceDelta,\n         ChoiceDeltaToolCall,\n         ChoiceDeltaToolCallFunction,\n     )\n+    from openai.types.chat.chat_completion_chunk import (\n+        Choice as ChoiceChunk,\n+    )\n     from openai.types.chat.completion_create_params import CompletionCreateParamsStreaming\n     from openai.types.responses import (\n         Response,\n@@ -345,8 +348,11 @@ def delete_model(self):\n             self._timer.cancel()\n \n     def timeout_reached(self):\n-        self.delete_model()\n-        logger.info(f\"{self._name_or_path} was removed from memory after {self.timeout_seconds} seconds of inactivity\")\n+        if self.timeout_seconds > 0:\n+            self.delete_model()\n+            logger.info(\n+                f\"{self._name_or_path} was removed from memory after {self.timeout_seconds} seconds of inactivity\"\n+            )\n \n     def is_deleted(self):\n         \"\"\"Check if the instances have been deleted.\"\"\"\n@@ -412,9 +418,13 @@ class ServeArguments:\n     # Serving settings\n     host: str = field(default=\"localhost\", metadata={\"help\": \"Interface the server will listen to.\"})\n     port: int = field(default=8000, metadata={\"help\": \"Port the server will listen to.\"})\n-    model_timeout: int = field(\n-        default=300,\n-        metadata={\"help\": \"Time in seconds after which a model will be removed from memory.\"},\n+    model_timeout: Optional[int] = field(\n+        default=None,\n+        metadata={\n+            \"help\": \"Time in seconds after which a model will be removed from memory; defaults to 300 unless \"\n+            \"`force_model` is set, in which case the model will not be removed from memory unless a value\"\n+            \"is specified here.\"\n+        },\n     )\n \n     # Other settings\n@@ -512,6 +522,14 @@ def __init__(self, args: ServeArguments):\n         self.last_kv_cache = None\n         self.last_model = None\n \n+        if self.args.model_timeout is None:\n+            self.args.model_timeout = -1 if self.args.force_model else 300\n+\n+        if self.args.force_model:\n+            model_id_and_revision = self.process_model_name(self.args.force_model)\n+            self.last_model = model_id_and_revision\n+            self.load_model_and_processor(model_id_and_revision)\n+\n     def _validate_request(\n         self,\n         request: dict,\n@@ -595,7 +613,7 @@ def build_chat_completion_chunk(\n         tool_calls: Optional[list[\"ChoiceDeltaToolCall\"]] = None,\n         decode_stream: Optional[DecodeStream] = None,\n         tokenizer: Optional[PreTrainedTokenizerFast] = None,\n-    ) -> str:\n+    ) -> ChatCompletionChunk:\n         \"\"\"\n         Builds a chunk of a streaming OpenAI Chat Completion response.\n \n@@ -621,12 +639,13 @@ def build_chat_completion_chunk(\n         \"\"\"\n         if decode_stream is not None and content is not None and tokenizer is not None:\n             content = decode_stream.step(tokenizer._tokenizer, content)\n+\n         chunk = ChatCompletionChunk(\n             id=request_id,\n             created=int(time.time()),\n             model=model,\n             choices=[\n-                Choice(\n+                ChoiceChunk(\n                     delta=ChoiceDelta(\n                         content=content,\n                         role=role,\n@@ -639,23 +658,25 @@ def build_chat_completion_chunk(\n             system_fingerprint=\"\",\n             object=\"chat.completion.chunk\",\n         )\n-        return f\"data: {chunk.model_dump_json(exclude_none=True)}\\n\\n\"\n \n-    def build_response_event(self, response: \"BaseModel\") -> str:\n+        return chunk\n+\n+    @staticmethod\n+    def chunk_to_sse_element(chunk: ChatCompletionChunk | BaseModel) -> str:\n         \"\"\"\n-        Builds a event of a streaming OpenAI Response response.\n+        Builds an event of a streaming OpenAI Response model or a ChatCompletion chunk.\n \n         IMPORTANT: The serialized chunk won't contain empty fields (fields with `None`). Some downstream apps,\n         like Cursor, assume that when the field exists, it has data.\n \n         Args:\n-            response (`BaseModel`):\n+            chunk (`BaseModel` or `ChatCompletionChunk`):\n                 The response to build an event from. One of the multiple OpenAI Response output types\n \n         Returns:\n             `str`: The built chunk, a string containing a JSON string with the payload.\n         \"\"\"\n-        return f\"data: {response.model_dump_json(exclude_none=True)}\\n\\n\"\n+        return f\"data: {chunk.model_dump_json(exclude_none=True)}\\n\\n\"\n \n     def run(self):\n         \"\"\"\n@@ -668,6 +689,7 @@ def run(self):\n         - POST /v1/responses: Generates responses.\n         - POST /v1/audio/transcriptions: Generates transcriptions from audio.\n         - GET /v1/models: Lists available models for 3rd party tools.\n+        - GET /health: Health check.\n \n         Requires FastAPI and Uvicorn to be installed.\n         \"\"\"\n@@ -703,10 +725,9 @@ def chat_completion(request: Request, body: dict):\n             self.validate_chat_completion_request(request=body)\n \n             if self.use_continuous_batching:\n-                output = self.continuous_batching_chat_completion(body, request.state.request_id)\n+                return self.continuous_batching_chat_completion(body, request.state.request_id)\n             else:\n-                output = self.generate_chat_completion(body)\n-            return StreamingResponse(output, media_type=\"text/event-stream\")\n+                return self.generate_chat_completion(body)\n \n         @app.post(\"/v1/responses\")\n         def responses(request: dict):\n@@ -803,7 +824,7 @@ def get_gen_models(self) -> list[dict[str, any]]:\n                 for model in model_infos\n             ]\n \n-    def continuous_batching_chat_completion(self, req: dict, request_id: str) -> AsyncGenerator[str, None]:\n+    def continuous_batching_chat_completion(self, req: dict, request_id: str) -> StreamingResponse | JSONResponse:\n         \"\"\"\n         Generates an OpenAI Chat Completion using continuous batching.\n \n@@ -816,14 +837,16 @@ def continuous_batching_chat_completion(self, req: dict, request_id: str) -> Asy\n \n         model_id_and_revision = self.process_model_name(req[\"model\"])\n         must_discard_cache = model_id_and_revision != self.last_model\n+\n         self.last_model = model_id_and_revision\n+\n+        # When switching models, terminate a continuous batching manager if it is running.\n         if must_discard_cache:\n-            # When switching models, terminate a continuous batching manager if it is running.\n             if self.running_continuous_batching_manager is not None:\n                 self.running_continuous_batching_manager.stop(block=True, timeout=2)\n                 self.running_continuous_batching_manager = None\n-        model, processor = self.load_model_and_processor(model_id_and_revision)\n \n+        model, processor = self.load_model_and_processor(model_id_and_revision)\n         tokenizer = processor.tokenizer if hasattr(processor, \"tokenizer\") else processor\n \n         generation_config = create_generation_config_from_req(\n@@ -838,18 +861,17 @@ def continuous_batching_chat_completion(self, req: dict, request_id: str) -> Asy\n \n         if self.running_continuous_batching_manager is None:\n             self.running_continuous_batching_manager = model.init_continuous_batching(\n-                generation_config=generation_config, streaming=True\n+                generation_config=generation_config\n             )\n \n-            # TODO (Joao, Lysandre): the logits processors should be fixed in continuous batching\n-            # and correctly applied in non-cb\n+            # TODO (Joao, Lysandre): the logits processors should be fixed in continuous batching and correctly applied in non-cb\n             self.running_continuous_batching_manager.logit_processor = LogitsProcessorList()\n             self.running_continuous_batching_manager.start()\n \n         # TODO (Joao, Lysandre): this should also work with tool support\n         inputs = processor.apply_chat_template(req[\"messages\"], return_tensors=\"pt\", add_generation_prompt=True).to(\n             model.device\n-        )\n+        )[0]\n \n         def stream_chat_completion(request_id, decode_stream):\n             try:\n@@ -879,21 +901,61 @@ def stream_chat_completion(request_id, decode_stream):\n                 self.running_continuous_batching_manager.cancel_request(request_id)\n                 yield f'data: {{\"error\": \"{str(e)}\"}}'\n \n-        async def cancellation_wrapper(_inputs, request_id):\n+        def buffer_chat_completion(_request_id):\n+            result = None\n+            while self.running_continuous_batching_manager.is_running() and result is None:\n+                result = self.running_continuous_batching_manager.get_result(request_id=_request_id, timeout=1)\n+\n+            content = tokenizer.decode(result.generated_tokens)\n+\n+            chat_completion_result = ChatCompletion(\n+                id=_request_id,\n+                created=int(time.time()),\n+                object=\"chat.completion\",\n+                model=model_id_and_revision,\n+                choices=[\n+                    Choice(\n+                        # TODO check the index\n+                        index=0,\n+                        message=ChatCompletionMessage(content=content, role=\"assistant\"),\n+                        finish_reason=\"stop\",\n+                    )\n+                ],\n+                # TODO implement function calling\n+                # TODO implement usage\n+            )\n+\n+            return chat_completion_result\n+\n+        async def cancellation_wrapper_stream(_request_id):\n+            # Enables cancellation in an async context\n             try:\n-                decode_stream = DecodeStream(_inputs.tolist(), False)\n-                # XXX: using returned request_id as safety in case it is None\n-                request_id = self.running_continuous_batching_manager.add_request(\n-                    _inputs, request_id=request_id, max_new_tokens=generation_config.max_new_tokens\n-                )\n-                for chunk in stream_chat_completion(request_id, decode_stream):\n-                    yield chunk\n-                    await asyncio.sleep(0)  # Yield control to the event loop to check for cancellations\n+                decode_stream = DecodeStream(inputs.tolist(), False)\n+                for _chunk in stream_chat_completion(_request_id, decode_stream):\n+                    yield self.chunk_to_sse_element(_chunk)\n+                    await asyncio.sleep(0)\n             except asyncio.CancelledError:\n-                self.running_continuous_batching_manager.cancel_request(request_id)\n-                logger.warning(f\"Request {request_id} was cancelled.\")\n+                self.running_continuous_batching_manager.cancel_request(_request_id)\n+                logger.warning(f\"Request {_request_id} was cancelled.\")\n \n-        return cancellation_wrapper(inputs[0], request_id)\n+        def cancellation_wrapper_buffer(_request_id):\n+            # Enables cancellation in an async context\n+            try:\n+                return buffer_chat_completion(_request_id)\n+            except asyncio.CancelledError:\n+                self.running_continuous_batching_manager.cancel_request(_request_id)\n+                logger.warning(f\"Request {_request_id} was cancelled.\")\n+\n+        request_id = self.running_continuous_batching_manager.add_request(\n+            inputs, request_id=request_id, max_new_tokens=generation_config.max_new_tokens, streaming=req.get(\"stream\")\n+        )\n+\n+        if req.get(\"stream\"):\n+            return StreamingResponse(cancellation_wrapper_stream(request_id), media_type=\"text/event-stream\")\n+        else:\n+            chunk = cancellation_wrapper_buffer(request_id)\n+            json_chunk = chunk.model_dump_json(exclude_none=True)\n+            return JSONResponse(json_chunk, media_type=\"application/json\")\n \n     @staticmethod\n     def get_model_modality(model: \"PreTrainedModel\") -> Modality:\n@@ -953,7 +1015,7 @@ def get_processor_inputs_from_inbound_messages(messages, modality: Modality):\n             processor_inputs.append(parsed_message)\n         return processor_inputs\n \n-    def generate_chat_completion(self, req: dict) -> Generator[str, None, None]:\n+    def generate_chat_completion(self, req: dict) -> StreamingResponse | JSONResponse:\n         \"\"\"\n         Generates an OpenAI Chat Completion using `generate`.\n \n@@ -1132,7 +1194,10 @@ def generate_with_cache(**kwargs):\n                                 )\n \n                             yield self.build_chat_completion_chunk(\n-                                request_id=_request_id, role=None, tool_calls=[tool], model=model_id_and_revision\n+                                request_id=_request_id,\n+                                role=None,\n+                                tool_calls=[tool],\n+                                model=model_id_and_revision,\n                             )\n                             continue\n                     # ====== END OF TOOL CALL LOGIC ======\n@@ -1152,7 +1217,47 @@ def generate_with_cache(**kwargs):\n             finally:\n                 thread.join()\n \n-        return stream_chat_completion(generation_streamer, request_id)\n+        if req.get(\"stream\"):\n+            return StreamingResponse(\n+                map(self.chunk_to_sse_element, stream_chat_completion(generation_streamer, request_id)),\n+                media_type=\"text/event-stream\",\n+            )\n+        else:\n+            content = []\n+            finish_reason = \"stop\"\n+\n+            generator = stream_chat_completion(generation_streamer, request_id)\n+            usage = None\n+\n+            for chunk in generator:\n+                choice = chunk.choices[0]\n+                if getattr(choice.delta, \"content\", None):\n+                    content.append(choice.delta.content)\n+                if choice.finish_reason:\n+                    finish_reason = choice.finish_reason\n+                if getattr(chunk, \"usage\", None):\n+                    usage = chunk.usage\n+\n+            chat_completion_result = ChatCompletion(\n+                id=request_id,\n+                created=int(time.time()),\n+                object=\"chat.completion\",\n+                model=model_id_and_revision,\n+                choices=[\n+                    Choice(\n+                        # TODO check the index\n+                        index=0,\n+                        message=ChatCompletionMessage(content=\"\".join(content), role=\"assistant\"),\n+                        finish_reason=finish_reason,\n+                    )\n+                ],\n+                # TODO implement function calling\n+                usage=usage,\n+            )\n+\n+            result = chat_completion_result.model_dump(exclude_none=True)\n+\n+            return JSONResponse(result, media_type=\"application/json\")\n \n     def generate_response(self, req: dict) -> Generator[str, None, None]:\n         \"\"\"\n@@ -1263,7 +1368,7 @@ def generate_with_cache(**kwargs):\n                     ),\n                 )\n                 sequence_number += 1\n-                yield self.build_response_event(response_created)\n+                yield self.chunk_to_sse_element(response_created)\n \n                 response_in_progress = ResponseInProgressEvent(\n                     type=\"response.in_progress\",\n@@ -1284,7 +1389,7 @@ def generate_with_cache(**kwargs):\n                     ),\n                 )\n                 sequence_number += 1\n-                yield self.build_response_event(response_in_progress)\n+                yield self.chunk_to_sse_element(response_in_progress)\n \n                 # Start the output item. Emit the assistant role to start the stream. Other chunks won't have a role,\n                 # as it is implicit\n@@ -1297,7 +1402,7 @@ def generate_with_cache(**kwargs):\n                     ),\n                 )\n                 sequence_number += 1\n-                yield self.build_response_event(response_output_item_added)\n+                yield self.chunk_to_sse_element(response_output_item_added)\n \n                 # Start the content part of the event\n                 response_content_part_added = ResponseContentPartAddedEvent(\n@@ -1309,7 +1414,7 @@ def generate_with_cache(**kwargs):\n                     part=ResponseOutputText(type=\"output_text\", text=\"\", annotations=[]),\n                 )\n                 sequence_number += 1\n-                yield self.build_response_event(response_content_part_added)\n+                yield self.chunk_to_sse_element(response_content_part_added)\n \n                 # Stream the actual generated text\n                 results = \"\"\n@@ -1336,7 +1441,7 @@ def generate_with_cache(**kwargs):\n                                 logprobs=[],\n                             )\n                             sequence_number += 1\n-                            yield self.build_response_event(response_output_text_delta)\n+                            yield self.chunk_to_sse_element(response_output_text_delta)\n                     else:\n                         # Normal path: emit token deltas when not filtering CoT\n                         if result:\n@@ -1350,7 +1455,7 @@ def generate_with_cache(**kwargs):\n                                 logprobs=[],\n                             )\n                             sequence_number += 1\n-                            yield self.build_response_event(response_output_text_delta)\n+                            yield self.chunk_to_sse_element(response_output_text_delta)\n \n                 # Signal the end of the text generation\n                 response_output_text_done = ResponseTextDoneEvent(\n@@ -1363,7 +1468,7 @@ def generate_with_cache(**kwargs):\n                     logprobs=[],\n                 )\n                 sequence_number += 1\n-                yield self.build_response_event(response_output_text_done)\n+                yield self.chunk_to_sse_element(response_output_text_done)\n \n                 # Complete the content part\n                 response_content_part_done = ResponseContentPartDoneEvent(\n@@ -1376,7 +1481,7 @@ def generate_with_cache(**kwargs):\n                 )\n                 sequence_number += 1\n                 content_index += 1\n-                yield self.build_response_event(response_content_part_done)\n+                yield self.chunk_to_sse_element(response_content_part_done)\n \n                 # Complete the output item\n                 response_output_item_done = ResponseOutputItemDoneEvent(\n@@ -1394,7 +1499,7 @@ def generate_with_cache(**kwargs):\n                 )\n                 sequence_number += 1\n                 output_index += 1\n-                yield self.build_response_event(response_output_item_done)\n+                yield self.chunk_to_sse_element(response_output_item_done)\n \n                 # Finally, Complete the event\n                 response_completed = ResponseCompletedEvent(\n@@ -1416,7 +1521,7 @@ def generate_with_cache(**kwargs):\n                     ),\n                 )\n                 sequence_number += 1\n-                yield self.build_response_event(response_completed)\n+                yield self.chunk_to_sse_element(response_completed)\n \n                 thread.join()\n             except Exception as e:\n@@ -1427,7 +1532,7 @@ def generate_with_cache(**kwargs):\n                     message=str(e),\n                 )\n                 sequence_number += 1\n-                yield self.build_response_event(error_event)\n+                yield self.chunk_to_sse_element(error_event)\n \n                 response_failed = ResponseFailedEvent(\n                     type=\"response.failed\",\n@@ -1452,7 +1557,7 @@ def generate_with_cache(**kwargs):\n                     ),\n                 )\n                 sequence_number += 1\n-                yield self.build_response_event(response_failed)\n+                yield self.chunk_to_sse_element(response_failed)\n \n             finally:\n                 thread.join()"
      },
      {
        "filename": "src/transformers/generation/continuous_batching/continuous_api.py",
        "status": "modified",
        "additions": 0,
        "deletions": 1,
        "changes": 1,
        "patch": "@@ -907,7 +907,6 @@ def get_result(\n             if request_id is not None and result.request_id != request_id:\n                 self.output_queue.put(result)\n                 return None\n-            logger.debug(f\"Retrieved result for request {result.request_id}\")\n             return result\n         except queue.Empty:\n             return None"
      },
      {
        "filename": "src/transformers/modeling_utils.py",
        "status": "modified",
        "additions": 3,
        "deletions": 3,
        "changes": 6,
        "patch": "@@ -2509,14 +2509,14 @@ def _check_and_adjust_attn_implementation(\n             try:\n                 load_and_register_attn_kernel(applicable_attn_implementation)\n                 # log that we used kernel fallback if successful\n-                if attn_implementation.startswith(\"flash_attention\"):\n+                if \"flash_\" in attn_implementation:\n                     logger.warning_once(\n                         f\"You do not have `flash_attn` installed, using `{applicable_attn_implementation}` \"\n                         \"from the `kernels` library instead!\"\n                     )\n             except Exception as e:\n                 # raise the proper exception for requested flash attention\n-                if attn_implementation.startswith(\"flash_attention\"):\n+                if attn_implementation.startswith(\"flash_\"):\n                     if attn_implementation.endswith(\"2\"):\n                         self._flash_attn_2_can_dispatch()\n                     else:\n@@ -2529,7 +2529,7 @@ def _check_and_adjust_attn_implementation(\n                 applicable_attn_implementation, is_init_check\n             )\n             # preload flash attention here to allow compile with fullgraph\n-            if applicable_attn_implementation.startswith(\"flash_attention\"):\n+            if applicable_attn_implementation.startswith(\"flash_\"):\n                 lazy_import_flash_attention(applicable_attn_implementation, force_import=True)\n         return applicable_attn_implementation\n "
      },
      {
        "filename": "tests/commands/test_serving.py",
        "status": "modified",
        "additions": 51,
        "deletions": 17,
        "changes": 68,
        "patch": "@@ -85,6 +85,7 @@ def test_build_chat_completion_chunk(self):\n         chunk = ServeCommand.build_chat_completion_chunk(\n             dummy, request_id=\"req0\", content=\"hello\", finish_reason=\"stop\", role=\"user\", model=\"dummy_model@main\"\n         )\n+        chunk = ServeCommand.chunk_to_sse_element(chunk)\n         for field in MANDATORY_FIELDS:\n             self.assertIn(field, chunk)\n         self.assertIn(\n@@ -93,12 +94,14 @@ def test_build_chat_completion_chunk(self):\n \n         # Case 2: only the role is provided -- other fields in 'choices' are omitted\n         chunk = dummy.build_chat_completion_chunk(request_id=\"req0\", role=\"user\", model=\"dummy_model@main\")\n+        chunk = ServeCommand.chunk_to_sse_element(chunk)\n         for field in MANDATORY_FIELDS:\n             self.assertIn(field, chunk)\n         self.assertIn('\"choices\":[{\"delta\":{\"role\":\"user\"},\"index\":0}]', chunk)\n \n         # Case 3: only the content is provided -- other fields in 'choices' are omitted\n         chunk = dummy.build_chat_completion_chunk(request_id=\"req0\", content=\"hello\", model=\"dummy_model@main\")\n+        chunk = ServeCommand.chunk_to_sse_element(chunk)\n         for field in MANDATORY_FIELDS:\n             self.assertIn(field, chunk)\n         self.assertIn('\"choices\":[{\"delta\":{\"content\":\"hello\"},\"index\":0}]', chunk)\n@@ -110,6 +113,7 @@ def test_build_chat_completion_chunk(self):\n             type=\"function\",\n         )\n         chunk = dummy.build_chat_completion_chunk(request_id=\"req0\", tool_calls=[tool_call], model=\"dummy_model@main\")\n+        chunk = ServeCommand.chunk_to_sse_element(chunk)\n         for field in MANDATORY_FIELDS:\n             self.assertIn(field, chunk)\n         expected_choices_content = (\n@@ -147,7 +151,7 @@ def test_build_response_event(self):\n             ),\n         )\n \n-        event = dummy.build_response_event(response_created)\n+        event = dummy.chunk_to_sse_element(response_created)\n         self.assertTrue(event.startswith(\"data: \"))  # Sanity check: event formatting\n         self.assertIn('\"model\":\"dummy_model@main\"', event)  # Sanity check: set field\n         self.assertIn('\"status\":\"queued\"', event)\n@@ -411,10 +415,18 @@ def setUpClass(cls):\n         \"\"\"Starts a server for tests to connect to.\"\"\"\n         cls.port = 8001\n         args = ServeArguments(port=cls.port)\n-        serve_command = ServeCommand(args)\n-        thread = Thread(target=serve_command.run)\n-        thread.daemon = True\n-        thread.start()\n+        cls.serve_command = ServeCommand(args)\n+        cls.thread = Thread(target=cls.serve_command.run)\n+        cls.thread.daemon = True\n+        cls.thread.start()\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        cls.thread.join(timeout=1)\n+\n+    def setUp(self):\n+        \"\"\"Ensures that the healthcheck works before each test.\"\"\"\n+        _call_healthcheck(f\"http://localhost:{self.port}\")\n \n     @slow\n     def test_tool_call(self):\n@@ -548,13 +560,19 @@ class ServeCompletionsContinuousBatchingIntegrationTest(ServeCompletionsMixin, u\n     def setUpClass(cls):\n         \"\"\"Starts a server for tests to connect to.\"\"\"\n         cls.port = 8002\n-        args = ServeArguments(\n-            port=cls.port, continuous_batching=True, attn_implementation=\"sdpa_paged\", default_seed=42\n-        )\n+        args = ServeArguments(port=cls.port, continuous_batching=True, default_seed=42)\n         cls.serve_command = ServeCommand(args)\n-        thread = Thread(target=cls.serve_command.run)\n-        thread.daemon = True\n-        thread.start()\n+        cls.thread = Thread(target=cls.serve_command.run)\n+        cls.thread.daemon = True\n+        cls.thread.start()\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        cls.thread.join(timeout=1)\n+\n+    def setUp(self):\n+        \"\"\"Ensures that the healthcheck works before each test.\"\"\"\n+        _call_healthcheck(f\"http://localhost:{self.port}\")\n \n     def test_full_request(self):\n         \"\"\"Tests that an inference using the Responses API and Continuous Batching works\"\"\"\n@@ -703,9 +721,17 @@ def setUpClass(cls):\n         cls.port = 8003\n         args = ServeArguments(port=cls.port, default_seed=42)\n         serve_command = ServeCommand(args)\n-        thread = Thread(target=serve_command.run)\n-        thread.daemon = True\n-        thread.start()\n+        cls.thread = Thread(target=serve_command.run)\n+        cls.thread.daemon = True\n+        cls.thread.start()\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        cls.thread.join(timeout=1)\n+\n+    def setUp(self):\n+        \"\"\"Ensures that the healthcheck works before each test.\"\"\"\n+        _call_healthcheck(f\"http://localhost:{self.port}\")\n \n     @slow\n     def test_full_request(self):\n@@ -767,9 +793,17 @@ def setUpClass(cls):\n         cls.port = 8042\n         args = ServeArguments(port=cls.port)\n         serve_command = ServeCommand(args)\n-        thread = Thread(target=serve_command.run)\n-        thread.daemon = True\n-        thread.start()\n+        cls.thread = Thread(target=serve_command.run)\n+        cls.thread.daemon = True\n+        cls.thread.start()\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        cls.thread.join(timeout=1)\n+\n+    def setUp(self):\n+        \"\"\"Ensures that the healthcheck works before each test.\"\"\"\n+        _call_healthcheck(f\"http://localhost:{self.port}\")\n \n     def test_healthcheck(self):\n         \"\"\"Tests that the healthcheck endpoint works.\"\"\""
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T21:18:00.722357",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR contains non-trivial logic changes to enable non-streaming mode in the transformers serve command, including modifications to request handling, response building, import restructuring, and test updates. The changes involve architectural decisions about how to support both streaming and non-streaming modes, with sufficient complexity to generate meaningful technical questions about the codebase.",
      "substance_level": "medium"
    }
  },
  {
    "pr_number": 41432,
    "title": "Refactor check_auto_docstring using AST",
    "body": "# What does this PR do?\r\n\r\nUse AST instead of parsing the raw code to check for missing args in docstrings of class/functions using auto_docstring",
    "html_url": "https://github.com/huggingface/transformers/pull/41432",
    "created_at": "2025-10-08T02:57:20Z",
    "merged_at": "2025-11-14T14:57:08Z",
    "merge_commit_sha": "8976ceb0510e139282050a1b12d9e6afb21bce35",
    "base_ref": "main",
    "head_sha": "7eb3c7e4d71de7e392d00084302d7c24d808da85",
    "user": "yonigozlan",
    "files": [
      {
        "filename": "src/transformers/models/glm4v/modeling_glm4v.py",
        "status": "modified",
        "additions": 0,
        "deletions": 3,
        "changes": 3,
        "patch": "@@ -1418,14 +1418,11 @@ def forward(\n         pixel_values_videos: Optional[torch.FloatTensor] = None,\n         image_grid_thw: Optional[torch.LongTensor] = None,\n         video_grid_thw: Optional[torch.LongTensor] = None,\n-        rope_deltas: Optional[torch.LongTensor] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, Glm4vCausalLMOutputWithPast]:\n         r\"\"\"\n-        rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):\n-            The rope index difference between sequence length and multimodal rope.\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n             config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
      },
      {
        "filename": "src/transformers/models/glm4v/modular_glm4v.py",
        "status": "modified",
        "additions": 0,
        "deletions": 3,
        "changes": 3,
        "patch": "@@ -1341,14 +1341,11 @@ def forward(\n         pixel_values_videos: Optional[torch.FloatTensor] = None,\n         image_grid_thw: Optional[torch.LongTensor] = None,\n         video_grid_thw: Optional[torch.LongTensor] = None,\n-        rope_deltas: Optional[torch.LongTensor] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, Glm4vCausalLMOutputWithPast]:\n         r\"\"\"\n-        rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):\n-            The rope index difference between sequence length and multimodal rope.\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n             config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
      },
      {
        "filename": "src/transformers/models/glm4v_moe/modeling_glm4v_moe.py",
        "status": "modified",
        "additions": 0,
        "deletions": 2,
        "changes": 2,
        "patch": "@@ -1631,8 +1631,6 @@ def forward(\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, Glm4vMoeCausalLMOutputWithPast]:\n         r\"\"\"\n-        rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):\n-            The rope index difference between sequence length and multimodal rope.\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n             config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
      },
      {
        "filename": "utils/check_docstrings.py",
        "status": "modified",
        "additions": 243,
        "deletions": 206,
        "changes": 449,
        "patch": "@@ -42,8 +42,9 @@\n import os\n import re\n from collections import OrderedDict\n+from dataclasses import dataclass\n from pathlib import Path\n-from typing import Any\n+from typing import Any, Optional, Union\n \n from check_repo import ignore_undocumented\n from git import Repo\n@@ -59,6 +60,25 @@\n )\n \n \n+@dataclass\n+class DecoratedItem:\n+    \"\"\"Information about a single @auto_docstring decorated function or class.\"\"\"\n+\n+    decorator_line: int  # 1-based line number of the decorator\n+    def_line: int  # 1-based line number of the def/class statement\n+    kind: str  # 'function' or 'class'\n+    body_start_line: (\n+        int  # 1-based line number where body starts (for functions) or __init__ body start (for classes with __init__)\n+    )\n+    args: list[str]  # List of argument names (excluding self, *args, **kwargs) - for classes, these are __init__ args\n+    custom_args_text: Optional[str] = None  # custom_args string if provided in decorator\n+\n+    # Class-specific fields (only populated when kind == 'class')\n+    has_init: bool = False  # Whether the class has an __init__ method\n+    init_def_line: Optional[int] = None  # 1-based line number of __init__ def (if has_init)\n+    is_model_output: bool = False  # Whether the class inherits from ModelOutput\n+\n+\n PATH_TO_REPO = Path(__file__).parent.parent.resolve()\n PATH_TO_TRANSFORMERS = Path(\"src\").resolve() / \"transformers\"\n \n@@ -874,34 +894,35 @@ def fix_docstring(obj: Any, old_doc_args: str, new_doc_args: str):\n         f.write(\"\\n\".join(lines))\n \n \n-def _find_sig_line(lines, line_end):\n-    parenthesis_count = 0\n-    sig_line_end = line_end\n-    found_sig = False\n-    while not found_sig:\n-        for char in lines[sig_line_end]:\n-            if char == \"(\":\n-                parenthesis_count += 1\n-            elif char == \")\":\n-                parenthesis_count -= 1\n-                if parenthesis_count == 0:\n-                    found_sig = True\n-                    break\n-        sig_line_end += 1\n-    return sig_line_end\n-\n-\n def _find_docstring_end_line(lines, docstring_start_line):\n-    if '\"\"\"' not in lines[docstring_start_line]:\n+    \"\"\"Find the line number where a docstring ends. Only handles triple double quotes.\"\"\"\n+    if docstring_start_line is None or docstring_start_line < 0 or docstring_start_line >= len(lines):\n         return None\n-    docstring_end = docstring_start_line\n-    if docstring_start_line is not None:\n-        docstring_end = docstring_start_line\n-        if not lines[docstring_start_line].count('\"\"\"') >= 2:\n-            docstring_end += 1\n-            while '\"\"\"' not in lines[docstring_end]:\n-                docstring_end += 1\n-    return docstring_end\n+    start_line = lines[docstring_start_line]\n+    if '\"\"\"' not in start_line:\n+        return None\n+    # Check if docstring starts and ends on the same line\n+    if start_line.count('\"\"\"') >= 2:\n+        return docstring_start_line\n+    # Find the closing triple quotes on subsequent lines\n+    for idx in range(docstring_start_line + 1, len(lines)):\n+        if '\"\"\"' in lines[idx]:\n+            return idx\n+    return len(lines) - 1\n+\n+\n+def _is_auto_docstring_decorator(dec):\n+    \"\"\"Return True if the decorator expression corresponds to `@auto_docstring`.\"\"\"\n+    # Handle @auto_docstring(...) - unwrap the Call to get the function\n+    target = dec.func if isinstance(dec, ast.Call) else dec\n+    # Check if it's named \"auto_docstring\"\n+    return isinstance(target, ast.Name) and target.id == \"auto_docstring\"\n+\n+\n+def _extract_function_args(func_node: Union[ast.FunctionDef, ast.AsyncFunctionDef]) -> list[str]:\n+    \"\"\"Extract argument names from a function node, excluding 'self', *args, **kwargs.\"\"\"\n+    all_args = (func_node.args.posonlyargs or []) + func_node.args.args + func_node.args.kwonlyargs\n+    return [a.arg for a in all_args if a.arg != \"self\"]\n \n \n def find_matching_model_files(check_all: bool = False):\n@@ -947,64 +968,20 @@ def find_matching_model_files(check_all: bool = False):\n def find_files_with_auto_docstring(matching_files, decorator=\"@auto_docstring\"):\n     \"\"\"\n     From a list of files, return those that contain the @auto_docstring decorator.\n+    Fast path: simple substring presence check.\n     \"\"\"\n     auto_docstrings_files = []\n     for file_path in matching_files:\n-        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n-            content_base_file = f.read()\n-            if decorator in content_base_file:\n-                lines = content_base_file.split(\"\\n\")\n-                line_numbers = [i for i, line in enumerate(lines) if decorator in line]\n-                for line_number in line_numbers:\n-                    line_end = line_number\n-                    end_patterns = [\"class \", \"    def\"]\n-                    stop_condition = False\n-                    while line_end < len(lines) and not stop_condition:\n-                        line_end += 1\n-                        stop_condition = any(lines[line_end].startswith(end_pattern) for end_pattern in end_patterns)\n-                    candidate_patterns = [\"class \", \"    def\"]\n-                    candidate = any(\n-                        lines[line_end].startswith(candidate_pattern) for candidate_pattern in candidate_patterns\n-                    )\n-                    if stop_condition and candidate:\n-                        auto_docstrings_files.append(file_path)\n-                        break\n+        try:\n+            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n+                source = f.read()\n+        except OSError:\n+            continue\n+        if decorator in source:\n+            auto_docstrings_files.append(file_path)\n     return auto_docstrings_files\n \n \n-def get_auto_docstring_candidate_lines(lines):\n-    \"\"\"\n-    For a file's lines, find the start and end line indices of all @auto_docstring candidates.\n-    Returns two lists: starts and ends.\n-    \"\"\"\n-    line_numbers = [i for i, line in enumerate(lines) if \"@auto_docstring\" in line]\n-    line_starts_candidates = []\n-    line_ends_candidates = []\n-    for line_number in line_numbers:\n-        line_end = line_number\n-        end_patterns = [\"class \", \"    def\"]\n-        stop_condition = False\n-        while line_end < len(lines) and not stop_condition:\n-            line_end += 1\n-            stop_condition = any(lines[line_end].startswith(end_pattern) for end_pattern in end_patterns)\n-        candidate_patterns = [\"class \", \"    def\"]\n-        candidate = any(lines[line_end].startswith(candidate_pattern) for candidate_pattern in candidate_patterns)\n-        if stop_condition and candidate:\n-            line_ends_candidates.append(line_end)\n-            line_starts_candidates.append(line_number)\n-    return line_starts_candidates, line_ends_candidates\n-\n-\n-def get_args_in_signature(lines, signature_content):\n-    signature_content = [line.split(\"#\")[0] for line in signature_content]\n-    signature_content = \"\".join(signature_content)\n-    signature_content = \"\".join(signature_content.split(\")\")[:-1])\n-    args_in_signature = re.findall(r\"[,(]\\s*(\\w+)\\s*(?=:|=|,|\\))\", signature_content)\n-    if \"self\" in args_in_signature:\n-        args_in_signature.remove(\"self\")\n-    return args_in_signature\n-\n-\n def get_args_in_dataclass(lines, dataclass_content):\n     dataclass_content = [line.split(\"#\")[0] for line in dataclass_content]\n     dataclass_content = \"\\n\".join(dataclass_content)\n@@ -1051,6 +1028,9 @@ def generate_new_docstring_for_signature(\n     else:\n         docstring_end_line = None\n \n+    # Remove pre-existing entries for *args and untyped **kwargs from the docstring\n+    # (No longer needed since *args are excluded from args_in_signature)\n+\n     # Remove args that are the same as the ones in the source args doc\n     for arg in args_docstring_dict:\n         if arg in get_args_doc_from_source(source_args_doc) and arg not in ALWAYS_OVERRIDE:\n@@ -1132,13 +1112,16 @@ def generate_new_docstring_for_signature(\n     )\n \n \n-def generate_new_docstring_for_function(lines, current_line_end, custom_args_dict):\n+def generate_new_docstring_for_function(\n+    lines,\n+    item: DecoratedItem,\n+    custom_args_dict,\n+):\n     \"\"\"\n     Wrapper for function docstring generation using the generalized helper.\n     \"\"\"\n-    sig_end_line = _find_sig_line(lines, current_line_end)\n-    signature_content = lines[current_line_end:sig_end_line]\n-    args_in_signature = get_args_in_signature(lines, signature_content)\n+    sig_end_line = item.body_start_line - 1  # Convert to 0-based\n+    args_in_signature = item.args\n     docstring_start_line = sig_end_line if '\"\"\"' in lines[sig_end_line] else None\n     return generate_new_docstring_for_signature(\n         lines,\n@@ -1150,34 +1133,27 @@ def generate_new_docstring_for_function(lines, current_line_end, custom_args_dic\n     )\n \n \n-def generate_new_docstring_for_class(lines, current_line_end, custom_args_dict):\n+def generate_new_docstring_for_class(\n+    lines,\n+    item: DecoratedItem,\n+    custom_args_dict,\n+    source: str,\n+):\n     \"\"\"\n     Wrapper for class docstring generation (via __init__) using the generalized helper.\n     Returns the new docstring and relevant signature/docstring indices.\n     \"\"\"\n-    sig_start_line = current_line_end\n-    found_init_method = False\n-    found_model_output = False\n-    while sig_start_line < len(lines) - 1 and not found_init_method:\n-        sig_start_line += 1\n-        if \"    def __init__\" in lines[sig_start_line]:\n-            found_init_method = True\n-        elif lines[sig_start_line].startswith(\"class \") or lines[sig_start_line].startswith(\"def \"):\n-            break\n-    if not found_init_method:\n-        if \"ModelOutput\" in lines[current_line_end]:\n-            found_model_output = True\n-            sig_start_line = current_line_end\n-        else:\n-            return \"\", None, None, [], [], []\n-\n-    if found_init_method:\n-        sig_end_line = _find_sig_line(lines, sig_start_line)\n-        signature_content = lines[sig_start_line:sig_end_line]\n-        args_in_signature = get_args_in_signature(lines, signature_content)\n-    else:\n-        # we have a ModelOutput class, the class attributes are the args\n-        sig_end_line = sig_start_line + 1\n+    # Use pre-extracted information from DecoratedItem (no need to search or re-parse!)\n+    if item.has_init:\n+        # Class has an __init__ method - use its args and body start\n+        sig_end_line = item.body_start_line - 1  # Convert from body start to sig end (0-based)\n+        args_in_signature = item.args\n+        output_docstring_indent = 8\n+        source_args_doc = [ModelArgs, ImageProcessorArgs]\n+    elif item.is_model_output:\n+        # ModelOutput class - extract args from dataclass attributes\n+        current_line_end = item.def_line - 1  # Convert to 0-based\n+        sig_end_line = current_line_end + 1\n         docstring_end = _find_docstring_end_line(lines, sig_end_line)\n         model_output_class_start = docstring_end + 1 if docstring_end is not None else sig_end_line - 1\n         model_output_class_end = model_output_class_start\n@@ -1187,6 +1163,11 @@ def generate_new_docstring_for_class(lines, current_line_end, custom_args_dict):\n             model_output_class_end += 1\n         dataclass_content = lines[model_output_class_start : model_output_class_end - 1]\n         args_in_signature = get_args_in_dataclass(lines, dataclass_content)\n+        output_docstring_indent = 4\n+        source_args_doc = [ModelOutputArgs]\n+    else:\n+        # Class has no __init__ and is not a ModelOutput - nothing to document\n+        return \"\", None, None, [], [], []\n \n     docstring_start_line = sig_end_line if '\"\"\"' in lines[sig_end_line] else None\n \n@@ -1197,127 +1178,177 @@ def generate_new_docstring_for_class(lines, current_line_end, custom_args_dict):\n         docstring_start_line,\n         arg_indent=\"\",\n         custom_args_dict=custom_args_dict,\n-        output_docstring_indent=4 if found_model_output else 8,\n-        source_args_doc=[ModelArgs, ImageProcessorArgs] if not found_model_output else [ModelOutputArgs],\n+        output_docstring_indent=output_docstring_indent,\n+        source_args_doc=source_args_doc,\n     )\n \n \n-def find_custom_args_with_details(file_content: str, custom_args_var_name: str) -> list[dict]:\n-    \"\"\"\n-    Find the given custom args variable in the file content and return its content.\n+def _build_ast_indexes(source: str) -> list[DecoratedItem]:\n+    \"\"\"Parse source once and return list of all @auto_docstring decorated items.\n \n-    Args:\n-        file_content: The string content of the Python file.\n-        custom_args_var_name: The name of the custom args variable.\n+    Returns:\n+        List of DecoratedItem objects, one for each @auto_docstring decorated function or class.\n     \"\"\"\n-    # Escape the variable_name to handle any special regex characters it might contain\n-    escaped_variable_name = re.escape(custom_args_var_name)\n-\n-    # Construct the regex pattern dynamically with the specific variable name\n-    # This regex looks for:\n-    # ^\\s* : Start of a line with optional leading whitespace.\n-    # ({escaped_variable_name}) : Capture the exact variable name.\n-    # \\s*=\\s* : An equals sign, surrounded by optional whitespace.\n-    # (r?\\\"\\\"\\\")               : Capture the opening triple quotes (raw or normal string).\n-    # (.*?)                    : Capture the content (non-greedy).\n-    # (\\\"\\\"\\\")                  : Match the closing triple quotes.\n-    regex_pattern = rf\"^\\s*({escaped_variable_name})\\s*=\\s*(r?\\\"\\\"\\\")(.*?)(\\\"\\\"\\\")\"\n-\n-    flags = re.MULTILINE | re.DOTALL\n+    tree = ast.parse(source)\n+    # First pass: collect top-level string variables (for resolving custom_args variable references)\n+    var_to_string: dict[str, str] = {}\n+    for node in tree.body:\n+        # Handle: ARGS = \"some string\"\n+        if isinstance(node, ast.Assign) and isinstance(node.value, ast.Constant):\n+            if isinstance(node.value.value, str):\n+                for target in node.targets:\n+                    if isinstance(target, ast.Name):\n+                        var_to_string[target.id] = node.value.value\n+        # Handle: ARGS: str = \"some string\"\n+        elif isinstance(node, ast.AnnAssign) and isinstance(node.value, ast.Constant):\n+            if isinstance(node.value.value, str) and isinstance(node.target, ast.Name):\n+                var_to_string[node.target.id] = node.value.value\n+    # Second pass: find all @auto_docstring decorated functions/classes\n+    decorated_items: list[DecoratedItem] = []\n+    for node in ast.walk(tree):\n+        if not isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):\n+            continue\n+        # Find @auto_docstring decorator and extract custom_args if present\n+        decorator_line = None\n+        custom_args_text = None\n+        for dec in node.decorator_list:\n+            if not _is_auto_docstring_decorator(dec):\n+                continue\n+            decorator_line = dec.lineno\n+            # Extract custom_args from @auto_docstring(custom_args=...)\n+            if isinstance(dec, ast.Call):\n+                for kw in dec.keywords:\n+                    if kw.arg == \"custom_args\":\n+                        if isinstance(kw.value, ast.Constant) and isinstance(kw.value.value, str):\n+                            custom_args_text = kw.value.value.strip()\n+                        elif isinstance(kw.value, ast.Name):\n+                            custom_args_text = var_to_string.get(kw.value.id, \"\").strip()\n+            break\n+        if decorator_line is None:  # No @auto_docstring decorator found\n+            continue\n+        # Extract info for this decorated item\n+        kind = \"class\" if isinstance(node, ast.ClassDef) else \"function\"\n+        body_start_line = node.body[0].lineno if node.body else node.lineno + 1\n+        # Extract function arguments (skip self, *args, **kwargs)\n+        arg_names = []\n+        has_init = False\n+        init_def_line = None\n+        is_model_output = False\n+        if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):\n+            # For functions, extract args directly\n+            arg_names = _extract_function_args(node)\n+        elif isinstance(node, ast.ClassDef):\n+            # For classes, look for __init__ method and check if it's a ModelOutput\n+            # Check if class inherits from ModelOutput\n+            for base in node.bases:\n+                if isinstance(base, ast.Name) and \"ModelOutput\" in base.id:\n+                    is_model_output = True\n+                    break\n+            # Look for __init__ method in the class body\n+            for class_item in node.body:\n+                if isinstance(class_item, ast.FunctionDef) and class_item.name == \"__init__\":\n+                    has_init = True\n+                    init_def_line = class_item.lineno\n+                    arg_names = _extract_function_args(class_item)\n+                    # Update body_start_line to be the __init__ body start\n+                    body_start_line = class_item.body[0].lineno if class_item.body else class_item.lineno + 1\n+                    break\n \n-    # Use re.search to find the first match\n-    match = re.search(regex_pattern, file_content, flags)\n+        decorated_items.append(\n+            DecoratedItem(\n+                decorator_line=decorator_line,\n+                def_line=node.lineno,\n+                kind=kind,\n+                body_start_line=body_start_line,\n+                args=arg_names,\n+                custom_args_text=custom_args_text,\n+                has_init=has_init,\n+                init_def_line=init_def_line,\n+                is_model_output=is_model_output,\n+            )\n+        )\n \n-    if match:\n-        # match.group(1) will be the variable_name itself\n-        # match.group(3) will be the content inside the triple quotes\n-        content = match.group(3).strip()\n-        return content\n-    return None\n+    return sorted(decorated_items, key=lambda x: x.decorator_line)\n \n \n def update_file_with_new_docstrings(\n-    candidate_file, lines, line_starts_candidates, line_ends_candidates, overwrite=False\n+    candidate_file,\n+    lines,\n+    decorated_items: list[DecoratedItem],\n+    source: str,\n+    overwrite=False,\n ):\n     \"\"\"\n     For a given file, update the docstrings for all @auto_docstring candidates and write the new content.\n     \"\"\"\n-    content_base_file_new_lines = lines[: line_ends_candidates[0]]\n-    current_line_start = line_starts_candidates[0]\n-    current_line_end = line_ends_candidates[0]\n-    index = 1\n+    if not decorated_items:\n+        return [], [], []\n+\n     missing_docstring_args_warnings = []\n     fill_docstring_args_warnings = []\n     docstring_args_ro_remove_warnings = []\n \n-    while index <= len(line_starts_candidates):\n+    # Build new file content by processing decorated items and unchanged sections\n+    content_base_file_new_lines = []\n+    last_line_added = 0  # Track the last line we've already added to output (0-based)\n+\n+    for index, item in enumerate(decorated_items):\n+        def_line_0 = item.def_line - 1  # Convert to 0-based\n+\n+        # Parse custom_args if present\n         custom_args_dict = {}\n-        auto_docstring_signature_content = \"\".join(lines[current_line_start:current_line_end])\n-        match = re.findall(r\"custom_args=(\\w+)\", auto_docstring_signature_content)\n-        if match:\n-            custom_args_var_name = match[0]\n-            custom_args_var_content = find_custom_args_with_details(\"\\n\".join(lines), custom_args_var_name)\n-            if custom_args_var_content:\n-                custom_args_dict, _ = parse_docstring(custom_args_var_content)\n-        new_docstring = \"\"\n-        modify_class_docstring = False\n-        # Function\n-        if \"    def\" in lines[current_line_end]:\n+        if item.custom_args_text:\n+            custom_args_dict, _ = parse_docstring(item.custom_args_text)\n+\n+        # Generate new docstring based on kind\n+        if item.kind == \"function\":\n             (\n                 new_docstring,\n                 sig_line_end,\n                 docstring_end,\n                 missing_docstring_args,\n                 fill_docstring_args,\n                 docstring_args_ro_remove,\n-            ) = generate_new_docstring_for_function(lines, current_line_end, custom_args_dict)\n-        # Class\n-        elif \"class \" in lines[current_line_end]:\n+            ) = generate_new_docstring_for_function(lines, item, custom_args_dict)\n+        else:  # class\n             (\n                 new_docstring,\n-                class_sig_line_end,\n-                class_docstring_end_line,\n+                sig_line_end,\n+                docstring_end,\n                 missing_docstring_args,\n                 fill_docstring_args,\n                 docstring_args_ro_remove,\n-            ) = generate_new_docstring_for_class(lines, current_line_end, custom_args_dict)\n-            modify_class_docstring = class_sig_line_end is not None\n-        # Add warnings if needed\n-        if missing_docstring_args:\n-            for arg in missing_docstring_args:\n-                missing_docstring_args_warnings.append(f\"    - {arg} line {current_line_end}\")\n-        if fill_docstring_args:\n-            for arg in fill_docstring_args:\n-                fill_docstring_args_warnings.append(f\"    - {arg} line {current_line_end}\")\n-        if docstring_args_ro_remove:\n-            for arg in docstring_args_ro_remove:\n-                docstring_args_ro_remove_warnings.append(f\"    - {arg} line {current_line_end}\")\n-        # Write new lines\n-        if index >= len(line_ends_candidates) or line_ends_candidates[index] > current_line_end:\n-            if \"    def\" in lines[current_line_end]:\n-                content_base_file_new_lines += lines[current_line_end:sig_line_end]\n-                if new_docstring != \"\":\n-                    content_base_file_new_lines += new_docstring.split(\"\\n\")\n-                if index < len(line_ends_candidates):\n-                    content_base_file_new_lines += lines[docstring_end + 1 : line_ends_candidates[index]]\n-                else:\n-                    content_base_file_new_lines += lines[docstring_end + 1 :]\n-            elif modify_class_docstring:\n-                content_base_file_new_lines += lines[current_line_end:class_sig_line_end]\n-                if new_docstring != \"\":\n-                    content_base_file_new_lines += new_docstring.split(\"\\n\")\n-                if index < len(line_ends_candidates):\n-                    content_base_file_new_lines += lines[class_docstring_end_line + 1 : line_ends_candidates[index]]\n-                else:\n-                    content_base_file_new_lines += lines[class_docstring_end_line + 1 :]\n-            elif index < len(line_ends_candidates):\n-                content_base_file_new_lines += lines[current_line_end : line_ends_candidates[index]]\n-            else:\n-                content_base_file_new_lines += lines[current_line_end:]\n-            if index < len(line_ends_candidates):\n-                current_line_end = line_ends_candidates[index]\n-                current_line_start = line_starts_candidates[index]\n-        index += 1\n+            ) = generate_new_docstring_for_class(lines, item, custom_args_dict, source)\n+\n+        # If sig_line_end is None, this item couldn't be processed (e.g., class with no __init__)\n+        # In this case, we don't modify anything and just continue to the next item\n+        if sig_line_end is None:\n+            continue\n+\n+        # Add all lines from last processed line up to current def line\n+        content_base_file_new_lines += lines[last_line_added:def_line_0]\n+\n+        # Collect warnings\n+        for arg in missing_docstring_args:\n+            missing_docstring_args_warnings.append(f\"    - {arg} line {def_line_0}\")\n+        for arg in fill_docstring_args:\n+            fill_docstring_args_warnings.append(f\"    - {arg} line {def_line_0}\")\n+        for arg in docstring_args_ro_remove:\n+            docstring_args_ro_remove_warnings.append(f\"    - {arg} line {def_line_0}\")\n+\n+        # Add lines from current def through signature\n+        content_base_file_new_lines += lines[def_line_0:sig_line_end]\n+\n+        # Add new docstring if generated\n+        if new_docstring:\n+            content_base_file_new_lines += new_docstring.split(\"\\n\")\n+\n+        # Update last_line_added to skip the old docstring\n+        last_line_added = (docstring_end + 1) if docstring_end is not None else sig_line_end\n+\n+    # Add any remaining lines after the last decorated item\n+    content_base_file_new_lines += lines[last_line_added:]\n+\n     content_base_file_new = \"\\n\".join(content_base_file_new_lines)\n     if overwrite:\n         with open(candidate_file, \"w\", encoding=\"utf-8\") as f:\n@@ -1330,12 +1361,6 @@ def update_file_with_new_docstrings(\n     )\n \n \n-# TODO (Yoni): The functions in check_auto_docstrings rely on direct code parsing, which is prone to\n-# failure on edge cases and not robust to code changes. While this approach is significantly faster\n-# than using inspect (like in check_docstrings) and allows parsing any object including non-public\n-# ones, it may need to be refactored in the future to use a more robust parsing method. Note that\n-# we still need auto_docstring for some non-public objects since their docstrings are included in the\n-# docs of public objects (e.g. ModelOutput classes).\n def check_auto_docstrings(overwrite: bool = False, check_all: bool = False):\n     \"\"\"\n     Check docstrings of all public objects that are decorated with `@auto_docstrings`.\n@@ -1351,11 +1376,23 @@ def check_auto_docstrings(overwrite: bool = False, check_all: bool = False):\n     # 3. For each file, update docstrings for all candidates\n     for candidate_file in auto_docstrings_files:\n         with open(candidate_file, \"r\", encoding=\"utf-8\") as f:\n-            lines = f.read().split(\"\\n\")\n-        line_starts_candidates, line_ends_candidates = get_auto_docstring_candidate_lines(lines)\n+            content = f.read()\n+        lines = content.split(\"\\n\")\n+\n+        # Parse file once to find all @auto_docstring decorated items\n+        decorated_items = _build_ast_indexes(content)\n+\n+        if not decorated_items:\n+            continue\n+\n+        # Update docstrings for all decorated items\n         missing_docstring_args_warnings, fill_docstring_args_warnings, docstring_args_ro_remove_warnings = (\n             update_file_with_new_docstrings(\n-                candidate_file, lines, line_starts_candidates, line_ends_candidates, overwrite=overwrite\n+                candidate_file,\n+                lines,\n+                decorated_items,\n+                content,\n+                overwrite=overwrite,\n             )\n         )\n         if missing_docstring_args_warnings:"
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T21:18:04.556069",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR refactors a docstring validation utility to use AST parsing instead of raw code parsing, which is a non-trivial algorithmic change. The PR includes substantial logic changes (new dataclass, AST-based parsing logic) and provides meaningful context about the refactoring goal. Developers would need to understand the AST approach and how it differs from the previous implementation to work on related validation features.",
      "substance_level": "high"
    }
  },
  {
    "pr_number": 41421,
    "title": "Restore cuda graphs to continuous batching",
    "body": "This PR restores cuda graphs in continuous batching. The main changes associated with this are:\r\n1. the logic of how to generate tokens have been moved to the CB processor, which also handles the cuda graphs\r\n2. the generation step automatically slices the tensors to remove all padding unless cuda graphs are activated\r\n3. cuda graphs are captured on padded shapes, which is 25%, 50%, 75% or 100% of the queries axis and 1/8, ... 8/8 of the keys values axis, to strike a balance between the amount of padding and the quantity of cuda graphs \r\n\r\nDocumentation is kind of lacking but will be added in next commits, I am opening the PR so @ArthurZucker can test stuff out\r\n\r\n- [x] Add more documentation\r\n- [x] Test it out and add performance numbers with / without on AMD / Nvidia with the three main attn implementations",
    "html_url": "https://github.com/huggingface/transformers/pull/41421",
    "created_at": "2025-10-07T16:03:25Z",
    "merged_at": "2025-10-13T09:57:57Z",
    "merge_commit_sha": "cf1e9834ec7339f4c605ba96d9c4e5cf59594cad",
    "base_ref": "main",
    "head_sha": "1443d62e28fcb43445006f1dd37a0b94d4c92188",
    "user": "remi-or",
    "files": [
      {
        "filename": "examples/pytorch/continuous_batching.py",
        "status": "modified",
        "additions": 32,
        "deletions": 24,
        "changes": 56,
        "patch": "@@ -26,22 +26,25 @@\n \n from transformers import AutoModelForCausalLM, AutoTokenizer\n from transformers.generation import GenerationConfig\n+from transformers.generation.continuous_batching.requests import logger\n \n \n # MODEL_ID = \"Qwen/Qwen3-4B-Instruct-2507\"\n SLIDING_WINDOW = 0\n-MODEL_ID = \"google/gemma-2-2b-it\" if SLIDING_WINDOW > 0 else \"Qwen/Qwen3-4B-Instruct-2507\"\n+MODEL_ID = \"google/gemma-2-2b-it\" if SLIDING_WINDOW > 0 else \"meta-llama/Meta-Llama-3-8B\"\n FORCE_MAX_LENGTH = False  # should be False unless you are debugging sliding window features\n+SKIP_SPECIAL_TOKENS = False\n \n \n def generate_simple(\n     attn_impl: str, simple_batch_inputs: list[int], generation_config: GenerationConfig\n ) -> dict[str, str]:\n     attn_impl = {\n-        \"sdpa_paged\": \"sdpa\",\n-        \"eager_paged\": \"eager\",\n+        \"sdpa\": \"sdpa\",\n+        \"eager\": \"eager\",\n         \"paged_attention\": \"eager\",  # TODO: this does not work on AMD docker\n         \"flash_paged\": \"flash_attention_2\",  # TODO: this does not work on AMD docker\n+        \"kernels-community/flash-attn\": \"eager\",\n     }[attn_impl]\n \n     model = AutoModelForCausalLM.from_pretrained(MODEL_ID, dtype=torch.bfloat16, attn_implementation=attn_impl)\n@@ -56,7 +59,7 @@ def generate_simple(\n         # attention_mask = torch.ones_like(input_ids)\n         outputs = model.generate(input_ids, generation_config=generation_config, use_model_defaults=False)\n         generated_tokens = outputs[0][input_ids.shape[1] :]\n-        decoded_output = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n+        decoded_output = tokenizer.decode(generated_tokens, skip_special_tokens=SKIP_SPECIAL_TOKENS)\n         decoded_outputs[key] = decoded_output\n     return decoded_outputs\n \n@@ -99,7 +102,6 @@ def batch_generate(\n     displayed_samples: int = 0,  # -1: no display, 0: display stats, >0: display inputs and some outputs\n     output_file: Optional[str] = None,\n     expected_outputs: Optional[list[str]] = None,\n-    slice_inputs: bool = True,\n ) -> tuple[float, float]:\n     # Actual batch generation\n     if displayed_samples >= 0:\n@@ -108,7 +110,6 @@ def batch_generate(\n     batch_outputs = model.generate_batch(\n         inputs=simple_batch_inputs,\n         generation_config=generation_config,\n-        slice_inputs=slice_inputs,  # TODO: move this to the generation config\n     )\n     end_time_simple = time.time()\n     if displayed_samples >= 0:\n@@ -118,19 +119,21 @@ def batch_generate(\n     token_count = 0\n     data = []\n     for i, request in enumerate(batch_outputs):\n-        input_text = tokenizer.decode(batch_outputs[request].prompt_ids, skip_special_tokens=True)\n+        input_text = tokenizer.decode(batch_outputs[request].prompt_ids, skip_special_tokens=SKIP_SPECIAL_TOKENS)\n         # The key is used to tie back to the output of unbatched generation\n         key = \" \".join(map(str, batch_outputs[request].prompt_ids))\n         data.append({\"input\": input_text, \"key\": key})\n \n         # Try to decode the output\n         try:\n-            output_text = tokenizer.decode(batch_outputs[request].generated_tokens, skip_special_tokens=True)\n+            output_text = tokenizer.decode(\n+                batch_outputs[request].generated_tokens, skip_special_tokens=SKIP_SPECIAL_TOKENS\n+            )\n             token_count += len(batch_outputs[request].generated_tokens[1:])\n-            data[-1][\"output\"] = output_text\n+            data[-1][\"cb_outputs\"] = output_text\n         except Exception as e:\n             print(f\"Decoding failed for request {request}: {e}\")\n-            data[-1][\"output\"] = \"__ERROR__\"\n+            data[-1][\"cb_outputs\"] = \"__ERROR__\"\n             continue\n \n         # Display sample if asked\n@@ -148,7 +151,7 @@ def batch_generate(\n         if expected_outputs is not None:\n             expected_output = expected_outputs.pop(key)\n             matches = output_text == expected_output  # TODO: rework this for a better distance metric\n-            data[-1][\"ref\"] = expected_output\n+            data[-1][\"without_cb\"] = expected_output\n             data[-1][\"matches\"] = matches\n             data[-1].pop(\"key\")\n             print(f\"Request {i} matches\" if matches else f\"Request {i} does NOT match!\")\n@@ -186,19 +189,20 @@ def batch_generate(\n \n     parser.add_argument(\"--attn\", type=str, default=\"kernels-community/flash-attn\", help=\"Attention implementation\")\n     parser.add_argument(\"--matmul-precision\", \"-mp\", type=str, default=\"high\")  # set to \"none\" to disable\n-    parser.add_argument(\"--no-slice-inputs\", action=\"store_true\")  # slicing is enabled by default because much faster\n-    parser.add_argument(\"--use-cuda-graph\", \"-cg\", action=\"store_true\")\n-    parser.add_argument(\"--compile\", action=\"store_true\")\n+    parser.add_argument(\"--cuda-graph\", \"-cg\", help=\"Use cuda graphs\", type=str, default=None)\n+    parser.add_argument(\"--compile\", action=\"store_true\", help=\"Compile the model using torch.compile\")\n \n-    parser.add_argument(\"--samples\", type=int, default=500)\n+    parser.add_argument(\"--samples\", type=int, default=500, help=\"Number of samples to generate\")\n     parser.add_argument(\"--displayed\", type=int, default=0, help=\"Number of samples to display\")\n+    parser.add_argument(\"--log-level\", type=str, default=\"INFO\")\n     parser.add_argument(\"--output-file\", type=str, default=None)\n     parser.add_argument(\"--compare\", action=\"store_true\")\n     parser.add_argument(\"--metrics\", action=\"store_true\")\n     parser.add_argument(\"--profile\", type=str, default=None)\n     args = parser.parse_args()\n \n-    args.slice_inputs = not args.no_slice_inputs\n+    # Set log level\n+    logger.setLevel(args.log_level.upper())\n \n     # If turned on, we setup metrics\n     if args.metrics:\n@@ -207,6 +211,15 @@ def batch_generate(\n     # Set matmul precision if not none\n     if args.matmul_precision != \"none\":\n         torch.set_float32_matmul_precision(args.matmul_precision)\n+    # Parse cuda graph argument\n+    if args.cuda_graph is not None:\n+        use_cuda_graph = {\n+            \"none\": None,\n+            \"yes\": True, \"y\": True, \"true\": True, \"t\": True, \"1\": True,\n+            \"no\": False, \"n\": False, \"false\": False, \"f\": False, \"0\": False,\n+        }[args.cuda_graph.lower()]  # fmt: skip\n+    else:\n+        use_cuda_graph = None\n \n     # Prepare model\n     model = AutoModelForCausalLM.from_pretrained(\n@@ -222,9 +235,6 @@ def batch_generate(\n     # If turned on, we compile the model\n     if args.compile:\n         model.forward = torch.compile(model.forward, mode=\"max-autotune-no-cudagraphs\")\n-    if args.slice_inputs:\n-        assert not args.compile, \"Slicing inputs requires is not the model to be compiled\"\n-        assert not args.use_cuda_graph, \"Slicing inputs is not compatible with cuda graphs\"\n \n     # Prepare tokenizer and dataset\n     tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, padding_side=\"left\")\n@@ -237,10 +247,10 @@ def batch_generate(\n     # Prepare generation config\n     generation_config = GenerationConfig(\n         max_new_tokens=512,\n-        use_cuda_graph=args.use_cuda_graph,\n+        use_cuda_graph=use_cuda_graph,\n         eos_token_id=tokenizer.pad_token_id if FORCE_MAX_LENGTH else tokenizer.eos_token_id,\n         pad_token_id=tokenizer.pad_token_id,\n-        do_sample=True,\n+        do_sample=not args.compare,\n         temperature=0.8,\n         top_p=0.9,\n         num_blocks=args.num_blocks,\n@@ -265,7 +275,6 @@ def batch_generate(\n         generation_config,\n         tokenizer,\n         displayed_samples=-1,\n-        slice_inputs=args.slice_inputs,\n     )\n \n     if args.profile is not None:\n@@ -282,12 +291,11 @@ def batch_generate(\n             displayed_samples=args.displayed,\n             output_file=args.output_file,\n             expected_outputs=expected_outputs,\n-            slice_inputs=args.slice_inputs,\n         )\n     if args.profile is not None:\n         filename = args.profile if args.profile.endswith(\".json\") else args.profile + \".json\"\n         prof.export_chrome_trace(filename)\n \n # Example usage:\n-# python examples/pytorch/continuous_batching.py --attn sdpa_paged -mp none --slice-inputs --samples 3 --compare\n+# python examples/pytorch/continuous_batching.py --attn sdpa_paged -mp none --samples 3 --compare\n # python examples/pytorch/continuous_batching.py --num-blocks 369 --max-batch-tokens 23 --attn sdpa_paged -mp none --samples 1 --displayed 0 --output-file sliced.json"
      },
      {
        "filename": "examples/pytorch/continuous_batching_simple.py",
        "status": "modified",
        "additions": 0,
        "deletions": 2,
        "changes": 2,
        "patch": "@@ -68,7 +68,6 @@\n     _ = model.generate_batch(\n         inputs=simple_batch_inputs[: min(5, args.samples)],\n         generation_config=generation_config,\n-        slice_inputs=True,\n     )\n \n     # Actual batch generation\n@@ -77,7 +76,6 @@\n     batch_outputs = model.generate_batch(\n         inputs=simple_batch_inputs,\n         generation_config=generation_config,\n-        slice_inputs=True,\n     )\n     end_time = time.time()\n     print(\"Done with batch generation.\")"
      },
      {
        "filename": "src/transformers/generation/continuous_batching/cache.py",
        "status": "modified",
        "additions": 5,
        "deletions": 6,
        "changes": 11,
        "patch": "@@ -204,8 +204,8 @@ def __init__(\n         # Initialize the cache\n         self.key_cache: list[torch.Tensor] = []\n         self.value_cache: list[torch.Tensor] = []\n-        # We add one extra token to the cache to handle padding and generally discard unwanted tokens\n-        self.cache_shape = (num_blocks * self.block_size + 1, self.num_key_value_heads, self.head_dim)\n+        # We add two extra tokens to the cache to handle padding and generally discard unwanted tokens\n+        self.cache_shape = (num_blocks * self.block_size + 2, self.num_key_value_heads, self.head_dim)\n         for _ in range(group_size):\n             new_layer_key_cache = torch.empty(self.cache_shape, dtype=self.dtype, device=self.device)\n             new_layer_value_cache = torch.empty(self.cache_shape, dtype=self.dtype, device=self.device)\n@@ -290,7 +290,6 @@ def update(\n         layer_idx: int,\n         read_index: list[torch.Tensor],  # shape [num_layer_groups, seqlen_kv + past_length]\n         write_index: list[torch.Tensor],  # shape [num_layer_groups, seqlen_q]\n-        **kwargs,\n     ) -> tuple[torch.Tensor, torch.Tensor]:  # shape [seqlen_kv + past_length, num_kv_heads, head_dim]\n         \"\"\"Update the cache with new key-value states for a specific layer. This method writes new KV states to the\n         appropriate cache locations. The behavior differs based on the layer's attention type:\n@@ -324,11 +323,11 @@ def update(\n         # the only case where you may write over cache you need to use\n         else:\n             # Add the cache to the key and value states\n-            mask = layer_read_index == -1  # TODO: can this can be efficiently precomputed?\n+            mask = (layer_read_index == -1).unsqueeze(-1).unsqueeze(-1)  # TODO: should this be precomputed?\n             key_states_with_cache = k_cache[layer_read_index, :, :]\n-            key_states_with_cache[mask] = key_states\n+            key_states_with_cache.masked_scatter_(mask, key_states)\n             value_states_with_cache = v_cache[layer_read_index, :, :]\n-            value_states_with_cache[mask] = value_states\n+            value_states_with_cache.masked_scatter_(mask, value_states)\n             # Write new KV values to the cache\n             k_cache[layer_write_index, :, :] = key_states\n             v_cache[layer_write_index, :, :] = value_states"
      },
      {
        "filename": "src/transformers/generation/continuous_batching/continuous_api.py",
        "status": "modified",
        "additions": 316,
        "deletions": 195,
        "changes": 511,
        "patch": "@@ -15,18 +15,21 @@\n # limitations under the License.\n import queue\n import threading\n+from collections.abc import Generator\n from dataclasses import dataclass\n from functools import partial\n from itertools import count\n+from math import ceil\n from time import perf_counter\n from typing import Optional, Union\n \n import torch\n from torch import nn\n from tqdm import tqdm\n \n-from ...configuration_utils import PreTrainedConfig\n+from ...configuration_utils import PretrainedConfig\n from ...generation.configuration_utils import GenerationConfig\n+from ...generation.logits_process import LogitsProcessor\n from ...integrations.hub_kernels import load_and_register_attn_kernel\n from ...utils.logging import logging\n from ...utils.metrics import ContinuousBatchProcessorMetrics, attach_tracer, traced\n@@ -35,10 +38,44 @@\n from .scheduler import SCHEDULER_MAPPING, FIFOScheduler, Scheduler\n \n \n+\"\"\"\n+To enable cuda graphs, we need the dimensions of all tensors to be static, which is counter-intuitive for CB. In CB, as\n+generation goes on, there are two dimensions that change:\n+- the number of queries tokens (Q), which can vary from batch to batch\n+- the number of keys/values tokens (KV), which grows as the cache does\n+\n+To solve this, we slice along those dimensions to fixed lengths. The size of the slices is controlled by the variables\n+below: NUM_X_CUDA_GRAPHS means that we create at most NUM_X_CUDA_GRAPHS graphs for the X dimension. So if the maximum\n+number of queries tokens is 1000, and NUM_Q_CUDA_GRAPHS is 4, we will slice the number of queries token by intervals of\n+1000 / 4 = 250 tokens, ie. to 250, 500, 750 or 1000 queries tokens.\n+\n+Smaller slices means more granularity and thus less padding. But since each graph takes up space on the GPU and time to\n+create, we don't want to many graphs. And since the size of the KV dimension is the number of queries tokens plus the\n+number of tokens cached, dimension of KV is usually much larger than the the dimension of Q. So we have more granularity\n+for the KV dimension than the query dimension.\n+\"\"\"\n+NUM_Q_CUDA_GRAPHS = 4\n+NUM_KV_CUDA_GRAPHS = 8\n+\n+\n+def pad_by_intervals(size: int, max_value: int, nb_intervals: int) -> int:\n+    \"\"\"Return the smallest multiple of (max_value) // (nb_intervals) greater than (size).\"\"\"\n+    interval_size = max_value // nb_intervals\n+    if interval_size == 0:\n+        return max_value\n+    padded = ceil(size / interval_size) * interval_size\n+    return min(padded, max_value)\n+\n+\n+def attn_mask_is_needed(config: PretrainedConfig) -> bool:\n+    \"\"\"Checks if attention mask is needed for the given (config).\"\"\"\n+    return config._attn_implementation in [\"paged|eager\", \"paged|sdpa\"]\n+\n+\n def build_attention_mask(\n     attention_mask: torch.Tensor,\n-    cumulative_seqlens_q: torch.Tensor,\n-    cumulative_seqlens_k: torch.Tensor,\n+    cumulative_seqlens_q: list[int],\n+    cumulative_seqlens_k: list[int],\n     sliding_window: int = 1,\n ) -> None:\n     \"\"\"Builds an attention mask inplace using the cumulative seqlens of the query and key. If given a sliding window, it\n@@ -57,7 +94,7 @@ def build_attention_mask(\n            \u2588 \u2588 \u2588 \u2588 \u2588 \u2588 \u2588 \u2588\n \n     SLIDING WINDOW MASK:\n-         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 seqlen_k - seqlen_q - sliding_window = 8 - 4 - 6 = -2 offset to the right\n+         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 seqlen_k - seqlen_q - sliding_window = 8 - 4 - 6 = -2 offset to the left\n        <\u2500\u2534\u2500>\n      \u2591 \u2588 | \u2588 \u2588 \u2588 \u2588 \u2588 \u2588 \u2588 \u2588\n      \u2591 \u2591 | \u2588 \u2588 \u2588 \u2588 \u2588 \u2588 \u2588 \u2588\n@@ -80,7 +117,7 @@ def build_attention_mask(\n            \u2588 \u2588 \u2588 \u2588 \u2588\n \n     SLIDING WINDOW MASK:\n-         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 seqlen_k - seqlen_q - sliding_window = 5 - 3 - 2 = 0 offset to the right\n+         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 seqlen_k - seqlen_q - sliding_window = 5 - 3 - 2 = 0 offset to the left\n         <\u2534>\n          | \u2591 \u2588 \u2588 \u2588 \u2588\n          | \u2591 \u2591 \u2588 \u2588 \u2588\n@@ -141,16 +178,16 @@ class ContinuousBatchProcessor:\n     def __init__(\n         self,\n         cache: PagedAttentionCache,\n-        config: PreTrainedConfig,\n+        config: PretrainedConfig,\n         generation_config: GenerationConfig,\n         input_queue: queue.Queue,\n         output_queue: queue.Queue,\n         stop_event: threading.Event,\n         model_device: torch.device,\n         model_dtype: torch.dtype,\n         scheduler: Scheduler,\n-        manual_eviction: bool = False,\n-        slice_inputs: bool = True,  # TODO: There should be an heuristic to decide on slicing, compile, cuda graphs...\n+        manual_eviction: bool,\n+        use_cuda_graph: bool,\n     ) -> None:\n         \"\"\"Initialize the continuous batch processor.\n \n@@ -165,7 +202,8 @@ def __init__(\n             model_dtype: Data type for model inputs/outputs\n             scheduler: The [`Scheduler`] to use\n             manual_eviction: Whether to manually evict blocks from the cache\n-            slice_inputs: Whether to slice the inputs to the model\n+            use_cuda_graph: Whether to use cuda graphs or not during CB. Check the docstring at the top of the file for\n+                more details.\n         \"\"\"\n         self.cache = cache\n         self.config = config\n@@ -177,36 +215,39 @@ def __init__(\n         self.model_dtype = model_dtype\n         self.scheduler = scheduler\n         self.manual_eviction = manual_eviction\n-        self.slice_inputs = slice_inputs\n \n         # Retrieve the size of the sliding window if there is one\n         self.sliding_window = 1 if getattr(config, \"sliding_window\", None) is None else config.sliding_window\n-\n+        # Accumulator for batch scheduling\n         self.requests_in_batch: list[RequestState] = []\n+        # Cuda graphs for the generation step\n+        self._graphs: Optional[dict[tuple[int, int], torch.cuda.CUDAGraph]] = {} if use_cuda_graph else None\n \n         # Set up metrics collector\n         self.max_batch_tokens = cache.max_batch_tokens\n         self.metrics = ContinuousBatchProcessorMetrics(cache.max_batch_tokens)\n \n         # Setup static tensors\n-        self.total_query_length = 0\n-        self.total_key_length = 0\n-        self.total_batch_size = 0\n+        self.actual_query_length = 0  # This is the actual number of queries tokens in the batch\n+        self.actual_key_length = 0  # This is the actual number of keys/values tokens in the batch\n+        self.actual_batch_size = 0  # This is the actual number of requests in the batch\n+        self.actual_index_sizes = [(0, 0) for _ in range(cache.num_groups)]\n         self.setup_static_tensors(cache.num_groups)\n \n     @traced(standalone=True)\n     def setup_static_tensors(self, num_groups: int) -> None:\n-        T = self.max_batch_tokens\n+        \"\"\"Setup the static tensors that are used for storage during the generation step. No other tensor will be\n+        allowed for the inputs or the outputs of the generation step.\"\"\"\n         num_pages = self.cache.num_blocks * self.cache.block_size\n         self.tensor_metadata = {\"dtype\": torch.int32, \"device\": self.model_device}\n \n         # Some tensors always have the same shape regardless of the model\n-        self.input_ids = torch.empty((1, T), **self.tensor_metadata)\n-        self.position_ids = torch.empty((1, T), **self.tensor_metadata)\n-        self.cumulative_seqlens_q = torch.empty((T + 1,), **self.tensor_metadata)\n+        self.input_ids = torch.empty((1, self.max_batch_tokens), **self.tensor_metadata)\n+        self.position_ids = torch.empty((1, self.max_batch_tokens), **self.tensor_metadata)\n+        self.cumulative_seqlens_q = torch.empty((self.max_batch_tokens + 1,), **self.tensor_metadata)\n         self.max_seqlen_q = 0\n-        self.logits_indices = torch.empty((T,), **self.tensor_metadata)\n-        self.output_ids = torch.empty((1, T), **self.tensor_metadata)\n+        self.logits_indices = torch.empty((self.max_batch_tokens,), **self.tensor_metadata)\n+        self.output_ids = torch.empty((1, self.max_batch_tokens), **self.tensor_metadata)\n \n         # For some kwargs, we have a dict of tensors with as many items as there are attention types\n         layer_types = getattr(self.config, \"layer_types\", None)\n@@ -216,13 +257,13 @@ def setup_static_tensors(self, num_groups: int) -> None:\n         layer_types = list(set(layer_types))\n \n         self.cumulative_seqlens_k = {\n-            layer_type: torch.empty((T + 1), **self.tensor_metadata) for layer_type in layer_types\n+            l_type: torch.empty((self.max_batch_tokens + 1), **self.tensor_metadata) for l_type in layer_types\n         }\n         self.max_seqlen_k = dict.fromkeys(layer_types, 0)\n \n-        if self.return_attention_mask():\n+        if attn_mask_is_needed(self.config):\n             attn_mask_kwargs = {\n-                \"size\": (1, 1, T, num_pages + T),\n+                \"size\": (1, 1, self.max_batch_tokens, num_pages + self.max_batch_tokens),\n                 \"dtype\": self.model_dtype,\n                 \"device\": self.model_device,\n             }\n@@ -231,33 +272,26 @@ def setup_static_tensors(self, num_groups: int) -> None:\n             self.attention_mask = None\n \n         # For other kwargs, we need a list of tensors with as many tensors as there are groups\n-        self.write_index_storage = [torch.empty((T,), **self.tensor_metadata) for _ in range(num_groups)]\n-        self.read_index_storage = [torch.empty((num_pages + T), **self.tensor_metadata) for _ in range(num_groups)]\n+        self.write_index_storage = [\n+            torch.empty((self.max_batch_tokens,), **self.tensor_metadata) for _ in range(num_groups)\n+        ]\n+        self.read_index_storage = [\n+            torch.empty((num_pages + self.max_batch_tokens), **self.tensor_metadata) for _ in range(num_groups)\n+        ]\n         # For read index, the +T is because there are -1 for seqlen_q when model uses a sliding window\n \n         # After allocating empty tensors, we reset them to the right value\n         self.reset_static_tensors(full_reset=True)\n \n-    def return_attention_mask(self) -> bool:\n-        return self.config._attn_implementation in [\n-            \"paged|eager\",\n-            \"paged|sdpa\",\n-        ]  # we set `is_causal` to True in paged call\n-\n     @traced\n     @torch.no_grad()\n-    def reset_static_tensors(self, full_reset: bool = False):\n+    def reset_static_tensors(self, full_reset: bool = False) -> None:\n         \"\"\"Reset static tensors for the next batch. In between batches, reset only the parts that were used in the last\n         batch, but for initialisation, we can reset everything using the (full_reset) flag.\"\"\"\n         # Compute the slice to reset\n-        if full_reset or not self.slice_inputs:\n-            q_len = self.write_index_storage[0].size(-1)\n-            k_len = self.read_index_storage[0].size(-1)\n-            b_size = self.write_index_storage[0].size(0)\n-        else:\n-            q_len = self.total_query_length\n-            k_len = self.total_key_length\n-            b_size = self.total_batch_size\n+        q_len = self.write_index_storage[0].size(-1) if full_reset else self.actual_query_length\n+        k_len = self.read_index_storage[0].size(-1) if full_reset else self.actual_key_length\n+        b_size = self.write_index_storage[0].size(0) if full_reset else self.actual_batch_size\n \n         # Reset the attributes that always have the same shape\n         self.input_ids[:, :q_len].zero_()\n@@ -276,14 +310,19 @@ def reset_static_tensors(self, full_reset: bool = False):\n \n         # Reset the attributes that are lists of tensors\n         for i in range(self.cache.num_groups):\n-            self.write_index_storage[i][:q_len].fill_(-1)\n-            self.read_index_storage[i][: q_len + k_len].fill_(-1)\n-\n-    def get_model_kwargs(self) -> PagedAttentionArgs:\n-        \"\"\"Get model keyword arguments for the current batch.\"\"\"\n-        # Compute the slice to return\n-        q_len = self.total_query_length if self.slice_inputs else self.write_index_storage[0].size(-1)\n-        b_size = self.total_batch_size if self.slice_inputs else self.cumulative_seqlens_q.size(-1) - 1\n+            self.write_index_storage[i][:q_len].fill_(-2)  # -1 is used to let the cache where new states go\n+            self.read_index_storage[i][: q_len + k_len].fill_(-2)  # same\n+\n+    def get_model_kwargs(self, padded_q_size: int = 0, padded_kv_cache_size: int = 0) -> PagedAttentionArgs:\n+        \"\"\"Get model keyword arguments for the current batch, eventually padding the query dimension to (padded_q_size)\n+        and the keys/values dimension to (padded_kv_cache_size). The padding is only useful if we want static shapes,\n+        like when using cuda graphs AND only activated if both Q and KV are padded.\"\"\"\n+        # Compute the slice to return, with the given padding if we are using cuda graphs\n+        use_padding = padded_q_size > 0 and padded_kv_cache_size > 0\n+        q_len = padded_q_size if use_padding else self.actual_query_length\n+        b_size = padded_q_size if use_padding else self.actual_batch_size\n+        # If there is padding, the size of the KV is the nb of padded Q tokens + the size padded of the padded KV cache\n+        padded_kv_size = padded_q_size + padded_kv_cache_size\n \n         # Prepare the kwargs, the attributes that are either tensors or dict of tensors are initialized to empty dicts\n         kwargs = {\n@@ -295,43 +334,57 @@ def get_model_kwargs(self) -> PagedAttentionArgs:\n             \"cu_seq_lens_k\": {},\n             \"max_seqlen_k\": {},\n             \"attention_mask\": {},\n-            \"read_index\": self.read_index,  # slicing is done during building\n-            \"write_index\": self.write_index,  # slicing is done during building\n+            \"read_index\": [],\n+            \"write_index\": [],\n             \"cache\": self.cache,\n             \"use_cache\": False,\n         }\n \n+        # If we use constant-sized slicing, there are some \"padding\" queries tokens which FA has some issues with. In\n+        # some models like Qwen3-4B-Instruct-2507, if we don't include these tokens in cumulative_seqlens_q, there are\n+        # some NaNs in the output logits even for non-padded tokens.\n+        if use_padding:\n+            self.max_seqlen_q = max(self.max_seqlen_q, q_len - self.total_seqlen_q)\n+            self.cumulative_seqlens_q[self.actual_batch_size + 1 :] = q_len\n+            # FIXME: is there another way to avoid this? It has a very slight impact on performance (~5 tok/s)\n+\n+        # For the attributes that are lists of tensors, we construct list of tensor references\n+        for i, (read_index_size, write_index_size) in enumerate(self.actual_index_sizes):\n+            read_index_size = padded_kv_size if use_padding else read_index_size\n+            write_index_size = padded_q_size if use_padding else write_index_size\n+            kwargs[\"read_index\"].append(self.read_index_storage[i][:read_index_size])\n+            kwargs[\"write_index\"].append(self.write_index_storage[i][:write_index_size])\n+\n         # For the attributes that are dict of tensors, we replace the dict with a tensor if there is only one entry\n         layer_types = list(self.cumulative_seqlens_k.keys())\n         if len(layer_types) > 1:\n             for layer_type, seqlens_k in self.cumulative_seqlens_k.items():\n                 kwargs[\"cu_seq_lens_k\"][layer_type] = seqlens_k[: b_size + 1]\n                 kwargs[\"max_seqlen_k\"][layer_type] = self.max_seqlen_k[layer_type]\n                 if self.attention_mask is not None:\n-                    k_len = seqlens_k[b_size] if self.slice_inputs else self.attention_mask[layer_type].size(-1)\n+                    k_len = padded_kv_size if use_padding else seqlens_k[b_size]\n                     kwargs[\"attention_mask\"][layer_type] = self.attention_mask[layer_type][..., :q_len, :k_len]\n         else:\n             layer_type = layer_types[0]\n             kwargs[\"cu_seq_lens_k\"] = self.cumulative_seqlens_k[layer_type][: b_size + 1]\n             kwargs[\"max_seqlen_k\"] = self.max_seqlen_k[layer_type]\n             if self.attention_mask is not None:\n-                k_len = self.cumulative_seqlens_k[layer_type][b_size]\n-                k_len = k_len if self.slice_inputs else self.attention_mask[layer_type].size(-1)\n+                k_len = padded_kv_size if use_padding else self.cumulative_seqlens_k[layer_type][b_size]\n                 kwargs[\"attention_mask\"] = self.attention_mask[layer_type][..., :q_len, :k_len]\n \n         if self.attention_mask is None:\n             kwargs[\"attention_mask\"] = None\n         return kwargs\n \n-    def __repr__(self):\n+    def __repr__(self) -> str:\n         return (\n             f\"ContinuousBatchProcessor(input_queue={self.input_queue}, output_queue={self.output_queue}, \"\n             f\"active_requests={self.scheduler.active_requests}, waiting_requests={self.scheduler.waiting_requests})\"\n             + self.get_model_kwargs().__repr__()\n         )\n \n     @traced\n-    def _get_new_requests(self):\n+    def _get_new_requests(self) -> None:\n         \"\"\"Pull new requests from the input queue and add to waiting list.\"\"\"\n         while not self.input_queue.empty():\n             try:\n@@ -349,7 +402,7 @@ def _get_new_requests(self):\n                     self._handle_request_error(e, state)\n \n     @traced\n-    def _handle_request_error(self, error, state: RequestState):\n+    def _handle_request_error(self, error: Exception, state: RequestState) -> None:\n         \"\"\"Handle general request processing error.\"\"\"\n         state.status = RequestStatus.FAILED\n         state.error = str(error)\n@@ -382,12 +435,12 @@ def prepare_next_batch(self) -> bool:\n         self.metrics.record_batch_metrics(self.requests_in_batch)\n \n         # Reset the static tensors used for storage\n-        self.reset_static_tensors()  # TODO: with slice_inputs, this might be unnecessary\n+        self.reset_static_tensors()  # TODO: this might be unnecessary\n \n         # Prepare accumulators\n-        self.total_query_length = 0\n-        self.total_key_length = 0\n-        self.total_batch_size = 0\n+        self.actual_query_length = 0\n+        self.actual_key_length = 0\n+        self.actual_batch_size = 0\n \n         input_ids = []\n         position_ids = []\n@@ -410,10 +463,10 @@ def prepare_next_batch(self) -> bool:\n             seqlens_k = self.cache.get_seqlens_k(state.request_id, past_length, query_length)\n \n             # Then we update the total lengths that are used for slicing\n-            self.total_query_length += query_length\n+            self.actual_query_length += query_length\n             # total_key_length is used to slice the keys so we need to take the max of all the key lengths\n-            self.total_key_length += max(seqlens_k.values())\n-            self.total_batch_size += 1\n+            self.actual_key_length += max(seqlens_k.values())\n+            self.actual_batch_size += 1\n             # And the attribute tracking the position in the request object\n             state.position_offset += query_length\n \n@@ -476,6 +529,7 @@ def _build_tensors(\n         self.position_ids[:, : len(position_ids)] = to_tensor(position_ids)\n         self.cumulative_seqlens_q[: len(cumulative_seqlens_q)] = to_tensor(cumulative_seqlens_q)\n         self.logits_indices[: len(logits_indices)] = to_tensor(logits_indices)\n+        self.total_seqlen_q = cumulative_seqlens_q[-1]\n \n         # Those kwargs are either dict of tensors or tensors, so we need to handle both cases\n         for layer_type, layer_type_seqlens_k in cumulative_seqlens_k.items():\n@@ -492,42 +546,32 @@ def _build_tensors(\n         self.read_index = []\n         self.write_index = []\n         for i, group_read_indices, group_write_indices in zip(count(), read_index, write_index):\n-            # Write in the actual tensors\n             self.read_index_storage[i][: len(group_read_indices)] = to_tensor(group_read_indices)\n             self.write_index_storage[i][: len(group_write_indices)] = to_tensor(group_write_indices)\n-            # Slice to the right size\n-            r = len(group_read_indices) if self.slice_inputs else self.read_index_storage[i].size(-1)\n-            w = len(group_write_indices) if self.slice_inputs else self.write_index_storage[i].size(-1)\n-            # Add to the index\n-            self.read_index.append(self.read_index_storage[i][:r])\n-            self.write_index.append(self.write_index_storage[i][:w])\n+            self.actual_index_sizes[i] = (len(group_read_indices), len(group_write_indices))\n \n     @traced\n-    def _sync(self):\n+    def _sync(self) -> list[int]:\n         if self.output_ids is not None:\n             try:\n-                out = self.output_ids.tolist()[0]  # should be the only sync we do\n+                return self.output_ids.tolist()[0]\n             except Exception:\n-                out = [0, 1]\n-        else:\n-            out = [0, 0]\n-        return out\n+                return [0, 1]\n+        return [0, 0]\n \n     @traced\n-    def _maybe_send_output(self, state: RequestState, token: int):\n+    def _maybe_send_output(self, state: RequestState) -> None:\n         \"\"\"Send output to the queue based on streaming mode and request state.\"\"\"\n         if state.streaming:\n             self.output_queue.put(state.to_generation_output())\n         elif state.status == RequestStatus.FINISHED:\n             self.output_queue.put(state.to_generation_output())\n \n     @traced\n-    def update_batch(self):\n+    def update_batch(self) -> None:\n         \"\"\"Update request states based on generated tokens.\"\"\"\n         out_tokens = self._sync()\n-        finished_request_ids = []\n         for i, state in enumerate(self.requests_in_batch):\n-            req_id = state.request_id\n             if len(state.remaining_prompt_ids) == 0:\n                 self.metrics.record_ttft_metric(state.created_time, state.request_id)\n                 state.status = RequestStatus.DECODING\n@@ -536,8 +580,7 @@ def update_batch(self):\n                 if state.update_with_token(token):\n                     self.metrics.record_request_completion(state.created_time, state.request_id)\n                     self.scheduler.finish_request(state.request_id, evict_from_cache=(not self.manual_eviction))\n-                    finished_request_ids.append(req_id)\n-                self._maybe_send_output(state, token)\n+                self._maybe_send_output(state)\n             elif state.status == RequestStatus.PREFILLING_SPLIT:\n                 state.status = RequestStatus.SPLIT_PENDING_REMAINDER\n         if self.cache.get_num_free_blocks() == 0:\n@@ -557,7 +600,7 @@ def handle_batch_error(self, error):\n             self.scheduler.finish_request(req.request_id)\n \n     @traced\n-    def fail_all_requests(self, error):\n+    def fail_all_requests(self, error: Exception) -> None:\n         \"\"\"Fail all active requests with the given error.\n \n         Args:\n@@ -577,6 +620,95 @@ def fail_all_requests(self, error):\n         # Clear the ordering queue\n         self.scheduler.waiting_requests_order.clear()\n \n+    @traced\n+    @torch.no_grad\n+    def _generation_step(self, model: nn.Module, logit_processor: LogitsProcessor, do_sample: bool) -> None:\n+        \"\"\"Perform a single generation step.\"\"\"\n+\n+        # If cuda graphs are disabled, we just use the actual size\n+        if self._graphs is None:\n+            batch_data = self.get_model_kwargs()\n+            self._forward_process_and_sample(model, batch_data, logit_processor, do_sample)\n+            return None\n+\n+        # Determine the padded size of the queries and keys/values\n+        padded_q = pad_by_intervals(self.actual_query_length, self.max_batch_tokens, NUM_Q_CUDA_GRAPHS)\n+\n+        max_read_index_size = max(self.actual_index_sizes[i][0] for i in range(self.cache.num_groups))\n+        padded_read_index_size = pad_by_intervals(\n+            max_read_index_size - self.max_batch_tokens,\n+            self.cache.num_blocks * self.cache.block_size,\n+            NUM_KV_CUDA_GRAPHS,\n+        )\n+\n+        # Get the batch data and the associated graph\n+        batch_data = self.get_model_kwargs(padded_q, padded_read_index_size)\n+\n+        graph = self._graphs.get((padded_q, padded_read_index_size))\n+\n+        # If we have a graph that fits, we replay it\n+        if graph is not None:\n+            graph.replay()\n+            return None\n+\n+        # Otherwise, we need to create it\n+        logger.info(f\"Creating graph for {(padded_q, padded_read_index_size) = }\")\n+        stream = torch.cuda.Stream(device=model.device)\n+        stream.wait_stream(torch.cuda.current_stream())\n+        # Warmup\n+        with torch.cuda.stream(stream):\n+            self._forward_process_and_sample(model, batch_data, logit_processor, do_sample)\n+        torch.cuda.current_stream().wait_stream(stream)\n+        # Catpure\n+        graph = torch.cuda.CUDAGraph()\n+        with torch.cuda.graph(graph, stream=stream):\n+            self._forward_process_and_sample(model, batch_data, logit_processor, do_sample)\n+        self._graphs[(padded_q, padded_read_index_size)] = graph\n+\n+    @traced\n+    def _forward_process_and_sample(\n+        self, model: nn.Module, batch_data: dict, logit_processor: LogitsProcessor, do_sample: bool\n+    ) -> None:\n+        \"\"\"This function performs the forward pass, logits processing, and sampling; which are broken down into smaller\n+        function to be easier to trace with OpenTelemetry.\"\"\"\n+        # with torch.no_grad():\n+        logits = self._model_forward(model, batch_data)\n+        # if self.log_prob_generation:    batch_processor.output_probs.copy_(logits)  # TODO\n+        probs = self._process_logit(batch_data, logits, logit_processor)\n+        self._sample(probs, do_sample)\n+\n+    @traced(span_name=\"model_forward\")\n+    def _model_forward(self, model: nn.Module, batch_data: dict) -> torch.Tensor:\n+        return model(**batch_data).logits\n+\n+    @traced(span_name=\"logit_processing\")\n+    def _process_logit(self, batch_data: dict, logits: torch.Tensor, logit_processor: LogitsProcessor) -> torch.Tensor:\n+        # Pass continuous batching context to logits processor if it supports it.\n+        if hasattr(logit_processor, \"set_continuous_batching_context\"):\n+            logit_processor.set_continuous_batching_context(batch_data[\"logits_indices\"], batch_data[\"cu_seq_lens_q\"])\n+        # Handle shape compatibility: logit processors expect 2D tensors [batch_size, vocab_size]\n+        # but continuous batching always produces 3D tensors [batch_size, seq_len, vocab_size]\n+        batch_size, seq_len, vocab_size = logits.shape\n+        logits_2d = logits.view(batch_size * seq_len, vocab_size)\n+        input_ids_2d = batch_data[\"input_ids\"].view(batch_size * seq_len)\n+        # Process with 2D tensors\n+        processed_logits_2d = logit_processor(input_ids_2d, logits_2d)\n+        # Reshape back to 3D\n+        return processed_logits_2d.view(batch_size, seq_len, vocab_size)\n+\n+    @traced(span_name=\"sampling\")\n+    def _sample(self, probs: torch.Tensor, do_sample: bool) -> None:\n+        if do_sample:\n+            probs = nn.functional.softmax(probs, dim=-1)\n+            # probs[0] has shape [seq_len, vocab_size], multinomial returns [seq_len, 1]\n+            next_tokens = torch.multinomial(probs[0], num_samples=1).squeeze(-1)  # Now [seq_len]\n+            # Add batch dimension back to match argmax output\n+            next_tokens = next_tokens.unsqueeze(0)  # Now [1, seq_len]\n+        else:\n+            next_tokens = torch.argmax(probs, dim=-1)  # Already [1, seq_len]\n+        tokens = next_tokens.size(1)  # Get seq_len dimension\n+        self.output_ids[:, :tokens].copy_(next_tokens)\n+\n \n # Manager Class (User Interface)\n @attach_tracer()\n@@ -589,19 +721,21 @@ class ContinuousBatchingManager:\n \n     def __init__(\n         self,\n-        model,\n+        model: nn.Module,\n         generation_config: GenerationConfig,\n         manual_eviction: bool = False,\n-        max_queue_size=0,\n-        slice_inputs: bool = True,\n-    ):\n-        \"\"\"\n-        Initialize the continuous batching manager.\n+        max_queue_size: int = 0,\n+        num_q_cuda_graphs: int = 0,\n+        num_kv_cuda_graphs: int = 0,\n+    ) -> None:\n+        \"\"\"Initialize the continuous batching manager.\n \n         Args:\n             model: The language model for generation\n             generation_config: Configuration for generation parameters\n             max_queue_size: Maximum size of the request queue (0 = unlimited)\n+            num_q_cuda_graphs: (optional) Number of CUDA graphs to use for the query dimension\n+            num_kv_cuda_graphs: (optional) Number of CUDA graphs to use for the keys/values dimension\n         \"\"\"\n         if \"paged|\" not in model.config._attn_implementation:\n             attn_implementation = f\"paged|{model.config._attn_implementation}\"\n@@ -627,17 +761,38 @@ def __init__(\n         self.model.generation_config.top_p = None\n         self.do_sample = getattr(generation_config, \"do_sample\", True)\n         self.logit_processor = self.model._get_logits_processor(generation_config)\n-        self.use_cuda_graph = getattr(generation_config, \"use_cuda_graph\", False)  # TODO: same as do_sample\n-        self.profile = getattr(generation_config, \"profile\", False)\n+        use_cuda_graph: Optional[bool] = getattr(generation_config, \"use_cuda_graph\", None)\n+        self.profile = getattr(generation_config, \"profile\", False)  # TODO: not supported yet\n         self.manual_eviction = manual_eviction\n         self.batch_processor: Optional[ContinuousBatchProcessor] = None\n-        self.slice_inputs = slice_inputs\n \n+        # If a number of cuda graphs was specified for either Q or KV, we activate cuda graphs\n+        if num_q_cuda_graphs > 0 or num_kv_cuda_graphs > 0:\n+            self.use_cuda_graph = True\n+        # If use_cuda_graph is specified, we follow the user's choice\n+        elif use_cuda_graph is not None:\n+            self.use_cuda_graph = use_cuda_graph\n+        # If the use of cuda graphs is not specified, we follow the user's choice, otherwise we have a default heuristic\n+        else:\n+            # Attention implementations where an attention mask is needed suffer a lot more from the padding associated\n+            # with cuda graphs, so default is to turn cuda graphs off for those implementations\n+            self.use_cuda_graph = not attn_mask_is_needed(self.model.config)\n+            logger.warning(\n+                f\"No behavior specified for use_cuda_graph, defaulting to {self.use_cuda_graph = } because \"\n+                f\"{self.model.config._attn_implementation = }. If you want to save memory, turn off cuda graphs, but \"\n+                \"they can improve performances.\"\n+            )\n+\n+        # If cuda graphs are activated, we set the number of cuda graphs for Q and KV if not specified\n         if self.use_cuda_graph:\n-            raise NotImplementedError(\"Cuda graphs are not supported yet\")\n+            self.num_q_cuda_graphs = num_q_cuda_graphs if num_q_cuda_graphs > 0 else NUM_Q_CUDA_GRAPHS\n+            self.num_kv_cuda_graphs = num_kv_cuda_graphs if num_kv_cuda_graphs > 0 else NUM_KV_CUDA_GRAPHS\n+\n+        if self.log_prob_generation:\n+            raise NotImplementedError(\"log_prob_generation is not supported yet\")\n \n     @traced\n-    def start(self):\n+    def start(self) -> None:\n         \"\"\"Start the background generation thread.\"\"\"\n         if self._generation_thread is not None and self._generation_thread.is_alive():\n             logger.warning(\"Manager thread is already running.\")\n@@ -647,11 +802,11 @@ def start(self):\n         self._generation_thread = threading.Thread(target=self._run_generation_loop)\n         self._generation_thread.start()\n \n-    def is_running(self):\n+    def is_running(self) -> bool:\n         \"\"\"Check if the background generation thread is running.\"\"\"\n         return self._generation_thread is not None and self._generation_thread.is_alive()\n \n-    def stop(self, block: bool = False, timeout: Optional[float] = None):\n+    def stop(self, block: bool = False, timeout: Optional[float] = None) -> None:\n         \"\"\"Signal the background thread to stop.\n \n         Args:\n@@ -669,7 +824,7 @@ def stop(self, block: bool = False, timeout: Optional[float] = None):\n         if block:\n             self.join(timeout)\n \n-    def join(self, timeout: Optional[float] = None):\n+    def join(self, timeout: Optional[float] = None) -> None:\n         \"\"\"Wait for the background thread to finish.\n \n         Args:\n@@ -719,14 +874,13 @@ def add_request(\n \n         # Use block=True with timeout to handle backpressure if queue is full\n         self.input_queue.put(state, block=True, timeout=10)  # XXX: pass timeout as fn arg?\n-        logger.debug(f\"Added request {request_id} to queue.\")\n         return request_id\n \n-    def add_requests(self, inputs: list[list[int]], **kwargs):\n+    def add_requests(self, inputs: list[list[int]], max_new_tokens: Optional[int] = None) -> None:\n         for input_ids in inputs:\n-            self.add_request(input_ids, **kwargs)\n+            self.add_request(input_ids, max_new_tokens=max_new_tokens)\n \n-    def cancel_request(self, request_id: str):\n+    def cancel_request(self, request_id: str) -> None:\n         \"\"\"Cancel a request by its ID.\n \n         Args:\n@@ -735,7 +889,9 @@ def cancel_request(self, request_id: str):\n         if self.batch_processor is not None:\n             self.batch_processor.scheduler.set_request_cancellation(request_id)\n \n-    def get_result(self, request_id=None, timeout=None) -> Optional[GenerationOutput]:\n+    def get_result(\n+        self, request_id: Optional[str] = None, timeout: Optional[float] = None\n+    ) -> Optional[GenerationOutput]:\n         \"\"\"Retrieve one result from the output queue.\n \n         Args:\n@@ -763,7 +919,7 @@ def __iter__(self):\n             if result is not None:\n                 yield result\n \n-    def request_id_iter(self, request_id):\n+    def request_id_iter(self, request_id: str) -> Generator[GenerationOutput]:\n         \"\"\"Iterate over results matching a specific request id as they become available.\"\"\"\n         request_cancelled = False\n         while self._generation_thread is not None and self._generation_thread.is_alive() and not request_cancelled:\n@@ -773,8 +929,16 @@ def request_id_iter(self, request_id):\n             if self.batch_processor is not None:\n                 request_cancelled = self.batch_processor.scheduler.request_is_cancelled(request_id)\n \n+    @staticmethod\n+    def supported_attention_implementations() -> set[str]:\n+        return {\"eager_paged\", \"sdpa_paged\", \"flash_attention_2\"}\n+\n+    @staticmethod\n+    def default_attention_implementation() -> str:\n+        return \"sdpa_paged\"\n+\n     @traced\n-    def warmup(self, batch_processor):\n+    def warmup(self, batch_processor: ContinuousBatchProcessor) -> None:\n         stream = torch.cuda.Stream(device=self.model.device)\n         stream.wait_stream(torch.cuda.current_stream())\n         with torch.cuda.stream(stream):\n@@ -788,67 +952,23 @@ def warmup(self, batch_processor):\n \n     @traced\n     # @torch.compile\n-    def _generation_step(self, batch_processor: ContinuousBatchProcessor):\n+    def _generation_step(self) -> None:\n         \"\"\"Perform a single generation step. This is cuda graphed\"\"\"\n-        batch_data = batch_processor.get_model_kwargs()\n-        with torch.no_grad():\n-            logits = self._model_forward(batch_data)\n-            if self.log_prob_generation:\n-                batch_processor.output_probs.copy_(logits)  # TODO\n-            probs = self._process_logit(batch_data, logits)\n-            self._sample(batch_processor, probs)\n-\n-    @traced(span_name=\"model_forward\")\n-    def _model_forward(self, batch_data):\n-        return self.model(**batch_data).logits\n-\n-    @traced(span_name=\"logit_processing\")\n-    def _process_logit(self, batch_data, logits):\n-        # Pass continuous batching context to logits processor if it supports it. TODO we should find a way to make this a little bit cleaner!\n-        if hasattr(self.logit_processor, \"set_continuous_batching_context\"):\n-            self.logit_processor.set_continuous_batching_context(\n-                batch_data[\"logits_indices\"], batch_data[\"cu_seq_lens_q\"]\n-            )\n-\n-        # Handle shape compatibility: logit processors expect 2D tensors [batch_size, vocab_size]\n-        # but continuous batching always produces 3D tensors [batch_size, seq_len, vocab_size]\n-        batch_size, seq_len, vocab_size = logits.shape\n-        logits_2d = logits.view(batch_size * seq_len, vocab_size)\n-        input_ids_2d = batch_data[\"input_ids\"].view(batch_size * seq_len)\n-\n-        # Process with 2D tensors\n-        processed_logits_2d = self.logit_processor(input_ids_2d, logits_2d)\n+        self.batch_processor._generation_step(self.model, self.logit_processor, self.do_sample)\n \n-        # Reshape back to 3D\n-        return processed_logits_2d.view(batch_size, seq_len, vocab_size)\n-\n-    @traced(span_name=\"sampling\")\n-    def _sample(self, batch_processor: ContinuousBatchProcessor, probs):\n-        if self.do_sample:  # sample\n-            probs = nn.functional.softmax(probs, dim=-1)\n-            # probs[0] has shape [seq_len, vocab_size], multinomial returns [seq_len, 1]\n-            next_tokens = torch.multinomial(probs[0], num_samples=1).squeeze(-1)  # Now [seq_len]\n-            # Add batch dimension back to match argmax output\n-            next_tokens = next_tokens.unsqueeze(0)  # Now [1, seq_len]\n-        else:\n-            next_tokens = torch.argmax(probs, dim=-1)  # Already [1, seq_len]\n-\n-        tokens = next_tokens.size(1)  # Get seq_len dimension\n-        batch_processor.output_ids[:, :tokens].copy_(next_tokens)\n-\n-    def _run_generation_loop(self):\n+    def _run_generation_loop(self) -> None:\n         \"\"\"Main processing loop running in the background thread.\"\"\"\n-        batch_processor = None\n+        batch_processor: Optional[ContinuousBatchProcessor] = None\n         try:\n-            ref_time = perf_counter()\n+            t0 = perf_counter()\n             paged_attention_cache = PagedAttentionCache(\n                 self.model.config,\n                 self.generation_config,\n                 self.model.device,\n                 self.model.dtype,\n                 tp_size=getattr(self.model, \"_tp_size\", None),  # Use model's actual TP setting\n             )\n-            logger.debug(f\"PagedAttentionCache created in {perf_counter() - ref_time} seconds\")\n+            logger.debug(f\"PagedAttentionCache created in {perf_counter() - t0} seconds\")\n \n             scheduler = None\n             if hasattr(self.generation_config, \"scheduler\"):\n@@ -860,23 +980,23 @@ def _run_generation_loop(self):\n                 # Default to fifo\n                 scheduler = FIFOScheduler\n \n-            ref_time = perf_counter()\n+            t1 = perf_counter()\n             batch_processor = ContinuousBatchProcessor(\n-                paged_attention_cache,\n-                self.model.config,\n-                self.generation_config,\n-                self.input_queue,\n-                self.output_queue,\n-                self.stop_event,\n-                self.model.device,\n-                self.model.dtype,\n-                scheduler(paged_attention_cache, self.manual_eviction),\n-                self.manual_eviction,\n-                slice_inputs=self.slice_inputs,\n+                cache=paged_attention_cache,\n+                config=self.model.config,\n+                generation_config=self.generation_config,\n+                input_queue=self.input_queue,\n+                output_queue=self.output_queue,\n+                stop_event=self.stop_event,\n+                model_device=self.model.device,\n+                model_dtype=self.model.dtype,\n+                scheduler=scheduler(paged_attention_cache, self.manual_eviction),\n+                manual_eviction=self.manual_eviction,\n+                use_cuda_graph=self.use_cuda_graph,\n             )\n             self.batch_processor = batch_processor\n             self.current_batch = 0\n-            logger.debug(f\"batch_processor created in {perf_counter() - ref_time} seconds\")\n+            logger.debug(f\"batch_processor created in {perf_counter() - t1} seconds\")\n             while (not self.stop_event.is_set()) or batch_processor.has_pending_requests():\n                 self._inner_generation_loop(batch_processor)\n                 self.current_batch += 1\n@@ -888,38 +1008,27 @@ def _run_generation_loop(self):\n             logger.info(\"Generation loop finished.\")\n \n     @traced(span_name=\"generation_loop\")\n-    def _inner_generation_loop(self, batch_processor: ContinuousBatchProcessor):\n+    def _inner_generation_loop(self, batch_processor: ContinuousBatchProcessor) -> None:\n+        # Pre-loop synchronization\n         if torch.cuda.is_available():\n             torch.cuda.synchronize()\n+        # Loop body ends if there is no requests in the batch\n         if not batch_processor.prepare_next_batch():\n             return\n+        # Debug logging of the current memory usage\n         if logger.level <= logging.DEBUG:\n             device, total, reserved, allocated = get_device_and_memory_breakdown()\n             logger.debug(f\"[Memory] Device: {device}, Total: {total}, Reserved: {reserved}, Allocated: {allocated}\")\n-        if torch.cuda.is_available() and self.use_cuda_graph:\n-            if self.current_batch == 0:\n-                self.warmup(batch_processor)\n-            elif hasattr(self, \"graph\"):\n-                try:\n-                    self._graph_replay()\n-                except Exception as e:\n-                    logger.error(f\"Model forward pass failed: {e}\", exc_info=True)\n-                    batch_processor.handle_batch_error(e)\n-                    return\n-            else:\n-                self._generation_step(batch_processor)\n-        else:\n-            self._generation_step(batch_processor)\n+\n+        self._generation_step()\n+\n         if torch.cuda.is_available():\n             torch.cuda.synchronize()\n+        # Processor updates the batch after generation step is truly over\n         batch_processor.update_batch()\n \n-    @traced(span_name=\"graph_replay\")\n-    def _graph_replay(self):\n-        self.graph.replay()\n-\n     @traced\n-    def _handle_critical_error(self, error, batch_processor: Optional[ContinuousBatchProcessor]):\n+    def _handle_critical_error(self, error: Exception, batch_processor: Optional[ContinuousBatchProcessor]) -> None:\n         \"\"\"Handle critical errors that terminate the generation loop.\"\"\"\n         # Signal stop\n         self.stop_event.set()\n@@ -938,7 +1047,7 @@ def _handle_critical_error(self, error, batch_processor: Optional[ContinuousBatc\n             batch_processor.fail_all_requests(error)\n \n     @traced\n-    def evict_request_from_cache(self, request_id: str):\n+    def evict_request_from_cache(self, request_id: str) -> None:\n         \"\"\"Evict a request from the cache. It is assumed that the request is already finished.\"\"\"\n         if not self.manual_eviction:\n             raise RuntimeError(\"Manual eviction is not enabled for this manager.\")\n@@ -954,13 +1063,17 @@ def init_continuous_batching(\n         generation_config: Optional[GenerationConfig] = None,\n         manual_eviction: bool = False,\n         max_queue_size: int = 0,\n-        slice_inputs: bool = True,\n+        num_q_cuda_graphs: int = 0,\n+        num_kv_cuda_graphs: int = 0,\n     ) -> ContinuousBatchingManager:\n         \"\"\"Initialize a manager for continuous batching inference.\n \n         Args:\n             generation_config: Custom generation configuration\n+            manual_eviction: Whether to manually evict requests from the cache\n             max_queue_size: Maximum size of the input request queue\n+            num_q_cuda_graphs: Number of CUDA graphs to use for the query dimension\n+            num_kv_cuda_graphs: Number of CUDA graphs to use for the keys/values dimension\n \n         Returns:\n             `ContinuousBatchingManager`: The manager instance to add requests and retrieve results.\n@@ -982,7 +1095,8 @@ def init_continuous_batching(\n             generation_config=gen_config,\n             manual_eviction=manual_eviction,\n             max_queue_size=max_queue_size,\n-            slice_inputs=slice_inputs,\n+            num_q_cuda_graphs=num_q_cuda_graphs,\n+            num_kv_cuda_graphs=num_kv_cuda_graphs,\n         )\n \n     @traced\n@@ -992,14 +1106,17 @@ def generate_batch(\n         inputs: list[list[int]],\n         generation_config: Optional[GenerationConfig] = None,\n         progress_bar: bool = True,\n-        slice_inputs: bool = True,\n+        num_q_cuda_graphs: int = 0,\n+        num_kv_cuda_graphs: int = 0,\n         **kwargs,\n-    ) -> list[list[int]]:\n+    ) -> dict[str, GenerationOutput]:\n         \"\"\"Generate sequences for a batch of prompts using continuous batching.\n \n         Args:\n             inputs: List of input token sequences (prompts)\n             generation_config: Optional generation configuration\n+            num_q_cuda_graphs: Number of CUDA graphs to use for the query dimension\n+            num_kv_cuda_graphs: Number of CUDA graphs to use for the keys/values dimension\n             **kwargs: Additional generation parameters\n \n         Returns:\n@@ -1008,13 +1125,17 @@ def generate_batch(\n                                 Returns an empty list `[]` for requests that failed.\n         \"\"\"\n         if not inputs:\n-            return []\n+            return {}\n         if logger.getEffectiveLevel() <= logging.DEBUG:\n             logger.warning(\"Progress bar is disabled when logger level is less than DEBUG\")\n             progress_bar = False\n \n         # Initialize manager with the batch inputs\n-        manager = self.init_continuous_batching(generation_config=generation_config, slice_inputs=slice_inputs)\n+        manager = self.init_continuous_batching(\n+            generation_config=generation_config,\n+            num_q_cuda_graphs=num_q_cuda_graphs,\n+            num_kv_cuda_graphs=num_kv_cuda_graphs,\n+        )\n         manager.start()\n         results = {}\n         num_requests = len(inputs)\n@@ -1028,7 +1149,7 @@ def generate_batch(\n                     desc=f\"Solving {num_requests} requests\",\n                     unit=\"request\",\n                 ) as pbar:\n-                    manager.add_requests(inputs, **kwargs)\n+                    manager.add_requests(inputs=inputs, max_new_tokens=kwargs.get(\"max_new_tokens\"))\n                     finished_count = 0\n                     while finished_count < num_requests:\n                         result = manager.get_result(timeout=1)"
      },
      {
        "filename": "src/transformers/generation/continuous_batching/requests.py",
        "status": "modified",
        "additions": 0,
        "deletions": 1,
        "changes": 1,
        "patch": "@@ -25,7 +25,6 @@\n \n # We centralize the logger here to coordinate between logging and progress bar\n logger = logging.getLogger(\"ContinuousBatchingLogger\")\n-# logger.setLevel(logging.INFO)\n \n \n @staticmethod"
      },
      {
        "filename": "src/transformers/integrations/eager_paged.py",
        "status": "modified",
        "additions": 10,
        "deletions": 2,
        "changes": 12,
        "patch": "@@ -3,6 +3,8 @@\n import torch\n from torch import nn\n \n+from ..generation.continuous_batching.cache import PagedAttentionCache\n+\n \n def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     \"\"\"\n@@ -26,10 +28,16 @@ def eager_paged_attention_forward(\n     **kwargs,\n ):\n     # Add KV cache to the key and value tensors\n-    cache = kwargs.pop(\"cache\", None)\n+    cache: Optional[PagedAttentionCache] = kwargs.pop(\"cache\", None)\n     if cache is not None:\n         # This changes the shape of k and v from [1, num_kv_heads, seqlen_kv, head_dim] to [-1, num_kv_heads, head_dim]\n-        key, value = cache.update(key, value, module.layer_idx, **kwargs)\n+        key, value = cache.update(\n+            key_states=key,\n+            value_states=value,\n+            layer_idx=module.layer_idx,\n+            read_index=kwargs[\"read_index\"],\n+            write_index=kwargs[\"write_index\"],\n+        )\n         key = key.transpose(0, 1).unsqueeze(0)\n         value = value.transpose(0, 1).unsqueeze(0)\n "
      },
      {
        "filename": "src/transformers/integrations/flash_paged.py",
        "status": "modified",
        "additions": 7,
        "deletions": 1,
        "changes": 8,
        "patch": "@@ -64,7 +64,13 @@ def paged_attention_forward(\n \n     # .update changes the shape of k and v from [1, num_kv_heads, seqlen_kv, head_dim] to [-1, num_kv_heads, head_dim]\n     if cache is not None:\n-        k, v = cache.update(k, v, module.layer_idx, **kwargs)\n+        k, v = cache.update(\n+            key_states=k,\n+            value_states=v,\n+            layer_idx=module.layer_idx,\n+            read_index=kwargs[\"read_index\"],\n+            write_index=kwargs[\"write_index\"],\n+        )\n \n     # Retrieve the cumulative sequence lengths for the current layer\n     if isinstance(cu_seq_lens_k, dict):"
      },
      {
        "filename": "src/transformers/integrations/sdpa_paged.py",
        "status": "modified",
        "additions": 10,
        "deletions": 2,
        "changes": 12,
        "patch": "@@ -2,6 +2,8 @@\n \n import torch\n \n+from ..generation.continuous_batching.cache import PagedAttentionCache\n+\n \n def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     \"\"\"\n@@ -26,10 +28,16 @@ def sdpa_attention_paged_forward(\n     **kwargs,\n ) -> tuple[torch.Tensor, None]:\n     # Add KV cache to the key and value tensors\n-    cache = kwargs.pop(\"cache\", None)\n+    cache: Optional[PagedAttentionCache] = kwargs.pop(\"cache\", None)\n     if cache is not None:\n         # This changes the shape of k and v from [1, num_kv_heads, seqlen_kv, head_dim] to [-1, num_kv_heads, head_dim]\n-        key, value = cache.update(key, value, module.layer_idx, **kwargs)\n+        key, value = cache.update(\n+            key_states=key,\n+            value_states=value,\n+            layer_idx=module.layer_idx,\n+            read_index=kwargs[\"read_index\"],\n+            write_index=kwargs[\"write_index\"],\n+        )\n         key = key.transpose(0, 1).unsqueeze(0)\n         value = value.transpose(0, 1).unsqueeze(0)\n "
      }
    ],
    "num_files": 8,
    "scraped_at": "2025-11-16T21:18:06.979739",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR contains substantial architectural changes to continuous batching with CUDA graphs support, involving non-trivial logic for tensor slicing, padding strategies, and cache management. The PR description provides meaningful context about the key changes (token generation moved to CB processor, automatic tensor slicing, CUDA graph capture on padded shapes), and the code modifications involve real algorithmic decisions that a developer would need to understand to work on generation and performance optimization features.",
      "substance_level": "high"
    }
  },
  {
    "pr_number": 41415,
    "title": "Fix bnb fsdp loading for pre-quantized checkpoint",
    "body": "# What does this PR do?\r\n\r\nThis PR fixes bnb loading when using FSDP for pre-quantized checkpoints. This happened because we changed how we load quantized checkpoints as we need to cache all the quantized stats before creating the quantized weight. ",
    "html_url": "https://github.com/huggingface/transformers/pull/41415",
    "created_at": "2025-10-07T15:30:31Z",
    "merged_at": "2025-10-09T16:05:35Z",
    "merge_commit_sha": "823fab4860ec7a5c71d8a21f834104c6deedfaa4",
    "base_ref": "main",
    "head_sha": "3b453005e9f4dd52fcd189006cff875e2789b0e4",
    "user": "SunMarc",
    "files": [
      {
        "filename": "src/transformers/modeling_utils.py",
        "status": "modified",
        "additions": 8,
        "deletions": 12,
        "changes": 20,
        "patch": "@@ -763,21 +763,17 @@ def _load_state_dict_into_meta_model(\n                 # and then cast it to CPU to avoid excessive memory usage on each GPU\n                 # in comparison to the sharded model across GPUs.\n                 if is_fsdp_enabled() or is_deepspeed_zero3_enabled():\n-                    param_name = hf_quantizer.update_param_name(param_name)\n+                    param_name = hf_quantizer.get_param_name(param_name)\n                     module, param_type = get_module_from_name(model, param_name)\n                     value = getattr(module, param_type)\n-                    # special case for gpt_oss model, we wait for the param to be leave the meta device before casting it to cpu\n-                    if model.config.model_type == \"gpt_oss\" and value.device.type == \"meta\":\n+                    # We need to wait until the quantized value is created\n+                    if value.device.type == \"meta\":\n                         continue\n-                    param_to = \"cpu\"\n-                    if is_fsdp_enabled() and not is_local_dist_rank_0():\n-                        param_to = \"meta\"\n-                    val_kwargs = {}\n-                    if (hasattr(module, \"weight\") and module.weight.__class__.__name__ == \"Int8Params\") or (\n-                        value.dtype == torch.uint8 or value.dtype == torch.int8\n-                    ):\n+                    val_kwargs = value.__dict__\n+                    if not value.is_floating_point():\n                         val_kwargs[\"requires_grad\"] = False\n-                    value = type(value)(value.data.to(param_to), **val_kwargs, **value.__dict__)\n+                    device = \"meta\" if is_fsdp_enabled() and not is_local_dist_rank_0() else \"cpu\"\n+                    value = type(value)(value.data.to(device), **val_kwargs)\n                     setattr(module, param_type, value)\n \n         # Remove the param from the state dict if it was not loaded on the fly to avoid wasting memory\n@@ -5822,7 +5818,7 @@ def caching_allocator_warmup(model: PreTrainedModel, expanded_device_map: dict,\n         # For example in the case of MXFP4 quantization, we need to update the param name to the original param name\n         # because the checkpoint contains blocks, and scales, but since we are dequantizing, we need to use the original param name\n         if hf_quantizer is not None:\n-            param_name = hf_quantizer.update_param_name(param_name)\n+            param_name = hf_quantizer.get_param_name(param_name)\n \n         try:\n             param = model.get_parameter_or_buffer(param_name)"
      },
      {
        "filename": "src/transformers/quantizers/base.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -283,7 +283,7 @@ def _dequantize(self, model):\n             f\"{self.quantization_config.quant_method} has no implementation of `dequantize`, please raise an issue on GitHub.\"\n         )\n \n-    def update_param_name(self, param_name: str) -> str:\n+    def get_param_name(self, param_name: str) -> str:\n         \"\"\"\n         Override this method if you want to adjust the `param_name`.\n         \"\"\""
      },
      {
        "filename": "src/transformers/quantizers/quantizer_bnb_4bit.py",
        "status": "modified",
        "additions": 16,
        "deletions": 5,
        "changes": 21,
        "patch": "@@ -154,6 +154,19 @@ def param_needs_quantization(self, model: \"PreTrainedModel\", param_name: str, **\n         module, name = get_module_from_name(model, param_name)\n         return isinstance(module, bnb.nn.Linear4bit) and name != \"bias\"\n \n+    def get_param_name(self, param_name: str) -> str:\n+        \"\"\"\n+        Get the right param_name in order to get the module associated with the param.\n+        This is useful for quantized stats lile absmax or quant_map as we need to update the param_name to get the module as they are stored in ...weight.absmax.\n+        \"\"\"\n+        if self.pre_quantized:\n+            # We need to get the param name of quantized weights and not its components. Otherwise, we won't be able to get the nn.Module associated.\n+            if any(param_name.endswith(x) for x in self.bnb_keys):\n+                param_name = (\n+                    param_name.rsplit(\".\", 1)[0] if \"quant_state.\" not in param_name else param_name.rsplit(\".\", 2)[0]\n+                )\n+        return param_name\n+\n     def create_quantized_param(\n         self,\n         model: \"PreTrainedModel\",\n@@ -164,12 +177,10 @@ def create_quantized_param(\n     ):\n         import bitsandbytes as bnb\n \n-        is_quant_stat = any(param_name.endswith(x) for x in self.bnb_keys)\n         full_name = param_name\n-        if is_quant_stat:\n-            param_name = (\n-                param_name.rsplit(\".\", 1)[0] if \"quant_state.\" not in param_name else param_name.rsplit(\".\", 2)[0]\n-            )\n+\n+        # update param name to get the weights instead of the quantized stats\n+        param_name = self.get_param_name(param_name)\n         module, tensor_name = get_module_from_name(model, param_name)\n \n         # `torch.Tensor.to(<int num>)` is not supported by `torch_npu` (see this [issue](https://github.com/Ascend/pytorch/issues/16))."
      },
      {
        "filename": "src/transformers/quantizers/quantizer_mxfp4.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -365,7 +365,7 @@ def update_ep_plan(self, config):\n                 )\n         return config\n \n-    def update_param_name(self, param_name: str) -> str:\n+    def get_param_name(self, param_name: str) -> str:\n         if self.quantization_config.dequantize:\n             if \"_blocks\" in param_name:\n                 return param_name.replace(\"_blocks\", \"\")"
      },
      {
        "filename": "tests/quantization/mxfp4/test_mxfp4.py",
        "status": "modified",
        "additions": 6,
        "deletions": 6,
        "changes": 12,
        "patch": "@@ -265,7 +265,7 @@ def test_update_expected_keys(self):\n \n         self.assertEqual(set(updated_keys), set(expected_updated))\n \n-    def test_update_param_name_dequantize(self):\n+    def test_get_param_name_dequantize(self):\n         \"\"\"Test parameter name updating when dequantizing\"\"\"\n         from transformers.quantizers.quantizer_mxfp4 import Mxfp4HfQuantizer\n \n@@ -274,28 +274,28 @@ def test_update_param_name_dequantize(self):\n \n         # Should remove _blocks suffix\n         param_name = \"model.layers.0.mlp.experts.gate_up_proj_blocks\"\n-        updated_name = quantizer.update_param_name(param_name)\n+        updated_name = quantizer.get_param_name(param_name)\n         self.assertEqual(updated_name, \"model.layers.0.mlp.experts.gate_up_proj\")\n \n         # Should remove _scales suffix\n         param_name = \"model.layers.0.mlp.experts.down_proj_scales\"\n-        updated_name = quantizer.update_param_name(param_name)\n+        updated_name = quantizer.get_param_name(param_name)\n         self.assertEqual(updated_name, \"model.layers.0.mlp.experts.down_proj\")\n \n         # Should not change other names\n         param_name = \"model.embed_tokens.weight\"\n-        updated_name = quantizer.update_param_name(param_name)\n+        updated_name = quantizer.get_param_name(param_name)\n         self.assertEqual(updated_name, \"model.embed_tokens.weight\")\n \n-    def test_update_param_name_no_dequantize(self):\n+    def test_get_param_name_no_dequantize(self):\n         \"\"\"Test parameter name updating when not dequantizing\"\"\"\n         from transformers.quantizers.quantizer_mxfp4 import Mxfp4HfQuantizer\n \n         config = Mxfp4Config(dequantize=False)\n         quantizer = Mxfp4HfQuantizer(config)\n \n         param_name = \"model.layers.0.mlp.experts.gate_up_proj_blocks\"\n-        updated_name = quantizer.update_param_name(param_name)\n+        updated_name = quantizer.get_param_name(param_name)\n         self.assertEqual(updated_name, param_name)\n \n     def test_is_trainable(self):"
      }
    ],
    "num_files": 5,
    "scraped_at": "2025-11-16T21:18:08.510381",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR contains non-trivial logic changes fixing a bug in BNB FSDP loading for pre-quantized checkpoints. It involves understanding quantization state management, parameter name resolution in hierarchical models, and device placement logic across distributed training scenarios\u2014all substantive architectural concerns that would help developers understand the codebase's quantization and distributed loading mechanisms.",
      "substance_level": "high"
    }
  },
  {
    "pr_number": 41408,
    "title": "Benchmark overhaul",
    "body": "This PR overhauls the benchmarking suite that is included in transformers. \r\nThe benchmarking suite is now based around three main components:\r\n\r\n- `BenchmarkingConfig` is a dataclass-like object which contains everything needed to reproduce a benchmark on the same machine: input length, generation length, whether to use `kernels` or `compile`, attention implementation, etc. (subject to name change)\r\n- `BenchmarkRunner` is the class that runs the benchmarks defined by the configs, with a given number of measurement iterations, warmup iterations, and a model-id. The runner takes care of setting up the runs in a way that ensures no run interacts with the downstream ones: the model is reloaded, the cache is emptied and the GPU memory is flushed. It also saves the results, the config, and any additional metadata needed to reproduce the benchmark, like hardware information and package versions. \r\n- The created results files, which contain enough informations to induces (to my knowledge) most of the metrics used to evaluate a model: e2e_atency, tpot, ttft, even inter-token latency. Results also include a sample of what has been generated, which is useful to check if it was gibberish. The results files are in json format and are made to be easily created from the dataclass-like objects and vice versa. \r\n\r\nFor now, the new benchmarking suite replaces the `benchmark_v2` part of `transformers` but it could also overwrite the `benchmark` (v1) part. It would be good to make that decision in this PR. And update the CI workflows that rely on the current `benchmark_v2` (putting the PR in draft mode until then).\r\nAn example of how to use the new benchmarking suite can be found in `run_benchmarks.py`.\r\n\r\nThe format of the results file can (and may be bound to) change as we develop tools to analyze them. \r\nIf there is a metric you want to see measured in `transformers`, please leave a comment before this is merged :slightly_smiling_face: ",
    "html_url": "https://github.com/huggingface/transformers/pull/41408",
    "created_at": "2025-10-07T13:10:05Z",
    "merged_at": "2025-10-14T19:41:43Z",
    "merge_commit_sha": "94df0e65602922be2831b3faa457a2bde78b936b",
    "base_ref": "main",
    "head_sha": "400a6165037079decfa1b3710f9f4031c38bd6ca",
    "user": "remi-or",
    "files": [
      {
        "filename": ".github/workflows/benchmark.yml",
        "status": "modified",
        "additions": 1,
        "deletions": 4,
        "changes": 5,
        "patch": "@@ -1,10 +1,7 @@\n name: Self-hosted runner (benchmark)\r\n \r\n on:\r\n-  push:\r\n-    branches: [main]\r\n-  pull_request:\r\n-    types: [ opened, labeled, reopened, synchronize ]\r\n+  workflow_dispatch:\r\n \r\n concurrency:\r\n   group: ${{ github.workflow }}-${{ github.head_ref || github.run_id }}\r"
      },
      {
        "filename": ".github/workflows/benchmark_v2.yml",
        "status": "modified",
        "additions": 2,
        "deletions": 30,
        "changes": 32,
        "patch": "@@ -1,35 +1,7 @@\n name: Benchmark v2 Framework\n \n on:\n-  workflow_call:\n-    inputs:\n-      runner:\n-        description: 'GH Actions runner group to use'\n-        required: true\n-        type: string\n-      container_image:\n-        description: 'Docker image to use'\n-        required: true\n-        type: string\n-      container_options:\n-        description: 'Container options to use'\n-        required: true\n-        type: string\n-      commit_sha:\n-        description: 'Commit SHA to benchmark'\n-        required: false\n-        type: string\n-        default: ''\n-      run_id:\n-        description: 'Custom run ID for organizing results (auto-generated if not provided)'\n-        required: false\n-        type: string\n-        default: ''\n-      benchmark_repo_id:\n-        description: 'HuggingFace Dataset to upload results to (e.g., \"org/benchmark-results\")'\n-        required: false\n-        type: string\n-        default: ''\n+  workflow_dispatch:\n \n env:\n   HF_HOME: /mnt/cache\n@@ -82,4 +54,4 @@ jobs:\n           --token '${{ secrets.TRANSFORMERS_CI_RESULTS_UPLOAD_TOKEN }}' \\\n           --log-level INFO\n         env:\n-          HF_TOKEN: ${{ secrets.HF_HUB_READ_TOKEN }}\n\\ No newline at end of file\n+          HF_TOKEN: ${{ secrets.HF_HUB_READ_TOKEN }}"
      },
      {
        "filename": ".github/workflows/benchmark_v2_a10_caller.yml",
        "status": "modified",
        "additions": 2,
        "deletions": 6,
        "changes": 8,
        "patch": "@@ -1,11 +1,7 @@\n name: Benchmark v2 Scheduled Runner - A10 Single-GPU\n \n on:\n-  schedule:\n-    # Run daily at 16:30 UTC\n-    - cron: \"30 16 * * *\"\n-  pull_request:\n-    types: [ opened, labeled, reopened, synchronize ]\n+  workflow_dispatch:\n \n jobs:\n   benchmark-v2-default:\n@@ -18,4 +14,4 @@ jobs:\n       commit_sha: ${{ github.sha }}\n       run_id: ${{ github.run_id }}\n       benchmark_repo_id: hf-internal-testing/transformers-daily-benchmarks\n-    secrets: inherit\n\\ No newline at end of file\n+    secrets: inherit"
      },
      {
        "filename": ".github/workflows/benchmark_v2_mi325_caller.yml",
        "status": "modified",
        "additions": 2,
        "deletions": 6,
        "changes": 8,
        "patch": "@@ -1,11 +1,7 @@\n name: Benchmark v2 Scheduled Runner - MI325 Single-GPU\n \n on:\n-  schedule:\n-    # Run daily at 16:30 UTC\n-    - cron: \"30 16 * * *\"\n-  pull_request:\n-    types: [ opened, labeled, reopened, synchronize ]\n+  workflow_dispatch:\n \n jobs:\n   benchmark-v2-default:\n@@ -18,4 +14,4 @@ jobs:\n       commit_sha: ${{ github.sha }}\n       run_id: ${{ github.run_id }}\n       benchmark_repo_id: hf-internal-testing/transformers-daily-benchmarks\n-    secrets: inherit\n\\ No newline at end of file\n+    secrets: inherit"
      },
      {
        "filename": "benchmark_v2/.gitignore",
        "status": "modified",
        "additions": 2,
        "deletions": 1,
        "changes": 3,
        "patch": "@@ -1 +1,2 @@\n-benchmark_results/\n\\ No newline at end of file\n+benchmark_results/\n+benchmark_results_profiles/"
      },
      {
        "filename": "benchmark_v2/benches/__init__.py",
        "status": "removed",
        "additions": 0,
        "deletions": 1,
        "changes": 1,
        "patch": "@@ -1 +0,0 @@\n-# Benchmark implementations directory"
      },
      {
        "filename": "benchmark_v2/benches/llama.py",
        "status": "removed",
        "additions": 0,
        "deletions": 165,
        "changes": 165,
        "patch": "@@ -1,165 +0,0 @@\n-# Copyright 2025 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-import logging\n-import os\n-from typing import Any\n-\n-import torch\n-from benchmark_framework import ModelBenchmark\n-\n-\n-os.environ[\"TOKENIZERS_PARALLELISM\"] = \"1\"\n-torch.set_float32_matmul_precision(\"high\")\n-\n-\n-class LLaMABenchmark(ModelBenchmark):\n-    \"\"\"Simplified LLaMA model benchmark implementation using the ModelBenchmark base class.\"\"\"\n-\n-    def __init__(self, logger: logging.Logger):\n-        super().__init__(logger)\n-        self._default_prompt = \"Why dogs are so cute?\"  # Custom prompt for LLaMA\n-\n-    def get_scenario_configs(self) -> list[dict[str, Any]]:\n-        \"\"\"\n-        Get LLaMA-specific scenario configurations.\n-\n-        Returns:\n-            List of scenario configuration dictionaries\n-        \"\"\"\n-        return [\n-            # Eager variants\n-            {\"variant\": \"eager\", \"compile_mode\": None, \"use_cache\": True, \"description\": \"Eager execution with cache\"},\n-            # Compiled variants\n-            {\n-                \"variant\": \"compiled\",\n-                \"compile_mode\": \"max-autotune\",\n-                \"use_cache\": True,\n-                \"description\": \"Compiled with max autotune\",\n-            },\n-            # Kernelized variant (if available)\n-            {\n-                \"variant\": \"kernelized\",\n-                \"compile_mode\": \"max-autotune\",\n-                \"use_cache\": True,\n-                \"description\": \"Kernelized execution\",\n-            },\n-        ]\n-\n-    def _is_kernelization_available(self) -> bool:\n-        \"\"\"Check if kernelization is available for LLaMA.\"\"\"\n-        try:\n-            from kernels import Mode, kernelize  # noqa: F401\n-\n-            return True\n-        except ImportError:\n-            self.logger.debug(\"Kernelization not available: kernels module not found\")\n-            return False\n-\n-    def get_default_generation_config(self) -> dict[str, Any]:\n-        \"\"\"Get LLaMA-specific generation configuration.\"\"\"\n-        return {\n-            \"do_sample\": False,\n-            \"top_p\": 1.0,\n-            \"temperature\": 1.0,\n-            \"repetition_penalty\": 1.0,\n-            \"max_new_tokens\": None,  # Will be set per scenario\n-        }\n-\n-    def get_model_init_kwargs(self, config) -> dict[str, Any]:\n-        \"\"\"Get LLaMA-specific model initialization kwargs.\"\"\"\n-        return {\n-            \"torch_dtype\": getattr(torch, config.torch_dtype),\n-            \"attn_implementation\": config.attn_implementation,\n-            \"use_cache\": True,\n-        }\n-\n-    def get_default_torch_dtype(self) -> str:\n-        \"\"\"Get default torch dtype for LLaMA.\"\"\"\n-        return \"float16\"  # LLaMA works well with float16\n-\n-    def get_default_device(self) -> str:\n-        \"\"\"Get default device for LLaMA.\"\"\"\n-        return \"cuda\"  # LLaMA prefers CUDA\n-\n-\n-def run_llama(logger, output_dir, **kwargs):\n-    \"\"\"\n-    Run LLaMA benchmark with the given configuration.\n-\n-    Args:\n-        logger: Logger instance\n-        output_dir: Output directory for results\n-        **kwargs: Additional configuration options\n-\n-    Returns:\n-        Path to output file if successful\n-    \"\"\"\n-    from benchmark_framework import BenchmarkRunner\n-\n-    # Extract parameters with defaults\n-    model_id = kwargs.get(\"model_id\", \"meta-llama/Llama-2-7b-hf\")\n-    warmup_iterations = kwargs.get(\"warmup_iterations\", 3)\n-    measurement_iterations = kwargs.get(\"measurement_iterations\", 5)\n-    num_tokens_to_generate = kwargs.get(\"num_tokens_to_generate\", 100)\n-    include_sdpa_variants = kwargs.get(\"include_sdpa_variants\", True)\n-    device = kwargs.get(\"device\", \"cuda\")\n-    torch_dtype = kwargs.get(\"torch_dtype\", \"float16\")\n-    batch_size = kwargs.get(\"batch_size\", 1)\n-    commit_id = kwargs.get(\"commit_id\")\n-\n-    logger.info(f\"Starting LLaMA benchmark for model: {model_id}\")\n-    logger.info(\n-        f\"Configuration: warmup={warmup_iterations}, measurement={measurement_iterations}, tokens={num_tokens_to_generate}\"\n-    )\n-\n-    try:\n-        # Create benchmark instance\n-        benchmark = LLaMABenchmark(logger)\n-\n-        # Create scenarios\n-        scenarios = benchmark.create_scenarios(\n-            model_id=model_id,\n-            warmup_iterations=warmup_iterations,\n-            measurement_iterations=measurement_iterations,\n-            num_tokens_to_generate=num_tokens_to_generate,\n-            include_sdpa_variants=include_sdpa_variants,\n-            device=device,\n-            torch_dtype=torch_dtype,\n-            batch_size=batch_size,\n-        )\n-\n-        logger.info(f\"Created {len(scenarios)} benchmark scenarios\")\n-\n-        # Create runner and execute benchmarks\n-        runner = BenchmarkRunner(logger, output_dir)\n-        results = runner.run_benchmark(benchmark, scenarios, commit_id=commit_id)\n-\n-        if not results:\n-            logger.warning(\"No successful benchmark results\")\n-            return None\n-\n-        # Save results\n-        model_name = model_id.split(\"/\")[-1]  # Extract model name from ID\n-        output_file = runner.save_results(model_name, results)\n-\n-        logger.info(f\"LLaMA benchmark completed successfully. Results saved to: {output_file}\")\n-        return output_file\n-\n-    except Exception as e:\n-        logger.error(f\"LLaMA benchmark failed: {e}\")\n-        import traceback\n-\n-        logger.debug(traceback.format_exc())\n-        raise"
      },
      {
        "filename": "benchmark_v2/benchmark_framework.py",
        "status": "removed",
        "additions": 0,
        "deletions": 1199,
        "changes": 1199,
        "patch": "@@ -1,1199 +0,0 @@\n-# Copyright 2025 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-import gc\n-import json\n-import logging\n-import os\n-import statistics\n-import sys\n-import threading\n-import time\n-from abc import ABC, abstractmethod\n-from dataclasses import asdict, dataclass, field\n-from datetime import datetime\n-from typing import Any, Optional, TypedDict, Union\n-\n-import gpustat\n-import numpy as np\n-import psutil\n-import torch\n-\n-\n-class GPUMetrics(TypedDict):\n-    \"\"\"GPU monitoring result with GPU metrics.\"\"\"\n-\n-    gpu_utilization_mean: float\n-    gpu_utilization_max: float\n-    gpu_utilization_min: float\n-    gpu_memory_used_mean: float\n-    gpu_memory_used_max: float\n-    gpu_memory_used_min: float\n-    sample_count: int\n-    gpu_monitoring_status: str\n-\n-\n-class NoGPU(TypedDict):\n-    \"\"\"GPU monitoring result without GPU metrics.\"\"\"\n-\n-    gpu_monitoring_status: str\n-    gpu_monitoring_reason: str\n-\n-\n-class ArchAwareTimer:\n-    \"\"\"Architecture-aware timer for supposedly better prescision\"\"\"\n-\n-    def __init__(self, device: Optional[str] = None):\n-        \"\"\"\n-        Initialize architecture-aware timer.\n-\n-        Args:\n-            device: Device to use. If None, uses current device.\n-        \"\"\"\n-        self.device = device\n-        self.use_cuda = torch.cuda.is_available()\n-\n-        if self.use_cuda:\n-            if device and device != \"cpu\":\n-                self.device_obj = torch.device(device)\n-            else:\n-                # Fall back to CPU timing if device is CPU or CUDA not available\n-                self.use_cuda = False\n-\n-        if self.use_cuda:\n-            try:\n-                # Create CUDA events for timing\n-                self.start_event = torch.cuda.Event(enable_timing=True)\n-                self.end_event = torch.cuda.Event(enable_timing=True)\n-            except RuntimeError:\n-                # Fall back to CPU timing if CUDA events fail\n-                self.use_cuda = False\n-\n-        if not self.use_cuda:\n-            self.start_time = None\n-            self.end_time = None\n-\n-    def start(self):\n-        \"\"\"Start timing.\"\"\"\n-        if self.use_cuda:\n-            torch.cuda.synchronize(self.device_obj)\n-            self.start_event.record(stream=torch.cuda.current_stream(self.device_obj))\n-        else:\n-            self.start_time = time.perf_counter()\n-\n-    def stop(self):\n-        \"\"\"Stop timing.\"\"\"\n-        if self.use_cuda:\n-            self.end_event.record(stream=torch.cuda.current_stream(self.device_obj))\n-            torch.cuda.synchronize(self.device_obj)\n-        else:\n-            self.end_time = time.perf_counter()\n-\n-    def elapsed_time(self) -> float:\n-        \"\"\"\n-        Get elapsed time in seconds.\n-\n-        Returns:\n-            Elapsed time in seconds\n-        \"\"\"\n-        if self.use_cuda:\n-            # CUDA events return time in milliseconds, convert to seconds\n-            return self.start_event.elapsed_time(self.end_event) / 1000.0\n-        else:\n-            if self.start_time is None or self.end_time is None:\n-                raise RuntimeError(\"Timer not properly started/stopped\")\n-            return self.end_time - self.start_time\n-\n-    @property\n-    def timing_method(self) -> str:\n-        \"\"\"Get the timing method being used.\"\"\"\n-        return \"CUDA Events\" if self.use_cuda else \"CPU perf_counter\"\n-\n-    def __enter__(self):\n-        \"\"\"Context manager entry.\"\"\"\n-        self.start()\n-        return self\n-\n-    def __exit__(self, exc_type, exc_val, exc_tb):\n-        \"\"\"Context manager exit.\"\"\"\n-        self.stop()\n-\n-\n-@dataclass\n-class BenchmarkConfig:\n-    \"\"\"Configuration for a single benchmark scenario.\"\"\"\n-\n-    name: str\n-    model_id: str\n-    variant: str = \"eager\"  # \"eager\", \"compiled\", \"kernelized\"\n-    warmup_iterations: int = 3\n-    measurement_iterations: int = 10\n-    num_tokens_to_generate: int = 100\n-    device: str = \"cuda\"\n-    torch_dtype: str = \"float16\"\n-    compile_mode: Optional[str] = None  # None, \"default\", \"reduce-overhead\", \"max-autotune\"\n-    compile_options: dict[str, Any] = field(default_factory=dict)\n-    use_cache: bool = True\n-    batch_size: int = 1\n-    sequence_length: Optional[int] = None\n-    attn_implementation: str = \"sdpa\"  # \"eager\", \"sdpa\", \"flash_attention_2\"\n-    sdpa_backend: Optional[str] = None  # None, \"math\", \"flash_attention\", \"efficient_attention\", \"cudnn_attention\"\n-    custom_params: dict[str, Any] = field(default_factory=dict)\n-\n-\n-class BenchmarkScenario:\n-    \"\"\"\n-    A benchmark scenario that encapsulates both configuration and setup logic.\n-    This makes it easier to define and adapt benchmarks for different models.\n-    \"\"\"\n-\n-    def __init__(self, name: str, config: BenchmarkConfig, description: str = \"\"):\n-        self.name = name\n-        self.config = config\n-        self.description = description\n-        self._setup_callbacks = []\n-        self._teardown_callbacks = []\n-\n-    def add_setup_callback(self, callback: callable):\n-        \"\"\"Add a callback to be executed during scenario setup.\"\"\"\n-        self._setup_callbacks.append(callback)\n-\n-    def add_teardown_callback(self, callback: callable):\n-        \"\"\"Add a callback to be executed during scenario teardown.\"\"\"\n-        self._teardown_callbacks.append(callback)\n-\n-    def setup(self, model, tokenizer, logger=None):\n-        \"\"\"Execute setup callbacks for this scenario.\"\"\"\n-        for callback in self._setup_callbacks:\n-            try:\n-                callback(model, tokenizer, self.config, logger)\n-            except Exception as e:\n-                if logger:\n-                    logger.warning(f\"Setup callback failed for scenario {self.name}: {e}\")\n-\n-    def teardown(self, model, tokenizer, logger=None):\n-        \"\"\"Execute teardown callbacks for this scenario.\"\"\"\n-        for callback in self._teardown_callbacks:\n-            try:\n-                callback(model, tokenizer, self.config, logger)\n-            except Exception as e:\n-                if logger:\n-                    logger.warning(f\"Teardown callback failed for scenario {self.name}: {e}\")\n-\n-    def __repr__(self):\n-        return f\"BenchmarkScenario(name='{self.name}', variant='{self.config.variant}')\"\n-\n-\n-@dataclass\n-class TimingResult:\n-    \"\"\"Result from a timing measurement.\"\"\"\n-\n-    time_to_first_token_seconds: Optional[float] = None\n-    latency_seconds: float = 0.0\n-    tokens_per_second: Optional[float] = None\n-    time_per_output_token_seconds: Optional[float] = None\n-    total_tokens_generated: int = 0\n-    metadata: dict[str, Any] = field(default_factory=dict)\n-\n-\n-@dataclass\n-class BenchmarkStatistics:\n-    \"\"\"Statistical analysis of benchmark measurements.\"\"\"\n-\n-    name: str\n-    measurements: list[float]\n-    mean: float\n-    median: float\n-    std: float\n-    min: float\n-    max: float\n-    p25: float  # 25th percentile\n-    p75: float  # 75th percentile\n-    p90: float  # 90th percentile\n-    p95: float  # 95th percentile\n-    p99: float  # 99th percentile\n-    unit: str = \"seconds\"\n-\n-    @classmethod\n-    def from_measurements(cls, name: str, measurements: list[float], unit: str = \"seconds\") -> \"BenchmarkStatistics\":\n-        \"\"\"Create statistics from a list of measurements.\"\"\"\n-        if not measurements:\n-            raise ValueError(\"Cannot create statistics from empty measurements\")\n-\n-        measurements_array = np.array(measurements)\n-\n-        return cls(\n-            name=name,\n-            measurements=measurements,\n-            mean=float(np.mean(measurements_array)),\n-            median=float(np.median(measurements_array)),\n-            std=float(np.std(measurements_array)),\n-            min=float(np.min(measurements_array)),\n-            max=float(np.max(measurements_array)),\n-            p25=float(np.percentile(measurements_array, 25)),\n-            p75=float(np.percentile(measurements_array, 75)),\n-            p90=float(np.percentile(measurements_array, 90)),\n-            p95=float(np.percentile(measurements_array, 95)),\n-            p99=float(np.percentile(measurements_array, 99)),\n-            unit=unit,\n-        )\n-\n-\n-@dataclass\n-class HardwareInfo:\n-    \"\"\"Hardware information collected during benchmarking.\"\"\"\n-\n-    gpu_name: str\n-    gpu_memory_total_mb: int\n-    cpu_count: int\n-    memory_total_mb: int\n-    python_version: str\n-    torch_version: Optional[str] = None\n-    cuda_version: Optional[str] = None\n-\n-\n-@dataclass\n-class BenchmarkMetadata:\n-    \"\"\"Metadata collected for each benchmark run.\"\"\"\n-\n-    timestamp: str\n-    commit_id: str\n-    hardware_info: HardwareInfo\n-    config: BenchmarkConfig\n-\n-\n-class GPUMonitor:\n-    \"\"\"Monitor GPU utilization during benchmark execution.\"\"\"\n-\n-    def __init__(self, sample_interval: float = 0.1, logger: Optional[logging.Logger] = None):\n-        self.sample_interval = sample_interval\n-        self.logger = logger or logging.getLogger(__name__)\n-        self.stop_event = threading.Event()\n-        self.thread = None\n-        self.gpu_utilization = []\n-        self.gpu_memory_used = []\n-        self.timestamps = []\n-        self.gpu_available = False\n-        self.warning_logged = False\n-\n-        # Test GPU availability on initialization\n-        self._test_gpu_availability()\n-\n-    def _test_gpu_availability(self):\n-        \"\"\"Test if GPU monitoring is available.\"\"\"\n-        try:\n-            gpu_stats = gpustat.GPUStatCollection.new_query()\n-            if gpu_stats and len(gpu_stats) > 0:\n-                self.gpu_available = True\n-                self.logger.debug(f\"GPU monitoring available: {len(gpu_stats)} GPU(s) detected\")\n-            else:\n-                self.gpu_available = False\n-                self.logger.debug(\"No GPUs detected by gpustat\")\n-        except Exception as e:\n-            self.gpu_available = False\n-            self.logger.debug(f\"GPU monitoring not available: {e}\")\n-\n-    def start(self):\n-        \"\"\"Start monitoring GPU metrics.\"\"\"\n-        if not self.gpu_available:\n-            self.logger.debug(\"GPU monitoring disabled: no GPUs available\")\n-            return\n-\n-        # Clear the stop event to enable monitoring\n-        self.stop_event.clear()\n-        self.gpu_utilization = []\n-        self.gpu_memory_used = []\n-        self.timestamps = []\n-        self.warning_logged = False  # Reset warning flag for new monitoring session\n-        self.thread = threading.Thread(target=self._monitor_loop)\n-        self.thread.start()\n-        self.logger.debug(\"GPU monitoring started\")\n-\n-    def stop_and_collect(self) -> Union[GPUMetrics, NoGPU]:\n-        \"\"\"Stop monitoring and return collected metrics.\"\"\"\n-        if not self.gpu_available:\n-            return NoGPU(gpu_monitoring_status=\"disabled\", gpu_monitoring_reason=\"no_gpus_available\")\n-\n-        # Signal the monitoring thread to stop\n-        self.stop_event.set()\n-        if self.thread:\n-            self.thread.join()\n-\n-        if self.gpu_utilization:\n-            metrics = GPUMetrics(\n-                gpu_utilization_mean=statistics.mean(self.gpu_utilization),\n-                gpu_utilization_max=max(self.gpu_utilization),\n-                gpu_utilization_min=min(self.gpu_utilization),\n-                gpu_memory_used_mean=statistics.mean(self.gpu_memory_used),\n-                gpu_memory_used_max=max(self.gpu_memory_used),\n-                gpu_memory_used_min=min(self.gpu_memory_used),\n-                sample_count=len(self.gpu_utilization),\n-                gpu_monitoring_status=\"success\",\n-            )\n-            self.logger.debug(f\"GPU monitoring completed: {len(self.gpu_utilization)} samples collected\")\n-            return metrics\n-        else:\n-            return NoGPU(gpu_monitoring_status=\"failed\", gpu_monitoring_reason=\"no_samples_collected\")\n-\n-    def _monitor_loop(self):\n-        \"\"\"Background monitoring loop using threading.Event for communication.\"\"\"\n-        consecutive_failures = 0\n-        max_consecutive_failures = 5\n-\n-        # Continue monitoring until stop_event is set\n-        while not self.stop_event.is_set():\n-            try:\n-                gpu_stats = gpustat.GPUStatCollection.new_query()\n-                if gpu_stats and len(gpu_stats) > 0:\n-                    gpu = gpu_stats[0]\n-                    self.gpu_utilization.append(gpu[\"utilization.gpu\"])\n-                    self.gpu_memory_used.append(gpu[\"memory.used\"])\n-                    self.timestamps.append(time.time())\n-                    consecutive_failures = 0  # Reset failure counter on success\n-                else:\n-                    consecutive_failures += 1\n-                    if consecutive_failures >= max_consecutive_failures and not self.warning_logged:\n-                        self.logger.warning(\"GPU monitoring: No GPU data returned by gpustat\")\n-                        self.warning_logged = True\n-\n-            except Exception as e:\n-                consecutive_failures += 1\n-                if consecutive_failures >= max_consecutive_failures and not self.warning_logged:\n-                    self.logger.warning(f\"GPU monitoring failed after {max_consecutive_failures} attempts: {e}\")\n-                    self.warning_logged = True\n-\n-            # Use Event.wait() with timeout instead of time.sleep()\n-            # This allows for immediate response to stop signal while still maintaining sample interval\n-            if self.stop_event.wait(timeout=self.sample_interval):\n-                # Event was set, break out of loop immediately\n-                break\n-\n-\n-def get_hardware_info() -> HardwareInfo:\n-    \"\"\"Collect hardware information.\"\"\"\n-    gpu_name = \"unknown\"\n-    gpu_memory_total = 0\n-\n-    try:\n-        gpu_stats = gpustat.GPUStatCollection.new_query()\n-        if gpu_stats and len(gpu_stats) > 0:\n-            gpu = gpu_stats[0]\n-            gpu_name = gpu[\"name\"]\n-            gpu_memory_total = gpu[\"memory.total\"]\n-    except Exception:\n-        pass\n-\n-    torch_version = torch.__version__\n-    cuda_version = None\n-    if hasattr(torch, \"cuda\") and torch.cuda.is_available():\n-        cuda_version = torch.version.cuda\n-\n-    return HardwareInfo(\n-        gpu_name=gpu_name,\n-        gpu_memory_total_mb=gpu_memory_total,\n-        cpu_count=psutil.cpu_count(),\n-        memory_total_mb=int(psutil.virtual_memory().total / (1024 * 1024)),\n-        python_version=f\"{sys.version.split()[0]}\",\n-        torch_version=torch_version,\n-        cuda_version=cuda_version,\n-    )\n-\n-\n-def flush_memory():\n-    \"\"\"Flush GPU memory and run garbage collection.\"\"\"\n-    gc.collect()\n-    if hasattr(torch, \"cuda\") and torch.cuda.is_available():\n-        torch.cuda.empty_cache()\n-        torch.cuda.reset_max_memory_allocated()\n-        torch.cuda.reset_peak_memory_stats()\n-        torch.cuda.synchronize()\n-\n-\n-def get_sdpa_backend(backend_name: Optional[str]):\n-    \"\"\"Get the SDPA backend enum from string name.\"\"\"\n-    if backend_name is None:\n-        return None\n-\n-    try:\n-        backend_map = {\n-            \"math\": torch.nn.attention.SDPBackend.MATH,\n-            \"flash_attention\": torch.nn.attention.SDPBackend.FLASH_ATTENTION,\n-            \"efficient_attention\": torch.nn.attention.SDPBackend.EFFICIENT_ATTENTION,\n-            \"cudnn_attention\": torch.nn.attention.SDPBackend.CUDNN_ATTENTION,\n-        }\n-        return backend_map.get(backend_name.lower())\n-    except AttributeError:\n-        # torch.nn.attention.SDPBackend not available in older torch versions\n-        return None\n-\n-\n-class SDPAContext:\n-    \"\"\"Context manager for SDPA kernel selection.\"\"\"\n-\n-    def __init__(self, backend_name: Optional[str], logger: Optional[logging.Logger] = None):\n-        self.backend_name = backend_name\n-        self.logger = logger or logging.getLogger(__name__)\n-        self.backend = get_sdpa_backend(backend_name) if backend_name else None\n-        self.context = None\n-\n-    def __enter__(self):\n-        if self.backend is not None:\n-            try:\n-                self.context = torch.nn.attention.sdpa_kernel(self.backend)\n-                self.context.__enter__()\n-                if self.logger:\n-                    self.logger.debug(f\"Using SDPA backend: {self.backend_name}\")\n-            except Exception as e:\n-                if self.logger:\n-                    self.logger.warning(f\"Failed to set SDPA backend {self.backend_name}: {e}\")\n-                self.context = None\n-        elif self.backend_name and self.logger:\n-            self.logger.debug(\n-                f\"SDPA backend '{self.backend_name}' requested but not using kernel context (backend={self.backend})\"\n-            )\n-        return self\n-\n-    def __exit__(self, exc_type, exc_val, exc_tb):\n-        if self.context is not None:\n-            try:\n-                self.context.__exit__(exc_type, exc_val, exc_tb)\n-            except Exception as e:\n-                if self.logger:\n-                    self.logger.warning(f\"Error exiting SDPA context: {e}\")\n-        return False\n-\n-\n-class AbstractModelBenchmark(ABC):\n-    \"\"\"Abstract base class for model benchmarks.\"\"\"\n-\n-    def __init__(self, logger: logging.Logger):\n-        self.logger = logger\n-        self.model = None\n-        self.tokenizer = None\n-        self.device = None\n-        self.scenarios = {}  # Map of scenario_name -> BenchmarkScenario\n-\n-    @abstractmethod\n-    def create_scenarios(self, **kwargs) -> dict[str, \"BenchmarkScenario\"]:\n-        \"\"\"Create and return a dictionary of benchmark scenarios.\"\"\"\n-        pass\n-\n-    @abstractmethod\n-    def setup_model(self, config: BenchmarkConfig) -> None:\n-        \"\"\"Setup the model for benchmarking with the given configuration.\"\"\"\n-        pass\n-\n-    @abstractmethod\n-    def cleanup_model(self) -> None:\n-        \"\"\"Cleanup model resources.\"\"\"\n-        pass\n-\n-    @abstractmethod\n-    def measure_time_to_first_token(self, config: BenchmarkConfig) -> float:\n-        \"\"\"Measure time to first token generation.\"\"\"\n-        pass\n-\n-    @abstractmethod\n-    def measure_latency(self, config: BenchmarkConfig) -> TimingResult:\n-        \"\"\"Measure full generation latency and compute tokens/sec.\"\"\"\n-        pass\n-\n-    def prepare_inputs(self, config: BenchmarkConfig) -> Any:\n-        \"\"\"Prepare inputs for the model. Override if needed.\"\"\"\n-        return None\n-\n-    def get_scenarios(self, **kwargs) -> dict[str, \"BenchmarkScenario\"]:\n-        \"\"\"Get benchmark scenarios. Creates them if they don't exist.\"\"\"\n-        if not self.scenarios:\n-            self.scenarios = self.create_scenarios(**kwargs)\n-        return self.scenarios\n-\n-\n-class ModelBenchmark(AbstractModelBenchmark):\n-    \"\"\"\n-    Base class for HuggingFace Transformers model benchmarks.\n-\n-    This class provides common scenario creation logic and handles the standard\n-    patterns for eager, compiled, and kernelized execution variants with different\n-    attention implementations and SDPA backends.\n-    \"\"\"\n-\n-    def __init__(self, logger: logging.Logger):\n-        super().__init__(logger)\n-        self.inputs = None\n-        self.compiled_model = None\n-        self.past_key_values = None\n-        self.config = None\n-        self._default_prompt = \"Why dogs are so cute?\"\n-\n-    @property\n-    def default_prompt(self) -> str:\n-        \"\"\"Default prompt for text generation. Override in subclasses if needed.\"\"\"\n-        return self._default_prompt\n-\n-    def get_attention_configs(self, include_sdpa_variants: bool = True) -> list[dict[str, Any]]:\n-        \"\"\"\n-        Get attention implementation configurations.\n-\n-        Args:\n-            include_sdpa_variants: Whether to include SDPA backend variants\n-\n-        Returns:\n-            List of attention configuration dictionaries\n-        \"\"\"\n-        attention_configs = [\n-            {\"attn_implementation\": \"eager\", \"sdpa_backends\": [None], \"desc_suffix\": \" with eager attention\"},\n-        ]\n-\n-        # Add SDPA variants if requested\n-        if include_sdpa_variants:\n-            attention_configs.append(\n-                {\n-                    \"attn_implementation\": \"sdpa\",\n-                    \"sdpa_backends\": [None, \"math\", \"flash_attention\", \"efficient_attention\"],\n-                    \"desc_suffix\": \"\",\n-                }\n-            )\n-\n-        return attention_configs\n-\n-    def get_scenario_configs(self) -> list[dict[str, Any]]:\n-        \"\"\"\n-        Get base scenario configurations. Override in subclasses to customize.\n-\n-        Returns:\n-            List of scenario configuration dictionaries\n-        \"\"\"\n-        return [\n-            # Eager variants\n-            {\"variant\": \"eager\", \"compile_mode\": None, \"use_cache\": True, \"description\": \"Eager execution with cache\"},\n-            # Compiled variants\n-            {\n-                \"variant\": \"compiled\",\n-                \"compile_mode\": \"max-autotune\",\n-                \"use_cache\": True,\n-                \"description\": \"Compiled with max autotune\",\n-            },\n-            # Kernelized variant (if available)\n-            {\n-                \"variant\": \"kernelized\",\n-                \"compile_mode\": \"max-autotune\",\n-                \"use_cache\": True,\n-                \"description\": \"Kernelized execution\",\n-            },\n-        ]\n-\n-    def _is_kernelization_available(self) -> bool:\n-        \"\"\"Check if kernelization is available. Override in subclasses.\"\"\"\n-        try:\n-            from kernels import Mode, kernelize  # noqa: F401\n-\n-            return True\n-        except ImportError:\n-            return False\n-\n-    def get_default_generation_config(self) -> dict[str, Any]:\n-        \"\"\"Get default generation configuration. Override in subclasses for model-specific defaults.\"\"\"\n-        return {\"do_sample\": False, \"top_p\": 1.0, \"temperature\": 1.0}\n-\n-    def get_model_init_kwargs(self, config: BenchmarkConfig) -> dict[str, Any]:\n-        \"\"\"Get model initialization kwargs. Override in subclasses for model-specific parameters.\"\"\"\n-        return {\"torch_dtype\": getattr(torch, config.torch_dtype), \"attn_implementation\": config.attn_implementation}\n-\n-    def get_default_torch_dtype(self) -> str:\n-        \"\"\"Get default torch dtype. Override in subclasses.\"\"\"\n-        return \"float16\"\n-\n-    def get_default_device(self) -> str:\n-        \"\"\"Get default device. Override in subclasses.\"\"\"\n-        return \"cuda\"\n-\n-    def create_scenarios(self, **kwargs) -> dict[str, \"BenchmarkScenario\"]:\n-        \"\"\"Create benchmark scenarios for HuggingFace models.\"\"\"\n-        scenarios = {}\n-\n-        # Extract parameters with model-specific defaults\n-        model_id = kwargs.get(\"model_id\", \"microsoft/DialoGPT-medium\")\n-        warmup_iterations = kwargs.get(\"warmup_iterations\", 3)\n-        measurement_iterations = kwargs.get(\"measurement_iterations\", 5)\n-        num_tokens_to_generate = kwargs.get(\"num_tokens_to_generate\", 100)\n-        include_sdpa_variants = kwargs.get(\"include_sdpa_variants\", True)\n-        device = kwargs.get(\"device\", self.get_default_device())\n-        torch_dtype = kwargs.get(\"torch_dtype\", self.get_default_torch_dtype())\n-        batch_size = kwargs.get(\"batch_size\", 1)\n-\n-        # Get configurations\n-        attention_configs = self.get_attention_configs(include_sdpa_variants)\n-        scenario_configs = self.get_scenario_configs()\n-\n-        # Create scenarios for each attention config and variant combination\n-        for attn_config in attention_configs:\n-            attn_implementation = attn_config[\"attn_implementation\"]\n-            sdpa_backends = attn_config[\"sdpa_backends\"]\n-            desc_suffix = attn_config[\"desc_suffix\"]\n-\n-            for scenario_config in scenario_configs:\n-                for sdpa_backend in sdpa_backends:\n-                    # Skip kernelized if not available\n-                    if scenario_config[\"variant\"] == \"kernelized\" and not self._is_kernelization_available():\n-                        continue\n-\n-                    # Create unique config for this scenario\n-                    config = BenchmarkConfig(\n-                        name=scenario_config[\"variant\"],\n-                        model_id=model_id,\n-                        variant=scenario_config[\"variant\"],\n-                        compile_mode=scenario_config[\"compile_mode\"],\n-                        use_cache=scenario_config[\"use_cache\"],\n-                        warmup_iterations=warmup_iterations,\n-                        measurement_iterations=measurement_iterations,\n-                        num_tokens_to_generate=num_tokens_to_generate,\n-                        device=device,\n-                        torch_dtype=torch_dtype,\n-                        batch_size=batch_size,\n-                        attn_implementation=attn_implementation,\n-                        sdpa_backend=sdpa_backend if attn_implementation == \"sdpa\" else None,\n-                    )\n-\n-                    # Create scenario name\n-                    scenario_name_parts = [scenario_config[\"variant\"]]\n-                    if scenario_config[\"compile_mode\"]:\n-                        scenario_name_parts.append(f\"compile_{scenario_config['compile_mode']}\")\n-\n-                    # Add attention implementation to name\n-                    if attn_implementation == \"eager\":\n-                        scenario_name_parts.append(\"eager_attn\")\n-                    elif attn_implementation == \"sdpa\":\n-                        if sdpa_backend:\n-                            scenario_name_parts.append(f\"sdpa_{sdpa_backend}\")\n-                        else:\n-                            scenario_name_parts.append(\"sdpa_default\")\n-\n-                    scenario_name = \"_\".join(scenario_name_parts)\n-\n-                    # Create description\n-                    description = scenario_config[\"description\"]\n-                    if attn_implementation == \"sdpa\" and sdpa_backend:\n-                        description += f\" with SDPA {sdpa_backend} backend\"\n-                    elif attn_implementation == \"sdpa\":\n-                        description += \" with SDPA default backend\"\n-                    else:\n-                        description += desc_suffix\n-\n-                    # Create scenario\n-                    scenario = BenchmarkScenario(name=scenario_name, config=config, description=description)\n-\n-                    # Add setup callbacks based on variant\n-                    if scenario_config[\"variant\"] == \"compiled\":\n-                        scenario.add_setup_callback(self._setup_compilation_callback)\n-                    elif scenario_config[\"variant\"] == \"kernelized\":\n-                        scenario.add_setup_callback(self._setup_kernelization_callback)\n-\n-                    scenarios[scenario_name] = scenario\n-\n-        return scenarios\n-\n-    def _setup_compilation_callback(self, model, tokenizer, config, logger):\n-        \"\"\"Setup callback for compilation scenarios.\"\"\"\n-        if logger:\n-            logger.info(f\"Setting up compilation with mode: {config.compile_mode}\")\n-\n-        # Perform torch.compile\n-        if config.compile_mode is not None:\n-            self.compiled_model = torch.compile(model, mode=config.compile_mode, **config.compile_options)\n-        else:\n-            self.compiled_model = torch.compile(model, **config.compile_options)\n-\n-        # Setup static cache for compiled mode if needed\n-        if config.use_cache and hasattr(self, \"inputs\") and self.inputs is not None:\n-            self._setup_static_cache(config)\n-\n-    def _setup_kernelization_callback(self, model, tokenizer, config, logger):\n-        \"\"\"Setup callback for kernelization scenarios.\"\"\"\n-        if logger:\n-            logger.info(\"Setting up kernelization\")\n-\n-        try:\n-            from kernels import Mode, kernelize\n-\n-            self.compiled_model = kernelize(model, mode=Mode.INFERENCE)\n-        except Exception as e:\n-            if logger:\n-                logger.warning(f\"Failed to setup kernelized mode: {e}\")\n-                logger.warning(\"Falling back to eager mode\")\n-            config.variant = \"eager\"\n-\n-    def _setup_static_cache(self, config: BenchmarkConfig):\n-        \"\"\"Setup static cache for compiled models. Override if needed.\"\"\"\n-        if hasattr(self, \"inputs\") and self.inputs is not None:\n-            try:\n-                from transformers import StaticCache\n-\n-                seq_length = self.inputs[\"input_ids\"].shape[1]\n-\n-                # Get the actual device the model is on\n-                if hasattr(self.model, \"device\"):\n-                    cache_device = self.model.device\n-                else:\n-                    cache_device = self.device\n-\n-                self.past_key_values = StaticCache(\n-                    config=self.model.config,\n-                    max_batch_size=config.batch_size,\n-                    max_cache_len=seq_length + config.num_tokens_to_generate,\n-                    device=cache_device,\n-                    dtype=getattr(torch, config.torch_dtype),\n-                )\n-                self.logger.debug(f\"StaticCache created on device: {cache_device}\")\n-            except (ImportError, TypeError) as e:\n-                # StaticCache not available or incompatible, continue without it\n-                self.logger.debug(f\"StaticCache setup failed: {e}, continuing without cache\")\n-                self.past_key_values = None\n-\n-    def setup_model(self, config: BenchmarkConfig) -> None:\n-        \"\"\"Setup the HuggingFace model for benchmarking with the given configuration.\"\"\"\n-\n-        self.logger.info(f\"Setting up model: {config.model_id} with variant: {config.variant}\")\n-        self.device = config.device\n-        self.config = config\n-\n-        # Load model and tokenizer\n-        self._load_model_and_tokenizer(config)\n-\n-        # Prepare inputs\n-        self._prepare_model_inputs(config)\n-\n-        # Configure generation settings\n-        self._configure_generation(config)\n-\n-        self.logger.info(\"Model setup complete\")\n-\n-    def _load_model_and_tokenizer(self, config: BenchmarkConfig):\n-        \"\"\"Load the model and tokenizer. Override in subclasses for custom loading.\"\"\"\n-\n-        from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n-\n-        # Load tokenizer\n-        self.tokenizer = AutoTokenizer.from_pretrained(config.model_id)\n-        if self.tokenizer.pad_token is None:\n-            self.tokenizer.pad_token = self.tokenizer.eos_token\n-\n-        # Prepare generation config\n-        generation_config_dict = self.get_default_generation_config()\n-        gen_config = GenerationConfig(**generation_config_dict)\n-\n-        # Load model\n-        self.logger.info(\"Loading model...\")\n-\n-        target_device = config.device\n-        # Get model initialization kwargs\n-        model_init_kwargs = self.get_model_init_kwargs(config)\n-        model_init_kwargs.update({\"generation_config\": gen_config})\n-\n-        self.model = AutoModelForCausalLM.from_pretrained(config.model_id, **model_init_kwargs).eval()\n-\n-        # Move model to target device\n-        self.logger.info(f\"Moving model to device: {target_device}\")\n-        self.model.to(target_device)\n-        self.device = target_device  # Update device to match actual device used\n-\n-    def _prepare_model_inputs(self, config: BenchmarkConfig):\n-        \"\"\"Prepare model inputs. Override in subclasses for custom inputs.\"\"\"\n-        # Prepare inputs\n-        self.inputs = self.tokenizer(self.default_prompt, return_tensors=\"pt\")\n-\n-        # Move inputs to the same device as the model\n-        if hasattr(self.model, \"device\"):\n-            # Model is on a single device\n-            model_device = self.model.device\n-        else:\n-            # Model might be distributed, use self.device which was set during model loading\n-            model_device = self.device\n-\n-        self.inputs = {k: v.to(model_device) for k, v in self.inputs.items()}\n-        self.logger.debug(f\"Moved inputs to device: {model_device}\")\n-\n-    def _configure_generation(self, config: BenchmarkConfig):\n-        \"\"\"Configure generation settings.\"\"\"\n-        seq_length = self.inputs[\"input_ids\"].shape[1]\n-        self.model.generation_config.max_length = seq_length + config.num_tokens_to_generate\n-\n-    def cleanup_model(self) -> None:\n-        \"\"\"Cleanup model resources.\"\"\"\n-        if hasattr(self, \"model\") and self.model is not None:\n-            del self.model\n-            self.model = None\n-        if hasattr(self, \"compiled_model\") and self.compiled_model is not None:\n-            del self.compiled_model\n-            self.compiled_model = None\n-        if hasattr(self, \"tokenizer\") and self.tokenizer is not None:\n-            del self.tokenizer\n-            self.tokenizer = None\n-        if hasattr(self, \"past_key_values\") and self.past_key_values is not None:\n-            del self.past_key_values\n-            self.past_key_values = None\n-\n-        # Clear CUDA cache\n-        flush_memory()\n-\n-    def measure_time_to_first_token(self, config: BenchmarkConfig) -> float:\n-        \"\"\"Measure time to first token generation.\"\"\"\n-        model_to_use = self.compiled_model if self.compiled_model is not None else self.model\n-\n-        # Prepare generation kwargs\n-        generation_kwargs = self._get_generation_kwargs(config, max_new_tokens=1)\n-\n-        # Use CUDA timer for high-precision measurement\n-        with ArchAwareTimer(device=config.device) as timer:\n-            # Use SDPA context if specified\n-            with SDPAContext(config.sdpa_backend, self.logger):\n-                with torch.no_grad():\n-                    _ = model_to_use.generate(**generation_kwargs)\n-\n-        return timer.elapsed_time()\n-\n-    def measure_latency(self, config: BenchmarkConfig) -> TimingResult:\n-        \"\"\"Measure full generation latency and compute tokens/sec.\"\"\"\n-        model_to_use = self.compiled_model if self.compiled_model is not None else self.model\n-\n-        # Prepare generation kwargs\n-        generation_kwargs = self._get_generation_kwargs(config, max_new_tokens=config.num_tokens_to_generate)\n-\n-        # Use CUDA timer for high-precision measurement\n-        with ArchAwareTimer(device=config.device) as timer:\n-            # Use SDPA context if specified\n-            with SDPAContext(config.sdpa_backend, self.logger):\n-                with torch.no_grad():\n-                    outputs = model_to_use.generate(**generation_kwargs)\n-\n-        # Calculate metrics\n-        latency = timer.elapsed_time()\n-        input_length = self.inputs[\"input_ids\"].shape[1]\n-        output_length = outputs.shape[1]\n-        tokens_generated = output_length - input_length\n-\n-        tokens_per_second = tokens_generated / latency if latency > 0 else 0\n-        time_per_output_token = latency / tokens_generated if tokens_generated > 0 else None\n-\n-        return TimingResult(\n-            latency_seconds=latency,\n-            tokens_per_second=tokens_per_second,\n-            time_per_output_token_seconds=time_per_output_token,\n-            total_tokens_generated=tokens_generated,\n-            metadata={\n-                \"input_length\": input_length,\n-                \"output_length\": output_length,\n-                \"variant\": config.variant,\n-                \"compile_mode\": config.compile_mode,\n-                \"attn_implementation\": config.attn_implementation,\n-                \"sdpa_backend\": config.sdpa_backend,\n-            },\n-        )\n-\n-    def _get_generation_kwargs(self, config: BenchmarkConfig, max_new_tokens: int) -> dict[str, Any]:\n-        \"\"\"Get generation kwargs. Override in subclasses for custom generation.\"\"\"\n-        generation_config_dict = self.get_default_generation_config()\n-        generation_kwargs = {\n-            **self.inputs,\n-            \"max_new_tokens\": max_new_tokens,\n-            \"do_sample\": generation_config_dict.get(\"do_sample\", False),\n-            \"temperature\": generation_config_dict.get(\"temperature\", 1.0),\n-            \"top_p\": generation_config_dict.get(\"top_p\", 1.0),\n-            \"pad_token_id\": self.tokenizer.pad_token_id,\n-        }\n-\n-        # Handle static cache for compiled models\n-        if self.past_key_values is not None and config.variant == \"compiled\":\n-            try:\n-                from transformers import StaticCache\n-\n-                # Reset cache for each measurement\n-                seq_length = self.inputs[\"input_ids\"].shape[1]\n-\n-                # Get the actual device the model is on\n-                if hasattr(self.model, \"device\"):\n-                    cache_device = self.model.device\n-                else:\n-                    cache_device = self.device\n-\n-                fresh_cache = StaticCache(\n-                    config=self.model.config,\n-                    max_batch_size=config.batch_size,\n-                    max_cache_len=seq_length + max_new_tokens,\n-                    device=cache_device,\n-                    dtype=getattr(torch, config.torch_dtype),\n-                )\n-                generation_kwargs[\"past_key_values\"] = fresh_cache\n-            except (ImportError, TypeError) as e:\n-                self.logger.debug(f\"Fresh StaticCache creation failed: {e}\")\n-                pass\n-\n-        return generation_kwargs\n-\n-\n-class BenchmarkRunner:\n-    \"\"\"Main benchmark runner that coordinates benchmark execution.\"\"\"\n-\n-    def __init__(self, logger: logging.Logger, output_dir: str = \"benchmark_results\"):\n-        self.logger = logger\n-        self.output_dir = output_dir\n-        os.makedirs(output_dir, exist_ok=True)\n-\n-    def run_benchmark(\n-        self,\n-        benchmark: ModelBenchmark,\n-        scenarios: dict[str, BenchmarkScenario],\n-        collect_gpu_metrics: bool = True,\n-        commit_id: Optional[str] = None,\n-    ) -> dict[str, dict[str, Any]]:\n-        \"\"\"\n-        Run benchmarks using scenarios.\n-\n-        Args:\n-            benchmark: The benchmark instance to run\n-            scenarios: Dictionary mapping scenario names to BenchmarkScenario instances\n-            collect_gpu_metrics: Whether to collect GPU utilization metrics\n-            commit_id: Git commit ID for metadata (if not provided, will auto-detect from git)\n-\n-        Returns:\n-            Dictionary mapping scenario names to results with statistics\n-        \"\"\"\n-        all_results = {}\n-\n-        for scenario_name, scenario in scenarios.items():\n-            self.logger.info(f\"Running benchmark scenario: {scenario_name}\")\n-            config = scenario.config\n-\n-            try:\n-                # Setup model for this configuration\n-                benchmark.setup_model(config)\n-\n-                # Run scenario setup callbacks\n-                scenario.setup(benchmark.model, benchmark.tokenizer, self.logger)\n-\n-                # Quick validation: try one measurement first to see if this scenario works\n-                try:\n-                    flush_memory()\n-                    test_result = benchmark.measure_time_to_first_token(config)\n-                    if test_result is None or test_result <= 0:\n-                        raise ValueError(\"Invalid measurement result\")\n-                except Exception as validation_error:\n-                    self.logger.warning(f\"Skipping scenario {scenario_name}: validation failed - {validation_error}\")\n-                    # Clean up and skip this scenario\n-                    try:\n-                        scenario.teardown(benchmark.model, benchmark.tokenizer, self.logger)\n-                        benchmark.cleanup_model()\n-                    except Exception:\n-                        pass\n-                    continue\n-\n-                # Collect metadata\n-                metadata = BenchmarkMetadata(\n-                    timestamp=datetime.utcnow().isoformat(),\n-                    commit_id=commit_id,\n-                    hardware_info=get_hardware_info(),\n-                    config=config,\n-                )\n-\n-                # Initialize GPU monitor\n-                gpu_monitor = None\n-                if collect_gpu_metrics:\n-                    gpu_monitor = GPUMonitor(logger=self.logger)\n-\n-                # Warmup runs\n-                self.logger.info(f\"Warming up with {config.warmup_iterations} iterations...\")\n-                warmup_failures = 0\n-                for i in range(config.warmup_iterations):\n-                    try:\n-                        _ = benchmark.measure_latency(config)\n-                    except Exception as e:\n-                        warmup_failures += 1\n-                        self.logger.warning(f\"Warmup iteration {i + 1} failed: {e}\")\n-\n-                # If more than half the warmup iterations failed, skip this scenario\n-                if warmup_failures > config.warmup_iterations // 2:\n-                    self.logger.warning(\n-                        f\"Skipping scenario {scenario_name}: too many warmup failures ({warmup_failures}/{config.warmup_iterations})\"\n-                    )\n-                    try:\n-                        scenario.teardown(benchmark.model, benchmark.tokenizer, self.logger)\n-                        benchmark.cleanup_model()\n-                    except Exception:\n-                        pass\n-                    continue\n-\n-                # Start GPU monitoring\n-                if gpu_monitor:\n-                    gpu_monitor.start()\n-\n-                # Measurement runs for latency\n-                self.logger.info(f\"Measuring latency with {config.measurement_iterations} iterations...\")\n-                latency_measurements = []\n-                ttft_measurements = []\n-                tokens_per_sec_measurements = []\n-                itl_measurements = []  # Inter-Token Latency\n-                measurement_failures = 0\n-\n-                for i in range(config.measurement_iterations):\n-                    try:\n-                        # Measure time to first token\n-                        ttft = benchmark.measure_time_to_first_token(config)\n-                        ttft_measurements.append(ttft)\n-\n-                        # Measure full latency\n-                        timing_result = benchmark.measure_latency(config)\n-                        latency_measurements.append(timing_result.latency_seconds)\n-\n-                        if timing_result.tokens_per_second is not None:\n-                            tokens_per_sec_measurements.append(timing_result.tokens_per_second)\n-\n-                        if timing_result.time_per_output_token_seconds is not None:\n-                            itl_measurements.append(timing_result.time_per_output_token_seconds)\n-\n-                        itl_str = (\n-                            f\", itl={timing_result.time_per_output_token_seconds:.4f}s/token\"\n-                            if timing_result.time_per_output_token_seconds\n-                            else \"\"\n-                        )\n-                        self.logger.debug(\n-                            f\"Iteration {i + 1}: latency={timing_result.latency_seconds:.4f}s, ttft={ttft:.4f}s{itl_str}\"\n-                        )\n-\n-                    except Exception as e:\n-                        measurement_failures += 1\n-                        self.logger.warning(f\"Measurement iteration {i + 1} failed: {e}\")\n-\n-                # Stop GPU monitoring\n-                gpu_metrics = {}\n-                if gpu_monitor:\n-                    gpu_metrics = gpu_monitor.stop_and_collect()\n-\n-                # If we don't have enough successful measurements, skip this scenario\n-                if not latency_measurements or len(latency_measurements) < config.measurement_iterations // 2:\n-                    self.logger.warning(\n-                        f\"Skipping scenario {scenario_name}: insufficient successful measurements ({len(latency_measurements)}/{config.measurement_iterations})\"\n-                    )\n-                    try:\n-                        scenario.teardown(benchmark.model, benchmark.tokenizer, self.logger)\n-                        benchmark.cleanup_model()\n-                    except Exception:\n-                        pass\n-                    continue\n-\n-                # Calculate statistics\n-                scenario_results = {\n-                    \"metadata\": asdict(metadata),\n-                    \"measurements\": {},\n-                    \"gpu_metrics\": gpu_metrics,\n-                    \"scenario_description\": scenario.description,\n-                }\n-\n-                if latency_measurements:\n-                    latency_stats = BenchmarkStatistics.from_measurements(\"latency_seconds\", latency_measurements)\n-                    scenario_results[\"measurements\"][\"latency_seconds\"] = asdict(latency_stats)\n-\n-                if ttft_measurements:\n-                    ttft_stats = BenchmarkStatistics.from_measurements(\n-                        \"time_to_first_token_seconds\", ttft_measurements\n-                    )\n-                    scenario_results[\"measurements\"][\"time_to_first_token_seconds\"] = asdict(ttft_stats)\n-\n-                if tokens_per_sec_measurements:\n-                    tps_stats = BenchmarkStatistics.from_measurements(\n-                        \"tokens_per_second\", tokens_per_sec_measurements, \"tokens/sec\"\n-                    )\n-                    scenario_results[\"measurements\"][\"tokens_per_second\"] = asdict(tps_stats)\n-\n-                if itl_measurements:\n-                    itl_stats = BenchmarkStatistics.from_measurements(\n-                        \"time_per_output_token_seconds\", itl_measurements, \"seconds/token\"\n-                    )\n-                    scenario_results[\"measurements\"][\"time_per_output_token_seconds\"] = asdict(itl_stats)\n-\n-                # Log summary\n-                if latency_measurements:\n-                    self.logger.info(f\"Latency: {latency_stats.mean:.4f}\u00b1{latency_stats.std:.4f}s (mean\u00b1std)\")\n-                if ttft_measurements:\n-                    self.logger.info(f\"TTFT: {ttft_stats.mean:.4f}\u00b1{ttft_stats.std:.4f}s (mean\u00b1std)\")\n-                if tokens_per_sec_measurements:\n-                    self.logger.info(f\"Throughput: {tps_stats.mean:.2f}\u00b1{tps_stats.std:.2f} tokens/sec (mean\u00b1std)\")\n-                if itl_measurements:\n-                    self.logger.info(f\"ITL: {itl_stats.mean:.4f}\u00b1{itl_stats.std:.4f}s/token (mean\u00b1std)\")\n-\n-                # Add note about partial results if some measurements failed\n-                if measurement_failures > 0:\n-                    scenario_results[\"warnings\"] = [f\"Some measurements failed ({measurement_failures} failures)\"]\n-                    self.logger.info(f\"Scenario completed with {measurement_failures} measurement failures\")\n-\n-                # Run scenario teardown callbacks\n-                scenario.teardown(benchmark.model, benchmark.tokenizer, self.logger)\n-\n-                # Cleanup model\n-                benchmark.cleanup_model()\n-\n-                all_results[scenario_name] = scenario_results\n-\n-            except Exception as e:\n-                self.logger.warning(f\"Skipping scenario {scenario_name}: setup failed - {e}\")\n-                import traceback\n-\n-                self.logger.debug(traceback.format_exc())\n-\n-                # Try to clean up if possible\n-                try:\n-                    scenario.teardown(benchmark.model, benchmark.tokenizer, self.logger)\n-                    benchmark.cleanup_model()\n-                except Exception:\n-                    pass\n-                # Skip storing failed scenarios - just continue to the next one\n-            finally:\n-                try:\n-                    scenario.teardown(benchmark.model, benchmark.tokenizer, self.logger)\n-                    benchmark.cleanup_model()\n-                except Exception as cleanup_error:\n-                    self.logger.warning(f\"Cleanup failed for scenario {scenario_name}: {cleanup_error}\")\n-\n-                flush_memory()\n-\n-        return all_results\n-\n-    def save_results(self, model_name: str, results: dict[str, dict[str, Any]]) -> str:\n-        \"\"\"Save benchmark results to JSON file.\"\"\"\n-        # Create model-specific subdirectory\n-        model_dir = os.path.join(self.output_dir, model_name)\n-        os.makedirs(model_dir, exist_ok=True)\n-\n-        # Create filename with timestamp\n-        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n-        filename = f\"{model_name}_benchmark_{timestamp}.json\"\n-        filepath = os.path.join(model_dir, filename)\n-\n-        # Prepare output structure\n-        output_data = {\"model_name\": model_name, \"benchmark_scenarios\": []}\n-\n-        for config_name, config_results in results.items():\n-            scenario = {\n-                \"scenario_name\": config_name,\n-                \"metadata\": config_results[\"metadata\"],\n-                \"measurements\": config_results[\"measurements\"],\n-                \"gpu_metrics\": config_results.get(\"gpu_metrics\", {}),\n-            }\n-            output_data[\"benchmark_scenarios\"].append(scenario)\n-\n-        # Save to JSON file\n-        with open(filepath, \"w\") as f:\n-            json.dump(output_data, f, indent=2, default=str)\n-\n-        self.logger.info(f\"Results saved to {filepath}\")\n-        return filepath"
      },
      {
        "filename": "benchmark_v2/framework/benchmark_config.py",
        "status": "added",
        "additions": 218,
        "deletions": 0,
        "changes": 218,
        "patch": "@@ -0,0 +1,218 @@\n+import hashlib\n+import json\n+import logging\n+from typing import Any, Optional\n+\n+\n+KERNELIZATION_AVAILABLE = False\n+try:\n+    from kernels import Mode, kernelize  # noqa: F401\n+\n+    KERNELIZATION_AVAILABLE = True\n+except ImportError:\n+    pass\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+class BenchmarkConfig:\n+    \"\"\"Configuration for a single benchmark scenario.\"\"\"\n+\n+    def __init__(\n+        self,\n+        warmup_iterations: int = 5,\n+        measurement_iterations: int = 20,\n+        gpu_monitoring: bool = False,  # False by default because it slows down the benchmark by a lot\n+        batch_size: int = 1,\n+        sequence_length: int = 128,\n+        num_tokens_to_generate: int = 128,\n+        attn_implementation: str = \"eager\",\n+        sdpa_backend: Optional[str] = None,\n+        compile_mode: Optional[str] = None,\n+        compile_options: Optional[dict[str, Any]] = None,\n+        kernelize: bool = False,\n+        name: Optional[str] = None,\n+        skip_validity_check: bool = False,\n+    ) -> None:\n+        # Benchmark parameters\n+        self.warmup_iterations = warmup_iterations\n+        self.measurement_iterations = measurement_iterations\n+        self.gpu_monitoring = gpu_monitoring\n+        # Input parameters\n+        self.batch_size = batch_size\n+        self.sequence_length = sequence_length\n+        self.num_tokens_to_generate = num_tokens_to_generate\n+        # Generation parameters\n+        self.attn_implementation = attn_implementation\n+        self.sdpa_backend = sdpa_backend\n+        # Optimization parameters\n+        self.compile_mode = compile_mode\n+        self.compile_options = compile_options if compile_options is not None else {}\n+        self.kernelize = kernelize\n+        # Constant parameters\n+        self.dtype = \"torch.bfloat16\"\n+        self.device = \"cuda\"\n+\n+        self.check_validity(skip_validity_check)\n+        self.name = name if name is not None else self.infer_name()\n+\n+    def check_validity(self, skip_validity_check: bool = False) -> None:\n+        if skip_validity_check:\n+            return\n+        # Flash attention does not support compile mode, so we turn it off # FIXME: it would be better to support it\n+        is_fa = self.attn_implementation == \"flash_attention_2\"\n+        is_fa |= self.attn_implementation == \"sdpa\" and self.sdpa_backend == \"flash_attention\"\n+        if is_fa:\n+            logger.warning(\"Flash attention does not support compile mode. Turning off compile mode.\")\n+            self.compile_mode = None\n+\n+    @property\n+    def hash(self) -> str:\n+        return hashlib.sha256(json.dumps(self.to_dict()).encode()).hexdigest()\n+\n+    def infer_name(self, compact: bool = True) -> str:\n+        \"\"\"Infer a human-readable name for the benchmark config, either compact or verbose.\"\"\"\n+        if compact:\n+            iter_str = f\"w{self.warmup_iterations}_i{self.measurement_iterations}\"\n+            gpu_monitor_str = \"monitored\" if self.gpu_monitoring else \"unmonitored\"\n+            dimensions_str = f\"b{self.batch_size}_s{self.sequence_length}_n{self.num_tokens_to_generate}\"\n+            attn_code = self.attn_implementation\n+            attn_code += f\"_{self.sdpa_backend}\" if self.attn_implementation == \"sdpa\" else \"\"\n+            compile_str = f\"compiled_{self.compile_mode}\" if self.compile_mode is not None else \"uncompiled\"\n+            kernelize_str = \"kernelized\" if self.kernelize else \"unkernelized\"\n+            sep = \"-\"\n+        else:\n+            iter_str = f\"{self.warmup_iterations} warmup, {self.measurement_iterations} iterations\"\n+            gpu_monitor_str = (\"with\" if self.gpu_monitoring else \"no\") + \" GPU monitoring\"\n+            dimensions_str = f\"batch size {self.batch_size}, sequence length {self.sequence_length}, {self.num_tokens_to_generate} generated tokens\"\n+            attn_code = f\"{self.attn_implementation} attention\"\n+            attn_code += f\" with {self.sdpa_backend} backend\" if self.attn_implementation == \"sdpa\" else \"\"\n+            compile_str = \"compiled\" if self.compile_mode is not None else \"not compiled\"\n+            kernelize_str = \"kernelized\" if self.kernelize else \"not kernelized\"\n+            sep = \", \"\n+        return sep.join([iter_str, gpu_monitor_str, dimensions_str, attn_code, compile_str, kernelize_str])\n+\n+    def to_dict(self) -> dict[str, Any]:\n+        return {\n+            \"name\": self.name,\n+            \"warmup_iterations\": self.warmup_iterations,\n+            \"measurement_iterations\": self.measurement_iterations,\n+            \"gpu_monitoring\": self.gpu_monitoring,\n+            \"batch_size\": self.batch_size,\n+            \"sequence_length\": self.sequence_length,\n+            \"num_tokens_to_generate\": self.num_tokens_to_generate,\n+            \"attn_implementation\": self.attn_implementation,\n+            \"sdpa_backend\": self.sdpa_backend,\n+            \"compile_mode\": self.compile_mode,\n+            \"compile_options\": self.compile_options,\n+            \"kernelize\": self.kernelize,\n+        }\n+\n+    @classmethod\n+    def from_dict(cls, data: dict[str, Any], skip_validity_check: bool = False) -> \"BenchmarkConfig\":\n+        return cls(\n+            warmup_iterations=data.get(\"warmup_iterations\", 5),\n+            measurement_iterations=data.get(\"measurement_iterations\", 20),\n+            gpu_monitoring=data.get(\"gpu_monitoring\", False),\n+            batch_size=data.get(\"batch_size\", 1),\n+            sequence_length=data.get(\"sequence_length\", 128),\n+            num_tokens_to_generate=data.get(\"num_tokens_to_generate\", 128),\n+            attn_implementation=data.get(\"attn_implementation\", \"eager\"),\n+            sdpa_backend=data.get(\"sdpa_backend\"),\n+            compile_mode=data.get(\"compile_mode\"),\n+            compile_options=data.get(\"compile_options\"),\n+            kernelize=data.get(\"kernelize\", False),\n+            name=data.get(\"name\"),\n+            skip_validity_check=skip_validity_check,\n+        )\n+\n+\n+def cross_generate_configs(\n+    attn_impl_and_sdpa_backend: list[tuple[str, Optional[str]]],\n+    compiled_mode: list[Optional[str]],\n+    kernelized: list[bool],\n+    warmup_iterations: int = 5,\n+    measurement_iterations: int = 20,\n+    batch_size: int = 1,\n+    sequence_length: int = 128,\n+    num_tokens_to_generate: int = 128,\n+    gpu_monitoring: bool = False,  # this slows down the benchmark by a lot so we disable it by default\n+) -> list[BenchmarkConfig]:\n+    # Create kwargs common to all configs\n+    kwargs = {\n+        \"warmup_iterations\": warmup_iterations,\n+        \"measurement_iterations\": measurement_iterations,\n+        \"batch_size\": batch_size,\n+        \"sequence_length\": sequence_length,\n+        \"num_tokens_to_generate\": num_tokens_to_generate,\n+        \"gpu_monitoring\": gpu_monitoring,\n+    }\n+    # Cross-generate all combinations of attn_implementation, compiled_mode, and kernelized\n+    configs = []\n+    for attn_implementation, sdpa_backend in list(dict.fromkeys(attn_impl_and_sdpa_backend)):\n+        for cm in list(dict.fromkeys(compiled_mode)):\n+            for kernelize_on in list(dict.fromkeys(kernelized)):\n+                config = BenchmarkConfig(\n+                    attn_implementation=attn_implementation,\n+                    sdpa_backend=sdpa_backend,\n+                    compile_mode=cm,\n+                    kernelize=kernelize_on,\n+                    **kwargs,\n+                )\n+                configs.append(config)\n+    return configs\n+\n+\n+def generate_all_configs(\n+    warmup_iterations: int = 5,\n+    measurement_iterations: int = 20,\n+    batch_size: int = 1,\n+    sequence_length: int = 128,\n+    num_tokens_to_generate: int = 128,\n+    gpu_monitoring: bool = False,\n+) -> list[BenchmarkConfig]:\n+    all_attn_implementations = [\n+        (\"flash_attention_2\", None),\n+        (\"eager\", None),\n+        (\"sdpa\", \"math\"),\n+        (\"sdpa\", \"flash_attention\"),\n+        (\"flex_attention\", None),\n+    ]\n+    return cross_generate_configs(\n+        attn_impl_and_sdpa_backend=all_attn_implementations,\n+        compiled_mode=[None, \"default\", \"reduce-overhead\", \"max-autotune\", \"max-autotune-no-cudagraphs\"],\n+        kernelized=[False, KERNELIZATION_AVAILABLE],\n+        warmup_iterations=warmup_iterations,\n+        measurement_iterations=measurement_iterations,\n+        batch_size=batch_size,\n+        sequence_length=sequence_length,\n+        num_tokens_to_generate=num_tokens_to_generate,\n+        gpu_monitoring=gpu_monitoring,\n+    )\n+\n+\n+def generate_default_configs(\n+    warmup_iterations: int = 5,\n+    measurement_iterations: int = 20,\n+    batch_size: int = 1,\n+    sequence_length: int = 128,\n+    num_tokens_to_generate: int = 128,\n+    gpu_monitoring: bool = False,\n+) -> list[BenchmarkConfig]:\n+    all_attn_implementations = [\n+        (\"flash_attention_2\", None),\n+        (\"eager\", None),\n+        (\"sdpa\", \"math\"),\n+        (\"sdpa\", \"flash_attention\"),  # note: this one can fail with compile because of attn mask\n+    ]\n+    return cross_generate_configs(\n+        attn_impl_and_sdpa_backend=all_attn_implementations,\n+        compiled_mode=[None, \"max-autotune\"],\n+        kernelized=[False, KERNELIZATION_AVAILABLE],\n+        warmup_iterations=warmup_iterations,\n+        measurement_iterations=measurement_iterations,\n+        batch_size=batch_size,\n+        sequence_length=sequence_length,\n+        num_tokens_to_generate=num_tokens_to_generate,\n+        gpu_monitoring=gpu_monitoring,\n+    )"
      },
      {
        "filename": "benchmark_v2/framework/benchmark_runner.py",
        "status": "added",
        "additions": 388,
        "deletions": 0,
        "changes": 388,
        "patch": "@@ -0,0 +1,388 @@\n+import gc\n+import json\n+import logging\n+import os\n+import pathlib\n+import re\n+import time\n+from contextlib import nullcontext\n+from datetime import datetime\n+from queue import Queue\n+from typing import Any, Optional\n+\n+import torch\n+from tqdm import trange\n+\n+from transformers import (\n+    AutoModelForCausalLM,\n+    AutoTokenizer,\n+    CompileConfig,\n+    GenerationConfig,\n+    GenerationMixin,\n+)\n+from transformers.generation.streamers import BaseStreamer\n+\n+from .benchmark_config import BenchmarkConfig\n+from .data_classes import BenchmarkMetadata, BenchmarkResult, GPURawMetrics, pretty_print_dict\n+from .hardware_metrics import GPUMonitor\n+\n+\n+try:\n+    from kernels import Mode, kernelize  # noqa: F401\n+except ImportError:\n+    kernelize = None\n+    Mode = None\n+\n+\n+DEFAULT_PROMPT = \"\\n\".join([\n+    \"The French Revolution was a period of political and societal change in France that began with the Estates General of 1789 and ended with the Coup of 18 Brumaire on 9 November 1799.\",\n+    \"Many of the revolution's ideas are considered fundamental principles of liberal democracy, and its values remain central to modern French political discourse.\",\n+    \"It was caused by a combination of social, political, and economic factors which the existing regime proved unable to manage.\",\n+    \"Financial crisis and widespread social distress led to the convocation of the Estates General in May 1789, its first meeting since 1614.\",\n+    \"The representatives of the Third Estate broke away and re-constituted themselves as a National Assembly in June.\",\n+    \"The Storming of the Bastille in Paris on 14 July led to a series of radical measures by the Assembly, including the abolition of feudalism, state control over the Catholic Church in France, and issuing the Declaration of the Rights of Man and of the Citizen.\",\n+    \"The next three years were dominated by a struggle for political control.\",\n+    \"King Louis XVI's attempted flight to Varennes in June 1791 further discredited the monarchy, and military defeats after the outbreak of the French Revolutionary Wars in April 1792 led to the insurrection of 10 August 1792.\",\n+    \"As a result, the monarchy was replaced by the French First Republic in September, followed by the execution of Louis XVI himself in January 1793.\",\n+    \"After another revolt in June 1793, the constitution was suspended, and political power passed from the National Convention to the Committee of Public Safety, dominated by radical Jacobins led by Maximilien Robespierre.\",\n+    \"About 16,000 people were sentenced by the Revolutionary Tribunal and executed in the Reign of Terror, which ended in July 1794 with the Thermidorian Reaction.\",\n+    \"Weakened by external threats and internal opposition, the Committee of Public Safety was replaced in November 1795 by the Directory.\",\n+    \"Its instability ended in the coup of 18 Brumaire and the establishment of the Consulate, with Napoleon Bonaparte as First Consul.\",\n+])  # fmt: skip\n+\n+\n+def compact_json_numeric_arrays(data: dict):\n+    # Match arrays that contain only numbers (ints/floats), whitespace, commas, and newlines\n+    pattern = r\"\\[\\s*\\n\\s*((?:\\d+(?:\\.\\d+)?\\s*,\\s*)*\\d+(?:\\.\\d+)?)\\s*\\n\\s*\\]\"\n+\n+    def replace_numeric_array(match):\n+        # Get the array content\n+        content = match.group(1)\n+        # Remove extra whitespace but keep commas\n+        compact_content = re.sub(r\"\\s+\", \" \", content).strip()\n+        return f\"[{compact_content}]\"\n+\n+    return re.sub(pattern, replace_numeric_array, json.dumps(data, indent=4, default=str), flags=re.DOTALL)\n+\n+\n+def get_git_revision() -> str:\n+    base_path = pathlib.Path(__file__).parent.parent.parent\n+    git_dir = base_path / \".git\"\n+    with (git_dir / \"HEAD\").open(\"r\") as head:\n+        ref = head.readline().split(\" \")[-1].strip()\n+    with (git_dir / ref).open(\"r\") as git_hash:\n+        return git_hash.readline().strip()\n+\n+\n+def get_sdpa_backend(backend_name: Optional[str]) -> Optional[torch.nn.attention.SDPBackend]:\n+    \"\"\"Get the SDPA backend enum from string name.\"\"\"\n+    if backend_name is None:\n+        return None\n+\n+    try:\n+        backend_map = {\n+            \"math\": torch.nn.attention.SDPBackend.MATH,\n+            \"flash_attention\": torch.nn.attention.SDPBackend.FLASH_ATTENTION,\n+            \"efficient_attention\": torch.nn.attention.SDPBackend.EFFICIENT_ATTENTION,\n+            \"cudnn_attention\": torch.nn.attention.SDPBackend.CUDNN_ATTENTION,\n+        }\n+        return backend_map.get(backend_name.lower())\n+    except AttributeError:\n+        # torch.nn.attention.SDPBackend not available in older torch versions\n+        return None\n+\n+\n+def flush_memory():\n+    \"\"\"Flush GPU memory and run garbage collection.\"\"\"\n+    gc.collect()\n+    # Dynamo resets\n+    torch._dynamo.reset()\n+    torch._dynamo.reset_code_caches()\n+    if hasattr(torch._inductor, \"codecache\"):\n+        # Clear FX graph cache\n+        if hasattr(torch._inductor.codecache, \"FxGraphCache\"):\n+            torch._inductor.codecache.FxGraphCache.clear()\n+        # Clear PyCodeCache\n+        if hasattr(torch._inductor.codecache, \"PyCodeCache\"):\n+            torch._inductor.codecache.PyCodeCache.cache_clear()\n+        # Clear TritonFuture cache (for async compilation)\n+        if hasattr(torch._inductor.codecache, \"TritonFuture\"):\n+            if hasattr(torch._inductor.codecache.TritonFuture, \"_compile_cache\"):\n+                torch._inductor.codecache.TritonFuture._compile_cache.clear()\n+    # Clear CUDA cache\n+    if torch.cuda.is_available():\n+        torch.cuda.empty_cache()\n+        torch.cuda.reset_max_memory_allocated()\n+        torch.cuda.reset_peak_memory_stats()\n+        torch.cuda.synchronize()\n+    gc.collect()\n+\n+\n+class BenchmarkStreamer(BaseStreamer):\n+    def __init__(self, **kwargs) -> None:\n+        self.timestamps = []\n+        self.text_queue = Queue()\n+\n+    def put(self, value):\n+        \"\"\"Receives tokens and logs the timestamp of the generation.\"\"\"\n+        self.timestamps.append(time.perf_counter())\n+\n+    def end(self):\n+        self.timestamps.append(time.perf_counter())\n+\n+    def __iter__(self):\n+        return self\n+\n+    def __next__(self):\n+        value = self.text_queue.get(timeout=self.timeout)\n+        if value == self.stop_signal:\n+            raise StopIteration()\n+        else:\n+            return value\n+\n+\n+class BenchmarkRunner:\n+    \"\"\"Main benchmark runner that coordinates benchmark execution.\"\"\"\n+\n+    def __init__(\n+        self, logger: logging.Logger, output_dir: str = \"benchmark_results\", commit_id: Optional[str] = None\n+    ) -> None:\n+        # Those stay constant for the whole run\n+        self.logger = logger\n+        self.output_dir = output_dir\n+        self.commit_id = get_git_revision() if commit_id is None else commit_id\n+        os.makedirs(self.output_dir, exist_ok=True)\n+        self.profile_dir = None\n+        # Attributes that are reset for each model\n+        self._setup_for = \"\"\n+        # Attributes that are reset for each run\n+        self.model: Optional[GenerationMixin] = None\n+\n+    def cleanup(self) -> None:\n+        del self.model\n+        self.model = None\n+        flush_memory()\n+\n+    def setup_one_run(self, model_id: str, config: BenchmarkConfig) -> None:\n+        # Some attributes only need to be set once per model\n+        if self._setup_for != model_id:\n+            self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n+            # We set the EOS token to the padding token for open-ended generation\n+            self.tokenizer.eos_token = self.tokenizer.pad_token\n+            self._setup_for = model_id\n+\n+        # Prepare inputs\n+        self.inputs = self.tokenizer(\n+            [DEFAULT_PROMPT for _ in range(config.batch_size)],\n+            return_tensors=\"pt\",\n+            max_length=config.sequence_length,\n+            truncation=True,\n+            return_attention_mask=True,\n+        ).to(config.device)\n+        self.inputs[\"use_cache\"] = True\n+\n+        # Prepare generation config\n+        gen_config = GenerationConfig(\n+            do_sample=False, top_p=1.0, temperature=1.0, max_new_tokens=config.num_tokens_to_generate\n+        )\n+\n+        # Prepare compile config\n+        if config.compile_mode is not None:\n+            gen_config.compile_config = CompileConfig(mode=config.compile_mode, options=config.compile_options)\n+            gen_config.cache_implementation = \"static\"\n+\n+        # Load model\n+        self.logger.debug(f\"Loading model {model_id} on device {config.device}...\")\n+        dtype = getattr(torch, config.dtype.removeprefix(\"torch.\"))\n+        self.model = AutoModelForCausalLM.from_pretrained(\n+            model_id, dtype=dtype, attn_implementation=config.attn_implementation, generation_config=gen_config\n+        )\n+        self.model = self.model.eval().to(config.device)\n+\n+        # Kernelize the model if needed\n+        if config.kernelize:\n+            self.model = kernelize(self.model, mode=Mode.INFERENCE)\n+\n+    def run_one_benchmark(self, model_id: str, config: BenchmarkConfig, num_tokens_to_profile: int = 0) -> None:\n+        sdpa_ctx = nullcontext()\n+        if config.attn_implementation == \"sdpa\":\n+            sdpa_backend = get_sdpa_backend(config.sdpa_backend)\n+            sdpa_ctx = torch.nn.attention.sdpa_kernel(sdpa_backend)\n+\n+        with sdpa_ctx, torch.no_grad():\n+            self.logger.info(f\"Running benchmark scenario: {config.name}\")\n+\n+            # Quick validation: try one measurement first to see if this scenario works\n+            flush_memory()\n+            e2e_latency, token_generation_times, decoded_output, gpu_metrics = self.time_generate(\n+                max_new_tokens=1, gpu_monitor=None\n+            )\n+            if e2e_latency < 0:\n+                self.logger.warning(f\"Skipping config {config.name}: {e2e_latency = } (no GPU monitoring)\")\n+                return None\n+\n+            # Warmup runs\n+            self.logger.info(f\"Warming up with {config.warmup_iterations} iterations...\")\n+            for _ in trange(config.warmup_iterations):\n+                _ = self.time_generate(max_new_tokens=config.num_tokens_to_generate)\n+            self.logger.info(\"Warmup over.\")\n+\n+            # Measurement runs\n+            result = BenchmarkResult()\n+            self.logger.info(f\"Benchmarking with {config.measurement_iterations} iterations.\")\n+            for _ in trange(config.measurement_iterations):\n+                e2e_latency, token_generation_times, decoded_output, gpu_metrics = self.time_generate(\n+                    max_new_tokens=config.num_tokens_to_generate,\n+                    gpu_monitor=(GPUMonitor(logger=self.logger) if config.gpu_monitoring else None),\n+                )\n+                result.accumulate(e2e_latency, token_generation_times, decoded_output, gpu_metrics)\n+            self.logger.info(\"Benchmarking done. Cleaning up.\")\n+\n+            # Profile if needed\n+            if num_tokens_to_profile > 0:\n+                self.profile_generate(num_tokens_to_profile, config.name)\n+\n+            return {\n+                \"metadata\": BenchmarkMetadata(model_id=model_id, commit_id=self.commit_id),\n+                \"measurements\": result,\n+                \"config\": config,\n+            }\n+\n+    def time_generate(\n+        self,\n+        max_new_tokens: int,\n+        gpu_monitor: Optional[GPUMonitor] = None,\n+    ) -> tuple[float, list[float], str, Optional[GPURawMetrics]]:\n+        \"\"\"Time the latency of a call to model.generate() with the given (inputs) and (max_new_tokens).\"\"\"\n+        # Prepare gpu monitoring if needed\n+        if gpu_monitor is not None:\n+            gpu_monitor.start()\n+        # Prepare streamer\n+        streamer = BenchmarkStreamer()\n+        # Generate and time\n+        wall_time_0 = time.perf_counter()\n+        outputs = self.model.generate(\n+            **self.inputs,\n+            max_new_tokens=max_new_tokens,\n+            streamer=streamer,\n+        )\n+        wall_time_1 = time.perf_counter()\n+        # Stop gpu monitoring if needed\n+        gpu_metrics = gpu_monitor.stop_and_collect() if gpu_monitor is not None else None\n+        # Check if generation had the right number of tokens\n+        input_tokens = self.inputs[\"input_ids\"].size(-1)\n+        batch_size, output_tokens = outputs.shape\n+        new_tokens = output_tokens - input_tokens\n+        if new_tokens != max_new_tokens:\n+            raise RuntimeError(f\"Generated {new_tokens} tokens, expected {max_new_tokens}\")\n+        # Decode outputs\n+        decoded_output = self.tokenizer.decode(outputs[0, input_tokens:], skip_special_tokens=True)\n+        # Compute intermediate quantities\n+        e2e_latency = wall_time_1 - wall_time_0\n+        token_generation_times = [t - wall_time_0 for t in streamer.timestamps[1:]]\n+        return e2e_latency, token_generation_times, decoded_output, gpu_metrics\n+\n+    def profile_generate(self, num_tokens_to_profile: int, config_name: str) -> None:\n+        \"\"\"Profile the latency of a call to model.generate() with the given (inputs) and (max_new_tokens).\"\"\"\n+        profiler = torch.profiler.profile(\n+            activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],\n+            record_shapes=True,\n+        )\n+        with profiler as prof:\n+            _ = self.model.generate(\n+                **self.inputs,\n+                max_new_tokens=num_tokens_to_profile,\n+            )\n+        if self.profile_dir is None:\n+            self.profile_dir = self.output_dir + \"_profiles\"\n+            os.makedirs(self.profile_dir, exist_ok=True)\n+        prof.export_chrome_trace(f\"{self.profile_dir}/{config_name}.json\")\n+\n+    def run_benchmarks(\n+        self,\n+        model_id: str,\n+        benchmark_configs: list[BenchmarkConfig],\n+        num_tokens_to_profile: int = 0,\n+        pretty_print_summary: bool = True,\n+    ) -> dict[str, Any]:\n+        all_results = {}\n+        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n+        start_time = time.perf_counter()\n+\n+        n_configs = len(benchmark_configs)\n+        for i, config in enumerate(benchmark_configs):\n+            # Handle SDPA backend if not determined by the config (needs to be done before skipping duplicates)\n+            if config.attn_implementation == \"sdpa\" and config.sdpa_backend is None:\n+                default_backend = \"flash_attention\"  # FIXME: torch has a _cur_sdpa_kernel_backends but it fails\n+                self.logger.warning(f\"No SDPA backend provided, using {default_backend} instead.\")\n+                config.sdpa_backend = default_backend\n+\n+            # Skip if already run\n+            if config.hash in all_results:\n+                self.logger.info(f\"Skipping duplicate config {config.name} for model {model_id} ({i + 1}/{n_configs})\")\n+                continue\n+\n+            # Otherwise, run the benchmark\n+            self.setup_one_run(model_id, config)\n+            self.logger.info(\n+                f\"Running benchmark of model {model_id} with scenario: {config.name} ({i + 1}/{n_configs})\"\n+            )\n+\n+            # Launch benchmark in a try/except block to avoid stopping the whole run if one benchmark fails\n+            try:\n+                results = self.run_one_benchmark(model_id, config, num_tokens_to_profile)\n+                if results is not None:\n+                    all_results[config.hash] = results\n+\n+            except Exception as e:\n+                self.logger.error(f\"Error running with scenario: {config.name}:\\n{repr(e)}\")\n+            # Cleanup model and save results\n+            self.cleanup()\n+            self.save_results(model_id, all_results, timestamp=timestamp)\n+\n+        if pretty_print_summary:\n+            print()\n+            print(\"=\" * 100)\n+            print(f\"Finished benchmarks in {time.perf_counter() - start_time:.2f} seconds\")\n+            print(f\"Total number of benchmarks: {len(all_results)}\")\n+            if len(all_results) > 0:\n+                print(\"First run metadata:\")\n+                first_key = list(all_results.keys())[0]\n+                first_metadata = all_results[first_key][\"metadata\"].to_dict()\n+                hardware_info = first_metadata.pop(\"hardware_info\")\n+                pretty_print_dict(first_metadata | hardware_info, tabs=1)\n+            for value in all_results.values():\n+                print(\"=\" * 100)\n+                print(f\"Config: {value['config'].infer_name(compact=False)}\\n\")\n+                value[\"measurements\"].pprint(tabs=1)\n+            print(\"=\" * 100)\n+\n+        return all_results\n+\n+    def save_results(self, model_name: str, results: dict, timestamp: str = \"\") -> str:\n+        \"\"\"Save benchmark results to JSON file.\"\"\"\n+        # Create model-specific subdirectory\n+        model_name = model_name.replace(\"/\", \"_\")\n+        model_dir = os.path.join(self.output_dir, model_name)\n+        os.makedirs(model_dir, exist_ok=True)\n+\n+        # Create filename with timestamp\n+        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\") if not timestamp else timestamp\n+        filename = f\"{model_name}_benchmark_{timestamp}.json\"\n+        filepath = os.path.join(model_dir, filename)\n+\n+        # Convert results to dict\n+        converted_results = {}\n+        for cfg_hash in results.keys():\n+            converted_results[cfg_hash] = {\n+                \"metadata\": results[cfg_hash][\"metadata\"].to_dict(),\n+                \"measurements\": results[cfg_hash][\"measurements\"].to_dict(),\n+                \"config\": results[cfg_hash][\"config\"].to_dict(),\n+            }\n+\n+        # Save to JSON file\n+        with open(filepath, \"w\") as f:\n+            f.write(compact_json_numeric_arrays(converted_results))\n+\n+        self.logger.info(f\"Results saved to {filepath}\")\n+        return filepath"
      },
      {
        "filename": "benchmark_v2/framework/data_classes.py",
        "status": "added",
        "additions": 152,
        "deletions": 0,
        "changes": 152,
        "patch": "@@ -0,0 +1,152 @@\n+from dataclasses import dataclass\n+from datetime import datetime\n+from typing import Any, Optional, Union\n+\n+import numpy as np\n+\n+from .hardware_metrics import GPURawMetrics, HardwareInfo\n+\n+\n+def compute_basic_statistics(measurements: list[float]) -> dict[str, float]:\n+    return {\n+        \"avg\": np.mean(measurements),\n+        \"std\": np.std(measurements),\n+        \"min\": np.min(measurements),\n+        \"med\": np.median(measurements),\n+        \"max\": np.max(measurements),\n+        \"p95\": np.percentile(measurements, 95),\n+    }\n+\n+\n+def add_unit_to_duration(stats: dict[str, float]) -> dict[str, str]:\n+    for key in list(stats.keys()):\n+        value = stats[key]\n+        if value > 3600:\n+            stats[key] = f\"{(value / 3600):.2f}hr\"\n+        elif value > 60:\n+            stats[key] = f\"{(value / 60):.2f}min\"\n+        elif value > 1:\n+            stats[key] = f\"{value:.2f}s\"\n+        elif value > 1e-3:\n+            stats[key] = f\"{(value * 1e3):.2f}ms\"\n+        elif value > 1e-6:\n+            stats[key] = f\"{(value * 1e6):.2f}us\"\n+        else:\n+            stats[key] = f\"{(value * 1e9):.2f}ns\"\n+    return stats\n+\n+\n+def equalize_lengths_and_collate(stats: list[dict[str, str]]) -> list[str]:\n+    keys = [\"avg\", \"std\", \"min\", \"med\", \"max\", \"p95\"]\n+    for key in keys:\n+        max_length = max(len(stat[key]) for stat in stats)\n+        for stat in stats:\n+            stat[key] = stat[key].ljust(max_length, \" \")\n+    return [\" \".join([f\"{key}={stat[key]}\" for key in keys]) for stat in stats]\n+\n+\n+def pretty_print_dict(data: dict[str, Any], tabs: int = 0) -> None:\n+    max_key_length = max([len(key) for key in data.keys()])\n+    for key, value in data.items():\n+        tabs_str = \"  \" * tabs\n+        padded_key = key.ljust(max_key_length + 1, \".\")\n+        print(f\"{tabs_str}{padded_key}: {value}\")\n+\n+\n+@dataclass\n+class BenchmarkMetadata:\n+    \"\"\"Metadata collected for each benchmark run.\"\"\"\n+\n+    model_id: str\n+    timestamp: str\n+    commit_id: str\n+    hardware_info: HardwareInfo\n+\n+    def __init__(self, model_id: str, commit_id: str):\n+        self.model_id = model_id\n+        self.timestamp = datetime.utcnow().isoformat()\n+        self.commit_id = commit_id\n+        self.hardware_info = HardwareInfo()\n+\n+    def to_dict(self) -> dict[str, Any]:\n+        return {\n+            \"timestamp\": self.timestamp,\n+            \"commit_id\": self.commit_id,\n+            \"hardware_info\": self.hardware_info.to_dict(),\n+        }\n+\n+\n+class BenchmarkResult:\n+    \"\"\"Result from a series of benchmark runs.\"\"\"\n+\n+    def __init__(self) -> None:\n+        self.e2e_latency = []\n+        self.token_generation_times = []  # time at which each token was generated (relative to start of the generation)\n+        self.decoded_outputs = []\n+        self.gpu_metrics = []\n+\n+    def accumulate(\n+        self,\n+        e2e_latency: float,\n+        token_generation_times: list[float],\n+        decoded_output: str,\n+        gpu_metrics: Optional[GPURawMetrics],\n+    ) -> None:\n+        self.e2e_latency.append(e2e_latency)\n+        self.token_generation_times.append(token_generation_times)\n+        self.decoded_outputs.append(decoded_output)\n+        self.gpu_metrics.append(gpu_metrics)\n+\n+    def to_dict(self) -> dict[str, Union[None, int, float]]:\n+        # Save GPU metrics as None if it contains only None values\n+        if all(gm is None for gm in self.gpu_metrics):\n+            gpu_metrics = None\n+        else:\n+            gpu_metrics = [gm.to_dict() for gm in self.gpu_metrics]\n+        return {\n+            \"e2e_latency\": self.e2e_latency,\n+            \"token_generation_times\": self.token_generation_times,\n+            \"decoded_outputs\": self.decoded_outputs,\n+            \"gpu_metrics\": gpu_metrics,\n+        }\n+\n+    @classmethod\n+    def from_dict(cls, data: dict[str, Union[None, int, float]]) -> \"BenchmarkResult\":\n+        # Handle GPU metrics, which is saved as None if it contains only None values\n+        if data[\"gpu_metrics\"] is None:\n+            gpu_metrics = [None for _ in range(len(data[\"e2e_latency\"]))]\n+        else:\n+            gpu_metrics = [GPURawMetrics.from_dict(gm) for gm in data[\"gpu_metrics\"]]\n+        # Create a new instance and accumulate the data\n+        new_instance = cls()\n+        for i in range(len(data[\"e2e_latency\"])):\n+            new_instance.accumulate(\n+                e2e_latency=data[\"e2e_latency\"][i],\n+                token_generation_times=data[\"token_generation_times\"][i],\n+                decoded_output=data[\"decoded_output\"][i],\n+                gpu_metrics=gpu_metrics[i],\n+            )\n+        return new_instance\n+\n+    def get_measured_ttft(self) -> list[float]:\n+        return [dt[0] for dt in self.token_generation_times if len(dt) > 0]\n+\n+    def get_measured_itl(self) -> list[float]:\n+        return [(dt[-1] - dt[0]) / (len(dt) - 1) for dt in self.token_generation_times if len(dt) > 1]\n+\n+    def pprint(self, tabs: int = 0) -> None:\n+        collated_stats = equalize_lengths_and_collate(\n+            [\n+                add_unit_to_duration(compute_basic_statistics(self.e2e_latency)),\n+                add_unit_to_duration(compute_basic_statistics(self.get_measured_ttft())),\n+                add_unit_to_duration(compute_basic_statistics(self.get_measured_itl())),\n+            ]\n+        )\n+        pretty_print_dict(\n+            {\n+                \"E2E Latency\": collated_stats[0],\n+                \"Time to First Token\": collated_stats[1],\n+                \"Inter-Token Latency\": collated_stats[2],\n+            },\n+            tabs=tabs,\n+        )"
      },
      {
        "filename": "benchmark_v2/framework/hardware_metrics.py",
        "status": "added",
        "additions": 172,
        "deletions": 0,
        "changes": 172,
        "patch": "@@ -0,0 +1,172 @@\n+import json\n+import logging\n+import subprocess\n+import sys\n+import threading\n+import time\n+from dataclasses import dataclass\n+from enum import Enum\n+from logging import Logger\n+from typing import Optional, Union\n+\n+import gpustat\n+import psutil\n+import torch\n+\n+\n+# Data class to hold the hardware information\n+def get_device_name_and_memory_total() -> tuple[str, float]:\n+    \"\"\"Returns the name and memory total of GPU 0.\"\"\"\n+    device_name = torch.cuda.get_device_properties(0).name\n+    device_memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n+    return device_name, device_memory_total\n+\n+\n+class HardwareInfo:\n+    \"\"\"A class to hold information about the hardware.\"\"\"\n+\n+    def __init__(self) -> None:\n+        # Retrieve GPU stats\n+        try:\n+            self.gpu_name, self.gpu_memory_total_gb = get_device_name_and_memory_total()\n+        except Exception:\n+            self.gpu_name, self.gpu_memory_total_gb = None, None\n+        # Retrieve python, torch and CUDA version\n+        self.python_version = f\"{sys.version.split()[0]}\"\n+        self.torch_version = torch.__version__\n+        if hasattr(torch, \"cuda\") and torch.cuda.is_available():\n+            self.cuda_version = torch.version.cuda\n+        else:\n+            self.cuda_version = None\n+        # Retrieve general hardware information\n+        self.cpu_count = psutil.cpu_count()\n+        self.memory_total_mb = int(psutil.virtual_memory().total / (1024 * 1024))\n+\n+    def to_dict(self) -> dict[str, Union[None, int, float, str]]:\n+        return {\n+            \"gpu_name\": self.gpu_name,\n+            \"gpu_memory_total_gb\": self.gpu_memory_total_gb,\n+            \"python_version\": self.python_version,\n+            \"torch_version\": self.torch_version,\n+        }\n+\n+\n+# Functions to get information about the GPU\n+def get_amd_gpu_stats() -> tuple[int, float]:\n+    \"\"\"Returns the utilization and memory used of an AMD GPU, both in percent\"\"\"\n+    rocm_smi_output = subprocess.check_output([\"rocm-smi\", \"--json\", \"--showuse\", \"--showmeminfo\", \"VRAM\"])\n+    gpu_stats = json.loads(rocm_smi_output.decode(\"utf-8\"))\n+    gpu_stats = [\n+        (card_id, stats[\"GPU use (%)\"], stats[\"VRAM Total Used Memory (B)\"]) for card_id, stats in gpu_stats.items()\n+    ]\n+    gpu_stats.sort(key=lambda x: x[1], reverse=True)\n+    return int(gpu_stats[0][1]), float(gpu_stats[0][2]) / 1024**3\n+\n+\n+def get_nvidia_gpu_stats() -> tuple[int, float]:\n+    \"\"\"Returns the utilization and memory used of an NVIDIA GPU, both in percent\"\"\"\n+    gpu_stats = gpustat.GPUStatCollection.new_query()\n+    gpu_stats = gpu_stats[0]\n+    return int(gpu_stats[\"utilization.gpu\"]), float(gpu_stats[\"memory.used\"]) / 1024**3\n+\n+\n+class GPUStatsCollector:\n+    \"\"\"A class to get statistics about the GPU. It serves as a wrapper that holds the GPU total memory and its name,\n+    which is used to call the right function to get the utilization and memory used.\"\"\"\n+\n+    def __init__(self) -> None:\n+        self.device_name, self.device_memory_total = get_device_name_and_memory_total()\n+        # Monkey patch the get_utilization_and_memory_used method based on the GPU type\n+        if \"amd\" in self.device_name.lower():\n+            self.get_utilization_and_memory_used = get_amd_gpu_stats\n+        elif \"nvidia\" in self.device_name.lower():\n+            self.get_utilization_and_memory_used = get_nvidia_gpu_stats\n+        else:\n+            raise RuntimeError(f\"Unsupported GPU: {self.device_name}\")\n+\n+    def get_measurements(self) -> tuple[int, float]:\n+        \"\"\"Get the utilization and memory used of the GPU, both in percent\"\"\"\n+        raise NotImplementedError(\"This method is meant to be monkey patched during __init__\")\n+\n+\n+# Simple data classes to hold the raw GPU metrics\n+class GPUMonitoringStatus(Enum):\n+    \"\"\"Status of GPU monitoring.\"\"\"\n+\n+    SUCCESS = \"success\"\n+    FAILED = \"failed\"\n+    NO_GPUS_AVAILABLE = \"no_gpus_available\"\n+    NO_SAMPLES_COLLECTED = \"no_samples_collected\"\n+\n+\n+@dataclass\n+class GPURawMetrics:\n+    \"\"\"Raw values for GPU utilization and memory used.\"\"\"\n+\n+    utilization: list[float]  # in percent\n+    memory_used: list[float]  # in GB\n+    timestamps: list[float]  # in seconds\n+    timestamp_0: float  # in seconds\n+    monitoring_status: GPUMonitoringStatus\n+\n+    def to_dict(self) -> dict[str, Union[None, int, float, str]]:\n+        return {\n+            \"utilization\": self.utilization,\n+            \"memory_used\": self.memory_used,\n+            \"timestamps\": self.timestamps,\n+            \"timestamp_0\": self.timestamp_0,\n+            \"monitoring_status\": self.monitoring_status.value,\n+        }\n+\n+\n+# Main class, used to monitor the GPU utilization during benchmark execution\n+class GPUMonitor:\n+    \"\"\"Monitor GPU utilization during benchmark execution.\"\"\"\n+\n+    def __init__(self, sample_interval_sec: float = 0.1, logger: Optional[Logger] = None):\n+        self.sample_interval_sec = sample_interval_sec\n+        self.logger = logger if logger is not None else logging.getLogger(__name__)\n+\n+        self.num_available_gpus = torch.cuda.device_count()\n+        if self.num_available_gpus == 0:\n+            raise RuntimeError(\"No GPUs detected by torch.cuda.device_count().\")\n+        self.gpu_stats_getter = GPUStatsCollector()\n+\n+    def start(self):\n+        \"\"\"Start monitoring GPU metrics.\"\"\"\n+        # Clear the stop event to enable monitoring\n+        self.stop_event = threading.Event()\n+        self.gpu_utilization = []\n+        self.gpu_memory_used = []\n+        self.timestamps = []\n+        self.thread = threading.Thread(target=self._monitor_loop)\n+        self.thread.start()\n+        self.logger.debug(\"GPU monitoring started\")\n+\n+    def stop_and_collect(self) -> GPURawMetrics:\n+        \"\"\"Stop monitoring and return collected metrics.\"\"\"\n+        self.stop_event.set()\n+        self.thread.join()\n+        if self.gpu_utilization:\n+            timestamp_0 = self.timestamps[0]\n+            metrics = GPURawMetrics(\n+                utilization=self.gpu_utilization,\n+                memory_used=self.gpu_memory_used,\n+                timestamps=[t - timestamp_0 for t in self.timestamps],\n+                timestamp_0=timestamp_0,\n+                monitoring_status=GPUMonitoringStatus.SUCCESS,\n+            )\n+            self.logger.debug(f\"GPU monitoring completed: {len(self.gpu_utilization)} samples collected\")\n+        else:\n+            metrics = GPURawMetrics(monitoring_status=GPUMonitoringStatus.NO_SAMPLES_COLLECTED)\n+        return metrics\n+\n+    def _monitor_loop(self):\n+        \"\"\"Background monitoring loop using threading.Event for communication.\"\"\"\n+        while not self.stop_event.is_set():\n+            utilization, memory_used = self.gpu_stats_getter.get_utilization_and_memory_used()\n+            self.gpu_utilization.append(utilization)\n+            self.gpu_memory_used.append(memory_used)\n+            self.timestamps.append(time.time())\n+            if self.stop_event.wait(timeout=self.sample_interval_sec):\n+                break"
      },
      {
        "filename": "benchmark_v2/run_benchmarks.py",
        "status": "modified",
        "additions": 68,
        "deletions": 452,
        "changes": 520,
        "patch": "@@ -19,477 +19,93 @@\n \"\"\"\n \n import argparse\n-import importlib.util\n-import json\n import logging\n-import os\n+import random\n import sys\n import uuid\n-from datetime import datetime\n-from pathlib import Path\n-from typing import Any, Optional\n \n+from framework.benchmark_config import BenchmarkConfig, generate_all_configs\n+from framework.benchmark_runner import BenchmarkRunner\n \n-def setup_logging(log_level: str = \"INFO\", enable_file_logging: bool = False) -> logging.Logger:\n-    \"\"\"Setup logging configuration.\"\"\"\n-    numeric_level = getattr(logging, log_level.upper(), None)\n-    if not isinstance(numeric_level, int):\n-        raise ValueError(f\"Invalid log level: {log_level}\")\n-\n-    handlers = [logging.StreamHandler(sys.stdout)]\n-\n-    if enable_file_logging:\n-        handlers.append(logging.FileHandler(f\"benchmark_run_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"))\n-\n-    logging.basicConfig(\n-        level=numeric_level, format=\"[%(levelname)s - %(asctime)s] %(name)s: %(message)s\", handlers=handlers\n-    )\n-\n-    return logging.getLogger(__name__)\n-\n-\n-def discover_benchmarks(benches_dir: str) -> list[dict[str, Any]]:\n-    \"\"\"\n-    Discover all benchmark modules in the benches directory.\n-\n-    Returns:\n-        List of dictionaries containing benchmark module info\n-    \"\"\"\n-    benchmarks = []\n-    benches_path = Path(benches_dir)\n-\n-    if not benches_path.exists():\n-        raise FileNotFoundError(f\"Benches directory not found: {benches_dir}\")\n-\n-    for py_file in benches_path.glob(\"*.py\"):\n-        if py_file.name.startswith(\"__\"):\n-            continue\n-\n-        module_name = py_file.stem\n-\n-        try:\n-            # Import the module\n-            spec = importlib.util.spec_from_file_location(module_name, py_file)\n-            module = importlib.util.module_from_spec(spec)\n-            spec.loader.exec_module(module)\n-\n-            # Check if it has a benchmark runner function\n-            if hasattr(module, f\"run_{module_name}\"):\n-                benchmarks.append(\n-                    {\n-                        \"name\": module_name,\n-                        \"path\": str(py_file),\n-                        \"module\": module,\n-                        \"runner_function\": getattr(module, f\"run_{module_name}\"),\n-                    }\n-                )\n-            elif hasattr(module, \"run_benchmark\"):\n-                benchmarks.append(\n-                    {\n-                        \"name\": module_name,\n-                        \"path\": str(py_file),\n-                        \"module\": module,\n-                        \"runner_function\": getattr(module, \"run_benchmark\"),\n-                    }\n-                )\n-            else:\n-                logging.warning(f\"No runner function found in {py_file}\")\n-\n-        except Exception as e:\n-            logging.error(f\"Failed to import {py_file}: {e}\")\n-\n-    return benchmarks\n-\n-\n-def run_single_benchmark(\n-    benchmark_info: dict[str, Any], output_dir: str, logger: logging.Logger, **kwargs\n-) -> Optional[str]:\n-    \"\"\"\n-    Run a single benchmark and return the output file path.\n-\n-    Args:\n-        benchmark_info: Dictionary containing benchmark module info\n-        output_dir: Base output directory\n-        logger: Logger instance\n-        **kwargs: Additional arguments to pass to the benchmark\n-\n-    Returns:\n-        Path to the output file if successful, None otherwise\n-    \"\"\"\n-    benchmark_name = benchmark_info[\"name\"]\n-    runner_func = benchmark_info[\"runner_function\"]\n-\n-    logger.info(f\"Running benchmark: {benchmark_name}\")\n-\n-    try:\n-        # Check function signature to determine what arguments to pass\n-        import inspect\n-\n-        sig = inspect.signature(runner_func)\n-\n-        # Prepare arguments based on function signature\n-        func_kwargs = {\"logger\": logger, \"output_dir\": output_dir}\n-\n-        # Add other kwargs if the function accepts them\n-        for param_name in sig.parameters:\n-            if param_name in kwargs:\n-                func_kwargs[param_name] = kwargs[param_name]\n-\n-        # Filter kwargs to only include parameters the function accepts\n-        # If function has **kwargs, include all provided kwargs\n-        has_var_kwargs = any(param.kind == param.VAR_KEYWORD for param in sig.parameters.values())\n-        if has_var_kwargs:\n-            valid_kwargs = {**func_kwargs, **kwargs}\n-        else:\n-            valid_kwargs = {k: v for k, v in func_kwargs.items() if k in sig.parameters}\n-\n-        # Run the benchmark\n-        result = runner_func(**valid_kwargs)\n-\n-        if isinstance(result, str):\n-            # Function returned a file path\n-            return result\n-        else:\n-            logger.info(f\"Benchmark {benchmark_name} completed successfully\")\n-            return \"completed\"\n-\n-    except Exception as e:\n-        logger.error(f\"Benchmark {benchmark_name} failed: {e}\")\n-        import traceback\n-\n-        logger.debug(traceback.format_exc())\n-        return None\n-\n-\n-def generate_summary_report(\n-    output_dir: str,\n-    benchmark_results: dict[str, Any],\n-    logger: logging.Logger,\n-    benchmark_run_uuid: Optional[str] = None,\n-) -> str:\n-    \"\"\"Generate a summary report of all benchmark runs.\"\"\"\n-    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n-    summary_file = os.path.join(output_dir, f\"benchmark_summary_{timestamp}.json\")\n-\n-    summary_data = {\n-        \"run_metadata\": {\n-            \"timestamp\": datetime.utcnow().isoformat(),\n-            \"benchmark_run_uuid\": benchmark_run_uuid,\n-            \"total_benchmarks\": len(benchmark_results),\n-            \"successful_benchmarks\": len([r for r in benchmark_results.values() if r is not None]),\n-            \"failed_benchmarks\": len([r for r in benchmark_results.values() if r is None]),\n-        },\n-        \"benchmark_results\": benchmark_results,\n-        \"output_directory\": output_dir,\n-    }\n-\n-    with open(summary_file, \"w\") as f:\n-        json.dump(summary_data, f, indent=2, default=str)\n-\n-    logger.info(f\"Summary report saved to: {summary_file}\")\n-    return summary_file\n-\n-\n-def upload_results_to_hf_dataset(\n-    output_dir: str,\n-    summary_file: str,\n-    dataset_name: str,\n-    run_id: Optional[str] = None,\n-    token: Optional[str] = None,\n-    logger: Optional[logging.Logger] = None,\n-) -> Optional[str]:\n-    \"\"\"\n-    Upload benchmark results to a HuggingFace Dataset.\n-    Based on upload_collated_report() from utils/collated_reports.py\n-    Args:\n-        output_dir: Local output directory containing results\n-        summary_file: Path to the summary file\n-        dataset_name: Name of the HuggingFace dataset to upload to\n-        run_id: Unique run identifier (if None, will generate one)\n-        token: HuggingFace token for authentication (if None, will use environment variables)\n-        logger: Logger instance\n-    Returns:\n-        The run_id used for the upload, None if upload failed\n-    \"\"\"\n-    if logger is None:\n-        logger = logging.getLogger(__name__)\n-\n-    import os\n-\n-    from huggingface_hub import HfApi\n-\n-    api = HfApi()\n-\n-    if run_id is None:\n-        github_run_number = os.getenv(\"GITHUB_RUN_NUMBER\")\n-        github_run_id = os.getenv(\"GITHUB_RUN_ID\")\n-        if github_run_number and github_run_id:\n-            run_id = f\"{github_run_number}-{github_run_id}\"\n-\n-    date_folder = datetime.now().strftime(\"%Y-%m-%d\")\n-\n-    github_event_name = os.getenv(\"GITHUB_EVENT_NAME\")\n-    if github_event_name != \"schedule\":\n-        # Non-scheduled runs go under a runs subfolder\n-        repo_path = f\"{date_folder}/runs/{run_id}/benchmark_results\"\n-    else:\n-        # Scheduled runs go directly under the date\n-        repo_path = f\"{date_folder}/{run_id}/benchmark_results\"\n-\n-    logger.info(f\"Uploading benchmark results to dataset '{dataset_name}' at path '{repo_path}'\")\n-\n-    try:\n-        # Upload all files in the output directory\n-        from pathlib import Path\n-\n-        output_path = Path(output_dir)\n-\n-        for file_path in output_path.rglob(\"*\"):\n-            if file_path.is_file():\n-                # Calculate relative path from output_dir\n-                relative_path = file_path.relative_to(output_path)\n-                path_in_repo = f\"{repo_path}/{relative_path}\"\n-\n-                logger.debug(f\"Uploading {file_path} to {path_in_repo}\")\n-\n-                api.upload_file(\n-                    path_or_fileobj=str(file_path),\n-                    path_in_repo=path_in_repo,\n-                    repo_id=dataset_name,\n-                    repo_type=\"dataset\",\n-                    token=token,\n-                    commit_message=f\"Upload benchmark results for run {run_id}\",\n-                )\n-\n-        logger.info(\n-            f\"Successfully uploaded results to: https://huggingface.co/datasets/{dataset_name}/tree/main/{repo_path}\"\n-        )\n-\n-        return run_id\n-\n-    except Exception as upload_error:\n-        logger.error(f\"Failed to upload results: {upload_error}\")\n-        import traceback\n-\n-        logger.debug(traceback.format_exc())\n-        return None\n-\n-\n-def main():\n-    \"\"\"Main entry point for the benchmarking script.\"\"\"\n-    # Generate a unique UUID for this benchmark run\n-    benchmark_run_uuid = str(uuid.uuid4())[:8]\n-\n-    parser = argparse.ArgumentParser(\n-        description=\"Run all benchmarks in the ./benches directory\",\n-        epilog=\"\"\"\n-Examples:\n-  # Run all available benchmarks\n-  python3 run_benchmarks.py\n-  \n-  # Run with specific model and upload to HuggingFace Dataset\n-  python3 run_benchmarks.py --model-id meta-llama/Llama-2-7b-hf --upload-to-hf username/benchmark-results\n-  \n-  # Run with custom run ID and upload to HuggingFace Dataset\n-  python3 run_benchmarks.py --run-id experiment_v1 --upload-to-hf org/benchmarks\n-  \n-  # Run only specific benchmarks with file logging\n-  python3 run_benchmarks.py --include llama --enable-file-logging\n-        \"\"\",  # noqa: W293\n-        formatter_class=argparse.RawDescriptionHelpFormatter,\n-    )\n-\n-    parser.add_argument(\n-        \"--output-dir\",\n-        type=str,\n-        default=\"benchmark_results\",\n-        help=\"Base output directory for benchmark results (default: benchmark_results)\",\n-    )\n-\n-    parser.add_argument(\n-        \"--benches-dir\",\n-        type=str,\n-        default=\"./benches\",\n-        help=\"Directory containing benchmark implementations (default: ./benches)\",\n-    )\n-\n-    parser.add_argument(\n-        \"--log-level\",\n-        type=str,\n-        choices=[\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\"],\n-        default=\"INFO\",\n-        help=\"Logging level (default: INFO)\",\n-    )\n \n+if __name__ == \"__main__\":\n+    # Parse arguments\n+    parser = argparse.ArgumentParser()\n+    parser.add_argument(\"--output-dir\", type=str, default=\"benchmark_results\", help=\"Output dir for benchmark results\")\n+    parser.add_argument(\"--log-level\", type=str, choices=[\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\"], default=\"INFO\")\n     parser.add_argument(\"--model-id\", type=str, help=\"Specific model ID to benchmark (if supported by benchmarks)\")\n \n-    parser.add_argument(\"--warmup-iterations\", type=int, default=3, help=\"Number of warmup iterations (default: 3)\")\n+    parser.add_argument(\"--warmup\", type=int, default=5, help=\"Number of warmup iterations\")\n+    parser.add_argument(\"--iterations\", type=int, default=20, help=\"Number of measurement iterations\")\n \n-    parser.add_argument(\n-        \"--measurement-iterations\", type=int, default=5, help=\"Number of measurement iterations (default: 5)\"\n-    )\n-\n-    parser.add_argument(\n-        \"--num-tokens-to-generate\",\n-        type=int,\n-        default=100,\n-        help=\"Number of tokens to generate in benchmarks (default: 100)\",\n-    )\n+    parser.add_argument(\"--batch-size\", \"-b\", type=int, nargs=\"+\", help=\"Batch size\")\n+    parser.add_argument(\"--sequence-length\", \"-s\", type=int, nargs=\"+\", help=\"Sequence length\")\n+    parser.add_argument(\"--num-tokens-to-generate\", \"-n\", type=int, nargs=\"+\", help=\"Number of tokens to generate\")\n \n-    parser.add_argument(\"--include\", type=str, nargs=\"*\", help=\"Only run benchmarks matching these names\")\n-\n-    parser.add_argument(\"--exclude\", type=str, nargs=\"*\", help=\"Exclude benchmarks matching these names\")\n-\n-    parser.add_argument(\"--enable-file-logging\", action=\"store_true\", help=\"Enable file logging (disabled by default)\")\n-\n-    parser.add_argument(\n-        \"--commit-id\", type=str, help=\"Git commit ID for metadata (if not provided, will auto-detect from git)\"\n-    )\n-\n-    parser.add_argument(\n-        \"--push-to-hub\",\n-        type=str,\n-        help=\"Upload results to HuggingFace Dataset (provide dataset name, e.g., 'username/benchmark-results')\",\n-    )\n-\n-    parser.add_argument(\n-        \"--run-id\", type=str, help=\"Custom run ID for organizing results (if not provided, will generate a unique ID)\"\n-    )\n-\n-    parser.add_argument(\n-        \"--token\",\n-        type=str,\n-        help=\"HuggingFace token for dataset uploads (if not provided, will use HF_TOKEN environment variable)\",\n-    )\n+    parser.add_argument(\"--num-tokens-to-profile\", \"-p\", type=int, default=0, help=\"Number of tokens to profile\")\n \n+    parser.add_argument(\"--commit-id\", type=str, help=\"Git commit ID (if not provided, will auto-detect from git)\")\n     args = parser.parse_args()\n \n     # Setup logging\n-    logger = setup_logging(args.log_level, args.enable_file_logging)\n+    benchmark_run_uuid = str(uuid.uuid4())[:8]\n+    numeric_level = getattr(logging, args.log_level.upper())\n \n+    handlers = [logging.StreamHandler(sys.stdout)]\n+    logging.basicConfig(\n+        level=numeric_level, format=\"[%(levelname)s - %(asctime)s] %(name)s: %(message)s\", handlers=handlers\n+    )\n+\n+    logger = logging.getLogger(\"benchmark_v2\")\n     logger.info(\"Starting benchmark discovery and execution\")\n     logger.info(f\"Benchmark run UUID: {benchmark_run_uuid}\")\n     logger.info(f\"Output directory: {args.output_dir}\")\n-    logger.info(f\"Benches directory: {args.benches_dir}\")\n-\n-    # Create output directory\n-    os.makedirs(args.output_dir, exist_ok=True)\n-\n-    try:\n-        # Discover benchmarks\n-        benchmarks = discover_benchmarks(args.benches_dir)\n-        logger.info(f\"Discovered {len(benchmarks)} benchmark(s): {[b['name'] for b in benchmarks]}\")\n-\n-        if not benchmarks:\n-            logger.warning(\"No benchmarks found!\")\n-            return 1\n-\n-        # Filter benchmarks based on include/exclude\n-        filtered_benchmarks = benchmarks\n-\n-        if args.include:\n-            filtered_benchmarks = [\n-                b for b in filtered_benchmarks if any(pattern in b[\"name\"] for pattern in args.include)\n-            ]\n-            logger.info(f\"Filtered to include: {[b['name'] for b in filtered_benchmarks]}\")\n \n-        if args.exclude:\n-            filtered_benchmarks = [\n-                b for b in filtered_benchmarks if not any(pattern in b[\"name\"] for pattern in args.exclude)\n-            ]\n-            logger.info(f\"After exclusion: {[b['name'] for b in filtered_benchmarks]}\")\n+    # Error out if one of the arguments is not provided\n+    if len(args.batch_size) * len(args.sequence_length) * len(args.num_tokens_to_generate) == 0:\n+        raise ValueError(\n+            \"At least one of the arguments --batch-size, --sequence-length, or --num-tokens-to-generate is required\"\n+        )\n \n-        if not filtered_benchmarks:\n-            logger.warning(\"No benchmarks remaining after filtering!\")\n-            return 1\n+    # If there is only one (batch_size, sequence_length, num_tokens_to_generate), we benchmark across configs\n+    elif len(args.batch_size) * len(args.sequence_length) * len(args.num_tokens_to_generate) == 1:\n+        benchmark_configs = generate_all_configs(\n+            warmup_iterations=args.warmup,\n+            measurement_iterations=args.iterations,\n+            batch_size=args.batch_size[0],\n+            sequence_length=args.sequence_length[0],\n+            num_tokens_to_generate=args.num_tokens_to_generate[0],\n+        )\n+        random.shuffle(benchmark_configs)\n \n-        # Prepare common kwargs for benchmarks\n-        benchmark_kwargs = {\n-            \"warmup_iterations\": args.warmup_iterations,\n-            \"measurement_iterations\": args.measurement_iterations,\n-            \"num_tokens_to_generate\": args.num_tokens_to_generate,\n+    # Otherwise, we benchmark across all combinations of dimensions\n+    else:\n+        kwargs = {\n+            \"warmup_iterations\": args.warmup,\n+            \"measurement_iterations\": args.iterations,\n+            \"gpu_monitoring\": False,\n+            \"batch_size\": args.batch_size[0],\n+            \"sequence_length\": args.sequence_length[0],\n+            \"num_tokens_to_generate\": args.num_tokens_to_generate[0],\n+            \"attn_implementation\": \"flex_attention\",\n+            \"sdpa_backend\": None,\n+            \"compile_mode\": \"default\",\n+            \"kernelize\": False,\n         }\n-\n-        if args.model_id:\n-            benchmark_kwargs[\"model_id\"] = args.model_id\n-\n-        # Add commit_id if provided\n-        if args.commit_id:\n-            benchmark_kwargs[\"commit_id\"] = args.commit_id\n-\n-        # Run benchmarks\n-        benchmark_results = {}\n-        successful_count = 0\n-\n-        for benchmark_info in filtered_benchmarks:\n-            result = run_single_benchmark(benchmark_info, args.output_dir, logger, **benchmark_kwargs)\n-\n-            benchmark_results[benchmark_info[\"name\"]] = result\n-\n-            if result is not None:\n-                successful_count += 1\n-\n-        # Generate summary report\n-        summary_file = generate_summary_report(args.output_dir, benchmark_results, logger, benchmark_run_uuid)\n-\n-        # Upload results to HuggingFace Dataset if requested\n-        upload_run_id = None\n-        if args.push_to_hub:\n-            logger.info(\"=\" * 60)\n-            logger.info(\"UPLOADING TO HUGGINGFACE DATASET\")\n-            logger.info(\"=\" * 60)\n-            # Use provided run_id or fallback to benchmark run UUID\n-            effective_run_id = args.run_id or benchmark_run_uuid\n-            upload_run_id = upload_results_to_hf_dataset(\n-                output_dir=args.output_dir,\n-                summary_file=summary_file,\n-                dataset_name=args.push_to_hub,\n-                run_id=effective_run_id,\n-                token=args.token,\n-                logger=logger,\n-            )\n-            if upload_run_id:\n-                logger.info(f\"Upload completed with run ID: {upload_run_id}\")\n-            else:\n-                logger.warning(\"Upload failed - continuing with local results\")\n-\n-        # Final summary\n-        total_benchmarks = len(filtered_benchmarks)\n-        failed_count = total_benchmarks - successful_count\n-\n-        logger.info(\"=\" * 60)\n-        logger.info(\"BENCHMARK RUN SUMMARY\")\n-        logger.info(\"=\" * 60)\n-        logger.info(f\"Total benchmarks: {total_benchmarks}\")\n-        logger.info(f\"Successful: {successful_count}\")\n-        logger.info(f\"Failed: {failed_count}\")\n-        logger.info(f\"Output directory: {args.output_dir}\")\n-        logger.info(f\"Summary report: {summary_file}\")\n-\n-        if args.push_to_hub:\n-            if upload_run_id:\n-                logger.info(f\"HuggingFace Dataset: {args.push_to_hub}\")\n-                logger.info(f\"Run ID: {upload_run_id}\")\n-                logger.info(\n-                    f\"View results: https://huggingface.co/datasets/{args.push_to_hub}/tree/main/{datetime.now().strftime('%Y-%m-%d')}/runs/{upload_run_id}\"\n-                )\n-            else:\n-                logger.warning(\"Upload to HuggingFace Dataset failed\")\n-\n-        if failed_count > 0:\n-            logger.warning(f\"{failed_count} benchmark(s) failed. Check logs for details.\")\n-            return 1\n-        else:\n-            logger.info(\"All benchmarks completed successfully!\")\n-            return 0\n-\n-    except Exception as e:\n-        logger.error(f\"Benchmark run failed: {e}\")\n-        import traceback\n-\n-        logger.debug(traceback.format_exc())\n-        return 1\n-\n-\n-if __name__ == \"__main__\":\n-    sys.exit(main())\n+        benchmark_configs = []\n+        for num_tokens_to_generate in args.num_tokens_to_generate:\n+            for sequence_length in args.sequence_length:\n+                for batch_size in args.batch_size:\n+                    kwargs[\"batch_size\"] = batch_size\n+                    kwargs[\"sequence_length\"] = sequence_length\n+                    kwargs[\"num_tokens_to_generate\"] = num_tokens_to_generate\n+                    benchmark_configs.append(BenchmarkConfig(**kwargs))\n+\n+    runner = BenchmarkRunner(logger, args.output_dir, args.commit_id)\n+    results = runner.run_benchmarks(\n+        args.model_id,\n+        benchmark_configs[:3],\n+        args.num_tokens_to_profile,\n+        pretty_print_summary=True,\n+    )\n+    # runner.save_results(args.model_id, results)"
      }
    ],
    "num_files": 13,
    "scraped_at": "2025-11-16T21:18:10.304198",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR represents a substantial architectural overhaul of the benchmarking suite with three new core components (BenchmarkConfig, BenchmarkRunner, and result files) that introduce meaningful logic for configuration management, benchmark execution, hardware monitoring, and metrics collection. The PR description provides clear context about the redesign rationale, and the code changes involve significant logic and design patterns that developers would need to understand to work with or extend the benchmarking system.",
      "substance_level": "high"
    }
  },
  {
    "pr_number": 41401,
    "title": "[Model] Lfm2Moe",
    "body": "# What does this PR do?\r\n\r\nThis PR implements [LFM2-8B-A1B](https://huggingface.co/LiquidAI/LFM2-8B-A1B), a hybrid Mixture-of-Experts architecture variant of [LFM2](https://huggingface.co/collections/LiquidAI/lfm2-686d721927015b2ad73eaa38). The LFM2 family is optimized for on-device inference by combining short\u2011range, input\u2011aware gated convolutions with grouped\u2011query attention (GQA). LFM2\u2011MoE keeps this fast backbone and introduces sparse MoE feed\u2011forward networks to add representational capacity without significantly increasing the active compute path.\r\n\r\n\r\n## Before submitting\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),\r\n      Pull Request section?\r\n- [x] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [x] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker @Cyrilvallez\r\n- vision models: @yonigozlan @molbap\r\n- audio models: @eustlb @ebezzam @vasqu\r\n- multimodal models: @zucchini-nlp\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- generate: @zucchini-nlp (visual-language models) or @gante (all others)\r\n- continuous batching: @remi-or @ArthurZucker @McPatate\r\n- pipelines: @Rocketknight1\r\n- tokenizers: @ArthurZucker and @itazap\r\n- trainer: @zach-huggingface @SunMarc\r\n- attention: @vasqu @ArthurZucker @CyrilVallez\r\n- model loading (from pretrained, etc): @CyrilVallez\r\n- distributed: @3outeille @ArthurZucker @S1ro1\r\n- CIs: @ydshieh\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @SunMarc @zach-huggingface\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber\r\n- kernels: @MekkCyber @drbh\r\n- peft: @BenjaminBossan @githubnemo\r\n\r\nDevices/Backends:\r\n\r\n- AMD ROCm: @ivarflakstad\r\n- Intel XPU: @IlyasMoutawwakil\r\n- Ascend NPU: @ivarflakstad \r\n\r\nDocumentation: @stevhliu\r\n\r\nResearch projects are not maintained and should be taken as is.\r\n\r\n -->\r\n",
    "html_url": "https://github.com/huggingface/transformers/pull/41401",
    "created_at": "2025-10-07T09:32:13Z",
    "merged_at": "2025-10-07T13:09:58Z",
    "merge_commit_sha": "0c9a72e4576fe4c84077f066e585129c97bfd4e6",
    "base_ref": "main",
    "head_sha": "b3a0924ec93a51b43d6d772d825ddde3b1d9f0ca",
    "user": "paulpak58",
    "files": [
      {
        "filename": "docs/source/en/_toctree.yml",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -562,6 +562,8 @@\n         title: LED\n       - local: model_doc/lfm2\n         title: LFM2\n+      - local: model_doc/lfm2_moe\n+        title: LFM2Moe\n       - local: model_doc/llama\n         title: LLaMA\n       - local: model_doc/llama2"
      },
      {
        "filename": "docs/source/en/model_doc/lfm2.md",
        "status": "modified",
        "additions": 4,
        "deletions": 4,
        "changes": 8,
        "patch": "@@ -23,15 +23,15 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-[LFM2](https://www.liquid.ai/blog/liquid-foundation-models-v2-our-second-series-of-generative-ai-models) represents a new generation of Liquid Foundation Models developed by [Liquid AI](https://liquid.ai/), specifically designed for edge AI and on-device deployment.\n+[LFM2](https://www.liquid.ai/blog/liquid-foundation-models-v2-our-second-series-of-generative-ai-models) represents a new generation of Liquid Foundation Models developed by Liquid AI, specifically designed for edge AI and on-device deployment.\n \n-The models are available in three sizes (350M, 700M, and 1.2B parameters) and are engineered to run efficiently on CPU, GPU, and NPU hardware, making them particularly well-suited for applications requiring low latency, offline operation, and privacy.\n+The models are available in four sizes (350M, 700M, 1.2B, and 2.6B parameters) and are engineered to run efficiently on CPU, GPU, and NPU hardware, making them particularly well-suited for applications requiring low latency, offline operation, and privacy.\n \n ## Architecture\n \n-The architecture consists of 16 blocks total: 10 double-gated short-range convolution blocks and 6 blocks of grouped query attention. This design stems from the concept of dynamical systems, where linear operations are modulated by input-dependent gates, allowing for \"liquid\" dynamics that can adapt in real-time. The short convolutions are particularly optimized for embedded SoC CPUs, making them ideal for devices that require fast, local inference without relying on cloud connectivity.\n+The architecture consists of blocks of gated short convolution blocks and blocks of grouped query attention with QK layernorm. This design stems from the concept of dynamical systems, where linear operations are modulated by input-dependent gates. The short convolutions are particularly optimized for embedded SoC CPUs, making them ideal for devices that require fast, local inference without relying on cloud connectivity.\n \n-The key architectural innovation of LFM2 lies in its systematic approach to balancing quality, latency, and memory efficiency through our STAR neural architecture search engine. Using STAR, Liquid AI optimized the models for real-world performance on embedded hardware, measuring actual peak memory usage and inference speed on Qualcomm Snapdragon processors. This results in models that achieve 2x faster decode and prefill performance compared to similar-sized models, while maintaining superior benchmark performance across knowledge, mathematics, instruction following, and multilingual tasks.\n+LFM2 was designed to maximize quality under strict speed and memory constraints. This was accomplished through a systematic architecture search to optimize the models for real-world performance on embedded hardware by measuring actual peak memory usage and inference speed on Qualcomm Snapdragon processors. This results in models that achieve 2x faster decode and prefill performance compared to similar-sized models, while maintaining superior benchmark performance across knowledge, mathematics, instruction following, and multilingual tasks.\n \n ## Example\n "
      },
      {
        "filename": "docs/source/en/model_doc/lfm2_moe.md",
        "status": "added",
        "additions": 83,
        "deletions": 0,
        "changes": 83,
        "patch": "@@ -0,0 +1,83 @@\n+<!--Copyright 2025 the HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+\n+\n+\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be rendered properly in your Markdown viewer.\n+\n+-->\n+\n+\n+# Lfm2Moe\n+\n+## Overview\n+\n+LFM2-MoE is a Mixture-of-Experts (MoE) variant of [LFM2](https://huggingface.co/collections/LiquidAI/lfm2-686d721927015b2ad73eaa38). The LFM2 family is optimized for on-device inference by combining short\u2011range, input\u2011aware gated convolutions with grouped\u2011query attention (GQA) in a layout tuned to maximize quality under strict speed and memory constraints.\n+\n+LFM2\u2011MoE keeps this fast backbone and introduces sparse MoE feed\u2011forward networks to add representational capacity without significantly increasing the active compute path. The first LFM2-MoE release is LFM2-8B-A1B, with 8.3B total parameters and 1.5B active parameters. The model excels in quality (comparable to 3-4B dense models) and speed (faster than other 1.5B class models). \n+\n+## Example\n+\n+The following example shows how to generate an answer using the `AutoModelForCausalLM` class.\n+\n+```python\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n+\n+# Load model and tokenizer\n+model_id = \"LiquidAI/LFM2-8B-A1B\"\n+model = AutoModelForCausalLM.from_pretrained(\n+    model_id,\n+    device_map=\"auto\",\n+    dtype=\"bfloat16\",\n+#    attn_implementation=\"flash_attention_2\" <- uncomment on compatible GPU\n+)\n+tokenizer = AutoTokenizer.from_pretrained(model_id)\n+\n+# Generate answer\n+prompt = \"What is C. elegans?\"\n+input_ids = tokenizer.apply_chat_template(\n+    [{\"role\": \"user\", \"content\": prompt}],\n+    add_generation_prompt=True,\n+    return_tensors=\"pt\",\n+    tokenize=True,\n+).to(model.device)\n+\n+output = model.generate(\n+    input_ids,\n+    do_sample=True,\n+    temperature=0.3,\n+    min_p=0.15,\n+    repetition_penalty=1.05,\n+    max_new_tokens=512,\n+)\n+\n+print(tokenizer.decode(output[0], skip_special_tokens=False))\n+```\n+\n+## Lfm2MoeConfig\n+\n+[[autodoc]] Lfm2MoeConfig\n+\n+## Lfm2MoeForCausalLM\n+\n+[[autodoc]] Lfm2MoeForCausalLM\n+\n+## Lfm2MoeModel\n+\n+[[autodoc]] Lfm2MoeModel\n+    - forward\n+\n+## Lfm2MoePreTrainedModel\n+\n+[[autodoc]] Lfm2MoePreTrainedModel\n+    - forward"
      },
      {
        "filename": "src/transformers/models/__init__.py",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -186,6 +186,7 @@\n     from .led import *\n     from .levit import *\n     from .lfm2 import *\n+    from .lfm2_moe import *\n     from .lfm2_vl import *\n     from .lightglue import *\n     from .lilt import *"
      },
      {
        "filename": "src/transformers/models/auto/configuration_auto.py",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -226,6 +226,7 @@\n         (\"led\", \"LEDConfig\"),\n         (\"levit\", \"LevitConfig\"),\n         (\"lfm2\", \"Lfm2Config\"),\n+        (\"lfm2_moe\", \"Lfm2MoeConfig\"),\n         (\"lfm2_vl\", \"Lfm2VlConfig\"),\n         (\"lightglue\", \"LightGlueConfig\"),\n         (\"lilt\", \"LiltConfig\"),\n@@ -670,6 +671,7 @@\n         (\"led\", \"LED\"),\n         (\"levit\", \"LeViT\"),\n         (\"lfm2\", \"Lfm2\"),\n+        (\"lfm2_moe\", \"Lfm2Moe\"),\n         (\"lfm2_vl\", \"Lfm2Vl\"),\n         (\"lightglue\", \"LightGlue\"),\n         (\"lilt\", \"LiLT\"),"
      },
      {
        "filename": "src/transformers/models/auto/modeling_auto.py",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -226,6 +226,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"led\", \"LEDModel\"),\n         (\"levit\", \"LevitModel\"),\n         (\"lfm2\", \"Lfm2Model\"),\n+        (\"lfm2_moe\", \"Lfm2MoeModel\"),\n         (\"lfm2_vl\", \"Lfm2VlModel\"),\n         (\"lightglue\", \"LightGlueForKeypointMatching\"),\n         (\"lilt\", \"LiltModel\"),\n@@ -694,6 +695,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"jamba\", \"JambaForCausalLM\"),\n         (\"jetmoe\", \"JetMoeForCausalLM\"),\n         (\"lfm2\", \"Lfm2ForCausalLM\"),\n+        (\"lfm2_moe\", \"Lfm2MoeForCausalLM\"),\n         (\"llama\", \"LlamaForCausalLM\"),\n         (\"llama4\", \"Llama4ForCausalLM\"),\n         (\"llama4_text\", \"Llama4ForCausalLM\"),"
      },
      {
        "filename": "src/transformers/models/lfm2/modeling_lfm2.py",
        "status": "modified",
        "additions": 0,
        "deletions": 2,
        "changes": 2,
        "patch": "@@ -163,7 +163,6 @@ def __init__(\n                 dtype=self._dtype,\n                 device=device,\n             )\n-            torch._dynamo.mark_static_address(conv_state)\n             self.conv_cache.append(conv_state)\n             self.key_cache.append(torch.tensor([]))\n             self.value_cache.append(torch.tensor([]))\n@@ -595,7 +594,6 @@ def __init__(self, config: Lfm2Config):\n         self.layers = nn.ModuleList(\n             [Lfm2DecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n         )\n-        self.rotary_emb = Lfm2RotaryEmbedding(config=config)\n         self.gradient_checkpointing = False\n         self.pos_emb = Lfm2RotaryEmbedding(config)\n         self.embedding_norm = Lfm2RMSNorm(config.hidden_size, eps=config.norm_eps)"
      },
      {
        "filename": "src/transformers/models/lfm2/modular_lfm2.py",
        "status": "modified",
        "additions": 1,
        "deletions": 2,
        "changes": 3,
        "patch": "@@ -121,7 +121,6 @@ def __init__(\n                 dtype=self._dtype,\n                 device=device,\n             )\n-            torch._dynamo.mark_static_address(conv_state)\n             self.conv_cache.append(conv_state)\n             self.key_cache.append(torch.tensor([]))\n             self.value_cache.append(torch.tensor([]))\n@@ -441,7 +440,7 @@ def __init__(self, config: Lfm2Config):\n         self.pos_emb = Lfm2RotaryEmbedding(config)\n         self.embedding_norm = Lfm2RMSNorm(config.hidden_size, eps=config.norm_eps)\n         del self.norm\n-        del self.rotary_emv\n+        del self.rotary_emb\n \n     def forward(\n         self,"
      },
      {
        "filename": "src/transformers/models/lfm2_moe/__init__.py",
        "status": "added",
        "additions": 29,
        "deletions": 0,
        "changes": 29,
        "patch": "@@ -0,0 +1,29 @@\n+# coding=utf-8\n+# Copyright 2025 the HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_lfm2_moe import *\n+    from .modeling_lfm2_moe import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
      },
      {
        "filename": "src/transformers/models/lfm2_moe/configuration_lfm2_moe.py",
        "status": "added",
        "additions": 169,
        "deletions": 0,
        "changes": 169,
        "patch": "@@ -0,0 +1,169 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import Optional\n+\n+from ...configuration_utils import PretrainedConfig\n+\n+\n+class Lfm2MoeConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Lfm2MoeModel`]. It is used to instantiate a LFM2 Moe\n+    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n+    defaults will yield a similar configuration to that of the LFM2-8B-A1B model.\n+    e.g. [LiquidAI/LFM2-8B-A1B](https://huggingface.co/LiquidAI/LFM2-8B-A1B)\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+\n+    Args:\n+        vocab_size (`int`, *optional*, defaults to 65536):\n+            Vocabulary size of the LLaMA model. Defines the number of different tokens that can be represented by the\n+            `inputs_ids` passed when calling [`Lfm2Model`]\n+        hidden_size (`int`, *optional*, defaults to 2048):\n+            Dimension of the hidden representations.\n+        intermediate_size (`int`, *optional*, defaults to 7168):\n+            Dimension of the MLP representations.\n+        moe_intermediate_size (`int`, *optional*, defaults to 1792):\n+            Intermediate size of the routed expert.\n+        num_hidden_layers (`int`, *optional*, defaults to 32):\n+            Number of hidden layers in the Transformer decoder.\n+        pad_token_id (`int`, *optional*, defaults to 0):\n+            Padding token id.\n+        bos_token_id (`int`, *optional*, defaults to 1):\n+            Beginning of stream token id.\n+        eos_token_id (`int`, *optional*, defaults to 2):\n+            End of stream token id.\n+        tie_word_embeddings (`bool`, *optional*, defaults to `True`):\n+            Whether to tie weight embeddings\n+        rope_theta (`float`, *optional*, defaults to 1000000.0):\n+            The base period of the RoPE embeddings.\n+        max_position_embeddings (`int`, *optional*, defaults to 128000):\n+            The maximum sequence length that this model might ever be used with.\n+        use_cache (`bool`, *optional*, defaults to `True`):\n+            Whether or not the model should return the last key/values attentions (not used by all models). Only\n+            relevant if `config.is_decoder=True`.\n+        norm_eps (`float`, *optional*, defaults to 1e-05):\n+            The epsilon used by the rms normalization layers.\n+        num_attention_heads (`int`, *optional*, defaults to 32):\n+            Number of attention heads for each attention layer in the Transformer decoder.\n+        num_key_value_heads (`int`, *optional*, defaults to 8):\n+            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n+            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n+            `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n+            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n+            by meanpooling all the original heads within that group. For more details, check out [this\n+            paper](https://huggingface.co/papers/2305.13245). If it is not specified, will default to\n+            `num_attention_heads`.\n+        conv_bias (`bool`, *optional*, defaults to `False`):\n+            Whether to use bias in the conv layers.\n+        conv_L_cache (`int`, *optional*, defaults to 3):\n+            L_cache dim in the conv layers.\n+        num_dense_layers (`int`, *optional*, defaults to 2):\n+            Number of dense Lfm2MoeMLP layers in shallow layers(embed->dense->dense->...->dense->moe->moe...->lm_head).\n+        num_experts_per_tok (`int`, *optional*, defaults to 4):\n+            Number of selected experts.\n+        num_experts (`int`, *optional*, defaults to 32):\n+            Number of routed experts.\n+        use_expert_bias (`bool`, *optional*, defaults to `True`):\n+            Whether to use the expert bias on the routing weights.\n+        routed_scaling_factor (`float`, *optional*, defaults to 1.0):\n+            Scaling factor for routed experts in MoE models.\n+        norm_topk_prob (`bool`, *optional*, defaults to `True`):\n+            Whether to normalize the topk probabilities.\n+        layer_types (`Optional`, *optional*):\n+            Type of each layers.\n+\n+    ```python\n+    >>> from transformers import Lfm2MoeModel, Lfm2MoeConfig\n+\n+    >>> # Initializing a LFM2 Moe model\n+    >>> configuration = Lfm2MoeConfig()\n+\n+    >>> # Initializing a model from the LFM2-8B-A1B style configuration\n+    >>> model = Lfm2MoeModel(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"lfm2_moe\"\n+    keys_to_ignore_at_inference = [\"past_key_values\"]\n+\n+    def __init__(\n+        self,\n+        vocab_size: int = 65536,\n+        hidden_size: int = 2048,\n+        intermediate_size: int = 7168,\n+        moe_intermediate_size: int = 1792,\n+        num_hidden_layers: int = 32,\n+        pad_token_id: int = 0,\n+        bos_token_id: int = 1,\n+        eos_token_id: int = 2,\n+        tie_word_embeddings: bool = True,\n+        rope_theta: float = 1000000.0,\n+        max_position_embeddings: int = 128_000,\n+        use_cache: bool = True,\n+        norm_eps: float = 0.00001,\n+        num_attention_heads: int = 32,\n+        num_key_value_heads: int = 8,\n+        conv_bias: bool = False,\n+        conv_L_cache: int = 3,\n+        num_dense_layers: int = 2,\n+        num_experts_per_tok: int = 4,\n+        num_experts: int = 32,\n+        use_expert_bias: bool = True,\n+        routed_scaling_factor: float = 1.0,\n+        norm_topk_prob: bool = True,\n+        layer_types: Optional[list[str]] = None,\n+        **kwargs,\n+    ):\n+        self.vocab_size = vocab_size\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.rope_theta = rope_theta\n+        self.max_position_embeddings = max_position_embeddings\n+        self.use_cache = use_cache\n+        self.norm_eps = norm_eps\n+\n+        # attn operator config\n+        self.num_attention_heads = num_attention_heads\n+        self.num_key_value_heads = num_key_value_heads\n+\n+        # custom operator config\n+        self.conv_bias = conv_bias\n+        self.conv_L_cache = conv_L_cache\n+\n+        # moe config\n+        self.num_dense_layers = num_dense_layers\n+        self.moe_intermediate_size = moe_intermediate_size\n+        self.num_experts_per_tok = num_experts_per_tok\n+        self.num_experts = num_experts\n+        self.use_expert_bias = use_expert_bias\n+        self.routed_scaling_factor = routed_scaling_factor\n+        self.norm_topk_prob = norm_topk_prob\n+        self.layer_types = layer_types\n+\n+        tie_word_embeddings = kwargs.get(\"tie_embedding\", tie_word_embeddings)  # to fit original config keys\n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n+\n+\n+__all__ = [\"Lfm2MoeConfig\"]"
      },
      {
        "filename": "src/transformers/models/lfm2_moe/modeling_lfm2_moe.py",
        "status": "added",
        "additions": 813,
        "deletions": 0,
        "changes": 813,
        "patch": "@@ -0,0 +1,813 @@\n+#                \ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\n+#           This file was automatically generated from src/transformers/models/lfm2_moe/modular_lfm2_moe.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_lfm2_moe.py file directly. One of our CI enforces this.\n+#                \ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import Any, Callable, Optional, Union\n+\n+import torch\n+import torch.nn.functional as F\n+from torch import nn\n+\n+from ...cache_utils import Cache\n+from ...generation import GenerationMixin\n+from ...integrations import use_kernel_forward_from_hub\n+from ...masking_utils import create_causal_mask\n+from ...modeling_layers import GradientCheckpointingLayer\n+from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast, MoeModelOutputWithPast\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils.deprecation import deprecate_kwarg\n+from ...utils.generic import check_model_inputs\n+from ...utils.import_utils import is_causal_conv1d_available\n+from .configuration_lfm2_moe import Lfm2MoeConfig\n+\n+\n+if is_causal_conv1d_available():\n+    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\n+else:\n+    causal_conv1d_fn, causal_conv1d_update = None, None\n+\n+\n+@use_kernel_forward_from_hub(\"RMSNorm\")\n+class Lfm2MoeRMSNorm(nn.Module):\n+    def __init__(self, hidden_size, eps=1e-6):\n+        \"\"\"\n+        Lfm2MoeRMSNorm is equivalent to T5LayerNorm\n+        \"\"\"\n+        super().__init__()\n+        self.weight = nn.Parameter(torch.ones(hidden_size))\n+        self.variance_epsilon = eps\n+\n+    def forward(self, hidden_states):\n+        input_dtype = hidden_states.dtype\n+        hidden_states = hidden_states.to(torch.float32)\n+        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n+        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n+        return self.weight * hidden_states.to(input_dtype)\n+\n+    def extra_repr(self):\n+        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n+\n+\n+class Lfm2MoeRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n+    def __init__(self, config: Lfm2MoeConfig, device=None):\n+        super().__init__()\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+        else:\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+    @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n+    def forward(self, x, position_ids):\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n+class Lfm2MoeMLP(nn.Module):\n+    def __init__(self, config: Lfm2MoeConfig, intermediate_size: Optional[int] = None):\n+        super().__init__()\n+        self.hidden_size = config.hidden_size\n+        self.intermediate_size = config.intermediate_size if intermediate_size is None else intermediate_size\n+        self.w1 = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.w3 = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.w2 = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n+\n+    def forward(self, x):\n+        return self.w2(F.silu(self.w1(x)) * self.w3(x))\n+\n+\n+class Lfm2MoeExperts(nn.ModuleList):\n+    \"\"\"\n+    ModuleList of experts.\n+    \"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+        self.num_experts = config.num_experts\n+        for _ in range(config.num_experts):\n+            self.append(Lfm2MoeMLP(config, intermediate_size=config.moe_intermediate_size))\n+\n+    def forward(\n+        self, hidden_states: torch.Tensor, top_k_index: torch.Tensor, top_k_weights: torch.Tensor\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        Args:\n+            hidden_states: (batch_size * sequence_length, hidden_dim)\n+            selected_experts: (batch_size * sequence_length, top_k)\n+            routing_weights: (batch_size * sequence_length, top_k)\n+        Returns:\n+            (batch_size * sequence_length, hidden_dim)\n+        \"\"\"\n+        final_hidden_states = torch.zeros_like(hidden_states)\n+        expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=self.num_experts).permute(2, 1, 0)\n+\n+        expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n+        for expert_idx in expert_hit:\n+            idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\n+            current_state = hidden_states[None, top_x].reshape(-1, hidden_states.shape[-1])\n+            current_hidden_states = self[expert_idx](current_state) * top_k_weights[top_x, idx, None]\n+            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n+        return final_hidden_states\n+\n+\n+class Lfm2MoeSparseMoeBlock(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.top_k = config.num_experts_per_tok\n+        self.routed_scaling_factor = config.routed_scaling_factor\n+        self.norm_topk_prob = config.norm_topk_prob\n+        self.use_expert_bias = config.use_expert_bias\n+\n+        self.gate = nn.Linear(config.hidden_size, config.num_experts, bias=False)\n+        self.experts = Lfm2MoeExperts(config)\n+        if self.use_expert_bias:\n+            self.register_buffer(\"expert_bias\", torch.zeros(config.num_experts, dtype=torch.float32))\n+\n+    def route_tokens_to_experts(self, router_logits):\n+        routing_weights = router_logits.sigmoid()\n+        if self.use_expert_bias:\n+            scores_for_routing = routing_weights + self.expert_bias\n+            _, selected_experts = torch.topk(scores_for_routing, k=self.top_k, dim=-1)\n+            routing_weights = torch.gather(routing_weights, dim=1, index=selected_experts).type_as(router_logits)\n+        else:\n+            routing_weights, selected_experts = torch.topk(routing_weights, k=self.top_k, dim=-1)\n+\n+        if self.norm_topk_prob:\n+            routing_weights = routing_weights / (routing_weights.sum(dim=-1, keepdim=True) + 1e-6)\n+        routing_weights = routing_weights * self.routed_scaling_factor\n+        return selected_experts, routing_weights\n+\n+    def forward(self, hidden_states: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n+        batch_size, sequence_length, hidden_dim = hidden_states.shape\n+        hidden_states_reshaped = hidden_states.view(-1, hidden_dim)\n+        router_logits = self.gate(hidden_states_reshaped)\n+        selected_experts, routing_weights = self.route_tokens_to_experts(router_logits)\n+        final_hidden_states = self.experts(hidden_states_reshaped, selected_experts, routing_weights)\n+        return final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n+\n+\n+class Lfm2MoeHybridConvCache:\n+    \"\"\"\n+    Attention and conv cache for Lfm2Moe.\n+\n+    It stores the Key and Value states as a list of tensors, one for each layer.\n+    Attention layer cache shape: `[batch_size, num_heads, seq_len, head_dim]`.\n+    Conv layer cache shape: `[batch_size, hidden_size, L_cache-1]`.\n+    \"\"\"\n+\n+    # Override @property existing in Cache\n+    max_batch_size = None\n+    is_compileable = False\n+    key_cache = None\n+    value_cache = None\n+\n+    def __init__(\n+        self,\n+        config: Lfm2MoeConfig,\n+        max_batch_size: int,\n+        dtype: torch.dtype = torch.float32,\n+        device: Union[torch.device, str, None] = None,\n+    ):\n+        self.key_cache = []\n+        self.value_cache = []\n+        self.max_batch_size = max_batch_size\n+        self.layer_types = config.layer_types\n+        self.first_attention_layer = self.layer_types.index(\"full_attention\")\n+        self.conv_L_cache = config.conv_L_cache\n+        self._dtype = dtype\n+\n+        self.conv_cache: list[torch.Tensor] = []\n+        device = torch.device(device) if device is not None else None\n+\n+        for _ in range(config.num_hidden_layers):\n+            conv_state = torch.zeros(\n+                self.max_batch_size,\n+                config.hidden_size,\n+                self.conv_L_cache,\n+                dtype=self._dtype,\n+                device=device,\n+            )\n+            self.conv_cache.append(conv_state)\n+            self.key_cache.append(torch.tensor([]))\n+            self.value_cache.append(torch.tensor([]))\n+\n+    def update(\n+        self,\n+        key_states: torch.Tensor,\n+        value_states: torch.Tensor,\n+        layer_idx: int,\n+        cache_kwargs: Optional[dict[str, Any]] = None,\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n+        \"\"\"\n+        Updates the cache with the new `key_states` and `value_states` for the layer `layer_idx`.\n+\n+        Parameters:\n+            key_states (`torch.Tensor`):\n+                The new key states to cache.\n+            value_states (`torch.Tensor`):\n+                The new value states to cache.\n+            layer_idx (`int`):\n+                The index of the layer to cache the states for.\n+            cache_kwargs (`Dict[str, Any]`, `optional`):\n+                Additional arguments for the cache subclass. No additional arguments are used in `DynamicCache`.\n+\n+        Return:\n+            A tuple containing the updated key and value states.\n+        \"\"\"\n+        # Update the cache\n+        if self.key_cache[layer_idx].numel() == 0:\n+            self.key_cache[layer_idx] = key_states\n+            self.value_cache[layer_idx] = value_states\n+        else:\n+            self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2)\n+            self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2)\n+\n+        return self.key_cache[layer_idx], self.value_cache[layer_idx]\n+\n+    def reorder_cache(self, beam_idx: torch.LongTensor):\n+        \"\"\"Reorders the cache for beam search, given the selected beam indices.\"\"\"\n+        for layer_idx in range(len(self.key_cache)):\n+            if self.key_cache[layer_idx].numel():\n+                device = self.key_cache[layer_idx].device\n+                self.key_cache[layer_idx] = self.key_cache[layer_idx].index_select(0, beam_idx.to(device))\n+                device = self.value_cache[layer_idx].device\n+                self.value_cache[layer_idx] = self.value_cache[layer_idx].index_select(0, beam_idx.to(device))\n+\n+            if self.conv_cache[layer_idx].numel():\n+                device = self.conv_cache[layer_idx].device\n+                self.conv_cache[layer_idx] = self.conv_cache[layer_idx].index_select(0, beam_idx.to(device))\n+\n+    def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n+        \"\"\"Returns the sequence length of the cached states. A layer index can be optionally passed.\"\"\"\n+        # take any layer that contains cache and not empty tensor\n+        layer_idx = self.first_attention_layer if self.layer_types[layer_idx] != \"full_attention\" else layer_idx\n+        if len(self.key_cache) <= layer_idx or self.key_cache[layer_idx].numel() == 0:\n+            return 0\n+        return self.key_cache[layer_idx].shape[-2]\n+\n+    def get_mask_sizes(self, cache_position: torch.Tensor, layer_idx: int) -> tuple[int, int]:\n+        \"\"\"\n+        Return a tuple (kv_length, kv_offset) corresponding to the length and offset that will be returned for\n+        the given layer at `layer_idx`.\n+        The masks are then prepared according to the given lengths (kv_length, kv_offset) and patterns (i.e. sliding_window, chunk_size),\n+        for each layer.\n+        \"\"\"\n+        full_mask_kv_offset = 0\n+        query_length = cache_position.shape[0]\n+        past_seen_tokens = self.get_seq_length()\n+        kv_length = query_length + past_seen_tokens\n+        return kv_length, full_mask_kv_offset\n+\n+    def crop(self, max_length: int):\n+        \"\"\"Crop the cache to the given length\"\"\"\n+        if max_length < 0:\n+            max_length = self.get_seq_length() - abs(max_length)\n+\n+        if self.get_seq_length() <= max_length:\n+            return\n+\n+        for idx in range(len(self.key_cache)):\n+            if self.key_cache[idx].numel():\n+                self.key_cache[idx] = self.key_cache[idx][..., :max_length, :]\n+                self.value_cache[idx] = self.value_cache[idx][..., :max_length, :]\n+\n+    def __len__(self) -> int:\n+        return len(self.key_cache)\n+\n+    def __getitem__(self, layer_idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n+        return self.key_cache[layer_idx], self.value_cache[layer_idx]\n+\n+    def reset(self):\n+        for layer_idx in range(len(self.conv_cache)):\n+            # In-place ops prevent breaking the static address\n+            self.conv_cache[layer_idx].zero_()\n+\n+\n+def rotate_half(x):\n+    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n+    x1 = x[..., : x.shape[-1] // 2]\n+    x2 = x[..., x.shape[-1] // 2 :]\n+    return torch.cat((-x2, x1), dim=-1)\n+\n+\n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n+    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n+\n+    Args:\n+        q (`torch.Tensor`): The query tensor.\n+        k (`torch.Tensor`): The key tensor.\n+        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n+        sin (`torch.Tensor`): The sine part of the rotary embedding.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n+        unsqueeze_dim (`int`, *optional*, defaults to 1):\n+            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n+            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n+            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n+            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n+            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n+            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n+    Returns:\n+        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n+    \"\"\"\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n+    q_embed = (q * cos) + (rotate_half(q) * sin)\n+    k_embed = (k * cos) + (rotate_half(k) * sin)\n+    return q_embed, k_embed\n+\n+\n+def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n+    \"\"\"\n+    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n+    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n+    \"\"\"\n+    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n+    if n_rep == 1:\n+        return hidden_states\n+    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n+    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n+\n+\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs: Unpack[TransformersKwargs],\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n+class Lfm2MoeAttention(nn.Module):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    def __init__(self, config: Lfm2MoeConfig, layer_idx: int):\n+        super().__init__()\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n+        self.scaling = self.head_dim**-0.5\n+        self.is_causal = True\n+        self.q_proj = nn.Linear(config.hidden_size, config.num_attention_heads * self.head_dim, bias=False)\n+        self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n+        self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n+        self.out_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n+        self.q_layernorm = Lfm2MoeRMSNorm(self.head_dim, eps=config.norm_eps)\n+        self.k_layernorm = Lfm2MoeRMSNorm(self.head_dim, eps=config.norm_eps)\n+\n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n+        past_key_values: Optional[Lfm2MoeHybridConvCache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+\n+        query_states = self.q_layernorm(self.q_proj(hidden_states).view(*hidden_shape)).transpose(1, 2)\n+        key_states = self.k_layernorm(self.k_proj(hidden_states).view(*hidden_shape)).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(*hidden_shape).transpose(1, 2)\n+\n+        cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        if past_key_values is not None:\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0,\n+            scaling=self.scaling,\n+            **kwargs,\n+        )\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        output = self.out_proj(attn_output)\n+        return output, attn_weights\n+\n+\n+def apply_mask_to_padding_states(hidden_states, attention_mask):\n+    \"\"\"\n+    Tunes out the hidden states for padding tokens, see https://github.com/state-spaces/mamba/issues/66\n+    \"\"\"\n+    if attention_mask is not None and attention_mask.shape[1] > 1 and attention_mask.shape[0] > 1:\n+        dtype = hidden_states.dtype\n+        hidden_states = (hidden_states * attention_mask[:, :, None]).to(dtype)\n+\n+    return hidden_states\n+\n+\n+kernel_modules = (causal_conv1d_fn, causal_conv1d_update)\n+is_fast_path_available = all(kernel_modules)\n+\n+\n+class Lfm2MoeShortConv(nn.Module):\n+    def __init__(\n+        self,\n+        config: Lfm2MoeConfig,\n+        layer_idx: int,\n+    ):\n+        super().__init__()\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        self.L_cache = config.conv_L_cache\n+        self.bias = config.conv_bias\n+\n+        self.conv = nn.Conv1d(\n+            in_channels=config.hidden_size,\n+            out_channels=config.hidden_size,\n+            kernel_size=self.L_cache,\n+            groups=config.hidden_size,\n+            bias=self.bias,\n+            padding=self.L_cache - 1,\n+        )\n+        self.in_proj = nn.Linear(config.hidden_size, 3 * config.hidden_size, bias=self.bias)\n+        self.out_proj = nn.Linear(config.hidden_size, config.hidden_size, bias=self.bias)\n+\n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n+    def cuda_kernels_forward(\n+        self,\n+        x: torch.Tensor,\n+        past_key_values: Optional[Lfm2MoeHybridConvCache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+    ):\n+        x = apply_mask_to_padding_states(x, attention_mask)\n+        BCx = self.in_proj(x).transpose(-1, -2)\n+        B, C, x = BCx.chunk(3, dim=-2)\n+\n+        Bx = B * x\n+\n+        conv_weights = self.conv.weight.view(self.conv.weight.size(0), self.conv.weight.size(2))\n+        if past_key_values is not None and cache_position[0] > 0:\n+            conv_out = causal_conv1d_update(\n+                Bx.squeeze(-1),\n+                past_key_values.conv_cache[self.layer_idx],\n+                conv_weights,\n+                self.conv.bias,\n+                None,\n+            )\n+            conv_out = conv_out.unsqueeze(-1)\n+        else:\n+            if past_key_values is not None:\n+                conv_state = nn.functional.pad(Bx, (self.L_cache - Bx.shape[-1], 0))\n+                past_key_values.conv_cache[self.layer_idx].copy_(conv_state)\n+\n+            conv_out = causal_conv1d_fn(Bx, conv_weights, self.conv.bias, activation=None)\n+\n+        y = C * conv_out\n+        y = self.out_proj(y.transpose(-1, -2).contiguous())\n+        return y\n+\n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n+    def slow_forward(\n+        self,\n+        x: torch.Tensor,\n+        past_key_values: Optional[Lfm2MoeHybridConvCache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+    ):\n+        seqlen = x.shape[1]\n+\n+        x = apply_mask_to_padding_states(x, attention_mask)\n+        BCx = self.in_proj(x).transpose(-1, -2)\n+        B, C, x = BCx.chunk(3, dim=-2)\n+\n+        Bx = B * x\n+\n+        if past_key_values is not None and cache_position[0] > 0:\n+            conv_state = past_key_values.conv_cache[self.layer_idx]\n+            cache_position = cache_position.clamp(0, self.L_cache - 1)\n+            conv_state = conv_state.roll(shifts=-1, dims=-1)\n+            conv_state[:, :, cache_position] = Bx.to(device=conv_state.device, dtype=conv_state.dtype)\n+            past_key_values.conv_cache[self.layer_idx].copy_(conv_state)\n+            conv_out = torch.sum(conv_state.to(Bx.device) * self.conv.weight[:, 0, :], dim=-1)\n+            if self.bias:\n+                conv_out += self.conv.bias\n+\n+            conv_out = conv_out.unsqueeze(-1)\n+        else:\n+            if past_key_values is not None:\n+                conv_state = nn.functional.pad(Bx, (self.L_cache - Bx.shape[-1], 0))\n+                past_key_values.conv_cache[self.layer_idx].copy_(conv_state)\n+\n+            conv_out = self.conv(Bx)[..., :seqlen]\n+\n+        y = C * conv_out\n+        y = y.transpose(-1, -2).contiguous()\n+        y = self.out_proj(y)\n+        return y\n+\n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        past_key_values: Optional[Lfm2MoeHybridConvCache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+    ):\n+        if is_fast_path_available and \"cuda\" in hidden_states.device.type and not torch._dynamo.is_compiling():\n+            return self.cuda_kernels_forward(hidden_states, past_key_values, cache_position, attention_mask)\n+        return self.slow_forward(hidden_states, past_key_values, cache_position, attention_mask)\n+\n+\n+class Lfm2MoeDecoderLayer(GradientCheckpointingLayer):\n+    def __init__(self, config: Lfm2MoeConfig, layer_idx: int):\n+        super().__init__()\n+        self.is_attention_layer = config.layer_types[layer_idx] == \"full_attention\"\n+\n+        if self.is_attention_layer:\n+            self.self_attn = Lfm2MoeAttention(config, layer_idx)\n+        else:\n+            self.conv = Lfm2MoeShortConv(config, layer_idx)\n+        self.feed_forward = (\n+            Lfm2MoeMLP(config, intermediate_size=config.intermediate_size)\n+            if layer_idx < config.num_dense_layers\n+            else Lfm2MoeSparseMoeBlock(config)\n+        )\n+        self.operator_norm = Lfm2MoeRMSNorm(config.hidden_size, eps=config.norm_eps)\n+        self.ffn_norm = Lfm2MoeRMSNorm(config.hidden_size, eps=config.norm_eps)\n+\n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Lfm2MoeHybridConvCache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n+    ) -> torch.Tensor:\n+        residual = hidden_states\n+        if self.is_attention_layer:\n+            hidden_states, _ = self.self_attn(\n+                hidden_states=self.operator_norm(hidden_states),\n+                position_embeddings=position_embeddings,\n+                attention_mask=attention_mask,\n+                position_ids=position_ids,\n+                past_key_values=past_key_values,\n+                cache_position=cache_position,\n+                **kwargs,\n+            )\n+        else:\n+            hidden_states = self.conv(\n+                hidden_states=self.operator_norm(hidden_states),\n+                past_key_values=past_key_values,\n+                cache_position=cache_position,\n+                attention_mask=attention_mask,\n+            )\n+        hidden_states = hidden_states + residual\n+        hidden_states = hidden_states + self.feed_forward(self.ffn_norm(hidden_states))\n+\n+        return hidden_states\n+\n+\n+@auto_docstring\n+class Lfm2MoePreTrainedModel(PreTrainedModel):\n+    config: Lfm2MoeConfig\n+    base_model_prefix = \"model\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"Lfm2MoeDecoderLayer\"]\n+    _skip_keys_device_placement = [\"past_key_values\"]\n+    _supports_flash_attn = True\n+    _supports_sdpa = True\n+    _supports_flex_attn = True\n+    _can_compile_fullgraph = False\n+    _supports_attention_backend = True\n+    _can_record_outputs = {\n+        \"hidden_states\": Lfm2MoeDecoderLayer,\n+        \"attentions\": Lfm2MoeAttention,\n+    }\n+\n+\n+@auto_docstring\n+class Lfm2MoeModel(Lfm2MoePreTrainedModel):\n+    def __init__(self, config: Lfm2MoeConfig):\n+        super().__init__(config)\n+        self.padding_idx = config.pad_token_id\n+        self.vocab_size = config.vocab_size\n+\n+        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n+        self.layers = nn.ModuleList(\n+            [Lfm2MoeDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+        self.gradient_checkpointing = False\n+        self.pos_emb = Lfm2MoeRotaryEmbedding(config)\n+        self.embedding_norm = Lfm2MoeRMSNorm(config.hidden_size, eps=config.norm_eps)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @check_model_inputs()\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Lfm2MoeHybridConvCache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> MoeModelOutputWithPast:\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.embed_tokens(input_ids)\n+\n+        if use_cache and past_key_values is None:\n+            batch_size = inputs_embeds.shape[0]\n+            past_key_values = Lfm2MoeHybridConvCache(\n+                config=self.config, max_batch_size=batch_size, dtype=self.dtype, device=self.device\n+            )\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            position_ids=position_ids,\n+        )\n+\n+        hidden_states = inputs_embeds\n+        position_embeddings = self.pos_emb(hidden_states, position_ids)\n+\n+        # decoder layers\n+        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n+            hidden_states = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_values=past_key_values,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                **kwargs,\n+            )\n+\n+        hidden_states = self.embedding_norm(hidden_states)\n+\n+        return MoeModelOutputWithPast(\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values,\n+        )\n+\n+\n+@auto_docstring\n+class Lfm2MoeForCausalLM(Lfm2MoePreTrainedModel, GenerationMixin):\n+    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tp_plan = {\"lm_head\": \"colwise_rep\"}\n+    _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.model = Lfm2MoeModel(config)\n+        self.vocab_size = config.vocab_size\n+        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> CausalLMOutputWithPast:\n+        r\"\"\"\n+        Example:\n+\n+        ```python\n+        >>> from transformers import AutoTokenizer, Lfm2MoeForCausalLM\n+\n+        >>> model = Lfm2MoeForCausalLM.from_pretrained(\"meta-lfm2_moe/Lfm2Moe-2-7b-hf\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"meta-lfm2_moe/Lfm2Moe-2-7b-hf\")\n+\n+        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n+        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n+\n+        >>> # Generate\n+        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n+        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n+        ```\"\"\"\n+        outputs: BaseModelOutputWithPast = self.model(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        hidden_states = outputs.last_hidden_state\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n+\n+        return CausalLMOutputWithPast(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+\n+__all__ = [\"Lfm2MoeForCausalLM\", \"Lfm2MoeModel\", \"Lfm2MoePreTrainedModel\"]"
      },
      {
        "filename": "src/transformers/models/lfm2_moe/modular_lfm2_moe.py",
        "status": "added",
        "additions": 204,
        "deletions": 0,
        "changes": 204,
        "patch": "@@ -0,0 +1,204 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import Optional\n+\n+import torch\n+from torch import nn\n+\n+from ...masking_utils import create_causal_mask\n+from ...modeling_outputs import MoeModelOutputWithPast\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, logging\n+from ...utils.import_utils import is_causal_conv1d_available\n+from ..lfm2.modeling_lfm2 import Lfm2Attention, Lfm2DecoderLayer, Lfm2HybridConvCache, Lfm2MLP, Lfm2ShortConv\n+from ..llama.modeling_llama import LlamaForCausalLM, LlamaPreTrainedModel, LlamaRMSNorm, LlamaRotaryEmbedding\n+from ..mixtral.modeling_mixtral import MixtralModel\n+from ..qwen2_moe.modeling_qwen2_moe import Qwen2MoeExperts\n+from .configuration_lfm2_moe import Lfm2MoeConfig\n+\n+\n+if is_causal_conv1d_available():\n+    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\n+else:\n+    causal_conv1d_fn, causal_conv1d_update = None, None\n+\n+\n+kernel_modules = (causal_conv1d_fn, causal_conv1d_update)\n+is_fast_path_available = all(kernel_modules)\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class Lfm2MoeRMSNorm(LlamaRMSNorm):\n+    pass\n+\n+\n+class Lfm2MoeRotaryEmbedding(LlamaRotaryEmbedding):\n+    pass\n+\n+\n+class Lfm2MoeMLP(Lfm2MLP):\n+    def __init__(self, config: Lfm2MoeConfig, intermediate_size: Optional[int] = None):\n+        nn.Module.__init__(self)\n+        self.hidden_size = config.hidden_size\n+        self.intermediate_size = config.intermediate_size if intermediate_size is None else intermediate_size\n+        self.w1 = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.w3 = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.w2 = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n+\n+\n+class Lfm2MoeExperts(Qwen2MoeExperts):\n+    pass\n+\n+\n+class Lfm2MoeSparseMoeBlock(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.top_k = config.num_experts_per_tok\n+        self.routed_scaling_factor = config.routed_scaling_factor\n+        self.norm_topk_prob = config.norm_topk_prob\n+        self.use_expert_bias = config.use_expert_bias\n+\n+        self.gate = nn.Linear(config.hidden_size, config.num_experts, bias=False)\n+        self.experts = Lfm2MoeExperts(config)\n+        if self.use_expert_bias:\n+            self.register_buffer(\"expert_bias\", torch.zeros(config.num_experts, dtype=torch.float32))\n+\n+    def route_tokens_to_experts(self, router_logits):\n+        routing_weights = router_logits.sigmoid()\n+        if self.use_expert_bias:\n+            scores_for_routing = routing_weights + self.expert_bias\n+            _, selected_experts = torch.topk(scores_for_routing, k=self.top_k, dim=-1)\n+            routing_weights = torch.gather(routing_weights, dim=1, index=selected_experts).type_as(router_logits)\n+        else:\n+            routing_weights, selected_experts = torch.topk(routing_weights, k=self.top_k, dim=-1)\n+\n+        if self.norm_topk_prob:\n+            routing_weights = routing_weights / (routing_weights.sum(dim=-1, keepdim=True) + 1e-6)\n+        routing_weights = routing_weights * self.routed_scaling_factor\n+        return selected_experts, routing_weights\n+\n+    def forward(self, hidden_states: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n+        batch_size, sequence_length, hidden_dim = hidden_states.shape\n+        hidden_states_reshaped = hidden_states.view(-1, hidden_dim)\n+        router_logits = self.gate(hidden_states_reshaped)\n+        selected_experts, routing_weights = self.route_tokens_to_experts(router_logits)\n+        final_hidden_states = self.experts(hidden_states_reshaped, selected_experts, routing_weights)\n+        return final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n+\n+\n+class Lfm2MoeHybridConvCache(Lfm2HybridConvCache):\n+    pass\n+\n+\n+class Lfm2MoeAttention(Lfm2Attention):\n+    pass\n+\n+\n+class Lfm2MoeShortConv(Lfm2ShortConv):\n+    pass\n+\n+\n+class Lfm2MoeDecoderLayer(Lfm2DecoderLayer):\n+    def __init__(self, config: Lfm2MoeConfig, layer_idx: int):\n+        super().__init__(config, layer_idx)\n+        self.feed_forward = (\n+            Lfm2MoeMLP(config, intermediate_size=config.intermediate_size)\n+            if layer_idx < config.num_dense_layers\n+            else Lfm2MoeSparseMoeBlock(config)\n+        )\n+\n+\n+class Lfm2MoePreTrainedModel(LlamaPreTrainedModel):\n+    _can_compile_fullgraph = False\n+\n+\n+class Lfm2MoeModel(MixtralModel):\n+    def __init__(self, config: Lfm2MoeConfig):\n+        super().__init__(config)\n+        self.pos_emb = Lfm2MoeRotaryEmbedding(config)\n+        self.embedding_norm = Lfm2MoeRMSNorm(config.hidden_size, eps=config.norm_eps)\n+        del self.norm\n+        del self.rotary_emb\n+\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Lfm2MoeHybridConvCache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> MoeModelOutputWithPast:\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.embed_tokens(input_ids)\n+\n+        if use_cache and past_key_values is None:\n+            batch_size = inputs_embeds.shape[0]\n+            past_key_values = Lfm2MoeHybridConvCache(\n+                config=self.config, max_batch_size=batch_size, dtype=self.dtype, device=self.device\n+            )\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            position_ids=position_ids,\n+        )\n+\n+        hidden_states = inputs_embeds\n+        position_embeddings = self.pos_emb(hidden_states, position_ids)\n+\n+        # decoder layers\n+        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n+            hidden_states = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_values=past_key_values,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                **kwargs,\n+            )\n+\n+        hidden_states = self.embedding_norm(hidden_states)\n+\n+        return MoeModelOutputWithPast(\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values,\n+        )\n+\n+\n+class Lfm2MoeForCausalLM(LlamaForCausalLM):\n+    pass\n+\n+\n+__all__ = [\"Lfm2MoeForCausalLM\", \"Lfm2MoeModel\", \"Lfm2MoePreTrainedModel\"]"
      },
      {
        "filename": "tests/causal_lm_tester.py",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -448,6 +448,7 @@ def test_model_rope_scaling_frequencies(self):\n         # named location of the RoPE layer class.\n         base_model = self.model_tester.base_model_class(config)\n         possible_rope_attributes = [\n+            \"pos_emb\",\n             \"rotary_emb\",  # most common case\n             \"global_rotary_emb\",\n             \"local_rotary_emb\","
      },
      {
        "filename": "tests/models/lfm2/test_modeling_lfm2.py",
        "status": "modified",
        "additions": 77,
        "deletions": 14,
        "changes": 91,
        "patch": "@@ -23,12 +23,15 @@\n     require_torch,\n     require_torch_accelerator,\n     slow,\n+    torch_device,\n )\n \n from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n \n \n if is_torch_available():\n+    import torch\n+\n     from transformers import Lfm2ForCausalLM, Lfm2Model\n \n \n@@ -60,22 +63,82 @@ class Lfm2ModelTest(CausalLMModelTest, unittest.TestCase):\n     # used in `test_torch_compile_for_training`\n     _torch_compile_train_cls = Lfm2ForCausalLM if is_torch_available() else None\n \n-    @unittest.skip(\n-        \"Lfm2 alternates between attention and conv layers, so attention are only returned for attention layers\"\n-    )\n     def test_attention_outputs(self):\n-        pass\n-\n-    @unittest.skip(\"Lfm2 has a special cache format as it alternates between attention and conv layers\")\n+        \"\"\"Lfm2Moe alternates between attention and short-conv layers.\"\"\"\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        config.return_dict = True\n+        # force eager attention to support output attentions\n+        config._attn_implementation = \"eager\"\n+        seq_len = getattr(self.model_tester, \"seq_length\", None)\n+\n+        for model_class in self.all_model_classes:\n+            inputs_dict[\"output_attentions\"] = True\n+            inputs_dict[\"output_hidden_states\"] = False\n+            config.return_dict = True\n+            model = model_class._from_config(config, attn_implementation=\"eager\").to(torch_device).eval()\n+            config = model.config\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+            attentions = outputs.attentions\n+            self.assertEqual(len(attentions), sum(layer == \"full_attention\" for layer in config.layer_types))\n+\n+            # check that output_attentions also work using config\n+            del inputs_dict[\"output_attentions\"]\n+            config.output_attentions = True\n+            model = model_class(config).to(torch_device).eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+            attentions = outputs.attentions\n+            self.assertEqual(len(attentions), sum(layer == \"full_attention\" for layer in config.layer_types))\n+            self.assertListEqual(list(attentions[0].shape[-3:]), [config.num_attention_heads, seq_len, seq_len])\n+            out_len = len(outputs)\n+\n+            # Check attention is always last and order is fine\n+            inputs_dict[\"output_attentions\"] = True\n+            inputs_dict[\"output_hidden_states\"] = True\n+            model = model_class(config).to(torch_device).eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+                self_attentions = outputs.attentions\n+\n+            self.assertEqual(out_len + 1, len(outputs))\n+            self.assertEqual(len(self_attentions), sum(layer == \"full_attention\" for layer in config.layer_types))\n+            self.assertListEqual(list(self_attentions[0].shape[-3:]), [config.num_attention_heads, seq_len, seq_len])\n+\n+    @pytest.mark.generate\n     def test_past_key_values_format(self):\n-        pass\n-\n-    @unittest.skip(\n-        \"Lfm2 has a special cache format which is not compatible with compile as it has static address for conv cache\"\n-    )\n-    @pytest.mark.torch_compile_test\n-    def test_sdpa_can_compile_dynamic(self):\n-        pass\n+        \"\"\"Lfm2Moe has a special cache format as it alternates between attention and conv layers\"\"\"\n+        for model_class in self.all_generative_model_classes:\n+            config, inputs = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+            model = model_class(config).to(torch_device).eval()\n+            if \"use_cache\" not in inputs:\n+                inputs[\"use_cache\"] = True\n+            outputs = model(**inputs)\n+\n+            past_kv = outputs[\"past_key_values\"]\n+\n+            num_query_attention_heads = config.num_attention_heads\n+            embed_dim = config.hidden_size\n+            per_head_embed_dim = embed_dim // num_query_attention_heads\n+            num_key_value_heads = getattr(config, \"num_key_value_heads\", num_query_attention_heads)\n+\n+            batch_size, seq_length = inputs[\"input_ids\"].shape[:2]\n+            default_self_attention_shape = (batch_size, num_key_value_heads, seq_length, per_head_embed_dim)\n+            default_conv_shape = (batch_size, config.hidden_size, config.conv_L_cache)\n+\n+            num_cache_decoder_layers = len(past_kv)\n+            self.assertEqual(num_cache_decoder_layers, config.num_hidden_layers)\n+\n+            for i in range(config.num_hidden_layers):\n+                if config.layer_types[i] == \"full_attention\":\n+                    self_attention_layer_keys = past_kv.key_cache[i]\n+                    self_attention_layer_values = past_kv.value_cache[i]\n+                    self.assertEqual(self_attention_layer_keys.shape, default_self_attention_shape)\n+                    self.assertEqual(self_attention_layer_values.shape, default_self_attention_shape)\n+                else:\n+                    conv_layer = past_kv.conv_cache[i]\n+                    self.assertEqual(conv_layer.shape, default_conv_shape)\n \n \n @require_torch_accelerator"
      },
      {
        "filename": "tests/models/lfm2_moe/__init__.py",
        "status": "added",
        "additions": 0,
        "deletions": 0,
        "changes": 0,
        "patch": ""
      },
      {
        "filename": "tests/models/lfm2_moe/test_modeling_lfm2_moe.py",
        "status": "added",
        "additions": 246,
        "deletions": 0,
        "changes": 246,
        "patch": "@@ -0,0 +1,246 @@\n+# coding=utf-8\n+# Copyright 2025 the HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch LLaMA model.\"\"\"\n+\n+import unittest\n+\n+import pytest\n+\n+from transformers import AutoTokenizer, is_torch_available, set_seed\n+from transformers.testing_utils import (\n+    cleanup,\n+    require_read_token,\n+    require_torch,\n+    require_torch_accelerator,\n+    slow,\n+    torch_device,\n+)\n+\n+from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+    from transformers import Lfm2MoeConfig, Lfm2MoeForCausalLM, Lfm2MoeModel\n+\n+\n+class Lfm2MoeModelTester(CausalLMModelTester):\n+    if is_torch_available():\n+        config_class = Lfm2MoeConfig\n+        base_model_class = Lfm2MoeModel\n+        causal_lm_class = Lfm2MoeForCausalLM\n+\n+    def __init__(\n+        self,\n+        parent,\n+        layer_types=[\"full_attention\", \"conv\"],\n+    ):\n+        super().__init__(parent)\n+        self.layer_types = layer_types\n+\n+\n+@require_torch\n+class Lfm2MoeModelTest(CausalLMModelTest, unittest.TestCase):\n+    all_model_classes = (Lfm2MoeModel, Lfm2MoeForCausalLM) if is_torch_available() else ()\n+    pipeline_model_mapping = (\n+        {\n+            \"feature-extraction\": Lfm2MoeModel,\n+            \"text-generation\": Lfm2MoeForCausalLM,\n+        }\n+        if is_torch_available()\n+        else {}\n+    )\n+    test_headmasking = False\n+    test_pruning = False\n+    fx_compatible = False\n+    model_tester_class = Lfm2MoeModelTester\n+    # used in `test_torch_compile_for_training`\n+    _torch_compile_train_cls = Lfm2MoeForCausalLM if is_torch_available() else None\n+\n+    def test_attention_outputs(self):\n+        \"\"\"Lfm2Moe alternates between attention and short-conv layers.\"\"\"\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        config.return_dict = True\n+        # force eager attention to support output attentions\n+        config._attn_implementation = \"eager\"\n+        seq_len = getattr(self.model_tester, \"seq_length\", None)\n+\n+        for model_class in self.all_model_classes:\n+            inputs_dict[\"output_attentions\"] = True\n+            inputs_dict[\"output_hidden_states\"] = False\n+            config.return_dict = True\n+            model = model_class._from_config(config, attn_implementation=\"eager\").to(torch_device).eval()\n+            config = model.config\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+            attentions = outputs.attentions\n+            self.assertEqual(len(attentions), sum(layer == \"full_attention\" for layer in config.layer_types))\n+\n+            # check that output_attentions also work using config\n+            del inputs_dict[\"output_attentions\"]\n+            config.output_attentions = True\n+            model = model_class(config).to(torch_device).eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+            attentions = outputs.attentions\n+            self.assertEqual(len(attentions), sum(layer == \"full_attention\" for layer in config.layer_types))\n+            self.assertListEqual(list(attentions[0].shape[-3:]), [config.num_attention_heads, seq_len, seq_len])\n+            out_len = len(outputs)\n+\n+            # Check attention is always last and order is fine\n+            inputs_dict[\"output_attentions\"] = True\n+            inputs_dict[\"output_hidden_states\"] = True\n+            model = model_class(config).to(torch_device).eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+                self_attentions = outputs.attentions\n+\n+            self.assertEqual(out_len + 1, len(outputs))\n+            self.assertEqual(len(self_attentions), sum(layer == \"full_attention\" for layer in config.layer_types))\n+            self.assertListEqual(list(self_attentions[0].shape[-3:]), [config.num_attention_heads, seq_len, seq_len])\n+\n+    @pytest.mark.generate\n+    def test_past_key_values_format(self):\n+        \"\"\"Lfm2Moe has a special cache format as it alternates between attention and conv layers\"\"\"\n+        for model_class in self.all_generative_model_classes:\n+            config, inputs = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+            model = model_class(config).to(torch_device).eval()\n+            if \"use_cache\" not in inputs:\n+                inputs[\"use_cache\"] = True\n+            outputs = model(**inputs)\n+\n+            past_kv = outputs[\"past_key_values\"]\n+\n+            num_query_attention_heads = config.num_attention_heads\n+            embed_dim = config.hidden_size\n+            per_head_embed_dim = embed_dim // num_query_attention_heads\n+            num_key_value_heads = getattr(config, \"num_key_value_heads\", num_query_attention_heads)\n+\n+            batch_size, seq_length = inputs[\"input_ids\"].shape[:2]\n+            default_self_attention_shape = (batch_size, num_key_value_heads, seq_length, per_head_embed_dim)\n+            default_conv_shape = (batch_size, config.hidden_size, config.conv_L_cache)\n+\n+            num_cache_decoder_layers = len(past_kv)\n+            self.assertEqual(num_cache_decoder_layers, config.num_hidden_layers)\n+\n+            for i in range(config.num_hidden_layers):\n+                if config.layer_types[i] == \"full_attention\":\n+                    self_attention_layer_keys = past_kv.key_cache[i]\n+                    self_attention_layer_values = past_kv.value_cache[i]\n+                    self.assertEqual(self_attention_layer_keys.shape, default_self_attention_shape)\n+                    self.assertEqual(self_attention_layer_values.shape, default_self_attention_shape)\n+                else:\n+                    conv_layer = past_kv.conv_cache[i]\n+                    self.assertEqual(conv_layer.shape, default_conv_shape)\n+\n+\n+@require_torch_accelerator\n+@require_read_token\n+@slow\n+class Lfm2MoeIntegrationTest(unittest.TestCase):\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.model = None\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        del cls.model\n+        cleanup(torch_device, gc_collect=True)\n+\n+    def tearDown(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n+    @classmethod\n+    def get_model(cls):\n+        if cls.model is None:\n+            cls.model = Lfm2MoeForCausalLM.from_pretrained(\n+                \"LiquidAI/LFM2-8B-A1B\", device_map=\"auto\", dtype=torch.bfloat16\n+            )\n+        return cls.model\n+\n+    @slow\n+    def test_model_1a8b_logits(self):\n+        set_seed(1789)\n+        input_ids = [1, 22998, 768, 1947, 797, 22017, 811, 6332, 928, 5743, 797, 779, 48123, 772, 33551, 60996, 523]\n+        model = self.get_model()\n+        input_ids = torch.tensor([input_ids]).to(model.model.embed_tokens.weight.device)\n+        with torch.no_grad():\n+            out = model(input_ids).logits.float().cpu()\n+        # Expected mean on dim = -1\n+        EXPECTED_MEAN = torch.tensor(\n+            [\n+                [\n+                    -1.3855,\n+                    -0.5123,\n+                    -1.3143,\n+                    -1.2144,\n+                    -1.0791,\n+                    -1.2117,\n+                    -1.4704,\n+                    -0.7648,\n+                    -0.6175,\n+                    -1.2402,\n+                    -1.1459,\n+                    -1.0083,\n+                    -1.0247,\n+                    -0.8830,\n+                    -1.5643,\n+                    -1.7266,\n+                    -1.6254,\n+                ]\n+            ]\n+        )\n+        torch.testing.assert_close(out.mean(-1), EXPECTED_MEAN, rtol=1e-2, atol=1e-2)\n+        # Expected portion of the logits\n+        EXPECTED_SLICE = torch.tensor(\n+            [-1.2656, 2.4844, 5.5000, -1.3359, -1.3203, -1.3438, 1.9375, 5.8438, -0.6523, -1.2891]\n+        )\n+        torch.testing.assert_close(out[0, 0, :10], EXPECTED_SLICE, rtol=1e-4, atol=1e-4)\n+\n+    @slow\n+    def test_model_1a8b_generation(self):\n+        EXPECTED_TEXT_COMPLETION = \"\"\"In 1st century A.D., the Roman Empire controlled much of Europe, North Africa, and parts of the Middle East.\"\"\"\n+        set_seed(1789)\n+        prompt = \"In 1st century A.D., the Roman Empire\"\n+        tokenizer = AutoTokenizer.from_pretrained(\"LiquidAI/LFM2-8B-A1B\", use_fast=False)\n+        model = self.get_model()\n+        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=True).to(\n+            model.model.embed_tokens.weight.device\n+        )\n+        with torch.no_grad():\n+            generated_ids = model.generate(input_ids, max_new_tokens=15, do_sample=False)\n+        text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n+        self.assertEqual(EXPECTED_TEXT_COMPLETION, text)\n+\n+    @slow\n+    def test_model_1a8b_batched_chat_generation(self):\n+        prompts = [\"Who are you?\", \"Complete the text: Lorem ipsum dolor \", \"The Meji Restoration in Japan ended\"]\n+        EXPECTED_TEXT_COMPLETIONS = [\n+            \"Who are you??  \\nI am an artificial intelligence assistant designed to provide information, answer questions\",\n+            \"Complete the text: Lorem ipsum dolor ipsum dolor ipsum dolor ipsum dolor ipsum dolor\",\n+            \"The Meji Restoration in Japan ended (1868) marked the:  \\nA) Establishment of a constitutional\",\n+        ]\n+        set_seed(1789)\n+        tokenizer = AutoTokenizer.from_pretrained(\"LiquidAI/LFM2-8B-A1B\", use_fast=False)\n+        model = self.get_model()\n+        batched_input_ids = tokenizer(prompts, return_tensors=\"pt\", padding=True).to(\n+            model.model.embed_tokens.weight.device\n+        )\n+        with torch.no_grad():\n+            generated_ids = model.generate(**batched_input_ids, max_new_tokens=15, do_sample=False)\n+        text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n+        self.assertEqual(EXPECTED_TEXT_COMPLETIONS, text)"
      },
      {
        "filename": "utils/check_config_attributes.py",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -36,6 +36,7 @@\n     \"Ernie4_5Config\": [\"tie_word_embeddings\"],\n     \"Ernie4_5_MoeConfig\": [\"tie_word_embeddings\"],\n     \"Lfm2Config\": [\"full_attn_idxs\", \"tie_word_embeddings\"],\n+    \"Lfm2MoeConfig\": [\"tie_word_embeddings\"],\n     # used internally during generation to provide the custom logit processors with their necessary information\n     \"DiaConfig\": [\n         \"delay_pattern\","
      }
    ],
    "num_files": 17,
    "scraped_at": "2025-11-16T21:18:12.158761",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR implements a new Mixture-of-Experts (MoE) model architecture (LFM2-MoE) with significant code additions including configuration classes, modeling layers with sparse MoE feed-forward networks, and integration into the transformers framework. The PR contains substantial architectural logic around MoE routing, expert selection, and hybrid attention mechanisms that would generate meaningful technical questions about how these components interact.",
      "substance_level": "high"
    }
  },
  {
    "pr_number": 41394,
    "title": "Adding superglue fast image processing",
    "body": "# What does this PR do?\r\n\r\nTLDR :\r\n- Implement fast processor for SuperGlue\r\n- About 3 times faster\r\n\r\nThis PR aims to translate the features of the class `SuperGlueImageProcessor` in the fast equivalent class `SuperGlueImageProcessorFast`.\r\nThe implementation heavily follows the standard implementation but reduces memory consumption and about 3 times the execution speed on my hardware.\r\nThe implementation mostly refactor the image formatting in the `preprocessing` step, notably by using torch tensors instead of PIL or Numpy.\r\n\r\n\r\n## Test Performed\r\nRUN_SLOW=1 python -m pytest tests/models/superglue/test_image_processing_superglue.py\r\n\r\nWith an additional test based on the default processor tester (this test has not to be included in the repo) :\r\n```python\r\n@require_vision\r\n@require_torch\r\ndef test_fast_is_faster_than_slow(self):\r\n    if not self.test_slow_image_processor or not self.test_fast_image_processor:\r\n        self.skipTest(reason=\"Skipping speed test\")\r\n\r\n    if self.image_processing_class is None or self.fast_image_processing_class is None:\r\n        self.skipTest(reason=\"Skipping speed test as one of the image processors is not defined\")\r\n\r\n    def measure_time(image_processor, image):\r\n        # Warmup\r\n        for _ in range(5):\r\n            _ = image_processor(image, return_tensors=\"pt\")\r\n        all_times = []\r\n        for _ in range(10):\r\n            start = time.time()\r\n            _ = image_processor(image, return_tensors=\"pt\")\r\n            all_times.append(time.time() - start)\r\n        # Take the average of the fastest 3 runs\r\n        avg_time = sum(sorted(all_times[:3])) / 3.0\r\n        return avg_time\r\n\r\n    dummy_images = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\r\n    image_processor_slow = self.image_processing_class(**self.image_processor_dict)\r\n    image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\r\n\r\n    fast_time = measure_time(image_processor_fast, dummy_images)\r\n    slow_time = measure_time(image_processor_slow, dummy_images)\r\n\r\n    self.assertLessEqual(fast_time, slow_time)\r\n```\r\nBy reviewing the flame graph, I noticed the improvement in every `__calls__` made to the fast version.\r\n\r\nCallers of the old processor, and the full execution time of the method:\r\n<img width=\"1239\" height=\"391\" alt=\"image\" src=\"https://github.com/user-attachments/assets/0fedbb2f-991c-481e-9425-b041b4f31767\" />\r\n\r\nThe equivalent but with the fast processor: \r\n<img width=\"1255\" height=\"385\" alt=\"image\" src=\"https://github.com/user-attachments/assets/ff658450-0c8c-459d-b58c-6da727555558\" />\r\n\r\nSome calls made during the test passes directly to the preprocess function, without passing by the `__call__` one, I am including them as well:\r\nSlow\r\n<img width=\"1150\" height=\"209\" alt=\"image\" src=\"https://github.com/user-attachments/assets/b763e4c8-0336-42fe-a1de-3611dc2b0f66\" />\r\nFast\r\n<img width=\"1230\" height=\"176\" alt=\"image\" src=\"https://github.com/user-attachments/assets/9979f600-e03b-4732-85eb-91a9ded018ee\" />\r\n\r\n## Before submitting\r\n- [    ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [ X ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),\r\n      Pull Request section?\r\n- [ X ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n           link :  [Contributions Welcome] Add Fast Image Processors https://github.com/huggingface/transformers/issues/36978#issue-2947632853\r\n- [ X ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [    ] Did you write any new necessary tests? \r\n\r\n\r\n## Who can review?\r\nThank you for reviewing my PR @yonigozlan (or anyone else :) )\r\n",
    "html_url": "https://github.com/huggingface/transformers/pull/41394",
    "created_at": "2025-10-06T22:29:36Z",
    "merged_at": "2025-10-16T19:34:10Z",
    "merge_commit_sha": "354567d955fbc5fbd70fc841b7a7bcc654bea3f1",
    "base_ref": "main",
    "head_sha": "4774877ad594a80466aab69c8938e5e96fb5ea55",
    "user": "AlphaOrOmega",
    "files": [
      {
        "filename": "docs/source/en/model_doc/superglue.md",
        "status": "modified",
        "additions": 11,
        "deletions": 4,
        "changes": 15,
        "patch": "@@ -88,16 +88,16 @@ processed_outputs = processor.post_process_keypoint_matching(outputs, image_size\n     import torch\n     from PIL import Image\n     import requests\n-    \n+\n     processor = AutoImageProcessor.from_pretrained(\"magic-leap-community/superglue_outdoor\")\n     model = AutoModel.from_pretrained(\"magic-leap-community/superglue_outdoor\")\n-    \n+\n     # SuperGlue requires pairs of images\n     images = [image1, image2]\n     inputs = processor(images, return_tensors=\"pt\")\n     with torch.inference_mode():\n         outputs = model(**inputs)\n-    \n+\n     # Extract matching information\n     keypoints0 = outputs.keypoints0  # Keypoints in first image\n     keypoints1 = outputs.keypoints1  # Keypoints in second image\n@@ -112,7 +112,7 @@ processed_outputs = processor.post_process_keypoint_matching(outputs, image_size\n     # Process outputs for visualization\n     image_sizes = [[(image.height, image.width) for image in images]]\n     processed_outputs = processor.post_process_keypoint_matching(outputs, image_sizes, threshold=0.2)\n-    \n+\n     for i, output in enumerate(processed_outputs):\n         print(f\"For the image pair {i}\")\n         for keypoint0, keypoint1, matching_score in zip(\n@@ -147,6 +147,13 @@ processed_outputs = processor.post_process_keypoint_matching(outputs, image_size\n     - post_process_keypoint_matching\n     - visualize_keypoint_matching\n \n+## SuperGlueImageProcessorFast\n+\n+[[autodoc]] SuperGlueImageProcessorFast\n+    - preprocess\n+    - post_process_keypoint_matching\n+    - visualize_keypoint_matching\n+\n ## SuperGlueForKeypointMatching\n \n [[autodoc]] SuperGlueForKeypointMatching"
      },
      {
        "filename": "src/transformers/models/auto/image_processing_auto.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -171,7 +171,7 @@\n             (\"siglip\", (\"SiglipImageProcessor\", \"SiglipImageProcessorFast\")),\n             (\"siglip2\", (\"Siglip2ImageProcessor\", \"Siglip2ImageProcessorFast\")),\n             (\"smolvlm\", (\"SmolVLMImageProcessor\", \"SmolVLMImageProcessorFast\")),\n-            (\"superglue\", (\"SuperGlueImageProcessor\", None)),\n+            (\"superglue\", (\"SuperGlueImageProcessor\", \"SuperGlueImageProcessorFast\")),\n             (\"superpoint\", (\"SuperPointImageProcessor\", \"SuperPointImageProcessorFast\")),\n             (\"swiftformer\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n             (\"swin\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),"
      },
      {
        "filename": "src/transformers/models/efficientloftr/image_processing_efficientloftr_fast.py",
        "status": "modified",
        "additions": 13,
        "deletions": 34,
        "changes": 47,
        "patch": "@@ -1,30 +1,17 @@\n-# coding=utf-8\n-# Copyright 2025 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Fast Image processor class for EfficientLoFTR.\"\"\"\n-\n-from typing import TYPE_CHECKING, Optional, Union\n+#                \ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\n+#           This file was automatically generated from src/transformers/models/efficientloftr/modular_efficientloftr.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_efficientloftr.py file directly. One of our CI enforces this.\n+#                \ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\n+from typing import Optional, Union\n \n import torch\n from PIL import Image, ImageDraw\n+from torchvision.transforms.v2 import functional as F\n \n-from ...image_processing_utils import BatchFeature\n-from ...image_processing_utils_fast import (\n-    BaseImageProcessorFast,\n-    group_images_by_shape,\n-    reorder_images,\n-)\n+from ...image_processing_utils_fast import BaseImageProcessorFast, BatchFeature\n+from ...image_transforms import group_images_by_shape, reorder_images\n from ...image_utils import (\n     ImageInput,\n     ImageType,\n@@ -35,17 +22,9 @@\n     is_valid_image,\n )\n from ...processing_utils import Unpack\n-from ...utils import (\n-    TensorType,\n-    auto_docstring,\n-)\n+from ...utils import TensorType, auto_docstring\n from .image_processing_efficientloftr import EfficientLoFTRImageProcessorKwargs\n-\n-\n-if TYPE_CHECKING:\n-    from .modeling_efficientloftr import KeypointMatchingOutput\n-\n-import torchvision.transforms.v2.functional as F\n+from .modeling_efficientloftr import KeypointMatchingOutput\n \n \n def _is_valid_image(image):\n@@ -299,7 +278,7 @@ def _get_color(self, score):\n         r = int(255 * (1 - score))\n         g = int(255 * score)\n         b = 0\n-        return (r, g, b)\n+        return r, g, b\n \n \n __all__ = [\"EfficientLoFTRImageProcessorFast\"]"
      },
      {
        "filename": "src/transformers/models/efficientloftr/modular_efficientloftr.py",
        "status": "added",
        "additions": 8,
        "deletions": 0,
        "changes": 8,
        "patch": "@@ -0,0 +1,8 @@\n+from ..superglue.image_processing_superglue_fast import SuperGlueImageProcessorFast\n+\n+\n+class EfficientLoFTRImageProcessorFast(SuperGlueImageProcessorFast):\n+    pass\n+\n+\n+__all__ = [\"EfficientLoFTRImageProcessorFast\"]"
      },
      {
        "filename": "src/transformers/models/superglue/__init__.py",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -20,6 +20,7 @@\n if TYPE_CHECKING:\n     from .configuration_superglue import *\n     from .image_processing_superglue import *\n+    from .image_processing_superglue_fast import *\n     from .modeling_superglue import *\n else:\n     import sys"
      },
      {
        "filename": "src/transformers/models/superglue/image_processing_superglue.py",
        "status": "modified",
        "additions": 10,
        "deletions": 0,
        "changes": 10,
        "patch": "@@ -35,6 +35,7 @@\n     valid_images,\n     validate_preprocess_arguments,\n )\n+from ...processing_utils import ImagesKwargs\n from ...utils import TensorType, logging, requires_backends\n from ...utils.import_utils import requires\n \n@@ -133,6 +134,15 @@ def _is_valid_image(image):\n     raise ValueError(error_message)\n \n \n+class SuperGlueImageProcessorKwargs(ImagesKwargs, total=False):\n+    r\"\"\"\n+    do_grayscale (`bool`, *optional*, defaults to `True`):\n+        Whether to convert the image to grayscale. Can be overridden by `do_grayscale` in the `preprocess` method.\n+    \"\"\"\n+\n+    do_grayscale: bool\n+\n+\n @requires(backends=(\"torch\",))\n class SuperGlueImageProcessor(BaseImageProcessor):\n     r\"\"\""
      },
      {
        "filename": "src/transformers/models/superglue/image_processing_superglue_fast.py",
        "status": "added",
        "additions": 292,
        "deletions": 0,
        "changes": 292,
        "patch": "@@ -0,0 +1,292 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import Optional, Union\n+\n+import torch\n+from PIL import Image, ImageDraw\n+from torchvision.transforms.v2 import functional as F\n+\n+from ...image_processing_utils_fast import BaseImageProcessorFast, BatchFeature\n+from ...image_transforms import group_images_by_shape, reorder_images\n+from ...image_utils import (\n+    ImageInput,\n+    ImageType,\n+    PILImageResampling,\n+    SizeDict,\n+    get_image_type,\n+    is_pil_image,\n+    is_valid_image,\n+)\n+from ...processing_utils import Unpack\n+from ...utils import TensorType, auto_docstring\n+from .image_processing_superglue import SuperGlueImageProcessorKwargs\n+from .modeling_superglue import KeypointMatchingOutput\n+\n+\n+def _is_valid_image(image):\n+    return is_pil_image(image) or (\n+        is_valid_image(image) and get_image_type(image) != ImageType.PIL and len(image.shape) == 3\n+    )\n+\n+\n+def flatten_pair_images(images):\n+    # Handle the pair validation and flattening similar to slow processor\n+    if isinstance(images, list):\n+        if len(images) == 2 and all((_is_valid_image(image) or isinstance(image, torch.Tensor)) for image in images):\n+            # Single pair of images - keep as is, they'll be processed by the base class\n+            return images\n+        elif all(\n+            isinstance(image_pair, list)\n+            and len(image_pair) == 2\n+            and all(_is_valid_image(image) or isinstance(image, torch.Tensor) for image in image_pair)\n+            for image_pair in images\n+        ):\n+            # Multiple pairs - flatten them\n+            images = [image for image_pair in images for image in image_pair]\n+            return images\n+    raise ValueError(\n+        \"Input images must be a one of the following :\",\n+        \" - A pair of PIL images.\",\n+        \" - A pair of 3D arrays.\",\n+        \" - A list of pairs of PIL images.\",\n+        \" - A list of pairs of 3D arrays.\",\n+    )\n+\n+\n+def is_grayscale(\n+    image: \"torch.Tensor\",\n+):\n+    \"\"\"Checks if an image is grayscale (all RGB channels are identical).\"\"\"\n+    if image.ndim < 3 or image.shape[0 if image.ndim == 3 else 1] == 1:\n+        return True\n+    return torch.all(image[..., 0, :, :] == image[..., 1, :, :]) and torch.all(\n+        image[..., 1, :, :] == image[..., 2, :, :]\n+    )\n+\n+\n+def convert_to_grayscale(\n+    image: \"torch.Tensor\",\n+) -> \"torch.Tensor\":\n+    \"\"\"\n+    Converts an image to grayscale format using the NTSC formula. Only support torch.Tensor.\n+\n+    This function is supposed to return a 1-channel image, but it returns a 3-channel image with the same value in each\n+    channel, because of an issue that is discussed in :\n+    https://github.com/huggingface/transformers/pull/25786#issuecomment-1730176446\n+\n+    Args:\n+        image (torch.Tensor):\n+            The image to convert.\n+    \"\"\"\n+    if is_grayscale(image):\n+        return image\n+    return F.rgb_to_grayscale(image, num_output_channels=3)\n+\n+\n+@auto_docstring\n+class SuperGlueImageProcessorFast(BaseImageProcessorFast):\n+    resample = PILImageResampling.BILINEAR\n+    size = {\"height\": 480, \"width\": 640}\n+    default_to_square = False\n+    do_resize = True\n+    do_rescale = True\n+    rescale_factor = 1 / 255\n+    do_normalize = None\n+    valid_kwargs = SuperGlueImageProcessorKwargs\n+\n+    def __init__(self, **kwargs: Unpack[SuperGlueImageProcessorKwargs]):\n+        super().__init__(**kwargs)\n+\n+    @auto_docstring\n+    def preprocess(self, images: ImageInput, **kwargs: Unpack[SuperGlueImageProcessorKwargs]) -> BatchFeature:\n+        return super().preprocess(images, **kwargs)\n+\n+    def _prepare_images_structure(\n+        self,\n+        images: ImageInput,\n+        **kwargs,\n+    ) -> ImageInput:\n+        # we need to handle image pairs validation and flattening\n+        return flatten_pair_images(images)\n+\n+    def _preprocess(\n+        self,\n+        images: list[\"torch.Tensor\"],\n+        size: Union[dict[str, int], SizeDict],\n+        rescale_factor: float,\n+        do_rescale: bool,\n+        do_resize: bool,\n+        interpolation: Optional[\"F.InterpolationMode\"],\n+        do_grayscale: bool,\n+        disable_grouping: bool,\n+        return_tensors: Union[str, TensorType],\n+        **kwargs,\n+    ) -> BatchFeature:\n+        grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n+        processed_images_grouped = {}\n+\n+        for shape, stacked_images in grouped_images.items():\n+            if do_resize:\n+                stacked_images = self.resize(stacked_images, size=size, interpolation=interpolation)\n+            processed_images_grouped[shape] = stacked_images\n+        resized_images = reorder_images(processed_images_grouped, grouped_images_index)\n+\n+        grouped_images, grouped_images_index = group_images_by_shape(resized_images, disable_grouping=disable_grouping)\n+        processed_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            if do_rescale:\n+                stacked_images = self.rescale(stacked_images, rescale_factor)\n+            if do_grayscale:\n+                stacked_images = convert_to_grayscale(stacked_images)\n+            processed_images_grouped[shape] = stacked_images\n+\n+        processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n+\n+        # Convert back to pairs format\n+        image_pairs = [processed_images[i : i + 2] for i in range(0, len(processed_images), 2)]\n+\n+        # Stack each pair into a single tensor to match slow processor format\n+        stacked_pairs = [torch.stack(pair, dim=0) for pair in image_pairs]\n+\n+        # Return in same format as slow processor\n+        image_pairs = torch.stack(stacked_pairs, dim=0) if return_tensors else stacked_pairs\n+\n+        return BatchFeature(data={\"pixel_values\": image_pairs})\n+\n+    def post_process_keypoint_matching(\n+        self,\n+        outputs: \"KeypointMatchingOutput\",\n+        target_sizes: Union[TensorType, list[tuple]],\n+        threshold: float = 0.0,\n+    ) -> list[dict[str, torch.Tensor]]:\n+        \"\"\"\n+        Converts the raw output of [`KeypointMatchingOutput`] into lists of keypoints, scores and descriptors\n+        with coordinates absolute to the original image sizes.\n+        Args:\n+            outputs ([`KeypointMatchingOutput`]):\n+                Raw outputs of the model.\n+            target_sizes (`torch.Tensor` or `List[Tuple[Tuple[int, int]]]`, *optional*):\n+                Tensor of shape `(batch_size, 2, 2)` or list of tuples of tuples (`Tuple[int, int]`) containing the\n+                target size `(height, width)` of each image in the batch. This must be the original image size (before\n+                any processing).\n+            threshold (`float`, *optional*, defaults to 0.0):\n+                Threshold to filter out the matches with low scores.\n+        Returns:\n+            `List[Dict]`: A list of dictionaries, each dictionary containing the keypoints in the first and second image\n+            of the pair, the matching scores and the matching indices.\n+        \"\"\"\n+        if outputs.matches.shape[0] != len(target_sizes):\n+            raise ValueError(\"Make sure that you pass in as many target sizes as the batch dimension of the mask\")\n+        if not all(len(target_size) == 2 for target_size in target_sizes):\n+            raise ValueError(\"Each element of target_sizes must contain the size (h, w) of each image of the batch\")\n+\n+        if isinstance(target_sizes, list):\n+            image_pair_sizes = torch.tensor(target_sizes, device=outputs.matches.device)\n+        else:\n+            if target_sizes.shape[1] != 2 or target_sizes.shape[2] != 2:\n+                raise ValueError(\n+                    \"Each element of target_sizes must contain the size (h, w) of each image of the batch\"\n+                )\n+            image_pair_sizes = target_sizes\n+\n+        keypoints = outputs.keypoints.clone()\n+        keypoints = keypoints * image_pair_sizes.flip(-1).reshape(-1, 2, 1, 2)\n+        keypoints = keypoints.to(torch.int32)\n+\n+        results = []\n+        for keypoints_pair, matches, scores in zip(keypoints, outputs.matches, outputs.matching_scores):\n+            # Filter out matches with low scores\n+            valid_matches = torch.logical_and(scores > threshold, matches > -1)\n+\n+            matched_keypoints0 = keypoints_pair[0][valid_matches[0]]\n+            matched_keypoints1 = keypoints_pair[1][valid_matches[1]]\n+            matching_scores = scores[0][valid_matches[0]]\n+\n+            results.append(\n+                {\n+                    \"keypoints0\": matched_keypoints0,\n+                    \"keypoints1\": matched_keypoints1,\n+                    \"matching_scores\": matching_scores,\n+                }\n+            )\n+\n+        return results\n+\n+    def visualize_keypoint_matching(\n+        self,\n+        images,\n+        keypoint_matching_output: list[dict[str, torch.Tensor]],\n+    ) -> list[\"Image.Image\"]:\n+        \"\"\"\n+        Plots the image pairs side by side with the detected keypoints as well as the matching between them.\n+\n+        Args:\n+            images:\n+                Image pairs to plot. Same as `EfficientLoFTRImageProcessor.preprocess`. Expects either a list of 2\n+                images or a list of list of 2 images list with pixel values ranging from 0 to 255.\n+            keypoint_matching_output (List[Dict[str, torch.Tensor]]]):\n+                A post processed keypoint matching output\n+\n+        Returns:\n+            `List[PIL.Image.Image]`: A list of PIL images, each containing the image pairs side by side with the detected\n+            keypoints as well as the matching between them.\n+        \"\"\"\n+        from ...image_utils import to_numpy_array\n+        from .image_processing_superglue import validate_and_format_image_pairs\n+\n+        images = validate_and_format_image_pairs(images)\n+        images = [to_numpy_array(image) for image in images]\n+        image_pairs = [images[i : i + 2] for i in range(0, len(images), 2)]\n+\n+        results = []\n+        for image_pair, pair_output in zip(image_pairs, keypoint_matching_output):\n+            height0, width0 = image_pair[0].shape[:2]\n+            height1, width1 = image_pair[1].shape[:2]\n+            plot_image = torch.zeros((max(height0, height1), width0 + width1, 3), dtype=torch.uint8)\n+            plot_image[:height0, :width0] = torch.from_numpy(image_pair[0])\n+            plot_image[:height1, width0:] = torch.from_numpy(image_pair[1])\n+\n+            plot_image_pil = Image.fromarray(plot_image.numpy())\n+            draw = ImageDraw.Draw(plot_image_pil)\n+\n+            keypoints0_x, keypoints0_y = pair_output[\"keypoints0\"].unbind(1)\n+            keypoints1_x, keypoints1_y = pair_output[\"keypoints1\"].unbind(1)\n+            for keypoint0_x, keypoint0_y, keypoint1_x, keypoint1_y, matching_score in zip(\n+                keypoints0_x, keypoints0_y, keypoints1_x, keypoints1_y, pair_output[\"matching_scores\"]\n+            ):\n+                color = self._get_color(matching_score)\n+                draw.line(\n+                    (keypoint0_x, keypoint0_y, keypoint1_x + width0, keypoint1_y),\n+                    fill=color,\n+                    width=3,\n+                )\n+                draw.ellipse((keypoint0_x - 2, keypoint0_y - 2, keypoint0_x + 2, keypoint0_y + 2), fill=\"black\")\n+                draw.ellipse(\n+                    (keypoint1_x + width0 - 2, keypoint1_y - 2, keypoint1_x + width0 + 2, keypoint1_y + 2),\n+                    fill=\"black\",\n+                )\n+\n+            results.append(plot_image_pil)\n+        return results\n+\n+    def _get_color(self, score):\n+        \"\"\"Maps a score to a color.\"\"\"\n+        r = int(255 * (1 - score))\n+        g = int(255 * score)\n+        b = 0\n+        return r, g, b\n+\n+\n+__all__ = [\"SuperGlueImageProcessorFast\"]"
      },
      {
        "filename": "tests/models/efficientloftr/test_image_processing_efficientloftr.py",
        "status": "modified",
        "additions": 0,
        "deletions": 67,
        "changes": 67,
        "patch": "@@ -15,19 +15,14 @@\n import unittest\n \n import numpy as np\n-import pytest\n-from packaging import version\n \n from tests.models.superglue.test_image_processing_superglue import (\n     SuperGlueImageProcessingTest,\n     SuperGlueImageProcessingTester,\n )\n from transformers.testing_utils import (\n     require_torch,\n-    require_torch_accelerator,\n     require_vision,\n-    slow,\n-    torch_device,\n )\n from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n@@ -103,46 +98,6 @@ def setUp(self) -> None:\n         super().setUp()\n         self.image_processor_tester = EfficientLoFTRImageProcessingTester(self)\n \n-    def test_slow_fast_equivalence(self):\n-        \"\"\"Override the generic test since EfficientLoFTR requires image pairs.\"\"\"\n-        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n-            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n-\n-        if self.image_processing_class is None or self.fast_image_processing_class is None:\n-            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n-\n-        # Create image pairs instead of single images\n-        dummy_images = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=False)\n-        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n-        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n-\n-        encoding_slow = image_processor_slow(dummy_images, return_tensors=\"pt\")\n-        encoding_fast = image_processor_fast(dummy_images, return_tensors=\"pt\")\n-        self._assert_slow_fast_tensors_equivalence(encoding_slow.pixel_values, encoding_fast.pixel_values)\n-\n-    def test_slow_fast_equivalence_batched(self):\n-        \"\"\"Override the generic test since EfficientLoFTR requires image pairs.\"\"\"\n-        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n-            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n-\n-        if self.image_processing_class is None or self.fast_image_processing_class is None:\n-            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n-\n-        if hasattr(self.image_processor_tester, \"do_center_crop\") and self.image_processor_tester.do_center_crop:\n-            self.skipTest(\n-                reason=\"Skipping as do_center_crop is True and center_crop functions are not equivalent for fast and slow processors\"\n-            )\n-\n-        # Create image pairs instead of single images\n-        dummy_images = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n-        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n-        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n-\n-        encoding_slow = image_processor_slow(dummy_images, return_tensors=\"pt\")\n-        encoding_fast = image_processor_fast(dummy_images, return_tensors=\"pt\")\n-\n-        self._assert_slow_fast_tensors_equivalence(encoding_slow.pixel_values, encoding_fast.pixel_values)\n-\n     @unittest.skip(reason=\"Many failing cases. This test needs a more deep investigation.\")\n     def test_fast_is_faster_than_slow(self):\n         \"\"\"Override the generic test since EfficientLoFTR requires image pairs.\"\"\"\n@@ -173,25 +128,3 @@ def test_fast_is_faster_than_slow(self):\n         self.assertLessEqual(\n             fast_time, slow_time * 1.2, \"Fast processor should not be significantly slower than slow processor\"\n         )\n-\n-    @slow\n-    @require_torch_accelerator\n-    @require_vision\n-    @pytest.mark.torch_compile_test\n-    def test_can_compile_fast_image_processor(self):\n-        \"\"\"Override the generic test since EfficientLoFTR requires image pairs.\"\"\"\n-        if self.fast_image_processing_class is None:\n-            self.skipTest(\"Skipping compilation test as fast image processor is not defined\")\n-        if version.parse(torch.__version__) < version.parse(\"2.3\"):\n-            self.skipTest(reason=\"This test requires torch >= 2.3 to run.\")\n-\n-        torch.compiler.reset()\n-        input_image = self.image_processor_tester.prepare_image_inputs(equal_resolution=True, torchify=False)\n-        image_processor = self.fast_image_processing_class(**self.image_processor_dict)\n-        output_eager = image_processor(input_image, device=torch_device, return_tensors=\"pt\")\n-\n-        image_processor = torch.compile(image_processor, mode=\"reduce-overhead\")\n-        output_compiled = image_processor(input_image, device=torch_device, return_tensors=\"pt\")\n-        self._assert_slow_fast_tensors_equivalence(\n-            output_eager.pixel_values, output_compiled.pixel_values, atol=1e-4, rtol=1e-4, mean_atol=1e-5\n-        )"
      },
      {
        "filename": "tests/models/lightglue/test_image_processing_lightglue.py",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -90,6 +90,7 @@ def prepare_keypoint_matching_output(self, pixel_values):\n @require_vision\n class LightGlueImageProcessingTest(SuperGlueImageProcessingTest, unittest.TestCase):\n     image_processing_class = LightGlueImageProcessor if is_vision_available() else None\n+    fast_image_processing_class = None\n \n     def setUp(self) -> None:\n         super().setUp()"
      },
      {
        "filename": "tests/models/superglue/test_image_processing_superglue.py",
        "status": "modified",
        "additions": 89,
        "deletions": 2,
        "changes": 91,
        "patch": "@@ -11,12 +11,22 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n+import time\n import unittest\n \n+import numpy as np\n+import pytest\n+from packaging import version\n from parameterized import parameterized\n \n-from transformers.testing_utils import require_torch, require_vision\n-from transformers.utils import is_torch_available, is_vision_available\n+from transformers.testing_utils import (\n+    require_torch,\n+    require_torch_accelerator,\n+    require_vision,\n+    slow,\n+    torch_device,\n+)\n+from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import (\n     ImageProcessingTestMixin,\n@@ -33,6 +43,9 @@\n if is_vision_available():\n     from transformers import SuperGlueImageProcessor\n \n+    if is_torchvision_available():\n+        from transformers import SuperGlueImageProcessorFast\n+\n \n def random_array(size):\n     return np.random.randint(255, size=size)\n@@ -119,6 +132,7 @@ def prepare_keypoint_matching_output(self, pixel_values):\n @require_vision\n class SuperGlueImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n     image_processing_class = SuperGlueImageProcessor if is_vision_available() else None\n+    fast_image_processing_class = SuperGlueImageProcessorFast if is_torchvision_available() else None\n \n     def setUp(self) -> None:\n         super().setUp()\n@@ -397,3 +411,76 @@ def check_post_processed_output(post_processed_output, image_pair_size):\n             tensor_post_processed_outputs = image_processor.post_process_keypoint_matching(outputs, tensor_image_sizes)\n \n             check_post_processed_output(tensor_post_processed_outputs, tensor_image_sizes)\n+\n+    @unittest.skip(reason=\"Many failing cases. This test needs a more deep investigation.\")\n+    def test_fast_is_faster_than_slow(self):\n+        \"\"\"Override the generic test since EfficientLoFTR requires image pairs.\"\"\"\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast speed test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast speed test as one of the image processors is not defined\")\n+\n+        # Create image pairs for speed test\n+        dummy_images = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=False)\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        # Time slow processor\n+        start_time = time.time()\n+        for _ in range(10):\n+            _ = image_processor_slow(dummy_images, return_tensors=\"pt\")\n+        slow_time = time.time() - start_time\n+\n+        # Time fast processor\n+        start_time = time.time()\n+        for _ in range(10):\n+            _ = image_processor_fast(dummy_images, return_tensors=\"pt\")\n+        fast_time = time.time() - start_time\n+\n+        # Fast should be faster (or at least not significantly slower)\n+        self.assertLessEqual(\n+            fast_time, slow_time * 1.2, \"Fast processor should not be significantly slower than slow processor\"\n+        )\n+\n+    @require_vision\n+    @require_torch\n+    def test_slow_fast_equivalence(self):\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n+\n+        dummy_image = self.image_processor_tester.prepare_image_inputs(\n+            equal_resolution=False, numpify=True, batch_size=2, pairs=False\n+        )\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        encoding_slow = image_processor_slow(dummy_image, return_tensors=\"pt\")\n+        encoding_fast = image_processor_fast(dummy_image, return_tensors=\"pt\")\n+\n+        self._assert_slow_fast_tensors_equivalence(encoding_slow.pixel_values, encoding_fast.pixel_values)\n+\n+    @slow\n+    @require_torch_accelerator\n+    @require_vision\n+    @pytest.mark.torch_compile_test\n+    def test_can_compile_fast_image_processor(self):\n+        \"\"\"Override the generic test since EfficientLoFTR requires image pairs.\"\"\"\n+        if self.fast_image_processing_class is None:\n+            self.skipTest(\"Skipping compilation test as fast image processor is not defined\")\n+        if version.parse(torch.__version__) < version.parse(\"2.3\"):\n+            self.skipTest(reason=\"This test requires torch >= 2.3 to run.\")\n+\n+        torch.compiler.reset()\n+        input_image = self.image_processor_tester.prepare_image_inputs(equal_resolution=True, torchify=False)\n+        image_processor = self.fast_image_processing_class(**self.image_processor_dict)\n+        output_eager = image_processor(input_image, device=torch_device, return_tensors=\"pt\")\n+\n+        image_processor = torch.compile(image_processor, mode=\"reduce-overhead\")\n+        output_compiled = image_processor(input_image, device=torch_device, return_tensors=\"pt\")\n+        self._assert_slow_fast_tensors_equivalence(\n+            output_eager.pixel_values, output_compiled.pixel_values, atol=1e-4, rtol=1e-4, mean_atol=1e-5\n+        )"
      }
    ],
    "num_files": 10,
    "scraped_at": "2025-11-16T21:18:13.456674",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR implements a significant performance optimization by creating a fast image processor for SuperGlue that is ~3x faster by refactoring image preprocessing to use torch tensors instead of PIL/Numpy. The changes involve non-trivial algorithmic decisions, memory optimizations, and architectural patterns (fast processor implementations) that would generate meaningful questions about performance tradeoffs, tensor operations, and processor design.",
      "substance_level": "high"
    }
  }
]