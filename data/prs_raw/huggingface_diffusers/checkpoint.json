{
  "processed_pr_numbers": [
    12289,
    12290,
    12293,
    12298,
    12301,
    12302,
    12305,
    12306,
    12307,
    12309,
    12310,
    12312,
    12315,
    12318,
    12320,
    12324,
    12325,
    12328,
    12331,
    12332,
    12333,
    12335,
    12336,
    12337,
    12339,
    12340,
    12343,
    12345,
    12347,
    12349,
    12350,
    12351,
    12352,
    12353,
    12354,
    12357,
    12360,
    12362,
    12363,
    12364,
    12366,
    12370,
    12374,
    12378,
    12381,
    12382,
    12384,
    12388,
    12389,
    12390,
    12393,
    12394,
    12395,
    12397,
    12398,
    12399,
    12407,
    12408,
    12416,
    12418,
    12419,
    12420,
    12421,
    12424,
    12425,
    12428,
    12430,
    12434,
    12435,
    12438,
    12440,
    12442,
    12445,
    12446,
    12449,
    12450,
    12451,
    12452,
    12454,
    12455,
    12456,
    12460,
    12461,
    12464,
    12472,
    12473,
    12474,
    12478,
    12479,
    12486,
    12487,
    12488,
    12490,
    12491,
    12493,
    12495,
    12496,
    12497,
    12500,
    12502,
    12503,
    12508,
    12512,
    12513,
    12514,
    12517,
    12519,
    12520,
    12521,
    12522,
    12523,
    12525,
    12526,
    12527,
    12528,
    12530,
    12531,
    12534,
    12537,
    12541,
    12543,
    12544,
    12545,
    12546,
    12549,
    12551,
    12552,
    12561,
    12562,
    12563,
    12566,
    12571,
    12573,
    12576,
    12577,
    12579,
    12581,
    12582,
    12583,
    12584,
    12585,
    12586,
    12591,
    12592,
    12593,
    12594,
    12596,
    12602,
    12604,
    12607,
    12611,
    12616,
    12617,
    12618,
    12621,
    12622,
    12623,
    12626,
    12628,
    12629,
    12636,
    12637,
    12642,
    12644,
    12647,
    12651,
    12654,
    12658,
    12665,
    12183,
    12184,
    12188,
    12190,
    12193,
    12194,
    12197,
    12199,
    12200,
    12201,
    12203,
    12204,
    12206,
    12209,
    12211,
    12212,
    12213,
    12214,
    12215,
    12217,
    12219,
    12220,
    12223,
    12225,
    12226,
    12228,
    12230,
    12231,
    12233,
    12234,
    12236,
    12237,
    12238,
    12240,
    12243,
    12244,
    12245,
    12246,
    12247,
    12248,
    12250,
    12251,
    12252,
    12254,
    12256,
    12259,
    12260,
    12261,
    12263,
    12264,
    12265,
    12266,
    12267,
    12268,
    12269,
    12271,
    12272,
    12275,
    12277,
    12280,
    12281,
    12283,
    12285,
    12286,
    12287
  ],
  "filtered_prs": [
    {
      "pr_number": 12593,
      "title": "add ChronoEdit",
      "body": "# add ChronoEdit\r\n\r\nThis PR adds [ChronoEdit](https://research.nvidia.com/labs/toronto-ai/chronoedit/), a state-of-the-art image editing model that reframes image editing as a video generation task to achieve physically consistent edits.\r\n\r\nHF Model: https://huggingface.co/nvidia/ChronoEdit-14B-Diffusers\r\nGradio Demo: https://huggingface.co/spaces/nvidia/ChronoEdit\r\nPaper: https://arxiv.org/abs/2510.04290\r\nCode: https://github.com/nv-tlabs/ChronoEdit\r\nWebsite: https://research.nvidia.com/labs/toronto-ai/chronoedit/\r\n\r\ncc: @sayakpaul @yiyixuxu @asomoza\r\n\r\n## Usage\r\n\r\n### Full model\r\n\r\n```python\r\nimport torch\r\nimport numpy as np\r\nfrom diffusers import AutoencoderKLWan, ChronoEditTransformer3DModel, ChronoEditPipeline\r\nfrom diffusers.utils import export_to_video, load_image\r\nfrom transformers import CLIPVisionModel\r\nfrom PIL import Image\r\n\r\nmodel_id = \"nvidia/ChronoEdit-14B-Diffusers\"\r\nimage_encoder = CLIPVisionModel.from_pretrained(model_id, subfolder=\"image_encoder\", torch_dtype=torch.float32)\r\nvae = AutoencoderKLWan.from_pretrained(model_id, subfolder=\"vae\", torch_dtype=torch.float32)\r\ntransformer = ChronoEditTransformer3DModel.from_pretrained(model_id, subfolder=\"transformer\", torch_dtype=torch.bfloat16)\r\npipe = ChronoEditPipeline.from_pretrained(model_id, image_encoder=image_encoder, transformer=transformer, vae=vae, torch_dtype=torch.bfloat16)\r\npipe.to(\"cuda\")\r\n\r\nimage = load_image(\r\n    \"https://huggingface.co/spaces/nvidia/ChronoEdit/resolve/main/examples/3.png\"\r\n)\r\nmax_area = 720 * 1280\r\naspect_ratio = image.height / image.width\r\nmod_value = pipe.vae_scale_factor_spatial * pipe.transformer.config.patch_size[1]\r\nheight = round(np.sqrt(max_area * aspect_ratio)) // mod_value * mod_value\r\nwidth = round(np.sqrt(max_area / aspect_ratio)) // mod_value * mod_value\r\nprint(\"width\", width, \"height\", height)\r\nimage = image.resize((width, height))\r\nprompt = (\r\n    \"The user wants to transform the image by adding a small, cute mouse sitting inside the floral teacup, enjoying a spa bath. The mouse should appear relaxed and cheerful, with a tiny white bath towel draped over its head like a turban. It should be positioned comfortably in the cup\u2019s liquid, with gentle steam rising around it to blend with the cozy atmosphere. \"\r\n    \"The mouse\u2019s pose should be natural\u2014perhaps sitting upright with paws resting lightly on the rim or submerged in the tea. The teacup\u2019s floral design, gold trim, and warm lighting must remain unchanged to preserve the original aesthetic. The steam should softly swirl around the mouse, enhancing the spa-like, whimsical mood.\"\r\n)\r\n\r\noutput = pipe(\r\n    image=image,\r\n    prompt=prompt,\r\n    height=height,\r\n    width=width,\r\n    num_frames=5,\r\n    num_inference_steps=50,\r\n    guidance_scale=5.0,\r\n    enable_temporal_reasoning=False,\r\n    num_temporal_reasoning_steps=0,\r\n).frames[0]\r\nexport_to_video(output, \"output.mp4\", fps=4)\r\nImage.fromarray((output[-1] * 255).clip(0, 255).astype(\"uint8\")).save(\"output.png\")\r\n```\r\n\r\n### Full model with temporal reasoning\r\n\r\n```python\r\noutput = pipe(\r\n    image=image,\r\n    prompt=prompt,\r\n    height=height,\r\n    width=width,\r\n    num_frames=29,\r\n    num_inference_steps=50,\r\n    guidance_scale=5.0,\r\n    enable_temporal_reasoning=True,\r\n    num_temporal_reasoning_steps=50,\r\n).frames[0]\r\n```\r\n\r\n### With 8-steps distillation LoRA\r\n\r\n```python\r\nimport torch\r\nimport numpy as np\r\nfrom diffusers import AutoencoderKLWan, ChronoEditTransformer3DModel, ChronoEditPipeline\r\nfrom diffusers.utils import export_to_video, load_image\r\nfrom transformers import CLIPVisionModel\r\nfrom PIL import Image\r\n\r\nmodel_id = \"nvidia/ChronoEdit-14B-Diffusers\"\r\nimage_encoder = CLIPVisionModel.from_pretrained(model_id, subfolder=\"image_encoder\", torch_dtype=torch.float32)\r\nvae = AutoencoderKLWan.from_pretrained(model_id, subfolder=\"vae\", torch_dtype=torch.float32)\r\ntransformer = ChronoEditTransformer3DModel.from_pretrained(model_id, subfolder=\"transformer\", torch_dtype=torch.bfloat16)\r\npipe = ChronoEditPipeline.from_pretrained(model_id, image_encoder=image_encoder, transformer=transformer, vae=vae, torch_dtype=torch.bfloat16)\r\nlora_path = hf_hub_download(repo_id=model_id, filename=\"lora/chronoedit_distill_lora.safetensors\")\r\npipe.load_lora_weights(lora_path)\r\npipe.fuse_lora(lora_scale=1.0)\r\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config, flow_shift=2.0)\r\npipe.to(\"cuda\")\r\n\r\nimage = load_image(\r\n    \"https://huggingface.co/spaces/nvidia/ChronoEdit/resolve/main/examples/3.png\"\r\n)\r\nmax_area = 720 * 1280\r\naspect_ratio = image.height / image.width\r\nmod_value = pipe.vae_scale_factor_spatial * pipe.transformer.config.patch_size[1]\r\nheight = round(np.sqrt(max_area * aspect_ratio)) // mod_value * mod_value\r\nwidth = round(np.sqrt(max_area / aspect_ratio)) // mod_value * mod_value\r\nprint(\"width\", width, \"height\", height)\r\nimage = image.resize((width, height))\r\nprompt = (\r\n    \"The user wants to transform the image by adding a small, cute mouse sitting inside the floral teacup, enjoying a spa bath. The mouse should appear relaxed and cheerful, with a tiny white bath towel draped over its head like a turban. It should be positioned comfortably in the cup\u2019s liquid, with gentle steam rising around it to blend with the cozy atmosphere. \"\r\n    \"The mouse\u2019s pose should be natural\u2014perhaps sitting upright with paws resting lightly on the rim or submerged in the tea. The teacup\u2019s floral design, gold trim, and warm lighting must remain unchanged to preserve the original aesthetic. The steam should softly swirl around the mouse, enhancing the spa-like, whimsical mood.\"\r\n)\r\n\r\noutput = pipe(\r\n    image=image,\r\n    prompt=prompt,\r\n    height=height,\r\n    width=width,\r\n    num_frames=5,\r\n    num_inference_steps=8,\r\n    guidance_scale=1.0,\r\n    enable_temporal_reasoning=False,\r\n    num_temporal_reasoning_steps=0,\r\n).frames[0]\r\nexport_to_video(output, \"output.mp4\", fps=4)\r\nImage.fromarray((output[-1] * 255).clip(0, 255).astype(\"uint8\")).save(\"output.png\")\r\n```\r\n\r\n",
      "html_url": "https://github.com/huggingface/diffusers/pull/12593",
      "created_at": "2025-11-05T05:00:04Z",
      "merged_at": "2025-11-10T06:07:00Z",
      "merge_commit_sha": "04f9d2bf3d26e026244a13111d2f18bc95a5bb04",
      "base_ref": "main",
      "head_sha": "45f66d3bcb84901fca28d6e54ca18eb8adf355ed",
      "user": "zhangjiewu",
      "files": [
        {
          "filename": "docs/source/en/_toctree.yml",
          "status": "modified",
          "additions": 4,
          "deletions": 0,
          "changes": 4,
          "patch": "@@ -329,6 +329,8 @@\n         title: BriaTransformer2DModel\n       - local: api/models/chroma_transformer\n         title: ChromaTransformer2DModel\n+      - local: api/models/chronoedit_transformer_3d\n+        title: ChronoEditTransformer3DModel\n       - local: api/models/cogvideox_transformer3d\n         title: CogVideoXTransformer3DModel\n       - local: api/models/cogview3plus_transformer2d\n@@ -628,6 +630,8 @@\n     - sections:\n       - local: api/pipelines/allegro\n         title: Allegro\n+      - local: api/pipelines/chronoedit\n+        title: ChronoEdit\n       - local: api/pipelines/cogvideox\n         title: CogVideoX\n       - local: api/pipelines/consisid"
        },
        {
          "filename": "docs/source/en/api/models/chronoedit_transformer_3d.md",
          "status": "added",
          "additions": 32,
          "deletions": 0,
          "changes": 32,
          "patch": "@@ -0,0 +1,32 @@\n+<!-- Copyright 2025 The ChronoEdit Team and HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License. -->\n+\n+# ChronoEditTransformer3DModel\n+\n+A Diffusion Transformer model for 3D video-like data from [ChronoEdit: Towards Temporal Reasoning for Image Editing and World Simulation](https://huggingface.co/papers/2510.04290) from NVIDIA and University of Toronto, by Jay Zhangjie Wu, Xuanchi Ren, Tianchang Shen, Tianshi Cao, Kai He, Yifan Lu, Ruiyuan Gao, Enze Xie, Shiyi Lan, Jose M. Alvarez, Jun Gao, Sanja Fidler, Zian Wang, Huan Ling.\n+\n+> **TL;DR:** ChronoEdit reframes image editing as a video generation task, using input and edited images as start/end frames to leverage pretrained video models with temporal consistency. A temporal reasoning stage introduces reasoning tokens to ensure physically plausible edits and visualize the editing trajectory.\n+\n+The model can be loaded with the following code snippet.\n+\n+```python\n+from diffusers import ChronoEditTransformer3DModel\n+\n+transformer = ChronoEditTransformer3DModel.from_pretrained(\"nvidia/ChronoEdit-14B-Diffusers\", subfolder=\"transformer\", torch_dtype=torch.bfloat16)\n+```\n+\n+## ChronoEditTransformer3DModel\n+\n+[[autodoc]] ChronoEditTransformer3DModel\n+\n+## Transformer2DModelOutput\n+\n+[[autodoc]] models.modeling_outputs.Transformer2DModelOutput"
        },
        {
          "filename": "docs/source/en/api/pipelines/chronoedit.md",
          "status": "added",
          "additions": 156,
          "deletions": 0,
          "changes": 156,
          "patch": "@@ -0,0 +1,156 @@\n+<!-- Copyright 2025 The ChronoEdit Team and HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License. -->\n+\n+<div style=\"float: right;\">\n+  <div class=\"flex flex-wrap space-x-1\">\n+    <a href=\"https://huggingface.co/docs/diffusers/main/en/tutorials/using_peft_for_inference\" target=\"_blank\" rel=\"noopener\">\n+      <img alt=\"LoRA\" src=\"https://img.shields.io/badge/LoRA-d8b4fe?style=flat\"/>\n+    </a>\n+  </div>\n+</div>\n+\n+# ChronoEdit\n+\n+[ChronoEdit: Towards Temporal Reasoning for Image Editing and World Simulation](https://huggingface.co/papers/2510.04290) from NVIDIA and University of Toronto, by Jay Zhangjie Wu, Xuanchi Ren, Tianchang Shen, Tianshi Cao, Kai He, Yifan Lu, Ruiyuan Gao, Enze Xie, Shiyi Lan, Jose M. Alvarez, Jun Gao, Sanja Fidler, Zian Wang, Huan Ling.\n+\n+> **TL;DR:** ChronoEdit reframes image editing as a video generation task, using input and edited images as start/end frames to leverage pretrained video models with temporal consistency. A temporal reasoning stage introduces reasoning tokens to ensure physically plausible edits and visualize the editing trajectory.\n+\n+*Recent advances in large generative models have greatly enhanced both image editing and in-context image generation, yet a critical gap remains in ensuring physical consistency, where edited objects must remain coherent. This capability is especially vital for world simulation related tasks. In this paper, we present ChronoEdit, a framework that reframes image editing as a video generation problem. First, ChronoEdit treats the input and edited images as the first and last frames of a video, allowing it to leverage large pretrained video generative models that capture not only object appearance but also the implicit physics of motion and interaction through learned temporal consistency. Second, ChronoEdit introduces a temporal reasoning stage that explicitly performs editing at inference time. Under this setting, target frame is jointly denoised with reasoning tokens to imagine a plausible editing trajectory that constrains the solution space to physically viable transformations. The reasoning tokens are then dropped after a few steps to avoid the high computational cost of rendering a full video. To validate ChronoEdit, we introduce PBench-Edit, a new benchmark of image-prompt pairs for contexts that require physical consistency, and demonstrate that ChronoEdit surpasses state-of-the-art baselines in both visual fidelity and physical plausibility. Project page for code and models: [this https URL](https://research.nvidia.com/labs/toronto-ai/chronoedit).*\n+\n+The ChronoEdit pipeline is developed by the ChronoEdit Team. The original code is available on [GitHub](https://github.com/nv-tlabs/ChronoEdit), and pretrained models can be found in the [nvidia/ChronoEdit](https://huggingface.co/collections/nvidia/chronoedit) collection on Hugging Face.\n+\n+\n+### Image Editing\n+\n+```py\n+import torch\n+import numpy as np\n+from diffusers import AutoencoderKLWan, ChronoEditTransformer3DModel, ChronoEditPipeline\n+from diffusers.utils import export_to_video, load_image\n+from transformers import CLIPVisionModel\n+from PIL import Image\n+\n+model_id = \"nvidia/ChronoEdit-14B-Diffusers\"\n+image_encoder = CLIPVisionModel.from_pretrained(model_id, subfolder=\"image_encoder\", torch_dtype=torch.float32)\n+vae = AutoencoderKLWan.from_pretrained(model_id, subfolder=\"vae\", torch_dtype=torch.float32)\n+transformer = ChronoEditTransformer3DModel.from_pretrained(model_id, subfolder=\"transformer\", torch_dtype=torch.bfloat16)\n+pipe = ChronoEditPipeline.from_pretrained(model_id, image_encoder=image_encoder, transformer=transformer, vae=vae, torch_dtype=torch.bfloat16)\n+pipe.to(\"cuda\")\n+\n+image = load_image(\n+    \"https://huggingface.co/spaces/nvidia/ChronoEdit/resolve/main/examples/3.png\"\n+)\n+max_area = 720 * 1280\n+aspect_ratio = image.height / image.width\n+mod_value = pipe.vae_scale_factor_spatial * pipe.transformer.config.patch_size[1]\n+height = round(np.sqrt(max_area * aspect_ratio)) // mod_value * mod_value\n+width = round(np.sqrt(max_area / aspect_ratio)) // mod_value * mod_value\n+print(\"width\", width, \"height\", height)\n+image = image.resize((width, height))\n+prompt = (\n+    \"The user wants to transform the image by adding a small, cute mouse sitting inside the floral teacup, enjoying a spa bath. The mouse should appear relaxed and cheerful, with a tiny white bath towel draped over its head like a turban. It should be positioned comfortably in the cup\u2019s liquid, with gentle steam rising around it to blend with the cozy atmosphere. \"\n+    \"The mouse\u2019s pose should be natural\u2014perhaps sitting upright with paws resting lightly on the rim or submerged in the tea. The teacup\u2019s floral design, gold trim, and warm lighting must remain unchanged to preserve the original aesthetic. The steam should softly swirl around the mouse, enhancing the spa-like, whimsical mood.\"\n+)\n+\n+output = pipe(\n+    image=image,\n+    prompt=prompt,\n+    height=height,\n+    width=width,\n+    num_frames=5,\n+    num_inference_steps=50,\n+    guidance_scale=5.0,\n+    enable_temporal_reasoning=False,\n+    num_temporal_reasoning_steps=0,\n+).frames[0]\n+Image.fromarray((output[-1] * 255).clip(0, 255).astype(\"uint8\")).save(\"output.png\")\n+```\n+\n+Optionally, enable **temporal reasoning** for improved physical consistency:\n+```py\n+output = pipe(\n+    image=image,\n+    prompt=prompt,\n+    height=height,\n+    width=width,\n+    num_frames=29,\n+    num_inference_steps=50,\n+    guidance_scale=5.0,\n+    enable_temporal_reasoning=True,\n+    num_temporal_reasoning_steps=50,\n+).frames[0]\n+export_to_video(output, \"output.mp4\", fps=16)\n+Image.fromarray((output[-1] * 255).clip(0, 255).astype(\"uint8\")).save(\"output.png\")\n+```\n+\n+### Inference with 8-Step Distillation Lora\n+\n+```py\n+import torch\n+import numpy as np\n+from diffusers import AutoencoderKLWan, ChronoEditTransformer3DModel, ChronoEditPipeline\n+from diffusers.utils import export_to_video, load_image\n+from transformers import CLIPVisionModel\n+from PIL import Image\n+\n+model_id = \"nvidia/ChronoEdit-14B-Diffusers\"\n+image_encoder = CLIPVisionModel.from_pretrained(model_id, subfolder=\"image_encoder\", torch_dtype=torch.float32)\n+vae = AutoencoderKLWan.from_pretrained(model_id, subfolder=\"vae\", torch_dtype=torch.float32)\n+transformer = ChronoEditTransformer3DModel.from_pretrained(model_id, subfolder=\"transformer\", torch_dtype=torch.bfloat16)\n+pipe = ChronoEditPipeline.from_pretrained(model_id, image_encoder=image_encoder, transformer=transformer, vae=vae, torch_dtype=torch.bfloat16)\n+lora_path = hf_hub_download(repo_id=model_id, filename=\"lora/chronoedit_distill_lora.safetensors\")\n+pipe.load_lora_weights(lora_path)\n+pipe.fuse_lora(lora_scale=1.0)\n+pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config, flow_shift=2.0)\n+pipe.to(\"cuda\")\n+\n+image = load_image(\n+    \"https://huggingface.co/spaces/nvidia/ChronoEdit/resolve/main/examples/3.png\"\n+)\n+max_area = 720 * 1280\n+aspect_ratio = image.height / image.width\n+mod_value = pipe.vae_scale_factor_spatial * pipe.transformer.config.patch_size[1]\n+height = round(np.sqrt(max_area * aspect_ratio)) // mod_value * mod_value\n+width = round(np.sqrt(max_area / aspect_ratio)) // mod_value * mod_value\n+print(\"width\", width, \"height\", height)\n+image = image.resize((width, height))\n+prompt = (\n+    \"The user wants to transform the image by adding a small, cute mouse sitting inside the floral teacup, enjoying a spa bath. The mouse should appear relaxed and cheerful, with a tiny white bath towel draped over its head like a turban. It should be positioned comfortably in the cup\u2019s liquid, with gentle steam rising around it to blend with the cozy atmosphere. \"\n+    \"The mouse\u2019s pose should be natural\u2014perhaps sitting upright with paws resting lightly on the rim or submerged in the tea. The teacup\u2019s floral design, gold trim, and warm lighting must remain unchanged to preserve the original aesthetic. The steam should softly swirl around the mouse, enhancing the spa-like, whimsical mood.\"\n+)\n+\n+output = pipe(\n+    image=image,\n+    prompt=prompt,\n+    height=height,\n+    width=width,\n+    num_frames=5,\n+    num_inference_steps=8,\n+    guidance_scale=1.0,\n+    enable_temporal_reasoning=False,\n+    num_temporal_reasoning_steps=0,\n+).frames[0]\n+export_to_video(output, \"output.mp4\", fps=16)\n+Image.fromarray((output[-1] * 255).clip(0, 255).astype(\"uint8\")).save(\"output.png\")\n+```\n+\n+## ChronoEditPipeline\n+\n+[[autodoc]] ChronoEditPipeline\n+  - all\n+  - __call__\n+\n+## ChronoEditPipelineOutput\n+\n+[[autodoc]] pipelines.chronoedit.pipeline_output.ChronoEditPipelineOutput\n\\ No newline at end of file"
        },
        {
          "filename": "src/diffusers/__init__.py",
          "status": "modified",
          "additions": 4,
          "deletions": 0,
          "changes": 4,
          "patch": "@@ -202,6 +202,7 @@\n             \"BriaTransformer2DModel\",\n             \"CacheMixin\",\n             \"ChromaTransformer2DModel\",\n+            \"ChronoEditTransformer3DModel\",\n             \"CogVideoXTransformer3DModel\",\n             \"CogView3PlusTransformer2DModel\",\n             \"CogView4Transformer2DModel\",\n@@ -436,6 +437,7 @@\n             \"BriaPipeline\",\n             \"ChromaImg2ImgPipeline\",\n             \"ChromaPipeline\",\n+            \"ChronoEditPipeline\",\n             \"CLIPImageProjection\",\n             \"CogVideoXFunControlPipeline\",\n             \"CogVideoXImageToVideoPipeline\",\n@@ -909,6 +911,7 @@\n             BriaTransformer2DModel,\n             CacheMixin,\n             ChromaTransformer2DModel,\n+            ChronoEditTransformer3DModel,\n             CogVideoXTransformer3DModel,\n             CogView3PlusTransformer2DModel,\n             CogView4Transformer2DModel,\n@@ -1113,6 +1116,7 @@\n             BriaPipeline,\n             ChromaImg2ImgPipeline,\n             ChromaPipeline,\n+            ChronoEditPipeline,\n             CLIPImageProjection,\n             CogVideoXFunControlPipeline,\n             CogVideoXImageToVideoPipeline,"
        },
        {
          "filename": "src/diffusers/models/__init__.py",
          "status": "modified",
          "additions": 2,
          "deletions": 0,
          "changes": 2,
          "patch": "@@ -86,6 +86,7 @@\n     _import_structure[\"transformers.transformer_bria\"] = [\"BriaTransformer2DModel\"]\n     _import_structure[\"transformers.transformer_bria_fibo\"] = [\"BriaFiboTransformer2DModel\"]\n     _import_structure[\"transformers.transformer_chroma\"] = [\"ChromaTransformer2DModel\"]\n+    _import_structure[\"transformers.transformer_chronoedit\"] = [\"ChronoEditTransformer3DModel\"]\n     _import_structure[\"transformers.transformer_cogview3plus\"] = [\"CogView3PlusTransformer2DModel\"]\n     _import_structure[\"transformers.transformer_cogview4\"] = [\"CogView4Transformer2DModel\"]\n     _import_structure[\"transformers.transformer_cosmos\"] = [\"CosmosTransformer3DModel\"]\n@@ -179,6 +180,7 @@\n             BriaFiboTransformer2DModel,\n             BriaTransformer2DModel,\n             ChromaTransformer2DModel,\n+            ChronoEditTransformer3DModel,\n             CogVideoXTransformer3DModel,\n             CogView3PlusTransformer2DModel,\n             CogView4Transformer2DModel,"
        },
        {
          "filename": "src/diffusers/models/transformers/__init__.py",
          "status": "modified",
          "additions": 1,
          "deletions": 0,
          "changes": 1,
          "patch": "@@ -20,6 +20,7 @@\n     from .transformer_bria import BriaTransformer2DModel\n     from .transformer_bria_fibo import BriaFiboTransformer2DModel\n     from .transformer_chroma import ChromaTransformer2DModel\n+    from .transformer_chronoedit import ChronoEditTransformer3DModel\n     from .transformer_cogview3plus import CogView3PlusTransformer2DModel\n     from .transformer_cogview4 import CogView4Transformer2DModel\n     from .transformer_cosmos import CosmosTransformer3DModel"
        },
        {
          "filename": "src/diffusers/models/transformers/transformer_chronoedit.py",
          "status": "added",
          "additions": 735,
          "deletions": 0,
          "changes": 735,
          "patch": "@@ -0,0 +1,735 @@\n+# Copyright 2025 The ChronoEdit Team and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import math\n+from typing import Any, Dict, Optional, Tuple, Union\n+\n+import torch\n+import torch.nn as nn\n+import torch.nn.functional as F\n+\n+from ...configuration_utils import ConfigMixin, register_to_config\n+from ...loaders import FromOriginalModelMixin, PeftAdapterMixin\n+from ...utils import USE_PEFT_BACKEND, deprecate, logging, scale_lora_layers, unscale_lora_layers\n+from ...utils.torch_utils import maybe_allow_in_graph\n+from .._modeling_parallel import ContextParallelInput, ContextParallelOutput\n+from ..attention import AttentionMixin, AttentionModuleMixin, FeedForward\n+from ..attention_dispatch import dispatch_attention_fn\n+from ..cache_utils import CacheMixin\n+from ..embeddings import PixArtAlphaTextProjection, TimestepEmbedding, Timesteps, get_1d_rotary_pos_embed\n+from ..modeling_outputs import Transformer2DModelOutput\n+from ..modeling_utils import ModelMixin\n+from ..normalization import FP32LayerNorm\n+\n+\n+logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n+\n+\n+# Copied from diffusers.models.transformers.transformer_wan._get_qkv_projections\n+def _get_qkv_projections(attn: \"WanAttention\", hidden_states: torch.Tensor, encoder_hidden_states: torch.Tensor):\n+    # encoder_hidden_states is only passed for cross-attention\n+    if encoder_hidden_states is None:\n+        encoder_hidden_states = hidden_states\n+\n+    if attn.fused_projections:\n+        if attn.cross_attention_dim_head is None:\n+            # In self-attention layers, we can fuse the entire QKV projection into a single linear\n+            query, key, value = attn.to_qkv(hidden_states).chunk(3, dim=-1)\n+        else:\n+            # In cross-attention layers, we can only fuse the KV projections into a single linear\n+            query = attn.to_q(hidden_states)\n+            key, value = attn.to_kv(encoder_hidden_states).chunk(2, dim=-1)\n+    else:\n+        query = attn.to_q(hidden_states)\n+        key = attn.to_k(encoder_hidden_states)\n+        value = attn.to_v(encoder_hidden_states)\n+    return query, key, value\n+\n+\n+# Copied from diffusers.models.transformers.transformer_wan._get_added_kv_projections\n+def _get_added_kv_projections(attn: \"WanAttention\", encoder_hidden_states_img: torch.Tensor):\n+    if attn.fused_projections:\n+        key_img, value_img = attn.to_added_kv(encoder_hidden_states_img).chunk(2, dim=-1)\n+    else:\n+        key_img = attn.add_k_proj(encoder_hidden_states_img)\n+        value_img = attn.add_v_proj(encoder_hidden_states_img)\n+    return key_img, value_img\n+\n+\n+# Copied from diffusers.models.transformers.transformer_wan.WanAttnProcessor\n+class WanAttnProcessor:\n+    _attention_backend = None\n+    _parallel_config = None\n+\n+    def __init__(self):\n+        if not hasattr(F, \"scaled_dot_product_attention\"):\n+            raise ImportError(\n+                \"WanAttnProcessor requires PyTorch 2.0. To use it, please upgrade PyTorch to version 2.0 or higher.\"\n+            )\n+\n+    def __call__(\n+        self,\n+        attn: \"WanAttention\",\n+        hidden_states: torch.Tensor,\n+        encoder_hidden_states: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        rotary_emb: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n+    ) -> torch.Tensor:\n+        encoder_hidden_states_img = None\n+        if attn.add_k_proj is not None:\n+            # 512 is the context length of the text encoder, hardcoded for now\n+            image_context_length = encoder_hidden_states.shape[1] - 512\n+            encoder_hidden_states_img = encoder_hidden_states[:, :image_context_length]\n+            encoder_hidden_states = encoder_hidden_states[:, image_context_length:]\n+\n+        query, key, value = _get_qkv_projections(attn, hidden_states, encoder_hidden_states)\n+\n+        query = attn.norm_q(query)\n+        key = attn.norm_k(key)\n+\n+        query = query.unflatten(2, (attn.heads, -1))\n+        key = key.unflatten(2, (attn.heads, -1))\n+        value = value.unflatten(2, (attn.heads, -1))\n+\n+        if rotary_emb is not None:\n+\n+            def apply_rotary_emb(\n+                hidden_states: torch.Tensor,\n+                freqs_cos: torch.Tensor,\n+                freqs_sin: torch.Tensor,\n+            ):\n+                x1, x2 = hidden_states.unflatten(-1, (-1, 2)).unbind(-1)\n+                cos = freqs_cos[..., 0::2]\n+                sin = freqs_sin[..., 1::2]\n+                out = torch.empty_like(hidden_states)\n+                out[..., 0::2] = x1 * cos - x2 * sin\n+                out[..., 1::2] = x1 * sin + x2 * cos\n+                return out.type_as(hidden_states)\n+\n+            query = apply_rotary_emb(query, *rotary_emb)\n+            key = apply_rotary_emb(key, *rotary_emb)\n+\n+        # I2V task\n+        hidden_states_img = None\n+        if encoder_hidden_states_img is not None:\n+            key_img, value_img = _get_added_kv_projections(attn, encoder_hidden_states_img)\n+            key_img = attn.norm_added_k(key_img)\n+\n+            key_img = key_img.unflatten(2, (attn.heads, -1))\n+            value_img = value_img.unflatten(2, (attn.heads, -1))\n+\n+            hidden_states_img = dispatch_attention_fn(\n+                query,\n+                key_img,\n+                value_img,\n+                attn_mask=None,\n+                dropout_p=0.0,\n+                is_causal=False,\n+                backend=self._attention_backend,\n+                parallel_config=self._parallel_config,\n+            )\n+            hidden_states_img = hidden_states_img.flatten(2, 3)\n+            hidden_states_img = hidden_states_img.type_as(query)\n+\n+        hidden_states = dispatch_attention_fn(\n+            query,\n+            key,\n+            value,\n+            attn_mask=attention_mask,\n+            dropout_p=0.0,\n+            is_causal=False,\n+            backend=self._attention_backend,\n+            parallel_config=self._parallel_config,\n+        )\n+        hidden_states = hidden_states.flatten(2, 3)\n+        hidden_states = hidden_states.type_as(query)\n+\n+        if hidden_states_img is not None:\n+            hidden_states = hidden_states + hidden_states_img\n+\n+        hidden_states = attn.to_out[0](hidden_states)\n+        hidden_states = attn.to_out[1](hidden_states)\n+        return hidden_states\n+\n+\n+# Copied from diffusers.models.transformers.transformer_wan.WanAttnProcessor2_0\n+class WanAttnProcessor2_0:\n+    def __new__(cls, *args, **kwargs):\n+        deprecation_message = (\n+            \"The WanAttnProcessor2_0 class is deprecated and will be removed in a future version. \"\n+            \"Please use WanAttnProcessor instead. \"\n+        )\n+        deprecate(\"WanAttnProcessor2_0\", \"1.0.0\", deprecation_message, standard_warn=False)\n+        return WanAttnProcessor(*args, **kwargs)\n+\n+\n+# Copied from diffusers.models.transformers.transformer_wan.WanAttention\n+class WanAttention(torch.nn.Module, AttentionModuleMixin):\n+    _default_processor_cls = WanAttnProcessor\n+    _available_processors = [WanAttnProcessor]\n+\n+    def __init__(\n+        self,\n+        dim: int,\n+        heads: int = 8,\n+        dim_head: int = 64,\n+        eps: float = 1e-5,\n+        dropout: float = 0.0,\n+        added_kv_proj_dim: Optional[int] = None,\n+        cross_attention_dim_head: Optional[int] = None,\n+        processor=None,\n+        is_cross_attention=None,\n+    ):\n+        super().__init__()\n+\n+        self.inner_dim = dim_head * heads\n+        self.heads = heads\n+        self.added_kv_proj_dim = added_kv_proj_dim\n+        self.cross_attention_dim_head = cross_attention_dim_head\n+        self.kv_inner_dim = self.inner_dim if cross_attention_dim_head is None else cross_attention_dim_head * heads\n+\n+        self.to_q = torch.nn.Linear(dim, self.inner_dim, bias=True)\n+        self.to_k = torch.nn.Linear(dim, self.kv_inner_dim, bias=True)\n+        self.to_v = torch.nn.Linear(dim, self.kv_inner_dim, bias=True)\n+        self.to_out = torch.nn.ModuleList(\n+            [\n+                torch.nn.Linear(self.inner_dim, dim, bias=True),\n+                torch.nn.Dropout(dropout),\n+            ]\n+        )\n+        self.norm_q = torch.nn.RMSNorm(dim_head * heads, eps=eps, elementwise_affine=True)\n+        self.norm_k = torch.nn.RMSNorm(dim_head * heads, eps=eps, elementwise_affine=True)\n+\n+        self.add_k_proj = self.add_v_proj = None\n+        if added_kv_proj_dim is not None:\n+            self.add_k_proj = torch.nn.Linear(added_kv_proj_dim, self.inner_dim, bias=True)\n+            self.add_v_proj = torch.nn.Linear(added_kv_proj_dim, self.inner_dim, bias=True)\n+            self.norm_added_k = torch.nn.RMSNorm(dim_head * heads, eps=eps)\n+\n+        self.is_cross_attention = cross_attention_dim_head is not None\n+\n+        self.set_processor(processor)\n+\n+    def fuse_projections(self):\n+        if getattr(self, \"fused_projections\", False):\n+            return\n+\n+        if self.cross_attention_dim_head is None:\n+            concatenated_weights = torch.cat([self.to_q.weight.data, self.to_k.weight.data, self.to_v.weight.data])\n+            concatenated_bias = torch.cat([self.to_q.bias.data, self.to_k.bias.data, self.to_v.bias.data])\n+            out_features, in_features = concatenated_weights.shape\n+            with torch.device(\"meta\"):\n+                self.to_qkv = nn.Linear(in_features, out_features, bias=True)\n+            self.to_qkv.load_state_dict(\n+                {\"weight\": concatenated_weights, \"bias\": concatenated_bias}, strict=True, assign=True\n+            )\n+        else:\n+            concatenated_weights = torch.cat([self.to_k.weight.data, self.to_v.weight.data])\n+            concatenated_bias = torch.cat([self.to_k.bias.data, self.to_v.bias.data])\n+            out_features, in_features = concatenated_weights.shape\n+            with torch.device(\"meta\"):\n+                self.to_kv = nn.Linear(in_features, out_features, bias=True)\n+            self.to_kv.load_state_dict(\n+                {\"weight\": concatenated_weights, \"bias\": concatenated_bias}, strict=True, assign=True\n+            )\n+\n+        if self.added_kv_proj_dim is not None:\n+            concatenated_weights = torch.cat([self.add_k_proj.weight.data, self.add_v_proj.weight.data])\n+            concatenated_bias = torch.cat([self.add_k_proj.bias.data, self.add_v_proj.bias.data])\n+            out_features, in_features = concatenated_weights.shape\n+            with torch.device(\"meta\"):\n+                self.to_added_kv = nn.Linear(in_features, out_features, bias=True)\n+            self.to_added_kv.load_state_dict(\n+                {\"weight\": concatenated_weights, \"bias\": concatenated_bias}, strict=True, assign=True\n+            )\n+\n+        self.fused_projections = True\n+\n+    @torch.no_grad()\n+    def unfuse_projections(self):\n+        if not getattr(self, \"fused_projections\", False):\n+            return\n+\n+        if hasattr(self, \"to_qkv\"):\n+            delattr(self, \"to_qkv\")\n+        if hasattr(self, \"to_kv\"):\n+            delattr(self, \"to_kv\")\n+        if hasattr(self, \"to_added_kv\"):\n+            delattr(self, \"to_added_kv\")\n+\n+        self.fused_projections = False\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        encoder_hidden_states: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        rotary_emb: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n+        **kwargs,\n+    ) -> torch.Tensor:\n+        return self.processor(self, hidden_states, encoder_hidden_states, attention_mask, rotary_emb, **kwargs)\n+\n+\n+# Copied from diffusers.models.transformers.transformer_wan.WanImageEmbedding\n+class WanImageEmbedding(torch.nn.Module):\n+    def __init__(self, in_features: int, out_features: int, pos_embed_seq_len=None):\n+        super().__init__()\n+\n+        self.norm1 = FP32LayerNorm(in_features)\n+        self.ff = FeedForward(in_features, out_features, mult=1, activation_fn=\"gelu\")\n+        self.norm2 = FP32LayerNorm(out_features)\n+        if pos_embed_seq_len is not None:\n+            self.pos_embed = nn.Parameter(torch.zeros(1, pos_embed_seq_len, in_features))\n+        else:\n+            self.pos_embed = None\n+\n+    def forward(self, encoder_hidden_states_image: torch.Tensor) -> torch.Tensor:\n+        if self.pos_embed is not None:\n+            batch_size, seq_len, embed_dim = encoder_hidden_states_image.shape\n+            encoder_hidden_states_image = encoder_hidden_states_image.view(-1, 2 * seq_len, embed_dim)\n+            encoder_hidden_states_image = encoder_hidden_states_image + self.pos_embed\n+\n+        hidden_states = self.norm1(encoder_hidden_states_image)\n+        hidden_states = self.ff(hidden_states)\n+        hidden_states = self.norm2(hidden_states)\n+        return hidden_states\n+\n+\n+# Copied from diffusers.models.transformers.transformer_wan.WanTimeTextImageEmbedding\n+class WanTimeTextImageEmbedding(nn.Module):\n+    def __init__(\n+        self,\n+        dim: int,\n+        time_freq_dim: int,\n+        time_proj_dim: int,\n+        text_embed_dim: int,\n+        image_embed_dim: Optional[int] = None,\n+        pos_embed_seq_len: Optional[int] = None,\n+    ):\n+        super().__init__()\n+\n+        self.timesteps_proj = Timesteps(num_channels=time_freq_dim, flip_sin_to_cos=True, downscale_freq_shift=0)\n+        self.time_embedder = TimestepEmbedding(in_channels=time_freq_dim, time_embed_dim=dim)\n+        self.act_fn = nn.SiLU()\n+        self.time_proj = nn.Linear(dim, time_proj_dim)\n+        self.text_embedder = PixArtAlphaTextProjection(text_embed_dim, dim, act_fn=\"gelu_tanh\")\n+\n+        self.image_embedder = None\n+        if image_embed_dim is not None:\n+            self.image_embedder = WanImageEmbedding(image_embed_dim, dim, pos_embed_seq_len=pos_embed_seq_len)\n+\n+    def forward(\n+        self,\n+        timestep: torch.Tensor,\n+        encoder_hidden_states: torch.Tensor,\n+        encoder_hidden_states_image: Optional[torch.Tensor] = None,\n+        timestep_seq_len: Optional[int] = None,\n+    ):\n+        timestep = self.timesteps_proj(timestep)\n+        if timestep_seq_len is not None:\n+            timestep = timestep.unflatten(0, (-1, timestep_seq_len))\n+\n+        time_embedder_dtype = next(iter(self.time_embedder.parameters())).dtype\n+        if timestep.dtype != time_embedder_dtype and time_embedder_dtype != torch.int8:\n+            timestep = timestep.to(time_embedder_dtype)\n+        temb = self.time_embedder(timestep).type_as(encoder_hidden_states)\n+        timestep_proj = self.time_proj(self.act_fn(temb))\n+\n+        encoder_hidden_states = self.text_embedder(encoder_hidden_states)\n+        if encoder_hidden_states_image is not None:\n+            encoder_hidden_states_image = self.image_embedder(encoder_hidden_states_image)\n+\n+        return temb, timestep_proj, encoder_hidden_states, encoder_hidden_states_image\n+\n+\n+class ChronoEditRotaryPosEmbed(nn.Module):\n+    def __init__(\n+        self,\n+        attention_head_dim: int,\n+        patch_size: Tuple[int, int, int],\n+        max_seq_len: int,\n+        theta: float = 10000.0,\n+        temporal_skip_len: int = 8,\n+    ):\n+        super().__init__()\n+\n+        self.attention_head_dim = attention_head_dim\n+        self.patch_size = patch_size\n+        self.max_seq_len = max_seq_len\n+        self.temporal_skip_len = temporal_skip_len\n+\n+        h_dim = w_dim = 2 * (attention_head_dim // 6)\n+        t_dim = attention_head_dim - h_dim - w_dim\n+        freqs_dtype = torch.float32 if torch.backends.mps.is_available() else torch.float64\n+\n+        freqs_cos = []\n+        freqs_sin = []\n+\n+        for dim in [t_dim, h_dim, w_dim]:\n+            freq_cos, freq_sin = get_1d_rotary_pos_embed(\n+                dim,\n+                max_seq_len,\n+                theta,\n+                use_real=True,\n+                repeat_interleave_real=True,\n+                freqs_dtype=freqs_dtype,\n+            )\n+            freqs_cos.append(freq_cos)\n+            freqs_sin.append(freq_sin)\n+\n+        self.register_buffer(\"freqs_cos\", torch.cat(freqs_cos, dim=1), persistent=False)\n+        self.register_buffer(\"freqs_sin\", torch.cat(freqs_sin, dim=1), persistent=False)\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        batch_size, num_channels, num_frames, height, width = hidden_states.shape\n+        p_t, p_h, p_w = self.patch_size\n+        ppf, pph, ppw = num_frames // p_t, height // p_h, width // p_w\n+\n+        split_sizes = [\n+            self.attention_head_dim - 2 * (self.attention_head_dim // 3),\n+            self.attention_head_dim // 3,\n+            self.attention_head_dim // 3,\n+        ]\n+\n+        freqs_cos = self.freqs_cos.split(split_sizes, dim=1)\n+        freqs_sin = self.freqs_sin.split(split_sizes, dim=1)\n+\n+        if num_frames == 2:\n+            freqs_cos_f = freqs_cos[0][: self.temporal_skip_len][[0, -1]].view(ppf, 1, 1, -1).expand(ppf, pph, ppw, -1)\n+        else:\n+            freqs_cos_f = freqs_cos[0][:ppf].view(ppf, 1, 1, -1).expand(ppf, pph, ppw, -1)\n+        freqs_cos_h = freqs_cos[1][:pph].view(1, pph, 1, -1).expand(ppf, pph, ppw, -1)\n+        freqs_cos_w = freqs_cos[2][:ppw].view(1, 1, ppw, -1).expand(ppf, pph, ppw, -1)\n+\n+        if num_frames == 2:\n+            freqs_sin_f = freqs_sin[0][: self.temporal_skip_len][[0, -1]].view(ppf, 1, 1, -1).expand(ppf, pph, ppw, -1)\n+        else:\n+            freqs_sin_f = freqs_sin[0][:ppf].view(ppf, 1, 1, -1).expand(ppf, pph, ppw, -1)\n+        freqs_sin_h = freqs_sin[1][:pph].view(1, pph, 1, -1).expand(ppf, pph, ppw, -1)\n+        freqs_sin_w = freqs_sin[2][:ppw].view(1, 1, ppw, -1).expand(ppf, pph, ppw, -1)\n+\n+        freqs_cos = torch.cat([freqs_cos_f, freqs_cos_h, freqs_cos_w], dim=-1).reshape(1, ppf * pph * ppw, 1, -1)\n+        freqs_sin = torch.cat([freqs_sin_f, freqs_sin_h, freqs_sin_w], dim=-1).reshape(1, ppf * pph * ppw, 1, -1)\n+\n+        return freqs_cos, freqs_sin\n+\n+\n+@maybe_allow_in_graph\n+# Copied from diffusers.models.transformers.transformer_wan.WanTransformerBlock\n+class WanTransformerBlock(nn.Module):\n+    def __init__(\n+        self,\n+        dim: int,\n+        ffn_dim: int,\n+        num_heads: int,\n+        qk_norm: str = \"rms_norm_across_heads\",\n+        cross_attn_norm: bool = False,\n+        eps: float = 1e-6,\n+        added_kv_proj_dim: Optional[int] = None,\n+    ):\n+        super().__init__()\n+\n+        # 1. Self-attention\n+        self.norm1 = FP32LayerNorm(dim, eps, elementwise_affine=False)\n+        self.attn1 = WanAttention(\n+            dim=dim,\n+            heads=num_heads,\n+            dim_head=dim // num_heads,\n+            eps=eps,\n+            cross_attention_dim_head=None,\n+            processor=WanAttnProcessor(),\n+        )\n+\n+        # 2. Cross-attention\n+        self.attn2 = WanAttention(\n+            dim=dim,\n+            heads=num_heads,\n+            dim_head=dim // num_heads,\n+            eps=eps,\n+            added_kv_proj_dim=added_kv_proj_dim,\n+            cross_attention_dim_head=dim // num_heads,\n+            processor=WanAttnProcessor(),\n+        )\n+        self.norm2 = FP32LayerNorm(dim, eps, elementwise_affine=True) if cross_attn_norm else nn.Identity()\n+\n+        # 3. Feed-forward\n+        self.ffn = FeedForward(dim, inner_dim=ffn_dim, activation_fn=\"gelu-approximate\")\n+        self.norm3 = FP32LayerNorm(dim, eps, elementwise_affine=False)\n+\n+        self.scale_shift_table = nn.Parameter(torch.randn(1, 6, dim) / dim**0.5)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        encoder_hidden_states: torch.Tensor,\n+        temb: torch.Tensor,\n+        rotary_emb: torch.Tensor,\n+    ) -> torch.Tensor:\n+        if temb.ndim == 4:\n+            # temb: batch_size, seq_len, 6, inner_dim (wan2.2 ti2v)\n+            shift_msa, scale_msa, gate_msa, c_shift_msa, c_scale_msa, c_gate_msa = (\n+                self.scale_shift_table.unsqueeze(0) + temb.float()\n+            ).chunk(6, dim=2)\n+            # batch_size, seq_len, 1, inner_dim\n+            shift_msa = shift_msa.squeeze(2)\n+            scale_msa = scale_msa.squeeze(2)\n+            gate_msa = gate_msa.squeeze(2)\n+            c_shift_msa = c_shift_msa.squeeze(2)\n+            c_scale_msa = c_scale_msa.squeeze(2)\n+            c_gate_msa = c_gate_msa.squeeze(2)\n+        else:\n+            # temb: batch_size, 6, inner_dim (wan2.1/wan2.2 14B)\n+            shift_msa, scale_msa, gate_msa, c_shift_msa, c_scale_msa, c_gate_msa = (\n+                self.scale_shift_table + temb.float()\n+            ).chunk(6, dim=1)\n+\n+        # 1. Self-attention\n+        norm_hidden_states = (self.norm1(hidden_states.float()) * (1 + scale_msa) + shift_msa).type_as(hidden_states)\n+        attn_output = self.attn1(norm_hidden_states, None, None, rotary_emb)\n+        hidden_states = (hidden_states.float() + attn_output * gate_msa).type_as(hidden_states)\n+\n+        # 2. Cross-attention\n+        norm_hidden_states = self.norm2(hidden_states.float()).type_as(hidden_states)\n+        attn_output = self.attn2(norm_hidden_states, encoder_hidden_states, None, None)\n+        hidden_states = hidden_states + attn_output\n+\n+        # 3. Feed-forward\n+        norm_hidden_states = (self.norm3(hidden_states.float()) * (1 + c_scale_msa) + c_shift_msa).type_as(\n+            hidden_states\n+        )\n+        ff_output = self.ffn(norm_hidden_states)\n+        hidden_states = (hidden_states.float() + ff_output.float() * c_gate_msa).type_as(hidden_states)\n+\n+        return hidden_states\n+\n+\n+# modified from diffusers.models.transformers.transformer_wan.WanTransformer3DModel\n+class ChronoEditTransformer3DModel(\n+    ModelMixin, ConfigMixin, PeftAdapterMixin, FromOriginalModelMixin, CacheMixin, AttentionMixin\n+):\n+    r\"\"\"\n+    A Transformer model for video-like data used in the ChronoEdit model.\n+\n+    Args:\n+        patch_size (`Tuple[int]`, defaults to `(1, 2, 2)`):\n+            3D patch dimensions for video embedding (t_patch, h_patch, w_patch).\n+        num_attention_heads (`int`, defaults to `40`):\n+            Fixed length for text embeddings.\n+        attention_head_dim (`int`, defaults to `128`):\n+            The number of channels in each head.\n+        in_channels (`int`, defaults to `16`):\n+            The number of channels in the input.\n+        out_channels (`int`, defaults to `16`):\n+            The number of channels in the output.\n+        text_dim (`int`, defaults to `512`):\n+            Input dimension for text embeddings.\n+        freq_dim (`int`, defaults to `256`):\n+            Dimension for sinusoidal time embeddings.\n+        ffn_dim (`int`, defaults to `13824`):\n+            Intermediate dimension in feed-forward network.\n+        num_layers (`int`, defaults to `40`):\n+            The number of layers of transformer blocks to use.\n+        window_size (`Tuple[int]`, defaults to `(-1, -1)`):\n+            Window size for local attention (-1 indicates global attention).\n+        cross_attn_norm (`bool`, defaults to `True`):\n+            Enable cross-attention normalization.\n+        qk_norm (`bool`, defaults to `True`):\n+            Enable query/key normalization.\n+        eps (`float`, defaults to `1e-6`):\n+            Epsilon value for normalization layers.\n+        add_img_emb (`bool`, defaults to `False`):\n+            Whether to use img_emb.\n+        added_kv_proj_dim (`int`, *optional*, defaults to `None`):\n+            The number of channels to use for the added key and value projections. If `None`, no projection is used.\n+    \"\"\"\n+\n+    _supports_gradient_checkpointing = True\n+    _skip_layerwise_casting_patterns = [\"patch_embedding\", \"condition_embedder\", \"norm\"]\n+    _no_split_modules = [\"WanTransformerBlock\"]\n+    _keep_in_fp32_modules = [\"time_embedder\", \"scale_shift_table\", \"norm1\", \"norm2\", \"norm3\"]\n+    _keys_to_ignore_on_load_unexpected = [\"norm_added_q\"]\n+    _repeated_blocks = [\"WanTransformerBlock\"]\n+    _cp_plan = {\n+        \"rope\": {\n+            0: ContextParallelInput(split_dim=1, expected_dims=4, split_output=True),\n+            1: ContextParallelInput(split_dim=1, expected_dims=4, split_output=True),\n+        },\n+        \"blocks.0\": {\n+            \"hidden_states\": ContextParallelInput(split_dim=1, expected_dims=3, split_output=False),\n+        },\n+        \"blocks.*\": {\n+            \"encoder_hidden_states\": ContextParallelInput(split_dim=1, expected_dims=3, split_output=False),\n+        },\n+        \"proj_out\": ContextParallelOutput(gather_dim=1, expected_dims=3),\n+    }\n+\n+    @register_to_config\n+    def __init__(\n+        self,\n+        patch_size: Tuple[int] = (1, 2, 2),\n+        num_attention_heads: int = 40,\n+        attention_head_dim: int = 128,\n+        in_channels: int = 16,\n+        out_channels: int = 16,\n+        text_dim: int = 4096,\n+        freq_dim: int = 256,\n+        ffn_dim: int = 13824,\n+        num_layers: int = 40,\n+        cross_attn_norm: bool = True,\n+        qk_norm: Optional[str] = \"rms_norm_across_heads\",\n+        eps: float = 1e-6,\n+        image_dim: Optional[int] = None,\n+        added_kv_proj_dim: Optional[int] = None,\n+        rope_max_seq_len: int = 1024,\n+        pos_embed_seq_len: Optional[int] = None,\n+        rope_temporal_skip_len: int = 8,\n+    ) -> None:\n+        super().__init__()\n+\n+        inner_dim = num_attention_heads * attention_head_dim\n+        out_channels = out_channels or in_channels\n+\n+        # 1. Patch & position embedding\n+        self.rope = ChronoEditRotaryPosEmbed(\n+            attention_head_dim, patch_size, rope_max_seq_len, temporal_skip_len=rope_temporal_skip_len\n+        )\n+        self.patch_embedding = nn.Conv3d(in_channels, inner_dim, kernel_size=patch_size, stride=patch_size)\n+\n+        # 2. Condition embeddings\n+        # image_embedding_dim=1280 for I2V model\n+        self.condition_embedder = WanTimeTextImageEmbedding(\n+            dim=inner_dim,\n+            time_freq_dim=freq_dim,\n+            time_proj_dim=inner_dim * 6,\n+            text_embed_dim=text_dim,\n+            image_embed_dim=image_dim,\n+            pos_embed_seq_len=pos_embed_seq_len,\n+        )\n+\n+        # 3. Transformer blocks\n+        self.blocks = nn.ModuleList(\n+            [\n+                WanTransformerBlock(\n+                    inner_dim, ffn_dim, num_attention_heads, qk_norm, cross_attn_norm, eps, added_kv_proj_dim\n+                )\n+                for _ in range(num_layers)\n+            ]\n+        )\n+\n+        # 4. Output norm & projection\n+        self.norm_out = FP32LayerNorm(inner_dim, eps, elementwise_affine=False)\n+        self.proj_out = nn.Linear(inner_dim, out_channels * math.prod(patch_size))\n+        self.scale_shift_table = nn.Parameter(torch.randn(1, 2, inner_dim) / inner_dim**0.5)\n+\n+        self.gradient_checkpointing = False\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        timestep: torch.LongTensor,\n+        encoder_hidden_states: torch.Tensor,\n+        encoder_hidden_states_image: Optional[torch.Tensor] = None,\n+        return_dict: bool = True,\n+        attention_kwargs: Optional[Dict[str, Any]] = None,\n+    ) -> Union[torch.Tensor, Dict[str, torch.Tensor]]:\n+        if attention_kwargs is not None:\n+            attention_kwargs = attention_kwargs.copy()\n+            lora_scale = attention_kwargs.pop(\"scale\", 1.0)\n+        else:\n+            lora_scale = 1.0\n+\n+        if USE_PEFT_BACKEND:\n+            # weight the lora layers by setting `lora_scale` for each PEFT layer\n+            scale_lora_layers(self, lora_scale)\n+        else:\n+            if attention_kwargs is not None and attention_kwargs.get(\"scale\", None) is not None:\n+                logger.warning(\n+                    \"Passing `scale` via `attention_kwargs` when not using the PEFT backend is ineffective.\"\n+                )\n+\n+        batch_size, num_channels, num_frames, height, width = hidden_states.shape\n+        p_t, p_h, p_w = self.config.patch_size\n+        post_patch_num_frames = num_frames // p_t\n+        post_patch_height = height // p_h\n+        post_patch_width = width // p_w\n+\n+        rotary_emb = self.rope(hidden_states)\n+\n+        hidden_states = self.patch_embedding(hidden_states)\n+        hidden_states = hidden_states.flatten(2).transpose(1, 2)\n+\n+        # timestep shape: batch_size, or batch_size, seq_len (wan 2.2 ti2v)\n+        if timestep.ndim == 2:\n+            ts_seq_len = timestep.shape[1]\n+            timestep = timestep.flatten()  # batch_size * seq_len\n+        else:\n+            ts_seq_len = None\n+\n+        temb, timestep_proj, encoder_hidden_states, encoder_hidden_states_image = self.condition_embedder(\n+            timestep, encoder_hidden_states, encoder_hidden_states_image, timestep_seq_len=ts_seq_len\n+        )\n+        if ts_seq_len is not None:\n+            # batch_size, seq_len, 6, inner_dim\n+            timestep_proj = timestep_proj.unflatten(2, (6, -1))\n+        else:\n+            # batch_size, 6, inner_dim\n+            timestep_proj = timestep_proj.unflatten(1, (6, -1))\n+\n+        if encoder_hidden_states_image is not None:\n+            encoder_hidden_states = torch.concat([encoder_hidden_states_image, encoder_hidden_states], dim=1)\n+\n+        # 4. Transformer blocks\n+        if torch.is_grad_enabled() and self.gradient_checkpointing:\n+            for block in self.blocks:\n+                hidden_states = self._gradient_checkpointing_func(\n+                    block, hidden_states, encoder_hidden_states, timestep_proj, rotary_emb\n+                )\n+        else:\n+            for block in self.blocks:\n+                hidden_states = block(hidden_states, encoder_hidden_states, timestep_proj, rotary_emb)\n+\n+        # 5. Output norm, projection & unpatchify\n+        if temb.ndim == 3:\n+            # batch_size, seq_len, inner_dim (wan 2.2 ti2v)\n+            shift, scale = (self.scale_shift_table.unsqueeze(0).to(temb.device) + temb.unsqueeze(2)).chunk(2, dim=2)\n+            shift = shift.squeeze(2)\n+            scale = scale.squeeze(2)\n+        else:\n+            # batch_size, inner_dim\n+            shift, scale = (self.scale_shift_table.to(temb.device) + temb.unsqueeze(1)).chunk(2, dim=1)\n+\n+        # Move the shift and scale tensors to the same device as hidden_states.\n+        # When using multi-GPU inference via accelerate these will be on the\n+        # first device rather than the last device, which hidden_states ends up\n+        # on.\n+        shift = shift.to(hidden_states.device)\n+        scale = scale.to(hidden_states.device)\n+\n+        hidden_states = (self.norm_out(hidden_states.float()) * (1 + scale) + shift).type_as(hidden_states)\n+        hidden_states = self.proj_out(hidden_states)\n+\n+        hidden_states = hidden_states.reshape(\n+            batch_size, post_patch_num_frames, post_patch_height, post_patch_width, p_t, p_h, p_w, -1\n+        )\n+        hidden_states = hidden_states.permute(0, 7, 1, 4, 2, 5, 3, 6)\n+        output = hidden_states.flatten(6, 7).flatten(4, 5).flatten(2, 3)\n+\n+        if USE_PEFT_BACKEND:\n+            # remove `lora_scale` from each PEFT layer\n+            unscale_lora_layers(self, lora_scale)\n+\n+        if not return_dict:\n+            return (output,)\n+\n+        return Transformer2DModelOutput(sample=output)"
        },
        {
          "filename": "src/diffusers/pipelines/__init__.py",
          "status": "modified",
          "additions": 2,
          "deletions": 0,
          "changes": 2,
          "patch": "@@ -404,6 +404,7 @@\n         \"QwenImageControlNetInpaintPipeline\",\n         \"QwenImageControlNetPipeline\",\n     ]\n+    _import_structure[\"chronoedit\"] = [\"ChronoEditPipeline\"]\n try:\n     if not is_onnx_available():\n         raise OptionalDependencyNotAvailable()\n@@ -566,6 +567,7 @@\n         from .bria import BriaPipeline\n         from .bria_fibo import BriaFiboPipeline\n         from .chroma import ChromaImg2ImgPipeline, ChromaPipeline\n+        from .chronoedit import ChronoEditPipeline\n         from .cogvideo import (\n             CogVideoXFunControlPipeline,\n             CogVideoXImageToVideoPipeline,"
        },
        {
          "filename": "src/diffusers/pipelines/chronoedit/__init__.py",
          "status": "added",
          "additions": 47,
          "deletions": 0,
          "changes": 47,
          "patch": "@@ -0,0 +1,47 @@\n+from typing import TYPE_CHECKING\n+\n+from ...utils import (\n+    DIFFUSERS_SLOW_IMPORT,\n+    OptionalDependencyNotAvailable,\n+    _LazyModule,\n+    get_objects_from_module,\n+    is_torch_available,\n+    is_transformers_available,\n+)\n+\n+\n+_dummy_objects = {}\n+_import_structure = {}\n+\n+\n+try:\n+    if not (is_transformers_available() and is_torch_available()):\n+        raise OptionalDependencyNotAvailable()\n+except OptionalDependencyNotAvailable:\n+    from ...utils import dummy_torch_and_transformers_objects  # noqa F403\n+\n+    _dummy_objects.update(get_objects_from_module(dummy_torch_and_transformers_objects))\n+else:\n+    _import_structure[\"pipeline_chronoedit\"] = [\"ChronoEditPipeline\"]\n+if TYPE_CHECKING or DIFFUSERS_SLOW_IMPORT:\n+    try:\n+        if not (is_transformers_available() and is_torch_available()):\n+            raise OptionalDependencyNotAvailable()\n+\n+    except OptionalDependencyNotAvailable:\n+        from ...utils.dummy_torch_and_transformers_objects import *\n+    else:\n+        from .pipeline_chronoedit import ChronoEditPipeline\n+\n+else:\n+    import sys\n+\n+    sys.modules[__name__] = _LazyModule(\n+        __name__,\n+        globals()[\"__file__\"],\n+        _import_structure,\n+        module_spec=__spec__,\n+    )\n+\n+    for name, value in _dummy_objects.items():\n+        setattr(sys.modules[__name__], name, value)"
        },
        {
          "filename": "src/diffusers/pipelines/chronoedit/pipeline_chronoedit.py",
          "status": "added",
          "additions": 752,
          "deletions": 0,
          "changes": 752,
          "patch": "@@ -0,0 +1,752 @@\n+# Copyright 2025 The ChronoEdit Team and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import html\n+from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n+\n+import PIL\n+import regex as re\n+import torch\n+from transformers import AutoTokenizer, CLIPImageProcessor, CLIPVisionModel, UMT5EncoderModel\n+\n+from ...callbacks import MultiPipelineCallbacks, PipelineCallback\n+from ...image_processor import PipelineImageInput\n+from ...loaders import WanLoraLoaderMixin\n+from ...models import AutoencoderKLWan, ChronoEditTransformer3DModel\n+from ...schedulers import FlowMatchEulerDiscreteScheduler\n+from ...utils import is_ftfy_available, is_torch_xla_available, logging, replace_example_docstring\n+from ...utils.torch_utils import randn_tensor\n+from ...video_processor import VideoProcessor\n+from ..pipeline_utils import DiffusionPipeline\n+from .pipeline_output import ChronoEditPipelineOutput\n+\n+\n+if is_torch_xla_available():\n+    import torch_xla.core.xla_model as xm\n+\n+    XLA_AVAILABLE = True\n+else:\n+    XLA_AVAILABLE = False\n+\n+logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n+\n+if is_ftfy_available():\n+    import ftfy\n+\n+EXAMPLE_DOC_STRING = \"\"\"\n+    Examples:\n+        ```python\n+        >>> import torch\n+        >>> import numpy as np\n+        >>> from diffusers import AutoencoderKLWan, ChronoEditTransformer3DModel, ChronoEditPipeline\n+        >>> from diffusers.utils import export_to_video, load_image\n+        >>> from transformers import CLIPVisionModel\n+\n+        >>> # Available models: nvidia/ChronoEdit-14B-Diffusers\n+        >>> model_id = \"nvidia/ChronoEdit-14B-Diffusers\"\n+        >>> image_encoder = CLIPVisionModel.from_pretrained(\n+        ...     model_id, subfolder=\"image_encoder\", torch_dtype=torch.float32\n+        ... )\n+        >>> vae = AutoencoderKLWan.from_pretrained(model_id, subfolder=\"vae\", torch_dtype=torch.float32)\n+        >>> transformer = ChronoEditTransformer3DModel.from_pretrained(\n+        ...     model_id, subfolder=\"transformer\", torch_dtype=torch.bfloat16\n+        ... )\n+        >>> pipe = ChronoEditPipeline.from_pretrained(\n+        ...     model_id, vae=vae, image_encoder=image_encoder, transformer=transformer, torch_dtype=torch.bfloat16\n+        ... )\n+        >>> pipe.to(\"cuda\")\n+\n+        >>> image = load_image(\"https://huggingface.co/spaces/nvidia/ChronoEdit/resolve/main/examples/3.png\")\n+        >>> max_area = 720 * 1280\n+        >>> aspect_ratio = image.height / image.width\n+        >>> mod_value = pipe.vae_scale_factor_spatial * pipe.transformer.config.patch_size[1]\n+        >>> height = round(np.sqrt(max_area * aspect_ratio)) // mod_value * mod_value\n+        >>> width = round(np.sqrt(max_area / aspect_ratio)) // mod_value * mod_value\n+        >>> image = image.resize((width, height))\n+        >>> prompt = (\n+        ...     \"The user wants to transform the image by adding a small, cute mouse sitting inside the floral teacup, enjoying a spa bath. The mouse should appear relaxed and cheerful, with a tiny white bath towel draped over its head like a turban. It should be positioned comfortably in the cup\u2019s liquid, with gentle steam rising around it to blend with the cozy atmosphere. \"\n+        ...     \"The mouse\u2019s pose should be natural\u2014perhaps sitting upright with paws resting lightly on the rim or submerged in the tea. The teacup\u2019s floral design, gold trim, and warm lighting must remain unchanged to preserve the original aesthetic. The steam should softly swirl around the mouse, enhancing the spa-like, whimsical mood.\"\n+        ... )\n+\n+        >>> output = pipe(\n+        ...     image=image,\n+        ...     prompt=prompt,\n+        ...     height=height,\n+        ...     width=width,\n+        ...     num_frames=5,\n+        ...     guidance_scale=5.0,\n+        ...     enable_temporal_reasoning=False,\n+        ...     num_temporal_reasoning_steps=0,\n+        ... ).frames[0]\n+        >>> export_to_video(output, \"output.mp4\", fps=16)\n+        ```\n+\"\"\"\n+\n+\n+def basic_clean(text):\n+    text = ftfy.fix_text(text)\n+    text = html.unescape(html.unescape(text))\n+    return text.strip()\n+\n+\n+def whitespace_clean(text):\n+    text = re.sub(r\"\\s+\", \" \", text)\n+    text = text.strip()\n+    return text\n+\n+\n+def prompt_clean(text):\n+    text = whitespace_clean(basic_clean(text))\n+    return text\n+\n+\n+# Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_img2img.retrieve_latents\n+def retrieve_latents(\n+    encoder_output: torch.Tensor, generator: Optional[torch.Generator] = None, sample_mode: str = \"sample\"\n+):\n+    if hasattr(encoder_output, \"latent_dist\") and sample_mode == \"sample\":\n+        return encoder_output.latent_dist.sample(generator)\n+    elif hasattr(encoder_output, \"latent_dist\") and sample_mode == \"argmax\":\n+        return encoder_output.latent_dist.mode()\n+    elif hasattr(encoder_output, \"latents\"):\n+        return encoder_output.latents\n+    else:\n+        raise AttributeError(\"Could not access latents of provided encoder_output\")\n+\n+\n+class ChronoEditPipeline(DiffusionPipeline, WanLoraLoaderMixin):\n+    r\"\"\"\n+    Pipeline for image-to-video generation using Wan.\n+\n+    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods\n+    implemented for all pipelines (downloading, saving, running on a particular device, etc.).\n+\n+    Args:\n+        tokenizer ([`T5Tokenizer`]):\n+            Tokenizer from [T5](https://huggingface.co/docs/transformers/en/model_doc/t5#transformers.T5Tokenizer),\n+            specifically the [google/umt5-xxl](https://huggingface.co/google/umt5-xxl) variant.\n+        text_encoder ([`T5EncoderModel`]):\n+            [T5](https://huggingface.co/docs/transformers/en/model_doc/t5#transformers.T5EncoderModel), specifically\n+            the [google/umt5-xxl](https://huggingface.co/google/umt5-xxl) variant.\n+        image_encoder ([`CLIPVisionModel`]):\n+            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPVisionModel), specifically\n+            the\n+            [clip-vit-huge-patch14](https://github.com/mlfoundations/open_clip/blob/main/docs/PRETRAINED.md#vit-h14-xlm-roberta-large)\n+            variant.\n+        transformer ([`WanTransformer3DModel`]):\n+            Conditional Transformer to denoise the input latents.\n+        scheduler ([`UniPCMultistepScheduler`]):\n+            A scheduler to be used in combination with `transformer` to denoise the encoded image latents.\n+        vae ([`AutoencoderKLWan`]):\n+            Variational Auto-Encoder (VAE) Model to encode and decode videos to and from latent representations.\n+    \"\"\"\n+\n+    model_cpu_offload_seq = \"text_encoder->image_encoder->transformer->vae\"\n+    _callback_tensor_inputs = [\"latents\", \"prompt_embeds\", \"negative_prompt_embeds\"]\n+\n+    def __init__(\n+        self,\n+        tokenizer: AutoTokenizer,\n+        text_encoder: UMT5EncoderModel,\n+        image_encoder: CLIPVisionModel,\n+        image_processor: CLIPImageProcessor,\n+        transformer: ChronoEditTransformer3DModel,\n+        vae: AutoencoderKLWan,\n+        scheduler: FlowMatchEulerDiscreteScheduler,\n+    ):\n+        super().__init__()\n+\n+        self.register_modules(\n+            vae=vae,\n+            text_encoder=text_encoder,\n+            tokenizer=tokenizer,\n+            image_encoder=image_encoder,\n+            transformer=transformer,\n+            scheduler=scheduler,\n+            image_processor=image_processor,\n+        )\n+\n+        self.vae_scale_factor_temporal = self.vae.config.scale_factor_temporal if getattr(self, \"vae\", None) else 4\n+        self.vae_scale_factor_spatial = self.vae.config.scale_factor_spatial if getattr(self, \"vae\", None) else 8\n+        self.video_processor = VideoProcessor(vae_scale_factor=self.vae_scale_factor_spatial)\n+        self.image_processor = image_processor\n+\n+    # Copied from diffusers.pipelines.wan.pipeline_wan_i2v.WanImageToVideoPipeline._get_t5_prompt_embeds\n+    def _get_t5_prompt_embeds(\n+        self,\n+        prompt: Union[str, List[str]] = None,\n+        num_videos_per_prompt: int = 1,\n+        max_sequence_length: int = 512,\n+        device: Optional[torch.device] = None,\n+        dtype: Optional[torch.dtype] = None,\n+    ):\n+        device = device or self._execution_device\n+        dtype = dtype or self.text_encoder.dtype\n+\n+        prompt = [prompt] if isinstance(prompt, str) else prompt\n+        prompt = [prompt_clean(u) for u in prompt]\n+        batch_size = len(prompt)\n+\n+        text_inputs = self.tokenizer(\n+            prompt,\n+            padding=\"max_length\",\n+            max_length=max_sequence_length,\n+            truncation=True,\n+            add_special_tokens=True,\n+            return_attention_mask=True,\n+            return_tensors=\"pt\",\n+        )\n+        text_input_ids, mask = text_inputs.input_ids, text_inputs.attention_mask\n+        seq_lens = mask.gt(0).sum(dim=1).long()\n+\n+        prompt_embeds = self.text_encoder(text_input_ids.to(device), mask.to(device)).last_hidden_state\n+        prompt_embeds = prompt_embeds.to(dtype=dtype, device=device)\n+        prompt_embeds = [u[:v] for u, v in zip(prompt_embeds, seq_lens)]\n+        prompt_embeds = torch.stack(\n+            [torch.cat([u, u.new_zeros(max_sequence_length - u.size(0), u.size(1))]) for u in prompt_embeds], dim=0\n+        )\n+\n+        # duplicate text embeddings for each generation per prompt, using mps friendly method\n+        _, seq_len, _ = prompt_embeds.shape\n+        prompt_embeds = prompt_embeds.repeat(1, num_videos_per_prompt, 1)\n+        prompt_embeds = prompt_embeds.view(batch_size * num_videos_per_prompt, seq_len, -1)\n+\n+        return prompt_embeds\n+\n+    # Copied from diffusers.pipelines.wan.pipeline_wan_i2v.WanImageToVideoPipeline.encode_image\n+    def encode_image(\n+        self,\n+        image: PipelineImageInput,\n+        device: Optional[torch.device] = None,\n+    ):\n+        device = device or self._execution_device\n+        image = self.image_processor(images=image, return_tensors=\"pt\").to(device)\n+        image_embeds = self.image_encoder(**image, output_hidden_states=True)\n+        return image_embeds.hidden_states[-2]\n+\n+    # Copied from diffusers.pipelines.wan.pipeline_wan.WanPipeline.encode_prompt\n+    def encode_prompt(\n+        self,\n+        prompt: Union[str, List[str]],\n+        negative_prompt: Optional[Union[str, List[str]]] = None,\n+        do_classifier_free_guidance: bool = True,\n+        num_videos_per_prompt: int = 1,\n+        prompt_embeds: Optional[torch.Tensor] = None,\n+        negative_prompt_embeds: Optional[torch.Tensor] = None,\n+        max_sequence_length: int = 226,\n+        device: Optional[torch.device] = None,\n+        dtype: Optional[torch.dtype] = None,\n+    ):\n+        r\"\"\"\n+        Encodes the prompt into text encoder hidden states.\n+\n+        Args:\n+            prompt (`str` or `List[str]`, *optional*):\n+                prompt to be encoded\n+            negative_prompt (`str` or `List[str]`, *optional*):\n+                The prompt or prompts not to guide the image generation. If not defined, one has to pass\n+                `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is\n+                less than `1`).\n+            do_classifier_free_guidance (`bool`, *optional*, defaults to `True`):\n+                Whether to use classifier free guidance or not.\n+            num_videos_per_prompt (`int`, *optional*, defaults to 1):\n+                Number of videos that should be generated per prompt. torch device to place the resulting embeddings on\n+            prompt_embeds (`torch.Tensor`, *optional*):\n+                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n+                provided, text embeddings will be generated from `prompt` input argument.\n+            negative_prompt_embeds (`torch.Tensor`, *optional*):\n+                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt\n+                weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt` input\n+                argument.\n+            device: (`torch.device`, *optional*):\n+                torch device\n+            dtype: (`torch.dtype`, *optional*):\n+                torch dtype\n+        \"\"\"\n+        device = device or self._execution_device\n+\n+        prompt = [prompt] if isinstance(prompt, str) else prompt\n+        if prompt is not None:\n+            batch_size = len(prompt)\n+        else:\n+            batch_size = prompt_embeds.shape[0]\n+\n+        if prompt_embeds is None:\n+            prompt_embeds = self._get_t5_prompt_embeds(\n+                prompt=prompt,\n+                num_videos_per_prompt=num_videos_per_prompt,\n+                max_sequence_length=max_sequence_length,\n+                device=device,\n+                dtype=dtype,\n+            )\n+\n+        if do_classifier_free_guidance and negative_prompt_embeds is None:\n+            negative_prompt = negative_prompt or \"\"\n+            negative_prompt = batch_size * [negative_prompt] if isinstance(negative_prompt, str) else negative_prompt\n+\n+            if prompt is not None and type(prompt) is not type(negative_prompt):\n+                raise TypeError(\n+                    f\"`negative_prompt` should be the same type to `prompt`, but got {type(negative_prompt)} !=\"\n+                    f\" {type(prompt)}.\"\n+                )\n+            elif batch_size != len(negative_prompt):\n+                raise ValueError(\n+                    f\"`negative_prompt`: {negative_prompt} has batch size {len(negative_prompt)}, but `prompt`:\"\n+                    f\" {prompt} has batch size {batch_size}. Please make sure that passed `negative_prompt` matches\"\n+                    \" the batch size of `prompt`.\"\n+                )\n+\n+            negative_prompt_embeds = self._get_t5_prompt_embeds(\n+                prompt=negative_prompt,\n+                num_videos_per_prompt=num_videos_per_prompt,\n+                max_sequence_length=max_sequence_length,\n+                device=device,\n+                dtype=dtype,\n+            )\n+\n+        return prompt_embeds, negative_prompt_embeds\n+\n+    # modified from diffusers.pipelines.wan.pipeline_wan_i2v.WanImageToVideoPipeline.check_inputs\n+    def check_inputs(\n+        self,\n+        prompt,\n+        negative_prompt,\n+        image,\n+        height,\n+        width,\n+        prompt_embeds=None,\n+        negative_prompt_embeds=None,\n+        image_embeds=None,\n+        callback_on_step_end_tensor_inputs=None,\n+    ):\n+        if image is not None and image_embeds is not None:\n+            raise ValueError(\n+                f\"Cannot forward both `image`: {image} and `image_embeds`: {image_embeds}. Please make sure to\"\n+                \" only forward one of the two.\"\n+            )\n+        if image is None and image_embeds is None:\n+            raise ValueError(\n+                \"Provide either `image` or `prompt_embeds`. Cannot leave both `image` and `image_embeds` undefined.\"\n+            )\n+        if image is not None and not isinstance(image, torch.Tensor) and not isinstance(image, PIL.Image.Image):\n+            raise ValueError(f\"`image` has to be of type `torch.Tensor` or `PIL.Image.Image` but is {type(image)}\")\n+        if height % 16 != 0 or width % 16 != 0:\n+            raise ValueError(f\"`height` and `width` have to be divisible by 16 but are {height} and {width}.\")\n+\n+        if callback_on_step_end_tensor_inputs is not None and not all(\n+            k in self._callback_tensor_inputs for k in callback_on_step_end_tensor_inputs\n+        ):\n+            raise ValueError(\n+                f\"`callback_on_step_end_tensor_inputs` has to be in {self._callback_tensor_inputs}, but found {[k for k in callback_on_step_end_tensor_inputs if k not in self._callback_tensor_inputs]}\"\n+            )\n+\n+        if prompt is not None and prompt_embeds is not None:\n+            raise ValueError(\n+                f\"Cannot forward both `prompt`: {prompt} and `prompt_embeds`: {prompt_embeds}. Please make sure to\"\n+                \" only forward one of the two.\"\n+            )\n+        elif negative_prompt is not None and negative_prompt_embeds is not None:\n+            raise ValueError(\n+                f\"Cannot forward both `negative_prompt`: {negative_prompt} and `negative_prompt_embeds`: {negative_prompt_embeds}. Please make sure to\"\n+                \" only forward one of the two.\"\n+            )\n+        elif prompt is None and prompt_embeds is None:\n+            raise ValueError(\n+                \"Provide either `prompt` or `prompt_embeds`. Cannot leave both `prompt` and `prompt_embeds` undefined.\"\n+            )\n+        elif prompt is not None and (not isinstance(prompt, str) and not isinstance(prompt, list)):\n+            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n+        elif negative_prompt is not None and (\n+            not isinstance(negative_prompt, str) and not isinstance(negative_prompt, list)\n+        ):\n+            raise ValueError(f\"`negative_prompt` has to be of type `str` or `list` but is {type(negative_prompt)}\")\n+\n+    # modified from diffusers.pipelines.wan.pipeline_wan_i2v.WanImageToVideoPipeline.prepare_latents\n+    def prepare_latents(\n+        self,\n+        image: PipelineImageInput,\n+        batch_size: int,\n+        num_channels_latents: int = 16,\n+        height: int = 480,\n+        width: int = 832,\n+        num_frames: int = 81,\n+        dtype: Optional[torch.dtype] = None,\n+        device: Optional[torch.device] = None,\n+        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n+        latents: Optional[torch.Tensor] = None,\n+    ) -> Tuple[torch.Tensor, torch.Tensor]:\n+        num_latent_frames = (num_frames - 1) // self.vae_scale_factor_temporal + 1\n+        latent_height = height // self.vae_scale_factor_spatial\n+        latent_width = width // self.vae_scale_factor_spatial\n+\n+        shape = (batch_size, num_channels_latents, num_latent_frames, latent_height, latent_width)\n+        if isinstance(generator, list) and len(generator) != batch_size:\n+            raise ValueError(\n+                f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n+                f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n+            )\n+\n+        if latents is None:\n+            latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n+        else:\n+            latents = latents.to(device=device, dtype=dtype)\n+\n+        image = image.unsqueeze(2)  # [batch_size, channels, 1, height, width]\n+        video_condition = torch.cat(\n+            [image, image.new_zeros(image.shape[0], image.shape[1], num_frames - 1, height, width)], dim=2\n+        )\n+        video_condition = video_condition.to(device=device, dtype=self.vae.dtype)\n+\n+        latents_mean = (\n+            torch.tensor(self.vae.config.latents_mean)\n+            .view(1, self.vae.config.z_dim, 1, 1, 1)\n+            .to(latents.device, latents.dtype)\n+        )\n+        latents_std = 1.0 / torch.tensor(self.vae.config.latents_std).view(1, self.vae.config.z_dim, 1, 1, 1).to(\n+            latents.device, latents.dtype\n+        )\n+\n+        if isinstance(generator, list):\n+            latent_condition = [\n+                retrieve_latents(self.vae.encode(video_condition), sample_mode=\"argmax\") for _ in generator\n+            ]\n+            latent_condition = torch.cat(latent_condition)\n+        else:\n+            latent_condition = retrieve_latents(self.vae.encode(video_condition), sample_mode=\"argmax\")\n+            latent_condition = latent_condition.repeat(batch_size, 1, 1, 1, 1)\n+\n+        latent_condition = latent_condition.to(dtype)\n+        latent_condition = (latent_condition - latents_mean) * latents_std\n+\n+        mask_lat_size = torch.ones(batch_size, 1, num_frames, latent_height, latent_width)\n+        mask_lat_size[:, :, list(range(1, num_frames))] = 0\n+        first_frame_mask = mask_lat_size[:, :, 0:1]\n+        first_frame_mask = torch.repeat_interleave(first_frame_mask, dim=2, repeats=self.vae_scale_factor_temporal)\n+        mask_lat_size = torch.concat([first_frame_mask, mask_lat_size[:, :, 1:, :]], dim=2)\n+        mask_lat_size = mask_lat_size.view(batch_size, -1, self.vae_scale_factor_temporal, latent_height, latent_width)\n+        mask_lat_size = mask_lat_size.transpose(1, 2)\n+        mask_lat_size = mask_lat_size.to(latent_condition.device)\n+\n+        return latents, torch.concat([mask_lat_size, latent_condition], dim=1)\n+\n+    @property\n+    def guidance_scale(self):\n+        return self._guidance_scale\n+\n+    @property\n+    def do_classifier_free_guidance(self):\n+        return self._guidance_scale > 1\n+\n+    @property\n+    def num_timesteps(self):\n+        return self._num_timesteps\n+\n+    @property\n+    def current_timestep(self):\n+        return self._current_timestep\n+\n+    @property\n+    def interrupt(self):\n+        return self._interrupt\n+\n+    @property\n+    def attention_kwargs(self):\n+        return self._attention_kwargs\n+\n+    @torch.no_grad()\n+    @replace_example_docstring(EXAMPLE_DOC_STRING)\n+    def __call__(\n+        self,\n+        image: PipelineImageInput,\n+        prompt: Union[str, List[str]] = None,\n+        negative_prompt: Union[str, List[str]] = None,\n+        height: int = 480,\n+        width: int = 832,\n+        num_frames: int = 81,\n+        num_inference_steps: int = 50,\n+        guidance_scale: float = 5.0,\n+        num_videos_per_prompt: Optional[int] = 1,\n+        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n+        latents: Optional[torch.Tensor] = None,\n+        prompt_embeds: Optional[torch.Tensor] = None,\n+        negative_prompt_embeds: Optional[torch.Tensor] = None,\n+        image_embeds: Optional[torch.Tensor] = None,\n+        output_type: Optional[str] = \"np\",\n+        return_dict: bool = True,\n+        attention_kwargs: Optional[Dict[str, Any]] = None,\n+        callback_on_step_end: Optional[\n+            Union[Callable[[int, int, Dict], None], PipelineCallback, MultiPipelineCallbacks]\n+        ] = None,\n+        callback_on_step_end_tensor_inputs: List[str] = [\"latents\"],\n+        max_sequence_length: int = 512,\n+        enable_temporal_reasoning: bool = False,\n+        num_temporal_reasoning_steps: int = 0,\n+    ):\n+        r\"\"\"\n+        The call function to the pipeline for generation.\n+\n+        Args:\n+            image (`PipelineImageInput`):\n+                The input image to condition the generation on. Must be an image, a list of images or a `torch.Tensor`.\n+            prompt (`str` or `List[str]`, *optional*):\n+                The prompt or prompts to guide the image generation. If not defined, one has to pass `prompt_embeds`.\n+                instead.\n+            negative_prompt (`str` or `List[str]`, *optional*):\n+                The prompt or prompts not to guide the image generation. If not defined, one has to pass\n+                `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is\n+                less than `1`).\n+            height (`int`, defaults to `480`):\n+                The height of the generated video.\n+            width (`int`, defaults to `832`):\n+                The width of the generated video.\n+            num_frames (`int`, defaults to `81`):\n+                The number of frames in the generated video.\n+            num_inference_steps (`int`, defaults to `50`):\n+                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n+                expense of slower inference.\n+            guidance_scale (`float`, defaults to `5.0`):\n+                Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).\n+                `guidance_scale` is defined as `w` of equation 2. of [Imagen\n+                Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale >\n+                1`. Higher guidance scale encourages to generate images that are closely linked to the text `prompt`,\n+                usually at the expense of lower image quality.\n+            num_videos_per_prompt (`int`, *optional*, defaults to 1):\n+                The number of images to generate per prompt.\n+            generator (`torch.Generator` or `List[torch.Generator]`, *optional*):\n+                A [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html) to make\n+                generation deterministic.\n+            latents (`torch.Tensor`, *optional*):\n+                Pre-generated noisy latents sampled from a Gaussian distribution, to be used as inputs for image\n+                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents\n+                tensor is generated by sampling using the supplied random `generator`.\n+            prompt_embeds (`torch.Tensor`, *optional*):\n+                Pre-generated text embeddings. Can be used to easily tweak text inputs (prompt weighting). If not\n+                provided, text embeddings are generated from the `prompt` input argument.\n+            negative_prompt_embeds (`torch.Tensor`, *optional*):\n+                Pre-generated text embeddings. Can be used to easily tweak text inputs (prompt weighting). If not\n+                provided, text embeddings are generated from the `negative_prompt` input argument.\n+            image_embeds (`torch.Tensor`, *optional*):\n+                Pre-generated image embeddings. Can be used to easily tweak image inputs (weighting). If not provided,\n+                image embeddings are generated from the `image` input argument.\n+            output_type (`str`, *optional*, defaults to `\"np\"`):\n+                The output format of the generated image. Choose between `PIL.Image` or `np.array`.\n+            return_dict (`bool`, *optional*, defaults to `True`):\n+                Whether or not to return a [`ChronoEditPipelineOutput`] instead of a plain tuple.\n+            attention_kwargs (`dict`, *optional*):\n+                A kwargs dictionary that if specified is passed along to the `AttentionProcessor` as defined under\n+                `self.processor` in\n+                [diffusers.models.attention_processor](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).\n+            callback_on_step_end (`Callable`, `PipelineCallback`, `MultiPipelineCallbacks`, *optional*):\n+                A function or a subclass of `PipelineCallback` or `MultiPipelineCallbacks` that is called at the end of\n+                each denoising step during the inference. with the following arguments: `callback_on_step_end(self:\n+                DiffusionPipeline, step: int, timestep: int, callback_kwargs: Dict)`. `callback_kwargs` will include a\n+                list of all tensors as specified by `callback_on_step_end_tensor_inputs`.\n+            callback_on_step_end_tensor_inputs (`List`, *optional*):\n+                The list of tensor inputs for the `callback_on_step_end` function. The tensors specified in the list\n+                will be passed as `callback_kwargs` argument. You will only be able to include variables listed in the\n+                `._callback_tensor_inputs` attribute of your pipeline class.\n+            max_sequence_length (`int`, defaults to `512`):\n+                The maximum sequence length of the text encoder. If the prompt is longer than this, it will be\n+                truncated. If the prompt is shorter, it will be padded to this length.\n+            enable_temporal_reasoning (`bool`, *optional*, defaults to `False`):\n+                Whether to enable temporal reasoning.\n+            num_temporal_reasoning_steps (`int`, *optional*, defaults to `0`):\n+                The number of steps to enable temporal reasoning.\n+\n+        Examples:\n+\n+        Returns:\n+            [`~ChronoEditPipelineOutput`] or `tuple`:\n+                If `return_dict` is `True`, [`ChronoEditPipelineOutput`] is returned, otherwise a `tuple` is returned\n+                where the first element is a list with the generated images and the second element is a list of `bool`s\n+                indicating whether the corresponding generated image contains \"not-safe-for-work\" (nsfw) content.\n+        \"\"\"\n+\n+        if isinstance(callback_on_step_end, (PipelineCallback, MultiPipelineCallbacks)):\n+            callback_on_step_end_tensor_inputs = callback_on_step_end.tensor_inputs\n+\n+        # 1. Check inputs. Raise error if not correct\n+        self.check_inputs(\n+            prompt,\n+            negative_prompt,\n+            image,\n+            height,\n+            width,\n+            prompt_embeds,\n+            negative_prompt_embeds,\n+            image_embeds,\n+            callback_on_step_end_tensor_inputs,\n+        )\n+\n+        num_frames = 5 if not enable_temporal_reasoning else num_frames\n+\n+        if num_frames % self.vae_scale_factor_temporal != 1:\n+            logger.warning(\n+                f\"`num_frames - 1` has to be divisible by {self.vae_scale_factor_temporal}. Rounding to the nearest number.\"\n+            )\n+            num_frames = num_frames // self.vae_scale_factor_temporal * self.vae_scale_factor_temporal + 1\n+        num_frames = max(num_frames, 1)\n+\n+        self._guidance_scale = guidance_scale\n+        self._attention_kwargs = attention_kwargs\n+        self._current_timestep = None\n+        self._interrupt = False\n+\n+        device = self._execution_device\n+\n+        # 2. Define call parameters\n+        if prompt is not None and isinstance(prompt, str):\n+            batch_size = 1\n+        elif prompt is not None and isinstance(prompt, list):\n+            batch_size = len(prompt)\n+        else:\n+            batch_size = prompt_embeds.shape[0]\n+\n+        # 3. Encode input prompt\n+        prompt_embeds, negative_prompt_embeds = self.encode_prompt(\n+            prompt=prompt,\n+            negative_prompt=negative_prompt,\n+            do_classifier_free_guidance=self.do_classifier_free_guidance,\n+            num_videos_per_prompt=num_videos_per_prompt,\n+            prompt_embeds=prompt_embeds,\n+            negative_prompt_embeds=negative_prompt_embeds,\n+            max_sequence_length=max_sequence_length,\n+            device=device,\n+        )\n+\n+        # Encode image embedding\n+        transformer_dtype = self.transformer.dtype\n+        prompt_embeds = prompt_embeds.to(transformer_dtype)\n+        if negative_prompt_embeds is not None:\n+            negative_prompt_embeds = negative_prompt_embeds.to(transformer_dtype)\n+\n+        if image_embeds is None:\n+            image_embeds = self.encode_image(image, device)\n+        image_embeds = image_embeds.repeat(batch_size, 1, 1)\n+        image_embeds = image_embeds.to(transformer_dtype)\n+\n+        # 4. Prepare timesteps\n+        self.scheduler.set_timesteps(num_inference_steps, device=device)\n+        timesteps = self.scheduler.timesteps\n+\n+        # 5. Prepare latent variables\n+        num_channels_latents = self.vae.config.z_dim\n+        image = self.video_processor.preprocess(image, height=height, width=width).to(device, dtype=torch.float32)\n+        latents, condition = self.prepare_latents(\n+            image,\n+            batch_size * num_videos_per_prompt,\n+            num_channels_latents,\n+            height,\n+            width,\n+            num_frames,\n+            torch.float32,\n+            device,\n+            generator,\n+            latents,\n+        )\n+\n+        # 6. Denoising loop\n+        num_warmup_steps = len(timesteps) - num_inference_steps * self.scheduler.order\n+        self._num_timesteps = len(timesteps)\n+\n+        with self.progress_bar(total=num_inference_steps) as progress_bar:\n+            for i, t in enumerate(timesteps):\n+                if self.interrupt:\n+                    continue\n+\n+                if enable_temporal_reasoning and i == num_temporal_reasoning_steps:\n+                    latents = latents[:, :, [0, -1]]\n+                    condition = condition[:, :, [0, -1]]\n+\n+                    for j in range(len(self.scheduler.model_outputs)):\n+                        if self.scheduler.model_outputs[j] is not None:\n+                            if latents.shape[-3] != self.scheduler.model_outputs[j].shape[-3]:\n+                                self.scheduler.model_outputs[j] = self.scheduler.model_outputs[j][:, :, [0, -1]]\n+                    if self.scheduler.last_sample is not None:\n+                        self.scheduler.last_sample = self.scheduler.last_sample[:, :, [0, -1]]\n+\n+                self._current_timestep = t\n+                latent_model_input = torch.cat([latents, condition], dim=1).to(transformer_dtype)\n+                timestep = t.expand(latents.shape[0])\n+\n+                noise_pred = self.transformer(\n+                    hidden_states=latent_model_input,\n+                    timestep=timestep,\n+                    encoder_hidden_states=prompt_embeds,\n+                    encoder_hidden_states_image=image_embeds,\n+                    attention_kwargs=attention_kwargs,\n+                    return_dict=False,\n+                )[0]\n+\n+                if self.do_classifier_free_guidance:\n+                    noise_uncond = self.transformer(\n+                        hidden_states=latent_model_input,\n+                        timestep=timestep,\n+                        encoder_hidden_states=negative_prompt_embeds,\n+                        encoder_hidden_states_image=image_embeds,\n+                        attention_kwargs=attention_kwargs,\n+                        return_dict=False,\n+                    )[0]\n+                    noise_pred = noise_uncond + guidance_scale * (noise_pred - noise_uncond)\n+\n+                # compute the previous noisy sample x_t -> x_t-1\n+                latents = self.scheduler.step(noise_pred, t, latents, return_dict=False)[0]\n+\n+                if callback_on_step_end is not None:\n+                    callback_kwargs = {}\n+                    for k in callback_on_step_end_tensor_inputs:\n+                        callback_kwargs[k] = locals()[k]\n+                    callback_outputs = callback_on_step_end(self, i, t, callback_kwargs)\n+\n+                    latents = callback_outputs.pop(\"latents\", latents)\n+                    prompt_embeds = callback_outputs.pop(\"prompt_embeds\", prompt_embeds)\n+                    negative_prompt_embeds = callback_outputs.pop(\"negative_prompt_embeds\", negative_prompt_embeds)\n+\n+                # call the callback, if provided\n+                if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n+                    progress_bar.update()\n+\n+                if XLA_AVAILABLE:\n+                    xm.mark_step()\n+\n+        self._current_timestep = None\n+\n+        if not output_type == \"latent\":\n+            latents = latents.to(self.vae.dtype)\n+            latents_mean = (\n+                torch.tensor(self.vae.config.latents_mean)\n+                .view(1, self.vae.config.z_dim, 1, 1, 1)\n+                .to(latents.device, latents.dtype)\n+            )\n+            latents_std = 1.0 / torch.tensor(self.vae.config.latents_std).view(1, self.vae.config.z_dim, 1, 1, 1).to(\n+                latents.device, latents.dtype\n+            )\n+            latents = latents / latents_std + latents_mean\n+            if enable_temporal_reasoning and latents.shape[2] > 2:\n+                video_edit = self.vae.decode(latents[:, :, [0, -1]], return_dict=False)[0]\n+                video_reason = self.vae.decode(latents[:, :, :-1], return_dict=False)[0]\n+                video = torch.cat([video_reason, video_edit[:, :, 1:]], dim=2)\n+            else:\n+                video = self.vae.decode(latents, return_dict=False)[0]\n+            video = self.video_processor.postprocess_video(video, output_type=output_type)\n+        else:\n+            video = latents\n+\n+        # Offload all models\n+        self.maybe_free_model_hooks()\n+\n+        if not return_dict:\n+            return (video,)\n+\n+        return ChronoEditPipelineOutput(frames=video)"
        },
        {
          "filename": "src/diffusers/pipelines/chronoedit/pipeline_output.py",
          "status": "added",
          "additions": 20,
          "deletions": 0,
          "changes": 20,
          "patch": "@@ -0,0 +1,20 @@\n+from dataclasses import dataclass\n+\n+import torch\n+\n+from diffusers.utils import BaseOutput\n+\n+\n+@dataclass\n+class ChronoEditPipelineOutput(BaseOutput):\n+    r\"\"\"\n+    Output class for ChronoEdit pipelines.\n+\n+    Args:\n+        frames (`torch.Tensor`, `np.ndarray`, or List[List[PIL.Image.Image]]):\n+            List of video outputs - It can be a nested list of length `batch_size,` with each sub-list containing\n+            denoised PIL image sequences of length `num_frames.` It can also be a NumPy array or Torch tensor of shape\n+            `(batch_size, num_frames, channels, height, width)`.\n+    \"\"\"\n+\n+    frames: torch.Tensor"
        },
        {
          "filename": "src/diffusers/utils/dummy_pt_objects.py",
          "status": "modified",
          "additions": 15,
          "deletions": 0,
          "changes": 15,
          "patch": "@@ -648,6 +648,21 @@ def from_pretrained(cls, *args, **kwargs):\n         requires_backends(cls, [\"torch\"])\n \n \n+class ChronoEditTransformer3DModel(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+    @classmethod\n+    def from_config(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\"])\n+\n+    @classmethod\n+    def from_pretrained(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\"])\n+\n+\n class CogVideoXTransformer3DModel(metaclass=DummyObject):\n     _backends = [\"torch\"]\n "
        },
        {
          "filename": "src/diffusers/utils/dummy_torch_and_transformers_objects.py",
          "status": "modified",
          "additions": 15,
          "deletions": 0,
          "changes": 15,
          "patch": "@@ -542,6 +542,21 @@ def from_pretrained(cls, *args, **kwargs):\n         requires_backends(cls, [\"torch\", \"transformers\"])\n \n \n+class ChronoEditPipeline(metaclass=DummyObject):\n+    _backends = [\"torch\", \"transformers\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_config(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_pretrained(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+\n class CLIPImageProjection(metaclass=DummyObject):\n     _backends = [\"torch\", \"transformers\"]\n "
        },
        {
          "filename": "tests/pipelines/chronoedit/__init__.py",
          "status": "added",
          "additions": 0,
          "deletions": 0,
          "changes": 0,
          "patch": ""
        },
        {
          "filename": "tests/pipelines/chronoedit/test_chronoedit.py",
          "status": "added",
          "additions": 176,
          "deletions": 0,
          "changes": 176,
          "patch": "@@ -0,0 +1,176 @@\n+# Copyright 2025 The HuggingFace Team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import unittest\n+\n+import torch\n+from PIL import Image\n+from transformers import (\n+    AutoTokenizer,\n+    CLIPImageProcessor,\n+    CLIPVisionConfig,\n+    CLIPVisionModelWithProjection,\n+    T5EncoderModel,\n+)\n+\n+from diffusers import (\n+    AutoencoderKLWan,\n+    ChronoEditPipeline,\n+    ChronoEditTransformer3DModel,\n+    FlowMatchEulerDiscreteScheduler,\n+)\n+\n+from ...testing_utils import enable_full_determinism\n+from ..pipeline_params import TEXT_TO_IMAGE_BATCH_PARAMS, TEXT_TO_IMAGE_IMAGE_PARAMS, TEXT_TO_IMAGE_PARAMS\n+from ..test_pipelines_common import PipelineTesterMixin\n+\n+\n+enable_full_determinism()\n+\n+\n+class ChronoEditPipelineFastTests(PipelineTesterMixin, unittest.TestCase):\n+    pipeline_class = ChronoEditPipeline\n+    params = TEXT_TO_IMAGE_PARAMS - {\"cross_attention_kwargs\", \"height\", \"width\"}\n+    batch_params = TEXT_TO_IMAGE_BATCH_PARAMS\n+    image_params = TEXT_TO_IMAGE_IMAGE_PARAMS\n+    image_latents_params = TEXT_TO_IMAGE_IMAGE_PARAMS\n+    required_optional_params = frozenset(\n+        [\n+            \"num_inference_steps\",\n+            \"generator\",\n+            \"latents\",\n+            \"return_dict\",\n+            \"callback_on_step_end\",\n+            \"callback_on_step_end_tensor_inputs\",\n+        ]\n+    )\n+    test_xformers_attention = False\n+    supports_dduf = False\n+\n+    def get_dummy_components(self):\n+        torch.manual_seed(0)\n+        vae = AutoencoderKLWan(\n+            base_dim=3,\n+            z_dim=16,\n+            dim_mult=[1, 1, 1, 1],\n+            num_res_blocks=1,\n+            temperal_downsample=[False, True, True],\n+        )\n+\n+        torch.manual_seed(0)\n+        # TODO: impl FlowDPMSolverMultistepScheduler\n+        scheduler = FlowMatchEulerDiscreteScheduler(shift=7.0)\n+        text_encoder = T5EncoderModel.from_pretrained(\"hf-internal-testing/tiny-random-t5\")\n+        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-t5\")\n+\n+        torch.manual_seed(0)\n+        transformer = ChronoEditTransformer3DModel(\n+            patch_size=(1, 2, 2),\n+            num_attention_heads=2,\n+            attention_head_dim=12,\n+            in_channels=36,\n+            out_channels=16,\n+            text_dim=32,\n+            freq_dim=256,\n+            ffn_dim=32,\n+            num_layers=2,\n+            cross_attn_norm=True,\n+            qk_norm=\"rms_norm_across_heads\",\n+            rope_max_seq_len=32,\n+            image_dim=4,\n+        )\n+\n+        torch.manual_seed(0)\n+        image_encoder_config = CLIPVisionConfig(\n+            hidden_size=4,\n+            projection_dim=4,\n+            num_hidden_layers=2,\n+            num_attention_heads=2,\n+            image_size=32,\n+            intermediate_size=16,\n+            patch_size=1,\n+        )\n+        image_encoder = CLIPVisionModelWithProjection(image_encoder_config)\n+\n+        torch.manual_seed(0)\n+        image_processor = CLIPImageProcessor(crop_size=32, size=32)\n+\n+        components = {\n+            \"transformer\": transformer,\n+            \"vae\": vae,\n+            \"scheduler\": scheduler,\n+            \"text_encoder\": text_encoder,\n+            \"tokenizer\": tokenizer,\n+            \"image_encoder\": image_encoder,\n+            \"image_processor\": image_processor,\n+        }\n+        return components\n+\n+    def get_dummy_inputs(self, device, seed=0):\n+        if str(device).startswith(\"mps\"):\n+            generator = torch.manual_seed(seed)\n+        else:\n+            generator = torch.Generator(device=device).manual_seed(seed)\n+        image_height = 16\n+        image_width = 16\n+        image = Image.new(\"RGB\", (image_width, image_height))\n+        inputs = {\n+            \"image\": image,\n+            \"prompt\": \"dance monkey\",\n+            \"negative_prompt\": \"negative\",  # TODO\n+            \"height\": image_height,\n+            \"width\": image_width,\n+            \"generator\": generator,\n+            \"num_inference_steps\": 2,\n+            \"guidance_scale\": 6.0,\n+            \"num_frames\": 5,\n+            \"max_sequence_length\": 16,\n+            \"output_type\": \"pt\",\n+        }\n+        return inputs\n+\n+    def test_inference(self):\n+        device = \"cpu\"\n+\n+        components = self.get_dummy_components()\n+        pipe = self.pipeline_class(**components)\n+        pipe.to(device)\n+        pipe.set_progress_bar_config(disable=None)\n+\n+        inputs = self.get_dummy_inputs(device)\n+        video = pipe(**inputs).frames\n+        generated_video = video[0]\n+        self.assertEqual(generated_video.shape, (5, 3, 16, 16))\n+\n+        # fmt: off\n+        expected_slice = torch.tensor([0.4525, 0.4520, 0.4485, 0.4534, 0.4523, 0.4522, 0.4529, 0.4528, 0.5022, 0.5064, 0.5011, 0.5061, 0.5028, 0.4979, 0.5117, 0.5192])\n+        # fmt: on\n+\n+        generated_slice = generated_video.flatten()\n+        generated_slice = torch.cat([generated_slice[:8], generated_slice[-8:]])\n+        self.assertTrue(torch.allclose(generated_slice, expected_slice, atol=1e-3))\n+\n+    @unittest.skip(\"Test not supported\")\n+    def test_attention_slicing_forward_pass(self):\n+        pass\n+\n+    @unittest.skip(\"TODO: revisit failing as it requires a very high threshold to pass\")\n+    def test_inference_batch_single_identical(self):\n+        pass\n+\n+    @unittest.skip(\n+        \"ChronoEditPipeline has to run in mixed precision. Save/Load the entire pipeline in FP16 will result in errors\"\n+    )\n+    def test_save_load_float16(self):\n+        pass"
        }
      ],
      "num_files": 15,
      "scraped_at": "2025-11-16T21:18:44.317439"
    },
    {
      "pr_number": 12585,
      "title": "[modular] add tests for qwen modular",
      "body": "# What does this PR do?\r\n\r\nSome things came up with the tests that needed fixing. But LMK if you would like me to revert them.\r\n\r\nI also fixed a bunch of small things related to tests as I was running them on both GPU and CPU.\r\n\r\nWill propagate the changes from #12579 once it's merged.",
      "html_url": "https://github.com/huggingface/diffusers/pull/12585",
      "created_at": "2025-11-04T04:37:03Z",
      "merged_at": "2025-11-12T12:07:42Z",
      "merge_commit_sha": "f5e5f348238e3ae30ef2ba49153e2c59e709401b",
      "base_ref": "main",
      "head_sha": "d36d22927ee111e82e13bb11e001f9160d8a948f",
      "user": "sayakpaul",
      "files": [
        {
          "filename": "src/diffusers/modular_pipelines/qwenimage/before_denoise.py",
          "status": "modified",
          "additions": 7,
          "deletions": 7,
          "changes": 14,
          "patch": "@@ -132,6 +132,7 @@ def expected_components(self) -> List[ComponentSpec]:\n     @property\n     def inputs(self) -> List[InputParam]:\n         return [\n+            InputParam(\"latents\"),\n             InputParam(name=\"height\"),\n             InputParam(name=\"width\"),\n             InputParam(name=\"num_images_per_prompt\", default=1),\n@@ -196,11 +197,11 @@ def __call__(self, components: QwenImageModularPipeline, state: PipelineState) -\n                 f\"You have passed a list of generators of length {len(block_state.generator)}, but requested an effective batch\"\n                 f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n             )\n-\n-        block_state.latents = randn_tensor(\n-            shape, generator=block_state.generator, device=device, dtype=block_state.dtype\n-        )\n-        block_state.latents = components.pachifier.pack_latents(block_state.latents)\n+        if block_state.latents is None:\n+            block_state.latents = randn_tensor(\n+                shape, generator=block_state.generator, device=device, dtype=block_state.dtype\n+            )\n+            block_state.latents = components.pachifier.pack_latents(block_state.latents)\n \n         self.set_block_state(state, block_state)\n         return components, state\n@@ -549,8 +550,7 @@ def __call__(self, components: QwenImageModularPipeline, state: PipelineState) -\n                     block_state.width // components.vae_scale_factor // 2,\n                 )\n             ]\n-            * block_state.batch_size\n-        ]\n+        ] * block_state.batch_size\n         block_state.txt_seq_lens = (\n             block_state.prompt_embeds_mask.sum(dim=1).tolist() if block_state.prompt_embeds_mask is not None else None\n         )"
        },
        {
          "filename": "src/diffusers/modular_pipelines/qwenimage/decoders.py",
          "status": "modified",
          "additions": 2,
          "deletions": 1,
          "changes": 3,
          "patch": "@@ -74,8 +74,9 @@ def __call__(self, components: QwenImageModularPipeline, state: PipelineState) -\n         block_state = self.get_block_state(state)\n \n         # YiYi Notes: remove support for output_type = \"latents', we can just skip decode/encode step in modular\n+        vae_scale_factor = components.vae_scale_factor\n         block_state.latents = components.pachifier.unpack_latents(\n-            block_state.latents, block_state.height, block_state.width\n+            block_state.latents, block_state.height, block_state.width, vae_scale_factor=vae_scale_factor\n         )\n         block_state.latents = block_state.latents.to(components.vae.dtype)\n "
        },
        {
          "filename": "src/diffusers/modular_pipelines/qwenimage/encoders.py",
          "status": "modified",
          "additions": 6,
          "deletions": 0,
          "changes": 6,
          "patch": "@@ -503,6 +503,8 @@ def __call__(self, components: QwenImageModularPipeline, state: PipelineState):\n         block_state.prompt_embeds = block_state.prompt_embeds[:, : block_state.max_sequence_length]\n         block_state.prompt_embeds_mask = block_state.prompt_embeds_mask[:, : block_state.max_sequence_length]\n \n+        block_state.negative_prompt_embeds = None\n+        block_state.negative_prompt_embeds_mask = None\n         if components.requires_unconditional_embeds:\n             negative_prompt = block_state.negative_prompt or \"\"\n             block_state.negative_prompt_embeds, block_state.negative_prompt_embeds_mask = get_qwen_prompt_embeds(\n@@ -627,6 +629,8 @@ def __call__(self, components: QwenImageModularPipeline, state: PipelineState):\n             device=device,\n         )\n \n+        block_state.negative_prompt_embeds = None\n+        block_state.negative_prompt_embeds_mask = None\n         if components.requires_unconditional_embeds:\n             negative_prompt = block_state.negative_prompt or \" \"\n             block_state.negative_prompt_embeds, block_state.negative_prompt_embeds_mask = get_qwen_prompt_embeds_edit(\n@@ -679,6 +683,8 @@ def __call__(self, components: QwenImageModularPipeline, state: PipelineState):\n             device=device,\n         )\n \n+        block_state.negative_prompt_embeds = None\n+        block_state.negative_prompt_embeds_mask = None\n         if components.requires_unconditional_embeds:\n             negative_prompt = block_state.negative_prompt or \" \"\n             block_state.negative_prompt_embeds, block_state.negative_prompt_embeds_mask = ("
        },
        {
          "filename": "src/diffusers/modular_pipelines/qwenimage/modular_pipeline.py",
          "status": "modified",
          "additions": 1,
          "deletions": 4,
          "changes": 5,
          "patch": "@@ -26,10 +26,7 @@ class QwenImagePachifier(ConfigMixin):\n     config_name = \"config.json\"\n \n     @register_to_config\n-    def __init__(\n-        self,\n-        patch_size: int = 2,\n-    ):\n+    def __init__(self, patch_size: int = 2):\n         super().__init__()\n \n     def pack_latents(self, latents):"
        },
        {
          "filename": "tests/modular_pipelines/flux/test_modular_pipeline_flux.py",
          "status": "modified",
          "additions": 9,
          "deletions": 0,
          "changes": 9,
          "patch": "@@ -55,6 +55,9 @@ def get_dummy_inputs(self, seed=0):\n         }\n         return inputs\n \n+    def test_float16_inference(self):\n+        super().test_float16_inference(9e-2)\n+\n \n class TestFluxImg2ImgModularPipelineFast(ModularPipelineTesterMixin):\n     pipeline_class = FluxModularPipeline\n@@ -118,6 +121,9 @@ def test_save_from_pretrained(self):\n \n         assert torch.abs(image_slices[0] - image_slices[1]).max() < 1e-3\n \n+    def test_float16_inference(self):\n+        super().test_float16_inference(8e-2)\n+\n \n class TestFluxKontextModularPipelineFast(ModularPipelineTesterMixin):\n     pipeline_class = FluxKontextModularPipeline\n@@ -170,3 +176,6 @@ def test_save_from_pretrained(self):\n             image_slices.append(image[0, -3:, -3:, -1].flatten())\n \n         assert torch.abs(image_slices[0] - image_slices[1]).max() < 1e-3\n+\n+    def test_float16_inference(self):\n+        super().test_float16_inference(9e-2)"
        },
        {
          "filename": "tests/modular_pipelines/qwen/__init__.py",
          "status": "added",
          "additions": 0,
          "deletions": 0,
          "changes": 0,
          "patch": ""
        },
        {
          "filename": "tests/modular_pipelines/qwen/test_modular_pipeline_qwenimage.py",
          "status": "added",
          "additions": 120,
          "deletions": 0,
          "changes": 120,
          "patch": "@@ -0,0 +1,120 @@\n+# coding=utf-8\n+# Copyright 2025 HuggingFace Inc.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\n+import PIL\n+import pytest\n+\n+from diffusers.modular_pipelines import (\n+    QwenImageAutoBlocks,\n+    QwenImageEditAutoBlocks,\n+    QwenImageEditModularPipeline,\n+    QwenImageEditPlusAutoBlocks,\n+    QwenImageEditPlusModularPipeline,\n+    QwenImageModularPipeline,\n+)\n+\n+from ..test_modular_pipelines_common import ModularGuiderTesterMixin, ModularPipelineTesterMixin\n+\n+\n+class TestQwenImageModularPipelineFast(ModularPipelineTesterMixin, ModularGuiderTesterMixin):\n+    pipeline_class = QwenImageModularPipeline\n+    pipeline_blocks_class = QwenImageAutoBlocks\n+    repo = \"hf-internal-testing/tiny-qwenimage-modular\"\n+\n+    params = frozenset([\"prompt\", \"height\", \"width\", \"negative_prompt\", \"attention_kwargs\", \"image\", \"mask_image\"])\n+    batch_params = frozenset([\"prompt\", \"negative_prompt\", \"image\", \"mask_image\"])\n+\n+    def get_dummy_inputs(self):\n+        generator = self.get_generator()\n+        inputs = {\n+            \"prompt\": \"dance monkey\",\n+            \"negative_prompt\": \"bad quality\",\n+            \"generator\": generator,\n+            \"num_inference_steps\": 2,\n+            \"height\": 32,\n+            \"width\": 32,\n+            \"max_sequence_length\": 16,\n+            \"output_type\": \"pt\",\n+        }\n+        return inputs\n+\n+    def test_inference_batch_single_identical(self):\n+        super().test_inference_batch_single_identical(expected_max_diff=5e-4)\n+\n+\n+class TestQwenImageEditModularPipelineFast(ModularPipelineTesterMixin, ModularGuiderTesterMixin):\n+    pipeline_class = QwenImageEditModularPipeline\n+    pipeline_blocks_class = QwenImageEditAutoBlocks\n+    repo = \"hf-internal-testing/tiny-qwenimage-edit-modular\"\n+\n+    params = frozenset([\"prompt\", \"height\", \"width\", \"negative_prompt\", \"attention_kwargs\", \"image\", \"mask_image\"])\n+    batch_params = frozenset([\"prompt\", \"negative_prompt\", \"image\", \"mask_image\"])\n+\n+    def get_dummy_inputs(self):\n+        generator = self.get_generator()\n+        inputs = {\n+            \"prompt\": \"dance monkey\",\n+            \"negative_prompt\": \"bad quality\",\n+            \"generator\": generator,\n+            \"num_inference_steps\": 2,\n+            \"height\": 32,\n+            \"width\": 32,\n+            \"output_type\": \"pt\",\n+        }\n+        inputs[\"image\"] = PIL.Image.new(\"RGB\", (32, 32), 0)\n+        return inputs\n+\n+    def test_guider_cfg(self):\n+        super().test_guider_cfg(7e-5)\n+\n+\n+class TestQwenImageEditPlusModularPipelineFast(ModularPipelineTesterMixin, ModularGuiderTesterMixin):\n+    pipeline_class = QwenImageEditPlusModularPipeline\n+    pipeline_blocks_class = QwenImageEditPlusAutoBlocks\n+    repo = \"hf-internal-testing/tiny-qwenimage-edit-plus-modular\"\n+\n+    # No `mask_image` yet.\n+    params = frozenset([\"prompt\", \"height\", \"width\", \"negative_prompt\", \"attention_kwargs\", \"image\"])\n+    batch_params = frozenset([\"prompt\", \"negative_prompt\", \"image\"])\n+\n+    def get_dummy_inputs(self):\n+        generator = self.get_generator()\n+        inputs = {\n+            \"prompt\": \"dance monkey\",\n+            \"negative_prompt\": \"bad quality\",\n+            \"generator\": generator,\n+            \"num_inference_steps\": 2,\n+            \"height\": 32,\n+            \"width\": 32,\n+            \"output_type\": \"pt\",\n+        }\n+        inputs[\"image\"] = PIL.Image.new(\"RGB\", (32, 32), 0)\n+        return inputs\n+\n+    @pytest.mark.xfail(condition=True, reason=\"Batch of multiple images needs to be revisited\", strict=True)\n+    def test_num_images_per_prompt(self):\n+        super().test_num_images_per_prompt()\n+\n+    @pytest.mark.xfail(condition=True, reason=\"Batch of multiple images needs to be revisited\", strict=True)\n+    def test_inference_batch_consistent():\n+        super().test_inference_batch_consistent()\n+\n+    @pytest.mark.xfail(condition=True, reason=\"Batch of multiple images needs to be revisited\", strict=True)\n+    def test_inference_batch_single_identical():\n+        super().test_inference_batch_single_identical()\n+\n+    def test_guider_cfg(self):\n+        super().test_guider_cfg(1e-3)"
        },
        {
          "filename": "tests/modular_pipelines/stable_diffusion_xl/test_modular_pipeline_stable_diffusion_xl.py",
          "status": "modified",
          "additions": 11,
          "deletions": 65,
          "changes": 76,
          "patch": "@@ -25,7 +25,7 @@\n \n from ...models.unets.test_models_unet_2d_condition import create_ip_adapter_state_dict\n from ...testing_utils import enable_full_determinism, floats_tensor, torch_device\n-from ..test_modular_pipelines_common import ModularPipelineTesterMixin\n+from ..test_modular_pipelines_common import ModularGuiderTesterMixin, ModularPipelineTesterMixin\n \n \n enable_full_determinism()\n@@ -37,13 +37,11 @@ class SDXLModularTesterMixin:\n     \"\"\"\n \n     def _test_stable_diffusion_xl_euler(self, expected_image_shape, expected_slice, expected_max_diff=1e-2):\n-        sd_pipe = self.get_pipeline()\n-        sd_pipe = sd_pipe.to(torch_device)\n-        sd_pipe.set_progress_bar_config(disable=None)\n+        sd_pipe = self.get_pipeline().to(torch_device)\n \n         inputs = self.get_dummy_inputs()\n         image = sd_pipe(**inputs, output=\"images\")\n-        image_slice = image[0, -3:, -3:, -1]\n+        image_slice = image[0, -3:, -3:, -1].cpu()\n \n         assert image.shape == expected_image_shape\n         max_diff = torch.abs(image_slice.flatten() - expected_slice).max()\n@@ -110,7 +108,7 @@ def test_ip_adapter(self, expected_max_diff: float = 1e-4, expected_pipe_slice=N\n         pipe = blocks.init_pipeline(self.repo)\n         pipe.load_components(torch_dtype=torch.float32)\n         pipe = pipe.to(torch_device)\n-        pipe.set_progress_bar_config(disable=None)\n+\n         cross_attention_dim = pipe.unet.config.get(\"cross_attention_dim\")\n \n         # forward pass without ip adapter\n@@ -219,9 +217,7 @@ def test_controlnet(self, expected_max_diff: float = 1e-4, expected_pipe_slice=N\n         # compare against static slices and that can be shaky (with a VVVV low probability).\n         expected_max_diff = 9e-4 if torch_device == \"cpu\" else expected_max_diff\n \n-        pipe = self.get_pipeline()\n-        pipe = pipe.to(torch_device)\n-        pipe.set_progress_bar_config(disable=None)\n+        pipe = self.get_pipeline().to(torch_device)\n \n         # forward pass without controlnet\n         inputs = self.get_dummy_inputs()\n@@ -251,9 +247,7 @@ def test_controlnet(self, expected_max_diff: float = 1e-4, expected_pipe_slice=N\n         assert max_diff_with_controlnet_scale > 1e-2, \"Output with controlnet must be different from normal inference\"\n \n     def test_controlnet_cfg(self):\n-        pipe = self.get_pipeline()\n-        pipe = pipe.to(torch_device)\n-        pipe.set_progress_bar_config(disable=None)\n+        pipe = self.get_pipeline().to(torch_device)\n \n         # forward pass with CFG not applied\n         guider = ClassifierFreeGuidance(guidance_scale=1.0)\n@@ -273,35 +267,11 @@ def test_controlnet_cfg(self):\n         assert max_diff > 1e-2, \"Output with CFG must be different from normal inference\"\n \n \n-class SDXLModularGuiderTesterMixin:\n-    def test_guider_cfg(self):\n-        pipe = self.get_pipeline()\n-        pipe = pipe.to(torch_device)\n-        pipe.set_progress_bar_config(disable=None)\n-\n-        # forward pass with CFG not applied\n-        guider = ClassifierFreeGuidance(guidance_scale=1.0)\n-        pipe.update_components(guider=guider)\n-\n-        inputs = self.get_dummy_inputs()\n-        out_no_cfg = pipe(**inputs, output=\"images\")\n-\n-        # forward pass with CFG applied\n-        guider = ClassifierFreeGuidance(guidance_scale=7.5)\n-        pipe.update_components(guider=guider)\n-        inputs = self.get_dummy_inputs()\n-        out_cfg = pipe(**inputs, output=\"images\")\n-\n-        assert out_cfg.shape == out_no_cfg.shape\n-        max_diff = np.abs(out_cfg - out_no_cfg).max()\n-        assert max_diff > 1e-2, \"Output with CFG must be different from normal inference\"\n-\n-\n class TestSDXLModularPipelineFast(\n     SDXLModularTesterMixin,\n     SDXLModularIPAdapterTesterMixin,\n     SDXLModularControlNetTesterMixin,\n-    SDXLModularGuiderTesterMixin,\n+    ModularGuiderTesterMixin,\n     ModularPipelineTesterMixin,\n ):\n     \"\"\"Test cases for Stable Diffusion XL modular pipeline fast tests.\"\"\"\n@@ -335,18 +305,7 @@ def test_stable_diffusion_xl_euler(self):\n         self._test_stable_diffusion_xl_euler(\n             expected_image_shape=self.expected_image_output_shape,\n             expected_slice=torch.tensor(\n-                [\n-                    0.5966781,\n-                    0.62939394,\n-                    0.48465094,\n-                    0.51573336,\n-                    0.57593524,\n-                    0.47035995,\n-                    0.53410417,\n-                    0.51436996,\n-                    0.47313565,\n-                ],\n-                device=torch_device,\n+                [0.3886, 0.4685, 0.4953, 0.4217, 0.4317, 0.3945, 0.4847, 0.4704, 0.4731],\n             ),\n             expected_max_diff=1e-2,\n         )\n@@ -359,7 +318,7 @@ class TestSDXLImg2ImgModularPipelineFast(\n     SDXLModularTesterMixin,\n     SDXLModularIPAdapterTesterMixin,\n     SDXLModularControlNetTesterMixin,\n-    SDXLModularGuiderTesterMixin,\n+    ModularGuiderTesterMixin,\n     ModularPipelineTesterMixin,\n ):\n     \"\"\"Test cases for Stable Diffusion XL image-to-image modular pipeline fast tests.\"\"\"\n@@ -400,20 +359,7 @@ def get_dummy_inputs(self, seed=0):\n     def test_stable_diffusion_xl_euler(self):\n         self._test_stable_diffusion_xl_euler(\n             expected_image_shape=self.expected_image_output_shape,\n-            expected_slice=torch.tensor(\n-                [\n-                    0.56943184,\n-                    0.4702148,\n-                    0.48048905,\n-                    0.6235963,\n-                    0.551138,\n-                    0.49629188,\n-                    0.60031277,\n-                    0.5688907,\n-                    0.43996853,\n-                ],\n-                device=torch_device,\n-            ),\n+            expected_slice=torch.tensor([0.5246, 0.4466, 0.444, 0.3246, 0.4443, 0.5108, 0.5225, 0.559, 0.5147]),\n             expected_max_diff=1e-2,\n         )\n \n@@ -425,7 +371,7 @@ class SDXLInpaintingModularPipelineFastTests(\n     SDXLModularTesterMixin,\n     SDXLModularIPAdapterTesterMixin,\n     SDXLModularControlNetTesterMixin,\n-    SDXLModularGuiderTesterMixin,\n+    ModularGuiderTesterMixin,\n     ModularPipelineTesterMixin,\n ):\n     \"\"\"Test cases for Stable Diffusion XL inpainting modular pipeline fast tests.\"\"\""
        },
        {
          "filename": "tests/modular_pipelines/test_modular_pipelines_common.py",
          "status": "modified",
          "additions": 38,
          "deletions": 46,
          "changes": 84,
          "patch": "@@ -2,22 +2,17 @@\n import tempfile\n from typing import Callable, Union\n \n+import pytest\n import torch\n \n import diffusers\n from diffusers import ComponentsManager, ModularPipeline, ModularPipelineBlocks\n+from diffusers.guiders import ClassifierFreeGuidance\n from diffusers.utils import logging\n \n-from ..testing_utils import (\n-    backend_empty_cache,\n-    numpy_cosine_similarity_distance,\n-    require_accelerator,\n-    require_torch,\n-    torch_device,\n-)\n+from ..testing_utils import backend_empty_cache, numpy_cosine_similarity_distance, require_accelerator, torch_device\n \n \n-@require_torch\n class ModularPipelineTesterMixin:\n     \"\"\"\n     It provides a set of common tests for each modular pipeline,\n@@ -32,20 +27,9 @@ class ModularPipelineTesterMixin:\n     # Canonical parameters that are passed to `__call__` regardless\n     # of the type of pipeline. They are always optional and have common\n     # sense default values.\n-    optional_params = frozenset(\n-        [\n-            \"num_inference_steps\",\n-            \"num_images_per_prompt\",\n-            \"latents\",\n-            \"output_type\",\n-        ]\n-    )\n+    optional_params = frozenset([\"num_inference_steps\", \"num_images_per_prompt\", \"latents\", \"output_type\"])\n     # this is modular specific: generator needs to be a intermediate input because it's mutable\n-    intermediate_params = frozenset(\n-        [\n-            \"generator\",\n-        ]\n-    )\n+    intermediate_params = frozenset([\"generator\"])\n \n     def get_generator(self, seed=0):\n         generator = torch.Generator(\"cpu\").manual_seed(seed)\n@@ -121,6 +105,7 @@ def teardown_method(self):\n     def get_pipeline(self, components_manager=None, torch_dtype=torch.float32):\n         pipeline = self.pipeline_blocks_class().init_pipeline(self.repo, components_manager=components_manager)\n         pipeline.load_components(torch_dtype=torch_dtype)\n+        pipeline.set_progress_bar_config(disable=None)\n         return pipeline\n \n     def test_pipeline_call_signature(self):\n@@ -138,9 +123,7 @@ def _check_for_parameters(parameters, expected_parameters, param_type):\n         _check_for_parameters(self.optional_params, optional_parameters, \"optional\")\n \n     def test_inference_batch_consistent(self, batch_sizes=[2], batch_generator=True):\n-        pipe = self.get_pipeline()\n-        pipe.to(torch_device)\n-        pipe.set_progress_bar_config(disable=None)\n+        pipe = self.get_pipeline().to(torch_device)\n \n         inputs = self.get_dummy_inputs()\n         inputs[\"generator\"] = self.get_generator(0)\n@@ -179,9 +162,8 @@ def test_inference_batch_single_identical(\n         batch_size=2,\n         expected_max_diff=1e-4,\n     ):\n-        pipe = self.get_pipeline()\n-        pipe.to(torch_device)\n-        pipe.set_progress_bar_config(disable=None)\n+        pipe = self.get_pipeline().to(torch_device)\n+\n         inputs = self.get_dummy_inputs()\n \n         # Reset generator in case it is has been used in self.get_dummy_inputs\n@@ -219,11 +201,9 @@ def test_inference_batch_single_identical(\n     def test_float16_inference(self, expected_max_diff=5e-2):\n         pipe = self.get_pipeline()\n         pipe.to(torch_device, torch.float32)\n-        pipe.set_progress_bar_config(disable=None)\n \n         pipe_fp16 = self.get_pipeline()\n         pipe_fp16.to(torch_device, torch.float16)\n-        pipe_fp16.set_progress_bar_config(disable=None)\n \n         inputs = self.get_dummy_inputs()\n         # Reset generator in case it is used inside dummy inputs\n@@ -237,19 +217,16 @@ def test_float16_inference(self, expected_max_diff=5e-2):\n             fp16_inputs[\"generator\"] = self.get_generator(0)\n         output_fp16 = pipe_fp16(**fp16_inputs, output=\"images\")\n \n-        if isinstance(output, torch.Tensor):\n-            output = output.cpu()\n-            output_fp16 = output_fp16.cpu()\n+        output = output.cpu()\n+        output_fp16 = output_fp16.cpu()\n \n         max_diff = numpy_cosine_similarity_distance(output.flatten(), output_fp16.flatten())\n         assert max_diff < expected_max_diff, \"FP16 inference is different from FP32 inference\"\n \n     @require_accelerator\n     def test_to_device(self):\n-        pipe = self.get_pipeline()\n-        pipe.set_progress_bar_config(disable=None)\n+        pipe = self.get_pipeline().to(\"cpu\")\n \n-        pipe.to(\"cpu\")\n         model_devices = [\n             component.device.type for component in pipe.components.values() if hasattr(component, \"device\")\n         ]\n@@ -264,30 +241,23 @@ def test_to_device(self):\n         )\n \n     def test_inference_is_not_nan_cpu(self):\n-        pipe = self.get_pipeline()\n-        pipe.set_progress_bar_config(disable=None)\n-        pipe.to(\"cpu\")\n+        pipe = self.get_pipeline().to(\"cpu\")\n \n         output = pipe(**self.get_dummy_inputs(), output=\"images\")\n         assert torch.isnan(output).sum() == 0, \"CPU Inference returns NaN\"\n \n     @require_accelerator\n     def test_inference_is_not_nan(self):\n-        pipe = self.get_pipeline()\n-        pipe.set_progress_bar_config(disable=None)\n-        pipe.to(torch_device)\n+        pipe = self.get_pipeline().to(torch_device)\n \n         output = pipe(**self.get_dummy_inputs(), output=\"images\")\n         assert torch.isnan(output).sum() == 0, \"Accelerator Inference returns NaN\"\n \n     def test_num_images_per_prompt(self):\n-        pipe = self.get_pipeline()\n+        pipe = self.get_pipeline().to(torch_device)\n \n         if \"num_images_per_prompt\" not in pipe.blocks.input_names:\n-            return\n-\n-        pipe = pipe.to(torch_device)\n-        pipe.set_progress_bar_config(disable=None)\n+            pytest.mark.skip(\"Skipping test as `num_images_per_prompt` is not present in input names.\")\n \n         batch_sizes = [1, 2]\n         num_images_per_prompts = [1, 2]\n@@ -342,3 +312,25 @@ def test_save_from_pretrained(self):\n             image_slices.append(image[0, -3:, -3:, -1].flatten())\n \n         assert torch.abs(image_slices[0] - image_slices[1]).max() < 1e-3\n+\n+\n+class ModularGuiderTesterMixin:\n+    def test_guider_cfg(self, expected_max_diff=1e-2):\n+        pipe = self.get_pipeline().to(torch_device)\n+\n+        # forward pass with CFG not applied\n+        guider = ClassifierFreeGuidance(guidance_scale=1.0)\n+        pipe.update_components(guider=guider)\n+\n+        inputs = self.get_dummy_inputs()\n+        out_no_cfg = pipe(**inputs, output=\"images\")\n+\n+        # forward pass with CFG applied\n+        guider = ClassifierFreeGuidance(guidance_scale=7.5)\n+        pipe.update_components(guider=guider)\n+        inputs = self.get_dummy_inputs()\n+        out_cfg = pipe(**inputs, output=\"images\")\n+\n+        assert out_cfg.shape == out_no_cfg.shape\n+        max_diff = torch.abs(out_cfg - out_no_cfg).max()\n+        assert max_diff > expected_max_diff, \"Output with CFG must be different from normal inference\""
        }
      ],
      "num_files": 9,
      "scraped_at": "2025-11-16T21:18:45.466999"
    },
    {
      "pr_number": 12584,
      "title": "[SANA-Video] Adding 5s pre-trained 480p SANA-Video inference",
      "body": "# What does this PR do?\r\nThis PR add SANA-Video, a new text/image-to-video model from NVIDIA\r\n[Paper](https://arxiv.org/abs/2509.24695)\r\n[Project](https://nvlabs.github.io/Sana/Video)\r\n[HF weight](https://huggingface.co/Efficient-Large-Model/SANA-Video_2B_480p_diffusers)\r\n\r\nCc: @yiyixuxu @asomoza @sayakpaul \r\n```python\r\nimport torch\r\nfrom diffusers import SanaPipeline, SanaVideoPipeline, UniPCMultistepScheduler, DPMSolverMultistepScheduler\r\nfrom diffusers import AutoencoderKLWan\r\nfrom diffusers.utils import export_to_video\r\n\r\n\r\nmodel_id = \"Efficient-Large-Model/SANA-Video_2B_480p_diffusers\"\r\npipe = SanaVideoPipeline.from_pretrained(model_id, torch_dtype=torch.bfloat16)\r\n# pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config, flow_shift=8.0)\r\n# pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config, flow_shift=8.0)\r\npipe.vae.to(torch.float32)\r\npipe.text_encoder.to(torch.bfloat16)\r\npipe.to(\"cuda\")\r\nmodel_score = 30\r\n\r\nprompt = \"Evening, backlight, side lighting, soft light, high contrast, mid-shot, centered composition, clean solo shot, warm color. A young Caucasian man stands in a forest, golden light glimmers on his hair as sunlight filters through the leaves. He wears a light shirt, wind gently blowing his hair and collar, light dances across his face with his movements. The background is blurred, with dappled light and soft tree shadows in the distance. The camera focuses on his lifted gaze, clear and emotional.\"\r\nnegative_prompt = \"A chaotic sequence with misshapen, deformed limbs in heavy motion blur, sudden disappearance, jump cuts, jerky movements, rapid shot changes, frames out of sync, inconsistent character shapes, temporal artifacts, jitter, and ghosting effects, creating a disorienting visual experience.\"\r\nmotion_prompt = f\" motion score: {model_score}.\"\r\nprompt = prompt + motion_prompt\r\n\r\nvideo = pipe(\r\n    prompt=prompt,\r\n    negative_prompt=negative_prompt,\r\n    height=480,\r\n    width=832,\r\n    frames=81,\r\n    guidance_scale=6,\r\n    num_inference_steps=50,\r\n    generator=torch.Generator(device=\"cuda\").manual_seed(42),\r\n).frames[0]\r\n\r\nexport_to_video(video, \"sana_video.mp4\", fps=16)\r\n```\r\n\r\nResults:\r\n\r\nhttps://github.com/user-attachments/assets/bf914906-b974-4bea-a6ae-cf9914f49a68\r\n",
      "html_url": "https://github.com/huggingface/diffusers/pull/12584",
      "created_at": "2025-11-04T03:09:51Z",
      "merged_at": "2025-11-06T05:08:47Z",
      "merge_commit_sha": "b3e9dfced7c9e8d00f646c710766b532383f04c6",
      "base_ref": "main",
      "head_sha": "f3c87f48b6c74b4331667a4a5e9fc54611388773",
      "user": "lawrence-cj",
      "files": [
        {
          "filename": "docs/source/en/_toctree.yml",
          "status": "modified",
          "additions": 4,
          "deletions": 0,
          "changes": 4,
          "patch": "@@ -373,6 +373,8 @@\n         title: QwenImageTransformer2DModel\n       - local: api/models/sana_transformer2d\n         title: SanaTransformer2DModel\n+      - local: api/models/sana_video_transformer3d\n+        title: SanaVideoTransformer3DModel\n       - local: api/models/sd3_transformer2d\n         title: SD3Transformer2DModel\n       - local: api/models/skyreels_v2_transformer_3d\n@@ -563,6 +565,8 @@\n         title: Sana\n       - local: api/pipelines/sana_sprint\n         title: Sana Sprint\n+      - local: api/pipelines/sana_video\n+        title: Sana Video\n       - local: api/pipelines/self_attention_guidance\n         title: Self-Attention Guidance\n       - local: api/pipelines/semantic_stable_diffusion"
        },
        {
          "filename": "docs/source/en/api/models/sana_video_transformer3d.md",
          "status": "added",
          "additions": 36,
          "deletions": 0,
          "changes": 36,
          "patch": "@@ -0,0 +1,36 @@\n+<!-- Copyright 2025 The SANA-Video Authors and HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License. -->\n+\n+# SanaVideoTransformer3DModel\n+\n+A Diffusion Transformer model for 3D data (video) from [SANA-Video: Efficient Video Generation with Block Linear Diffusion Transformer](https://huggingface.co/papers/2509.24695) from NVIDIA and MIT HAN Lab, by Junsong Chen, Yuyang Zhao, Jincheng Yu, Ruihang Chu, Junyu Chen, Shuai Yang, Xianbang Wang, Yicheng Pan, Daquan Zhou, Huan Ling, Haozhe Liu, Hongwei Yi, Hao Zhang, Muyang Li, Yukang Chen, Han Cai, Sanja Fidler, Ping Luo, Song Han, Enze Xie.\n+\n+The abstract from the paper is:\n+\n+*We introduce SANA-Video, a small diffusion model that can efficiently generate videos up to 720x1280 resolution and minute-length duration. SANA-Video synthesizes high-resolution, high-quality and long videos with strong text-video alignment at a remarkably fast speed, deployable on RTX 5090 GPU. Two core designs ensure our efficient, effective and long video generation: (1) Linear DiT: We leverage linear attention as the core operation, which is more efficient than vanilla attention given the large number of tokens processed in video generation. (2) Constant-Memory KV cache for Block Linear Attention: we design block-wise autoregressive approach for long video generation by employing a constant-memory state, derived from the cumulative properties of linear attention. This KV cache provides the Linear DiT with global context at a fixed memory cost, eliminating the need for a traditional KV cache and enabling efficient, minute-long video generation. In addition, we explore effective data filters and model training strategies, narrowing the training cost to 12 days on 64 H100 GPUs, which is only 1% of the cost of MovieGen. Given its low cost, SANA-Video achieves competitive performance compared to modern state-of-the-art small diffusion models (e.g., Wan 2.1-1.3B and SkyReel-V2-1.3B) while being 16x faster in measured latency. Moreover, SANA-Video can be deployed on RTX 5090 GPUs with NVFP4 precision, accelerating the inference speed of generating a 5-second 720p video from 71s to 29s (2.4x speedup). In summary, SANA-Video enables low-cost, high-quality video generation.*\n+\n+The model can be loaded with the following code snippet.\n+\n+```python\n+from diffusers import SanaVideoTransformer3DModel\n+import torch\n+\n+transformer = SanaVideoTransformer3DModel.from_pretrained(\"Efficient-Large-Model/SANA-Video_2B_480p_diffusers\", subfolder=\"transformer\", torch_dtype=torch.bfloat16)\n+```\n+\n+## SanaVideoTransformer3DModel\n+\n+[[autodoc]] SanaVideoTransformer3DModel\n+\n+## Transformer2DModelOutput\n+\n+[[autodoc]] models.modeling_outputs.Transformer2DModelOutput\n+"
        },
        {
          "filename": "docs/source/en/api/pipelines/sana_sprint.md",
          "status": "modified",
          "additions": 0,
          "deletions": 3,
          "changes": 3,
          "patch": "@@ -24,9 +24,6 @@ The abstract from the paper is:\n \n *This paper presents SANA-Sprint, an efficient diffusion model for ultra-fast text-to-image (T2I) generation. SANA-Sprint is built on a pre-trained foundation model and augmented with hybrid distillation, dramatically reducing inference steps from 20 to 1-4. We introduce three key innovations: (1) We propose a training-free approach that transforms a pre-trained flow-matching model for continuous-time consistency distillation (sCM), eliminating costly training from scratch and achieving high training efficiency. Our hybrid distillation strategy combines sCM with latent adversarial distillation (LADD): sCM ensures alignment with the teacher model, while LADD enhances single-step generation fidelity. (2) SANA-Sprint is a unified step-adaptive model that achieves high-quality generation in 1-4 steps, eliminating step-specific training and improving efficiency. (3) We integrate ControlNet with SANA-Sprint for real-time interactive image generation, enabling instant visual feedback for user interaction. SANA-Sprint establishes a new Pareto frontier in speed-quality tradeoffs, achieving state-of-the-art performance with 7.59 FID and 0.74 GenEval in only 1 step \u2014 outperforming FLUX-schnell (7.94 FID / 0.71 GenEval) while being 10\u00d7 faster (0.1s vs 1.1s on H100). It also achieves 0.1s (T2I) and 0.25s (ControlNet) latency for 1024\u00d71024 images on H100, and 0.31s (T2I) on an RTX 4090, showcasing its exceptional efficiency and potential for AI-powered consumer applications (AIPC). Code and pre-trained models will be open-sourced.*\n \n-> [!TIP]\n-> Make sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-a-pipeline) section to learn how to efficiently load the same components into multiple pipelines.\n-\n This pipeline was contributed by [lawrence-cj](https://github.com/lawrence-cj), [shuchen Xue](https://github.com/scxue) and [Enze Xie](https://github.com/xieenze). The original codebase can be found [here](https://github.com/NVlabs/Sana). The original weights can be found under [hf.co/Efficient-Large-Model](https://huggingface.co/Efficient-Large-Model/).\n \n Available models:"
        },
        {
          "filename": "docs/source/en/api/pipelines/sana_video.md",
          "status": "added",
          "additions": 102,
          "deletions": 0,
          "changes": 102,
          "patch": "@@ -0,0 +1,102 @@\n+<!-- Copyright 2025 The SANA-Video Authors and HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License. -->\n+\n+# SanaVideoPipeline\n+\n+<div class=\"flex flex-wrap space-x-1\">\n+  <img alt=\"LoRA\" src=\"https://img.shields.io/badge/LoRA-d8b4fe?style=flat\"/>\n+  <img alt=\"MPS\" src=\"https://img.shields.io/badge/MPS-000000?style=flat&logo=apple&logoColor=white%22\">\n+</div>\n+\n+[SANA-Video: Efficient Video Generation with Block Linear Diffusion Transformer](https://huggingface.co/papers/2509.24695) from NVIDIA and MIT HAN Lab, by Junsong Chen, Yuyang Zhao, Jincheng Yu, Ruihang Chu, Junyu Chen, Shuai Yang, Xianbang Wang, Yicheng Pan, Daquan Zhou, Huan Ling, Haozhe Liu, Hongwei Yi, Hao Zhang, Muyang Li, Yukang Chen, Han Cai, Sanja Fidler, Ping Luo, Song Han, Enze Xie.\n+\n+The abstract from the paper is:\n+\n+*We introduce SANA-Video, a small diffusion model that can efficiently generate videos up to 720x1280 resolution and minute-length duration. SANA-Video synthesizes high-resolution, high-quality and long videos with strong text-video alignment at a remarkably fast speed, deployable on RTX 5090 GPU. Two core designs ensure our efficient, effective and long video generation: (1) Linear DiT: We leverage linear attention as the core operation, which is more efficient than vanilla attention given the large number of tokens processed in video generation. (2) Constant-Memory KV cache for Block Linear Attention: we design block-wise autoregressive approach for long video generation by employing a constant-memory state, derived from the cumulative properties of linear attention. This KV cache provides the Linear DiT with global context at a fixed memory cost, eliminating the need for a traditional KV cache and enabling efficient, minute-long video generation. In addition, we explore effective data filters and model training strategies, narrowing the training cost to 12 days on 64 H100 GPUs, which is only 1% of the cost of MovieGen. Given its low cost, SANA-Video achieves competitive performance compared to modern state-of-the-art small diffusion models (e.g., Wan 2.1-1.3B and SkyReel-V2-1.3B) while being 16x faster in measured latency. Moreover, SANA-Video can be deployed on RTX 5090 GPUs with NVFP4 precision, accelerating the inference speed of generating a 5-second 720p video from 71s to 29s (2.4x speedup). In summary, SANA-Video enables low-cost, high-quality video generation. [this https URL](https://github.com/NVlabs/SANA).*\n+\n+This pipeline was contributed by SANA Team. The original codebase can be found [here](https://github.com/NVlabs/Sana). The original weights can be found under [hf.co/Efficient-Large-Model](https://hf.co/collections/Efficient-Large-Model/sana-video).\n+\n+Available models:\n+\n+| Model | Recommended dtype |\n+|:-----:|:-----------------:|\n+| [`Efficient-Large-Model/SANA-Video_2B_480p_diffusers`](https://huggingface.co/Efficient-Large-Model/ANA-Video_2B_480p_diffusers) | `torch.bfloat16` |\n+\n+Refer to [this](https://huggingface.co/collections/Efficient-Large-Model/sana-video) collection for more information.\n+\n+Note: The recommended dtype mentioned is for the transformer weights. The text encoder and VAE weights must stay in `torch.bfloat16` or `torch.float32` for the model to work correctly. Please refer to the inference example below to see how to load the model with the recommended dtype. \n+\n+## Quantization\n+\n+Quantization helps reduce the memory requirements of very large models by storing model weights in a lower precision data type. However, quantization may have varying impact on video quality depending on the video model.\n+\n+Refer to the [Quantization](../../quantization/overview) overview to learn more about supported quantization backends and selecting a quantization backend that supports your use case. The example below demonstrates how to load a quantized [`SanaVideoPipeline`] for inference with bitsandbytes.\n+\n+```py\n+import torch\n+from diffusers import BitsAndBytesConfig as DiffusersBitsAndBytesConfig, SanaVideoTransformer3DModel, SanaVideoPipeline\n+from transformers import BitsAndBytesConfig as BitsAndBytesConfig, AutoModel\n+\n+quant_config = BitsAndBytesConfig(load_in_8bit=True)\n+text_encoder_8bit = AutoModel.from_pretrained(\n+    \"Efficient-Large-Model/SANA-Video_2B_480p_diffusers\",\n+    subfolder=\"text_encoder\",\n+    quantization_config=quant_config,\n+    torch_dtype=torch.float16,\n+)\n+\n+quant_config = DiffusersBitsAndBytesConfig(load_in_8bit=True)\n+transformer_8bit = SanaVideoTransformer3DModel.from_pretrained(\n+    \"Efficient-Large-Model/SANA-Video_2B_480p_diffusers\",\n+    subfolder=\"transformer\",\n+    quantization_config=quant_config,\n+    torch_dtype=torch.float16,\n+)\n+\n+pipeline = SanaVideoPipeline.from_pretrained(\n+    \"Efficient-Large-Model/SANA-Video_2B_480p_diffusers\",\n+    text_encoder=text_encoder_8bit,\n+    transformer=transformer_8bit,\n+    torch_dtype=torch.float16,\n+    device_map=\"balanced\",\n+)\n+\n+model_score = 30\n+prompt = \"Evening, backlight, side lighting, soft light, high contrast, mid-shot, centered composition, clean solo shot, warm color. A young Caucasian man stands in a forest, golden light glimmers on his hair as sunlight filters through the leaves. He wears a light shirt, wind gently blowing his hair and collar, light dances across his face with his movements. The background is blurred, with dappled light and soft tree shadows in the distance. The camera focuses on his lifted gaze, clear and emotional.\"\n+negative_prompt = \"A chaotic sequence with misshapen, deformed limbs in heavy motion blur, sudden disappearance, jump cuts, jerky movements, rapid shot changes, frames out of sync, inconsistent character shapes, temporal artifacts, jitter, and ghosting effects, creating a disorienting visual experience.\"\n+motion_prompt = f\" motion score: {model_score}.\"\n+prompt = prompt + motion_prompt\n+\n+output = pipeline(\n+    prompt=prompt,\n+    negative_prompt=negative_prompt,\n+    height=480,\n+    width=832,\n+    num_frames=81,\n+    guidance_scale=6.0,\n+    num_inference_steps=50\n+).frames[0]\n+export_to_video(output, \"sana-video-output.mp4\", fps=16)\n+```\n+\n+## SanaVideoPipeline\n+\n+[[autodoc]] SanaVideoPipeline\n+  - all\n+  - __call__\n+\n+\n+## SanaVideoPipelineOutput\n+\n+[[autodoc]] pipelines.sana.pipeline_sana_video.SanaVideoPipelineOutput"
        },
        {
          "filename": "scripts/convert_sana_video_to_diffusers.py",
          "status": "added",
          "additions": 324,
          "deletions": 0,
          "changes": 324,
          "patch": "@@ -0,0 +1,324 @@\n+#!/usr/bin/env python\n+from __future__ import annotations\n+\n+import argparse\n+import os\n+from contextlib import nullcontext\n+\n+import torch\n+from accelerate import init_empty_weights\n+from huggingface_hub import hf_hub_download, snapshot_download\n+from termcolor import colored\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n+\n+from diffusers import (\n+    AutoencoderKLWan,\n+    DPMSolverMultistepScheduler,\n+    FlowMatchEulerDiscreteScheduler,\n+    SanaVideoPipeline,\n+    SanaVideoTransformer3DModel,\n+    UniPCMultistepScheduler,\n+)\n+from diffusers.utils.import_utils import is_accelerate_available\n+\n+\n+CTX = init_empty_weights if is_accelerate_available else nullcontext\n+\n+ckpt_ids = [\"Efficient-Large-Model/SANA-Video_2B_480p/checkpoints/SANA_Video_2B_480p.pth\"]\n+# https://github.com/NVlabs/Sana/blob/main/inference_video_scripts/inference_sana_video.py\n+\n+\n+def main(args):\n+    cache_dir_path = os.path.expanduser(\"~/.cache/huggingface/hub\")\n+\n+    if args.orig_ckpt_path is None or args.orig_ckpt_path in ckpt_ids:\n+        ckpt_id = args.orig_ckpt_path or ckpt_ids[0]\n+        snapshot_download(\n+            repo_id=f\"{'/'.join(ckpt_id.split('/')[:2])}\",\n+            cache_dir=cache_dir_path,\n+            repo_type=\"model\",\n+        )\n+        file_path = hf_hub_download(\n+            repo_id=f\"{'/'.join(ckpt_id.split('/')[:2])}\",\n+            filename=f\"{'/'.join(ckpt_id.split('/')[2:])}\",\n+            cache_dir=cache_dir_path,\n+            repo_type=\"model\",\n+        )\n+    else:\n+        file_path = args.orig_ckpt_path\n+\n+    print(colored(f\"Loading checkpoint from {file_path}\", \"green\", attrs=[\"bold\"]))\n+    all_state_dict = torch.load(file_path, weights_only=True)\n+    state_dict = all_state_dict.pop(\"state_dict\")\n+    converted_state_dict = {}\n+\n+    # Patch embeddings.\n+    converted_state_dict[\"patch_embedding.weight\"] = state_dict.pop(\"x_embedder.proj.weight\")\n+    converted_state_dict[\"patch_embedding.bias\"] = state_dict.pop(\"x_embedder.proj.bias\")\n+\n+    # Caption projection.\n+    converted_state_dict[\"caption_projection.linear_1.weight\"] = state_dict.pop(\"y_embedder.y_proj.fc1.weight\")\n+    converted_state_dict[\"caption_projection.linear_1.bias\"] = state_dict.pop(\"y_embedder.y_proj.fc1.bias\")\n+    converted_state_dict[\"caption_projection.linear_2.weight\"] = state_dict.pop(\"y_embedder.y_proj.fc2.weight\")\n+    converted_state_dict[\"caption_projection.linear_2.bias\"] = state_dict.pop(\"y_embedder.y_proj.fc2.bias\")\n+\n+    converted_state_dict[\"time_embed.emb.timestep_embedder.linear_1.weight\"] = state_dict.pop(\n+        \"t_embedder.mlp.0.weight\"\n+    )\n+    converted_state_dict[\"time_embed.emb.timestep_embedder.linear_1.bias\"] = state_dict.pop(\"t_embedder.mlp.0.bias\")\n+    converted_state_dict[\"time_embed.emb.timestep_embedder.linear_2.weight\"] = state_dict.pop(\n+        \"t_embedder.mlp.2.weight\"\n+    )\n+    converted_state_dict[\"time_embed.emb.timestep_embedder.linear_2.bias\"] = state_dict.pop(\"t_embedder.mlp.2.bias\")\n+\n+    # Shared norm.\n+    converted_state_dict[\"time_embed.linear.weight\"] = state_dict.pop(\"t_block.1.weight\")\n+    converted_state_dict[\"time_embed.linear.bias\"] = state_dict.pop(\"t_block.1.bias\")\n+\n+    # y norm\n+    converted_state_dict[\"caption_norm.weight\"] = state_dict.pop(\"attention_y_norm.weight\")\n+\n+    # scheduler\n+    flow_shift = 8.0\n+\n+    # model config\n+    layer_num = 20\n+    # Positional embedding interpolation scale.\n+    qk_norm = True\n+\n+    # sample size\n+    if args.video_size == 480:\n+        sample_size = 30  # Wan-VAE: 8xp2 downsample factor\n+        patch_size = (1, 2, 2)\n+    elif args.video_size == 720:\n+        sample_size = 22  # Wan-VAE: 32xp1 downsample factor\n+        patch_size = (1, 1, 1)\n+    else:\n+        raise ValueError(f\"Video size {args.video_size} is not supported.\")\n+\n+    for depth in range(layer_num):\n+        # Transformer blocks.\n+        converted_state_dict[f\"transformer_blocks.{depth}.scale_shift_table\"] = state_dict.pop(\n+            f\"blocks.{depth}.scale_shift_table\"\n+        )\n+\n+        # Linear Attention is all you need \ud83e\udd18\n+        # Self attention.\n+        q, k, v = torch.chunk(state_dict.pop(f\"blocks.{depth}.attn.qkv.weight\"), 3, dim=0)\n+        converted_state_dict[f\"transformer_blocks.{depth}.attn1.to_q.weight\"] = q\n+        converted_state_dict[f\"transformer_blocks.{depth}.attn1.to_k.weight\"] = k\n+        converted_state_dict[f\"transformer_blocks.{depth}.attn1.to_v.weight\"] = v\n+        if qk_norm is not None:\n+            # Add Q/K normalization for self-attention (attn1) - needed for Sana-Sprint and Sana-1.5\n+            converted_state_dict[f\"transformer_blocks.{depth}.attn1.norm_q.weight\"] = state_dict.pop(\n+                f\"blocks.{depth}.attn.q_norm.weight\"\n+            )\n+            converted_state_dict[f\"transformer_blocks.{depth}.attn1.norm_k.weight\"] = state_dict.pop(\n+                f\"blocks.{depth}.attn.k_norm.weight\"\n+            )\n+        # Projection.\n+        converted_state_dict[f\"transformer_blocks.{depth}.attn1.to_out.0.weight\"] = state_dict.pop(\n+            f\"blocks.{depth}.attn.proj.weight\"\n+        )\n+        converted_state_dict[f\"transformer_blocks.{depth}.attn1.to_out.0.bias\"] = state_dict.pop(\n+            f\"blocks.{depth}.attn.proj.bias\"\n+        )\n+\n+        # Feed-forward.\n+        converted_state_dict[f\"transformer_blocks.{depth}.ff.conv_inverted.weight\"] = state_dict.pop(\n+            f\"blocks.{depth}.mlp.inverted_conv.conv.weight\"\n+        )\n+        converted_state_dict[f\"transformer_blocks.{depth}.ff.conv_inverted.bias\"] = state_dict.pop(\n+            f\"blocks.{depth}.mlp.inverted_conv.conv.bias\"\n+        )\n+        converted_state_dict[f\"transformer_blocks.{depth}.ff.conv_depth.weight\"] = state_dict.pop(\n+            f\"blocks.{depth}.mlp.depth_conv.conv.weight\"\n+        )\n+        converted_state_dict[f\"transformer_blocks.{depth}.ff.conv_depth.bias\"] = state_dict.pop(\n+            f\"blocks.{depth}.mlp.depth_conv.conv.bias\"\n+        )\n+        converted_state_dict[f\"transformer_blocks.{depth}.ff.conv_point.weight\"] = state_dict.pop(\n+            f\"blocks.{depth}.mlp.point_conv.conv.weight\"\n+        )\n+        converted_state_dict[f\"transformer_blocks.{depth}.ff.conv_temp.weight\"] = state_dict.pop(\n+            f\"blocks.{depth}.mlp.t_conv.weight\"\n+        )\n+\n+        # Cross-attention.\n+        q = state_dict.pop(f\"blocks.{depth}.cross_attn.q_linear.weight\")\n+        q_bias = state_dict.pop(f\"blocks.{depth}.cross_attn.q_linear.bias\")\n+        k, v = torch.chunk(state_dict.pop(f\"blocks.{depth}.cross_attn.kv_linear.weight\"), 2, dim=0)\n+        k_bias, v_bias = torch.chunk(state_dict.pop(f\"blocks.{depth}.cross_attn.kv_linear.bias\"), 2, dim=0)\n+\n+        converted_state_dict[f\"transformer_blocks.{depth}.attn2.to_q.weight\"] = q\n+        converted_state_dict[f\"transformer_blocks.{depth}.attn2.to_q.bias\"] = q_bias\n+        converted_state_dict[f\"transformer_blocks.{depth}.attn2.to_k.weight\"] = k\n+        converted_state_dict[f\"transformer_blocks.{depth}.attn2.to_k.bias\"] = k_bias\n+        converted_state_dict[f\"transformer_blocks.{depth}.attn2.to_v.weight\"] = v\n+        converted_state_dict[f\"transformer_blocks.{depth}.attn2.to_v.bias\"] = v_bias\n+        if qk_norm is not None:\n+            # Add Q/K normalization for cross-attention (attn2) - needed for Sana-Sprint and Sana-1.5\n+            converted_state_dict[f\"transformer_blocks.{depth}.attn2.norm_q.weight\"] = state_dict.pop(\n+                f\"blocks.{depth}.cross_attn.q_norm.weight\"\n+            )\n+            converted_state_dict[f\"transformer_blocks.{depth}.attn2.norm_k.weight\"] = state_dict.pop(\n+                f\"blocks.{depth}.cross_attn.k_norm.weight\"\n+            )\n+\n+        converted_state_dict[f\"transformer_blocks.{depth}.attn2.to_out.0.weight\"] = state_dict.pop(\n+            f\"blocks.{depth}.cross_attn.proj.weight\"\n+        )\n+        converted_state_dict[f\"transformer_blocks.{depth}.attn2.to_out.0.bias\"] = state_dict.pop(\n+            f\"blocks.{depth}.cross_attn.proj.bias\"\n+        )\n+\n+    # Final block.\n+    converted_state_dict[\"proj_out.weight\"] = state_dict.pop(\"final_layer.linear.weight\")\n+    converted_state_dict[\"proj_out.bias\"] = state_dict.pop(\"final_layer.linear.bias\")\n+    converted_state_dict[\"scale_shift_table\"] = state_dict.pop(\"final_layer.scale_shift_table\")\n+\n+    # Transformer\n+    with CTX():\n+        transformer_kwargs = {\n+            \"in_channels\": 16,\n+            \"out_channels\": 16,\n+            \"num_attention_heads\": 20,\n+            \"attention_head_dim\": 112,\n+            \"num_layers\": 20,\n+            \"num_cross_attention_heads\": 20,\n+            \"cross_attention_head_dim\": 112,\n+            \"cross_attention_dim\": 2240,\n+            \"caption_channels\": 2304,\n+            \"mlp_ratio\": 3.0,\n+            \"attention_bias\": False,\n+            \"sample_size\": sample_size,\n+            \"patch_size\": patch_size,\n+            \"norm_elementwise_affine\": False,\n+            \"norm_eps\": 1e-6,\n+            \"qk_norm\": \"rms_norm_across_heads\",\n+            \"rope_max_seq_len\": 1024,\n+        }\n+\n+        transformer = SanaVideoTransformer3DModel(**transformer_kwargs)\n+\n+    transformer.load_state_dict(converted_state_dict, strict=True, assign=True)\n+\n+    try:\n+        state_dict.pop(\"y_embedder.y_embedding\")\n+        state_dict.pop(\"pos_embed\")\n+        state_dict.pop(\"logvar_linear.weight\")\n+        state_dict.pop(\"logvar_linear.bias\")\n+    except KeyError:\n+        print(\"y_embedder.y_embedding or pos_embed not found in the state_dict\")\n+\n+    assert len(state_dict) == 0, f\"State dict is not empty, {state_dict.keys()}\"\n+\n+    num_model_params = sum(p.numel() for p in transformer.parameters())\n+    print(f\"Total number of transformer parameters: {num_model_params}\")\n+\n+    transformer = transformer.to(weight_dtype)\n+\n+    if not args.save_full_pipeline:\n+        print(\n+            colored(\n+                f\"Only saving transformer model of {args.model_type}. \"\n+                f\"Set --save_full_pipeline to save the whole Pipeline\",\n+                \"green\",\n+                attrs=[\"bold\"],\n+            )\n+        )\n+        transformer.save_pretrained(\n+            os.path.join(args.dump_path, \"transformer\"), safe_serialization=True, max_shard_size=\"5GB\"\n+        )\n+    else:\n+        print(colored(f\"Saving the whole Pipeline containing {args.model_type}\", \"green\", attrs=[\"bold\"]))\n+        # VAE\n+        vae = AutoencoderKLWan.from_pretrained(\n+            \"Wan-AI/Wan2.1-T2V-1.3B-Diffusers\", subfolder=\"vae\", torch_dtype=torch.float32\n+        )\n+\n+        # Text Encoder\n+        text_encoder_model_path = \"Efficient-Large-Model/gemma-2-2b-it\"\n+        tokenizer = AutoTokenizer.from_pretrained(text_encoder_model_path)\n+        tokenizer.padding_side = \"right\"\n+        text_encoder = AutoModelForCausalLM.from_pretrained(\n+            text_encoder_model_path, torch_dtype=torch.bfloat16\n+        ).get_decoder()\n+\n+        # Choose the appropriate pipeline and scheduler based on model type\n+        # Original Sana scheduler\n+        if args.scheduler_type == \"flow-dpm_solver\":\n+            scheduler = DPMSolverMultistepScheduler(\n+                flow_shift=flow_shift,\n+                use_flow_sigmas=True,\n+                prediction_type=\"flow_prediction\",\n+            )\n+        elif args.scheduler_type == \"flow-euler\":\n+            scheduler = FlowMatchEulerDiscreteScheduler(shift=flow_shift)\n+        elif args.scheduler_type == \"uni-pc\":\n+            scheduler = UniPCMultistepScheduler(\n+                prediction_type=\"flow_prediction\",\n+                use_flow_sigmas=True,\n+                num_train_timesteps=1000,\n+                flow_shift=flow_shift,\n+            )\n+        else:\n+            raise ValueError(f\"Scheduler type {args.scheduler_type} is not supported\")\n+\n+        pipe = SanaVideoPipeline(\n+            tokenizer=tokenizer,\n+            text_encoder=text_encoder,\n+            transformer=transformer,\n+            vae=vae,\n+            scheduler=scheduler,\n+        )\n+\n+        pipe.save_pretrained(args.dump_path, safe_serialization=True, max_shard_size=\"5GB\")\n+\n+\n+DTYPE_MAPPING = {\n+    \"fp32\": torch.float32,\n+    \"fp16\": torch.float16,\n+    \"bf16\": torch.bfloat16,\n+}\n+\n+\n+if __name__ == \"__main__\":\n+    parser = argparse.ArgumentParser()\n+\n+    parser.add_argument(\n+        \"--orig_ckpt_path\", default=None, type=str, required=False, help=\"Path to the checkpoint to convert.\"\n+    )\n+    parser.add_argument(\n+        \"--video_size\",\n+        default=480,\n+        type=int,\n+        choices=[480, 720],\n+        required=False,\n+        help=\"Video size of pretrained model, 480 or 720.\",\n+    )\n+    parser.add_argument(\n+        \"--model_type\",\n+        default=\"SanaVideo\",\n+        type=str,\n+        choices=[\n+            \"SanaVideo\",\n+        ],\n+    )\n+    parser.add_argument(\n+        \"--scheduler_type\",\n+        default=\"flow-dpm_solver\",\n+        type=str,\n+        choices=[\"flow-dpm_solver\", \"flow-euler\", \"uni-pc\"],\n+        help=\"Scheduler type to use.\",\n+    )\n+    parser.add_argument(\"--dump_path\", default=None, type=str, required=True, help=\"Path to the output pipeline.\")\n+    parser.add_argument(\"--save_full_pipeline\", action=\"store_true\", help=\"save all the pipeline elements in one.\")\n+    parser.add_argument(\"--dtype\", default=\"fp32\", type=str, choices=[\"fp32\", \"fp16\", \"bf16\"], help=\"Weight dtype.\")\n+\n+    args = parser.parse_args()\n+\n+    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+    weight_dtype = DTYPE_MAPPING[args.dtype]\n+\n+    main(args)"
        },
        {
          "filename": "src/diffusers/__init__.py",
          "status": "modified",
          "additions": 4,
          "deletions": 0,
          "changes": 4,
          "patch": "@@ -246,6 +246,7 @@\n             \"QwenImageTransformer2DModel\",\n             \"SanaControlNetModel\",\n             \"SanaTransformer2DModel\",\n+            \"SanaVideoTransformer3DModel\",\n             \"SD3ControlNetModel\",\n             \"SD3MultiControlNetModel\",\n             \"SD3Transformer2DModel\",\n@@ -544,6 +545,7 @@\n             \"SanaPipeline\",\n             \"SanaSprintImg2ImgPipeline\",\n             \"SanaSprintPipeline\",\n+            \"SanaVideoPipeline\",\n             \"SemanticStableDiffusionPipeline\",\n             \"ShapEImg2ImgPipeline\",\n             \"ShapEPipeline\",\n@@ -951,6 +953,7 @@\n             QwenImageTransformer2DModel,\n             SanaControlNetModel,\n             SanaTransformer2DModel,\n+            SanaVideoTransformer3DModel,\n             SD3ControlNetModel,\n             SD3MultiControlNetModel,\n             SD3Transformer2DModel,\n@@ -1219,6 +1222,7 @@\n             SanaPipeline,\n             SanaSprintImg2ImgPipeline,\n             SanaSprintPipeline,\n+            SanaVideoPipeline,\n             SemanticStableDiffusionPipeline,\n             ShapEImg2ImgPipeline,\n             ShapEPipeline,"
        },
        {
          "filename": "src/diffusers/models/__init__.py",
          "status": "modified",
          "additions": 2,
          "deletions": 0,
          "changes": 2,
          "patch": "@@ -102,6 +102,7 @@\n     _import_structure[\"transformers.transformer_omnigen\"] = [\"OmniGenTransformer2DModel\"]\n     _import_structure[\"transformers.transformer_prx\"] = [\"PRXTransformer2DModel\"]\n     _import_structure[\"transformers.transformer_qwenimage\"] = [\"QwenImageTransformer2DModel\"]\n+    _import_structure[\"transformers.transformer_sana_video\"] = [\"SanaVideoTransformer3DModel\"]\n     _import_structure[\"transformers.transformer_sd3\"] = [\"SD3Transformer2DModel\"]\n     _import_structure[\"transformers.transformer_skyreels_v2\"] = [\"SkyReelsV2Transformer3DModel\"]\n     _import_structure[\"transformers.transformer_temporal\"] = [\"TransformerTemporalModel\"]\n@@ -204,6 +205,7 @@\n             PRXTransformer2DModel,\n             QwenImageTransformer2DModel,\n             SanaTransformer2DModel,\n+            SanaVideoTransformer3DModel,\n             SD3Transformer2DModel,\n             SkyReelsV2Transformer3DModel,\n             StableAudioDiTModel,"
        },
        {
          "filename": "src/diffusers/models/transformers/__init__.py",
          "status": "modified",
          "additions": 1,
          "deletions": 0,
          "changes": 1,
          "patch": "@@ -36,6 +36,7 @@\n     from .transformer_omnigen import OmniGenTransformer2DModel\n     from .transformer_prx import PRXTransformer2DModel\n     from .transformer_qwenimage import QwenImageTransformer2DModel\n+    from .transformer_sana_video import SanaVideoTransformer3DModel\n     from .transformer_sd3 import SD3Transformer2DModel\n     from .transformer_skyreels_v2 import SkyReelsV2Transformer3DModel\n     from .transformer_temporal import TransformerTemporalModel"
        },
        {
          "filename": "src/diffusers/models/transformers/transformer_sana_video.py",
          "status": "added",
          "additions": 703,
          "deletions": 0,
          "changes": 703,
          "patch": "@@ -0,0 +1,703 @@\n+# Copyright 2025 The HuggingFace Team and SANA-Video Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import math\n+from typing import Any, Dict, Optional, Tuple, Union\n+\n+import torch\n+import torch.nn.functional as F\n+from torch import nn\n+\n+from ...configuration_utils import ConfigMixin, register_to_config\n+from ...loaders import FromOriginalModelMixin, PeftAdapterMixin\n+from ...utils import USE_PEFT_BACKEND, logging, scale_lora_layers, unscale_lora_layers\n+from ..attention import AttentionMixin\n+from ..attention_dispatch import dispatch_attention_fn\n+from ..attention_processor import Attention\n+from ..embeddings import PixArtAlphaTextProjection, TimestepEmbedding, Timesteps, get_1d_rotary_pos_embed\n+from ..modeling_outputs import Transformer2DModelOutput\n+from ..modeling_utils import ModelMixin\n+from ..normalization import AdaLayerNormSingle, RMSNorm\n+\n+\n+logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n+\n+\n+class GLUMBTempConv(nn.Module):\n+    def __init__(\n+        self,\n+        in_channels: int,\n+        out_channels: int,\n+        expand_ratio: float = 4,\n+        norm_type: Optional[str] = None,\n+        residual_connection: bool = True,\n+    ) -> None:\n+        super().__init__()\n+\n+        hidden_channels = int(expand_ratio * in_channels)\n+        self.norm_type = norm_type\n+        self.residual_connection = residual_connection\n+\n+        self.nonlinearity = nn.SiLU()\n+        self.conv_inverted = nn.Conv2d(in_channels, hidden_channels * 2, 1, 1, 0)\n+        self.conv_depth = nn.Conv2d(hidden_channels * 2, hidden_channels * 2, 3, 1, 1, groups=hidden_channels * 2)\n+        self.conv_point = nn.Conv2d(hidden_channels, out_channels, 1, 1, 0, bias=False)\n+\n+        self.norm = None\n+        if norm_type == \"rms_norm\":\n+            self.norm = RMSNorm(out_channels, eps=1e-5, elementwise_affine=True, bias=True)\n+\n+        self.conv_temp = nn.Conv2d(\n+            out_channels, out_channels, kernel_size=(3, 1), stride=1, padding=(1, 0), bias=False\n+        )\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        if self.residual_connection:\n+            residual = hidden_states\n+        batch_size, num_frames, height, width, num_channels = hidden_states.shape\n+        hidden_states = hidden_states.view(batch_size * num_frames, height, width, num_channels).permute(0, 3, 1, 2)\n+\n+        hidden_states = self.conv_inverted(hidden_states)\n+        hidden_states = self.nonlinearity(hidden_states)\n+\n+        hidden_states = self.conv_depth(hidden_states)\n+        hidden_states, gate = torch.chunk(hidden_states, 2, dim=1)\n+        hidden_states = hidden_states * self.nonlinearity(gate)\n+\n+        hidden_states = self.conv_point(hidden_states)\n+\n+        # Temporal aggregation\n+        hidden_states_temporal = hidden_states.view(batch_size, num_frames, num_channels, height * width).permute(\n+            0, 2, 1, 3\n+        )\n+        hidden_states = hidden_states_temporal + self.conv_temp(hidden_states_temporal)\n+        hidden_states = hidden_states.permute(0, 2, 3, 1).view(batch_size, num_frames, height, width, num_channels)\n+\n+        if self.norm_type == \"rms_norm\":\n+            # move channel to the last dimension so we apply RMSnorm across channel dimension\n+            hidden_states = self.norm(hidden_states.movedim(1, -1)).movedim(-1, 1)\n+\n+        if self.residual_connection:\n+            hidden_states = hidden_states + residual\n+\n+        return hidden_states\n+\n+\n+class SanaLinearAttnProcessor3_0:\n+    r\"\"\"\n+    Processor for implementing scaled dot-product linear attention.\n+    \"\"\"\n+\n+    def __call__(\n+        self,\n+        attn: Attention,\n+        hidden_states: torch.Tensor,\n+        encoder_hidden_states: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        rotary_emb: Optional[torch.Tensor] = None,\n+    ) -> torch.Tensor:\n+        original_dtype = hidden_states.dtype\n+\n+        if encoder_hidden_states is None:\n+            encoder_hidden_states = hidden_states\n+\n+        query = attn.to_q(hidden_states)\n+        key = attn.to_k(encoder_hidden_states)\n+        value = attn.to_v(encoder_hidden_states)\n+\n+        if attn.norm_q is not None:\n+            query = attn.norm_q(query)\n+        if attn.norm_k is not None:\n+            key = attn.norm_k(key)\n+\n+        query = query.unflatten(2, (attn.heads, -1))\n+        key = key.unflatten(2, (attn.heads, -1))\n+        value = value.unflatten(2, (attn.heads, -1))\n+        # B,N,H,C\n+\n+        query = F.relu(query)\n+        key = F.relu(key)\n+\n+        if rotary_emb is not None:\n+\n+            def apply_rotary_emb(\n+                hidden_states: torch.Tensor,\n+                freqs_cos: torch.Tensor,\n+                freqs_sin: torch.Tensor,\n+            ):\n+                x1, x2 = hidden_states.unflatten(-1, (-1, 2)).unbind(-1)\n+                cos = freqs_cos[..., 0::2]\n+                sin = freqs_sin[..., 1::2]\n+                out = torch.empty_like(hidden_states)\n+                out[..., 0::2] = x1 * cos - x2 * sin\n+                out[..., 1::2] = x1 * sin + x2 * cos\n+                return out.type_as(hidden_states)\n+\n+            query_rotate = apply_rotary_emb(query, *rotary_emb)\n+            key_rotate = apply_rotary_emb(key, *rotary_emb)\n+\n+        # B,H,C,N\n+        query = query.permute(0, 2, 3, 1)\n+        key = key.permute(0, 2, 3, 1)\n+        query_rotate = query_rotate.permute(0, 2, 3, 1)\n+        key_rotate = key_rotate.permute(0, 2, 3, 1)\n+        value = value.permute(0, 2, 3, 1)\n+\n+        query_rotate, key_rotate, value = query_rotate.float(), key_rotate.float(), value.float()\n+\n+        z = 1 / (key.sum(dim=-1, keepdim=True).transpose(-2, -1) @ query + 1e-15)\n+\n+        scores = torch.matmul(value, key_rotate.transpose(-1, -2))\n+        hidden_states = torch.matmul(scores, query_rotate)\n+\n+        hidden_states = hidden_states * z\n+        # B,H,C,N\n+        hidden_states = hidden_states.flatten(1, 2).transpose(1, 2)\n+        hidden_states = hidden_states.to(original_dtype)\n+\n+        hidden_states = attn.to_out[0](hidden_states)\n+        hidden_states = attn.to_out[1](hidden_states)\n+\n+        return hidden_states\n+\n+\n+# Copied from diffusers.models.transformers.transformer_wan.WanRotaryPosEmbed\n+class WanRotaryPosEmbed(nn.Module):\n+    def __init__(\n+        self,\n+        attention_head_dim: int,\n+        patch_size: Tuple[int, int, int],\n+        max_seq_len: int,\n+        theta: float = 10000.0,\n+    ):\n+        super().__init__()\n+\n+        self.attention_head_dim = attention_head_dim\n+        self.patch_size = patch_size\n+        self.max_seq_len = max_seq_len\n+\n+        h_dim = w_dim = 2 * (attention_head_dim // 6)\n+        t_dim = attention_head_dim - h_dim - w_dim\n+        freqs_dtype = torch.float32 if torch.backends.mps.is_available() else torch.float64\n+\n+        freqs_cos = []\n+        freqs_sin = []\n+\n+        for dim in [t_dim, h_dim, w_dim]:\n+            freq_cos, freq_sin = get_1d_rotary_pos_embed(\n+                dim,\n+                max_seq_len,\n+                theta,\n+                use_real=True,\n+                repeat_interleave_real=True,\n+                freqs_dtype=freqs_dtype,\n+            )\n+            freqs_cos.append(freq_cos)\n+            freqs_sin.append(freq_sin)\n+\n+        self.register_buffer(\"freqs_cos\", torch.cat(freqs_cos, dim=1), persistent=False)\n+        self.register_buffer(\"freqs_sin\", torch.cat(freqs_sin, dim=1), persistent=False)\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        batch_size, num_channels, num_frames, height, width = hidden_states.shape\n+        p_t, p_h, p_w = self.patch_size\n+        ppf, pph, ppw = num_frames // p_t, height // p_h, width // p_w\n+\n+        split_sizes = [\n+            self.attention_head_dim - 2 * (self.attention_head_dim // 3),\n+            self.attention_head_dim // 3,\n+            self.attention_head_dim // 3,\n+        ]\n+\n+        freqs_cos = self.freqs_cos.split(split_sizes, dim=1)\n+        freqs_sin = self.freqs_sin.split(split_sizes, dim=1)\n+\n+        freqs_cos_f = freqs_cos[0][:ppf].view(ppf, 1, 1, -1).expand(ppf, pph, ppw, -1)\n+        freqs_cos_h = freqs_cos[1][:pph].view(1, pph, 1, -1).expand(ppf, pph, ppw, -1)\n+        freqs_cos_w = freqs_cos[2][:ppw].view(1, 1, ppw, -1).expand(ppf, pph, ppw, -1)\n+\n+        freqs_sin_f = freqs_sin[0][:ppf].view(ppf, 1, 1, -1).expand(ppf, pph, ppw, -1)\n+        freqs_sin_h = freqs_sin[1][:pph].view(1, pph, 1, -1).expand(ppf, pph, ppw, -1)\n+        freqs_sin_w = freqs_sin[2][:ppw].view(1, 1, ppw, -1).expand(ppf, pph, ppw, -1)\n+\n+        freqs_cos = torch.cat([freqs_cos_f, freqs_cos_h, freqs_cos_w], dim=-1).reshape(1, ppf * pph * ppw, 1, -1)\n+        freqs_sin = torch.cat([freqs_sin_f, freqs_sin_h, freqs_sin_w], dim=-1).reshape(1, ppf * pph * ppw, 1, -1)\n+\n+        return freqs_cos, freqs_sin\n+\n+\n+# Copied from diffusers.models.transformers.sana_transformer.SanaModulatedNorm\n+class SanaModulatedNorm(nn.Module):\n+    def __init__(self, dim: int, elementwise_affine: bool = False, eps: float = 1e-6):\n+        super().__init__()\n+        self.norm = nn.LayerNorm(dim, elementwise_affine=elementwise_affine, eps=eps)\n+\n+    def forward(\n+        self, hidden_states: torch.Tensor, temb: torch.Tensor, scale_shift_table: torch.Tensor\n+    ) -> torch.Tensor:\n+        hidden_states = self.norm(hidden_states)\n+        shift, scale = (scale_shift_table[None] + temb[:, None].to(scale_shift_table.device)).chunk(2, dim=1)\n+        hidden_states = hidden_states * (1 + scale) + shift\n+        return hidden_states\n+\n+\n+class SanaCombinedTimestepGuidanceEmbeddings(nn.Module):\n+    def __init__(self, embedding_dim):\n+        super().__init__()\n+        self.time_proj = Timesteps(num_channels=256, flip_sin_to_cos=True, downscale_freq_shift=0)\n+        self.timestep_embedder = TimestepEmbedding(in_channels=256, time_embed_dim=embedding_dim)\n+\n+        self.guidance_condition_proj = Timesteps(num_channels=256, flip_sin_to_cos=True, downscale_freq_shift=0)\n+        self.guidance_embedder = TimestepEmbedding(in_channels=256, time_embed_dim=embedding_dim)\n+\n+        self.silu = nn.SiLU()\n+        self.linear = nn.Linear(embedding_dim, 6 * embedding_dim, bias=True)\n+\n+    def forward(self, timestep: torch.Tensor, guidance: torch.Tensor = None, hidden_dtype: torch.dtype = None):\n+        timesteps_proj = self.time_proj(timestep)\n+        timesteps_emb = self.timestep_embedder(timesteps_proj.to(dtype=hidden_dtype))  # (N, D)\n+\n+        guidance_proj = self.guidance_condition_proj(guidance)\n+        guidance_emb = self.guidance_embedder(guidance_proj.to(dtype=hidden_dtype))\n+        conditioning = timesteps_emb + guidance_emb\n+\n+        return self.linear(self.silu(conditioning)), conditioning\n+\n+\n+class SanaAttnProcessor2_0:\n+    r\"\"\"\n+    Processor for implementing scaled dot-product attention (enabled by default if you're using PyTorch 2.0).\n+    \"\"\"\n+\n+    _attention_backend = None\n+    _parallel_config = None\n+\n+    def __init__(self):\n+        if not hasattr(F, \"scaled_dot_product_attention\"):\n+            raise ImportError(\"SanaAttnProcessor2_0 requires PyTorch 2.0, to use it, please upgrade PyTorch to 2.0.\")\n+\n+    def __call__(\n+        self,\n+        attn: Attention,\n+        hidden_states: torch.Tensor,\n+        encoder_hidden_states: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+    ) -> torch.Tensor:\n+        batch_size, sequence_length, _ = (\n+            hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape\n+        )\n+\n+        if attention_mask is not None:\n+            attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n+            # scaled_dot_product_attention expects attention_mask shape to be\n+            # (batch, heads, source_length, target_length)\n+            attention_mask = attention_mask.view(batch_size, attn.heads, -1, attention_mask.shape[-1])\n+\n+        query = attn.to_q(hidden_states)\n+\n+        if encoder_hidden_states is None:\n+            encoder_hidden_states = hidden_states\n+\n+        key = attn.to_k(encoder_hidden_states)\n+        value = attn.to_v(encoder_hidden_states)\n+\n+        if attn.norm_q is not None:\n+            query = attn.norm_q(query)\n+        if attn.norm_k is not None:\n+            key = attn.norm_k(key)\n+\n+        inner_dim = key.shape[-1]\n+        head_dim = inner_dim // attn.heads\n+\n+        query = query.view(batch_size, -1, attn.heads, head_dim)\n+        key = key.view(batch_size, -1, attn.heads, head_dim)\n+        value = value.view(batch_size, -1, attn.heads, head_dim)\n+\n+        # the output of sdp = (batch, num_heads, seq_len, head_dim)\n+        hidden_states = dispatch_attention_fn(\n+            query,\n+            key,\n+            value,\n+            attn_mask=attention_mask,\n+            dropout_p=0.0,\n+            is_causal=False,\n+            backend=self._attention_backend,\n+            parallel_config=self._parallel_config,\n+        )\n+        hidden_states = hidden_states.flatten(2, 3)\n+        hidden_states = hidden_states.type_as(query)\n+\n+        # linear proj\n+        hidden_states = attn.to_out[0](hidden_states)\n+        # dropout\n+        hidden_states = attn.to_out[1](hidden_states)\n+\n+        hidden_states = hidden_states / attn.rescale_output_factor\n+\n+        return hidden_states\n+\n+\n+class SanaVideoTransformerBlock(nn.Module):\n+    r\"\"\"\n+    Transformer block introduced in [Sana-Video](https://huggingface.co/papers/2509.24695).\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        dim: int = 2240,\n+        num_attention_heads: int = 20,\n+        attention_head_dim: int = 112,\n+        dropout: float = 0.0,\n+        num_cross_attention_heads: Optional[int] = 20,\n+        cross_attention_head_dim: Optional[int] = 112,\n+        cross_attention_dim: Optional[int] = 2240,\n+        attention_bias: bool = True,\n+        norm_elementwise_affine: bool = False,\n+        norm_eps: float = 1e-6,\n+        attention_out_bias: bool = True,\n+        mlp_ratio: float = 3.0,\n+        qk_norm: Optional[str] = \"rms_norm_across_heads\",\n+        rope_max_seq_len: int = 1024,\n+    ) -> None:\n+        super().__init__()\n+\n+        # 1. Self Attention\n+        self.norm1 = nn.LayerNorm(dim, elementwise_affine=False, eps=norm_eps)\n+        self.attn1 = Attention(\n+            query_dim=dim,\n+            heads=num_attention_heads,\n+            dim_head=attention_head_dim,\n+            kv_heads=num_attention_heads if qk_norm is not None else None,\n+            qk_norm=qk_norm,\n+            dropout=dropout,\n+            bias=attention_bias,\n+            cross_attention_dim=None,\n+            processor=SanaLinearAttnProcessor3_0(),\n+        )\n+\n+        # 2. Cross Attention\n+        if cross_attention_dim is not None:\n+            self.norm2 = nn.LayerNorm(dim, elementwise_affine=norm_elementwise_affine, eps=norm_eps)\n+            self.attn2 = Attention(\n+                query_dim=dim,\n+                qk_norm=qk_norm,\n+                kv_heads=num_cross_attention_heads if qk_norm is not None else None,\n+                cross_attention_dim=cross_attention_dim,\n+                heads=num_cross_attention_heads,\n+                dim_head=cross_attention_head_dim,\n+                dropout=dropout,\n+                bias=True,\n+                out_bias=attention_out_bias,\n+                processor=SanaAttnProcessor2_0(),\n+            )\n+\n+        # 3. Feed-forward\n+        self.ff = GLUMBTempConv(dim, dim, mlp_ratio, norm_type=None, residual_connection=False)\n+\n+        self.scale_shift_table = nn.Parameter(torch.randn(6, dim) / dim**0.5)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        encoder_hidden_states: Optional[torch.Tensor] = None,\n+        encoder_attention_mask: Optional[torch.Tensor] = None,\n+        timestep: Optional[torch.LongTensor] = None,\n+        frames: int = None,\n+        height: int = None,\n+        width: int = None,\n+        rotary_emb: Optional[torch.Tensor] = None,\n+    ) -> torch.Tensor:\n+        batch_size = hidden_states.shape[0]\n+\n+        # 1. Modulation\n+        shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = (\n+            self.scale_shift_table[None] + timestep.reshape(batch_size, 6, -1)\n+        ).chunk(6, dim=1)\n+\n+        # 2. Self Attention\n+        norm_hidden_states = self.norm1(hidden_states)\n+        norm_hidden_states = norm_hidden_states * (1 + scale_msa) + shift_msa\n+        norm_hidden_states = norm_hidden_states.to(hidden_states.dtype)\n+\n+        attn_output = self.attn1(norm_hidden_states, rotary_emb=rotary_emb)\n+        hidden_states = hidden_states + gate_msa * attn_output\n+\n+        # 3. Cross Attention\n+        if self.attn2 is not None:\n+            attn_output = self.attn2(\n+                hidden_states,\n+                encoder_hidden_states=encoder_hidden_states,\n+                attention_mask=encoder_attention_mask,\n+            )\n+            hidden_states = attn_output + hidden_states\n+\n+        # 4. Feed-forward\n+        norm_hidden_states = self.norm2(hidden_states)\n+        norm_hidden_states = norm_hidden_states * (1 + scale_mlp) + shift_mlp\n+\n+        norm_hidden_states = norm_hidden_states.unflatten(1, (frames, height, width))\n+        ff_output = self.ff(norm_hidden_states)\n+        ff_output = ff_output.flatten(1, 3)\n+        hidden_states = hidden_states + gate_mlp * ff_output\n+\n+        return hidden_states\n+\n+\n+class SanaVideoTransformer3DModel(ModelMixin, ConfigMixin, PeftAdapterMixin, FromOriginalModelMixin, AttentionMixin):\n+    r\"\"\"\n+    A 3D Transformer model introduced in [Sana-Video](https://huggingface.co/papers/2509.24695) family of models.\n+\n+    Args:\n+        in_channels (`int`, defaults to `16`):\n+            The number of channels in the input.\n+        out_channels (`int`, *optional*, defaults to `16`):\n+            The number of channels in the output.\n+        num_attention_heads (`int`, defaults to `20`):\n+            The number of heads to use for multi-head attention.\n+        attention_head_dim (`int`, defaults to `112`):\n+            The number of channels in each head.\n+        num_layers (`int`, defaults to `20`):\n+            The number of layers of Transformer blocks to use.\n+        num_cross_attention_heads (`int`, *optional*, defaults to `20`):\n+            The number of heads to use for cross-attention.\n+        cross_attention_head_dim (`int`, *optional*, defaults to `112`):\n+            The number of channels in each head for cross-attention.\n+        cross_attention_dim (`int`, *optional*, defaults to `2240`):\n+            The number of channels in the cross-attention output.\n+        caption_channels (`int`, defaults to `2304`):\n+            The number of channels in the caption embeddings.\n+        mlp_ratio (`float`, defaults to `2.5`):\n+            The expansion ratio to use in the GLUMBConv layer.\n+        dropout (`float`, defaults to `0.0`):\n+            The dropout probability.\n+        attention_bias (`bool`, defaults to `False`):\n+            Whether to use bias in the attention layer.\n+        sample_size (`int`, defaults to `32`):\n+            The base size of the input latent.\n+        patch_size (`int`, defaults to `1`):\n+            The size of the patches to use in the patch embedding layer.\n+        norm_elementwise_affine (`bool`, defaults to `False`):\n+            Whether to use elementwise affinity in the normalization layer.\n+        norm_eps (`float`, defaults to `1e-6`):\n+            The epsilon value for the normalization layer.\n+        qk_norm (`str`, *optional*, defaults to `None`):\n+            The normalization to use for the query and key.\n+    \"\"\"\n+\n+    _supports_gradient_checkpointing = True\n+    _no_split_modules = [\"SanaVideoTransformerBlock\", \"SanaModulatedNorm\"]\n+    _skip_layerwise_casting_patterns = [\"patch_embedding\", \"norm\"]\n+\n+    @register_to_config\n+    def __init__(\n+        self,\n+        in_channels: int = 16,\n+        out_channels: Optional[int] = 16,\n+        num_attention_heads: int = 20,\n+        attention_head_dim: int = 112,\n+        num_layers: int = 20,\n+        num_cross_attention_heads: Optional[int] = 20,\n+        cross_attention_head_dim: Optional[int] = 112,\n+        cross_attention_dim: Optional[int] = 2240,\n+        caption_channels: int = 2304,\n+        mlp_ratio: float = 2.5,\n+        dropout: float = 0.0,\n+        attention_bias: bool = False,\n+        sample_size: int = 30,\n+        patch_size: Tuple[int, int, int] = (1, 2, 2),\n+        norm_elementwise_affine: bool = False,\n+        norm_eps: float = 1e-6,\n+        interpolation_scale: Optional[int] = None,\n+        guidance_embeds: bool = False,\n+        guidance_embeds_scale: float = 0.1,\n+        qk_norm: Optional[str] = \"rms_norm_across_heads\",\n+        rope_max_seq_len: int = 1024,\n+    ) -> None:\n+        super().__init__()\n+\n+        out_channels = out_channels or in_channels\n+        inner_dim = num_attention_heads * attention_head_dim\n+\n+        # 1. Patch & position embedding\n+        self.rope = WanRotaryPosEmbed(attention_head_dim, patch_size, rope_max_seq_len)\n+        self.patch_embedding = nn.Conv3d(in_channels, inner_dim, kernel_size=patch_size, stride=patch_size)\n+\n+        # 2. Additional condition embeddings\n+        if guidance_embeds:\n+            self.time_embed = SanaCombinedTimestepGuidanceEmbeddings(inner_dim)\n+        else:\n+            self.time_embed = AdaLayerNormSingle(inner_dim)\n+\n+        self.caption_projection = PixArtAlphaTextProjection(in_features=caption_channels, hidden_size=inner_dim)\n+        self.caption_norm = RMSNorm(inner_dim, eps=1e-5, elementwise_affine=True)\n+\n+        # 3. Transformer blocks\n+        self.transformer_blocks = nn.ModuleList(\n+            [\n+                SanaVideoTransformerBlock(\n+                    inner_dim,\n+                    num_attention_heads,\n+                    attention_head_dim,\n+                    dropout=dropout,\n+                    num_cross_attention_heads=num_cross_attention_heads,\n+                    cross_attention_head_dim=cross_attention_head_dim,\n+                    cross_attention_dim=cross_attention_dim,\n+                    attention_bias=attention_bias,\n+                    norm_elementwise_affine=norm_elementwise_affine,\n+                    norm_eps=norm_eps,\n+                    mlp_ratio=mlp_ratio,\n+                    qk_norm=qk_norm,\n+                )\n+                for _ in range(num_layers)\n+            ]\n+        )\n+\n+        # 4. Output blocks\n+        self.scale_shift_table = nn.Parameter(torch.randn(2, inner_dim) / inner_dim**0.5)\n+        self.norm_out = SanaModulatedNorm(inner_dim, elementwise_affine=False, eps=1e-6)\n+        self.proj_out = nn.Linear(inner_dim, math.prod(patch_size) * out_channels)\n+\n+        self.gradient_checkpointing = False\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        encoder_hidden_states: torch.Tensor,\n+        timestep: torch.Tensor,\n+        guidance: Optional[torch.Tensor] = None,\n+        encoder_attention_mask: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        attention_kwargs: Optional[Dict[str, Any]] = None,\n+        controlnet_block_samples: Optional[Tuple[torch.Tensor]] = None,\n+        return_dict: bool = True,\n+    ) -> Union[Tuple[torch.Tensor, ...], Transformer2DModelOutput]:\n+        if attention_kwargs is not None:\n+            attention_kwargs = attention_kwargs.copy()\n+            lora_scale = attention_kwargs.pop(\"scale\", 1.0)\n+        else:\n+            lora_scale = 1.0\n+\n+        if USE_PEFT_BACKEND:\n+            # weight the lora layers by setting `lora_scale` for each PEFT layer\n+            scale_lora_layers(self, lora_scale)\n+        else:\n+            if attention_kwargs is not None and attention_kwargs.get(\"scale\", None) is not None:\n+                logger.warning(\n+                    \"Passing `scale` via `attention_kwargs` when not using the PEFT backend is ineffective.\"\n+                )\n+\n+        # ensure attention_mask is a bias, and give it a singleton query_tokens dimension.\n+        #   we may have done this conversion already, e.g. if we came here via UNet2DConditionModel#forward.\n+        #   we can tell by counting dims; if ndim == 2: it's a mask rather than a bias.\n+        # expects mask of shape:\n+        #   [batch, key_tokens]\n+        # adds singleton query_tokens dimension:\n+        #   [batch,                    1, key_tokens]\n+        # this helps to broadcast it as a bias over attention scores, which will be in one of the following shapes:\n+        #   [batch,  heads, query_tokens, key_tokens] (e.g. torch sdp attn)\n+        #   [batch * heads, query_tokens, key_tokens] (e.g. xformers or classic attn)\n+        if attention_mask is not None and attention_mask.ndim == 2:\n+            # assume that mask is expressed as:\n+            #   (1 = keep,      0 = discard)\n+            # convert mask into a bias that can be added to attention scores:\n+            #       (keep = +0,     discard = -10000.0)\n+            attention_mask = (1 - attention_mask.to(hidden_states.dtype)) * -10000.0\n+            attention_mask = attention_mask.unsqueeze(1)\n+\n+        # convert encoder_attention_mask to a bias the same way we do for attention_mask\n+        if encoder_attention_mask is not None and encoder_attention_mask.ndim == 2:\n+            encoder_attention_mask = (1 - encoder_attention_mask.to(hidden_states.dtype)) * -10000.0\n+            encoder_attention_mask = encoder_attention_mask.unsqueeze(1)\n+\n+        # 1. Input\n+        batch_size, num_channels, num_frames, height, width = hidden_states.shape\n+        p_t, p_h, p_w = self.config.patch_size\n+        post_patch_num_frames = num_frames // p_t\n+        post_patch_height = height // p_h\n+        post_patch_width = width // p_w\n+\n+        rotary_emb = self.rope(hidden_states)\n+\n+        hidden_states = self.patch_embedding(hidden_states)\n+        hidden_states = hidden_states.flatten(2).transpose(1, 2)\n+\n+        if guidance is not None:\n+            timestep, embedded_timestep = self.time_embed(\n+                timestep, guidance=guidance, hidden_dtype=hidden_states.dtype\n+            )\n+        else:\n+            timestep, embedded_timestep = self.time_embed(\n+                timestep, batch_size=batch_size, hidden_dtype=hidden_states.dtype\n+            )\n+\n+        encoder_hidden_states = self.caption_projection(encoder_hidden_states)\n+        encoder_hidden_states = encoder_hidden_states.view(batch_size, -1, hidden_states.shape[-1])\n+\n+        encoder_hidden_states = self.caption_norm(encoder_hidden_states)\n+\n+        # 2. Transformer blocks\n+        if torch.is_grad_enabled() and self.gradient_checkpointing:\n+            for index_block, block in enumerate(self.transformer_blocks):\n+                hidden_states = self._gradient_checkpointing_func(\n+                    block,\n+                    hidden_states,\n+                    attention_mask,\n+                    encoder_hidden_states,\n+                    encoder_attention_mask,\n+                    timestep,\n+                    post_patch_num_frames,\n+                    post_patch_height,\n+                    post_patch_width,\n+                    rotary_emb,\n+                )\n+                if controlnet_block_samples is not None and 0 < index_block <= len(controlnet_block_samples):\n+                    hidden_states = hidden_states + controlnet_block_samples[index_block - 1]\n+\n+        else:\n+            for index_block, block in enumerate(self.transformer_blocks):\n+                hidden_states = block(\n+                    hidden_states,\n+                    attention_mask,\n+                    encoder_hidden_states,\n+                    encoder_attention_mask,\n+                    timestep,\n+                    post_patch_num_frames,\n+                    post_patch_height,\n+                    post_patch_width,\n+                    rotary_emb,\n+                )\n+                if controlnet_block_samples is not None and 0 < index_block <= len(controlnet_block_samples):\n+                    hidden_states = hidden_states + controlnet_block_samples[index_block - 1]\n+\n+        # 3. Normalization\n+        hidden_states = self.norm_out(hidden_states, embedded_timestep, self.scale_shift_table)\n+\n+        hidden_states = self.proj_out(hidden_states)\n+\n+        # 5. Unpatchify\n+        hidden_states = hidden_states.reshape(\n+            batch_size, post_patch_num_frames, post_patch_height, post_patch_width, p_t, p_h, p_w, -1\n+        )\n+        hidden_states = hidden_states.permute(0, 7, 1, 4, 2, 5, 3, 6)\n+        output = hidden_states.flatten(6, 7).flatten(4, 5).flatten(2, 3)\n+\n+        if USE_PEFT_BACKEND:\n+            # remove `lora_scale` from each PEFT layer\n+            unscale_lora_layers(self, lora_scale)\n+\n+        if not return_dict:\n+            return (output,)\n+\n+        return Transformer2DModelOutput(sample=output)"
        },
        {
          "filename": "src/diffusers/pipelines/__init__.py",
          "status": "modified",
          "additions": 8,
          "deletions": 1,
          "changes": 9,
          "patch": "@@ -308,6 +308,7 @@\n         \"SanaSprintPipeline\",\n         \"SanaControlNetPipeline\",\n         \"SanaSprintImg2ImgPipeline\",\n+        \"SanaVideoPipeline\",\n     ]\n     _import_structure[\"semantic_stable_diffusion\"] = [\"SemanticStableDiffusionPipeline\"]\n     _import_structure[\"shap_e\"] = [\"ShapEImg2ImgPipeline\", \"ShapEPipeline\"]\n@@ -735,7 +736,13 @@\n             QwenImageInpaintPipeline,\n             QwenImagePipeline,\n         )\n-        from .sana import SanaControlNetPipeline, SanaPipeline, SanaSprintImg2ImgPipeline, SanaSprintPipeline\n+        from .sana import (\n+            SanaControlNetPipeline,\n+            SanaPipeline,\n+            SanaSprintImg2ImgPipeline,\n+            SanaSprintPipeline,\n+            SanaVideoPipeline,\n+        )\n         from .semantic_stable_diffusion import SemanticStableDiffusionPipeline\n         from .shap_e import ShapEImg2ImgPipeline, ShapEPipeline\n         from .stable_audio import StableAudioPipeline, StableAudioProjectionModel"
        },
        {
          "filename": "src/diffusers/pipelines/sana/__init__.py",
          "status": "modified",
          "additions": 2,
          "deletions": 0,
          "changes": 2,
          "patch": "@@ -26,6 +26,7 @@\n     _import_structure[\"pipeline_sana_controlnet\"] = [\"SanaControlNetPipeline\"]\n     _import_structure[\"pipeline_sana_sprint\"] = [\"SanaSprintPipeline\"]\n     _import_structure[\"pipeline_sana_sprint_img2img\"] = [\"SanaSprintImg2ImgPipeline\"]\n+    _import_structure[\"pipeline_sana_video\"] = [\"SanaVideoPipeline\"]\n \n if TYPE_CHECKING or DIFFUSERS_SLOW_IMPORT:\n     try:\n@@ -39,6 +40,7 @@\n         from .pipeline_sana_controlnet import SanaControlNetPipeline\n         from .pipeline_sana_sprint import SanaSprintPipeline\n         from .pipeline_sana_sprint_img2img import SanaSprintImg2ImgPipeline\n+        from .pipeline_sana_video import SanaVideoPipeline\n else:\n     import sys\n "
        },
        {
          "filename": "src/diffusers/pipelines/sana/pipeline_output.py",
          "status": "modified",
          "additions": 16,
          "deletions": 0,
          "changes": 16,
          "patch": "@@ -3,6 +3,7 @@\n \n import numpy as np\n import PIL.Image\n+import torch\n \n from ...utils import BaseOutput\n \n@@ -19,3 +20,18 @@ class SanaPipelineOutput(BaseOutput):\n     \"\"\"\n \n     images: Union[List[PIL.Image.Image], np.ndarray]\n+\n+\n+@dataclass\n+class SanaVideoPipelineOutput(BaseOutput):\n+    r\"\"\"\n+    Output class for Sana-Video pipelines.\n+\n+    Args:\n+        frames (`torch.Tensor`, `np.ndarray`, or List[List[PIL.Image.Image]]):\n+            List of video outputs - It can be a nested list of length `batch_size,` with each sub-list containing\n+            denoised PIL image sequences of length `num_frames.` It can also be a NumPy array or Torch tensor of shape\n+            `(batch_size, num_frames, channels, height, width)`.\n+    \"\"\"\n+\n+    frames: torch.Tensor"
        },
        {
          "filename": "src/diffusers/pipelines/sana/pipeline_sana.py",
          "status": "modified",
          "additions": 1,
          "deletions": 1,
          "changes": 2,
          "patch": "@@ -1,4 +1,4 @@\n-# Copyright 2025 PixArt-Sigma Authors and The HuggingFace Team. All rights reserved.\n+# Copyright 2025 SANA Authors and The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License."
        },
        {
          "filename": "src/diffusers/pipelines/sana/pipeline_sana_sprint.py",
          "status": "modified",
          "additions": 1,
          "deletions": 1,
          "changes": 2,
          "patch": "@@ -1,4 +1,4 @@\n-# Copyright 2025 PixArt-Sigma Authors and The HuggingFace Team. All rights reserved.\n+# Copyright 2025 SANA-Sprint Authors and The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License."
        },
        {
          "filename": "src/diffusers/pipelines/sana/pipeline_sana_video.py",
          "status": "added",
          "additions": 1017,
          "deletions": 0,
          "changes": 1017,
          "patch": "@@ -0,0 +1,1017 @@\n+# Copyright 2025 SANA-Video Authors and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import html\n+import inspect\n+import re\n+import urllib.parse as ul\n+import warnings\n+from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n+\n+import torch\n+from transformers import Gemma2PreTrainedModel, GemmaTokenizer, GemmaTokenizerFast\n+\n+from ...callbacks import MultiPipelineCallbacks, PipelineCallback\n+from ...loaders import SanaLoraLoaderMixin\n+from ...models import AutoencoderDC, AutoencoderKLWan, SanaVideoTransformer3DModel\n+from ...schedulers import DPMSolverMultistepScheduler\n+from ...utils import (\n+    BACKENDS_MAPPING,\n+    USE_PEFT_BACKEND,\n+    is_bs4_available,\n+    is_ftfy_available,\n+    is_torch_xla_available,\n+    logging,\n+    replace_example_docstring,\n+    scale_lora_layers,\n+    unscale_lora_layers,\n+)\n+from ...utils.torch_utils import get_device, is_torch_version, randn_tensor\n+from ...video_processor import VideoProcessor\n+from ..pipeline_utils import DiffusionPipeline\n+from .pipeline_output import SanaVideoPipelineOutput\n+\n+\n+ASPECT_RATIO_480_BIN = {\n+    \"0.5\": [448.0, 896.0],\n+    \"0.57\": [480.0, 832.0],\n+    \"0.68\": [528.0, 768.0],\n+    \"0.78\": [560.0, 720.0],\n+    \"1.0\": [624.0, 624.0],\n+    \"1.13\": [672.0, 592.0],\n+    \"1.29\": [720.0, 560.0],\n+    \"1.46\": [768.0, 528.0],\n+    \"1.67\": [816.0, 496.0],\n+    \"1.75\": [832.0, 480.0],\n+    \"2.0\": [896.0, 448.0],\n+}\n+\n+\n+ASPECT_RATIO_720_BIN = {\n+    \"0.5\": [672.0, 1344.0],\n+    \"0.57\": [704.0, 1280.0],\n+    \"0.68\": [800.0, 1152.0],\n+    \"0.78\": [832.0, 1088.0],\n+    \"1.0\": [960.0, 960.0],\n+    \"1.13\": [1024.0, 896.0],\n+    \"1.29\": [1088.0, 832.0],\n+    \"1.46\": [1152.0, 800.0],\n+    \"1.67\": [1248.0, 736.0],\n+    \"1.75\": [1280.0, 704.0],\n+    \"2.0\": [1344.0, 672.0],\n+}\n+\n+if is_torch_xla_available():\n+    import torch_xla.core.xla_model as xm\n+\n+    XLA_AVAILABLE = True\n+else:\n+    XLA_AVAILABLE = False\n+\n+logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n+\n+if is_bs4_available():\n+    from bs4 import BeautifulSoup\n+\n+if is_ftfy_available():\n+    import ftfy\n+\n+\n+EXAMPLE_DOC_STRING = \"\"\"\n+    Examples:\n+        ```py\n+        >>> import torch\n+        >>> from diffusers import SanaVideoPipeline\n+        >>> from diffusers.utils import export_to_video\n+\n+        >>> model_id = \"Efficient-Large-Model/SANA-Video_2B_480p_diffusers\"\n+        >>> pipe = SanaVideoPipeline.from_pretrained(model_id)\n+        >>> pipe.transformer.to(torch.bfloat16)\n+        >>> pipe.text_encoder.to(torch.bfloat16)\n+        >>> pipe.vae.to(torch.float32)\n+        >>> pipe.to(\"cuda\")\n+        >>> model_score = 30\n+\n+        >>> prompt = \"Evening, backlight, side lighting, soft light, high contrast, mid-shot, centered composition, clean solo shot, warm color. A young Caucasian man stands in a forest, golden light glimmers on his hair as sunlight filters through the leaves. He wears a light shirt, wind gently blowing his hair and collar, light dances across his face with his movements. The background is blurred, with dappled light and soft tree shadows in the distance. The camera focuses on his lifted gaze, clear and emotional.\"\n+        >>> negative_prompt = \"A chaotic sequence with misshapen, deformed limbs in heavy motion blur, sudden disappearance, jump cuts, jerky movements, rapid shot changes, frames out of sync, inconsistent character shapes, temporal artifacts, jitter, and ghosting effects, creating a disorienting visual experience.\"\n+        >>> motion_prompt = f\" motion score: {model_score}.\"\n+        >>> prompt = prompt + motion_prompt\n+\n+        >>> output = pipe(\n+        ...     prompt=prompt,\n+        ...     negative_prompt=negative_prompt,\n+        ...     height=480,\n+        ...     width=832,\n+        ...     frames=81,\n+        ...     guidance_scale=6,\n+        ...     num_inference_steps=50,\n+        ...     generator=torch.Generator(device=\"cuda\").manual_seed(42),\n+        ... ).frames[0]\n+\n+        >>> export_to_video(output, \"sana-video-output.mp4\", fps=16)\n+        ```\n+\"\"\"\n+\n+\n+# Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.retrieve_timesteps\n+def retrieve_timesteps(\n+    scheduler,\n+    num_inference_steps: Optional[int] = None,\n+    device: Optional[Union[str, torch.device]] = None,\n+    timesteps: Optional[List[int]] = None,\n+    sigmas: Optional[List[float]] = None,\n+    **kwargs,\n+):\n+    r\"\"\"\n+    Calls the scheduler's `set_timesteps` method and retrieves timesteps from the scheduler after the call. Handles\n+    custom timesteps. Any kwargs will be supplied to `scheduler.set_timesteps`.\n+\n+    Args:\n+        scheduler (`SchedulerMixin`):\n+            The scheduler to get timesteps from.\n+        num_inference_steps (`int`):\n+            The number of diffusion steps used when generating samples with a pre-trained model. If used, `timesteps`\n+            must be `None`.\n+        device (`str` or `torch.device`, *optional*):\n+            The device to which the timesteps should be moved to. If `None`, the timesteps are not moved.\n+        timesteps (`List[int]`, *optional*):\n+            Custom timesteps used to override the timestep spacing strategy of the scheduler. If `timesteps` is passed,\n+            `num_inference_steps` and `sigmas` must be `None`.\n+        sigmas (`List[float]`, *optional*):\n+            Custom sigmas used to override the timestep spacing strategy of the scheduler. If `sigmas` is passed,\n+            `num_inference_steps` and `timesteps` must be `None`.\n+\n+    Returns:\n+        `Tuple[torch.Tensor, int]`: A tuple where the first element is the timestep schedule from the scheduler and the\n+        second element is the number of inference steps.\n+    \"\"\"\n+    if timesteps is not None and sigmas is not None:\n+        raise ValueError(\"Only one of `timesteps` or `sigmas` can be passed. Please choose one to set custom values\")\n+    if timesteps is not None:\n+        accepts_timesteps = \"timesteps\" in set(inspect.signature(scheduler.set_timesteps).parameters.keys())\n+        if not accepts_timesteps:\n+            raise ValueError(\n+                f\"The current scheduler class {scheduler.__class__}'s `set_timesteps` does not support custom\"\n+                f\" timestep schedules. Please check whether you are using the correct scheduler.\"\n+            )\n+        scheduler.set_timesteps(timesteps=timesteps, device=device, **kwargs)\n+        timesteps = scheduler.timesteps\n+        num_inference_steps = len(timesteps)\n+    elif sigmas is not None:\n+        accept_sigmas = \"sigmas\" in set(inspect.signature(scheduler.set_timesteps).parameters.keys())\n+        if not accept_sigmas:\n+            raise ValueError(\n+                f\"The current scheduler class {scheduler.__class__}'s `set_timesteps` does not support custom\"\n+                f\" sigmas schedules. Please check whether you are using the correct scheduler.\"\n+            )\n+        scheduler.set_timesteps(sigmas=sigmas, device=device, **kwargs)\n+        timesteps = scheduler.timesteps\n+        num_inference_steps = len(timesteps)\n+    else:\n+        scheduler.set_timesteps(num_inference_steps, device=device, **kwargs)\n+        timesteps = scheduler.timesteps\n+    return timesteps, num_inference_steps\n+\n+\n+class SanaVideoPipeline(DiffusionPipeline, SanaLoraLoaderMixin):\n+    r\"\"\"\n+    Pipeline for text-to-video generation using [Sana](https://huggingface.co/papers/2509.24695). This model inherits\n+    from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods implemented for all\n+    pipelines (downloading, saving, running on a particular device, etc.).\n+\n+    Args:\n+        tokenizer ([`GemmaTokenizer`] or [`GemmaTokenizerFast`]):\n+            The tokenizer used to tokenize the prompt.\n+        text_encoder ([`Gemma2PreTrainedModel`]):\n+            Text encoder model to encode the input prompts.\n+        vae ([`AutoencoderKLWan` or `AutoencoderDCAEV`]):\n+            Variational Auto-Encoder (VAE) Model to encode and decode videos to and from latent representations.\n+        transformer ([`SanaVideoTransformer3DModel`]):\n+            Conditional Transformer to denoise the input latents.\n+        scheduler ([`DPMSolverMultistepScheduler`]):\n+            A scheduler to be used in combination with `transformer` to denoise the encoded video latents.\n+    \"\"\"\n+\n+    # fmt: off\n+    bad_punct_regex = re.compile(r\"[\" + \"#\u00ae\u2022\u00a9\u2122&@\u00b7\u00ba\u00bd\u00be\u00bf\u00a1\u00a7~\" + r\"\\)\" + r\"\\(\" + r\"\\]\" + r\"\\[\" + r\"\\}\" + r\"\\{\" + r\"\\|\" + \"\\\\\" + r\"\\/\" + r\"\\*\" + r\"]{1,}\")\n+    # fmt: on\n+\n+    model_cpu_offload_seq = \"text_encoder->transformer->vae\"\n+    _callback_tensor_inputs = [\"latents\", \"prompt_embeds\", \"negative_prompt_embeds\"]\n+\n+    def __init__(\n+        self,\n+        tokenizer: Union[GemmaTokenizer, GemmaTokenizerFast],\n+        text_encoder: Gemma2PreTrainedModel,\n+        vae: Union[AutoencoderDC, AutoencoderKLWan],\n+        transformer: SanaVideoTransformer3DModel,\n+        scheduler: DPMSolverMultistepScheduler,\n+    ):\n+        super().__init__()\n+\n+        self.register_modules(\n+            tokenizer=tokenizer, text_encoder=text_encoder, vae=vae, transformer=transformer, scheduler=scheduler\n+        )\n+\n+        self.vae_scale_factor_temporal = self.vae.config.scale_factor_temporal if getattr(self, \"vae\", None) else 4\n+        self.vae_scale_factor_spatial = self.vae.config.scale_factor_spatial if getattr(self, \"vae\", None) else 8\n+\n+        self.vae_scale_factor = self.vae_scale_factor_spatial\n+\n+        self.video_processor = VideoProcessor(vae_scale_factor=self.vae_scale_factor_spatial)\n+\n+    def _get_gemma_prompt_embeds(\n+        self,\n+        prompt: Union[str, List[str]],\n+        device: torch.device,\n+        dtype: torch.dtype,\n+        clean_caption: bool = False,\n+        max_sequence_length: int = 300,\n+        complex_human_instruction: Optional[List[str]] = None,\n+    ):\n+        r\"\"\"\n+        Encodes the prompt into text encoder hidden states.\n+\n+        Args:\n+            prompt (`str` or `List[str]`, *optional*):\n+                prompt to be encoded\n+            device: (`torch.device`, *optional*):\n+                torch device to place the resulting embeddings on\n+            clean_caption (`bool`, defaults to `False`):\n+                If `True`, the function will preprocess and clean the provided caption before encoding.\n+            max_sequence_length (`int`, defaults to 300): Maximum sequence length to use for the prompt.\n+            complex_human_instruction (`list[str]`, defaults to `complex_human_instruction`):\n+                If `complex_human_instruction` is not empty, the function will use the complex Human instruction for\n+                the prompt.\n+        \"\"\"\n+        prompt = [prompt] if isinstance(prompt, str) else prompt\n+\n+        if getattr(self, \"tokenizer\", None) is not None:\n+            self.tokenizer.padding_side = \"right\"\n+\n+        prompt = self._text_preprocessing(prompt, clean_caption=clean_caption)\n+\n+        # prepare complex human instruction\n+        if not complex_human_instruction:\n+            max_length_all = max_sequence_length\n+        else:\n+            chi_prompt = \"\\n\".join(complex_human_instruction)\n+            prompt = [chi_prompt + p for p in prompt]\n+            num_chi_prompt_tokens = len(self.tokenizer.encode(chi_prompt))\n+            max_length_all = num_chi_prompt_tokens + max_sequence_length - 2\n+\n+        text_inputs = self.tokenizer(\n+            prompt,\n+            padding=\"max_length\",\n+            max_length=max_length_all,\n+            truncation=True,\n+            add_special_tokens=True,\n+            return_tensors=\"pt\",\n+        )\n+        text_input_ids = text_inputs.input_ids\n+\n+        prompt_attention_mask = text_inputs.attention_mask\n+        prompt_attention_mask = prompt_attention_mask.to(device)\n+\n+        prompt_embeds = self.text_encoder(text_input_ids.to(device), attention_mask=prompt_attention_mask)\n+        prompt_embeds = prompt_embeds[0].to(dtype=dtype, device=device)\n+\n+        return prompt_embeds, prompt_attention_mask\n+\n+    def encode_prompt(\n+        self,\n+        prompt: Union[str, List[str]],\n+        do_classifier_free_guidance: bool = True,\n+        negative_prompt: str = \"\",\n+        num_videos_per_prompt: int = 1,\n+        device: Optional[torch.device] = None,\n+        prompt_embeds: Optional[torch.Tensor] = None,\n+        negative_prompt_embeds: Optional[torch.Tensor] = None,\n+        prompt_attention_mask: Optional[torch.Tensor] = None,\n+        negative_prompt_attention_mask: Optional[torch.Tensor] = None,\n+        clean_caption: bool = False,\n+        max_sequence_length: int = 300,\n+        complex_human_instruction: Optional[List[str]] = None,\n+        lora_scale: Optional[float] = None,\n+    ):\n+        r\"\"\"\n+        Encodes the prompt into text encoder hidden states.\n+\n+        Args:\n+            prompt (`str` or `List[str]`, *optional*):\n+                prompt to be encoded\n+            negative_prompt (`str` or `List[str]`, *optional*):\n+                The prompt not to guide the video generation. If not defined, one has to pass `negative_prompt_embeds`\n+                instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is less than `1`). For\n+                PixArt-Alpha, this should be \"\".\n+            do_classifier_free_guidance (`bool`, *optional*, defaults to `True`):\n+                whether to use classifier free guidance or not\n+            num_videos_per_prompt (`int`, *optional*, defaults to 1):\n+                number of videos that should be generated per prompt\n+            device: (`torch.device`, *optional*):\n+                torch device to place the resulting embeddings on\n+            prompt_embeds (`torch.Tensor`, *optional*):\n+                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n+                provided, text embeddings will be generated from `prompt` input argument.\n+            negative_prompt_embeds (`torch.Tensor`, *optional*):\n+                Pre-generated negative text embeddings. For Sana, it's should be the embeddings of the \"\" string.\n+            clean_caption (`bool`, defaults to `False`):\n+                If `True`, the function will preprocess and clean the provided caption before encoding.\n+            max_sequence_length (`int`, defaults to 300): Maximum sequence length to use for the prompt.\n+            complex_human_instruction (`list[str]`, defaults to `complex_human_instruction`):\n+                If `complex_human_instruction` is not empty, the function will use the complex Human instruction for\n+                the prompt.\n+        \"\"\"\n+\n+        if device is None:\n+            device = self._execution_device\n+\n+        if self.text_encoder is not None:\n+            dtype = self.text_encoder.dtype\n+        else:\n+            dtype = None\n+\n+        # set lora scale so that monkey patched LoRA\n+        # function of text encoder can correctly access it\n+        if lora_scale is not None and isinstance(self, SanaLoraLoaderMixin):\n+            self._lora_scale = lora_scale\n+\n+            # dynamically adjust the LoRA scale\n+            if self.text_encoder is not None and USE_PEFT_BACKEND:\n+                scale_lora_layers(self.text_encoder, lora_scale)\n+\n+        if prompt is not None and isinstance(prompt, str):\n+            batch_size = 1\n+        elif prompt is not None and isinstance(prompt, list):\n+            batch_size = len(prompt)\n+        else:\n+            batch_size = prompt_embeds.shape[0]\n+\n+        if getattr(self, \"tokenizer\", None) is not None:\n+            self.tokenizer.padding_side = \"right\"\n+\n+        # See Section 3.1. of the paper.\n+        max_length = max_sequence_length\n+        select_index = [0] + list(range(-max_length + 1, 0))\n+\n+        if prompt_embeds is None:\n+            prompt_embeds, prompt_attention_mask = self._get_gemma_prompt_embeds(\n+                prompt=prompt,\n+                device=device,\n+                dtype=dtype,\n+                clean_caption=clean_caption,\n+                max_sequence_length=max_sequence_length,\n+                complex_human_instruction=complex_human_instruction,\n+            )\n+\n+            prompt_embeds = prompt_embeds[:, select_index]\n+            prompt_attention_mask = prompt_attention_mask[:, select_index]\n+\n+        bs_embed, seq_len, _ = prompt_embeds.shape\n+        # duplicate text embeddings and attention mask for each generation per prompt, using mps friendly method\n+        prompt_embeds = prompt_embeds.repeat(1, num_videos_per_prompt, 1)\n+        prompt_embeds = prompt_embeds.view(bs_embed * num_videos_per_prompt, seq_len, -1)\n+        prompt_attention_mask = prompt_attention_mask.view(bs_embed, -1)\n+        prompt_attention_mask = prompt_attention_mask.repeat(num_videos_per_prompt, 1)\n+\n+        # get unconditional embeddings for classifier free guidance\n+        if do_classifier_free_guidance and negative_prompt_embeds is None:\n+            negative_prompt = [negative_prompt] * batch_size if isinstance(negative_prompt, str) else negative_prompt\n+            negative_prompt_embeds, negative_prompt_attention_mask = self._get_gemma_prompt_embeds(\n+                prompt=negative_prompt,\n+                device=device,\n+                dtype=dtype,\n+                clean_caption=clean_caption,\n+                max_sequence_length=max_sequence_length,\n+                complex_human_instruction=False,\n+            )\n+\n+        if do_classifier_free_guidance:\n+            # duplicate unconditional embeddings for each generation per prompt, using mps friendly method\n+            seq_len = negative_prompt_embeds.shape[1]\n+\n+            negative_prompt_embeds = negative_prompt_embeds.to(dtype=dtype, device=device)\n+\n+            negative_prompt_embeds = negative_prompt_embeds.repeat(1, num_videos_per_prompt, 1)\n+            negative_prompt_embeds = negative_prompt_embeds.view(batch_size * num_videos_per_prompt, seq_len, -1)\n+\n+            negative_prompt_attention_mask = negative_prompt_attention_mask.view(bs_embed, -1)\n+            negative_prompt_attention_mask = negative_prompt_attention_mask.repeat(num_videos_per_prompt, 1)\n+        else:\n+            negative_prompt_embeds = None\n+            negative_prompt_attention_mask = None\n+\n+        if self.text_encoder is not None:\n+            if isinstance(self, SanaLoraLoaderMixin) and USE_PEFT_BACKEND:\n+                # Retrieve the original scale by scaling back the LoRA layers\n+                unscale_lora_layers(self.text_encoder, lora_scale)\n+\n+        return prompt_embeds, prompt_attention_mask, negative_prompt_embeds, negative_prompt_attention_mask\n+\n+    # Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline.prepare_extra_step_kwargs\n+    def prepare_extra_step_kwargs(self, generator, eta):\n+        # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n+        # eta (\u03b7) is only used with the DDIMScheduler, it will be ignored for other schedulers.\n+        # eta corresponds to \u03b7 in DDIM paper: https://huggingface.co/papers/2010.02502\n+        # and should be between [0, 1]\n+\n+        accepts_eta = \"eta\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n+        extra_step_kwargs = {}\n+        if accepts_eta:\n+            extra_step_kwargs[\"eta\"] = eta\n+\n+        # check if the scheduler accepts generator\n+        accepts_generator = \"generator\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n+        if accepts_generator:\n+            extra_step_kwargs[\"generator\"] = generator\n+        return extra_step_kwargs\n+\n+    def check_inputs(\n+        self,\n+        prompt,\n+        height,\n+        width,\n+        callback_on_step_end_tensor_inputs=None,\n+        negative_prompt=None,\n+        prompt_embeds=None,\n+        negative_prompt_embeds=None,\n+        prompt_attention_mask=None,\n+        negative_prompt_attention_mask=None,\n+    ):\n+        if height % 32 != 0 or width % 32 != 0:\n+            raise ValueError(f\"`height` and `width` have to be divisible by 32 but are {height} and {width}.\")\n+\n+        if callback_on_step_end_tensor_inputs is not None and not all(\n+            k in self._callback_tensor_inputs for k in callback_on_step_end_tensor_inputs\n+        ):\n+            raise ValueError(\n+                f\"`callback_on_step_end_tensor_inputs` has to be in {self._callback_tensor_inputs}, but found {[k for k in callback_on_step_end_tensor_inputs if k not in self._callback_tensor_inputs]}\"\n+            )\n+\n+        if prompt is not None and prompt_embeds is not None:\n+            raise ValueError(\n+                f\"Cannot forward both `prompt`: {prompt} and `prompt_embeds`: {prompt_embeds}. Please make sure to\"\n+                \" only forward one of the two.\"\n+            )\n+        elif prompt is None and prompt_embeds is None:\n+            raise ValueError(\n+                \"Provide either `prompt` or `prompt_embeds`. Cannot leave both `prompt` and `prompt_embeds` undefined.\"\n+            )\n+        elif prompt is not None and (not isinstance(prompt, str) and not isinstance(prompt, list)):\n+            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n+\n+        if prompt is not None and negative_prompt_embeds is not None:\n+            raise ValueError(\n+                f\"Cannot forward both `prompt`: {prompt} and `negative_prompt_embeds`:\"\n+                f\" {negative_prompt_embeds}. Please make sure to only forward one of the two.\"\n+            )\n+\n+        if negative_prompt is not None and negative_prompt_embeds is not None:\n+            raise ValueError(\n+                f\"Cannot forward both `negative_prompt`: {negative_prompt} and `negative_prompt_embeds`:\"\n+                f\" {negative_prompt_embeds}. Please make sure to only forward one of the two.\"\n+            )\n+\n+        if prompt_embeds is not None and prompt_attention_mask is None:\n+            raise ValueError(\"Must provide `prompt_attention_mask` when specifying `prompt_embeds`.\")\n+\n+        if negative_prompt_embeds is not None and negative_prompt_attention_mask is None:\n+            raise ValueError(\"Must provide `negative_prompt_attention_mask` when specifying `negative_prompt_embeds`.\")\n+\n+        if prompt_embeds is not None and negative_prompt_embeds is not None:\n+            if prompt_embeds.shape != negative_prompt_embeds.shape:\n+                raise ValueError(\n+                    \"`prompt_embeds` and `negative_prompt_embeds` must have the same shape when passed directly, but\"\n+                    f\" got: `prompt_embeds` {prompt_embeds.shape} != `negative_prompt_embeds`\"\n+                    f\" {negative_prompt_embeds.shape}.\"\n+                )\n+            if prompt_attention_mask.shape != negative_prompt_attention_mask.shape:\n+                raise ValueError(\n+                    \"`prompt_attention_mask` and `negative_prompt_attention_mask` must have the same shape when passed directly, but\"\n+                    f\" got: `prompt_attention_mask` {prompt_attention_mask.shape} != `negative_prompt_attention_mask`\"\n+                    f\" {negative_prompt_attention_mask.shape}.\"\n+                )\n+\n+    # Copied from diffusers.pipelines.deepfloyd_if.pipeline_if.IFPipeline._text_preprocessing\n+    def _text_preprocessing(self, text, clean_caption=False):\n+        if clean_caption and not is_bs4_available():\n+            logger.warning(BACKENDS_MAPPING[\"bs4\"][-1].format(\"Setting `clean_caption=True`\"))\n+            logger.warning(\"Setting `clean_caption` to False...\")\n+            clean_caption = False\n+\n+        if clean_caption and not is_ftfy_available():\n+            logger.warning(BACKENDS_MAPPING[\"ftfy\"][-1].format(\"Setting `clean_caption=True`\"))\n+            logger.warning(\"Setting `clean_caption` to False...\")\n+            clean_caption = False\n+\n+        if not isinstance(text, (tuple, list)):\n+            text = [text]\n+\n+        def process(text: str):\n+            if clean_caption:\n+                text = self._clean_caption(text)\n+                text = self._clean_caption(text)\n+            else:\n+                text = text.lower().strip()\n+            return text\n+\n+        return [process(t) for t in text]\n+\n+    # Copied from diffusers.pipelines.deepfloyd_if.pipeline_if.IFPipeline._clean_caption\n+    def _clean_caption(self, caption):\n+        caption = str(caption)\n+        caption = ul.unquote_plus(caption)\n+        caption = caption.strip().lower()\n+        caption = re.sub(\"<person>\", \"person\", caption)\n+        # urls:\n+        caption = re.sub(\n+            r\"\\b((?:https?:(?:\\/{1,3}|[a-zA-Z0-9%])|[a-zA-Z0-9.\\-]+[.](?:com|co|ru|net|org|edu|gov|it)[\\w/-]*\\b\\/?(?!@)))\",  # noqa\n+            \"\",\n+            caption,\n+        )  # regex for urls\n+        caption = re.sub(\n+            r\"\\b((?:www:(?:\\/{1,3}|[a-zA-Z0-9%])|[a-zA-Z0-9.\\-]+[.](?:com|co|ru|net|org|edu|gov|it)[\\w/-]*\\b\\/?(?!@)))\",  # noqa\n+            \"\",\n+            caption,\n+        )  # regex for urls\n+        # html:\n+        caption = BeautifulSoup(caption, features=\"html.parser\").text\n+\n+        # @<nickname>\n+        caption = re.sub(r\"@[\\w\\d]+\\b\", \"\", caption)\n+\n+        # 31C0\u201431EF CJK Strokes\n+        # 31F0\u201431FF Katakana Phonetic Extensions\n+        # 3200\u201432FF Enclosed CJK Letters and Months\n+        # 3300\u201433FF CJK Compatibility\n+        # 3400\u20144DBF CJK Unified Ideographs Extension A\n+        # 4DC0\u20144DFF Yijing Hexagram Symbols\n+        # 4E00\u20149FFF CJK Unified Ideographs\n+        caption = re.sub(r\"[\\u31c0-\\u31ef]+\", \"\", caption)\n+        caption = re.sub(r\"[\\u31f0-\\u31ff]+\", \"\", caption)\n+        caption = re.sub(r\"[\\u3200-\\u32ff]+\", \"\", caption)\n+        caption = re.sub(r\"[\\u3300-\\u33ff]+\", \"\", caption)\n+        caption = re.sub(r\"[\\u3400-\\u4dbf]+\", \"\", caption)\n+        caption = re.sub(r\"[\\u4dc0-\\u4dff]+\", \"\", caption)\n+        caption = re.sub(r\"[\\u4e00-\\u9fff]+\", \"\", caption)\n+        #######################################################\n+\n+        # \u0432\u0441\u0435 \u0432\u0438\u0434\u044b \u0442\u0438\u0440\u0435 / all types of dash --> \"-\"\n+        caption = re.sub(\n+            r\"[\\u002D\\u058A\\u05BE\\u1400\\u1806\\u2010-\\u2015\\u2E17\\u2E1A\\u2E3A\\u2E3B\\u2E40\\u301C\\u3030\\u30A0\\uFE31\\uFE32\\uFE58\\uFE63\\uFF0D]+\",  # noqa\n+            \"-\",\n+            caption,\n+        )\n+\n+        # \u043a\u0430\u0432\u044b\u0447\u043a\u0438 \u043a \u043e\u0434\u043d\u043e\u043c\u0443 \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u0443\n+        caption = re.sub(r\"[`\u00b4\u00ab\u00bb\u201c\u201d\u00a8]\", '\"', caption)\n+        caption = re.sub(r\"[\u2018\u2019]\", \"'\", caption)\n+\n+        # &quot;\n+        caption = re.sub(r\"&quot;?\", \"\", caption)\n+        # &amp\n+        caption = re.sub(r\"&amp\", \"\", caption)\n+\n+        # ip addresses:\n+        caption = re.sub(r\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\", \" \", caption)\n+\n+        # article ids:\n+        caption = re.sub(r\"\\d:\\d\\d\\s+$\", \"\", caption)\n+\n+        # \\n\n+        caption = re.sub(r\"\\\\n\", \" \", caption)\n+\n+        # \"#123\"\n+        caption = re.sub(r\"#\\d{1,3}\\b\", \"\", caption)\n+        # \"#12345..\"\n+        caption = re.sub(r\"#\\d{5,}\\b\", \"\", caption)\n+        # \"123456..\"\n+        caption = re.sub(r\"\\b\\d{6,}\\b\", \"\", caption)\n+        # filenames:\n+        caption = re.sub(r\"[\\S]+\\.(?:png|jpg|jpeg|bmp|webp|eps|pdf|apk|mp4)\", \"\", caption)\n+\n+        #\n+        caption = re.sub(r\"[\\\"\\']{2,}\", r'\"', caption)  # \"\"\"AUSVERKAUFT\"\"\"\n+        caption = re.sub(r\"[\\.]{2,}\", r\" \", caption)  # \"\"\"AUSVERKAUFT\"\"\"\n+\n+        caption = re.sub(self.bad_punct_regex, r\" \", caption)  # ***AUSVERKAUFT***, #AUSVERKAUFT\n+        caption = re.sub(r\"\\s+\\.\\s+\", r\" \", caption)  # \" . \"\n+\n+        # this-is-my-cute-cat / this_is_my_cute_cat\n+        regex2 = re.compile(r\"(?:\\-|\\_)\")\n+        if len(re.findall(regex2, caption)) > 3:\n+            caption = re.sub(regex2, \" \", caption)\n+\n+        caption = ftfy.fix_text(caption)\n+        caption = html.unescape(html.unescape(caption))\n+\n+        caption = re.sub(r\"\\b[a-zA-Z]{1,3}\\d{3,15}\\b\", \"\", caption)  # jc6640\n+        caption = re.sub(r\"\\b[a-zA-Z]+\\d+[a-zA-Z]+\\b\", \"\", caption)  # jc6640vc\n+        caption = re.sub(r\"\\b\\d+[a-zA-Z]+\\d+\\b\", \"\", caption)  # 6640vc231\n+\n+        caption = re.sub(r\"(worldwide\\s+)?(free\\s+)?shipping\", \"\", caption)\n+        caption = re.sub(r\"(free\\s)?download(\\sfree)?\", \"\", caption)\n+        caption = re.sub(r\"\\bclick\\b\\s(?:for|on)\\s\\w+\", \"\", caption)\n+        caption = re.sub(r\"\\b(?:png|jpg|jpeg|bmp|webp|eps|pdf|apk|mp4)(\\simage[s]?)?\", \"\", caption)\n+        caption = re.sub(r\"\\bpage\\s+\\d+\\b\", \"\", caption)\n+\n+        caption = re.sub(r\"\\b\\d*[a-zA-Z]+\\d+[a-zA-Z]+\\d+[a-zA-Z\\d]*\\b\", r\" \", caption)  # j2d1a2a...\n+\n+        caption = re.sub(r\"\\b\\d+\\.?\\d*[x\u0445\u00d7]\\d+\\.?\\d*\\b\", \"\", caption)\n+\n+        caption = re.sub(r\"\\b\\s+\\:\\s+\", r\": \", caption)\n+        caption = re.sub(r\"(\\D[,\\./])\\b\", r\"\\1 \", caption)\n+        caption = re.sub(r\"\\s+\", \" \", caption)\n+\n+        caption.strip()\n+\n+        caption = re.sub(r\"^[\\\"\\']([\\w\\W]+)[\\\"\\']$\", r\"\\1\", caption)\n+        caption = re.sub(r\"^[\\'\\_,\\-\\:;]\", r\"\", caption)\n+        caption = re.sub(r\"[\\'\\_,\\-\\:\\-\\+]$\", r\"\", caption)\n+        caption = re.sub(r\"^\\.\\S+$\", \"\", caption)\n+\n+        return caption.strip()\n+\n+    def prepare_latents(\n+        self,\n+        batch_size: int,\n+        num_channels_latents: int = 16,\n+        height: int = 480,\n+        width: int = 832,\n+        num_frames: int = 81,\n+        dtype: Optional[torch.dtype] = None,\n+        device: Optional[torch.device] = None,\n+        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n+        latents: Optional[torch.Tensor] = None,\n+    ) -> torch.Tensor:\n+        if latents is not None:\n+            return latents.to(device=device, dtype=dtype)\n+\n+        num_latent_frames = (num_frames - 1) // self.vae_scale_factor_temporal + 1\n+        shape = (\n+            batch_size,\n+            num_channels_latents,\n+            num_latent_frames,\n+            int(height) // self.vae_scale_factor_spatial,\n+            int(width) // self.vae_scale_factor_spatial,\n+        )\n+        if isinstance(generator, list) and len(generator) != batch_size:\n+            raise ValueError(\n+                f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n+                f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n+            )\n+\n+        if latents is None:\n+            latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n+        else:\n+            latents = latents.to(device=device, dtype=dtype)\n+        return latents\n+\n+    @property\n+    def guidance_scale(self):\n+        return self._guidance_scale\n+\n+    @property\n+    def attention_kwargs(self):\n+        return self._attention_kwargs\n+\n+    @property\n+    def do_classifier_free_guidance(self):\n+        return self._guidance_scale > 1.0\n+\n+    @property\n+    def num_timesteps(self):\n+        return self._num_timesteps\n+\n+    @property\n+    def interrupt(self):\n+        return self._interrupt\n+\n+    @torch.no_grad()\n+    @replace_example_docstring(EXAMPLE_DOC_STRING)\n+    def __call__(\n+        self,\n+        prompt: Union[str, List[str]] = None,\n+        negative_prompt: str = \"\",\n+        num_inference_steps: int = 50,\n+        timesteps: List[int] = None,\n+        sigmas: List[float] = None,\n+        guidance_scale: float = 6.0,\n+        num_videos_per_prompt: Optional[int] = 1,\n+        height: int = 480,\n+        width: int = 832,\n+        frames: int = 81,\n+        eta: float = 0.0,\n+        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n+        latents: Optional[torch.Tensor] = None,\n+        prompt_embeds: Optional[torch.Tensor] = None,\n+        prompt_attention_mask: Optional[torch.Tensor] = None,\n+        negative_prompt_embeds: Optional[torch.Tensor] = None,\n+        negative_prompt_attention_mask: Optional[torch.Tensor] = None,\n+        output_type: Optional[str] = \"pil\",\n+        return_dict: bool = True,\n+        clean_caption: bool = False,\n+        use_resolution_binning: bool = True,\n+        attention_kwargs: Optional[Dict[str, Any]] = None,\n+        callback_on_step_end: Optional[Callable[[int, int, Dict], None]] = None,\n+        callback_on_step_end_tensor_inputs: List[str] = [\"latents\"],\n+        max_sequence_length: int = 300,\n+        complex_human_instruction: List[str] = [\n+            \"Given a user prompt, generate an 'Enhanced prompt' that provides detailed visual descriptions suitable for video generation. Evaluate the level of detail in the user prompt:\",\n+            \"- If the prompt is simple, focus on adding specifics about colors, shapes, sizes, textures, motion, and temporal relationships to create vivid and dynamic scenes.\",\n+            \"- If the prompt is already detailed, refine and enhance the existing details slightly without overcomplicating.\",\n+            \"Here are examples of how to transform or refine prompts:\",\n+            \"- User Prompt: A cat sleeping -> Enhanced: A small, fluffy white cat slowly settling into a curled position, peacefully falling asleep on a warm sunny windowsill, with gentle sunlight filtering through surrounding pots of blooming red flowers.\",\n+            \"- User Prompt: A busy city street -> Enhanced: A bustling city street scene at dusk, featuring glowing street lamps gradually lighting up, a diverse crowd of people in colorful clothing walking past, and a double-decker bus smoothly passing by towering glass skyscrapers.\",\n+            \"Please generate only the enhanced description for the prompt below and avoid including any additional commentary or evaluations:\",\n+            \"User Prompt: \",\n+        ],\n+    ) -> Union[SanaVideoPipelineOutput, Tuple]:\n+        \"\"\"\n+        Function invoked when calling the pipeline for generation.\n+\n+        Args:\n+            prompt (`str` or `List[str]`, *optional*):\n+                The prompt or prompts to guide the video generation. If not defined, one has to pass `prompt_embeds`.\n+                instead.\n+            negative_prompt (`str` or `List[str]`, *optional*):\n+                The prompt or prompts not to guide the video generation. If not defined, one has to pass\n+                `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is\n+                less than `1`).\n+            num_inference_steps (`int`, *optional*, defaults to 50):\n+                The number of denoising steps. More denoising steps usually lead to a higher quality video at the\n+                expense of slower inference.\n+            timesteps (`List[int]`, *optional*):\n+                Custom timesteps to use for the denoising process with schedulers which support a `timesteps` argument\n+                in their `set_timesteps` method. If not defined, the default behavior when `num_inference_steps` is\n+                passed will be used. Must be in descending order.\n+            sigmas (`List[float]`, *optional*):\n+                Custom sigmas to use for the denoising process with schedulers which support a `sigmas` argument in\n+                their `set_timesteps` method. If not defined, the default behavior when `num_inference_steps` is passed\n+                will be used.\n+            guidance_scale (`float`, *optional*, defaults to 4.5):\n+                Guidance scale as defined in [Classifier-Free Diffusion\n+                Guidance](https://huggingface.co/papers/2207.12598). `guidance_scale` is defined as `w` of equation 2.\n+                of [Imagen Paper](https://huggingface.co/papers/2205.11487). Guidance scale is enabled by setting\n+                `guidance_scale > 1`. Higher guidance scale encourages to generate videos that are closely linked to\n+                the text `prompt`, usually at the expense of lower video quality.\n+            num_videos_per_prompt (`int`, *optional*, defaults to 1):\n+                The number of videos to generate per prompt.\n+            height (`int`, *optional*, defaults to 480):\n+                The height in pixels of the generated video.\n+            width (`int`, *optional*, defaults to 832):\n+                The width in pixels of the generated video.\n+            frames (`int`, *optional*, defaults to 81):\n+                The number of frames in the generated video.\n+            eta (`float`, *optional*, defaults to 0.0):\n+                Corresponds to parameter eta (\u03b7) in the DDIM paper: https://huggingface.co/papers/2010.02502. Only\n+                applies to [`schedulers.DDIMScheduler`], will be ignored for others.\n+            generator (`torch.Generator` or `List[torch.Generator]`, *optional*):\n+                One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)\n+                to make generation deterministic.\n+            latents (`torch.Tensor`, *optional*):\n+                Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for video\n+                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents\n+                tensor will be generated by sampling using the supplied random `generator`.\n+            prompt_embeds (`torch.Tensor`, *optional*):\n+                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n+                provided, text embeddings will be generated from `prompt` input argument.\n+            prompt_attention_mask (`torch.Tensor`, *optional*): Pre-generated attention mask for text embeddings.\n+            negative_prompt_embeds (`torch.Tensor`, *optional*):\n+                Pre-generated negative text embeddings. For PixArt-Sigma this negative prompt should be \"\". If not\n+                provided, negative_prompt_embeds will be generated from `negative_prompt` input argument.\n+            negative_prompt_attention_mask (`torch.Tensor`, *optional*):\n+                Pre-generated attention mask for negative text embeddings.\n+            output_type (`str`, *optional*, defaults to `\"pil\"`):\n+                The output format of the generated video. Choose between mp4 or `np.array`.\n+            return_dict (`bool`, *optional*, defaults to `True`):\n+                Whether or not to return a [`SanaVideoPipelineOutput`] instead of a plain tuple.\n+            attention_kwargs:\n+                A kwargs dictionary that if specified is passed along to the `AttentionProcessor` as defined under\n+                `self.processor` in\n+                [diffusers.models.attention_processor](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).\n+            clean_caption (`bool`, *optional*, defaults to `True`):\n+                Whether or not to clean the caption before creating embeddings. Requires `beautifulsoup4` and `ftfy` to\n+                be installed. If the dependencies are not installed, the embeddings will be created from the raw\n+                prompt.\n+            use_resolution_binning (`bool` defaults to `True`):\n+                If set to `True`, the requested height and width are first mapped to the closest resolutions using\n+                `ASPECT_RATIO_480_BIN` or `ASPECT_RATIO_720_BIN`. After the produced latents are decoded into videos,\n+                they are resized back to the requested resolution. Useful for generating non-square videos.\n+            callback_on_step_end (`Callable`, *optional*):\n+                A function that calls at the end of each denoising steps during the inference. The function is called\n+                with the following arguments: `callback_on_step_end(self: DiffusionPipeline, step: int, timestep: int,\n+                callback_kwargs: Dict)`. `callback_kwargs` will include a list of all tensors as specified by\n+                `callback_on_step_end_tensor_inputs`.\n+            callback_on_step_end_tensor_inputs (`List`, *optional*):\n+                The list of tensor inputs for the `callback_on_step_end` function. The tensors specified in the list\n+                will be passed as `callback_kwargs` argument. You will only be able to include variables listed in the\n+                `._callback_tensor_inputs` attribute of your pipeline class.\n+            max_sequence_length (`int` defaults to `300`):\n+                Maximum sequence length to use with the `prompt`.\n+            complex_human_instruction (`List[str]`, *optional*):\n+                Instructions for complex human attention:\n+                https://github.com/NVlabs/Sana/blob/main/configs/sana_app_config/Sana_1600M_app.yaml#L55.\n+\n+        Examples:\n+\n+        Returns:\n+            [`~pipelines.sana.pipeline_output.SanaVideoPipelineOutput`] or `tuple`:\n+                If `return_dict` is `True`, [`~pipelines.sana.pipeline_output.SanaVideoPipelineOutput`] is returned,\n+                otherwise a `tuple` is returned where the first element is a list with the generated videos\n+        \"\"\"\n+\n+        if isinstance(callback_on_step_end, (PipelineCallback, MultiPipelineCallbacks)):\n+            callback_on_step_end_tensor_inputs = callback_on_step_end.tensor_inputs\n+\n+        # 1. Check inputs. Raise error if not correct\n+        if use_resolution_binning:\n+            if self.transformer.config.sample_size == 30:\n+                aspect_ratio_bin = ASPECT_RATIO_480_BIN\n+            elif self.transformer.config.sample_size == 22:\n+                aspect_ratio_bin = ASPECT_RATIO_720_BIN\n+            else:\n+                raise ValueError(\"Invalid sample size\")\n+            orig_height, orig_width = height, width\n+            height, width = self.video_processor.classify_height_width_bin(height, width, ratios=aspect_ratio_bin)\n+\n+        self.check_inputs(\n+            prompt,\n+            height,\n+            width,\n+            callback_on_step_end_tensor_inputs,\n+            negative_prompt,\n+            prompt_embeds,\n+            negative_prompt_embeds,\n+            prompt_attention_mask,\n+            negative_prompt_attention_mask,\n+        )\n+\n+        self._guidance_scale = guidance_scale\n+        self._attention_kwargs = attention_kwargs\n+        self._interrupt = False\n+\n+        # 2. Default height and width to transformer\n+        if prompt is not None and isinstance(prompt, str):\n+            batch_size = 1\n+        elif prompt is not None and isinstance(prompt, list):\n+            batch_size = len(prompt)\n+        else:\n+            batch_size = prompt_embeds.shape[0]\n+\n+        device = self._execution_device\n+        lora_scale = self.attention_kwargs.get(\"scale\", None) if self.attention_kwargs is not None else None\n+\n+        # 3. Encode input prompt\n+        (\n+            prompt_embeds,\n+            prompt_attention_mask,\n+            negative_prompt_embeds,\n+            negative_prompt_attention_mask,\n+        ) = self.encode_prompt(\n+            prompt,\n+            self.do_classifier_free_guidance,\n+            negative_prompt=negative_prompt,\n+            num_videos_per_prompt=num_videos_per_prompt,\n+            device=device,\n+            prompt_embeds=prompt_embeds,\n+            negative_prompt_embeds=negative_prompt_embeds,\n+            prompt_attention_mask=prompt_attention_mask,\n+            negative_prompt_attention_mask=negative_prompt_attention_mask,\n+            clean_caption=clean_caption,\n+            max_sequence_length=max_sequence_length,\n+            complex_human_instruction=complex_human_instruction,\n+            lora_scale=lora_scale,\n+        )\n+        if self.do_classifier_free_guidance:\n+            prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds], dim=0)\n+            prompt_attention_mask = torch.cat([negative_prompt_attention_mask, prompt_attention_mask], dim=0)\n+\n+        # 4. Prepare timesteps\n+        timesteps, num_inference_steps = retrieve_timesteps(\n+            self.scheduler, num_inference_steps, device, timesteps, sigmas\n+        )\n+\n+        # 5. Prepare latents.\n+        latent_channels = self.transformer.config.in_channels\n+        latents = self.prepare_latents(\n+            batch_size * num_videos_per_prompt,\n+            latent_channels,\n+            height,\n+            width,\n+            frames,\n+            torch.float32,\n+            device,\n+            generator,\n+            latents,\n+        )\n+\n+        # 6. Prepare extra step kwargs. TODO: Logic should ideally just be moved out of the pipeline\n+        extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)\n+\n+        # 7. Denoising loop\n+        num_warmup_steps = max(len(timesteps) - num_inference_steps * self.scheduler.order, 0)\n+        self._num_timesteps = len(timesteps)\n+\n+        transformer_dtype = self.transformer.dtype\n+        with self.progress_bar(total=num_inference_steps) as progress_bar:\n+            for i, t in enumerate(timesteps):\n+                if self.interrupt:\n+                    continue\n+\n+                latent_model_input = torch.cat([latents] * 2) if self.do_classifier_free_guidance else latents\n+\n+                # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n+                timestep = t.expand(latent_model_input.shape[0])\n+\n+                # predict noise model_output\n+                noise_pred = self.transformer(\n+                    latent_model_input.to(dtype=transformer_dtype),\n+                    encoder_hidden_states=prompt_embeds.to(dtype=transformer_dtype),\n+                    encoder_attention_mask=prompt_attention_mask,\n+                    timestep=timestep,\n+                    return_dict=False,\n+                    attention_kwargs=self.attention_kwargs,\n+                )[0]\n+                noise_pred = noise_pred.float()\n+\n+                # perform guidance\n+                if self.do_classifier_free_guidance:\n+                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n+                    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n+\n+                # learned sigma\n+                if self.transformer.config.out_channels // 2 == latent_channels:\n+                    noise_pred = noise_pred.chunk(2, dim=1)[0]\n+\n+                # compute previous image: x_t -> x_t-1\n+                latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs, return_dict=False)[0]\n+\n+                if callback_on_step_end is not None:\n+                    callback_kwargs = {}\n+                    for k in callback_on_step_end_tensor_inputs:\n+                        callback_kwargs[k] = locals()[k]\n+                    callback_outputs = callback_on_step_end(self, i, t, callback_kwargs)\n+\n+                    latents = callback_outputs.pop(\"latents\", latents)\n+                    prompt_embeds = callback_outputs.pop(\"prompt_embeds\", prompt_embeds)\n+                    negative_prompt_embeds = callback_outputs.pop(\"negative_prompt_embeds\", negative_prompt_embeds)\n+\n+                # call the callback, if provided\n+                if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n+                    progress_bar.update()\n+\n+                if XLA_AVAILABLE:\n+                    xm.mark_step()\n+\n+        if output_type == \"latent\":\n+            video = latents\n+        else:\n+            latents = latents.to(self.vae.dtype)\n+            torch_accelerator_module = getattr(torch, get_device(), torch.cuda)\n+            oom_error = (\n+                torch.OutOfMemoryError\n+                if is_torch_version(\">=\", \"2.5.0\")\n+                else torch_accelerator_module.OutOfMemoryError\n+            )\n+            latents_mean = (\n+                torch.tensor(self.vae.config.latents_mean)\n+                .view(1, self.vae.config.z_dim, 1, 1, 1)\n+                .to(latents.device, latents.dtype)\n+            )\n+            latents_std = 1.0 / torch.tensor(self.vae.config.latents_std).view(1, self.vae.config.z_dim, 1, 1, 1).to(\n+                latents.device, latents.dtype\n+            )\n+            latents = latents / latents_std + latents_mean\n+            try:\n+                video = self.vae.decode(latents, return_dict=False)[0]\n+            except oom_error as e:\n+                warnings.warn(\n+                    f\"{e}. \\n\"\n+                    f\"Try to use VAE tiling for large images. For example: \\n\"\n+                    f\"pipe.vae.enable_tiling(tile_sample_min_width=512, tile_sample_min_height=512)\"\n+                )\n+\n+            if use_resolution_binning:\n+                video = self.video_processor.resize_and_crop_tensor(video, orig_width, orig_height)\n+\n+            video = self.video_processor.postprocess_video(video, output_type=output_type)\n+\n+        # Offload all models\n+        self.maybe_free_model_hooks()\n+\n+        if not return_dict:\n+            return (video,)\n+\n+        return SanaVideoPipelineOutput(frames=video)"
        },
        {
          "filename": "src/diffusers/utils/dummy_pt_objects.py",
          "status": "modified",
          "additions": 15,
          "deletions": 0,
          "changes": 15,
          "patch": "@@ -1308,6 +1308,21 @@ def from_pretrained(cls, *args, **kwargs):\n         requires_backends(cls, [\"torch\"])\n \n \n+class SanaVideoTransformer3DModel(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+    @classmethod\n+    def from_config(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\"])\n+\n+    @classmethod\n+    def from_pretrained(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\"])\n+\n+\n class SD3ControlNetModel(metaclass=DummyObject):\n     _backends = [\"torch\"]\n "
        },
        {
          "filename": "src/diffusers/utils/dummy_torch_and_transformers_objects.py",
          "status": "modified",
          "additions": 15,
          "deletions": 0,
          "changes": 15,
          "patch": "@@ -2177,6 +2177,21 @@ def from_pretrained(cls, *args, **kwargs):\n         requires_backends(cls, [\"torch\", \"transformers\"])\n \n \n+class SanaVideoPipeline(metaclass=DummyObject):\n+    _backends = [\"torch\", \"transformers\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_config(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_pretrained(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+\n class SemanticStableDiffusionPipeline(metaclass=DummyObject):\n     _backends = [\"torch\", \"transformers\"]\n "
        },
        {
          "filename": "src/diffusers/video_processor.py",
          "status": "modified",
          "additions": 64,
          "deletions": 1,
          "changes": 65,
          "patch": "@@ -13,11 +13,12 @@\n # limitations under the License.\n \n import warnings\n-from typing import List, Optional, Union\n+from typing import List, Optional, Tuple, Union\n \n import numpy as np\n import PIL\n import torch\n+import torch.nn.functional as F\n \n from .image_processor import VaeImageProcessor, is_valid_image, is_valid_image_imagelist\n \n@@ -111,3 +112,65 @@ def postprocess_video(\n             raise ValueError(f\"{output_type} does not exist. Please choose one of ['np', 'pt', 'pil']\")\n \n         return outputs\n+\n+    @staticmethod\n+    def classify_height_width_bin(height: int, width: int, ratios: dict) -> Tuple[int, int]:\n+        r\"\"\"\n+        Returns the binned height and width based on the aspect ratio.\n+\n+        Args:\n+            height (`int`): The height of the image.\n+            width (`int`): The width of the image.\n+            ratios (`dict`): A dictionary where keys are aspect ratios and values are tuples of (height, width).\n+\n+        Returns:\n+            `Tuple[int, int]`: The closest binned height and width.\n+        \"\"\"\n+        ar = float(height / width)\n+        closest_ratio = min(ratios.keys(), key=lambda ratio: abs(float(ratio) - ar))\n+        default_hw = ratios[closest_ratio]\n+        return int(default_hw[0]), int(default_hw[1])\n+\n+    @staticmethod\n+    def resize_and_crop_tensor(samples: torch.Tensor, new_width: int, new_height: int) -> torch.Tensor:\n+        r\"\"\"\n+        Resizes and crops a tensor of videos to the specified dimensions.\n+\n+        Args:\n+            samples (`torch.Tensor`):\n+                A tensor of shape (N, C, T, H, W) where N is the batch size, C is the number of channels, T is the\n+                number of frames, H is the height, and W is the width.\n+            new_width (`int`): The desired width of the output videos.\n+            new_height (`int`): The desired height of the output videos.\n+\n+        Returns:\n+            `torch.Tensor`: A tensor containing the resized and cropped videos.\n+        \"\"\"\n+        orig_height, orig_width = samples.shape[3], samples.shape[4]\n+\n+        # Check if resizing is needed\n+        if orig_height != new_height or orig_width != new_width:\n+            ratio = max(new_height / orig_height, new_width / orig_width)\n+            resized_width = int(orig_width * ratio)\n+            resized_height = int(orig_height * ratio)\n+\n+            # Reshape to (N*T, C, H, W) for interpolation\n+            n, c, t, h, w = samples.shape\n+            samples = samples.permute(0, 2, 1, 3, 4).reshape(n * t, c, h, w)\n+\n+            # Resize\n+            samples = F.interpolate(\n+                samples, size=(resized_height, resized_width), mode=\"bilinear\", align_corners=False\n+            )\n+\n+            # Center Crop\n+            start_x = (resized_width - new_width) // 2\n+            end_x = start_x + new_width\n+            start_y = (resized_height - new_height) // 2\n+            end_y = start_y + new_height\n+            samples = samples[:, :, start_y:end_y, start_x:end_x]\n+\n+            # Reshape back to (N, C, T, H, W)\n+            samples = samples.reshape(n, t, c, new_height, new_width).permute(0, 2, 1, 3, 4)\n+\n+        return samples"
        },
        {
          "filename": "tests/models/transformers/test_models_transformer_sana_video.py",
          "status": "added",
          "additions": 97,
          "deletions": 0,
          "changes": 97,
          "patch": "@@ -0,0 +1,97 @@\n+# Copyright 2025 HuggingFace Inc.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import unittest\n+\n+import torch\n+\n+from diffusers import SanaVideoTransformer3DModel\n+\n+from ...testing_utils import (\n+    enable_full_determinism,\n+    torch_device,\n+)\n+from ..test_modeling_common import ModelTesterMixin, TorchCompileTesterMixin\n+\n+\n+enable_full_determinism()\n+\n+\n+class SanaVideoTransformer3DTests(ModelTesterMixin, unittest.TestCase):\n+    model_class = SanaVideoTransformer3DModel\n+    main_input_name = \"hidden_states\"\n+    uses_custom_attn_processor = True\n+\n+    @property\n+    def dummy_input(self):\n+        batch_size = 1\n+        num_channels = 16\n+        num_frames = 2\n+        height = 16\n+        width = 16\n+        text_encoder_embedding_dim = 16\n+        sequence_length = 12\n+\n+        hidden_states = torch.randn((batch_size, num_channels, num_frames, height, width)).to(torch_device)\n+        timestep = torch.randint(0, 1000, size=(batch_size,)).to(torch_device)\n+        encoder_hidden_states = torch.randn((batch_size, sequence_length, text_encoder_embedding_dim)).to(torch_device)\n+\n+        return {\n+            \"hidden_states\": hidden_states,\n+            \"encoder_hidden_states\": encoder_hidden_states,\n+            \"timestep\": timestep,\n+        }\n+\n+    @property\n+    def input_shape(self):\n+        return (16, 2, 16, 16)\n+\n+    @property\n+    def output_shape(self):\n+        return (16, 2, 16, 16)\n+\n+    def prepare_init_args_and_inputs_for_common(self):\n+        init_dict = {\n+            \"in_channels\": 16,\n+            \"out_channels\": 16,\n+            \"num_attention_heads\": 2,\n+            \"attention_head_dim\": 12,\n+            \"num_layers\": 2,\n+            \"num_cross_attention_heads\": 2,\n+            \"cross_attention_head_dim\": 12,\n+            \"cross_attention_dim\": 24,\n+            \"caption_channels\": 16,\n+            \"mlp_ratio\": 2.5,\n+            \"dropout\": 0.0,\n+            \"attention_bias\": False,\n+            \"sample_size\": 8,\n+            \"patch_size\": (1, 2, 2),\n+            \"norm_elementwise_affine\": False,\n+            \"norm_eps\": 1e-6,\n+            \"qk_norm\": \"rms_norm_across_heads\",\n+            \"rope_max_seq_len\": 32,\n+        }\n+        inputs_dict = self.dummy_input\n+        return init_dict, inputs_dict\n+\n+    def test_gradient_checkpointing_is_applied(self):\n+        expected_set = {\"SanaVideoTransformer3DModel\"}\n+        super().test_gradient_checkpointing_is_applied(expected_set=expected_set)\n+\n+\n+class SanaVideoTransformerCompileTests(TorchCompileTesterMixin, unittest.TestCase):\n+    model_class = SanaVideoTransformer3DModel\n+\n+    def prepare_init_args_and_inputs_for_common(self):\n+        return SanaVideoTransformer3DTests().prepare_init_args_and_inputs_for_common()"
        },
        {
          "filename": "tests/pipelines/sana/test_sana_video.py",
          "status": "added",
          "additions": 225,
          "deletions": 0,
          "changes": 225,
          "patch": "@@ -0,0 +1,225 @@\n+# Copyright 2025 The HuggingFace Team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import gc\n+import tempfile\n+import unittest\n+\n+import numpy as np\n+import torch\n+from transformers import Gemma2Config, Gemma2Model, GemmaTokenizer\n+\n+from diffusers import AutoencoderKLWan, DPMSolverMultistepScheduler, SanaVideoPipeline, SanaVideoTransformer3DModel\n+\n+from ...testing_utils import (\n+    backend_empty_cache,\n+    enable_full_determinism,\n+    require_torch_accelerator,\n+    slow,\n+    torch_device,\n+)\n+from ..pipeline_params import TEXT_TO_IMAGE_BATCH_PARAMS, TEXT_TO_IMAGE_IMAGE_PARAMS, TEXT_TO_IMAGE_PARAMS\n+from ..test_pipelines_common import PipelineTesterMixin\n+\n+\n+enable_full_determinism()\n+\n+\n+class SanaVideoPipelineFastTests(PipelineTesterMixin, unittest.TestCase):\n+    pipeline_class = SanaVideoPipeline\n+    params = TEXT_TO_IMAGE_PARAMS - {\"cross_attention_kwargs\"}\n+    batch_params = TEXT_TO_IMAGE_BATCH_PARAMS\n+    image_params = TEXT_TO_IMAGE_IMAGE_PARAMS\n+    image_latents_params = TEXT_TO_IMAGE_IMAGE_PARAMS\n+    required_optional_params = frozenset(\n+        [\n+            \"num_inference_steps\",\n+            \"generator\",\n+            \"latents\",\n+            \"return_dict\",\n+            \"callback_on_step_end\",\n+            \"callback_on_step_end_tensor_inputs\",\n+        ]\n+    )\n+    test_xformers_attention = False\n+    supports_dduf = False\n+\n+    def get_dummy_components(self):\n+        torch.manual_seed(0)\n+        vae = AutoencoderKLWan(\n+            base_dim=3,\n+            z_dim=16,\n+            dim_mult=[1, 1, 1, 1],\n+            num_res_blocks=1,\n+            temperal_downsample=[False, True, True],\n+        )\n+\n+        torch.manual_seed(0)\n+        scheduler = DPMSolverMultistepScheduler()\n+\n+        torch.manual_seed(0)\n+        text_encoder_config = Gemma2Config(\n+            head_dim=16,\n+            hidden_size=8,\n+            initializer_range=0.02,\n+            intermediate_size=64,\n+            max_position_embeddings=8192,\n+            model_type=\"gemma2\",\n+            num_attention_heads=2,\n+            num_hidden_layers=1,\n+            num_key_value_heads=2,\n+            vocab_size=8,\n+            attn_implementation=\"eager\",\n+        )\n+        text_encoder = Gemma2Model(text_encoder_config)\n+        tokenizer = GemmaTokenizer.from_pretrained(\"hf-internal-testing/dummy-gemma\")\n+\n+        torch.manual_seed(0)\n+        transformer = SanaVideoTransformer3DModel(\n+            in_channels=16,\n+            out_channels=16,\n+            num_attention_heads=2,\n+            attention_head_dim=12,\n+            num_layers=2,\n+            num_cross_attention_heads=2,\n+            cross_attention_head_dim=12,\n+            cross_attention_dim=24,\n+            caption_channels=8,\n+            mlp_ratio=2.5,\n+            dropout=0.0,\n+            attention_bias=False,\n+            sample_size=8,\n+            patch_size=(1, 2, 2),\n+            norm_elementwise_affine=False,\n+            norm_eps=1e-6,\n+            qk_norm=\"rms_norm_across_heads\",\n+            rope_max_seq_len=32,\n+        )\n+\n+        components = {\n+            \"transformer\": transformer,\n+            \"vae\": vae,\n+            \"scheduler\": scheduler,\n+            \"text_encoder\": text_encoder,\n+            \"tokenizer\": tokenizer,\n+        }\n+        return components\n+\n+    def get_dummy_inputs(self, device, seed=0):\n+        if str(device).startswith(\"mps\"):\n+            generator = torch.manual_seed(seed)\n+        else:\n+            generator = torch.Generator(device=device).manual_seed(seed)\n+        inputs = {\n+            \"prompt\": \"\",\n+            \"negative_prompt\": \"\",\n+            \"generator\": generator,\n+            \"num_inference_steps\": 2,\n+            \"guidance_scale\": 6.0,\n+            \"height\": 32,\n+            \"width\": 32,\n+            \"frames\": 9,\n+            \"max_sequence_length\": 16,\n+            \"output_type\": \"pt\",\n+            \"complex_human_instruction\": [],\n+            \"use_resolution_binning\": False,\n+        }\n+        return inputs\n+\n+    def test_inference(self):\n+        device = \"cpu\"\n+\n+        components = self.get_dummy_components()\n+        pipe = self.pipeline_class(**components)\n+        pipe.to(device)\n+        pipe.set_progress_bar_config(disable=None)\n+\n+        inputs = self.get_dummy_inputs(device)\n+        video = pipe(**inputs).frames\n+        generated_video = video[0]\n+        self.assertEqual(generated_video.shape, (9, 3, 32, 32))\n+\n+    @unittest.skip(\"Test not supported\")\n+    def test_attention_slicing_forward_pass(self):\n+        pass\n+\n+    def test_save_load_local(self, expected_max_difference=5e-4):\n+        components = self.get_dummy_components()\n+        pipe = self.pipeline_class(**components)\n+        for component in pipe.components.values():\n+            if hasattr(component, \"set_default_attn_processor\"):\n+                component.set_default_attn_processor()\n+        pipe.to(torch_device)\n+        pipe.set_progress_bar_config(disable=None)\n+\n+        inputs = self.get_dummy_inputs(torch_device)\n+        torch.manual_seed(0)\n+        output = pipe(**inputs)[0]\n+\n+        with tempfile.TemporaryDirectory() as tmpdir:\n+            pipe.save_pretrained(tmpdir, safe_serialization=False)\n+            pipe_loaded = self.pipeline_class.from_pretrained(tmpdir)\n+            for component in pipe_loaded.components.values():\n+                if hasattr(component, \"set_default_attn_processor\"):\n+                    component.set_default_attn_processor()\n+            pipe_loaded.to(torch_device)\n+            pipe_loaded.set_progress_bar_config(disable=None)\n+\n+        inputs = self.get_dummy_inputs(torch_device)\n+        torch.manual_seed(0)\n+        output_loaded = pipe_loaded(**inputs)[0]\n+\n+        max_diff = np.abs(output.detach().cpu().numpy() - output_loaded.detach().cpu().numpy()).max()\n+        self.assertLess(max_diff, expected_max_difference)\n+\n+    # TODO(aryan): Create a dummy gemma model with smol vocab size\n+    @unittest.skip(\n+        \"A very small vocab size is used for fast tests. So, any kind of prompt other than the empty default used in other tests will lead to a embedding lookup error. This test uses a long prompt that causes the error.\"\n+    )\n+    def test_inference_batch_consistent(self):\n+        pass\n+\n+    @unittest.skip(\n+        \"A very small vocab size is used for fast tests. So, any kind of prompt other than the empty default used in other tests will lead to a embedding lookup error. This test uses a long prompt that causes the error.\"\n+    )\n+    def test_inference_batch_single_identical(self):\n+        pass\n+\n+    def test_float16_inference(self):\n+        # Requires higher tolerance as model seems very sensitive to dtype\n+        super().test_float16_inference(expected_max_diff=0.08)\n+\n+    def test_save_load_float16(self):\n+        # Requires higher tolerance as model seems very sensitive to dtype\n+        super().test_save_load_float16(expected_max_diff=0.2)\n+\n+\n+@slow\n+@require_torch_accelerator\n+class SanaVideoPipelineIntegrationTests(unittest.TestCase):\n+    prompt = \"Evening, backlight, side lighting, soft light, high contrast, mid-shot, centered composition, clean solo shot, warm color. A young Caucasian man stands in a forest.\"\n+\n+    def setUp(self):\n+        super().setUp()\n+        gc.collect()\n+        backend_empty_cache(torch_device)\n+\n+    def tearDown(self):\n+        super().tearDown()\n+        gc.collect()\n+        backend_empty_cache(torch_device)\n+\n+    @unittest.skip(\"TODO: test needs to be implemented\")\n+    def test_sana_video_480p(self):\n+        pass"
        }
      ],
      "num_files": 20,
      "scraped_at": "2025-11-16T21:18:45.820186"
    },
    {
      "pr_number": 12566,
      "title": "[tests] add tests for flux modular (t2i, i2i, kontext)",
      "body": "# What does this PR do?\r\n\r\nThe tests also helped me uncover some bugs and fix them. Some comments are in line.",
      "html_url": "https://github.com/huggingface/diffusers/pull/12566",
      "created_at": "2025-10-31T12:06:23Z",
      "merged_at": "2025-11-02T05:21:11Z",
      "merge_commit_sha": "8f80dda193f79af3ccd0f985906d61123d69df08",
      "base_ref": "main",
      "head_sha": "dd4d639e4e9501d1502f93036f8cbf7c0f3ee2a2",
      "user": "sayakpaul",
      "files": [
        {
          "filename": "src/diffusers/modular_pipelines/components_manager.py",
          "status": "modified",
          "additions": 7,
          "deletions": 1,
          "changes": 8,
          "patch": "@@ -164,7 +164,11 @@ def __call__(self, hooks, model_id, model, execution_device):\n \n         device_type = execution_device.type\n         device_module = getattr(torch, device_type, torch.cuda)\n-        mem_on_device = device_module.mem_get_info(execution_device.index)[0]\n+        try:\n+            mem_on_device = device_module.mem_get_info(execution_device.index)[0]\n+        except AttributeError:\n+            raise AttributeError(f\"Do not know how to obtain obtain memory info for {str(device_module)}.\")\n+\n         mem_on_device = mem_on_device - self.memory_reserve_margin\n         if current_module_size < mem_on_device:\n             return []\n@@ -699,6 +703,8 @@ def enable_auto_cpu_offload(self, device: Union[str, int, torch.device] = None,\n         if not is_accelerate_available():\n             raise ImportError(\"Make sure to install accelerate to use auto_cpu_offload\")\n \n+        # TODO: add a warning if mem_get_info isn't available on `device`.\n+\n         for name, component in self.components.items():\n             if isinstance(component, torch.nn.Module) and hasattr(component, \"_hf_hook\"):\n                 remove_hook_from_module(component, recurse=True)"
        },
        {
          "filename": "src/diffusers/modular_pipelines/flux/before_denoise.py",
          "status": "modified",
          "additions": 1,
          "deletions": 1,
          "changes": 2,
          "patch": "@@ -598,7 +598,7 @@ def __call__(self, components: FluxModularPipeline, state: PipelineState) -> Pip\n             and getattr(block_state, \"image_width\", None) is not None\n         ):\n             image_latent_height = 2 * (int(block_state.image_height) // (components.vae_scale_factor * 2))\n-            image_latent_width = 2 * (int(block_state.width) // (components.vae_scale_factor * 2))\n+            image_latent_width = 2 * (int(block_state.image_width) // (components.vae_scale_factor * 2))\n             img_ids = FluxPipeline._prepare_latent_image_ids(\n                 None, image_latent_height // 2, image_latent_width // 2, device, dtype\n             )"
        },
        {
          "filename": "src/diffusers/modular_pipelines/flux/denoise.py",
          "status": "modified",
          "additions": 2,
          "deletions": 2,
          "changes": 4,
          "patch": "@@ -59,7 +59,7 @@ def inputs(self) -> List[Tuple[str, Any]]:\n             ),\n             InputParam(\n                 \"guidance\",\n-                required=True,\n+                required=False,\n                 type_hint=torch.Tensor,\n                 description=\"Guidance scale as a tensor\",\n             ),\n@@ -141,7 +141,7 @@ def inputs(self) -> List[Tuple[str, Any]]:\n             ),\n             InputParam(\n                 \"guidance\",\n-                required=True,\n+                required=False,\n                 type_hint=torch.Tensor,\n                 description=\"Guidance scale as a tensor\",\n             ),"
        },
        {
          "filename": "src/diffusers/modular_pipelines/flux/encoders.py",
          "status": "modified",
          "additions": 4,
          "deletions": 7,
          "changes": 11,
          "patch": "@@ -95,7 +95,7 @@ def expected_components(self) -> List[ComponentSpec]:\n             ComponentSpec(\n                 \"image_processor\",\n                 VaeImageProcessor,\n-                config=FrozenDict({\"vae_scale_factor\": 16}),\n+                config=FrozenDict({\"vae_scale_factor\": 16, \"vae_latent_channels\": 16}),\n                 default_creation_method=\"from_config\",\n             ),\n         ]\n@@ -143,10 +143,6 @@ def __call__(self, components: FluxModularPipeline, state: PipelineState):\n class FluxKontextProcessImagesInputStep(ModularPipelineBlocks):\n     model_name = \"flux-kontext\"\n \n-    def __init__(self, _auto_resize=True):\n-        self._auto_resize = _auto_resize\n-        super().__init__()\n-\n     @property\n     def description(self) -> str:\n         return (\n@@ -167,7 +163,7 @@ def expected_components(self) -> List[ComponentSpec]:\n \n     @property\n     def inputs(self) -> List[InputParam]:\n-        return [InputParam(\"image\")]\n+        return [InputParam(\"image\"), InputParam(\"_auto_resize\", type_hint=bool, default=True)]\n \n     @property\n     def intermediate_outputs(self) -> List[OutputParam]:\n@@ -195,7 +191,8 @@ def __call__(self, components: FluxModularPipeline, state: PipelineState):\n             img = images[0]\n             image_height, image_width = components.image_processor.get_default_height_width(img)\n             aspect_ratio = image_width / image_height\n-            if self._auto_resize:\n+            _auto_resize = block_state._auto_resize\n+            if _auto_resize:\n                 # Kontext is trained on specific resolutions, using one of them is recommended\n                 _, image_width, image_height = min(\n                     (abs(aspect_ratio - w / h), w, h) for w, h in PREFERRED_KONTEXT_RESOLUTIONS"
        },
        {
          "filename": "src/diffusers/modular_pipelines/flux/inputs.py",
          "status": "modified",
          "additions": 4,
          "deletions": 0,
          "changes": 4,
          "patch": "@@ -112,6 +112,10 @@ def __call__(self, components: FluxModularPipeline, state: PipelineState) -> Pip\n         block_state.prompt_embeds = block_state.prompt_embeds.view(\n             block_state.batch_size * block_state.num_images_per_prompt, seq_len, -1\n         )\n+        pooled_prompt_embeds = block_state.pooled_prompt_embeds.repeat(1, block_state.num_images_per_prompt)\n+        block_state.pooled_prompt_embeds = pooled_prompt_embeds.view(\n+            block_state.batch_size * block_state.num_images_per_prompt, -1\n+        )\n         self.set_block_state(state, block_state)\n \n         return components, state"
        },
        {
          "filename": "tests/modular_pipelines/flux/__init__.py",
          "status": "added",
          "additions": 0,
          "deletions": 0,
          "changes": 0,
          "patch": ""
        },
        {
          "filename": "tests/modular_pipelines/flux/test_modular_pipeline_flux.py",
          "status": "added",
          "additions": 130,
          "deletions": 0,
          "changes": 130,
          "patch": "@@ -0,0 +1,130 @@\n+# coding=utf-8\n+# Copyright 2025 HuggingFace Inc.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import random\n+import tempfile\n+import unittest\n+\n+import numpy as np\n+import PIL\n+import torch\n+\n+from diffusers.image_processor import VaeImageProcessor\n+from diffusers.modular_pipelines import (\n+    FluxAutoBlocks,\n+    FluxKontextAutoBlocks,\n+    FluxKontextModularPipeline,\n+    FluxModularPipeline,\n+    ModularPipeline,\n+)\n+\n+from ...testing_utils import floats_tensor, torch_device\n+from ..test_modular_pipelines_common import ModularPipelineTesterMixin\n+\n+\n+class FluxModularTests:\n+    pipeline_class = FluxModularPipeline\n+    pipeline_blocks_class = FluxAutoBlocks\n+    repo = \"hf-internal-testing/tiny-flux-modular\"\n+\n+    def get_pipeline(self, components_manager=None, torch_dtype=torch.float32):\n+        pipeline = self.pipeline_blocks_class().init_pipeline(self.repo, components_manager=components_manager)\n+        pipeline.load_components(torch_dtype=torch_dtype)\n+        return pipeline\n+\n+    def get_dummy_inputs(self, device, seed=0):\n+        if str(device).startswith(\"mps\"):\n+            generator = torch.manual_seed(seed)\n+        else:\n+            generator = torch.Generator(device=device).manual_seed(seed)\n+        inputs = {\n+            \"prompt\": \"A painting of a squirrel eating a burger\",\n+            \"generator\": generator,\n+            \"num_inference_steps\": 2,\n+            \"guidance_scale\": 5.0,\n+            \"height\": 8,\n+            \"width\": 8,\n+            \"max_sequence_length\": 48,\n+            \"output_type\": \"np\",\n+        }\n+        return inputs\n+\n+\n+class FluxModularPipelineFastTests(FluxModularTests, ModularPipelineTesterMixin, unittest.TestCase):\n+    params = frozenset([\"prompt\", \"height\", \"width\", \"guidance_scale\"])\n+    batch_params = frozenset([\"prompt\"])\n+\n+\n+class FluxImg2ImgModularPipelineFastTests(FluxModularTests, ModularPipelineTesterMixin, unittest.TestCase):\n+    params = frozenset([\"prompt\", \"height\", \"width\", \"guidance_scale\", \"image\"])\n+    batch_params = frozenset([\"prompt\", \"image\"])\n+\n+    def get_pipeline(self, components_manager=None, torch_dtype=torch.float32):\n+        pipeline = super().get_pipeline(components_manager, torch_dtype)\n+        # Override `vae_scale_factor` here as currently, `image_processor` is initialized with\n+        # fixed constants instead of\n+        # https://github.com/huggingface/diffusers/blob/d54622c2679d700b425ad61abce9b80fc36212c0/src/diffusers/pipelines/flux/pipeline_flux_img2img.py#L230C9-L232C10\n+        pipeline.image_processor = VaeImageProcessor(vae_scale_factor=2)\n+        return pipeline\n+\n+    def get_dummy_inputs(self, device, seed=0):\n+        inputs = super().get_dummy_inputs(device, seed)\n+        image = floats_tensor((1, 3, 32, 32), rng=random.Random(seed)).to(device)\n+        image = image / 2 + 0.5\n+        inputs[\"image\"] = image\n+        inputs[\"strength\"] = 0.8\n+        inputs[\"height\"] = 8\n+        inputs[\"width\"] = 8\n+        return inputs\n+\n+    def test_save_from_pretrained(self):\n+        pipes = []\n+        base_pipe = self.get_pipeline().to(torch_device)\n+        pipes.append(base_pipe)\n+\n+        with tempfile.TemporaryDirectory() as tmpdirname:\n+            base_pipe.save_pretrained(tmpdirname)\n+            pipe = ModularPipeline.from_pretrained(tmpdirname).to(torch_device)\n+            pipe.load_components(torch_dtype=torch.float32)\n+            pipe.to(torch_device)\n+            pipe.image_processor = VaeImageProcessor(vae_scale_factor=2)\n+\n+        pipes.append(pipe)\n+\n+        image_slices = []\n+        for pipe in pipes:\n+            inputs = self.get_dummy_inputs(torch_device)\n+            image = pipe(**inputs, output=\"images\")\n+\n+            image_slices.append(image[0, -3:, -3:, -1].flatten())\n+\n+        assert np.abs(image_slices[0] - image_slices[1]).max() < 1e-3\n+\n+\n+class FluxKontextModularPipelineFastTests(FluxImg2ImgModularPipelineFastTests):\n+    pipeline_class = FluxKontextModularPipeline\n+    pipeline_blocks_class = FluxKontextAutoBlocks\n+    repo = \"hf-internal-testing/tiny-flux-kontext-pipe\"\n+\n+    def get_dummy_inputs(self, device, seed=0):\n+        inputs = super().get_dummy_inputs(device, seed)\n+        image = PIL.Image.new(\"RGB\", (32, 32), 0)\n+        _ = inputs.pop(\"strength\")\n+        inputs[\"image\"] = image\n+        inputs[\"height\"] = 8\n+        inputs[\"width\"] = 8\n+        inputs[\"max_area\"] = 8 * 8\n+        inputs[\"_auto_resize\"] = False\n+        return inputs"
        },
        {
          "filename": "tests/modular_pipelines/stable_diffusion_xl/test_modular_pipeline_stable_diffusion_xl.py",
          "status": "modified",
          "additions": 4,
          "deletions": 16,
          "changes": 20,
          "patch": "@@ -21,24 +21,12 @@\n import torch\n from PIL import Image\n \n-from diffusers import (\n-    ClassifierFreeGuidance,\n-    StableDiffusionXLAutoBlocks,\n-    StableDiffusionXLModularPipeline,\n-)\n+from diffusers import ClassifierFreeGuidance, StableDiffusionXLAutoBlocks, StableDiffusionXLModularPipeline\n from diffusers.loaders import ModularIPAdapterMixin\n \n-from ...models.unets.test_models_unet_2d_condition import (\n-    create_ip_adapter_state_dict,\n-)\n-from ...testing_utils import (\n-    enable_full_determinism,\n-    floats_tensor,\n-    torch_device,\n-)\n-from ..test_modular_pipelines_common import (\n-    ModularPipelineTesterMixin,\n-)\n+from ...models.unets.test_models_unet_2d_condition import create_ip_adapter_state_dict\n+from ...testing_utils import enable_full_determinism, floats_tensor, torch_device\n+from ..test_modular_pipelines_common import ModularPipelineTesterMixin\n \n \n enable_full_determinism()"
        }
      ],
      "num_files": 8,
      "scraped_at": "2025-11-16T21:18:48.301884"
    },
    {
      "pr_number": 12551,
      "title": "[ci] don't run sana layerwise casting tests in CI.",
      "body": "# What does this PR do?\r\n\r\nTests pass locally perfectly fine on different GPUs (RT 4090, A100, H100).\r\n\r\nExample failures:\r\n* https://github.com/huggingface/diffusers/actions/runs/18846080835/job/53770552500?pr=12524#step:7:2966\r\n* https://github.com/huggingface/diffusers/actions/runs/18846080835/job/53770552496?pr=12524#step:7:9304",
      "html_url": "https://github.com/huggingface/diffusers/pull/12551",
      "created_at": "2025-10-27T15:43:08Z",
      "merged_at": "2025-10-28T07:59:52Z",
      "merge_commit_sha": "55d49d4379007740af20629bb61aba9546c6b053",
      "base_ref": "main",
      "head_sha": "651a6847df3596f80503121c934fae649ec43f5a",
      "user": "sayakpaul",
      "files": [
        {
          "filename": "tests/lora/test_lora_layers_sana.py",
          "status": "modified",
          "additions": 5,
          "deletions": 1,
          "changes": 6,
          "patch": "@@ -20,7 +20,7 @@\n \n from diffusers import AutoencoderDC, FlowMatchEulerDiscreteScheduler, SanaPipeline, SanaTransformer2DModel\n \n-from ..testing_utils import floats_tensor, require_peft_backend\n+from ..testing_utils import IS_GITHUB_ACTIONS, floats_tensor, require_peft_backend\n \n \n sys.path.append(\".\")\n@@ -136,3 +136,7 @@ def test_simple_inference_with_text_lora_fused(self):\n     @unittest.skip(\"Text encoder LoRA is not supported in SANA.\")\n     def test_simple_inference_with_text_lora_save_load(self):\n         pass\n+\n+    @unittest.skipIf(IS_GITHUB_ACTIONS, reason=\"Skipping test inside GitHub Actions environment\")\n+    def test_layerwise_casting_inference_denoiser(self):\n+        return super().test_layerwise_casting_inference_denoiser()"
        },
        {
          "filename": "tests/models/autoencoders/test_models_autoencoder_dc.py",
          "status": "modified",
          "additions": 5,
          "deletions": 5,
          "changes": 10,
          "patch": "@@ -17,11 +17,7 @@\n \n from diffusers import AutoencoderDC\n \n-from ...testing_utils import (\n-    enable_full_determinism,\n-    floats_tensor,\n-    torch_device,\n-)\n+from ...testing_utils import IS_GITHUB_ACTIONS, enable_full_determinism, floats_tensor, torch_device\n from ..test_modeling_common import ModelTesterMixin\n from .testing_utils import AutoencoderTesterMixin\n \n@@ -82,3 +78,7 @@ def prepare_init_args_and_inputs_for_common(self):\n         init_dict = self.get_autoencoder_dc_config()\n         inputs_dict = self.dummy_input\n         return init_dict, inputs_dict\n+\n+    @unittest.skipIf(IS_GITHUB_ACTIONS, reason=\"Skipping test inside GitHub Actions environment\")\n+    def test_layerwise_casting_inference(self):\n+        super().test_layerwise_casting_inference()"
        },
        {
          "filename": "tests/pipelines/sana/test_sana.py",
          "status": "modified",
          "additions": 5,
          "deletions": 0,
          "changes": 5,
          "patch": "@@ -23,6 +23,7 @@\n from diffusers import AutoencoderDC, FlowMatchEulerDiscreteScheduler, SanaPipeline, SanaTransformer2DModel\n \n from ...testing_utils import (\n+    IS_GITHUB_ACTIONS,\n     backend_empty_cache,\n     enable_full_determinism,\n     require_torch_accelerator,\n@@ -304,6 +305,10 @@ def test_float16_inference(self):\n         # Requires higher tolerance as model seems very sensitive to dtype\n         super().test_float16_inference(expected_max_diff=0.08)\n \n+    @unittest.skipIf(IS_GITHUB_ACTIONS, reason=\"Skipping test inside GitHub Actions environment\")\n+    def test_layerwise_casting_inference(self):\n+        super().test_layerwise_casting_inference()\n+\n \n @slow\n @require_torch_accelerator"
        },
        {
          "filename": "tests/pipelines/sana/test_sana_controlnet.py",
          "status": "modified",
          "additions": 5,
          "deletions": 4,
          "changes": 9,
          "patch": "@@ -28,10 +28,7 @@\n )\n from diffusers.utils.torch_utils import randn_tensor\n \n-from ...testing_utils import (\n-    enable_full_determinism,\n-    torch_device,\n-)\n+from ...testing_utils import IS_GITHUB_ACTIONS, enable_full_determinism, torch_device\n from ..pipeline_params import TEXT_TO_IMAGE_BATCH_PARAMS, TEXT_TO_IMAGE_IMAGE_PARAMS, TEXT_TO_IMAGE_PARAMS\n from ..test_pipelines_common import PipelineTesterMixin, to_np\n \n@@ -326,3 +323,7 @@ def test_inference_batch_single_identical(self):\n     def test_float16_inference(self):\n         # Requires higher tolerance as model seems very sensitive to dtype\n         super().test_float16_inference(expected_max_diff=0.08)\n+\n+    @unittest.skipIf(IS_GITHUB_ACTIONS, reason=\"Skipping test inside GitHub Actions environment\")\n+    def test_layerwise_casting_inference(self):\n+        super().test_layerwise_casting_inference()"
        },
        {
          "filename": "tests/pipelines/sana/test_sana_sprint.py",
          "status": "modified",
          "additions": 5,
          "deletions": 4,
          "changes": 9,
          "patch": "@@ -21,10 +21,7 @@\n \n from diffusers import AutoencoderDC, SanaSprintPipeline, SanaTransformer2DModel, SCMScheduler\n \n-from ...testing_utils import (\n-    enable_full_determinism,\n-    torch_device,\n-)\n+from ...testing_utils import IS_GITHUB_ACTIONS, enable_full_determinism, torch_device\n from ..pipeline_params import TEXT_TO_IMAGE_BATCH_PARAMS, TEXT_TO_IMAGE_IMAGE_PARAMS, TEXT_TO_IMAGE_PARAMS\n from ..test_pipelines_common import PipelineTesterMixin, to_np\n \n@@ -300,3 +297,7 @@ def test_inference_batch_single_identical(self):\n     def test_float16_inference(self):\n         # Requires higher tolerance as model seems very sensitive to dtype\n         super().test_float16_inference(expected_max_diff=0.08)\n+\n+    @unittest.skipIf(IS_GITHUB_ACTIONS, reason=\"Skipping test inside GitHub Actions environment\")\n+    def test_layerwise_casting_inference(self):\n+        super().test_layerwise_casting_inference()"
        },
        {
          "filename": "tests/pipelines/sana/test_sana_sprint_img2img.py",
          "status": "modified",
          "additions": 5,
          "deletions": 4,
          "changes": 9,
          "patch": "@@ -22,10 +22,7 @@\n from diffusers import AutoencoderDC, SanaSprintImg2ImgPipeline, SanaTransformer2DModel, SCMScheduler\n from diffusers.utils.torch_utils import randn_tensor\n \n-from ...testing_utils import (\n-    enable_full_determinism,\n-    torch_device,\n-)\n+from ...testing_utils import IS_GITHUB_ACTIONS, enable_full_determinism, torch_device\n from ..pipeline_params import (\n     IMAGE_TO_IMAGE_IMAGE_PARAMS,\n     TEXT_GUIDED_IMAGE_VARIATION_BATCH_PARAMS,\n@@ -312,3 +309,7 @@ def test_inference_batch_single_identical(self):\n     def test_float16_inference(self):\n         # Requires higher tolerance as model seems very sensitive to dtype\n         super().test_float16_inference(expected_max_diff=0.08)\n+\n+    @unittest.skipIf(IS_GITHUB_ACTIONS, reason=\"Skipping test inside GitHub Actions environment\")\n+    def test_layerwise_casting_inference(self):\n+        super().test_layerwise_casting_inference()"
        },
        {
          "filename": "tests/testing_utils.py",
          "status": "modified",
          "additions": 2,
          "deletions": 0,
          "changes": 2,
          "patch": "@@ -63,6 +63,8 @@\n     IS_CUDA_SYSTEM = False\n     IS_XPU_SYSTEM = False\n \n+IS_GITHUB_ACTIONS = os.getenv(\"GITHUB_ACTIONS\") == \"true\" and os.getenv(\"DIFFUSERS_IS_CI\") == \"yes\"\n+\n global_rng = random.Random()\n \n logger = get_logger(__name__)"
        }
      ],
      "num_files": 7,
      "scraped_at": "2025-11-16T21:18:49.656017"
    },
    {
      "pr_number": 12549,
      "title": "Add AITER attention backend",
      "body": "# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\n[AITER](https://github.com/ROCm/aiter) is AMD\u2019s centralized repository to support high performance AI operators such as attention kernels for AMD ROCm enabled accelerators. This PR adds support for FlashAttention through AITER by introducing a new attention backend.\r\n\r\nTest code for Flux inference below. Requires installation of `aiter>=0.15.0` and a supported ROCm enabled accelerator.\r\n```\r\nimport torch\r\nfrom diffusers import FluxPipeline, FluxTransformer2DModel, attention_backend\r\n\r\nmodel_id = \"black-forest-labs/FLUX.1-dev\"\r\ntransformer = FluxTransformer2DModel.from_pretrained(model_id, subfolder=\"transformer\", torch_dtype=torch.bfloat16, device_map=\"cuda\")\r\ntransformer.set_attention_backend(\"aiter\")\r\npipe = FluxPipeline.from_pretrained(\"black-forest-labs/FLUX.1-dev\", transformer=transformer, torch_dtype=torch.bfloat16)\r\npipe.text_encoder.to(\"cuda\")\r\npipe.text_encoder_2.to(\"cuda\")\r\npipe.vae.to(\"cuda\")\r\n\r\nprompt = \"A cat holding a sign that says 'hello world'\"\r\n\r\nimage = pipe(prompt, num_inference_steps=28, guidance_scale=4.0).images[0]\r\nimage.save(\"output.png\")\r\n```\r\n\r\nWe are interested in following up this PR by eventually also enabling AITER backend support for context parallelism across multiple devices as the feature becomes more mature.\r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/diffusers/blob/main/CONTRIBUTING.md)?\r\n- [x] Did you read our [philosophy doc](https://github.com/huggingface/diffusers/blob/main/PHILOSOPHY.md) (important for complex PRs)?\r\n- [ ] Was this discussed/approved via a GitHub issue or the [forum](https://discuss.huggingface.co/c/discussion-related-to-httpsgithubcomhuggingfacediffusers/63)? Please add a link to it if that's the case.\r\n- [x] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/diffusers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/diffusers/tree/main/docs#writing-source-documentation).\r\n- [x] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\ncc: @sayakpaul @DN6 for review and any comments\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @.\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nCore library:\r\n\r\n- Schedulers: @yiyixuxu\r\n- Pipelines and pipeline callbacks: @yiyixuxu and @asomoza\r\n- Training examples: @sayakpaul\r\n- Docs: @stevhliu and @sayakpaul\r\n- JAX and MPS: @pcuenca\r\n- Audio: @sanchit-gandhi\r\n- General functionalities: @sayakpaul @yiyixuxu @DN6\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @SunMarc\r\n- PEFT: @sayakpaul @BenjaminBossan\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- transformers: [different repo](https://github.com/huggingface/transformers)\r\n- safetensors: [different repo](https://github.com/huggingface/safetensors)\r\n\r\n-->\r\n",
      "html_url": "https://github.com/huggingface/diffusers/pull/12549",
      "created_at": "2025-10-27T09:51:54Z",
      "merged_at": "2025-10-27T14:55:02Z",
      "merge_commit_sha": "250f5cb53db1554f32dee07ad002f6c3834306d0",
      "base_ref": "main",
      "head_sha": "3df9fa7867a2cabf18f79ca5962962367334b5a7",
      "user": "lauri9",
      "files": [
        {
          "filename": "docs/source/en/optimization/attention_backends.md",
          "status": "modified",
          "additions": 2,
          "deletions": 0,
          "changes": 2,
          "patch": "@@ -21,6 +21,7 @@ Refer to the table below for an overview of the available attention families and\n | attention family | main feature |\n |---|---|\n | FlashAttention | minimizes memory reads/writes through tiling and recomputation |\n+| AI Tensor Engine for ROCm | FlashAttention implementation optimized for AMD ROCm accelerators |\n | SageAttention | quantizes attention to int8 |\n | PyTorch native | built-in PyTorch implementation using [scaled_dot_product_attention](./fp16#scaled-dot-product-attention) |\n | xFormers | memory-efficient attention with support for various attention kernels |\n@@ -139,6 +140,7 @@ Refer to the table below for a complete list of available attention backends and\n | `_native_xla` | [PyTorch native](https://docs.pytorch.org/docs/stable/generated/torch.nn.attention.SDPBackend.html#torch.nn.attention.SDPBackend) | XLA-optimized attention |\n | `flash` | [FlashAttention](https://github.com/Dao-AILab/flash-attention) | FlashAttention-2 |\n | `flash_varlen` | [FlashAttention](https://github.com/Dao-AILab/flash-attention) | Variable length FlashAttention |\n+| `aiter` | [AI Tensor Engine for ROCm](https://github.com/ROCm/aiter) | FlashAttention for AMD ROCm |\n | `_flash_3` | [FlashAttention](https://github.com/Dao-AILab/flash-attention) | FlashAttention-3 |\n | `_flash_varlen_3` | [FlashAttention](https://github.com/Dao-AILab/flash-attention) | Variable length FlashAttention-3 |\n | `_flash_3_hub` | [FlashAttention](https://github.com/Dao-AILab/flash-attention) | FlashAttention-3 from kernels |"
        },
        {
          "filename": "src/diffusers/models/attention_dispatch.py",
          "status": "modified",
          "additions": 60,
          "deletions": 0,
          "changes": 60,
          "patch": "@@ -27,6 +27,8 @@\n \n from ..utils import (\n     get_logger,\n+    is_aiter_available,\n+    is_aiter_version,\n     is_flash_attn_3_available,\n     is_flash_attn_available,\n     is_flash_attn_version,\n@@ -47,13 +49,15 @@\n     from ._modeling_parallel import ParallelConfig\n \n _REQUIRED_FLASH_VERSION = \"2.6.3\"\n+_REQUIRED_AITER_VERSION = \"0.1.5\"\n _REQUIRED_SAGE_VERSION = \"2.1.1\"\n _REQUIRED_FLEX_VERSION = \"2.5.0\"\n _REQUIRED_XLA_VERSION = \"2.2\"\n _REQUIRED_XFORMERS_VERSION = \"0.0.29\"\n \n _CAN_USE_FLASH_ATTN = is_flash_attn_available() and is_flash_attn_version(\">=\", _REQUIRED_FLASH_VERSION)\n _CAN_USE_FLASH_ATTN_3 = is_flash_attn_3_available()\n+_CAN_USE_AITER_ATTN = is_aiter_available() and is_aiter_version(\">=\", _REQUIRED_AITER_VERSION)\n _CAN_USE_SAGE_ATTN = is_sageattention_available() and is_sageattention_version(\">=\", _REQUIRED_SAGE_VERSION)\n _CAN_USE_FLEX_ATTN = is_torch_version(\">=\", _REQUIRED_FLEX_VERSION)\n _CAN_USE_NPU_ATTN = is_torch_npu_available()\n@@ -78,6 +82,12 @@\n     flash_attn_3_func = None\n     flash_attn_3_varlen_func = None\n \n+\n+if _CAN_USE_AITER_ATTN:\n+    from aiter import flash_attn_func as aiter_flash_attn_func\n+else:\n+    aiter_flash_attn_func = None\n+\n if DIFFUSERS_ENABLE_HUB_KERNELS:\n     if not is_kernels_available():\n         raise ImportError(\n@@ -178,6 +188,9 @@ class AttentionBackendName(str, Enum):\n     _FLASH_3_HUB = \"_flash_3_hub\"\n     # _FLASH_VARLEN_3_HUB = \"_flash_varlen_3_hub\"  # not supported yet.\n \n+    # `aiter`\n+    AITER = \"aiter\"\n+\n     # PyTorch native\n     FLEX = \"flex\"\n     NATIVE = \"native\"\n@@ -414,6 +427,12 @@ def _check_attention_backend_requirements(backend: AttentionBackendName) -> None\n                 f\"Flash Attention 3 Hub backend '{backend.value}' is not usable because the `kernels` package isn't available. Please install it with `pip install kernels`.\"\n             )\n \n+    elif backend == AttentionBackendName.AITER:\n+        if not _CAN_USE_AITER_ATTN:\n+            raise RuntimeError(\n+                f\"Aiter Attention backend '{backend.value}' is not usable because of missing package or the version is too old. Please install `aiter>={_REQUIRED_AITER_VERSION}`.\"\n+            )\n+\n     elif backend in [\n         AttentionBackendName.SAGE,\n         AttentionBackendName.SAGE_VARLEN,\n@@ -1397,6 +1416,47 @@ def _flash_varlen_attention_3(\n     return (out, lse) if return_lse else out\n \n \n+@_AttentionBackendRegistry.register(\n+    AttentionBackendName.AITER,\n+    constraints=[_check_device_cuda, _check_qkv_dtype_bf16_or_fp16, _check_shape],\n+)\n+def _aiter_flash_attention(\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    dropout_p: float = 0.0,\n+    is_causal: bool = False,\n+    scale: Optional[float] = None,\n+    return_lse: bool = False,\n+    _parallel_config: Optional[\"ParallelConfig\"] = None,\n+) -> torch.Tensor:\n+    if not return_lse and torch.is_grad_enabled():\n+        # aiter requires return_lse=True by assertion when gradients are enabled.\n+        out, lse, *_ = aiter_flash_attn_func(\n+            q=query,\n+            k=key,\n+            v=value,\n+            dropout_p=dropout_p,\n+            softmax_scale=scale,\n+            causal=is_causal,\n+            return_lse=True,\n+        )\n+    else:\n+        out = aiter_flash_attn_func(\n+            q=query,\n+            k=key,\n+            v=value,\n+            dropout_p=dropout_p,\n+            softmax_scale=scale,\n+            causal=is_causal,\n+            return_lse=return_lse,\n+        )\n+        if return_lse:\n+            out, lse, *_ = out\n+\n+    return (out, lse) if return_lse else out\n+\n+\n @_AttentionBackendRegistry.register(\n     AttentionBackendName.FLEX,\n     constraints=[_check_attn_mask_or_causal, _check_device, _check_shape],"
        },
        {
          "filename": "src/diffusers/utils/__init__.py",
          "status": "modified",
          "additions": 2,
          "deletions": 0,
          "changes": 2,
          "patch": "@@ -64,6 +64,8 @@\n     get_objects_from_module,\n     is_accelerate_available,\n     is_accelerate_version,\n+    is_aiter_available,\n+    is_aiter_version,\n     is_better_profanity_available,\n     is_bitsandbytes_available,\n     is_bitsandbytes_version,"
        },
        {
          "filename": "src/diffusers/utils/import_utils.py",
          "status": "modified",
          "additions": 21,
          "deletions": 0,
          "changes": 21,
          "patch": "@@ -226,6 +226,7 @@ def _is_package_available(pkg_name: str, get_dist_name: bool = False) -> Tuple[b\n _sageattention_available, _sageattention_version = _is_package_available(\"sageattention\")\n _flash_attn_available, _flash_attn_version = _is_package_available(\"flash_attn\")\n _flash_attn_3_available, _flash_attn_3_version = _is_package_available(\"flash_attn_3\")\n+_aiter_available, _aiter_version = _is_package_available(\"aiter\")\n _kornia_available, _kornia_version = _is_package_available(\"kornia\")\n _nvidia_modelopt_available, _nvidia_modelopt_version = _is_package_available(\"modelopt\", get_dist_name=True)\n \n@@ -406,6 +407,10 @@ def is_flash_attn_3_available():\n     return _flash_attn_3_available\n \n \n+def is_aiter_available():\n+    return _aiter_available\n+\n+\n def is_kornia_available():\n     return _kornia_available\n \n@@ -911,6 +916,22 @@ def is_flash_attn_version(operation: str, version: str):\n     return compare_versions(parse(_flash_attn_version), operation, version)\n \n \n+@cache\n+def is_aiter_version(operation: str, version: str):\n+    \"\"\"\n+    Compares the current aiter version to a given reference with an operation.\n+\n+    Args:\n+        operation (`str`):\n+            A string representation of an operator, such as `\">\"` or `\"<=\"`\n+        version (`str`):\n+            A version string\n+    \"\"\"\n+    if not _aiter_available:\n+        return False\n+    return compare_versions(parse(_aiter_version), operation, version)\n+\n+\n def get_objects_from_module(module):\n     \"\"\"\n     Returns a dict of object names and values in a module, while skipping private/internal objects"
        },
        {
          "filename": "tests/others/test_attention_backends.py",
          "status": "modified",
          "additions": 13,
          "deletions": 0,
          "changes": 13,
          "patch": "@@ -14,6 +14,10 @@\n \n Tests were conducted on an H100 with PyTorch 2.8.0 (CUDA 12.9). Slices for the compilation tests in\n \"native\" variants were obtained with a torch nightly version (2.10.0.dev20250924+cu128).\n+\n+Tests for aiter backend were conducted and slices for the aiter backend tests collected on a MI355X\n+with torch 2025-09-25 nightly version (ad2f7315ca66b42497047bb7951f696b50f1e81b) and\n+aiter 0.1.5.post4.dev20+ga25e55e79.\n \"\"\"\n \n import os\n@@ -44,6 +48,10 @@\n         \"_native_cudnn\",\n         torch.tensor([0.0781, 0.0840, 0.0879, 0.0957, 0.0898, 0.0957, 0.0957, 0.0977, 0.2168, 0.2246, 0.2324, 0.2500, 0.2539, 0.2480, 0.2441, 0.2695], dtype=torch.bfloat16),\n     ),\n+    (\n+        \"aiter\",\n+        torch.tensor([0.0781, 0.0820, 0.0879, 0.0957, 0.0898, 0.0938, 0.0957, 0.0957, 0.2285, 0.2363, 0.2461, 0.2637, 0.2695, 0.2617, 0.2617, 0.2891], dtype=torch.bfloat16),\n+    )\n ]\n \n COMPILE_CASES = [\n@@ -63,6 +71,11 @@\n         torch.tensor([0.0410, 0.0410, 0.0430, 0.0508, 0.0488, 0.0586, 0.0605, 0.0586, 0.2344, 0.2461, 0.2578, 0.2773, 0.2871, 0.2832, 0.2793, 0.3086], dtype=torch.bfloat16),\n         True,\n     ),\n+    (\n+        \"aiter\",\n+        torch.tensor([0.0391, 0.0391, 0.0430, 0.0488, 0.0469, 0.0566, 0.0586, 0.0566, 0.2402, 0.2539, 0.2637, 0.2812, 0.2930, 0.2910, 0.2891, 0.3164], dtype=torch.bfloat16),\n+        True,\n+    )\n ]\n # fmt: on\n "
        }
      ],
      "num_files": 5,
      "scraped_at": "2025-11-16T21:18:49.956203"
    },
    {
      "pr_number": 12545,
      "title": "Bria fibo",
      "body": "# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes # (issue)\r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/diffusers/blob/main/CONTRIBUTING.md)?\r\n- [x] Did you read our [philosophy doc](https://github.com/huggingface/diffusers/blob/main/PHILOSOPHY.md) (important for complex PRs)?\r\n- [ ] Was this discussed/approved via a GitHub issue or the [forum](https://discuss.huggingface.co/c/discussion-related-to-httpsgithubcomhuggingfacediffusers/63)? Please add a link to it if that's the case.\r\n- [x] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/diffusers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/diffusers/tree/main/docs#writing-source-documentation).\r\n- [x] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @.\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nCore library:\r\n\r\n- Schedulers: @yiyixuxu\r\n- Pipelines and pipeline callbacks: @yiyixuxu and @asomoza\r\n- Training examples: @sayakpaul\r\n- Docs: @stevhliu and @sayakpaul\r\n- JAX and MPS: @pcuenca\r\n- Audio: @sanchit-gandhi\r\n- General functionalities: @sayakpaul @yiyixuxu @DN6\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @SunMarc\r\n- PEFT: @sayakpaul @BenjaminBossan\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- transformers: [different repo](https://github.com/huggingface/transformers)\r\n- safetensors: [different repo](https://github.com/huggingface/safetensors)\r\n\r\n-->\r\n",
      "html_url": "https://github.com/huggingface/diffusers/pull/12545",
      "created_at": "2025-10-26T17:11:02Z",
      "merged_at": "2025-10-28T10:57:48Z",
      "merge_commit_sha": "84e16575e4c5e90b6b49301cfa162ced4cf478d2",
      "base_ref": "main",
      "head_sha": "7f3dd1dc4d110ca1406f55cd430f38442f7b0c57",
      "user": "galbria",
      "files": [
        {
          "filename": "docs/source/en/_toctree.yml",
          "status": "modified",
          "additions": 4,
          "deletions": 0,
          "changes": 4,
          "patch": "@@ -323,6 +323,8 @@\n         title: AllegroTransformer3DModel\n       - local: api/models/aura_flow_transformer2d\n         title: AuraFlowTransformer2DModel\n+      - local: api/models/transformer_bria_fibo\n+        title: BriaFiboTransformer2DModel\n       - local: api/models/bria_transformer\n         title: BriaTransformer2DModel\n       - local: api/models/chroma_transformer\n@@ -469,6 +471,8 @@\n         title: BLIP-Diffusion\n       - local: api/pipelines/bria_3_2\n         title: Bria 3.2\n+      - local: api/pipelines/bria_fibo\n+        title: Bria Fibo\n       - local: api/pipelines/chroma\n         title: Chroma\n       - local: api/pipelines/cogview3"
        },
        {
          "filename": "docs/source/en/api/models/transformer_bria_fibo.md",
          "status": "added",
          "additions": 19,
          "deletions": 0,
          "changes": 19,
          "patch": "@@ -0,0 +1,19 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+-->\n+\n+# BriaFiboTransformer2DModel\n+\n+A modified flux Transformer model from [Bria](https://huggingface.co/briaai/FIBO)\n+\n+## BriaFiboTransformer2DModel\n+\n+[[autodoc]] BriaFiboTransformer2DModel"
        },
        {
          "filename": "docs/source/en/api/pipelines/bria_fibo.md",
          "status": "added",
          "additions": 45,
          "deletions": 0,
          "changes": 45,
          "patch": "@@ -0,0 +1,45 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+-->\n+\n+# Bria Fibo\n+\n+Text-to-image models have mastered imagination - but not control. FIBO changes that.\n+\n+FIBO is trained on structured JSON captions up to 1,000+ words and designed to understand and control different visual parameters such as lighting, composition, color, and camera settings, enabling precise and reproducible outputs.\n+\n+With only 8 billion parameters, FIBO provides a new level of image quality, prompt adherence and proffesional control.\n+\n+FIBO is trained exclusively on a structured prompt and will not work with freeform text prompts.\n+you can use the [FIBO-VLM-prompt-to-JSON](https://huggingface.co/briaai/FIBO-VLM-prompt-to-JSON) model or the [FIBO-gemini-prompt-to-JSON](https://huggingface.co/briaai/FIBO-gemini-prompt-to-JSON)  to convert your freeform text prompt to a structured JSON prompt.\n+\n+its not recommended to use freeform text prompts directly with FIBO, as it will not produce the best results.\n+\n+you can learn more about FIBO in  [Bria Fibo Hugging Face page](https://huggingface.co/briaai/FIBO).\n+\n+\n+## Usage\n+\n+_As the model is gated, before using it with diffusers you first need to go to the [Bria Fibo Hugging Face page](https://huggingface.co/briaai/FIBO), fill in the form and accept the gate. Once you are in, you need to login so that your system knows you\u2019ve accepted the gate._\n+\n+Use the command below to log in:\n+\n+```bash\n+hf auth login\n+```\n+\n+\n+## BriaPipeline\n+\n+[[autodoc]] BriaPipeline\n+\t- all\n+\t- __call__\n+"
        },
        {
          "filename": "src/diffusers/__init__.py",
          "status": "modified",
          "additions": 4,
          "deletions": 0,
          "changes": 4,
          "patch": "@@ -198,6 +198,7 @@\n             \"AutoencoderOobleck\",\n             \"AutoencoderTiny\",\n             \"AutoModel\",\n+            \"BriaFiboTransformer2DModel\",\n             \"BriaTransformer2DModel\",\n             \"CacheMixin\",\n             \"ChromaTransformer2DModel\",\n@@ -430,6 +431,7 @@\n             \"AuraFlowPipeline\",\n             \"BlipDiffusionControlNetPipeline\",\n             \"BlipDiffusionPipeline\",\n+            \"BriaFiboPipeline\",\n             \"BriaPipeline\",\n             \"ChromaImg2ImgPipeline\",\n             \"ChromaPipeline\",\n@@ -901,6 +903,7 @@\n             AutoencoderOobleck,\n             AutoencoderTiny,\n             AutoModel,\n+            BriaFiboTransformer2DModel,\n             BriaTransformer2DModel,\n             CacheMixin,\n             ChromaTransformer2DModel,\n@@ -1103,6 +1106,7 @@\n             AudioLDM2UNet2DConditionModel,\n             AudioLDMPipeline,\n             AuraFlowPipeline,\n+            BriaFiboPipeline,\n             BriaPipeline,\n             ChromaImg2ImgPipeline,\n             ChromaPipeline,"
        },
        {
          "filename": "src/diffusers/models/__init__.py",
          "status": "modified",
          "additions": 2,
          "deletions": 0,
          "changes": 2,
          "patch": "@@ -84,6 +84,7 @@\n     _import_structure[\"transformers.transformer_2d\"] = [\"Transformer2DModel\"]\n     _import_structure[\"transformers.transformer_allegro\"] = [\"AllegroTransformer3DModel\"]\n     _import_structure[\"transformers.transformer_bria\"] = [\"BriaTransformer2DModel\"]\n+    _import_structure[\"transformers.transformer_bria_fibo\"] = [\"BriaFiboTransformer2DModel\"]\n     _import_structure[\"transformers.transformer_chroma\"] = [\"ChromaTransformer2DModel\"]\n     _import_structure[\"transformers.transformer_cogview3plus\"] = [\"CogView3PlusTransformer2DModel\"]\n     _import_structure[\"transformers.transformer_cogview4\"] = [\"CogView4Transformer2DModel\"]\n@@ -174,6 +175,7 @@\n         from .transformers import (\n             AllegroTransformer3DModel,\n             AuraFlowTransformer2DModel,\n+            BriaFiboTransformer2DModel,\n             BriaTransformer2DModel,\n             ChromaTransformer2DModel,\n             CogVideoXTransformer3DModel,"
        },
        {
          "filename": "src/diffusers/models/transformers/__init__.py",
          "status": "modified",
          "additions": 1,
          "deletions": 0,
          "changes": 1,
          "patch": "@@ -18,6 +18,7 @@\n     from .transformer_2d import Transformer2DModel\n     from .transformer_allegro import AllegroTransformer3DModel\n     from .transformer_bria import BriaTransformer2DModel\n+    from .transformer_bria_fibo import BriaFiboTransformer2DModel\n     from .transformer_chroma import ChromaTransformer2DModel\n     from .transformer_cogview3plus import CogView3PlusTransformer2DModel\n     from .transformer_cogview4 import CogView4Transformer2DModel"
        },
        {
          "filename": "src/diffusers/models/transformers/transformer_bria_fibo.py",
          "status": "added",
          "additions": 655,
          "deletions": 0,
          "changes": 655,
          "patch": "@@ -0,0 +1,655 @@\n+# Copyright (c) Bria.ai. All rights reserved.\n+#\n+# This file is licensed under the Creative Commons Attribution-NonCommercial 4.0 International Public License (CC-BY-NC-4.0).\n+# You may obtain a copy of the license at https://creativecommons.org/licenses/by-nc/4.0/\n+#\n+# You are free to share and adapt this material for non-commercial purposes provided you give appropriate credit,\n+# indicate if changes were made, and do not use the material for commercial purposes.\n+#\n+# See the license for further details.\n+import inspect\n+from typing import Any, Dict, List, Optional, Tuple, Union\n+\n+import torch\n+import torch.nn as nn\n+import torch.nn.functional as F\n+\n+from ...configuration_utils import ConfigMixin, register_to_config\n+from ...loaders import FromOriginalModelMixin, PeftAdapterMixin\n+from ...models.attention_processor import Attention\n+from ...models.embeddings import TimestepEmbedding, apply_rotary_emb, get_1d_rotary_pos_embed, get_timestep_embedding\n+from ...models.modeling_outputs import Transformer2DModelOutput\n+from ...models.modeling_utils import ModelMixin\n+from ...models.transformers.transformer_bria import BriaAttnProcessor\n+from ...utils import (\n+    USE_PEFT_BACKEND,\n+    logging,\n+    scale_lora_layers,\n+    unscale_lora_layers,\n+)\n+from ...utils.torch_utils import maybe_allow_in_graph\n+from ..attention import AttentionModuleMixin, FeedForward\n+from ..attention_dispatch import dispatch_attention_fn\n+from ..normalization import AdaLayerNormContinuous, AdaLayerNormZero, AdaLayerNormZeroSingle\n+\n+\n+logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n+\n+\n+def _get_projections(attn: \"BriaFiboAttention\", hidden_states, encoder_hidden_states=None):\n+    query = attn.to_q(hidden_states)\n+    key = attn.to_k(hidden_states)\n+    value = attn.to_v(hidden_states)\n+\n+    encoder_query = encoder_key = encoder_value = None\n+    if encoder_hidden_states is not None and attn.added_kv_proj_dim is not None:\n+        encoder_query = attn.add_q_proj(encoder_hidden_states)\n+        encoder_key = attn.add_k_proj(encoder_hidden_states)\n+        encoder_value = attn.add_v_proj(encoder_hidden_states)\n+\n+    return query, key, value, encoder_query, encoder_key, encoder_value\n+\n+\n+def _get_fused_projections(attn: \"BriaFiboAttention\", hidden_states, encoder_hidden_states=None):\n+    query, key, value = attn.to_qkv(hidden_states).chunk(3, dim=-1)\n+\n+    encoder_query = encoder_key = encoder_value = (None,)\n+    if encoder_hidden_states is not None and hasattr(attn, \"to_added_qkv\"):\n+        encoder_query, encoder_key, encoder_value = attn.to_added_qkv(encoder_hidden_states).chunk(3, dim=-1)\n+\n+    return query, key, value, encoder_query, encoder_key, encoder_value\n+\n+\n+def _get_qkv_projections(attn: \"BriaFiboAttention\", hidden_states, encoder_hidden_states=None):\n+    if attn.fused_projections:\n+        return _get_fused_projections(attn, hidden_states, encoder_hidden_states)\n+    return _get_projections(attn, hidden_states, encoder_hidden_states)\n+\n+\n+# Copied from diffusers.models.transformers.transformer_flux.FluxAttnProcessor with FluxAttnProcessor->BriaFiboAttnProcessor, FluxAttention->BriaFiboAttention\n+class BriaFiboAttnProcessor:\n+    _attention_backend = None\n+    _parallel_config = None\n+\n+    def __init__(self):\n+        if not hasattr(F, \"scaled_dot_product_attention\"):\n+            raise ImportError(f\"{self.__class__.__name__} requires PyTorch 2.0. Please upgrade your pytorch version.\")\n+\n+    def __call__(\n+        self,\n+        attn: \"BriaFiboAttention\",\n+        hidden_states: torch.Tensor,\n+        encoder_hidden_states: torch.Tensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        image_rotary_emb: Optional[torch.Tensor] = None,\n+    ) -> torch.Tensor:\n+        query, key, value, encoder_query, encoder_key, encoder_value = _get_qkv_projections(\n+            attn, hidden_states, encoder_hidden_states\n+        )\n+\n+        query = query.unflatten(-1, (attn.heads, -1))\n+        key = key.unflatten(-1, (attn.heads, -1))\n+        value = value.unflatten(-1, (attn.heads, -1))\n+\n+        query = attn.norm_q(query)\n+        key = attn.norm_k(key)\n+\n+        if attn.added_kv_proj_dim is not None:\n+            encoder_query = encoder_query.unflatten(-1, (attn.heads, -1))\n+            encoder_key = encoder_key.unflatten(-1, (attn.heads, -1))\n+            encoder_value = encoder_value.unflatten(-1, (attn.heads, -1))\n+\n+            encoder_query = attn.norm_added_q(encoder_query)\n+            encoder_key = attn.norm_added_k(encoder_key)\n+\n+            query = torch.cat([encoder_query, query], dim=1)\n+            key = torch.cat([encoder_key, key], dim=1)\n+            value = torch.cat([encoder_value, value], dim=1)\n+\n+        if image_rotary_emb is not None:\n+            query = apply_rotary_emb(query, image_rotary_emb, sequence_dim=1)\n+            key = apply_rotary_emb(key, image_rotary_emb, sequence_dim=1)\n+\n+        hidden_states = dispatch_attention_fn(\n+            query,\n+            key,\n+            value,\n+            attn_mask=attention_mask,\n+            backend=self._attention_backend,\n+            parallel_config=self._parallel_config,\n+        )\n+        hidden_states = hidden_states.flatten(2, 3)\n+        hidden_states = hidden_states.to(query.dtype)\n+\n+        if encoder_hidden_states is not None:\n+            encoder_hidden_states, hidden_states = hidden_states.split_with_sizes(\n+                [encoder_hidden_states.shape[1], hidden_states.shape[1] - encoder_hidden_states.shape[1]], dim=1\n+            )\n+            hidden_states = attn.to_out[0](hidden_states)\n+            hidden_states = attn.to_out[1](hidden_states)\n+            encoder_hidden_states = attn.to_add_out(encoder_hidden_states)\n+\n+            return hidden_states, encoder_hidden_states\n+        else:\n+            return hidden_states\n+\n+\n+# Based on https://github.com/huggingface/diffusers/blob/55d49d4379007740af20629bb61aba9546c6b053/src/diffusers/models/transformers/transformer_flux.py\n+class BriaFiboAttention(torch.nn.Module, AttentionModuleMixin):\n+    _default_processor_cls = BriaFiboAttnProcessor\n+    _available_processors = [BriaFiboAttnProcessor]\n+\n+    def __init__(\n+        self,\n+        query_dim: int,\n+        heads: int = 8,\n+        dim_head: int = 64,\n+        dropout: float = 0.0,\n+        bias: bool = False,\n+        added_kv_proj_dim: Optional[int] = None,\n+        added_proj_bias: Optional[bool] = True,\n+        out_bias: bool = True,\n+        eps: float = 1e-5,\n+        out_dim: int = None,\n+        context_pre_only: Optional[bool] = None,\n+        pre_only: bool = False,\n+        elementwise_affine: bool = True,\n+        processor=None,\n+    ):\n+        super().__init__()\n+\n+        self.head_dim = dim_head\n+        self.inner_dim = out_dim if out_dim is not None else dim_head * heads\n+        self.query_dim = query_dim\n+        self.use_bias = bias\n+        self.dropout = dropout\n+        self.out_dim = out_dim if out_dim is not None else query_dim\n+        self.context_pre_only = context_pre_only\n+        self.pre_only = pre_only\n+        self.heads = out_dim // dim_head if out_dim is not None else heads\n+        self.added_kv_proj_dim = added_kv_proj_dim\n+        self.added_proj_bias = added_proj_bias\n+\n+        self.norm_q = torch.nn.RMSNorm(dim_head, eps=eps, elementwise_affine=elementwise_affine)\n+        self.norm_k = torch.nn.RMSNorm(dim_head, eps=eps, elementwise_affine=elementwise_affine)\n+        self.to_q = torch.nn.Linear(query_dim, self.inner_dim, bias=bias)\n+        self.to_k = torch.nn.Linear(query_dim, self.inner_dim, bias=bias)\n+        self.to_v = torch.nn.Linear(query_dim, self.inner_dim, bias=bias)\n+\n+        if not self.pre_only:\n+            self.to_out = torch.nn.ModuleList([])\n+            self.to_out.append(torch.nn.Linear(self.inner_dim, self.out_dim, bias=out_bias))\n+            self.to_out.append(torch.nn.Dropout(dropout))\n+\n+        if added_kv_proj_dim is not None:\n+            self.norm_added_q = torch.nn.RMSNorm(dim_head, eps=eps)\n+            self.norm_added_k = torch.nn.RMSNorm(dim_head, eps=eps)\n+            self.add_q_proj = torch.nn.Linear(added_kv_proj_dim, self.inner_dim, bias=added_proj_bias)\n+            self.add_k_proj = torch.nn.Linear(added_kv_proj_dim, self.inner_dim, bias=added_proj_bias)\n+            self.add_v_proj = torch.nn.Linear(added_kv_proj_dim, self.inner_dim, bias=added_proj_bias)\n+            self.to_add_out = torch.nn.Linear(self.inner_dim, query_dim, bias=out_bias)\n+\n+        if processor is None:\n+            processor = self._default_processor_cls()\n+        self.set_processor(processor)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        encoder_hidden_states: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        image_rotary_emb: Optional[torch.Tensor] = None,\n+        **kwargs,\n+    ) -> torch.Tensor:\n+        attn_parameters = set(inspect.signature(self.processor.__call__).parameters.keys())\n+        quiet_attn_parameters = {\"ip_adapter_masks\", \"ip_hidden_states\"}\n+        unused_kwargs = [k for k, _ in kwargs.items() if k not in attn_parameters and k not in quiet_attn_parameters]\n+        if len(unused_kwargs) > 0:\n+            logger.warning(\n+                f\"joint_attention_kwargs {unused_kwargs} are not expected by {self.processor.__class__.__name__} and will be ignored.\"\n+            )\n+        kwargs = {k: w for k, w in kwargs.items() if k in attn_parameters}\n+        return self.processor(self, hidden_states, encoder_hidden_states, attention_mask, image_rotary_emb, **kwargs)\n+\n+\n+class BriaFiboEmbedND(torch.nn.Module):\n+    # modified from https://github.com/black-forest-labs/flux/blob/c00d7c60b085fce8058b9df845e036090873f2ce/src/flux/modules/layers.py#L11\n+    def __init__(self, theta: int, axes_dim: List[int]):\n+        super().__init__()\n+        self.theta = theta\n+        self.axes_dim = axes_dim\n+\n+    def forward(self, ids: torch.Tensor) -> torch.Tensor:\n+        n_axes = ids.shape[-1]\n+        cos_out = []\n+        sin_out = []\n+        pos = ids.float()\n+        is_mps = ids.device.type == \"mps\"\n+        freqs_dtype = torch.float32 if is_mps else torch.float64\n+        for i in range(n_axes):\n+            cos, sin = get_1d_rotary_pos_embed(\n+                self.axes_dim[i],\n+                pos[:, i],\n+                theta=self.theta,\n+                repeat_interleave_real=True,\n+                use_real=True,\n+                freqs_dtype=freqs_dtype,\n+            )\n+            cos_out.append(cos)\n+            sin_out.append(sin)\n+        freqs_cos = torch.cat(cos_out, dim=-1).to(ids.device)\n+        freqs_sin = torch.cat(sin_out, dim=-1).to(ids.device)\n+        return freqs_cos, freqs_sin\n+\n+\n+@maybe_allow_in_graph\n+class BriaFiboSingleTransformerBlock(nn.Module):\n+    def __init__(self, dim: int, num_attention_heads: int, attention_head_dim: int, mlp_ratio: float = 4.0):\n+        super().__init__()\n+        self.mlp_hidden_dim = int(dim * mlp_ratio)\n+\n+        self.norm = AdaLayerNormZeroSingle(dim)\n+        self.proj_mlp = nn.Linear(dim, self.mlp_hidden_dim)\n+        self.act_mlp = nn.GELU(approximate=\"tanh\")\n+        self.proj_out = nn.Linear(dim + self.mlp_hidden_dim, dim)\n+\n+        processor = BriaAttnProcessor()\n+\n+        self.attn = Attention(\n+            query_dim=dim,\n+            cross_attention_dim=None,\n+            dim_head=attention_head_dim,\n+            heads=num_attention_heads,\n+            out_dim=dim,\n+            bias=True,\n+            processor=processor,\n+            qk_norm=\"rms_norm\",\n+            eps=1e-6,\n+            pre_only=True,\n+        )\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        temb: torch.Tensor,\n+        image_rotary_emb: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n+        joint_attention_kwargs: Optional[Dict[str, Any]] = None,\n+    ) -> torch.Tensor:\n+        residual = hidden_states\n+        norm_hidden_states, gate = self.norm(hidden_states, emb=temb)\n+        mlp_hidden_states = self.act_mlp(self.proj_mlp(norm_hidden_states))\n+        joint_attention_kwargs = joint_attention_kwargs or {}\n+        attn_output = self.attn(\n+            hidden_states=norm_hidden_states,\n+            image_rotary_emb=image_rotary_emb,\n+            **joint_attention_kwargs,\n+        )\n+\n+        hidden_states = torch.cat([attn_output, mlp_hidden_states], dim=2)\n+        gate = gate.unsqueeze(1)\n+        hidden_states = gate * self.proj_out(hidden_states)\n+        hidden_states = residual + hidden_states\n+        if hidden_states.dtype == torch.float16:\n+            hidden_states = hidden_states.clip(-65504, 65504)\n+\n+        return hidden_states\n+\n+\n+class BriaFiboTextProjection(nn.Module):\n+    def __init__(self, in_features, hidden_size):\n+        super().__init__()\n+        self.linear = nn.Linear(in_features=in_features, out_features=hidden_size, bias=False)\n+\n+    def forward(self, caption):\n+        hidden_states = self.linear(caption)\n+        return hidden_states\n+\n+\n+@maybe_allow_in_graph\n+# Based on from diffusers.models.transformers.transformer_flux.FluxTransformerBlock\n+class BriaFiboTransformerBlock(nn.Module):\n+    def __init__(\n+        self, dim: int, num_attention_heads: int, attention_head_dim: int, qk_norm: str = \"rms_norm\", eps: float = 1e-6\n+    ):\n+        super().__init__()\n+\n+        self.norm1 = AdaLayerNormZero(dim)\n+        self.norm1_context = AdaLayerNormZero(dim)\n+\n+        self.attn = BriaFiboAttention(\n+            query_dim=dim,\n+            added_kv_proj_dim=dim,\n+            dim_head=attention_head_dim,\n+            heads=num_attention_heads,\n+            out_dim=dim,\n+            context_pre_only=False,\n+            bias=True,\n+            processor=BriaFiboAttnProcessor(),\n+            eps=eps,\n+        )\n+\n+        self.norm2 = nn.LayerNorm(dim, elementwise_affine=False, eps=1e-6)\n+        self.ff = FeedForward(dim=dim, dim_out=dim, activation_fn=\"gelu-approximate\")\n+\n+        self.norm2_context = nn.LayerNorm(dim, elementwise_affine=False, eps=1e-6)\n+        self.ff_context = FeedForward(dim=dim, dim_out=dim, activation_fn=\"gelu-approximate\")\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        encoder_hidden_states: torch.Tensor,\n+        temb: torch.Tensor,\n+        image_rotary_emb: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n+        joint_attention_kwargs: Optional[Dict[str, Any]] = None,\n+    ) -> Tuple[torch.Tensor, torch.Tensor]:\n+        norm_hidden_states, gate_msa, shift_mlp, scale_mlp, gate_mlp = self.norm1(hidden_states, emb=temb)\n+\n+        norm_encoder_hidden_states, c_gate_msa, c_shift_mlp, c_scale_mlp, c_gate_mlp = self.norm1_context(\n+            encoder_hidden_states, emb=temb\n+        )\n+        joint_attention_kwargs = joint_attention_kwargs or {}\n+\n+        # Attention.\n+        attention_outputs = self.attn(\n+            hidden_states=norm_hidden_states,\n+            encoder_hidden_states=norm_encoder_hidden_states,\n+            image_rotary_emb=image_rotary_emb,\n+            **joint_attention_kwargs,\n+        )\n+\n+        if len(attention_outputs) == 2:\n+            attn_output, context_attn_output = attention_outputs\n+        elif len(attention_outputs) == 3:\n+            attn_output, context_attn_output, ip_attn_output = attention_outputs\n+\n+        # Process attention outputs for the `hidden_states`.\n+        attn_output = gate_msa.unsqueeze(1) * attn_output\n+        hidden_states = hidden_states + attn_output\n+\n+        norm_hidden_states = self.norm2(hidden_states)\n+        norm_hidden_states = norm_hidden_states * (1 + scale_mlp[:, None]) + shift_mlp[:, None]\n+\n+        ff_output = self.ff(norm_hidden_states)\n+        ff_output = gate_mlp.unsqueeze(1) * ff_output\n+\n+        hidden_states = hidden_states + ff_output\n+        if len(attention_outputs) == 3:\n+            hidden_states = hidden_states + ip_attn_output\n+\n+        # Process attention outputs for the `encoder_hidden_states`.\n+        context_attn_output = c_gate_msa.unsqueeze(1) * context_attn_output\n+        encoder_hidden_states = encoder_hidden_states + context_attn_output\n+\n+        norm_encoder_hidden_states = self.norm2_context(encoder_hidden_states)\n+        norm_encoder_hidden_states = norm_encoder_hidden_states * (1 + c_scale_mlp[:, None]) + c_shift_mlp[:, None]\n+\n+        context_ff_output = self.ff_context(norm_encoder_hidden_states)\n+        encoder_hidden_states = encoder_hidden_states + c_gate_mlp.unsqueeze(1) * context_ff_output\n+        if encoder_hidden_states.dtype == torch.float16:\n+            encoder_hidden_states = encoder_hidden_states.clip(-65504, 65504)\n+\n+        return encoder_hidden_states, hidden_states\n+\n+\n+class BriaFiboTimesteps(nn.Module):\n+    def __init__(\n+        self, num_channels: int, flip_sin_to_cos: bool, downscale_freq_shift: float, scale: int = 1, time_theta=10000\n+    ):\n+        super().__init__()\n+        self.num_channels = num_channels\n+        self.flip_sin_to_cos = flip_sin_to_cos\n+        self.downscale_freq_shift = downscale_freq_shift\n+        self.scale = scale\n+        self.time_theta = time_theta\n+\n+    def forward(self, timesteps):\n+        t_emb = get_timestep_embedding(\n+            timesteps,\n+            self.num_channels,\n+            flip_sin_to_cos=self.flip_sin_to_cos,\n+            downscale_freq_shift=self.downscale_freq_shift,\n+            scale=self.scale,\n+            max_period=self.time_theta,\n+        )\n+        return t_emb\n+\n+\n+class BriaFiboTimestepProjEmbeddings(nn.Module):\n+    def __init__(self, embedding_dim, time_theta):\n+        super().__init__()\n+\n+        self.time_proj = BriaFiboTimesteps(\n+            num_channels=256, flip_sin_to_cos=True, downscale_freq_shift=0, time_theta=time_theta\n+        )\n+        self.timestep_embedder = TimestepEmbedding(in_channels=256, time_embed_dim=embedding_dim)\n+\n+    def forward(self, timestep, dtype):\n+        timesteps_proj = self.time_proj(timestep)\n+        timesteps_emb = self.timestep_embedder(timesteps_proj.to(dtype=dtype))  # (N, D)\n+        return timesteps_emb\n+\n+\n+class BriaFiboTransformer2DModel(ModelMixin, ConfigMixin, PeftAdapterMixin, FromOriginalModelMixin):\n+    \"\"\"\n+    Parameters:\n+        patch_size (`int`): Patch size to turn the input data into small patches.\n+        in_channels (`int`, *optional*, defaults to 16): The number of channels in the input.\n+        num_layers (`int`, *optional*, defaults to 18): The number of layers of MMDiT blocks to use.\n+        num_single_layers (`int`, *optional*, defaults to 18): The number of layers of single DiT blocks to use.\n+        attention_head_dim (`int`, *optional*, defaults to 64): The number of channels in each head.\n+        num_attention_heads (`int`, *optional*, defaults to 18): The number of heads to use for multi-head attention.\n+        joint_attention_dim (`int`, *optional*): The number of `encoder_hidden_states` dimensions to use.\n+        pooled_projection_dim (`int`): Number of dimensions to use when projecting the `pooled_projections`.\n+        guidance_embeds (`bool`, defaults to False): Whether to use guidance embeddings.\n+        ...\n+    \"\"\"\n+\n+    _supports_gradient_checkpointing = True\n+\n+    @register_to_config\n+    def __init__(\n+        self,\n+        patch_size: int = 1,\n+        in_channels: int = 64,\n+        num_layers: int = 19,\n+        num_single_layers: int = 38,\n+        attention_head_dim: int = 128,\n+        num_attention_heads: int = 24,\n+        joint_attention_dim: int = 4096,\n+        pooled_projection_dim: int = None,\n+        guidance_embeds: bool = False,\n+        axes_dims_rope: List[int] = [16, 56, 56],\n+        rope_theta=10000,\n+        time_theta=10000,\n+        text_encoder_dim: int = 2048,\n+    ):\n+        super().__init__()\n+        self.out_channels = in_channels\n+        self.inner_dim = self.config.num_attention_heads * self.config.attention_head_dim\n+\n+        self.pos_embed = BriaFiboEmbedND(theta=rope_theta, axes_dim=axes_dims_rope)\n+\n+        self.time_embed = BriaFiboTimestepProjEmbeddings(embedding_dim=self.inner_dim, time_theta=time_theta)\n+\n+        if guidance_embeds:\n+            self.guidance_embed = BriaFiboTimestepProjEmbeddings(embedding_dim=self.inner_dim)\n+\n+        self.context_embedder = nn.Linear(self.config.joint_attention_dim, self.inner_dim)\n+        self.x_embedder = torch.nn.Linear(self.config.in_channels, self.inner_dim)\n+\n+        self.transformer_blocks = nn.ModuleList(\n+            [\n+                BriaFiboTransformerBlock(\n+                    dim=self.inner_dim,\n+                    num_attention_heads=self.config.num_attention_heads,\n+                    attention_head_dim=self.config.attention_head_dim,\n+                )\n+                for i in range(self.config.num_layers)\n+            ]\n+        )\n+\n+        self.single_transformer_blocks = nn.ModuleList(\n+            [\n+                BriaFiboSingleTransformerBlock(\n+                    dim=self.inner_dim,\n+                    num_attention_heads=self.config.num_attention_heads,\n+                    attention_head_dim=self.config.attention_head_dim,\n+                )\n+                for i in range(self.config.num_single_layers)\n+            ]\n+        )\n+\n+        self.norm_out = AdaLayerNormContinuous(self.inner_dim, self.inner_dim, elementwise_affine=False, eps=1e-6)\n+        self.proj_out = nn.Linear(self.inner_dim, patch_size * patch_size * self.out_channels, bias=True)\n+\n+        self.gradient_checkpointing = False\n+\n+        caption_projection = [\n+            BriaFiboTextProjection(in_features=text_encoder_dim, hidden_size=self.inner_dim // 2)\n+            for i in range(self.config.num_layers + self.config.num_single_layers)\n+        ]\n+        self.caption_projection = nn.ModuleList(caption_projection)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        encoder_hidden_states: torch.Tensor = None,\n+        text_encoder_layers: list = None,\n+        pooled_projections: torch.Tensor = None,\n+        timestep: torch.LongTensor = None,\n+        img_ids: torch.Tensor = None,\n+        txt_ids: torch.Tensor = None,\n+        guidance: torch.Tensor = None,\n+        joint_attention_kwargs: Optional[Dict[str, Any]] = None,\n+        return_dict: bool = True,\n+    ) -> Union[torch.FloatTensor, Transformer2DModelOutput]:\n+        \"\"\"\n+\n+        Args:\n+            hidden_states (`torch.FloatTensor` of shape `(batch size, channel, height, width)`):\n+                Input `hidden_states`.\n+            encoder_hidden_states (`torch.FloatTensor` of shape `(batch size, sequence_len, embed_dims)`):\n+                Conditional embeddings (embeddings computed from the input conditions such as prompts) to use.\n+            pooled_projections (`torch.FloatTensor` of shape `(batch_size, projection_dim)`): Embeddings projected\n+                from the embeddings of input conditions.\n+            timestep ( `torch.LongTensor`):\n+                Used to indicate denoising step.\n+            joint_attention_kwargs (`dict`, *optional*):\n+                A kwargs dictionary that if specified is passed along to the `AttentionProcessor` as defined under\n+                `self.processor` in\n+                [diffusers.models.attention_processor](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).\n+            return_dict (`bool`, *optional*, defaults to `True`):\n+                Whether or not to return a [`~models.transformer_2d.Transformer2DModelOutput`] instead of a plain\n+                tuple.\n+        Returns:\n+            If `return_dict` is True, an [`~models.transformer_2d.Transformer2DModelOutput`] is returned, otherwise a\n+            `tuple` where the first element is the sample tensor.\n+        \"\"\"\n+        if joint_attention_kwargs is not None:\n+            joint_attention_kwargs = joint_attention_kwargs.copy()\n+            lora_scale = joint_attention_kwargs.pop(\"scale\", 1.0)\n+        else:\n+            lora_scale = 1.0\n+\n+        if USE_PEFT_BACKEND:\n+            # weight the lora layers by setting `lora_scale` for each PEFT layer\n+            scale_lora_layers(self, lora_scale)\n+        else:\n+            if joint_attention_kwargs is not None and joint_attention_kwargs.get(\"scale\", None) is not None:\n+                logger.warning(\n+                    \"Passing `scale` via `joint_attention_kwargs` when not using the PEFT backend is ineffective.\"\n+                )\n+        hidden_states = self.x_embedder(hidden_states)\n+\n+        timestep = timestep.to(hidden_states.dtype)\n+        if guidance is not None:\n+            guidance = guidance.to(hidden_states.dtype)\n+        else:\n+            guidance = None\n+\n+        temb = self.time_embed(timestep, dtype=hidden_states.dtype)\n+\n+        if guidance:\n+            temb += self.guidance_embed(guidance, dtype=hidden_states.dtype)\n+\n+        encoder_hidden_states = self.context_embedder(encoder_hidden_states)\n+\n+        if len(txt_ids.shape) == 3:\n+            txt_ids = txt_ids[0]\n+\n+        if len(img_ids.shape) == 3:\n+            img_ids = img_ids[0]\n+\n+        ids = torch.cat((txt_ids, img_ids), dim=0)\n+        image_rotary_emb = self.pos_embed(ids)\n+\n+        new_text_encoder_layers = []\n+        for i, text_encoder_layer in enumerate(text_encoder_layers):\n+            text_encoder_layer = self.caption_projection[i](text_encoder_layer)\n+            new_text_encoder_layers.append(text_encoder_layer)\n+        text_encoder_layers = new_text_encoder_layers\n+\n+        block_id = 0\n+        for index_block, block in enumerate(self.transformer_blocks):\n+            current_text_encoder_layer = text_encoder_layers[block_id]\n+            encoder_hidden_states = torch.cat(\n+                [encoder_hidden_states[:, :, : self.inner_dim // 2], current_text_encoder_layer], dim=-1\n+            )\n+            block_id += 1\n+            if torch.is_grad_enabled() and self.gradient_checkpointing:\n+                encoder_hidden_states, hidden_states = self._gradient_checkpointing_func(\n+                    block,\n+                    hidden_states,\n+                    encoder_hidden_states,\n+                    temb,\n+                    image_rotary_emb,\n+                    joint_attention_kwargs,\n+                )\n+\n+            else:\n+                encoder_hidden_states, hidden_states = block(\n+                    hidden_states=hidden_states,\n+                    encoder_hidden_states=encoder_hidden_states,\n+                    temb=temb,\n+                    image_rotary_emb=image_rotary_emb,\n+                    joint_attention_kwargs=joint_attention_kwargs,\n+                )\n+\n+        for index_block, block in enumerate(self.single_transformer_blocks):\n+            current_text_encoder_layer = text_encoder_layers[block_id]\n+            encoder_hidden_states = torch.cat(\n+                [encoder_hidden_states[:, :, : self.inner_dim // 2], current_text_encoder_layer], dim=-1\n+            )\n+            block_id += 1\n+            hidden_states = torch.cat([encoder_hidden_states, hidden_states], dim=1)\n+            if torch.is_grad_enabled() and self.gradient_checkpointing:\n+                hidden_states = self._gradient_checkpointing_func(\n+                    block,\n+                    hidden_states,\n+                    temb,\n+                    image_rotary_emb,\n+                    joint_attention_kwargs,\n+                )\n+\n+            else:\n+                hidden_states = block(\n+                    hidden_states=hidden_states,\n+                    temb=temb,\n+                    image_rotary_emb=image_rotary_emb,\n+                    joint_attention_kwargs=joint_attention_kwargs,\n+                )\n+\n+            encoder_hidden_states = hidden_states[:, : encoder_hidden_states.shape[1], ...]\n+            hidden_states = hidden_states[:, encoder_hidden_states.shape[1] :, ...]\n+\n+        hidden_states = self.norm_out(hidden_states, temb)\n+        output = self.proj_out(hidden_states)\n+\n+        if USE_PEFT_BACKEND:\n+            # remove `lora_scale` from each PEFT layer\n+            unscale_lora_layers(self, lora_scale)\n+\n+        if not return_dict:\n+            return (output,)\n+\n+        return Transformer2DModelOutput(sample=output)"
        },
        {
          "filename": "src/diffusers/pipelines/__init__.py",
          "status": "modified",
          "additions": 2,
          "deletions": 0,
          "changes": 2,
          "patch": "@@ -128,6 +128,7 @@\n         \"AnimateDiffVideoToVideoControlNetPipeline\",\n     ]\n     _import_structure[\"bria\"] = [\"BriaPipeline\"]\n+    _import_structure[\"bria_fibo\"] = [\"BriaFiboPipeline\"]\n     _import_structure[\"flux\"] = [\n         \"FluxControlPipeline\",\n         \"FluxControlInpaintPipeline\",\n@@ -562,6 +563,7 @@\n         from .aura_flow import AuraFlowPipeline\n         from .blip_diffusion import BlipDiffusionPipeline\n         from .bria import BriaPipeline\n+        from .bria_fibo import BriaFiboPipeline\n         from .chroma import ChromaImg2ImgPipeline, ChromaPipeline\n         from .cogvideo import (\n             CogVideoXFunControlPipeline,"
        },
        {
          "filename": "src/diffusers/pipelines/bria_fibo/__init__.py",
          "status": "added",
          "additions": 48,
          "deletions": 0,
          "changes": 48,
          "patch": "@@ -0,0 +1,48 @@\n+from typing import TYPE_CHECKING\n+\n+from ...utils import (\n+    DIFFUSERS_SLOW_IMPORT,\n+    OptionalDependencyNotAvailable,\n+    _LazyModule,\n+    get_objects_from_module,\n+    is_torch_available,\n+    is_transformers_available,\n+)\n+\n+\n+_dummy_objects = {}\n+_import_structure = {}\n+\n+\n+try:\n+    if not (is_transformers_available() and is_torch_available()):\n+        raise OptionalDependencyNotAvailable()\n+except OptionalDependencyNotAvailable:\n+    from ...utils import dummy_torch_and_transformers_objects  # noqa F403\n+\n+    _dummy_objects.update(get_objects_from_module(dummy_torch_and_transformers_objects))\n+else:\n+    _import_structure[\"pipeline_bria_fibo\"] = [\"BriaFiboPipeline\"]\n+\n+if TYPE_CHECKING or DIFFUSERS_SLOW_IMPORT:\n+    try:\n+        if not (is_transformers_available() and is_torch_available()):\n+            raise OptionalDependencyNotAvailable()\n+\n+    except OptionalDependencyNotAvailable:\n+        from ...utils.dummy_torch_and_transformers_objects import *\n+    else:\n+        from .pipeline_bria_fibo import BriaFiboPipeline\n+\n+else:\n+    import sys\n+\n+    sys.modules[__name__] = _LazyModule(\n+        __name__,\n+        globals()[\"__file__\"],\n+        _import_structure,\n+        module_spec=__spec__,\n+    )\n+\n+    for name, value in _dummy_objects.items():\n+        setattr(sys.modules[__name__], name, value)"
        },
        {
          "filename": "src/diffusers/pipelines/bria_fibo/pipeline_bria_fibo.py",
          "status": "added",
          "additions": 838,
          "deletions": 0,
          "changes": 838,
          "patch": "@@ -0,0 +1,838 @@\n+# Copyright (c) Bria.ai. All rights reserved.\n+#\n+# This file is licensed under the Creative Commons Attribution-NonCommercial 4.0 International Public License (CC-BY-NC-4.0).\n+# You may obtain a copy of the license at https://creativecommons.org/licenses/by-nc/4.0/\n+#\n+# You are free to share and adapt this material for non-commercial purposes provided you give appropriate credit,\n+# indicate if changes were made, and do not use the material for commercial purposes.\n+#\n+# See the license for further details.\n+\n+from typing import Any, Callable, Dict, List, Optional, Union\n+\n+import numpy as np\n+import torch\n+from transformers import AutoTokenizer\n+from transformers.models.smollm3.modeling_smollm3 import SmolLM3ForCausalLM\n+\n+from ...image_processor import VaeImageProcessor\n+from ...loaders import FluxLoraLoaderMixin\n+from ...models.autoencoders.autoencoder_kl_wan import AutoencoderKLWan\n+from ...models.transformers.transformer_bria_fibo import BriaFiboTransformer2DModel\n+from ...pipelines.bria_fibo.pipeline_output import BriaFiboPipelineOutput\n+from ...pipelines.flux.pipeline_flux import calculate_shift, retrieve_timesteps\n+from ...pipelines.pipeline_utils import DiffusionPipeline\n+from ...schedulers import FlowMatchEulerDiscreteScheduler, KarrasDiffusionSchedulers\n+from ...utils import (\n+    USE_PEFT_BACKEND,\n+    is_torch_xla_available,\n+    logging,\n+    replace_example_docstring,\n+    scale_lora_layers,\n+    unscale_lora_layers,\n+)\n+from ...utils.torch_utils import randn_tensor\n+\n+\n+if is_torch_xla_available():\n+    import torch_xla.core.xla_model as xm\n+\n+    XLA_AVAILABLE = True\n+else:\n+    XLA_AVAILABLE = False\n+\n+\n+logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n+\n+EXAMPLE_DOC_STRING = \"\"\"\n+    Example:\n+    ```python\n+    import torch\n+    from diffusers import BriaFiboPipeline\n+    from diffusers.modular_pipelines import ModularPipeline\n+\n+    torch.set_grad_enabled(False)\n+    vlm_pipe = ModularPipeline.from_pretrained(\"briaai/FIBO-VLM-prompt-to-JSON\", trust_remote_code=True)\n+\n+    pipe = BriaFiboPipeline.from_pretrained(\n+        \"briaai/FIBO\",\n+        trust_remote_code=True,\n+        torch_dtype=torch.bfloat16,\n+    )\n+    pipe.enable_model_cpu_offload()\n+\n+    with torch.inference_mode():\n+        # 1. Create a prompt to generate an initial image\n+        output = vlm_pipe(prompt=\"a beautiful dog\")\n+        json_prompt_generate = output.values[\"json_prompt\"]\n+\n+        # Generate the image from the structured json prompt\n+        results_generate = pipe(prompt=json_prompt_generate, num_inference_steps=50, guidance_scale=5)\n+        results_generate.images[0].save(\"image_generate.png\")\n+    ```\n+\"\"\"\n+\n+\n+class BriaFiboPipeline(DiffusionPipeline):\n+    r\"\"\"\n+    Args:\n+        transformer (`BriaFiboTransformer2DModel`):\n+            The transformer model for 2D diffusion modeling.\n+        scheduler (`FlowMatchEulerDiscreteScheduler` or `KarrasDiffusionSchedulers`):\n+            Scheduler to be used with `transformer` to denoise the encoded latents.\n+        vae (`AutoencoderKLWan`):\n+            Variational Auto-Encoder for encoding and decoding images to and from latent representations.\n+        text_encoder (`SmolLM3ForCausalLM`):\n+            Text encoder for processing input prompts.\n+        tokenizer (`AutoTokenizer`):\n+            Tokenizer used for processing the input text prompts for the text_encoder.\n+    \"\"\"\n+\n+    model_cpu_offload_seq = \"text_encoder->text_encoder_2->image_encoder->transformer->vae\"\n+    _callback_tensor_inputs = [\"latents\", \"prompt_embeds\"]\n+\n+    def __init__(\n+        self,\n+        transformer: BriaFiboTransformer2DModel,\n+        scheduler: Union[FlowMatchEulerDiscreteScheduler, KarrasDiffusionSchedulers],\n+        vae: AutoencoderKLWan,\n+        text_encoder: SmolLM3ForCausalLM,\n+        tokenizer: AutoTokenizer,\n+    ):\n+        self.register_modules(\n+            vae=vae,\n+            text_encoder=text_encoder,\n+            tokenizer=tokenizer,\n+            transformer=transformer,\n+            scheduler=scheduler,\n+        )\n+\n+        self.vae_scale_factor = 16\n+        self.image_processor = VaeImageProcessor(vae_scale_factor=self.vae_scale_factor * 2)\n+        self.default_sample_size = 64\n+\n+    def get_prompt_embeds(\n+        self,\n+        prompt: Union[str, List[str]],\n+        num_images_per_prompt: int = 1,\n+        max_sequence_length: int = 2048,\n+        device: Optional[torch.device] = None,\n+        dtype: Optional[torch.dtype] = None,\n+    ):\n+        device = device or self._execution_device\n+        dtype = dtype or self.text_encoder.dtype\n+\n+        prompt = [prompt] if isinstance(prompt, str) else prompt\n+        if not prompt:\n+            raise ValueError(\"`prompt` must be a non-empty string or list of strings.\")\n+\n+        batch_size = len(prompt)\n+        bot_token_id = 128000\n+\n+        text_encoder_device = device if device is not None else torch.device(\"cpu\")\n+        if not isinstance(text_encoder_device, torch.device):\n+            text_encoder_device = torch.device(text_encoder_device)\n+\n+        if all(p == \"\" for p in prompt):\n+            input_ids = torch.full((batch_size, 1), bot_token_id, dtype=torch.long, device=text_encoder_device)\n+            attention_mask = torch.ones_like(input_ids)\n+        else:\n+            tokenized = self.tokenizer(\n+                prompt,\n+                padding=\"longest\",\n+                max_length=max_sequence_length,\n+                truncation=True,\n+                add_special_tokens=True,\n+                return_tensors=\"pt\",\n+            )\n+            input_ids = tokenized.input_ids.to(text_encoder_device)\n+            attention_mask = tokenized.attention_mask.to(text_encoder_device)\n+\n+            if any(p == \"\" for p in prompt):\n+                empty_rows = torch.tensor([p == \"\" for p in prompt], dtype=torch.bool, device=text_encoder_device)\n+                input_ids[empty_rows] = bot_token_id\n+                attention_mask[empty_rows] = 1\n+\n+        encoder_outputs = self.text_encoder(\n+            input_ids,\n+            attention_mask=attention_mask,\n+            output_hidden_states=True,\n+        )\n+        hidden_states = encoder_outputs.hidden_states\n+\n+        prompt_embeds = torch.cat([hidden_states[-1], hidden_states[-2]], dim=-1)\n+        prompt_embeds = prompt_embeds.to(device=device, dtype=dtype)\n+\n+        prompt_embeds = prompt_embeds.repeat_interleave(num_images_per_prompt, dim=0)\n+        hidden_states = tuple(\n+            layer.repeat_interleave(num_images_per_prompt, dim=0).to(device=device) for layer in hidden_states\n+        )\n+        attention_mask = attention_mask.repeat_interleave(num_images_per_prompt, dim=0).to(device=device)\n+\n+        return prompt_embeds, hidden_states, attention_mask\n+\n+    @staticmethod\n+    def pad_embedding(prompt_embeds, max_tokens, attention_mask=None):\n+        # Pad embeddings to `max_tokens` while preserving the mask of real tokens.\n+        batch_size, seq_len, dim = prompt_embeds.shape\n+\n+        if attention_mask is None:\n+            attention_mask = torch.ones((batch_size, seq_len), dtype=prompt_embeds.dtype, device=prompt_embeds.device)\n+        else:\n+            attention_mask = attention_mask.to(device=prompt_embeds.device, dtype=prompt_embeds.dtype)\n+\n+        if max_tokens < seq_len:\n+            raise ValueError(\"`max_tokens` must be greater or equal to the current sequence length.\")\n+\n+        if max_tokens > seq_len:\n+            pad_length = max_tokens - seq_len\n+            padding = torch.zeros(\n+                (batch_size, pad_length, dim), dtype=prompt_embeds.dtype, device=prompt_embeds.device\n+            )\n+            prompt_embeds = torch.cat([prompt_embeds, padding], dim=1)\n+\n+            mask_padding = torch.zeros(\n+                (batch_size, pad_length), dtype=prompt_embeds.dtype, device=prompt_embeds.device\n+            )\n+            attention_mask = torch.cat([attention_mask, mask_padding], dim=1)\n+\n+        return prompt_embeds, attention_mask\n+\n+    def encode_prompt(\n+        self,\n+        prompt: Union[str, List[str]],\n+        device: Optional[torch.device] = None,\n+        num_images_per_prompt: int = 1,\n+        guidance_scale: float = 5,\n+        negative_prompt: Optional[Union[str, List[str]]] = None,\n+        prompt_embeds: Optional[torch.FloatTensor] = None,\n+        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n+        max_sequence_length: int = 3000,\n+        lora_scale: Optional[float] = None,\n+    ):\n+        r\"\"\"\n+        Args:\n+            prompt (`str` or `List[str]`, *optional*):\n+                prompt to be encoded\n+            device: (`torch.device`):\n+                torch device\n+            num_images_per_prompt (`int`):\n+                number of images that should be generated per prompt\n+            guidance_scale (`float`):\n+                Guidance scale for classifier free guidance.\n+            negative_prompt (`str` or `List[str]`, *optional*):\n+                The prompt or prompts not to guide the image generation. If not defined, one has to pass\n+                `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is\n+                less than `1`).\n+            prompt_embeds (`torch.FloatTensor`, *optional*):\n+                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n+                provided, text embeddings will be generated from `prompt` input argument.\n+            negative_prompt_embeds (`torch.FloatTensor`, *optional*):\n+                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt\n+                weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt` input\n+                argument.\n+        \"\"\"\n+        device = device or self._execution_device\n+\n+        # set lora scale so that monkey patched LoRA\n+        # function of text encoder can correctly access it\n+        if lora_scale is not None and isinstance(self, FluxLoraLoaderMixin):\n+            self._lora_scale = lora_scale\n+\n+            # dynamically adjust the LoRA scale\n+            if self.text_encoder is not None and USE_PEFT_BACKEND:\n+                scale_lora_layers(self.text_encoder, lora_scale)\n+\n+        prompt = [prompt] if isinstance(prompt, str) else prompt\n+        if prompt is not None:\n+            batch_size = len(prompt)\n+        else:\n+            batch_size = prompt_embeds.shape[0]\n+\n+        prompt_attention_mask = None\n+        negative_prompt_attention_mask = None\n+        if prompt_embeds is None:\n+            prompt_embeds, prompt_layers, prompt_attention_mask = self.get_prompt_embeds(\n+                prompt=prompt,\n+                num_images_per_prompt=num_images_per_prompt,\n+                max_sequence_length=max_sequence_length,\n+                device=device,\n+            )\n+            prompt_embeds = prompt_embeds.to(dtype=self.transformer.dtype)\n+            prompt_layers = [tensor.to(dtype=self.transformer.dtype) for tensor in prompt_layers]\n+\n+        if guidance_scale > 1:\n+            if isinstance(negative_prompt, list) and negative_prompt[0] is None:\n+                negative_prompt = \"\"\n+            negative_prompt = negative_prompt or \"\"\n+            negative_prompt = batch_size * [negative_prompt] if isinstance(negative_prompt, str) else negative_prompt\n+            if prompt is not None and type(prompt) is not type(negative_prompt):\n+                raise TypeError(\n+                    f\"`negative_prompt` should be the same type to `prompt`, but got {type(negative_prompt)} !=\"\n+                    f\" {type(prompt)}.\"\n+                )\n+            elif batch_size != len(negative_prompt):\n+                raise ValueError(\n+                    f\"`negative_prompt`: {negative_prompt} has batch size {len(negative_prompt)}, but `prompt`:\"\n+                    f\" {prompt} has batch size {batch_size}. Please make sure that passed `negative_prompt` matches\"\n+                    \" the batch size of `prompt`.\"\n+                )\n+\n+            negative_prompt_embeds, negative_prompt_layers, negative_prompt_attention_mask = self.get_prompt_embeds(\n+                prompt=negative_prompt,\n+                num_images_per_prompt=num_images_per_prompt,\n+                max_sequence_length=max_sequence_length,\n+                device=device,\n+            )\n+            negative_prompt_embeds = negative_prompt_embeds.to(dtype=self.transformer.dtype)\n+            negative_prompt_layers = [tensor.to(dtype=self.transformer.dtype) for tensor in negative_prompt_layers]\n+\n+        if self.text_encoder is not None:\n+            if isinstance(self, FluxLoraLoaderMixin) and USE_PEFT_BACKEND:\n+                # Retrieve the original scale by scaling back the LoRA layers\n+                unscale_lora_layers(self.text_encoder, lora_scale)\n+\n+        # Pad to longest\n+        if prompt_attention_mask is not None:\n+            prompt_attention_mask = prompt_attention_mask.to(device=prompt_embeds.device, dtype=prompt_embeds.dtype)\n+\n+        if negative_prompt_embeds is not None:\n+            if negative_prompt_attention_mask is not None:\n+                negative_prompt_attention_mask = negative_prompt_attention_mask.to(\n+                    device=negative_prompt_embeds.device, dtype=negative_prompt_embeds.dtype\n+                )\n+            max_tokens = max(negative_prompt_embeds.shape[1], prompt_embeds.shape[1])\n+\n+            prompt_embeds, prompt_attention_mask = self.pad_embedding(\n+                prompt_embeds, max_tokens, attention_mask=prompt_attention_mask\n+            )\n+            prompt_layers = [self.pad_embedding(layer, max_tokens)[0] for layer in prompt_layers]\n+\n+            negative_prompt_embeds, negative_prompt_attention_mask = self.pad_embedding(\n+                negative_prompt_embeds, max_tokens, attention_mask=negative_prompt_attention_mask\n+            )\n+            negative_prompt_layers = [self.pad_embedding(layer, max_tokens)[0] for layer in negative_prompt_layers]\n+        else:\n+            max_tokens = prompt_embeds.shape[1]\n+            prompt_embeds, prompt_attention_mask = self.pad_embedding(\n+                prompt_embeds, max_tokens, attention_mask=prompt_attention_mask\n+            )\n+            negative_prompt_layers = None\n+\n+        dtype = self.text_encoder.dtype\n+        text_ids = torch.zeros(prompt_embeds.shape[0], max_tokens, 3).to(device=device, dtype=dtype)\n+\n+        return (\n+            prompt_embeds,\n+            negative_prompt_embeds,\n+            text_ids,\n+            prompt_attention_mask,\n+            negative_prompt_attention_mask,\n+            prompt_layers,\n+            negative_prompt_layers,\n+        )\n+\n+    @property\n+    def guidance_scale(self):\n+        return self._guidance_scale\n+\n+    # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)\n+    # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`\n+    # corresponds to doing no classifier free guidance.\n+\n+    @property\n+    def joint_attention_kwargs(self):\n+        return self._joint_attention_kwargs\n+\n+    @property\n+    def num_timesteps(self):\n+        return self._num_timesteps\n+\n+    @property\n+    def interrupt(self):\n+        return self._interrupt\n+\n+    @staticmethod\n+    # Based on diffusers.pipelines.flux.pipeline_flux.FluxPipeline._unpack_latents\n+    def _unpack_latents(latents, height, width, vae_scale_factor):\n+        batch_size, num_patches, channels = latents.shape\n+\n+        height = height // vae_scale_factor\n+        width = width // vae_scale_factor\n+\n+        latents = latents.view(batch_size, height // 2, width // 2, channels // 4, 2, 2)\n+        latents = latents.permute(0, 3, 1, 4, 2, 5)\n+\n+        latents = latents.reshape(batch_size, channels // (2 * 2), height, width)\n+        return latents\n+\n+    @staticmethod\n+    # Copied from diffusers.pipelines.flux.pipeline_flux.FluxPipeline._prepare_latent_image_ids\n+    def _prepare_latent_image_ids(batch_size, height, width, device, dtype):\n+        latent_image_ids = torch.zeros(height, width, 3)\n+        latent_image_ids[..., 1] = latent_image_ids[..., 1] + torch.arange(height)[:, None]\n+        latent_image_ids[..., 2] = latent_image_ids[..., 2] + torch.arange(width)[None, :]\n+\n+        latent_image_id_height, latent_image_id_width, latent_image_id_channels = latent_image_ids.shape\n+\n+        latent_image_ids = latent_image_ids.reshape(\n+            latent_image_id_height * latent_image_id_width, latent_image_id_channels\n+        )\n+\n+        return latent_image_ids.to(device=device, dtype=dtype)\n+\n+    @staticmethod\n+    def _unpack_latents_no_patch(latents, height, width, vae_scale_factor):\n+        batch_size, num_patches, channels = latents.shape\n+\n+        height = height // vae_scale_factor\n+        width = width // vae_scale_factor\n+\n+        latents = latents.view(batch_size, height, width, channels)\n+        latents = latents.permute(0, 3, 1, 2)\n+\n+        return latents\n+\n+    @staticmethod\n+    def _pack_latents_no_patch(latents, batch_size, num_channels_latents, height, width):\n+        latents = latents.permute(0, 2, 3, 1)\n+        latents = latents.reshape(batch_size, height * width, num_channels_latents)\n+        return latents\n+\n+    @staticmethod\n+    # Copied from diffusers.pipelines.flux.pipeline_flux.FluxPipeline._pack_latents\n+    def _pack_latents(latents, batch_size, num_channels_latents, height, width):\n+        latents = latents.view(batch_size, num_channels_latents, height // 2, 2, width // 2, 2)\n+        latents = latents.permute(0, 2, 4, 1, 3, 5)\n+        latents = latents.reshape(batch_size, (height // 2) * (width // 2), num_channels_latents * 4)\n+\n+        return latents\n+\n+    def prepare_latents(\n+        self,\n+        batch_size,\n+        num_channels_latents,\n+        height,\n+        width,\n+        dtype,\n+        device,\n+        generator,\n+        latents=None,\n+        do_patching=False,\n+    ):\n+        height = int(height) // self.vae_scale_factor\n+        width = int(width) // self.vae_scale_factor\n+\n+        shape = (batch_size, num_channels_latents, height, width)\n+\n+        if latents is not None:\n+            latent_image_ids = self._prepare_latent_image_ids(batch_size, height, width, device, dtype)\n+            return latents.to(device=device, dtype=dtype), latent_image_ids\n+\n+        if isinstance(generator, list) and len(generator) != batch_size:\n+            raise ValueError(\n+                f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n+                f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n+            )\n+\n+        latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n+        if do_patching:\n+            latents = self._pack_latents(latents, batch_size, num_channels_latents, height, width)\n+            latent_image_ids = self._prepare_latent_image_ids(batch_size, height // 2, width // 2, device, dtype)\n+        else:\n+            latents = self._pack_latents_no_patch(latents, batch_size, num_channels_latents, height, width)\n+            latent_image_ids = self._prepare_latent_image_ids(batch_size, height, width, device, dtype)\n+\n+        return latents, latent_image_ids\n+\n+    @staticmethod\n+    def _prepare_attention_mask(attention_mask):\n+        attention_matrix = torch.einsum(\"bi,bj->bij\", attention_mask, attention_mask)\n+\n+        # convert to 0 - keep, -inf ignore\n+        attention_matrix = torch.where(\n+            attention_matrix == 1, 0.0, -torch.inf\n+        )  # Apply -inf to ignored tokens for nulling softmax score\n+        return attention_matrix\n+\n+    @torch.no_grad()\n+    @replace_example_docstring(EXAMPLE_DOC_STRING)\n+    def __call__(\n+        self,\n+        prompt: Union[str, List[str]] = None,\n+        height: Optional[int] = None,\n+        width: Optional[int] = None,\n+        num_inference_steps: int = 30,\n+        timesteps: List[int] = None,\n+        guidance_scale: float = 5,\n+        negative_prompt: Optional[Union[str, List[str]]] = None,\n+        num_images_per_prompt: Optional[int] = 1,\n+        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n+        latents: Optional[torch.FloatTensor] = None,\n+        prompt_embeds: Optional[torch.FloatTensor] = None,\n+        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n+        output_type: Optional[str] = \"pil\",\n+        return_dict: bool = True,\n+        joint_attention_kwargs: Optional[Dict[str, Any]] = None,\n+        callback_on_step_end: Optional[Callable[[int, int, Dict], None]] = None,\n+        callback_on_step_end_tensor_inputs: List[str] = [\"latents\"],\n+        max_sequence_length: int = 3000,\n+        do_patching=False,\n+    ):\n+        r\"\"\"\n+        Function invoked when calling the pipeline for generation.\n+\n+        Args:\n+            prompt (`str` or `List[str]`, *optional*):\n+                The prompt or prompts to guide the image generation. If not defined, one has to pass `prompt_embeds`.\n+                instead.\n+            height (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n+                The height in pixels of the generated image. This is set to 1024 by default for the best results.\n+            width (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n+                The width in pixels of the generated image. This is set to 1024 by default for the best results.\n+            num_inference_steps (`int`, *optional*, defaults to 50):\n+                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n+                expense of slower inference.\n+            timesteps (`List[int]`, *optional*):\n+                Custom timesteps to use for the denoising process with schedulers which support a `timesteps` argument\n+                in their `set_timesteps` method. If not defined, the default behavior when `num_inference_steps` is\n+                passed will be used. Must be in descending order.\n+            guidance_scale (`float`, *optional*, defaults to 5.0):\n+                Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).\n+                `guidance_scale` is defined as `w` of equation 2. of [Imagen\n+                Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale >\n+                1`. Higher guidance scale encourages to generate images that are closely linked to the text `prompt`,\n+                usually at the expense of lower image quality.\n+            negative_prompt (`str` or `List[str]`, *optional*):\n+                The prompt or prompts not to guide the image generation. If not defined, one has to pass\n+                `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is\n+                less than `1`).\n+            num_images_per_prompt (`int`, *optional*, defaults to 1):\n+                The number of images to generate per prompt.\n+            generator (`torch.Generator` or `List[torch.Generator]`, *optional*):\n+                One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)\n+                to make generation deterministic.\n+            latents (`torch.FloatTensor`, *optional*):\n+                Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for image\n+                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents\n+                tensor will ge generated by sampling using the supplied random `generator`.\n+            prompt_embeds (`torch.FloatTensor`, *optional*):\n+                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n+                provided, text embeddings will be generated from `prompt` input argument.\n+            negative_prompt_embeds (`torch.FloatTensor`, *optional*):\n+                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt\n+                weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt` input\n+                argument.\n+            output_type (`str`, *optional*, defaults to `\"pil\"`):\n+                The output format of the generate image. Choose between\n+                [PIL](https://pillow.readthedocs.io/en/stable/): `PIL.Image.Image` or `np.array`.\n+            return_dict (`bool`, *optional*, defaults to `True`):\n+                Whether or not to return a [`~pipelines.stable_diffusion_xl.StableDiffusionXLPipelineOutput`] instead\n+                of a plain tuple.\n+            joint_attention_kwargs (`dict`, *optional*):\n+                A kwargs dictionary that if specified is passed along to the `AttentionProcessor` as defined under\n+                `self.processor` in\n+                [diffusers.models.attention_processor](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).\n+            callback_on_step_end (`Callable`, *optional*):\n+                A function that calls at the end of each denoising steps during the inference. The function is called\n+                with the following arguments: `callback_on_step_end(self: DiffusionPipeline, step: int, timestep: int,\n+                callback_kwargs: Dict)`. `callback_kwargs` will include a list of all tensors as specified by\n+                `callback_on_step_end_tensor_inputs`.\n+            callback_on_step_end_tensor_inputs (`List`, *optional*):\n+                The list of tensor inputs for the `callback_on_step_end` function. The tensors specified in the list\n+                will be passed as `callback_kwargs` argument. You will only be able to include variables listed in the\n+                `._callback_tensor_inputs` attribute of your pipeline class.\n+            max_sequence_length (`int` defaults to 3000): Maximum sequence length to use with the `prompt`.\n+            do_patching (`bool`, *optional*, defaults to `False`): Whether to use patching.\n+        Examples:\n+          Returns:\n+            [`~pipelines.flux.BriaFiboPipelineOutput`] or `tuple`: [`~pipelines.flux.BriaFiboPipelineOutput`] if\n+            `return_dict` is True, otherwise a `tuple`. When returning a tuple, the first element is a list with the\n+            generated images.\n+        \"\"\"\n+\n+        height = height or self.default_sample_size * self.vae_scale_factor\n+        width = width or self.default_sample_size * self.vae_scale_factor\n+\n+        # 1. Check inputs. Raise error if not correct\n+        self.check_inputs(\n+            prompt=prompt,\n+            height=height,\n+            width=width,\n+            prompt_embeds=prompt_embeds,\n+            callback_on_step_end_tensor_inputs=callback_on_step_end_tensor_inputs,\n+            max_sequence_length=max_sequence_length,\n+        )\n+\n+        self._guidance_scale = guidance_scale\n+        self._joint_attention_kwargs = joint_attention_kwargs\n+        self._interrupt = False\n+\n+        # 2. Define call parameters\n+        if prompt is not None and isinstance(prompt, str):\n+            batch_size = 1\n+        elif prompt is not None and isinstance(prompt, list):\n+            batch_size = len(prompt)\n+        else:\n+            batch_size = prompt_embeds.shape[0]\n+\n+        device = self._execution_device\n+\n+        lora_scale = (\n+            self.joint_attention_kwargs.get(\"scale\", None) if self.joint_attention_kwargs is not None else None\n+        )\n+\n+        (\n+            prompt_embeds,\n+            negative_prompt_embeds,\n+            text_ids,\n+            prompt_attention_mask,\n+            negative_prompt_attention_mask,\n+            prompt_layers,\n+            negative_prompt_layers,\n+        ) = self.encode_prompt(\n+            prompt=prompt,\n+            negative_prompt=negative_prompt,\n+            guidance_scale=guidance_scale,\n+            prompt_embeds=prompt_embeds,\n+            negative_prompt_embeds=negative_prompt_embeds,\n+            device=device,\n+            max_sequence_length=max_sequence_length,\n+            num_images_per_prompt=num_images_per_prompt,\n+            lora_scale=lora_scale,\n+        )\n+        prompt_batch_size = prompt_embeds.shape[0]\n+\n+        if guidance_scale > 1:\n+            prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds], dim=0)\n+            prompt_layers = [\n+                torch.cat([negative_prompt_layers[i], prompt_layers[i]], dim=0) for i in range(len(prompt_layers))\n+            ]\n+            prompt_attention_mask = torch.cat([negative_prompt_attention_mask, prompt_attention_mask], dim=0)\n+\n+        total_num_layers_transformer = len(self.transformer.transformer_blocks) + len(\n+            self.transformer.single_transformer_blocks\n+        )\n+        if len(prompt_layers) >= total_num_layers_transformer:\n+            # remove first layers\n+            prompt_layers = prompt_layers[len(prompt_layers) - total_num_layers_transformer :]\n+        else:\n+            # duplicate last layer\n+            prompt_layers = prompt_layers + [prompt_layers[-1]] * (total_num_layers_transformer - len(prompt_layers))\n+\n+        # 5. Prepare latent variables\n+\n+        num_channels_latents = self.transformer.config.in_channels\n+        if do_patching:\n+            num_channels_latents = int(num_channels_latents / 4)\n+\n+        latents, latent_image_ids = self.prepare_latents(\n+            prompt_batch_size,\n+            num_channels_latents,\n+            height,\n+            width,\n+            prompt_embeds.dtype,\n+            device,\n+            generator,\n+            latents,\n+            do_patching,\n+        )\n+\n+        latent_attention_mask = torch.ones(\n+            [latents.shape[0], latents.shape[1]], dtype=latents.dtype, device=latents.device\n+        )\n+        if guidance_scale > 1:\n+            latent_attention_mask = latent_attention_mask.repeat(2, 1)\n+\n+        attention_mask = torch.cat([prompt_attention_mask, latent_attention_mask], dim=1)\n+        attention_mask = self._prepare_attention_mask(attention_mask)  # batch, seq => batch, seq, seq\n+        attention_mask = attention_mask.unsqueeze(dim=1).to(dtype=self.transformer.dtype)  # for head broadcasting\n+\n+        if self._joint_attention_kwargs is None:\n+            self._joint_attention_kwargs = {}\n+        self._joint_attention_kwargs[\"attention_mask\"] = attention_mask\n+\n+        # Adapt scheduler to dynamic shifting (resolution dependent)\n+\n+        if do_patching:\n+            seq_len = (height // (self.vae_scale_factor * 2)) * (width // (self.vae_scale_factor * 2))\n+        else:\n+            seq_len = (height // self.vae_scale_factor) * (width // self.vae_scale_factor)\n+\n+        sigmas = np.linspace(1.0, 1 / num_inference_steps, num_inference_steps)\n+\n+        mu = calculate_shift(\n+            seq_len,\n+            self.scheduler.config.base_image_seq_len,\n+            self.scheduler.config.max_image_seq_len,\n+            self.scheduler.config.base_shift,\n+            self.scheduler.config.max_shift,\n+        )\n+\n+        # Init sigmas and timesteps according to shift size\n+        # This changes the scheduler in-place according to the dynamic scheduling\n+        timesteps, num_inference_steps = retrieve_timesteps(\n+            self.scheduler,\n+            num_inference_steps=num_inference_steps,\n+            device=device,\n+            timesteps=None,\n+            sigmas=sigmas,\n+            mu=mu,\n+        )\n+\n+        num_warmup_steps = max(len(timesteps) - num_inference_steps * self.scheduler.order, 0)\n+        self._num_timesteps = len(timesteps)\n+\n+        # Support old different diffusers versions\n+        if len(latent_image_ids.shape) == 3:\n+            latent_image_ids = latent_image_ids[0]\n+\n+        if len(text_ids.shape) == 3:\n+            text_ids = text_ids[0]\n+\n+        # 6. Denoising loop\n+        with self.progress_bar(total=num_inference_steps) as progress_bar:\n+            for i, t in enumerate(timesteps):\n+                if self.interrupt:\n+                    continue\n+\n+                # expand the latents if we are doing classifier free guidance\n+                latent_model_input = torch.cat([latents] * 2) if guidance_scale > 1 else latents\n+\n+                # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n+                timestep = t.expand(latent_model_input.shape[0]).to(\n+                    device=latent_model_input.device, dtype=latent_model_input.dtype\n+                )\n+\n+                # This is predicts \"v\" from flow-matching or eps from diffusion\n+                noise_pred = self.transformer(\n+                    hidden_states=latent_model_input,\n+                    timestep=timestep,\n+                    encoder_hidden_states=prompt_embeds,\n+                    text_encoder_layers=prompt_layers,\n+                    joint_attention_kwargs=self.joint_attention_kwargs,\n+                    return_dict=False,\n+                    txt_ids=text_ids,\n+                    img_ids=latent_image_ids,\n+                )[0]\n+\n+                # perform guidance\n+                if guidance_scale > 1:\n+                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n+                    noise_pred = noise_pred_uncond + self.guidance_scale * (noise_pred_text - noise_pred_uncond)\n+\n+                # compute the previous noisy sample x_t -> x_t-1\n+                latents_dtype = latents.dtype\n+                latents = self.scheduler.step(noise_pred, t, latents, return_dict=False)[0]\n+\n+                if latents.dtype != latents_dtype:\n+                    if torch.backends.mps.is_available():\n+                        # some platforms (eg. apple mps) misbehave due to a pytorch bug: https://github.com/pytorch/pytorch/pull/99272\n+                        latents = latents.to(latents_dtype)\n+\n+                if callback_on_step_end is not None:\n+                    callback_kwargs = {}\n+                    for k in callback_on_step_end_tensor_inputs:\n+                        callback_kwargs[k] = locals()[k]\n+                    callback_outputs = callback_on_step_end(self, i, t, callback_kwargs)\n+\n+                    latents = callback_outputs.pop(\"latents\", latents)\n+                    prompt_embeds = callback_outputs.pop(\"prompt_embeds\", prompt_embeds)\n+                    negative_prompt_embeds = callback_outputs.pop(\"negative_prompt_embeds\", negative_prompt_embeds)\n+\n+                # call the callback, if provided\n+                if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n+                    progress_bar.update()\n+\n+                if XLA_AVAILABLE:\n+                    xm.mark_step()\n+\n+        if output_type == \"latent\":\n+            image = latents\n+\n+        else:\n+            if do_patching:\n+                latents = self._unpack_latents(latents, height, width, self.vae_scale_factor)\n+            else:\n+                latents = self._unpack_latents_no_patch(latents, height, width, self.vae_scale_factor)\n+\n+            latents = latents.unsqueeze(dim=2)\n+            latents_device = latents[0].device\n+            latents_dtype = latents[0].dtype\n+            latents_mean = (\n+                torch.tensor(self.vae.config.latents_mean)\n+                .view(1, self.vae.config.z_dim, 1, 1, 1)\n+                .to(latents_device, latents_dtype)\n+            )\n+            latents_std = 1.0 / torch.tensor(self.vae.config.latents_std).view(1, self.vae.config.z_dim, 1, 1, 1).to(\n+                latents_device, latents_dtype\n+            )\n+            latents_scaled = [latent / latents_std + latents_mean for latent in latents]\n+            latents_scaled = torch.cat(latents_scaled, dim=0)\n+            image = []\n+            for scaled_latent in latents_scaled:\n+                curr_image = self.vae.decode(scaled_latent.unsqueeze(0), return_dict=False)[0]\n+                curr_image = self.image_processor.postprocess(curr_image.squeeze(dim=2), output_type=output_type)\n+                image.append(curr_image)\n+            if len(image) == 1:\n+                image = image[0]\n+            else:\n+                image = np.stack(image, axis=0)\n+\n+        # Offload all models\n+        self.maybe_free_model_hooks()\n+\n+        if not return_dict:\n+            return (image,)\n+\n+        return BriaFiboPipelineOutput(images=image)\n+\n+    def check_inputs(\n+        self,\n+        prompt,\n+        height,\n+        width,\n+        negative_prompt=None,\n+        prompt_embeds=None,\n+        negative_prompt_embeds=None,\n+        callback_on_step_end_tensor_inputs=None,\n+        max_sequence_length=None,\n+    ):\n+        if height % 16 != 0 or width % 16 != 0:\n+            raise ValueError(f\"`height` and `width` have to be divisible by 16 but are {height} and {width}.\")\n+\n+        if callback_on_step_end_tensor_inputs is not None and not all(\n+            k in self._callback_tensor_inputs for k in callback_on_step_end_tensor_inputs\n+        ):\n+            raise ValueError(\n+                f\"`callback_on_step_end_tensor_inputs` has to be in {self._callback_tensor_inputs}, but found {[k for k in callback_on_step_end_tensor_inputs if k not in self._callback_tensor_inputs]}\"\n+            )\n+\n+        if prompt is not None and prompt_embeds is not None:\n+            raise ValueError(\n+                f\"Cannot forward both `prompt`: {prompt} and `prompt_embeds`: {prompt_embeds}. Please make sure to\"\n+                \" only forward one of the two.\"\n+            )\n+        elif prompt is None and prompt_embeds is None:\n+            raise ValueError(\n+                \"Provide either `prompt` or `prompt_embeds`. Cannot leave both `prompt` and `prompt_embeds` undefined.\"\n+            )\n+        elif prompt is not None and (not isinstance(prompt, str) and not isinstance(prompt, list)):\n+            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n+\n+        if negative_prompt is not None and negative_prompt_embeds is not None:\n+            raise ValueError(\n+                f\"Cannot forward both `negative_prompt`: {negative_prompt} and `negative_prompt_embeds`:\"\n+                f\" {negative_prompt_embeds}. Please make sure to only forward one of the two.\"\n+            )\n+\n+        if prompt_embeds is not None and negative_prompt_embeds is not None:\n+            if prompt_embeds.shape != negative_prompt_embeds.shape:\n+                raise ValueError(\n+                    \"`prompt_embeds` and `negative_prompt_embeds` must have the same shape when passed directly, but\"\n+                    f\" got: `prompt_embeds` {prompt_embeds.shape} != `negative_prompt_embeds`\"\n+                    f\" {negative_prompt_embeds.shape}.\"\n+                )\n+\n+        if max_sequence_length is not None and max_sequence_length > 3000:\n+            raise ValueError(f\"`max_sequence_length` cannot be greater than 3000 but is {max_sequence_length}\")"
        },
        {
          "filename": "src/diffusers/pipelines/bria_fibo/pipeline_output.py",
          "status": "added",
          "additions": 21,
          "deletions": 0,
          "changes": 21,
          "patch": "@@ -0,0 +1,21 @@\n+from dataclasses import dataclass\n+from typing import List, Union\n+\n+import numpy as np\n+import PIL.Image\n+\n+from ...utils import BaseOutput\n+\n+\n+@dataclass\n+class BriaFiboPipelineOutput(BaseOutput):\n+    \"\"\"\n+    Output class for BriaFibo pipelines.\n+\n+    Args:\n+        images (`List[PIL.Image.Image]` or `np.ndarray`)\n+            List of denoised PIL images of length `batch_size` or numpy array of shape `(batch_size, height, width,\n+            num_channels)`. PIL images or numpy array present the denoised images of the diffusion pipeline.\n+    \"\"\"\n+\n+    images: Union[List[PIL.Image.Image], np.ndarray]"
        },
        {
          "filename": "src/diffusers/utils/dummy_pt_objects.py",
          "status": "modified",
          "additions": 15,
          "deletions": 0,
          "changes": 15,
          "patch": "@@ -588,6 +588,21 @@ def from_pretrained(cls, *args, **kwargs):\n         requires_backends(cls, [\"torch\"])\n \n \n+class BriaFiboTransformer2DModel(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+    @classmethod\n+    def from_config(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\"])\n+\n+    @classmethod\n+    def from_pretrained(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\"])\n+\n+\n class BriaTransformer2DModel(metaclass=DummyObject):\n     _backends = [\"torch\"]\n "
        },
        {
          "filename": "src/diffusers/utils/dummy_torch_and_transformers_objects.py",
          "status": "modified",
          "additions": 15,
          "deletions": 0,
          "changes": 15,
          "patch": "@@ -482,6 +482,21 @@ def from_pretrained(cls, *args, **kwargs):\n         requires_backends(cls, [\"torch\", \"transformers\"])\n \n \n+class BriaFiboPipeline(metaclass=DummyObject):\n+    _backends = [\"torch\", \"transformers\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_config(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_pretrained(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+\n class BriaPipeline(metaclass=DummyObject):\n     _backends = [\"torch\", \"transformers\"]\n "
        },
        {
          "filename": "tests/models/transformers/test_models_transformer_bria_fibo.py",
          "status": "added",
          "additions": 89,
          "deletions": 0,
          "changes": 89,
          "patch": "@@ -0,0 +1,89 @@\n+# coding=utf-8\n+# Copyright 2025 HuggingFace Inc.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import unittest\n+\n+import torch\n+\n+from diffusers import BriaFiboTransformer2DModel\n+\n+from ...testing_utils import enable_full_determinism, torch_device\n+from ..test_modeling_common import ModelTesterMixin\n+\n+\n+enable_full_determinism()\n+\n+\n+class BriaFiboTransformerTests(ModelTesterMixin, unittest.TestCase):\n+    model_class = BriaFiboTransformer2DModel\n+    main_input_name = \"hidden_states\"\n+    # We override the items here because the transformer under consideration is small.\n+    model_split_percents = [0.8, 0.7, 0.7]\n+\n+    # Skip setting testing with default: AttnProcessor\n+    uses_custom_attn_processor = True\n+\n+    @property\n+    def dummy_input(self):\n+        batch_size = 1\n+        num_latent_channels = 48\n+        num_image_channels = 3\n+        height = width = 16\n+        sequence_length = 32\n+        embedding_dim = 64\n+\n+        hidden_states = torch.randn((batch_size, height * width, num_latent_channels)).to(torch_device)\n+        encoder_hidden_states = torch.randn((batch_size, sequence_length, embedding_dim)).to(torch_device)\n+        text_ids = torch.randn((sequence_length, num_image_channels)).to(torch_device)\n+        image_ids = torch.randn((height * width, num_image_channels)).to(torch_device)\n+        timestep = torch.tensor([1.0]).to(torch_device).expand(batch_size)\n+\n+        return {\n+            \"hidden_states\": hidden_states,\n+            \"encoder_hidden_states\": encoder_hidden_states,\n+            \"img_ids\": image_ids,\n+            \"txt_ids\": text_ids,\n+            \"timestep\": timestep,\n+            \"text_encoder_layers\": [encoder_hidden_states[:, :, :32], encoder_hidden_states[:, :, :32]],\n+        }\n+\n+    @property\n+    def input_shape(self):\n+        return (16, 16)\n+\n+    @property\n+    def output_shape(self):\n+        return (256, 48)\n+\n+    def prepare_init_args_and_inputs_for_common(self):\n+        init_dict = {\n+            \"patch_size\": 1,\n+            \"in_channels\": 48,\n+            \"num_layers\": 1,\n+            \"num_single_layers\": 1,\n+            \"attention_head_dim\": 8,\n+            \"num_attention_heads\": 2,\n+            \"joint_attention_dim\": 64,\n+            \"text_encoder_dim\": 32,\n+            \"pooled_projection_dim\": None,\n+            \"axes_dims_rope\": [0, 4, 4],\n+        }\n+\n+        inputs_dict = self.dummy_input\n+        return init_dict, inputs_dict\n+\n+    def test_gradient_checkpointing_is_applied(self):\n+        expected_set = {\"BriaFiboTransformer2DModel\"}\n+        super().test_gradient_checkpointing_is_applied(expected_set=expected_set)"
        },
        {
          "filename": "tests/pipelines/bria_fibo/__init__.py",
          "status": "added",
          "additions": 0,
          "deletions": 0,
          "changes": 0,
          "patch": ""
        },
        {
          "filename": "tests/pipelines/bria_fibo/test_pipeline_bria_fibo.py",
          "status": "added",
          "additions": 139,
          "deletions": 0,
          "changes": 139,
          "patch": "@@ -0,0 +1,139 @@\n+# Copyright 2024 Bria AI and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import unittest\n+\n+import numpy as np\n+import torch\n+from transformers import AutoTokenizer\n+from transformers.models.smollm3.modeling_smollm3 import SmolLM3Config, SmolLM3ForCausalLM\n+\n+from diffusers import (\n+    AutoencoderKLWan,\n+    BriaFiboPipeline,\n+    FlowMatchEulerDiscreteScheduler,\n+)\n+from diffusers.models.transformers.transformer_bria_fibo import BriaFiboTransformer2DModel\n+from tests.pipelines.test_pipelines_common import PipelineTesterMixin\n+\n+from ...testing_utils import (\n+    enable_full_determinism,\n+    torch_device,\n+)\n+\n+\n+enable_full_determinism()\n+\n+\n+class BriaFiboPipelineFastTests(PipelineTesterMixin, unittest.TestCase):\n+    pipeline_class = BriaFiboPipeline\n+    params = frozenset([\"prompt\", \"height\", \"width\", \"guidance_scale\"])\n+    batch_params = frozenset([\"prompt\"])\n+    test_xformers_attention = False\n+    test_layerwise_casting = False\n+    test_group_offloading = False\n+    supports_dduf = False\n+\n+    def get_dummy_components(self):\n+        torch.manual_seed(0)\n+        transformer = BriaFiboTransformer2DModel(\n+            patch_size=1,\n+            in_channels=16,\n+            num_layers=1,\n+            num_single_layers=1,\n+            attention_head_dim=8,\n+            num_attention_heads=2,\n+            joint_attention_dim=64,\n+            text_encoder_dim=32,\n+            pooled_projection_dim=None,\n+            axes_dims_rope=[0, 4, 4],\n+        )\n+\n+        torch.manual_seed(0)\n+        vae = AutoencoderKLWan(\n+            base_dim=160,\n+            decoder_base_dim=256,\n+            num_res_blocks=2,\n+            out_channels=12,\n+            patch_size=2,\n+            scale_factor_spatial=16,\n+            scale_factor_temporal=4,\n+            temperal_downsample=[False, True, True],\n+            z_dim=16,\n+        )\n+\n+        scheduler = FlowMatchEulerDiscreteScheduler()\n+\n+        torch.manual_seed(0)\n+        text_encoder = SmolLM3ForCausalLM(SmolLM3Config(hidden_size=32))\n+        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-t5\")\n+\n+        components = {\n+            \"scheduler\": scheduler,\n+            \"text_encoder\": text_encoder,\n+            \"tokenizer\": tokenizer,\n+            \"transformer\": transformer,\n+            \"vae\": vae,\n+        }\n+        return components\n+\n+    def get_dummy_inputs(self, device, seed=0):\n+        if str(device).startswith(\"mps\"):\n+            generator = torch.manual_seed(seed)\n+        else:\n+            generator = torch.Generator(device=\"cpu\").manual_seed(seed)\n+\n+        inputs = {\n+            \"prompt\": \"{'text': 'A painting of a squirrel eating a burger'}\",\n+            \"negative_prompt\": \"bad, ugly\",\n+            \"generator\": generator,\n+            \"num_inference_steps\": 2,\n+            \"guidance_scale\": 5.0,\n+            \"height\": 32,\n+            \"width\": 32,\n+            \"output_type\": \"np\",\n+        }\n+        return inputs\n+\n+    @unittest.skip(reason=\"will not be supported due to dim-fusion\")\n+    def test_encode_prompt_works_in_isolation(self):\n+        pass\n+\n+    def test_bria_fibo_different_prompts(self):\n+        pipe = self.pipeline_class(**self.get_dummy_components())\n+        pipe = pipe.to(torch_device)\n+        inputs = self.get_dummy_inputs(torch_device)\n+        output_same_prompt = pipe(**inputs).images[0]\n+\n+        inputs = self.get_dummy_inputs(torch_device)\n+        inputs[\"prompt\"] = \"a different prompt\"\n+        output_different_prompts = pipe(**inputs).images[0]\n+\n+        max_diff = np.abs(output_same_prompt - output_different_prompts).max()\n+        assert max_diff > 1e-6\n+\n+    def test_image_output_shape(self):\n+        pipe = self.pipeline_class(**self.get_dummy_components())\n+        pipe = pipe.to(torch_device)\n+        inputs = self.get_dummy_inputs(torch_device)\n+\n+        height_width_pairs = [(32, 32), (64, 64), (32, 64)]\n+        for height, width in height_width_pairs:\n+            expected_height = height\n+            expected_width = width\n+\n+            inputs.update({\"height\": height, \"width\": width})\n+            image = pipe(**inputs).images[0]\n+            output_height, output_width, _ = image.shape\n+            assert (output_height, output_width) == (expected_height, expected_width)"
        }
      ],
      "num_files": 16,
      "scraped_at": "2025-11-16T21:18:50.471348"
    },
    {
      "pr_number": 12526,
      "title": "[WIP]Add Wan2.2 Animate Pipeline (Continuation of #12442 by tolgacangoz)",
      "body": "# What does this PR do?\r\n\r\nThis PR is a continuation of #12442 by @tolgacangoz. It adds a pipeline for the Wan2.2-Animate-14B model [(project page](https://humanaigc.github.io/wan-animate/), [paper](https://arxiv.org/abs/2509.14055), [code](https://github.com/Wan-Video/Wan2.2), [weights](https://huggingface.co/Wan-AI/Wan2.2-Animate-14B)), a SOTA character animation and replacement video model.\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes #12441 (the original requesting issue).\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n@yiyixuxu\r\n@sayakpaul\r\n@tolgacangoz\r\n",
      "html_url": "https://github.com/huggingface/diffusers/pull/12526",
      "created_at": "2025-10-21T23:02:11Z",
      "merged_at": "2025-11-13T02:52:31Z",
      "merge_commit_sha": "d8e4805816df32ccecc070ccd6895e35cdafa723",
      "base_ref": "main",
      "head_sha": "2259ded86d5065691c30b72fc731ac9b92861fe3",
      "user": "dg845",
      "files": [
        {
          "filename": "docs/source/en/_toctree.yml",
          "status": "modified",
          "additions": 2,
          "deletions": 0,
          "changes": 2,
          "patch": "@@ -387,6 +387,8 @@\n         title: Transformer2DModel\n       - local: api/models/transformer_temporal\n         title: TransformerTemporalModel\n+      - local: api/models/wan_animate_transformer_3d\n+        title: WanAnimateTransformer3DModel\n       - local: api/models/wan_transformer_3d\n         title: WanTransformer3DModel\n       title: Transformers"
        },
        {
          "filename": "docs/source/en/api/models/wan_animate_transformer_3d.md",
          "status": "added",
          "additions": 30,
          "deletions": 0,
          "changes": 30,
          "patch": "@@ -0,0 +1,30 @@\n+<!-- Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License. -->\n+\n+# WanAnimateTransformer3DModel\n+\n+A Diffusion Transformer model for 3D video-like data was introduced in [Wan Animate](https://github.com/Wan-Video/Wan2.2) by the Alibaba Wan Team.\n+\n+The model can be loaded with the following code snippet.\n+\n+```python\n+from diffusers import WanAnimateTransformer3DModel\n+\n+transformer = WanAnimateTransformer3DModel.from_pretrained(\"Wan-AI/Wan2.2-Animate-14B-720P-Diffusers\", subfolder=\"transformer\", torch_dtype=torch.bfloat16)\n+```\n+\n+## WanAnimateTransformer3DModel\n+\n+[[autodoc]] WanAnimateTransformer3DModel\n+\n+## Transformer2DModelOutput\n+\n+[[autodoc]] models.modeling_outputs.Transformer2DModelOutput"
        },
        {
          "filename": "docs/source/en/api/pipelines/wan.md",
          "status": "modified",
          "additions": 238,
          "deletions": 17,
          "changes": 255,
          "patch": "@@ -40,6 +40,7 @@ The following Wan models are supported in Diffusers:\n - [Wan 2.2 T2V 14B](https://huggingface.co/Wan-AI/Wan2.2-T2V-A14B-Diffusers)\n - [Wan 2.2 I2V 14B](https://huggingface.co/Wan-AI/Wan2.2-I2V-A14B-Diffusers)\n - [Wan 2.2 TI2V 5B](https://huggingface.co/Wan-AI/Wan2.2-TI2V-5B-Diffusers)\n+- [Wan 2.2 Animate 14B](https://huggingface.co/Wan-AI/Wan2.2-Animate-14B-Diffusers)\n \n > [!TIP]\n > Click on the Wan models in the right sidebar for more examples of video generation.\n@@ -95,15 +96,15 @@ pipeline = WanPipeline.from_pretrained(\n pipeline.to(\"cuda\")\n \n prompt = \"\"\"\n-The camera rushes from far to near in a low-angle shot, \n-revealing a white ferret on a log. It plays, leaps into the water, and emerges, as the camera zooms in \n-for a close-up. Water splashes berry bushes nearby, while moss, snow, and leaves blanket the ground. \n-Birch trees and a light blue sky frame the scene, with ferns in the foreground. Side lighting casts dynamic \n+The camera rushes from far to near in a low-angle shot,\n+revealing a white ferret on a log. It plays, leaps into the water, and emerges, as the camera zooms in\n+for a close-up. Water splashes berry bushes nearby, while moss, snow, and leaves blanket the ground.\n+Birch trees and a light blue sky frame the scene, with ferns in the foreground. Side lighting casts dynamic\n shadows and warm highlights. Medium composition, front view, low angle, with depth of field.\n \"\"\"\n negative_prompt = \"\"\"\n-Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, \n-low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, \n+Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality,\n+low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured,\n misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards\n \"\"\"\n \n@@ -150,15 +151,15 @@ pipeline.transformer = torch.compile(\n )\n \n prompt = \"\"\"\n-The camera rushes from far to near in a low-angle shot, \n-revealing a white ferret on a log. It plays, leaps into the water, and emerges, as the camera zooms in \n-for a close-up. Water splashes berry bushes nearby, while moss, snow, and leaves blanket the ground. \n-Birch trees and a light blue sky frame the scene, with ferns in the foreground. Side lighting casts dynamic \n+The camera rushes from far to near in a low-angle shot,\n+revealing a white ferret on a log. It plays, leaps into the water, and emerges, as the camera zooms in\n+for a close-up. Water splashes berry bushes nearby, while moss, snow, and leaves blanket the ground.\n+Birch trees and a light blue sky frame the scene, with ferns in the foreground. Side lighting casts dynamic\n shadows and warm highlights. Medium composition, front view, low angle, with depth of field.\n \"\"\"\n negative_prompt = \"\"\"\n-Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, \n-low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, \n+Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality,\n+low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured,\n misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards\n \"\"\"\n \n@@ -249,6 +250,220 @@ The code snippets available in [this](https://github.com/huggingface/diffusers/p\n \n The general rule of thumb to keep in mind when preparing inputs for the VACE pipeline is that the input images, or frames of a video that you want to use for conditioning, should have a corresponding mask that is black in color. The black mask signifies that the model will not generate new content for that area, and only use those parts for conditioning the generation process. For parts/frames that should be generated by the model, the mask should be white in color.\n \n+</hfoption>\n+</hfoptions>\n+\n+### Wan-Animate: Unified Character Animation and Replacement with Holistic Replication\n+\n+[Wan-Animate](https://huggingface.co/papers/2509.14055) by the Wan Team.\n+\n+*We introduce Wan-Animate, a unified framework for character animation and replacement. Given a character image and a reference video, Wan-Animate can animate the character by precisely replicating the expressions and movements of the character in the video to generate high-fidelity character videos. Alternatively, it can integrate the animated character into the reference video to replace the original character, replicating the scene's lighting and color tone to achieve seamless environmental integration. Wan-Animate is built upon the Wan model. To adapt it for character animation tasks, we employ a modified input paradigm to differentiate between reference conditions and regions for generation. This design unifies multiple tasks into a common symbolic representation. We use spatially-aligned skeleton signals to replicate body motion and implicit facial features extracted from source images to reenact expressions, enabling the generation of character videos with high controllability and expressiveness. Furthermore, to enhance environmental integration during character replacement, we develop an auxiliary Relighting LoRA. This module preserves the character's appearance consistency while applying the appropriate environmental lighting and color tone. Experimental results demonstrate that Wan-Animate achieves state-of-the-art performance. We are committed to open-sourcing the model weights and its source code.*\n+\n+The project page: https://humanaigc.github.io/wan-animate\n+\n+This model was mostly contributed by [M. Tolga Cang\u00f6z](https://github.com/tolgacangoz).\n+\n+#### Usage\n+\n+The Wan-Animate pipeline supports two modes of operation:\n+\n+1. **Animation Mode** (default): Animates a character image based on motion and expression from reference videos\n+2. **Replacement Mode**: Replaces a character in a background video with a new character while preserving the scene\n+\n+##### Prerequisites\n+\n+Before using the pipeline, you need to preprocess your reference video to extract:\n+- **Pose video**: Contains skeletal keypoints representing body motion\n+- **Face video**: Contains facial feature representations for expression control\n+\n+For replacement mode, you additionally need:\n+- **Background video**: The original video containing the scene\n+- **Mask video**: A mask indicating where to generate content (white) vs. preserve original (black)\n+\n+> [!NOTE]\n+> The preprocessing tools are available in the original Wan-Animate repository. Integration of these preprocessing steps into Diffusers is planned for a future release.\n+\n+The example below demonstrates how to use the Wan-Animate pipeline:\n+\n+<hfoptions id=\"Animate usage\">\n+<hfoption id=\"Animation mode\">\n+\n+```python\n+import numpy as np\n+import torch\n+from diffusers import AutoencoderKLWan, WanAnimatePipeline\n+from diffusers.utils import export_to_video, load_image, load_video\n+from transformers import CLIPVisionModel\n+\n+model_id = \"Wan-AI/Wan2.2-Animate-14B-Diffusers\"\n+vae = AutoencoderKLWan.from_pretrained(model_id, subfolder=\"vae\", torch_dtype=torch.float32)\n+pipe = WanAnimatePipeline.from_pretrained(\n+    model_id, vae=vae, torch_dtype=torch.bfloat16\n+)\n+pipe.to(\"cuda\")\n+\n+# Load character image and preprocessed videos\n+image = load_image(\"path/to/character.jpg\")\n+pose_video = load_video(\"path/to/pose_video.mp4\")  # Preprocessed skeletal keypoints\n+face_video = load_video(\"path/to/face_video.mp4\")  # Preprocessed facial features\n+\n+# Resize image to match VAE constraints\n+def aspect_ratio_resize(image, pipe, max_area=720 * 1280):\n+    aspect_ratio = image.height / image.width\n+    mod_value = pipe.vae_scale_factor_spatial * pipe.transformer.config.patch_size[1]\n+    height = round(np.sqrt(max_area * aspect_ratio)) // mod_value * mod_value\n+    width = round(np.sqrt(max_area / aspect_ratio)) // mod_value * mod_value\n+    image = image.resize((width, height))\n+    return image, height, width\n+\n+image, height, width = aspect_ratio_resize(image, pipe)\n+\n+prompt = \"A person dancing energetically in a studio with dynamic lighting and professional camera work\"\n+negative_prompt = \"blurry, low quality, distorted, deformed, static, poorly drawn\"\n+\n+# Generate animated video\n+output = pipe(\n+    image=image,\n+    pose_video=pose_video,\n+    face_video=face_video,\n+    prompt=prompt,\n+    negative_prompt=negative_prompt,\n+    height=height,\n+    width=width,\n+    num_frames=81,\n+    guidance_scale=5.0,\n+    mode=\"animation\",  # Animation mode (default)\n+).frames[0]\n+export_to_video(output, \"animated_character.mp4\", fps=16)\n+```\n+\n+</hfoption>\n+<hfoption id=\"Replacement mode\">\n+\n+```python\n+import numpy as np\n+import torch\n+from diffusers import AutoencoderKLWan, WanAnimatePipeline\n+from diffusers.utils import export_to_video, load_image, load_video\n+from transformers import CLIPVisionModel\n+\n+model_id = \"Wan-AI/Wan2.2-Animate-14B-Diffusers\"\n+image_encoder = CLIPVisionModel.from_pretrained(model_id, subfolder=\"image_encoder\", torch_dtype=torch.float16)\n+vae = AutoencoderKLWan.from_pretrained(model_id, subfolder=\"vae\", torch_dtype=torch.float32)\n+pipe = WanAnimatePipeline.from_pretrained(\n+    model_id, vae=vae, image_encoder=image_encoder, torch_dtype=torch.bfloat16\n+)\n+pipe.to(\"cuda\")\n+\n+# Load all required inputs for replacement mode\n+image = load_image(\"path/to/new_character.jpg\")\n+pose_video = load_video(\"path/to/pose_video.mp4\")  # Preprocessed skeletal keypoints\n+face_video = load_video(\"path/to/face_video.mp4\")  # Preprocessed facial features\n+background_video = load_video(\"path/to/background_video.mp4\")  # Original scene\n+mask_video = load_video(\"path/to/mask_video.mp4\")  # Black: preserve, White: generate\n+\n+# Resize image to match video dimensions\n+def aspect_ratio_resize(image, pipe, max_area=720 * 1280):\n+    aspect_ratio = image.height / image.width\n+    mod_value = pipe.vae_scale_factor_spatial * pipe.transformer.config.patch_size[1]\n+    height = round(np.sqrt(max_area * aspect_ratio)) // mod_value * mod_value\n+    width = round(np.sqrt(max_area / aspect_ratio)) // mod_value * mod_value\n+    image = image.resize((width, height))\n+    return image, height, width\n+\n+image, height, width = aspect_ratio_resize(image, pipe)\n+\n+prompt = \"A person seamlessly integrated into the scene with consistent lighting and environment\"\n+negative_prompt = \"blurry, low quality, inconsistent lighting, floating, disconnected from scene\"\n+\n+# Replace character in background video\n+output = pipe(\n+    image=image,\n+    pose_video=pose_video,\n+    face_video=face_video,\n+    background_video=background_video,\n+    mask_video=mask_video,\n+    prompt=prompt,\n+    negative_prompt=negative_prompt,\n+    height=height,\n+    width=width,\n+    num_frames=81,\n+    guidance_scale=5.0,\n+    mode=\"replacement\",  # Replacement mode\n+).frames[0]\n+export_to_video(output, \"character_replaced.mp4\", fps=16)\n+```\n+\n+</hfoption>\n+<hfoption id=\"Advanced options\">\n+\n+```python\n+import numpy as np\n+import torch\n+from diffusers import AutoencoderKLWan, WanAnimatePipeline\n+from diffusers.utils import export_to_video, load_image, load_video\n+from transformers import CLIPVisionModel\n+\n+model_id = \"Wan-AI/Wan2.2-Animate-14B-Diffusers\"\n+image_encoder = CLIPVisionModel.from_pretrained(model_id, subfolder=\"image_encoder\", torch_dtype=torch.float16)\n+vae = AutoencoderKLWan.from_pretrained(model_id, subfolder=\"vae\", torch_dtype=torch.float32)\n+pipe = WanAnimatePipeline.from_pretrained(\n+    model_id, vae=vae, image_encoder=image_encoder, torch_dtype=torch.bfloat16\n+)\n+pipe.to(\"cuda\")\n+\n+image = load_image(\"path/to/character.jpg\")\n+pose_video = load_video(\"path/to/pose_video.mp4\")\n+face_video = load_video(\"path/to/face_video.mp4\")\n+\n+def aspect_ratio_resize(image, pipe, max_area=720 * 1280):\n+    aspect_ratio = image.height / image.width\n+    mod_value = pipe.vae_scale_factor_spatial * pipe.transformer.config.patch_size[1]\n+    height = round(np.sqrt(max_area * aspect_ratio)) // mod_value * mod_value\n+    width = round(np.sqrt(max_area / aspect_ratio)) // mod_value * mod_value\n+    image = image.resize((width, height))\n+    return image, height, width\n+\n+image, height, width = aspect_ratio_resize(image, pipe)\n+\n+prompt = \"A person dancing energetically in a studio\"\n+negative_prompt = \"blurry, low quality\"\n+\n+# Advanced: Use temporal guidance and custom callback\n+def callback_fn(pipe, step_index, timestep, callback_kwargs):\n+    # You can modify latents or other tensors here\n+    print(f\"Step {step_index}, Timestep {timestep}\")\n+    return callback_kwargs\n+\n+output = pipe(\n+    image=image,\n+    pose_video=pose_video,\n+    face_video=face_video,\n+    prompt=prompt,\n+    negative_prompt=negative_prompt,\n+    height=height,\n+    width=width,\n+    num_frames=81,\n+    num_inference_steps=50,\n+    guidance_scale=5.0,\n+    num_frames_for_temporal_guidance=5,  # Use 5 frames for temporal guidance (1 or 5 recommended)\n+    callback_on_step_end=callback_fn,\n+    callback_on_step_end_tensor_inputs=[\"latents\"],\n+).frames[0]\n+export_to_video(output, \"animated_advanced.mp4\", fps=16)\n+```\n+\n+</hfoption>\n+</hfoptions>\n+\n+#### Key Parameters\n+\n+- **mode**: Choose between `\"animation\"` (default) or `\"replacement\"`\n+- **num_frames_for_temporal_guidance**: Number of frames for temporal guidance (1 or 5 recommended). Using 5 provides better temporal consistency but requires more memory\n+- **guidance_scale**: Controls how closely the output follows the text prompt. Higher values (5-7) produce results more aligned with the prompt\n+- **num_frames**: Total number of frames to generate. Should be divisible by `vae_scale_factor_temporal` (default: 4)\n+\n+\n ## Notes\n \n - Wan2.1 supports LoRAs with [`~loaders.WanLoraLoaderMixin.load_lora_weights`].\n@@ -281,10 +496,10 @@ The general rule of thumb to keep in mind when preparing inputs for the VACE pip\n \n   # use \"steamboat willie style\" to trigger the LoRA\n   prompt = \"\"\"\n-  steamboat willie style, golden era animation, The camera rushes from far to near in a low-angle shot, \n-  revealing a white ferret on a log. It plays, leaps into the water, and emerges, as the camera zooms in \n-  for a close-up. Water splashes berry bushes nearby, while moss, snow, and leaves blanket the ground. \n-  Birch trees and a light blue sky frame the scene, with ferns in the foreground. Side lighting casts dynamic \n+  steamboat willie style, golden era animation, The camera rushes from far to near in a low-angle shot,\n+  revealing a white ferret on a log. It plays, leaps into the water, and emerges, as the camera zooms in\n+  for a close-up. Water splashes berry bushes nearby, while moss, snow, and leaves blanket the ground.\n+  Birch trees and a light blue sky frame the scene, with ferns in the foreground. Side lighting casts dynamic\n   shadows and warm highlights. Medium composition, front view, low angle, with depth of field.\n   \"\"\"\n \n@@ -359,6 +574,12 @@ The general rule of thumb to keep in mind when preparing inputs for the VACE pip\n   - all\n   - __call__\n \n+## WanAnimatePipeline\n+\n+[[autodoc]] WanAnimatePipeline\n+  - all\n+  - __call__\n+\n ## WanPipelineOutput\n \n-[[autodoc]] pipelines.wan.pipeline_output.WanPipelineOutput\n\\ No newline at end of file\n+[[autodoc]] pipelines.wan.pipeline_output.WanPipelineOutput"
        },
        {
          "filename": "scripts/convert_wan_to_diffusers.py",
          "status": "modified",
          "additions": 265,
          "deletions": 6,
          "changes": 271,
          "patch": "@@ -6,11 +6,20 @@\n from accelerate import init_empty_weights\n from huggingface_hub import hf_hub_download, snapshot_download\n from safetensors.torch import load_file\n-from transformers import AutoProcessor, AutoTokenizer, CLIPVisionModelWithProjection, UMT5EncoderModel\n+from transformers import (\n+    AutoProcessor,\n+    AutoTokenizer,\n+    CLIPImageProcessor,\n+    CLIPVisionModel,\n+    CLIPVisionModelWithProjection,\n+    UMT5EncoderModel,\n+)\n \n from diffusers import (\n     AutoencoderKLWan,\n     UniPCMultistepScheduler,\n+    WanAnimatePipeline,\n+    WanAnimateTransformer3DModel,\n     WanImageToVideoPipeline,\n     WanPipeline,\n     WanTransformer3DModel,\n@@ -105,8 +114,203 @@\n     \"after_proj\": \"proj_out\",\n }\n \n+ANIMATE_TRANSFORMER_KEYS_RENAME_DICT = {\n+    \"time_embedding.0\": \"condition_embedder.time_embedder.linear_1\",\n+    \"time_embedding.2\": \"condition_embedder.time_embedder.linear_2\",\n+    \"text_embedding.0\": \"condition_embedder.text_embedder.linear_1\",\n+    \"text_embedding.2\": \"condition_embedder.text_embedder.linear_2\",\n+    \"time_projection.1\": \"condition_embedder.time_proj\",\n+    \"head.modulation\": \"scale_shift_table\",\n+    \"head.head\": \"proj_out\",\n+    \"modulation\": \"scale_shift_table\",\n+    \"ffn.0\": \"ffn.net.0.proj\",\n+    \"ffn.2\": \"ffn.net.2\",\n+    # Hack to swap the layer names\n+    # The original model calls the norms in following order: norm1, norm3, norm2\n+    # We convert it to: norm1, norm2, norm3\n+    \"norm2\": \"norm__placeholder\",\n+    \"norm3\": \"norm2\",\n+    \"norm__placeholder\": \"norm3\",\n+    \"img_emb.proj.0\": \"condition_embedder.image_embedder.norm1\",\n+    \"img_emb.proj.1\": \"condition_embedder.image_embedder.ff.net.0.proj\",\n+    \"img_emb.proj.3\": \"condition_embedder.image_embedder.ff.net.2\",\n+    \"img_emb.proj.4\": \"condition_embedder.image_embedder.norm2\",\n+    # Add attention component mappings\n+    \"self_attn.q\": \"attn1.to_q\",\n+    \"self_attn.k\": \"attn1.to_k\",\n+    \"self_attn.v\": \"attn1.to_v\",\n+    \"self_attn.o\": \"attn1.to_out.0\",\n+    \"self_attn.norm_q\": \"attn1.norm_q\",\n+    \"self_attn.norm_k\": \"attn1.norm_k\",\n+    \"cross_attn.q\": \"attn2.to_q\",\n+    \"cross_attn.k\": \"attn2.to_k\",\n+    \"cross_attn.v\": \"attn2.to_v\",\n+    \"cross_attn.o\": \"attn2.to_out.0\",\n+    \"cross_attn.norm_q\": \"attn2.norm_q\",\n+    \"cross_attn.norm_k\": \"attn2.norm_k\",\n+    \"cross_attn.k_img\": \"attn2.to_k_img\",\n+    \"cross_attn.v_img\": \"attn2.to_v_img\",\n+    \"cross_attn.norm_k_img\": \"attn2.norm_k_img\",\n+    # After cross_attn -> attn2 rename, we need to rename the img keys\n+    \"attn2.to_k_img\": \"attn2.add_k_proj\",\n+    \"attn2.to_v_img\": \"attn2.add_v_proj\",\n+    \"attn2.norm_k_img\": \"attn2.norm_added_k\",\n+    # Wan Animate-specific mappings (motion encoder, face encoder, face adapter)\n+    # Motion encoder mappings\n+    # The name mapping is complicated for the convolutional part so we handle that in its own function\n+    \"motion_encoder.enc.fc\": \"motion_encoder.motion_network\",\n+    \"motion_encoder.dec.direction.weight\": \"motion_encoder.motion_synthesis_weight\",\n+    # Face encoder mappings - CausalConv1d has a .conv submodule that we need to flatten\n+    \"face_encoder.conv1_local.conv\": \"face_encoder.conv1_local\",\n+    \"face_encoder.conv2.conv\": \"face_encoder.conv2\",\n+    \"face_encoder.conv3.conv\": \"face_encoder.conv3\",\n+    # Face adapter mappings are handled in a separate function\n+}\n+\n+\n+# TODO: Verify this and simplify if possible.\n+def convert_animate_motion_encoder_weights(key: str, state_dict: Dict[str, Any], final_conv_idx: int = 8) -> None:\n+    \"\"\"\n+    Convert all motion encoder weights for Animate model.\n+\n+    In the original model:\n+    - All Linear layers in fc use EqualLinear\n+    - All Conv2d layers in convs use EqualConv2d (except blur_conv which is initialized separately)\n+    - Blur kernels are stored as buffers in Sequential modules\n+    - ConvLayer is nn.Sequential with indices: [Blur (optional), EqualConv2d, FusedLeakyReLU (optional)]\n+\n+    Conversion strategy:\n+    1. Drop .kernel buffers (blur kernels)\n+    2. Rename sequential indices to named components (e.g., 0 -> conv2d, 1 -> bias_leaky_relu)\n+    \"\"\"\n+    # Skip if not a weight, bias, or kernel\n+    if \".weight\" not in key and \".bias\" not in key and \".kernel\" not in key:\n+        return\n+\n+    # Handle Blur kernel buffers from original implementation.\n+    # After renaming, these appear under: motion_encoder.res_blocks.*.conv{2,skip}.blur_kernel\n+    # Diffusers constructs blur kernels as a non-persistent buffer so we must drop these keys\n+    if \".kernel\" in key and \"motion_encoder\" in key:\n+        # Remove unexpected blur kernel buffers to avoid strict load errors\n+        state_dict.pop(key, None)\n+        return\n+\n+    # Rename Sequential indices to named components in ConvLayer and ResBlock\n+    if \".enc.net_app.convs.\" in key and (\".weight\" in key or \".bias\" in key):\n+        parts = key.split(\".\")\n+\n+        # Find the sequential index (digit) after convs or after conv1/conv2/skip\n+        # Examples:\n+        # - enc.net_app.convs.0.0.weight -> conv_in.weight (initial conv layer weight)\n+        # - enc.net_app.convs.0.1.bias -> conv_in.act_fn.bias (initial conv layer bias)\n+        # - enc.net_app.convs.{n:1-7}.conv1.0.weight -> res_blocks.{(n-1):0-6}.conv1.weight (conv1 weight)\n+        #     - e.g. enc.net_app.convs.1.conv1.0.weight -> res_blocks.0.conv1.weight\n+        # - enc.net_app.convs.{n:1-7}.conv1.1.bias -> res_blocks.{(n-1):0-6}.conv1.act_fn.bias (conv1 bias)\n+        #     - e.g. enc.net_app.convs.1.conv1.1.bias -> res_blocks.0.conv1.act_fn.bias\n+        # - enc.net_app.convs.{n:1-7}.conv2.1.weight -> res_blocks.{(n-1):0-6}.conv2.weight (conv2 weight)\n+        # - enc.net_app.convs.1.conv2.2.bias -> res_blocks.0.conv2.act_fn.bias (conv2 bias)\n+        # - enc.net_app.convs.{n:1-7}.skip.1.weight -> res_blocks.{(n-1):0-6}.conv_skip.weight (skip conv weight)\n+        # - enc.net_app.convs.8 -> conv_out (final conv layer)\n+\n+        convs_idx = parts.index(\"convs\") if \"convs\" in parts else -1\n+        if convs_idx >= 0 and len(parts) - convs_idx >= 2:\n+            bias = False\n+            # The nn.Sequential index will always follow convs\n+            sequential_idx = int(parts[convs_idx + 1])\n+            if sequential_idx == 0:\n+                if key.endswith(\".weight\"):\n+                    new_key = \"motion_encoder.conv_in.weight\"\n+                elif key.endswith(\".bias\"):\n+                    new_key = \"motion_encoder.conv_in.act_fn.bias\"\n+                    bias = True\n+            elif sequential_idx == final_conv_idx:\n+                if key.endswith(\".weight\"):\n+                    new_key = \"motion_encoder.conv_out.weight\"\n+            else:\n+                # Intermediate .convs. layers, which get mapped to .res_blocks.\n+                prefix = \"motion_encoder.res_blocks.\"\n+\n+                layer_name = parts[convs_idx + 2]\n+                if layer_name == \"skip\":\n+                    layer_name = \"conv_skip\"\n+\n+                if key.endswith(\".weight\"):\n+                    param_name = \"weight\"\n+                elif key.endswith(\".bias\"):\n+                    param_name = \"act_fn.bias\"\n+                    bias = True\n+\n+                suffix_parts = [str(sequential_idx - 1), layer_name, param_name]\n+                suffix = \".\".join(suffix_parts)\n+                new_key = prefix + suffix\n+\n+            param = state_dict.pop(key)\n+            if bias:\n+                param = param.squeeze()\n+            state_dict[new_key] = param\n+            return\n+        return\n+    return\n+\n+\n+def convert_animate_face_adapter_weights(key: str, state_dict: Dict[str, Any]) -> None:\n+    \"\"\"\n+    Convert face adapter weights for the Animate model.\n+\n+    The original model uses a fused KV projection but the diffusers models uses separate K and V projections.\n+    \"\"\"\n+    # Skip if not a weight or bias\n+    if \".weight\" not in key and \".bias\" not in key:\n+        return\n+\n+    prefix = \"face_adapter.\"\n+    if \".fuser_blocks.\" in key:\n+        parts = key.split(\".\")\n+\n+        module_list_idx = parts.index(\"fuser_blocks\") if \"fuser_blocks\" in parts else -1\n+        if module_list_idx >= 0 and (len(parts) - 1) - module_list_idx == 3:\n+            block_idx = parts[module_list_idx + 1]\n+            layer_name = parts[module_list_idx + 2]\n+            param_name = parts[module_list_idx + 3]\n+\n+            if layer_name == \"linear1_kv\":\n+                layer_name_k = \"to_k\"\n+                layer_name_v = \"to_v\"\n+\n+                suffix_k = \".\".join([block_idx, layer_name_k, param_name])\n+                suffix_v = \".\".join([block_idx, layer_name_v, param_name])\n+                new_key_k = prefix + suffix_k\n+                new_key_v = prefix + suffix_v\n+\n+                kv_proj = state_dict.pop(key)\n+                k_proj, v_proj = torch.chunk(kv_proj, 2, dim=0)\n+                state_dict[new_key_k] = k_proj\n+                state_dict[new_key_v] = v_proj\n+                return\n+            else:\n+                if layer_name == \"q_norm\":\n+                    new_layer_name = \"norm_q\"\n+                elif layer_name == \"k_norm\":\n+                    new_layer_name = \"norm_k\"\n+                elif layer_name == \"linear1_q\":\n+                    new_layer_name = \"to_q\"\n+                elif layer_name == \"linear2\":\n+                    new_layer_name = \"to_out\"\n+\n+                suffix_parts = [block_idx, new_layer_name, param_name]\n+                suffix = \".\".join(suffix_parts)\n+                new_key = prefix + suffix\n+                state_dict[new_key] = state_dict.pop(key)\n+                return\n+    return\n+\n+\n TRANSFORMER_SPECIAL_KEYS_REMAP = {}\n VACE_TRANSFORMER_SPECIAL_KEYS_REMAP = {}\n+ANIMATE_TRANSFORMER_SPECIAL_KEYS_REMAP = {\n+    \"motion_encoder\": convert_animate_motion_encoder_weights,\n+    \"face_adapter\": convert_animate_face_adapter_weights,\n+}\n \n \n def update_state_dict_(state_dict: Dict[str, Any], old_key: str, new_key: str) -> Dict[str, Any]:\n@@ -364,6 +568,37 @@ def get_transformer_config(model_type: str) -> Tuple[Dict[str, Any], ...]:\n         }\n         RENAME_DICT = TRANSFORMER_KEYS_RENAME_DICT\n         SPECIAL_KEYS_REMAP = TRANSFORMER_SPECIAL_KEYS_REMAP\n+    elif model_type == \"Wan2.2-Animate-14B\":\n+        config = {\n+            \"model_id\": \"Wan-AI/Wan2.2-Animate-14B\",\n+            \"diffusers_config\": {\n+                \"image_dim\": 1280,\n+                \"added_kv_proj_dim\": 5120,\n+                \"attention_head_dim\": 128,\n+                \"cross_attn_norm\": True,\n+                \"eps\": 1e-06,\n+                \"ffn_dim\": 13824,\n+                \"freq_dim\": 256,\n+                \"in_channels\": 36,\n+                \"num_attention_heads\": 40,\n+                \"num_layers\": 40,\n+                \"out_channels\": 16,\n+                \"patch_size\": (1, 2, 2),\n+                \"qk_norm\": \"rms_norm_across_heads\",\n+                \"text_dim\": 4096,\n+                \"rope_max_seq_len\": 1024,\n+                \"pos_embed_seq_len\": None,\n+                \"motion_encoder_size\": 512,  # Start of Wan Animate-specific configs\n+                \"motion_style_dim\": 512,\n+                \"motion_dim\": 20,\n+                \"motion_encoder_dim\": 512,\n+                \"face_encoder_hidden_dim\": 1024,\n+                \"face_encoder_num_heads\": 4,\n+                \"inject_face_latents_blocks\": 5,\n+            },\n+        }\n+        RENAME_DICT = ANIMATE_TRANSFORMER_KEYS_RENAME_DICT\n+        SPECIAL_KEYS_REMAP = ANIMATE_TRANSFORMER_SPECIAL_KEYS_REMAP\n     return config, RENAME_DICT, SPECIAL_KEYS_REMAP\n \n \n@@ -380,10 +615,12 @@ def convert_transformer(model_type: str, stage: str = None):\n     original_state_dict = load_sharded_safetensors(model_dir)\n \n     with init_empty_weights():\n-        if \"VACE\" not in model_type:\n-            transformer = WanTransformer3DModel.from_config(diffusers_config)\n-        else:\n+        if \"Animate\" in model_type:\n+            transformer = WanAnimateTransformer3DModel.from_config(diffusers_config)\n+        elif \"VACE\" in model_type:\n             transformer = WanVACETransformer3DModel.from_config(diffusers_config)\n+        else:\n+            transformer = WanTransformer3DModel.from_config(diffusers_config)\n \n     for key in list(original_state_dict.keys()):\n         new_key = key[:]\n@@ -397,7 +634,12 @@ def convert_transformer(model_type: str, stage: str = None):\n                 continue\n             handler_fn_inplace(key, original_state_dict)\n \n+    # Load state dict into the meta model, which will materialize the tensors\n     transformer.load_state_dict(original_state_dict, strict=True, assign=True)\n+\n+    # Move to CPU to ensure all tensors are materialized\n+    transformer = transformer.to(\"cpu\")\n+\n     return transformer\n \n \n@@ -926,7 +1168,7 @@ def get_args():\n if __name__ == \"__main__\":\n     args = get_args()\n \n-    if \"Wan2.2\" in args.model_type and \"TI2V\" not in args.model_type:\n+    if \"Wan2.2\" in args.model_type and \"TI2V\" not in args.model_type and \"Animate\" not in args.model_type:\n         transformer = convert_transformer(args.model_type, stage=\"high_noise_model\")\n         transformer_2 = convert_transformer(args.model_type, stage=\"low_noise_model\")\n     else:\n@@ -942,7 +1184,7 @@ def get_args():\n     tokenizer = AutoTokenizer.from_pretrained(\"google/umt5-xxl\")\n     if \"FLF2V\" in args.model_type:\n         flow_shift = 16.0\n-    elif \"TI2V\" in args.model_type:\n+    elif \"TI2V\" in args.model_type or \"Animate\" in args.model_type:\n         flow_shift = 5.0\n     else:\n         flow_shift = 3.0\n@@ -954,6 +1196,8 @@ def get_args():\n     if args.dtype != \"none\":\n         dtype = DTYPE_MAPPING[args.dtype]\n         transformer.to(dtype)\n+        if transformer_2 is not None:\n+            transformer_2.to(dtype)\n \n     if \"Wan2.2\" and \"I2V\" in args.model_type and \"TI2V\" not in args.model_type:\n         pipe = WanImageToVideoPipeline(\n@@ -1016,6 +1260,21 @@ def get_args():\n             vae=vae,\n             scheduler=scheduler,\n         )\n+    elif \"Animate\" in args.model_type:\n+        image_encoder = CLIPVisionModel.from_pretrained(\n+            \"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\", torch_dtype=torch.bfloat16\n+        )\n+        image_processor = CLIPImageProcessor.from_pretrained(\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\")\n+\n+        pipe = WanAnimatePipeline(\n+            transformer=transformer,\n+            text_encoder=text_encoder,\n+            tokenizer=tokenizer,\n+            vae=vae,\n+            scheduler=scheduler,\n+            image_encoder=image_encoder,\n+            image_processor=image_processor,\n+        )\n     else:\n         pipe = WanPipeline(\n             transformer=transformer,"
        },
        {
          "filename": "src/diffusers/__init__.py",
          "status": "modified",
          "additions": 4,
          "deletions": 0,
          "changes": 4,
          "patch": "@@ -268,6 +268,7 @@\n             \"UNetSpatioTemporalConditionModel\",\n             \"UVit2DModel\",\n             \"VQModel\",\n+            \"WanAnimateTransformer3DModel\",\n             \"WanTransformer3DModel\",\n             \"WanVACETransformer3DModel\",\n             \"attention_backend\",\n@@ -636,6 +637,7 @@\n             \"VisualClozeGenerationPipeline\",\n             \"VisualClozePipeline\",\n             \"VQDiffusionPipeline\",\n+            \"WanAnimatePipeline\",\n             \"WanImageToVideoPipeline\",\n             \"WanPipeline\",\n             \"WanVACEPipeline\",\n@@ -977,6 +979,7 @@\n             UNetSpatioTemporalConditionModel,\n             UVit2DModel,\n             VQModel,\n+            WanAnimateTransformer3DModel,\n             WanTransformer3DModel,\n             WanVACETransformer3DModel,\n             attention_backend,\n@@ -1315,6 +1318,7 @@\n             VisualClozeGenerationPipeline,\n             VisualClozePipeline,\n             VQDiffusionPipeline,\n+            WanAnimatePipeline,\n             WanImageToVideoPipeline,\n             WanPipeline,\n             WanVACEPipeline,"
        },
        {
          "filename": "src/diffusers/image_processor.py",
          "status": "modified",
          "additions": 2,
          "deletions": 2,
          "changes": 4,
          "patch": "@@ -409,7 +409,7 @@ def _resize_and_fill(\n         src_w = width if ratio < src_ratio else image.width * height // image.height\n         src_h = height if ratio >= src_ratio else image.height * width // image.width\n \n-        resized = image.resize((src_w, src_h), resample=PIL_INTERPOLATION[\"lanczos\"])\n+        resized = image.resize((src_w, src_h), resample=PIL_INTERPOLATION[self.config.resample])\n         res = Image.new(\"RGB\", (width, height))\n         res.paste(resized, box=(width // 2 - src_w // 2, height // 2 - src_h // 2))\n \n@@ -460,7 +460,7 @@ def _resize_and_crop(\n         src_w = width if ratio > src_ratio else image.width * height // image.height\n         src_h = height if ratio <= src_ratio else image.height * width // image.width\n \n-        resized = image.resize((src_w, src_h), resample=PIL_INTERPOLATION[\"lanczos\"])\n+        resized = image.resize((src_w, src_h), resample=PIL_INTERPOLATION[self.config.resample])\n         res = Image.new(\"RGB\", (width, height))\n         res.paste(resized, box=(width // 2 - src_w // 2, height // 2 - src_h // 2))\n         return res"
        },
        {
          "filename": "src/diffusers/models/__init__.py",
          "status": "modified",
          "additions": 2,
          "deletions": 0,
          "changes": 2,
          "patch": "@@ -108,6 +108,7 @@\n     _import_structure[\"transformers.transformer_skyreels_v2\"] = [\"SkyReelsV2Transformer3DModel\"]\n     _import_structure[\"transformers.transformer_temporal\"] = [\"TransformerTemporalModel\"]\n     _import_structure[\"transformers.transformer_wan\"] = [\"WanTransformer3DModel\"]\n+    _import_structure[\"transformers.transformer_wan_animate\"] = [\"WanAnimateTransformer3DModel\"]\n     _import_structure[\"transformers.transformer_wan_vace\"] = [\"WanVACETransformer3DModel\"]\n     _import_structure[\"unets.unet_1d\"] = [\"UNet1DModel\"]\n     _import_structure[\"unets.unet_2d\"] = [\"UNet2DModel\"]\n@@ -214,6 +215,7 @@\n             T5FilmDecoder,\n             Transformer2DModel,\n             TransformerTemporalModel,\n+            WanAnimateTransformer3DModel,\n             WanTransformer3DModel,\n             WanVACETransformer3DModel,\n         )"
        },
        {
          "filename": "src/diffusers/models/transformers/__init__.py",
          "status": "modified",
          "additions": 1,
          "deletions": 0,
          "changes": 1,
          "patch": "@@ -42,4 +42,5 @@\n     from .transformer_skyreels_v2 import SkyReelsV2Transformer3DModel\n     from .transformer_temporal import TransformerTemporalModel\n     from .transformer_wan import WanTransformer3DModel\n+    from .transformer_wan_animate import WanAnimateTransformer3DModel\n     from .transformer_wan_vace import WanVACETransformer3DModel"
        },
        {
          "filename": "src/diffusers/models/transformers/transformer_sana_video.py",
          "status": "modified",
          "additions": 6,
          "deletions": 5,
          "changes": 11,
          "patch": "@@ -188,6 +188,11 @@ def __init__(\n \n         h_dim = w_dim = 2 * (attention_head_dim // 6)\n         t_dim = attention_head_dim - h_dim - w_dim\n+\n+        self.t_dim = t_dim\n+        self.h_dim = h_dim\n+        self.w_dim = w_dim\n+\n         freqs_dtype = torch.float32 if torch.backends.mps.is_available() else torch.float64\n \n         freqs_cos = []\n@@ -213,11 +218,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         p_t, p_h, p_w = self.patch_size\n         ppf, pph, ppw = num_frames // p_t, height // p_h, width // p_w\n \n-        split_sizes = [\n-            self.attention_head_dim - 2 * (self.attention_head_dim // 3),\n-            self.attention_head_dim // 3,\n-            self.attention_head_dim // 3,\n-        ]\n+        split_sizes = [self.t_dim, self.h_dim, self.w_dim]\n \n         freqs_cos = self.freqs_cos.split(split_sizes, dim=1)\n         freqs_sin = self.freqs_sin.split(split_sizes, dim=1)"
        },
        {
          "filename": "src/diffusers/models/transformers/transformer_wan_animate.py",
          "status": "added",
          "additions": 1298,
          "deletions": 0,
          "changes": 1298,
          "patch": "@@ -0,0 +1,1298 @@\n+# Copyright 2025 The Wan Team and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import math\n+from typing import Any, Dict, Optional, Tuple, Union\n+\n+import torch\n+import torch.nn as nn\n+import torch.nn.functional as F\n+\n+from ...configuration_utils import ConfigMixin, register_to_config\n+from ...loaders import FromOriginalModelMixin, PeftAdapterMixin\n+from ...utils import USE_PEFT_BACKEND, logging, scale_lora_layers, unscale_lora_layers\n+from ..attention import AttentionMixin, AttentionModuleMixin, FeedForward\n+from ..attention_dispatch import dispatch_attention_fn\n+from ..cache_utils import CacheMixin\n+from ..embeddings import PixArtAlphaTextProjection, TimestepEmbedding, Timesteps, get_1d_rotary_pos_embed\n+from ..modeling_outputs import Transformer2DModelOutput\n+from ..modeling_utils import ModelMixin\n+from ..normalization import FP32LayerNorm\n+\n+\n+logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n+\n+\n+WAN_ANIMATE_MOTION_ENCODER_CHANNEL_SIZES = {\n+    \"4\": 512,\n+    \"8\": 512,\n+    \"16\": 512,\n+    \"32\": 512,\n+    \"64\": 256,\n+    \"128\": 128,\n+    \"256\": 64,\n+    \"512\": 32,\n+    \"1024\": 16,\n+}\n+\n+\n+# Copied from diffusers.models.transformers.transformer_wan._get_qkv_projections\n+def _get_qkv_projections(attn: \"WanAttention\", hidden_states: torch.Tensor, encoder_hidden_states: torch.Tensor):\n+    # encoder_hidden_states is only passed for cross-attention\n+    if encoder_hidden_states is None:\n+        encoder_hidden_states = hidden_states\n+\n+    if attn.fused_projections:\n+        if attn.cross_attention_dim_head is None:\n+            # In self-attention layers, we can fuse the entire QKV projection into a single linear\n+            query, key, value = attn.to_qkv(hidden_states).chunk(3, dim=-1)\n+        else:\n+            # In cross-attention layers, we can only fuse the KV projections into a single linear\n+            query = attn.to_q(hidden_states)\n+            key, value = attn.to_kv(encoder_hidden_states).chunk(2, dim=-1)\n+    else:\n+        query = attn.to_q(hidden_states)\n+        key = attn.to_k(encoder_hidden_states)\n+        value = attn.to_v(encoder_hidden_states)\n+    return query, key, value\n+\n+\n+# Copied from diffusers.models.transformers.transformer_wan._get_added_kv_projections\n+def _get_added_kv_projections(attn: \"WanAttention\", encoder_hidden_states_img: torch.Tensor):\n+    if attn.fused_projections:\n+        key_img, value_img = attn.to_added_kv(encoder_hidden_states_img).chunk(2, dim=-1)\n+    else:\n+        key_img = attn.add_k_proj(encoder_hidden_states_img)\n+        value_img = attn.add_v_proj(encoder_hidden_states_img)\n+    return key_img, value_img\n+\n+\n+class FusedLeakyReLU(nn.Module):\n+    \"\"\"\n+    Fused LeakyRelu with scale factor and channel-wise bias.\n+    \"\"\"\n+\n+    def __init__(self, negative_slope: float = 0.2, scale: float = 2**0.5, bias_channels: Optional[int] = None):\n+        super().__init__()\n+        self.negative_slope = negative_slope\n+        self.scale = scale\n+        self.channels = bias_channels\n+\n+        if self.channels is not None:\n+            self.bias = nn.Parameter(\n+                torch.zeros(\n+                    self.channels,\n+                )\n+            )\n+        else:\n+            self.bias = None\n+\n+    def forward(self, x: torch.Tensor, channel_dim: int = 1) -> torch.Tensor:\n+        if self.bias is not None:\n+            # Expand self.bias to have all singleton dims except at self.channel_dim\n+            expanded_shape = [1] * x.ndim\n+            expanded_shape[channel_dim] = self.bias.shape[0]\n+            bias = self.bias.reshape(*expanded_shape)\n+            x = x + bias\n+        return F.leaky_relu(x, self.negative_slope) * self.scale\n+\n+\n+class MotionConv2d(nn.Module):\n+    def __init__(\n+        self,\n+        in_channels: int,\n+        out_channels: int,\n+        kernel_size: int,\n+        stride: int = 1,\n+        padding: int = 0,\n+        bias: bool = True,\n+        blur_kernel: Optional[Tuple[int, ...]] = None,\n+        blur_upsample_factor: int = 1,\n+        use_activation: bool = True,\n+    ):\n+        super().__init__()\n+        self.use_activation = use_activation\n+        self.in_channels = in_channels\n+\n+        # Handle blurring (applying a FIR filter with the given kernel) if available\n+        self.blur = False\n+        if blur_kernel is not None:\n+            p = (len(blur_kernel) - stride) + (kernel_size - 1)\n+            self.blur_padding = ((p + 1) // 2, p // 2)\n+\n+            kernel = torch.tensor(blur_kernel)\n+            # Convert kernel to 2D if necessary\n+            if kernel.ndim == 1:\n+                kernel = kernel[None, :] * kernel[:, None]\n+            # Normalize kernel\n+            kernel = kernel / kernel.sum()\n+            if blur_upsample_factor > 1:\n+                kernel = kernel * (blur_upsample_factor**2)\n+            self.register_buffer(\"blur_kernel\", kernel, persistent=False)\n+            self.blur = True\n+\n+        # Main Conv2d parameters (with scale factor)\n+        self.weight = nn.Parameter(torch.randn(out_channels, in_channels, kernel_size, kernel_size))\n+        self.scale = 1 / math.sqrt(in_channels * kernel_size**2)\n+\n+        self.stride = stride\n+        self.padding = padding\n+\n+        # If using an activation function, the bias will be fused into the activation\n+        if bias and not self.use_activation:\n+            self.bias = nn.Parameter(torch.zeros(out_channels))\n+        else:\n+            self.bias = None\n+\n+        if self.use_activation:\n+            self.act_fn = FusedLeakyReLU(bias_channels=out_channels)\n+        else:\n+            self.act_fn = None\n+\n+    def forward(self, x: torch.Tensor, channel_dim: int = 1) -> torch.Tensor:\n+        # Apply blur if using\n+        if self.blur:\n+            # NOTE: the original implementation uses a 2D upfirdn operation with the upsampling and downsampling rates\n+            # set to 1, which should be equivalent to a 2D convolution\n+            expanded_kernel = self.blur_kernel[None, None, :, :].expand(self.in_channels, 1, -1, -1)\n+            x = F.conv2d(x, expanded_kernel, padding=self.blur_padding, groups=self.in_channels)\n+\n+        # Main Conv2D with scaling\n+        x = F.conv2d(x, self.weight * self.scale, bias=self.bias, stride=self.stride, padding=self.padding)\n+\n+        # Activation with fused bias, if using\n+        if self.use_activation:\n+            x = self.act_fn(x, channel_dim=channel_dim)\n+        return x\n+\n+    def __repr__(self):\n+        return (\n+            f\"{self.__class__.__name__}({self.weight.shape[1]}, {self.weight.shape[0]},\"\n+            f\" kernel_size={self.weight.shape[2]}, stride={self.stride}, padding={self.padding})\"\n+        )\n+\n+\n+class MotionLinear(nn.Module):\n+    def __init__(\n+        self,\n+        in_dim: int,\n+        out_dim: int,\n+        bias: bool = True,\n+        use_activation: bool = False,\n+    ):\n+        super().__init__()\n+        self.use_activation = use_activation\n+\n+        # Linear weight with scale factor\n+        self.weight = nn.Parameter(torch.randn(out_dim, in_dim))\n+        self.scale = 1 / math.sqrt(in_dim)\n+\n+        # If an activation is present, the bias will be fused to it\n+        if bias and not self.use_activation:\n+            self.bias = nn.Parameter(torch.zeros(out_dim))\n+        else:\n+            self.bias = None\n+\n+        if self.use_activation:\n+            self.act_fn = FusedLeakyReLU(bias_channels=out_dim)\n+        else:\n+            self.act_fn = None\n+\n+    def forward(self, input: torch.Tensor, channel_dim: int = 1) -> torch.Tensor:\n+        out = F.linear(input, self.weight * self.scale, bias=self.bias)\n+        if self.use_activation:\n+            out = self.act_fn(out, channel_dim=channel_dim)\n+        return out\n+\n+    def __repr__(self):\n+        return (\n+            f\"{self.__class__.__name__}(in_features={self.weight.shape[1]}, out_features={self.weight.shape[0]},\"\n+            f\" bias={self.bias is not None})\"\n+        )\n+\n+\n+class MotionEncoderResBlock(nn.Module):\n+    def __init__(\n+        self,\n+        in_channels: int,\n+        out_channels: int,\n+        kernel_size: int = 3,\n+        kernel_size_skip: int = 1,\n+        blur_kernel: Tuple[int, ...] = (1, 3, 3, 1),\n+        downsample_factor: int = 2,\n+    ):\n+        super().__init__()\n+        self.downsample_factor = downsample_factor\n+\n+        # 3 x 3 Conv + fused leaky ReLU\n+        self.conv1 = MotionConv2d(\n+            in_channels,\n+            in_channels,\n+            kernel_size,\n+            stride=1,\n+            padding=kernel_size // 2,\n+            use_activation=True,\n+        )\n+\n+        # 3 x 3 Conv that downsamples 2x + fused leaky ReLU\n+        self.conv2 = MotionConv2d(\n+            in_channels,\n+            out_channels,\n+            kernel_size=kernel_size,\n+            stride=self.downsample_factor,\n+            padding=0,\n+            blur_kernel=blur_kernel,\n+            use_activation=True,\n+        )\n+\n+        # 1 x 1 Conv that downsamples 2x in skip connection\n+        self.conv_skip = MotionConv2d(\n+            in_channels,\n+            out_channels,\n+            kernel_size=kernel_size_skip,\n+            stride=self.downsample_factor,\n+            padding=0,\n+            bias=False,\n+            blur_kernel=blur_kernel,\n+            use_activation=False,\n+        )\n+\n+    def forward(self, x: torch.Tensor, channel_dim: int = 1) -> torch.Tensor:\n+        x_out = self.conv1(x, channel_dim)\n+        x_out = self.conv2(x_out, channel_dim)\n+\n+        x_skip = self.conv_skip(x, channel_dim)\n+\n+        x_out = (x_out + x_skip) / math.sqrt(2)\n+        return x_out\n+\n+\n+class WanAnimateMotionEncoder(nn.Module):\n+    def __init__(\n+        self,\n+        size: int = 512,\n+        style_dim: int = 512,\n+        motion_dim: int = 20,\n+        out_dim: int = 512,\n+        motion_blocks: int = 5,\n+        channels: Optional[Dict[str, int]] = None,\n+    ):\n+        super().__init__()\n+        self.size = size\n+\n+        # Appearance encoder: conv layers\n+        if channels is None:\n+            channels = WAN_ANIMATE_MOTION_ENCODER_CHANNEL_SIZES\n+\n+        self.conv_in = MotionConv2d(3, channels[str(size)], 1, use_activation=True)\n+\n+        self.res_blocks = nn.ModuleList()\n+        in_channels = channels[str(size)]\n+        log_size = int(math.log(size, 2))\n+        for i in range(log_size, 2, -1):\n+            out_channels = channels[str(2 ** (i - 1))]\n+            self.res_blocks.append(MotionEncoderResBlock(in_channels, out_channels))\n+            in_channels = out_channels\n+\n+        self.conv_out = MotionConv2d(in_channels, style_dim, 4, padding=0, bias=False, use_activation=False)\n+\n+        # Motion encoder: linear layers\n+        # NOTE: there are no activations in between the linear layers here, which is weird but I believe matches the\n+        # original code.\n+        linears = [MotionLinear(style_dim, style_dim) for _ in range(motion_blocks - 1)]\n+        linears.append(MotionLinear(style_dim, motion_dim))\n+        self.motion_network = nn.ModuleList(linears)\n+\n+        self.motion_synthesis_weight = nn.Parameter(torch.randn(out_dim, motion_dim))\n+\n+    def forward(self, face_image: torch.Tensor, channel_dim: int = 1) -> torch.Tensor:\n+        if (face_image.shape[-2] != self.size) or (face_image.shape[-1] != self.size):\n+            raise ValueError(\n+                f\"Face pixel values has resolution ({face_image.shape[-1]}, {face_image.shape[-2]}) but is expected\"\n+                f\" to have resolution ({self.size}, {self.size})\"\n+            )\n+\n+        # Appearance encoding through convs\n+        face_image = self.conv_in(face_image, channel_dim)\n+        for block in self.res_blocks:\n+            face_image = block(face_image, channel_dim)\n+        face_image = self.conv_out(face_image, channel_dim)\n+        motion_feat = face_image.squeeze(-1).squeeze(-1)\n+\n+        # Motion feature extraction\n+        for linear_layer in self.motion_network:\n+            motion_feat = linear_layer(motion_feat, channel_dim=channel_dim)\n+\n+        # Motion synthesis via Linear Motion Decomposition\n+        weight = self.motion_synthesis_weight + 1e-8\n+        # Upcast the QR orthogonalization operation to FP32\n+        original_motion_dtype = motion_feat.dtype\n+        motion_feat = motion_feat.to(torch.float32)\n+        weight = weight.to(torch.float32)\n+\n+        Q = torch.linalg.qr(weight)[0].to(device=motion_feat.device)\n+\n+        motion_feat_diag = torch.diag_embed(motion_feat)  # Alpha, diagonal matrix\n+        motion_decomposition = torch.matmul(motion_feat_diag, Q.T)\n+        motion_vec = torch.sum(motion_decomposition, dim=1)\n+\n+        motion_vec = motion_vec.to(dtype=original_motion_dtype)\n+\n+        return motion_vec\n+\n+\n+class WanAnimateFaceEncoder(nn.Module):\n+    def __init__(\n+        self,\n+        in_dim: int,\n+        out_dim: int,\n+        hidden_dim: int = 1024,\n+        num_heads: int = 4,\n+        kernel_size: int = 3,\n+        eps: float = 1e-6,\n+        pad_mode: str = \"replicate\",\n+    ):\n+        super().__init__()\n+        self.num_heads = num_heads\n+        self.time_causal_padding = (kernel_size - 1, 0)\n+        self.pad_mode = pad_mode\n+\n+        self.act = nn.SiLU()\n+\n+        self.conv1_local = nn.Conv1d(in_dim, hidden_dim * num_heads, kernel_size=kernel_size, stride=1)\n+        self.conv2 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size, stride=2)\n+        self.conv3 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size, stride=2)\n+\n+        self.norm1 = nn.LayerNorm(hidden_dim, eps, elementwise_affine=False)\n+        self.norm2 = nn.LayerNorm(hidden_dim, eps, elementwise_affine=False)\n+        self.norm3 = nn.LayerNorm(hidden_dim, eps, elementwise_affine=False)\n+\n+        self.out_proj = nn.Linear(hidden_dim, out_dim)\n+\n+        self.padding_tokens = nn.Parameter(torch.zeros(1, 1, 1, out_dim))\n+\n+    def forward(self, x: torch.Tensor) -> torch.Tensor:\n+        batch_size = x.shape[0]\n+\n+        # Reshape to channels-first to apply causal Conv1d over frame dim\n+        x = x.permute(0, 2, 1)\n+        x = F.pad(x, self.time_causal_padding, mode=self.pad_mode)\n+        x = self.conv1_local(x)  # [B, C, T_padded] --> [B, N * C, T]\n+        x = x.unflatten(1, (self.num_heads, -1)).flatten(0, 1)  # [B, N * C, T] --> [B * N, C, T]\n+        # Reshape back to channels-last to apply LayerNorm over channel dim\n+        x = x.permute(0, 2, 1)\n+        x = self.norm1(x)\n+        x = self.act(x)\n+\n+        x = x.permute(0, 2, 1)\n+        x = F.pad(x, self.time_causal_padding, mode=self.pad_mode)\n+        x = self.conv2(x)\n+        x = x.permute(0, 2, 1)\n+        x = self.norm2(x)\n+        x = self.act(x)\n+\n+        x = x.permute(0, 2, 1)\n+        x = F.pad(x, self.time_causal_padding, mode=self.pad_mode)\n+        x = self.conv3(x)\n+        x = x.permute(0, 2, 1)\n+        x = self.norm3(x)\n+        x = self.act(x)\n+\n+        x = self.out_proj(x)\n+        x = x.unflatten(0, (batch_size, -1)).permute(0, 2, 1, 3)  # [B * N, T, C_out] --> [B, T, N, C_out]\n+\n+        padding = self.padding_tokens.repeat(batch_size, x.shape[1], 1, 1).to(device=x.device)\n+        x = torch.cat([x, padding], dim=-2)  # [B, T, N, C_out] --> [B, T, N + 1, C_out]\n+\n+        return x\n+\n+\n+class WanAnimateFaceBlockAttnProcessor:\n+    _attention_backend = None\n+    _parallel_config = None\n+\n+    def __init__(self):\n+        if not hasattr(F, \"scaled_dot_product_attention\"):\n+            raise ImportError(\n+                f\"{self.__class__.__name__} requires PyTorch 2.0. To use it, please upgrade PyTorch to version 2.0 or\"\n+                f\" higher.\"\n+            )\n+\n+    def __call__(\n+        self,\n+        attn: \"WanAnimateFaceBlockCrossAttention\",\n+        hidden_states: torch.Tensor,\n+        encoder_hidden_states: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+    ) -> torch.Tensor:\n+        # encoder_hidden_states corresponds to the motion vec\n+        # attention_mask corresponds to the motion mask (if any)\n+        hidden_states = attn.pre_norm_q(hidden_states)\n+        encoder_hidden_states = attn.pre_norm_kv(encoder_hidden_states)\n+\n+        # B --> batch_size, T --> reduced inference segment len, N --> face_encoder_num_heads + 1, C --> attn.dim\n+        B, T, N, C = encoder_hidden_states.shape\n+\n+        query, key, value = _get_qkv_projections(attn, hidden_states, encoder_hidden_states)\n+\n+        query = query.unflatten(2, (attn.heads, -1))  # [B, S, H * D] --> [B, S, H, D]\n+        key = key.view(B, T, N, attn.heads, -1)  # [B, T, N, H * D_kv] --> [B, T, N, H, D_kv]\n+        value = value.view(B, T, N, attn.heads, -1)\n+\n+        query = attn.norm_q(query)\n+        key = attn.norm_k(key)\n+\n+        # NOTE: the below line (which follows the official code) means that in practice, the number of frames T in\n+        # encoder_hidden_states (the motion vector after applying the face encoder) must evenly divide the\n+        # post-patchify sequence length S of the transformer hidden_states. Is it possible to remove this dependency?\n+        query = query.unflatten(1, (T, -1)).flatten(0, 1)  # [B, S, H, D] --> [B * T, S / T, H, D]\n+        key = key.flatten(0, 1)  # [B, T, N, H, D_kv] --> [B * T, N, H, D_kv]\n+        value = value.flatten(0, 1)\n+\n+        hidden_states = dispatch_attention_fn(\n+            query,\n+            key,\n+            value,\n+            attn_mask=None,\n+            dropout_p=0.0,\n+            is_causal=False,\n+            backend=self._attention_backend,\n+            parallel_config=self._parallel_config,\n+        )\n+\n+        hidden_states = hidden_states.flatten(2, 3)\n+        hidden_states = hidden_states.type_as(query)\n+        hidden_states = hidden_states.unflatten(0, (B, T)).flatten(1, 2)\n+\n+        hidden_states = attn.to_out(hidden_states)\n+\n+        if attention_mask is not None:\n+            # NOTE: attention_mask is assumed to be a multiplicative mask\n+            attention_mask = attention_mask.flatten(start_dim=1)\n+            hidden_states = hidden_states * attention_mask\n+\n+        return hidden_states\n+\n+\n+class WanAnimateFaceBlockCrossAttention(nn.Module, AttentionModuleMixin):\n+    \"\"\"\n+    Temporally-aligned cross attention with the face motion signal in the Wan Animate Face Blocks.\n+    \"\"\"\n+\n+    _default_processor_cls = WanAnimateFaceBlockAttnProcessor\n+    _available_processors = [WanAnimateFaceBlockAttnProcessor]\n+\n+    def __init__(\n+        self,\n+        dim: int,\n+        heads: int = 8,\n+        dim_head: int = 64,\n+        eps: float = 1e-6,\n+        cross_attention_dim_head: Optional[int] = None,\n+        processor=None,\n+    ):\n+        super().__init__()\n+        self.inner_dim = dim_head * heads\n+        self.heads = heads\n+        self.cross_attention_head_dim = cross_attention_dim_head\n+        self.kv_inner_dim = self.inner_dim if cross_attention_dim_head is None else cross_attention_dim_head * heads\n+\n+        # 1. Pre-Attention Norms for the hidden_states (video latents) and encoder_hidden_states (motion vector).\n+        # NOTE: this is not used in \"vanilla\" WanAttention\n+        self.pre_norm_q = nn.LayerNorm(dim, eps, elementwise_affine=False)\n+        self.pre_norm_kv = nn.LayerNorm(dim, eps, elementwise_affine=False)\n+\n+        # 2. QKV and Output Projections\n+        self.to_q = torch.nn.Linear(dim, self.inner_dim, bias=True)\n+        self.to_k = torch.nn.Linear(dim, self.kv_inner_dim, bias=True)\n+        self.to_v = torch.nn.Linear(dim, self.kv_inner_dim, bias=True)\n+        self.to_out = torch.nn.Linear(self.inner_dim, dim, bias=True)\n+\n+        # 3. QK Norm\n+        # NOTE: this is applied after the reshape, so only over dim_head rather than dim_head * heads\n+        self.norm_q = torch.nn.RMSNorm(dim_head, eps=eps, elementwise_affine=True)\n+        self.norm_k = torch.nn.RMSNorm(dim_head, eps=eps, elementwise_affine=True)\n+\n+        # 4. Set attention processor\n+        if processor is None:\n+            processor = self._default_processor_cls()\n+        self.set_processor(processor)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        encoder_hidden_states: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        **kwargs,\n+    ) -> torch.Tensor:\n+        return self.processor(self, hidden_states, encoder_hidden_states, attention_mask)\n+\n+\n+# Copied from diffusers.models.transformers.transformer_wan.WanAttnProcessor\n+class WanAttnProcessor:\n+    _attention_backend = None\n+    _parallel_config = None\n+\n+    def __init__(self):\n+        if not hasattr(F, \"scaled_dot_product_attention\"):\n+            raise ImportError(\n+                \"WanAttnProcessor requires PyTorch 2.0. To use it, please upgrade PyTorch to version 2.0 or higher.\"\n+            )\n+\n+    def __call__(\n+        self,\n+        attn: \"WanAttention\",\n+        hidden_states: torch.Tensor,\n+        encoder_hidden_states: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        rotary_emb: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n+    ) -> torch.Tensor:\n+        encoder_hidden_states_img = None\n+        if attn.add_k_proj is not None:\n+            # 512 is the context length of the text encoder, hardcoded for now\n+            image_context_length = encoder_hidden_states.shape[1] - 512\n+            encoder_hidden_states_img = encoder_hidden_states[:, :image_context_length]\n+            encoder_hidden_states = encoder_hidden_states[:, image_context_length:]\n+\n+        query, key, value = _get_qkv_projections(attn, hidden_states, encoder_hidden_states)\n+\n+        query = attn.norm_q(query)\n+        key = attn.norm_k(key)\n+\n+        query = query.unflatten(2, (attn.heads, -1))\n+        key = key.unflatten(2, (attn.heads, -1))\n+        value = value.unflatten(2, (attn.heads, -1))\n+\n+        if rotary_emb is not None:\n+\n+            def apply_rotary_emb(\n+                hidden_states: torch.Tensor,\n+                freqs_cos: torch.Tensor,\n+                freqs_sin: torch.Tensor,\n+            ):\n+                x1, x2 = hidden_states.unflatten(-1, (-1, 2)).unbind(-1)\n+                cos = freqs_cos[..., 0::2]\n+                sin = freqs_sin[..., 1::2]\n+                out = torch.empty_like(hidden_states)\n+                out[..., 0::2] = x1 * cos - x2 * sin\n+                out[..., 1::2] = x1 * sin + x2 * cos\n+                return out.type_as(hidden_states)\n+\n+            query = apply_rotary_emb(query, *rotary_emb)\n+            key = apply_rotary_emb(key, *rotary_emb)\n+\n+        # I2V task\n+        hidden_states_img = None\n+        if encoder_hidden_states_img is not None:\n+            key_img, value_img = _get_added_kv_projections(attn, encoder_hidden_states_img)\n+            key_img = attn.norm_added_k(key_img)\n+\n+            key_img = key_img.unflatten(2, (attn.heads, -1))\n+            value_img = value_img.unflatten(2, (attn.heads, -1))\n+\n+            hidden_states_img = dispatch_attention_fn(\n+                query,\n+                key_img,\n+                value_img,\n+                attn_mask=None,\n+                dropout_p=0.0,\n+                is_causal=False,\n+                backend=self._attention_backend,\n+                parallel_config=self._parallel_config,\n+            )\n+            hidden_states_img = hidden_states_img.flatten(2, 3)\n+            hidden_states_img = hidden_states_img.type_as(query)\n+\n+        hidden_states = dispatch_attention_fn(\n+            query,\n+            key,\n+            value,\n+            attn_mask=attention_mask,\n+            dropout_p=0.0,\n+            is_causal=False,\n+            backend=self._attention_backend,\n+            parallel_config=self._parallel_config,\n+        )\n+        hidden_states = hidden_states.flatten(2, 3)\n+        hidden_states = hidden_states.type_as(query)\n+\n+        if hidden_states_img is not None:\n+            hidden_states = hidden_states + hidden_states_img\n+\n+        hidden_states = attn.to_out[0](hidden_states)\n+        hidden_states = attn.to_out[1](hidden_states)\n+        return hidden_states\n+\n+\n+# Copied from diffusers.models.transformers.transformer_wan.WanAttention\n+class WanAttention(torch.nn.Module, AttentionModuleMixin):\n+    _default_processor_cls = WanAttnProcessor\n+    _available_processors = [WanAttnProcessor]\n+\n+    def __init__(\n+        self,\n+        dim: int,\n+        heads: int = 8,\n+        dim_head: int = 64,\n+        eps: float = 1e-5,\n+        dropout: float = 0.0,\n+        added_kv_proj_dim: Optional[int] = None,\n+        cross_attention_dim_head: Optional[int] = None,\n+        processor=None,\n+        is_cross_attention=None,\n+    ):\n+        super().__init__()\n+\n+        self.inner_dim = dim_head * heads\n+        self.heads = heads\n+        self.added_kv_proj_dim = added_kv_proj_dim\n+        self.cross_attention_dim_head = cross_attention_dim_head\n+        self.kv_inner_dim = self.inner_dim if cross_attention_dim_head is None else cross_attention_dim_head * heads\n+\n+        self.to_q = torch.nn.Linear(dim, self.inner_dim, bias=True)\n+        self.to_k = torch.nn.Linear(dim, self.kv_inner_dim, bias=True)\n+        self.to_v = torch.nn.Linear(dim, self.kv_inner_dim, bias=True)\n+        self.to_out = torch.nn.ModuleList(\n+            [\n+                torch.nn.Linear(self.inner_dim, dim, bias=True),\n+                torch.nn.Dropout(dropout),\n+            ]\n+        )\n+        self.norm_q = torch.nn.RMSNorm(dim_head * heads, eps=eps, elementwise_affine=True)\n+        self.norm_k = torch.nn.RMSNorm(dim_head * heads, eps=eps, elementwise_affine=True)\n+\n+        self.add_k_proj = self.add_v_proj = None\n+        if added_kv_proj_dim is not None:\n+            self.add_k_proj = torch.nn.Linear(added_kv_proj_dim, self.inner_dim, bias=True)\n+            self.add_v_proj = torch.nn.Linear(added_kv_proj_dim, self.inner_dim, bias=True)\n+            self.norm_added_k = torch.nn.RMSNorm(dim_head * heads, eps=eps)\n+\n+        self.is_cross_attention = cross_attention_dim_head is not None\n+\n+        self.set_processor(processor)\n+\n+    def fuse_projections(self):\n+        if getattr(self, \"fused_projections\", False):\n+            return\n+\n+        if self.cross_attention_dim_head is None:\n+            concatenated_weights = torch.cat([self.to_q.weight.data, self.to_k.weight.data, self.to_v.weight.data])\n+            concatenated_bias = torch.cat([self.to_q.bias.data, self.to_k.bias.data, self.to_v.bias.data])\n+            out_features, in_features = concatenated_weights.shape\n+            with torch.device(\"meta\"):\n+                self.to_qkv = nn.Linear(in_features, out_features, bias=True)\n+            self.to_qkv.load_state_dict(\n+                {\"weight\": concatenated_weights, \"bias\": concatenated_bias}, strict=True, assign=True\n+            )\n+        else:\n+            concatenated_weights = torch.cat([self.to_k.weight.data, self.to_v.weight.data])\n+            concatenated_bias = torch.cat([self.to_k.bias.data, self.to_v.bias.data])\n+            out_features, in_features = concatenated_weights.shape\n+            with torch.device(\"meta\"):\n+                self.to_kv = nn.Linear(in_features, out_features, bias=True)\n+            self.to_kv.load_state_dict(\n+                {\"weight\": concatenated_weights, \"bias\": concatenated_bias}, strict=True, assign=True\n+            )\n+\n+        if self.added_kv_proj_dim is not None:\n+            concatenated_weights = torch.cat([self.add_k_proj.weight.data, self.add_v_proj.weight.data])\n+            concatenated_bias = torch.cat([self.add_k_proj.bias.data, self.add_v_proj.bias.data])\n+            out_features, in_features = concatenated_weights.shape\n+            with torch.device(\"meta\"):\n+                self.to_added_kv = nn.Linear(in_features, out_features, bias=True)\n+            self.to_added_kv.load_state_dict(\n+                {\"weight\": concatenated_weights, \"bias\": concatenated_bias}, strict=True, assign=True\n+            )\n+\n+        self.fused_projections = True\n+\n+    @torch.no_grad()\n+    def unfuse_projections(self):\n+        if not getattr(self, \"fused_projections\", False):\n+            return\n+\n+        if hasattr(self, \"to_qkv\"):\n+            delattr(self, \"to_qkv\")\n+        if hasattr(self, \"to_kv\"):\n+            delattr(self, \"to_kv\")\n+        if hasattr(self, \"to_added_kv\"):\n+            delattr(self, \"to_added_kv\")\n+\n+        self.fused_projections = False\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        encoder_hidden_states: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        rotary_emb: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n+        **kwargs,\n+    ) -> torch.Tensor:\n+        return self.processor(self, hidden_states, encoder_hidden_states, attention_mask, rotary_emb, **kwargs)\n+\n+\n+# Copied from diffusers.models.transformers.transformer_wan.WanImageEmbedding\n+class WanImageEmbedding(torch.nn.Module):\n+    def __init__(self, in_features: int, out_features: int, pos_embed_seq_len=None):\n+        super().__init__()\n+\n+        self.norm1 = FP32LayerNorm(in_features)\n+        self.ff = FeedForward(in_features, out_features, mult=1, activation_fn=\"gelu\")\n+        self.norm2 = FP32LayerNorm(out_features)\n+        if pos_embed_seq_len is not None:\n+            self.pos_embed = nn.Parameter(torch.zeros(1, pos_embed_seq_len, in_features))\n+        else:\n+            self.pos_embed = None\n+\n+    def forward(self, encoder_hidden_states_image: torch.Tensor) -> torch.Tensor:\n+        if self.pos_embed is not None:\n+            batch_size, seq_len, embed_dim = encoder_hidden_states_image.shape\n+            encoder_hidden_states_image = encoder_hidden_states_image.view(-1, 2 * seq_len, embed_dim)\n+            encoder_hidden_states_image = encoder_hidden_states_image + self.pos_embed\n+\n+        hidden_states = self.norm1(encoder_hidden_states_image)\n+        hidden_states = self.ff(hidden_states)\n+        hidden_states = self.norm2(hidden_states)\n+        return hidden_states\n+\n+\n+# Copied from diffusers.models.transformers.transformer_wan.WanTimeTextImageEmbedding\n+class WanTimeTextImageEmbedding(nn.Module):\n+    def __init__(\n+        self,\n+        dim: int,\n+        time_freq_dim: int,\n+        time_proj_dim: int,\n+        text_embed_dim: int,\n+        image_embed_dim: Optional[int] = None,\n+        pos_embed_seq_len: Optional[int] = None,\n+    ):\n+        super().__init__()\n+\n+        self.timesteps_proj = Timesteps(num_channels=time_freq_dim, flip_sin_to_cos=True, downscale_freq_shift=0)\n+        self.time_embedder = TimestepEmbedding(in_channels=time_freq_dim, time_embed_dim=dim)\n+        self.act_fn = nn.SiLU()\n+        self.time_proj = nn.Linear(dim, time_proj_dim)\n+        self.text_embedder = PixArtAlphaTextProjection(text_embed_dim, dim, act_fn=\"gelu_tanh\")\n+\n+        self.image_embedder = None\n+        if image_embed_dim is not None:\n+            self.image_embedder = WanImageEmbedding(image_embed_dim, dim, pos_embed_seq_len=pos_embed_seq_len)\n+\n+    def forward(\n+        self,\n+        timestep: torch.Tensor,\n+        encoder_hidden_states: torch.Tensor,\n+        encoder_hidden_states_image: Optional[torch.Tensor] = None,\n+        timestep_seq_len: Optional[int] = None,\n+    ):\n+        timestep = self.timesteps_proj(timestep)\n+        if timestep_seq_len is not None:\n+            timestep = timestep.unflatten(0, (-1, timestep_seq_len))\n+\n+        time_embedder_dtype = next(iter(self.time_embedder.parameters())).dtype\n+        if timestep.dtype != time_embedder_dtype and time_embedder_dtype != torch.int8:\n+            timestep = timestep.to(time_embedder_dtype)\n+        temb = self.time_embedder(timestep).type_as(encoder_hidden_states)\n+        timestep_proj = self.time_proj(self.act_fn(temb))\n+\n+        encoder_hidden_states = self.text_embedder(encoder_hidden_states)\n+        if encoder_hidden_states_image is not None:\n+            encoder_hidden_states_image = self.image_embedder(encoder_hidden_states_image)\n+\n+        return temb, timestep_proj, encoder_hidden_states, encoder_hidden_states_image\n+\n+\n+# Copied from diffusers.models.transformers.transformer_wan.WanRotaryPosEmbed\n+class WanRotaryPosEmbed(nn.Module):\n+    def __init__(\n+        self,\n+        attention_head_dim: int,\n+        patch_size: Tuple[int, int, int],\n+        max_seq_len: int,\n+        theta: float = 10000.0,\n+    ):\n+        super().__init__()\n+\n+        self.attention_head_dim = attention_head_dim\n+        self.patch_size = patch_size\n+        self.max_seq_len = max_seq_len\n+\n+        h_dim = w_dim = 2 * (attention_head_dim // 6)\n+        t_dim = attention_head_dim - h_dim - w_dim\n+\n+        self.t_dim = t_dim\n+        self.h_dim = h_dim\n+        self.w_dim = w_dim\n+\n+        freqs_dtype = torch.float32 if torch.backends.mps.is_available() else torch.float64\n+\n+        freqs_cos = []\n+        freqs_sin = []\n+\n+        for dim in [t_dim, h_dim, w_dim]:\n+            freq_cos, freq_sin = get_1d_rotary_pos_embed(\n+                dim,\n+                max_seq_len,\n+                theta,\n+                use_real=True,\n+                repeat_interleave_real=True,\n+                freqs_dtype=freqs_dtype,\n+            )\n+            freqs_cos.append(freq_cos)\n+            freqs_sin.append(freq_sin)\n+\n+        self.register_buffer(\"freqs_cos\", torch.cat(freqs_cos, dim=1), persistent=False)\n+        self.register_buffer(\"freqs_sin\", torch.cat(freqs_sin, dim=1), persistent=False)\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        batch_size, num_channels, num_frames, height, width = hidden_states.shape\n+        p_t, p_h, p_w = self.patch_size\n+        ppf, pph, ppw = num_frames // p_t, height // p_h, width // p_w\n+\n+        split_sizes = [self.t_dim, self.h_dim, self.w_dim]\n+\n+        freqs_cos = self.freqs_cos.split(split_sizes, dim=1)\n+        freqs_sin = self.freqs_sin.split(split_sizes, dim=1)\n+\n+        freqs_cos_f = freqs_cos[0][:ppf].view(ppf, 1, 1, -1).expand(ppf, pph, ppw, -1)\n+        freqs_cos_h = freqs_cos[1][:pph].view(1, pph, 1, -1).expand(ppf, pph, ppw, -1)\n+        freqs_cos_w = freqs_cos[2][:ppw].view(1, 1, ppw, -1).expand(ppf, pph, ppw, -1)\n+\n+        freqs_sin_f = freqs_sin[0][:ppf].view(ppf, 1, 1, -1).expand(ppf, pph, ppw, -1)\n+        freqs_sin_h = freqs_sin[1][:pph].view(1, pph, 1, -1).expand(ppf, pph, ppw, -1)\n+        freqs_sin_w = freqs_sin[2][:ppw].view(1, 1, ppw, -1).expand(ppf, pph, ppw, -1)\n+\n+        freqs_cos = torch.cat([freqs_cos_f, freqs_cos_h, freqs_cos_w], dim=-1).reshape(1, ppf * pph * ppw, 1, -1)\n+        freqs_sin = torch.cat([freqs_sin_f, freqs_sin_h, freqs_sin_w], dim=-1).reshape(1, ppf * pph * ppw, 1, -1)\n+\n+        return freqs_cos, freqs_sin\n+\n+\n+# Copied from diffusers.models.transformers.transformer_wan.WanTransformerBlock\n+class WanTransformerBlock(nn.Module):\n+    def __init__(\n+        self,\n+        dim: int,\n+        ffn_dim: int,\n+        num_heads: int,\n+        qk_norm: str = \"rms_norm_across_heads\",\n+        cross_attn_norm: bool = False,\n+        eps: float = 1e-6,\n+        added_kv_proj_dim: Optional[int] = None,\n+    ):\n+        super().__init__()\n+\n+        # 1. Self-attention\n+        self.norm1 = FP32LayerNorm(dim, eps, elementwise_affine=False)\n+        self.attn1 = WanAttention(\n+            dim=dim,\n+            heads=num_heads,\n+            dim_head=dim // num_heads,\n+            eps=eps,\n+            cross_attention_dim_head=None,\n+            processor=WanAttnProcessor(),\n+        )\n+\n+        # 2. Cross-attention\n+        self.attn2 = WanAttention(\n+            dim=dim,\n+            heads=num_heads,\n+            dim_head=dim // num_heads,\n+            eps=eps,\n+            added_kv_proj_dim=added_kv_proj_dim,\n+            cross_attention_dim_head=dim // num_heads,\n+            processor=WanAttnProcessor(),\n+        )\n+        self.norm2 = FP32LayerNorm(dim, eps, elementwise_affine=True) if cross_attn_norm else nn.Identity()\n+\n+        # 3. Feed-forward\n+        self.ffn = FeedForward(dim, inner_dim=ffn_dim, activation_fn=\"gelu-approximate\")\n+        self.norm3 = FP32LayerNorm(dim, eps, elementwise_affine=False)\n+\n+        self.scale_shift_table = nn.Parameter(torch.randn(1, 6, dim) / dim**0.5)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        encoder_hidden_states: torch.Tensor,\n+        temb: torch.Tensor,\n+        rotary_emb: torch.Tensor,\n+    ) -> torch.Tensor:\n+        if temb.ndim == 4:\n+            # temb: batch_size, seq_len, 6, inner_dim (wan2.2 ti2v)\n+            shift_msa, scale_msa, gate_msa, c_shift_msa, c_scale_msa, c_gate_msa = (\n+                self.scale_shift_table.unsqueeze(0) + temb.float()\n+            ).chunk(6, dim=2)\n+            # batch_size, seq_len, 1, inner_dim\n+            shift_msa = shift_msa.squeeze(2)\n+            scale_msa = scale_msa.squeeze(2)\n+            gate_msa = gate_msa.squeeze(2)\n+            c_shift_msa = c_shift_msa.squeeze(2)\n+            c_scale_msa = c_scale_msa.squeeze(2)\n+            c_gate_msa = c_gate_msa.squeeze(2)\n+        else:\n+            # temb: batch_size, 6, inner_dim (wan2.1/wan2.2 14B)\n+            shift_msa, scale_msa, gate_msa, c_shift_msa, c_scale_msa, c_gate_msa = (\n+                self.scale_shift_table + temb.float()\n+            ).chunk(6, dim=1)\n+\n+        # 1. Self-attention\n+        norm_hidden_states = (self.norm1(hidden_states.float()) * (1 + scale_msa) + shift_msa).type_as(hidden_states)\n+        attn_output = self.attn1(norm_hidden_states, None, None, rotary_emb)\n+        hidden_states = (hidden_states.float() + attn_output * gate_msa).type_as(hidden_states)\n+\n+        # 2. Cross-attention\n+        norm_hidden_states = self.norm2(hidden_states.float()).type_as(hidden_states)\n+        attn_output = self.attn2(norm_hidden_states, encoder_hidden_states, None, None)\n+        hidden_states = hidden_states + attn_output\n+\n+        # 3. Feed-forward\n+        norm_hidden_states = (self.norm3(hidden_states.float()) * (1 + c_scale_msa) + c_shift_msa).type_as(\n+            hidden_states\n+        )\n+        ff_output = self.ffn(norm_hidden_states)\n+        hidden_states = (hidden_states.float() + ff_output.float() * c_gate_msa).type_as(hidden_states)\n+\n+        return hidden_states\n+\n+\n+class WanAnimateTransformer3DModel(\n+    ModelMixin, ConfigMixin, PeftAdapterMixin, FromOriginalModelMixin, CacheMixin, AttentionMixin\n+):\n+    r\"\"\"\n+    A Transformer model for video-like data used in the WanAnimate model.\n+\n+    Args:\n+        patch_size (`Tuple[int]`, defaults to `(1, 2, 2)`):\n+            3D patch dimensions for video embedding (t_patch, h_patch, w_patch).\n+        num_attention_heads (`int`, defaults to `40`):\n+            Fixed length for text embeddings.\n+        attention_head_dim (`int`, defaults to `128`):\n+            The number of channels in each head.\n+        in_channels (`int`, defaults to `16`):\n+            The number of channels in the input.\n+        out_channels (`int`, defaults to `16`):\n+            The number of channels in the output.\n+        text_dim (`int`, defaults to `512`):\n+            Input dimension for text embeddings.\n+        freq_dim (`int`, defaults to `256`):\n+            Dimension for sinusoidal time embeddings.\n+        ffn_dim (`int`, defaults to `13824`):\n+            Intermediate dimension in feed-forward network.\n+        num_layers (`int`, defaults to `40`):\n+            The number of layers of transformer blocks to use.\n+        window_size (`Tuple[int]`, defaults to `(-1, -1)`):\n+            Window size for local attention (-1 indicates global attention).\n+        cross_attn_norm (`bool`, defaults to `True`):\n+            Enable cross-attention normalization.\n+        qk_norm (`bool`, defaults to `True`):\n+            Enable query/key normalization.\n+        eps (`float`, defaults to `1e-6`):\n+            Epsilon value for normalization layers.\n+        image_dim (`int`, *optional*, defaults to `1280`):\n+            The number of channels to use for the image embedding. If `None`, no projection is used.\n+        added_kv_proj_dim (`int`, *optional*, defaults to `5120`):\n+            The number of channels to use for the added key and value projections. If `None`, no projection is used.\n+    \"\"\"\n+\n+    _supports_gradient_checkpointing = True\n+    _skip_layerwise_casting_patterns = [\"patch_embedding\", \"condition_embedder\", \"norm\"]\n+    _no_split_modules = [\"WanTransformerBlock\", \"MotionEncoderResBlock\"]\n+    _keep_in_fp32_modules = [\n+        \"time_embedder\",\n+        \"scale_shift_table\",\n+        \"norm1\",\n+        \"norm2\",\n+        \"norm3\",\n+        \"motion_synthesis_weight\",\n+    ]\n+    _keys_to_ignore_on_load_unexpected = [\"norm_added_q\"]\n+    _repeated_blocks = [\"WanTransformerBlock\"]\n+\n+    @register_to_config\n+    def __init__(\n+        self,\n+        patch_size: Tuple[int] = (1, 2, 2),\n+        num_attention_heads: int = 40,\n+        attention_head_dim: int = 128,\n+        in_channels: Optional[int] = 36,\n+        latent_channels: Optional[int] = 16,\n+        out_channels: Optional[int] = 16,\n+        text_dim: int = 4096,\n+        freq_dim: int = 256,\n+        ffn_dim: int = 13824,\n+        num_layers: int = 40,\n+        cross_attn_norm: bool = True,\n+        qk_norm: Optional[str] = \"rms_norm_across_heads\",\n+        eps: float = 1e-6,\n+        image_dim: Optional[int] = 1280,\n+        added_kv_proj_dim: Optional[int] = None,\n+        rope_max_seq_len: int = 1024,\n+        pos_embed_seq_len: Optional[int] = None,\n+        motion_encoder_channel_sizes: Optional[Dict[str, int]] = None,  # Start of Wan Animate-specific args\n+        motion_encoder_size: int = 512,\n+        motion_style_dim: int = 512,\n+        motion_dim: int = 20,\n+        motion_encoder_dim: int = 512,\n+        face_encoder_hidden_dim: int = 1024,\n+        face_encoder_num_heads: int = 4,\n+        inject_face_latents_blocks: int = 5,\n+        motion_encoder_batch_size: int = 8,\n+    ) -> None:\n+        super().__init__()\n+\n+        inner_dim = num_attention_heads * attention_head_dim\n+        # Allow either only in_channels or only latent_channels to be set for convenience\n+        if in_channels is None and latent_channels is not None:\n+            in_channels = 2 * latent_channels + 4\n+        elif in_channels is not None and latent_channels is None:\n+            latent_channels = (in_channels - 4) // 2\n+        elif in_channels is not None and latent_channels is not None:\n+            # TODO: should this always be true?\n+            assert in_channels == 2 * latent_channels + 4, \"in_channels should be 2 * latent_channels + 4\"\n+        else:\n+            raise ValueError(\"At least one of `in_channels` and `latent_channels` must be supplied.\")\n+        out_channels = out_channels or latent_channels\n+\n+        # 1. Patch & position embedding\n+        self.rope = WanRotaryPosEmbed(attention_head_dim, patch_size, rope_max_seq_len)\n+        self.patch_embedding = nn.Conv3d(in_channels, inner_dim, kernel_size=patch_size, stride=patch_size)\n+        self.pose_patch_embedding = nn.Conv3d(latent_channels, inner_dim, kernel_size=patch_size, stride=patch_size)\n+\n+        # 2. Condition embeddings\n+        self.condition_embedder = WanTimeTextImageEmbedding(\n+            dim=inner_dim,\n+            time_freq_dim=freq_dim,\n+            time_proj_dim=inner_dim * 6,\n+            text_embed_dim=text_dim,\n+            image_embed_dim=image_dim,\n+            pos_embed_seq_len=pos_embed_seq_len,\n+        )\n+\n+        # Motion encoder\n+        self.motion_encoder = WanAnimateMotionEncoder(\n+            size=motion_encoder_size,\n+            style_dim=motion_style_dim,\n+            motion_dim=motion_dim,\n+            out_dim=motion_encoder_dim,\n+            channels=motion_encoder_channel_sizes,\n+        )\n+\n+        # Face encoder\n+        self.face_encoder = WanAnimateFaceEncoder(\n+            in_dim=motion_encoder_dim,\n+            out_dim=inner_dim,\n+            hidden_dim=face_encoder_hidden_dim,\n+            num_heads=face_encoder_num_heads,\n+        )\n+\n+        # 3. Transformer blocks\n+        self.blocks = nn.ModuleList(\n+            [\n+                WanTransformerBlock(\n+                    dim=inner_dim,\n+                    ffn_dim=ffn_dim,\n+                    num_heads=num_attention_heads,\n+                    qk_norm=qk_norm,\n+                    cross_attn_norm=cross_attn_norm,\n+                    eps=eps,\n+                    added_kv_proj_dim=added_kv_proj_dim,\n+                )\n+                for _ in range(num_layers)\n+            ]\n+        )\n+\n+        self.face_adapter = nn.ModuleList(\n+            [\n+                WanAnimateFaceBlockCrossAttention(\n+                    dim=inner_dim,\n+                    heads=num_attention_heads,\n+                    dim_head=inner_dim // num_attention_heads,\n+                    eps=eps,\n+                    cross_attention_dim_head=inner_dim // num_attention_heads,\n+                    processor=WanAnimateFaceBlockAttnProcessor(),\n+                )\n+                for _ in range(num_layers // inject_face_latents_blocks)\n+            ]\n+        )\n+\n+        # 4. Output norm & projection\n+        self.norm_out = FP32LayerNorm(inner_dim, eps, elementwise_affine=False)\n+        self.proj_out = nn.Linear(inner_dim, out_channels * math.prod(patch_size))\n+        self.scale_shift_table = nn.Parameter(torch.randn(1, 2, inner_dim) / inner_dim**0.5)\n+\n+        self.gradient_checkpointing = False\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        timestep: torch.LongTensor,\n+        encoder_hidden_states: torch.Tensor,\n+        encoder_hidden_states_image: Optional[torch.Tensor] = None,\n+        pose_hidden_states: Optional[torch.Tensor] = None,\n+        face_pixel_values: Optional[torch.Tensor] = None,\n+        motion_encode_batch_size: Optional[int] = None,\n+        return_dict: bool = True,\n+        attention_kwargs: Optional[Dict[str, Any]] = None,\n+    ) -> Union[torch.Tensor, Dict[str, torch.Tensor]]:\n+        \"\"\"\n+        Forward pass of Wan2.2-Animate transformer model.\n+\n+        Args:\n+            hidden_states (`torch.Tensor` of shape `(B, 2C + 4, T + 1, H, W)`):\n+                Input noisy video latents of shape `(B, 2C + 4, T + 1, H, W)`, where B is the batch size, C is the\n+                number of latent channels (16 for Wan VAE), T is the number of latent frames in an inference segment, H\n+                is the latent height, and W is the latent width.\n+            timestep: (`torch.LongTensor`):\n+                The current timestep in the denoising loop.\n+            encoder_hidden_states (`torch.Tensor`):\n+                Text embeddings from the text encoder (umT5 for Wan Animate).\n+            encoder_hidden_states_image (`torch.Tensor`):\n+                CLIP visual features of the reference (character) image.\n+            pose_hidden_states (`torch.Tensor` of shape `(B, C, T, H, W)`):\n+                Pose video latents. TODO: description\n+            face_pixel_values (`torch.Tensor` of shape `(B, C', S, H', W')`):\n+                Face video in pixel space (not latent space). Typically C' = 3 and H' and W' are the height/width of\n+                the face video in pixels. Here S is the inference segment length, usually set to 77.\n+            motion_encode_batch_size (`int`, *optional*):\n+                The batch size for batched encoding of the face video via the motion encoder. Will default to\n+                `self.config.motion_encoder_batch_size` if not set.\n+            return_dict (`bool`, *optional*, defaults to `True`):\n+                Whether to return the output as a dict or tuple.\n+        \"\"\"\n+\n+        if attention_kwargs is not None:\n+            attention_kwargs = attention_kwargs.copy()\n+            lora_scale = attention_kwargs.pop(\"scale\", 1.0)\n+        else:\n+            lora_scale = 1.0\n+\n+        if USE_PEFT_BACKEND:\n+            # weight the lora layers by setting `lora_scale` for each PEFT layer\n+            scale_lora_layers(self, lora_scale)\n+        else:\n+            if attention_kwargs is not None and attention_kwargs.get(\"scale\", None) is not None:\n+                logger.warning(\n+                    \"Passing `scale` via `attention_kwargs` when not using the PEFT backend is ineffective.\"\n+                )\n+\n+        # Check that shapes match up\n+        if pose_hidden_states is not None and pose_hidden_states.shape[2] + 1 != hidden_states.shape[2]:\n+            raise ValueError(\n+                f\"pose_hidden_states frame dim (dim 2) is {pose_hidden_states.shape[2]} but must be one less than the\"\n+                f\" hidden_states's corresponding frame dim: {hidden_states.shape[2]}\"\n+            )\n+\n+        batch_size, num_channels, num_frames, height, width = hidden_states.shape\n+        p_t, p_h, p_w = self.config.patch_size\n+        post_patch_num_frames = num_frames // p_t\n+        post_patch_height = height // p_h\n+        post_patch_width = width // p_w\n+\n+        # 1. Rotary position embedding\n+        rotary_emb = self.rope(hidden_states)\n+\n+        # 2. Patch embedding\n+        hidden_states = self.patch_embedding(hidden_states)\n+        pose_hidden_states = self.pose_patch_embedding(pose_hidden_states)\n+        # Add pose embeddings to hidden states\n+        hidden_states[:, :, 1:] = hidden_states[:, :, 1:] + pose_hidden_states\n+        # Calling contiguous() here is important so that we don't recompile when performing regional compilation\n+        hidden_states = hidden_states.flatten(2).transpose(1, 2).contiguous()\n+\n+        # 3. Condition embeddings (time, text, image)\n+        # Wan Animate is based on Wan 2.1 and thus uses Wan 2.1's timestep logic\n+        temb, timestep_proj, encoder_hidden_states, encoder_hidden_states_image = self.condition_embedder(\n+            timestep, encoder_hidden_states, encoder_hidden_states_image, timestep_seq_len=None\n+        )\n+\n+        # batch_size, 6, inner_dim\n+        timestep_proj = timestep_proj.unflatten(1, (6, -1))\n+\n+        if encoder_hidden_states_image is not None:\n+            encoder_hidden_states = torch.concat([encoder_hidden_states_image, encoder_hidden_states], dim=1)\n+\n+        # 4. Get motion features from the face video\n+        # Motion vector computation from face pixel values\n+        batch_size, channels, num_face_frames, height, width = face_pixel_values.shape\n+        # Rearrange from (B, C, T, H, W) to (B*T, C, H, W)\n+        face_pixel_values = face_pixel_values.permute(0, 2, 1, 3, 4).reshape(-1, channels, height, width)\n+\n+        # Extract motion features using motion encoder\n+        # Perform batched motion encoder inference to allow trading off inference speed for memory usage\n+        motion_encode_batch_size = motion_encode_batch_size or self.config.motion_encoder_batch_size\n+        face_batches = torch.split(face_pixel_values, motion_encode_batch_size)\n+        motion_vec_batches = []\n+        for face_batch in face_batches:\n+            motion_vec_batch = self.motion_encoder(face_batch)\n+            motion_vec_batches.append(motion_vec_batch)\n+        motion_vec = torch.cat(motion_vec_batches)\n+        motion_vec = motion_vec.view(batch_size, num_face_frames, -1)\n+\n+        # Now get face features from the motion vector\n+        motion_vec = self.face_encoder(motion_vec)\n+\n+        # Add padding at the beginning (prepend zeros)\n+        pad_face = torch.zeros_like(motion_vec[:, :1])\n+        motion_vec = torch.cat([pad_face, motion_vec], dim=1)\n+\n+        # 5. Transformer blocks with face adapter integration\n+        for block_idx, block in enumerate(self.blocks):\n+            if torch.is_grad_enabled() and self.gradient_checkpointing:\n+                hidden_states = self._gradient_checkpointing_func(\n+                    block, hidden_states, encoder_hidden_states, timestep_proj, rotary_emb\n+                )\n+            else:\n+                hidden_states = block(hidden_states, encoder_hidden_states, timestep_proj, rotary_emb)\n+\n+            # Face adapter integration: apply after every 5th block (0, 5, 10, 15, ...)\n+            if block_idx % self.config.inject_face_latents_blocks == 0:\n+                face_adapter_block_idx = block_idx // self.config.inject_face_latents_blocks\n+                face_adapter_output = self.face_adapter[face_adapter_block_idx](hidden_states, motion_vec)\n+                # In case the face adapter and main transformer blocks are on different devices, which can happen when\n+                # using model parallelism\n+                face_adapter_output = face_adapter_output.to(device=hidden_states.device)\n+                hidden_states = face_adapter_output + hidden_states\n+\n+        # 6. Output norm, projection & unpatchify\n+        # batch_size, inner_dim\n+        shift, scale = (self.scale_shift_table.to(temb.device) + temb.unsqueeze(1)).chunk(2, dim=1)\n+\n+        hidden_states_original_dtype = hidden_states.dtype\n+        hidden_states = self.norm_out(hidden_states.float())\n+        # Move the shift and scale tensors to the same device as hidden_states.\n+        # When using multi-GPU inference via accelerate these will be on the\n+        # first device rather than the last device, which hidden_states ends up\n+        # on.\n+        shift = shift.to(hidden_states.device)\n+        scale = scale.to(hidden_states.device)\n+        hidden_states = (hidden_states * (1 + scale) + shift).to(dtype=hidden_states_original_dtype)\n+\n+        hidden_states = self.proj_out(hidden_states)\n+\n+        hidden_states = hidden_states.reshape(\n+            batch_size, post_patch_num_frames, post_patch_height, post_patch_width, p_t, p_h, p_w, -1\n+        )\n+        hidden_states = hidden_states.permute(0, 7, 1, 4, 2, 5, 3, 6)\n+        output = hidden_states.flatten(6, 7).flatten(4, 5).flatten(2, 3)\n+\n+        if USE_PEFT_BACKEND:\n+            # remove `lora_scale` from each PEFT layer\n+            unscale_lora_layers(self, lora_scale)\n+\n+        if not return_dict:\n+            return (output,)\n+\n+        return Transformer2DModelOutput(sample=output)"
        },
        {
          "filename": "src/diffusers/pipelines/__init__.py",
          "status": "modified",
          "additions": 14,
          "deletions": 2,
          "changes": 16,
          "patch": "@@ -385,7 +385,13 @@\n         \"WuerstchenDecoderPipeline\",\n         \"WuerstchenPriorPipeline\",\n     ]\n-    _import_structure[\"wan\"] = [\"WanPipeline\", \"WanImageToVideoPipeline\", \"WanVideoToVideoPipeline\", \"WanVACEPipeline\"]\n+    _import_structure[\"wan\"] = [\n+        \"WanPipeline\",\n+        \"WanImageToVideoPipeline\",\n+        \"WanVideoToVideoPipeline\",\n+        \"WanVACEPipeline\",\n+        \"WanAnimatePipeline\",\n+    ]\n     _import_structure[\"kandinsky5\"] = [\"Kandinsky5T2VPipeline\"]\n     _import_structure[\"skyreels_v2\"] = [\n         \"SkyReelsV2DiffusionForcingPipeline\",\n@@ -803,7 +809,13 @@\n             UniDiffuserTextDecoder,\n         )\n         from .visualcloze import VisualClozeGenerationPipeline, VisualClozePipeline\n-        from .wan import WanImageToVideoPipeline, WanPipeline, WanVACEPipeline, WanVideoToVideoPipeline\n+        from .wan import (\n+            WanAnimatePipeline,\n+            WanImageToVideoPipeline,\n+            WanPipeline,\n+            WanVACEPipeline,\n+            WanVideoToVideoPipeline,\n+        )\n         from .wuerstchen import (\n             WuerstchenCombinedPipeline,\n             WuerstchenDecoderPipeline,"
        },
        {
          "filename": "src/diffusers/pipelines/wan/__init__.py",
          "status": "modified",
          "additions": 2,
          "deletions": 1,
          "changes": 3,
          "patch": "@@ -23,6 +23,7 @@\n     _dummy_objects.update(get_objects_from_module(dummy_torch_and_transformers_objects))\n else:\n     _import_structure[\"pipeline_wan\"] = [\"WanPipeline\"]\n+    _import_structure[\"pipeline_wan_animate\"] = [\"WanAnimatePipeline\"]\n     _import_structure[\"pipeline_wan_i2v\"] = [\"WanImageToVideoPipeline\"]\n     _import_structure[\"pipeline_wan_vace\"] = [\"WanVACEPipeline\"]\n     _import_structure[\"pipeline_wan_video2video\"] = [\"WanVideoToVideoPipeline\"]\n@@ -35,10 +36,10 @@\n         from ...utils.dummy_torch_and_transformers_objects import *\n     else:\n         from .pipeline_wan import WanPipeline\n+        from .pipeline_wan_animate import WanAnimatePipeline\n         from .pipeline_wan_i2v import WanImageToVideoPipeline\n         from .pipeline_wan_vace import WanVACEPipeline\n         from .pipeline_wan_video2video import WanVideoToVideoPipeline\n-\n else:\n     import sys\n "
        },
        {
          "filename": "src/diffusers/pipelines/wan/image_processor.py",
          "status": "added",
          "additions": 185,
          "deletions": 0,
          "changes": 185,
          "patch": "@@ -0,0 +1,185 @@\n+# Copyright 2025 The Wan Team and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import Optional, Tuple, Union\n+\n+import numpy as np\n+import PIL.Image\n+import torch\n+\n+from ...configuration_utils import register_to_config\n+from ...image_processor import VaeImageProcessor\n+from ...utils import PIL_INTERPOLATION\n+\n+\n+class WanAnimateImageProcessor(VaeImageProcessor):\n+    r\"\"\"\n+    Image processor to preprocess the reference (character) image for the Wan Animate model.\n+\n+    Args:\n+        do_resize (`bool`, *optional*, defaults to `True`):\n+            Whether to downscale the image's (height, width) dimensions to multiples of `vae_scale_factor`. Can accept\n+            `height` and `width` arguments from [`image_processor.VaeImageProcessor.preprocess`] method.\n+        vae_scale_factor (`int`, *optional*, defaults to `8`):\n+            VAE (spatial) scale factor. If `do_resize` is `True`, the image is automatically resized to multiples of\n+            this factor.\n+        vae_latent_channels (`int`, *optional*, defaults to `16`):\n+            VAE latent channels.\n+        spatial_patch_size (`Tuple[int, int]`, *optional*, defaults to `(2, 2)`):\n+            The spatial patch size used by the diffusion transformer. For Wan models, this is typically (2, 2).\n+        resample (`str`, *optional*, defaults to `lanczos`):\n+            Resampling filter to use when resizing the image.\n+        do_normalize (`bool`, *optional*, defaults to `True`):\n+            Whether to normalize the image to [-1,1].\n+        do_binarize (`bool`, *optional*, defaults to `False`):\n+            Whether to binarize the image to 0/1.\n+        do_convert_rgb (`bool`, *optional*, defaults to be `False`):\n+            Whether to convert the images to RGB format.\n+        do_convert_grayscale (`bool`, *optional*, defaults to be `False`):\n+            Whether to convert the images to grayscale format.\n+        fill_color (`str` or `float` or `Tuple[float, ...]`, *optional*, defaults to `None`):\n+            An optional fill color when `resize_mode` is set to `\"fill\"`. This will fill the empty space with that\n+            color instead of filling with data from the image. Any valid `color` argument to `PIL.Image.new` is valid;\n+            if `None`, will default to filling with data from `image`.\n+    \"\"\"\n+\n+    @register_to_config\n+    def __init__(\n+        self,\n+        do_resize: bool = True,\n+        vae_scale_factor: int = 8,\n+        vae_latent_channels: int = 16,\n+        spatial_patch_size: Tuple[int, int] = (2, 2),\n+        resample: str = \"lanczos\",\n+        reducing_gap: int = None,\n+        do_normalize: bool = True,\n+        do_binarize: bool = False,\n+        do_convert_rgb: bool = False,\n+        do_convert_grayscale: bool = False,\n+        fill_color: Optional[Union[str, float, Tuple[float, ...]]] = 0,\n+    ):\n+        super().__init__()\n+        if do_convert_rgb and do_convert_grayscale:\n+            raise ValueError(\n+                \"`do_convert_rgb` and `do_convert_grayscale` can not both be set to `True`,\"\n+                \" if you intended to convert the image into RGB format, please set `do_convert_grayscale = False`.\",\n+                \" if you intended to convert the image into grayscale format, please set `do_convert_rgb = False`\",\n+            )\n+\n+    def _resize_and_fill(\n+        self,\n+        image: PIL.Image.Image,\n+        width: int,\n+        height: int,\n+    ) -> PIL.Image.Image:\n+        r\"\"\"\n+        Resize the image to fit within the specified width and height, maintaining the aspect ratio, and then center\n+        the image within the dimensions, filling empty with data from image.\n+\n+        Args:\n+            image (`PIL.Image.Image`):\n+                The image to resize and fill.\n+            width (`int`):\n+                The width to resize the image to.\n+            height (`int`):\n+                The height to resize the image to.\n+\n+        Returns:\n+            `PIL.Image.Image`:\n+                The resized and filled image.\n+        \"\"\"\n+\n+        ratio = width / height\n+        src_ratio = image.width / image.height\n+        fill_with_image_data = self.config.fill_color is None\n+        fill_color = self.config.fill_color or 0\n+\n+        src_w = width if ratio < src_ratio else image.width * height // image.height\n+        src_h = height if ratio >= src_ratio else image.height * width // image.width\n+\n+        resized = image.resize((src_w, src_h), resample=PIL_INTERPOLATION[self.config.resample])\n+        res = PIL.Image.new(\"RGB\", (width, height), color=fill_color)\n+        res.paste(resized, box=(width // 2 - src_w // 2, height // 2 - src_h // 2))\n+\n+        if fill_with_image_data:\n+            if ratio < src_ratio:\n+                fill_height = height // 2 - src_h // 2\n+                if fill_height > 0:\n+                    res.paste(resized.resize((width, fill_height), box=(0, 0, width, 0)), box=(0, 0))\n+                    res.paste(\n+                        resized.resize((width, fill_height), box=(0, resized.height, width, resized.height)),\n+                        box=(0, fill_height + src_h),\n+                    )\n+            elif ratio > src_ratio:\n+                fill_width = width // 2 - src_w // 2\n+                if fill_width > 0:\n+                    res.paste(resized.resize((fill_width, height), box=(0, 0, 0, height)), box=(0, 0))\n+                    res.paste(\n+                        resized.resize((fill_width, height), box=(resized.width, 0, resized.width, height)),\n+                        box=(fill_width + src_w, 0),\n+                    )\n+\n+        return res\n+\n+    def get_default_height_width(\n+        self,\n+        image: Union[PIL.Image.Image, np.ndarray, torch.Tensor],\n+        height: Optional[int] = None,\n+        width: Optional[int] = None,\n+    ) -> Tuple[int, int]:\n+        r\"\"\"\n+        Returns the height and width of the image, downscaled to the next integer multiple of `vae_scale_factor`.\n+\n+        Args:\n+            image (`Union[PIL.Image.Image, np.ndarray, torch.Tensor]`):\n+                The image input, which can be a PIL image, NumPy array, or PyTorch tensor. If it is a NumPy array, it\n+                should have shape `[batch, height, width]` or `[batch, height, width, channels]`. If it is a PyTorch\n+                tensor, it should have shape `[batch, channels, height, width]`.\n+            height (`Optional[int]`, *optional*, defaults to `None`):\n+                The height of the preprocessed image. If `None`, the height of the `image` input will be used.\n+            width (`Optional[int]`, *optional*, defaults to `None`):\n+                The width of the preprocessed image. If `None`, the width of the `image` input will be used.\n+\n+        Returns:\n+            `Tuple[int, int]`:\n+                A tuple containing the height and width, both resized to the nearest integer multiple of\n+                `vae_scale_factor * spatial_patch_size`.\n+        \"\"\"\n+\n+        if height is None:\n+            if isinstance(image, PIL.Image.Image):\n+                height = image.height\n+            elif isinstance(image, torch.Tensor):\n+                height = image.shape[2]\n+            else:\n+                height = image.shape[1]\n+\n+        if width is None:\n+            if isinstance(image, PIL.Image.Image):\n+                width = image.width\n+            elif isinstance(image, torch.Tensor):\n+                width = image.shape[3]\n+            else:\n+                width = image.shape[2]\n+\n+        max_area = width * height\n+        aspect_ratio = height / width\n+        mod_value_h = self.config.vae_scale_factor * self.config.spatial_patch_size[0]\n+        mod_value_w = self.config.vae_scale_factor * self.config.spatial_patch_size[1]\n+\n+        # Try to preserve the aspect ratio\n+        height = round(np.sqrt(max_area * aspect_ratio)) // mod_value_h * mod_value_h\n+        width = round(np.sqrt(max_area / aspect_ratio)) // mod_value_w * mod_value_w\n+\n+        return height, width"
        },
        {
          "filename": "src/diffusers/pipelines/wan/pipeline_wan_animate.py",
          "status": "added",
          "additions": 1204,
          "deletions": 0,
          "changes": 1204,
          "patch": "@@ -0,0 +1,1204 @@\n+# Copyright 2025 The Wan Team and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import html\n+from copy import deepcopy\n+from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n+\n+import PIL\n+import regex as re\n+import torch\n+import torch.nn.functional as F\n+from transformers import AutoTokenizer, CLIPImageProcessor, CLIPVisionModel, UMT5EncoderModel\n+\n+from ...callbacks import MultiPipelineCallbacks, PipelineCallback\n+from ...image_processor import PipelineImageInput\n+from ...loaders import WanLoraLoaderMixin\n+from ...models import AutoencoderKLWan, WanAnimateTransformer3DModel\n+from ...schedulers import UniPCMultistepScheduler\n+from ...utils import is_ftfy_available, is_torch_xla_available, logging, replace_example_docstring\n+from ...utils.torch_utils import randn_tensor\n+from ...video_processor import VideoProcessor\n+from ..pipeline_utils import DiffusionPipeline\n+from .image_processor import WanAnimateImageProcessor\n+from .pipeline_output import WanPipelineOutput\n+\n+\n+if is_torch_xla_available():\n+    import torch_xla.core.xla_model as xm\n+\n+    XLA_AVAILABLE = True\n+else:\n+    XLA_AVAILABLE = False\n+\n+logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n+\n+if is_ftfy_available():\n+    import ftfy\n+\n+EXAMPLE_DOC_STRING = \"\"\"\n+    Examples:\n+        ```python\n+        >>> import torch\n+        >>> import numpy as np\n+        >>> from diffusers import WanAnimatePipeline\n+        >>> from diffusers.utils import export_to_video, load_image, load_video\n+\n+        >>> model_id = \"Wan-AI/Wan2.2-Animate-14B-Diffusers\"\n+        >>> pipe = WanAnimatePipeline.from_pretrained(model_id, torch_dtype=torch.bfloat16)\n+        >>> # Optionally upcast the Wan VAE to FP32\n+        >>> pipe.vae.to(torch.float32)\n+        >>> pipe.to(\"cuda\")\n+\n+        >>> # Load the reference character image\n+        >>> image = load_image(\n+        ...     \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/astronaut.jpg\"\n+        ... )\n+\n+        >>> # Load pose and face videos (preprocessed from reference video)\n+        >>> # Note: Videos should be preprocessed to extract pose keypoints and face features\n+        >>> # Refer to the Wan-Animate preprocessing documentation for details\n+        >>> pose_video = load_video(\"path/to/pose_video.mp4\")\n+        >>> face_video = load_video(\"path/to/face_video.mp4\")\n+\n+        >>> # CFG is generally not used for Wan Animate\n+        >>> prompt = (\n+        ...     \"An astronaut hatching from an egg, on the surface of the moon, the darkness and depth of space realised in \"\n+        ...     \"the background. High quality, ultrarealistic detail and breath-taking movie-like camera shot.\"\n+        ... )\n+\n+        >>> # Animation mode: Animate the character with the motion from pose/face videos\n+        >>> output = pipe(\n+        ...     image=image,\n+        ...     pose_video=pose_video,\n+        ...     face_video=face_video,\n+        ...     prompt=prompt,\n+        ...     height=height,\n+        ...     width=width,\n+        ...     segment_frame_length=77,  # Frame length of each inference segment\n+        ...     guidance_scale=1.0,\n+        ...     num_inference_steps=20,\n+        ...     mode=\"animate\",\n+        ... ).frames[0]\n+        >>> export_to_video(output, \"output_animation.mp4\", fps=30)\n+\n+        >>> # Replacement mode: Replace a character in the background video\n+        >>> # Requires additional background_video and mask_video inputs\n+        >>> background_video = load_video(\"path/to/background_video.mp4\")\n+        >>> mask_video = load_video(\"path/to/mask_video.mp4\")  # Black areas preserved, white areas generated\n+        >>> output = pipe(\n+        ...     image=image,\n+        ...     pose_video=pose_video,\n+        ...     face_video=face_video,\n+        ...     background_video=background_video,\n+        ...     mask_video=mask_video,\n+        ...     prompt=prompt,\n+        ...     height=height,\n+        ...     width=width,\n+        ...     segment_frame_length=77,  # Frame length of each inference segment\n+        ...     guidance_scale=1.0,\n+        ...     num_inference_steps=20,\n+        ...     mode=\"replace\",\n+        ... ).frames[0]\n+        >>> export_to_video(output, \"output_replacement.mp4\", fps=30)\n+        ```\n+\"\"\"\n+\n+\n+def basic_clean(text):\n+    text = ftfy.fix_text(text)\n+    text = html.unescape(html.unescape(text))\n+    return text.strip()\n+\n+\n+def whitespace_clean(text):\n+    text = re.sub(r\"\\s+\", \" \", text)\n+    text = text.strip()\n+    return text\n+\n+\n+def prompt_clean(text):\n+    text = whitespace_clean(basic_clean(text))\n+    return text\n+\n+\n+# Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_img2img.retrieve_latents\n+def retrieve_latents(\n+    encoder_output: torch.Tensor, generator: Optional[torch.Generator] = None, sample_mode: str = \"sample\"\n+):\n+    if hasattr(encoder_output, \"latent_dist\") and sample_mode == \"sample\":\n+        return encoder_output.latent_dist.sample(generator)\n+    elif hasattr(encoder_output, \"latent_dist\") and sample_mode == \"argmax\":\n+        return encoder_output.latent_dist.mode()\n+    elif hasattr(encoder_output, \"latents\"):\n+        return encoder_output.latents\n+    else:\n+        raise AttributeError(\"Could not access latents of provided encoder_output\")\n+\n+\n+class WanAnimatePipeline(DiffusionPipeline, WanLoraLoaderMixin):\n+    r\"\"\"\n+    Pipeline for unified character animation and replacement using Wan-Animate.\n+\n+    WanAnimatePipeline takes a character image, pose video, and face video as input, and generates a video in two\n+    modes:\n+\n+    1. **Animation mode**: The model generates a video of the character image that mimics the human motion in the input\n+       pose and face videos. The character is animated based on the provided motion controls, creating a new animated\n+       video of the character.\n+\n+    2. **Replacement mode**: The model replaces a character in a background video with the provided character image,\n+       using the pose and face videos for motion control. This mode requires additional `background_video` and\n+       `mask_video` inputs. The mask video should have black regions where the original content should be preserved and\n+       white regions where the new character should be generated.\n+\n+    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods\n+    implemented for all pipelines (downloading, saving, running on a particular device, etc.).\n+\n+    The pipeline also inherits the following loading methods:\n+        - [`~loaders.WanLoraLoaderMixin.load_lora_weights`] for loading LoRA weights\n+\n+    Args:\n+        tokenizer ([`T5Tokenizer`]):\n+            Tokenizer from [T5](https://huggingface.co/docs/transformers/en/model_doc/t5#transformers.T5Tokenizer),\n+            specifically the [google/umt5-xxl](https://huggingface.co/google/umt5-xxl) variant.\n+        text_encoder ([`T5EncoderModel`]):\n+            [T5](https://huggingface.co/docs/transformers/en/model_doc/t5#transformers.T5EncoderModel), specifically\n+            the [google/umt5-xxl](https://huggingface.co/google/umt5-xxl) variant.\n+        image_encoder ([`CLIPVisionModel`]):\n+            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPVisionModel), specifically\n+            the\n+            [clip-vit-huge-patch14](https://github.com/mlfoundations/open_clip/blob/main/docs/PRETRAINED.md#vit-h14-xlm-roberta-large)\n+            variant.\n+        transformer ([`WanAnimateTransformer3DModel`]):\n+            Conditional Transformer to denoise the input latents.\n+        scheduler ([`UniPCMultistepScheduler`]):\n+            A scheduler to be used in combination with `transformer` to denoise the encoded image latents.\n+        vae ([`AutoencoderKLWan`]):\n+            Variational Auto-Encoder (VAE) Model to encode and decode videos to and from latent representations.\n+        image_processor ([`CLIPImageProcessor`]):\n+            Image processor for preprocessing images before encoding.\n+    \"\"\"\n+\n+    model_cpu_offload_seq = \"text_encoder->image_encoder->transformer->vae\"\n+    _callback_tensor_inputs = [\"latents\", \"prompt_embeds\", \"negative_prompt_embeds\"]\n+\n+    def __init__(\n+        self,\n+        tokenizer: AutoTokenizer,\n+        text_encoder: UMT5EncoderModel,\n+        vae: AutoencoderKLWan,\n+        scheduler: UniPCMultistepScheduler,\n+        image_processor: CLIPImageProcessor,\n+        image_encoder: CLIPVisionModel,\n+        transformer: WanAnimateTransformer3DModel,\n+    ):\n+        super().__init__()\n+\n+        self.register_modules(\n+            vae=vae,\n+            text_encoder=text_encoder,\n+            tokenizer=tokenizer,\n+            image_encoder=image_encoder,\n+            transformer=transformer,\n+            scheduler=scheduler,\n+            image_processor=image_processor,\n+        )\n+\n+        self.vae_scale_factor_temporal = self.vae.config.scale_factor_temporal if getattr(self, \"vae\", None) else 4\n+        self.vae_scale_factor_spatial = self.vae.config.scale_factor_spatial if getattr(self, \"vae\", None) else 8\n+        self.video_processor = VideoProcessor(vae_scale_factor=self.vae_scale_factor_spatial)\n+        self.video_processor_for_mask = VideoProcessor(\n+            vae_scale_factor=self.vae_scale_factor_spatial, do_normalize=False, do_convert_grayscale=True\n+        )\n+        # In case self.transformer is None (e.g. for some pipeline tests)\n+        spatial_patch_size = self.transformer.config.patch_size[-2:] if self.transformer is not None else (2, 2)\n+        self.vae_image_processor = WanAnimateImageProcessor(\n+            vae_scale_factor=self.vae_scale_factor_spatial,\n+            spatial_patch_size=spatial_patch_size,\n+            resample=\"bilinear\",\n+            fill_color=0,\n+        )\n+        self.image_processor = image_processor\n+\n+    def _get_t5_prompt_embeds(\n+        self,\n+        prompt: Union[str, List[str]] = None,\n+        num_videos_per_prompt: int = 1,\n+        max_sequence_length: int = 512,\n+        device: Optional[torch.device] = None,\n+        dtype: Optional[torch.dtype] = None,\n+    ):\n+        device = device or self._execution_device\n+        dtype = dtype or self.text_encoder.dtype\n+\n+        prompt = [prompt] if isinstance(prompt, str) else prompt\n+        prompt = [prompt_clean(u) for u in prompt]\n+        batch_size = len(prompt)\n+\n+        text_inputs = self.tokenizer(\n+            prompt,\n+            padding=\"max_length\",\n+            max_length=max_sequence_length,\n+            truncation=True,\n+            add_special_tokens=True,\n+            return_attention_mask=True,\n+            return_tensors=\"pt\",\n+        )\n+        text_input_ids, mask = text_inputs.input_ids, text_inputs.attention_mask\n+        seq_lens = mask.gt(0).sum(dim=1).long()\n+\n+        prompt_embeds = self.text_encoder(text_input_ids.to(device), mask.to(device)).last_hidden_state\n+        prompt_embeds = prompt_embeds.to(dtype=dtype, device=device)\n+        prompt_embeds = [u[:v] for u, v in zip(prompt_embeds, seq_lens)]\n+        prompt_embeds = torch.stack(\n+            [torch.cat([u, u.new_zeros(max_sequence_length - u.size(0), u.size(1))]) for u in prompt_embeds], dim=0\n+        )\n+\n+        # duplicate text embeddings for each generation per prompt, using mps friendly method\n+        _, seq_len, _ = prompt_embeds.shape\n+        prompt_embeds = prompt_embeds.repeat(1, num_videos_per_prompt, 1)\n+        prompt_embeds = prompt_embeds.view(batch_size * num_videos_per_prompt, seq_len, -1)\n+\n+        return prompt_embeds\n+\n+    # Copied from diffusers.pipelines.wan.pipeline_wan_i2v.WanImageToVideoPipeline.encode_image\n+    def encode_image(\n+        self,\n+        image: PipelineImageInput,\n+        device: Optional[torch.device] = None,\n+    ):\n+        device = device or self._execution_device\n+        image = self.image_processor(images=image, return_tensors=\"pt\").to(device)\n+        image_embeds = self.image_encoder(**image, output_hidden_states=True)\n+        return image_embeds.hidden_states[-2]\n+\n+    # Copied from diffusers.pipelines.wan.pipeline_wan.WanPipeline.encode_prompt\n+    def encode_prompt(\n+        self,\n+        prompt: Union[str, List[str]],\n+        negative_prompt: Optional[Union[str, List[str]]] = None,\n+        do_classifier_free_guidance: bool = True,\n+        num_videos_per_prompt: int = 1,\n+        prompt_embeds: Optional[torch.Tensor] = None,\n+        negative_prompt_embeds: Optional[torch.Tensor] = None,\n+        max_sequence_length: int = 226,\n+        device: Optional[torch.device] = None,\n+        dtype: Optional[torch.dtype] = None,\n+    ):\n+        r\"\"\"\n+        Encodes the prompt into text encoder hidden states.\n+\n+        Args:\n+            prompt (`str` or `List[str]`, *optional*):\n+                prompt to be encoded\n+            negative_prompt (`str` or `List[str]`, *optional*):\n+                The prompt or prompts not to guide the image generation. If not defined, one has to pass\n+                `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is\n+                less than `1`).\n+            do_classifier_free_guidance (`bool`, *optional*, defaults to `True`):\n+                Whether to use classifier free guidance or not.\n+            num_videos_per_prompt (`int`, *optional*, defaults to 1):\n+                Number of videos that should be generated per prompt. torch device to place the resulting embeddings on\n+            prompt_embeds (`torch.Tensor`, *optional*):\n+                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n+                provided, text embeddings will be generated from `prompt` input argument.\n+            negative_prompt_embeds (`torch.Tensor`, *optional*):\n+                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt\n+                weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt` input\n+                argument.\n+            device: (`torch.device`, *optional*):\n+                torch device\n+            dtype: (`torch.dtype`, *optional*):\n+                torch dtype\n+        \"\"\"\n+        device = device or self._execution_device\n+\n+        prompt = [prompt] if isinstance(prompt, str) else prompt\n+        if prompt is not None:\n+            batch_size = len(prompt)\n+        else:\n+            batch_size = prompt_embeds.shape[0]\n+\n+        if prompt_embeds is None:\n+            prompt_embeds = self._get_t5_prompt_embeds(\n+                prompt=prompt,\n+                num_videos_per_prompt=num_videos_per_prompt,\n+                max_sequence_length=max_sequence_length,\n+                device=device,\n+                dtype=dtype,\n+            )\n+\n+        if do_classifier_free_guidance and negative_prompt_embeds is None:\n+            negative_prompt = negative_prompt or \"\"\n+            negative_prompt = batch_size * [negative_prompt] if isinstance(negative_prompt, str) else negative_prompt\n+\n+            if prompt is not None and type(prompt) is not type(negative_prompt):\n+                raise TypeError(\n+                    f\"`negative_prompt` should be the same type to `prompt`, but got {type(negative_prompt)} !=\"\n+                    f\" {type(prompt)}.\"\n+                )\n+            elif batch_size != len(negative_prompt):\n+                raise ValueError(\n+                    f\"`negative_prompt`: {negative_prompt} has batch size {len(negative_prompt)}, but `prompt`:\"\n+                    f\" {prompt} has batch size {batch_size}. Please make sure that passed `negative_prompt` matches\"\n+                    \" the batch size of `prompt`.\"\n+                )\n+\n+            negative_prompt_embeds = self._get_t5_prompt_embeds(\n+                prompt=negative_prompt,\n+                num_videos_per_prompt=num_videos_per_prompt,\n+                max_sequence_length=max_sequence_length,\n+                device=device,\n+                dtype=dtype,\n+            )\n+\n+        return prompt_embeds, negative_prompt_embeds\n+\n+    def check_inputs(\n+        self,\n+        prompt,\n+        negative_prompt,\n+        image,\n+        pose_video,\n+        face_video,\n+        background_video,\n+        mask_video,\n+        height,\n+        width,\n+        prompt_embeds=None,\n+        negative_prompt_embeds=None,\n+        image_embeds=None,\n+        callback_on_step_end_tensor_inputs=None,\n+        mode=None,\n+        prev_segment_conditioning_frames=None,\n+    ):\n+        if image is not None and image_embeds is not None:\n+            raise ValueError(\n+                f\"Cannot forward both `image`: {image} and `image_embeds`: {image_embeds}. Please make sure to\"\n+                \" only forward one of the two.\"\n+            )\n+        if image is None and image_embeds is None:\n+            raise ValueError(\n+                \"Provide either `image` or `prompt_embeds`. Cannot leave both `image` and `image_embeds` undefined.\"\n+            )\n+        if image is not None and not isinstance(image, torch.Tensor) and not isinstance(image, PIL.Image.Image):\n+            raise ValueError(f\"`image` has to be of type `torch.Tensor` or `PIL.Image.Image` but is {type(image)}\")\n+        if pose_video is None:\n+            raise ValueError(\"Provide `pose_video`. Cannot leave `pose_video` undefined.\")\n+        if face_video is None:\n+            raise ValueError(\"Provide `face_video`. Cannot leave `face_video` undefined.\")\n+        if not isinstance(pose_video, list) or not isinstance(face_video, list):\n+            raise ValueError(\"`pose_video` and `face_video` must be lists of PIL images.\")\n+        if len(pose_video) == 0 or len(face_video) == 0:\n+            raise ValueError(\"`pose_video` and `face_video` must contain at least one frame.\")\n+        if mode == \"replace\" and (background_video is None or mask_video is None):\n+            raise ValueError(\n+                \"Provide `background_video` and `mask_video`. Cannot leave both `background_video` and `mask_video`\"\n+                \" undefined when mode is `replace`.\"\n+            )\n+        if mode == \"replace\" and (not isinstance(background_video, list) or not isinstance(mask_video, list)):\n+            raise ValueError(\"`background_video` and `mask_video` must be lists of PIL images when mode is `replace`.\")\n+\n+        if height % 16 != 0 or width % 16 != 0:\n+            raise ValueError(f\"`height` and `width` have to be divisible by 16 but are {height} and {width}.\")\n+\n+        if callback_on_step_end_tensor_inputs is not None and not all(\n+            k in self._callback_tensor_inputs for k in callback_on_step_end_tensor_inputs\n+        ):\n+            raise ValueError(\n+                f\"`callback_on_step_end_tensor_inputs` has to be in {self._callback_tensor_inputs}, but found\"\n+                f\" {[k for k in callback_on_step_end_tensor_inputs if k not in self._callback_tensor_inputs]}\"\n+            )\n+\n+        if prompt is not None and prompt_embeds is not None:\n+            raise ValueError(\n+                f\"Cannot forward both `prompt`: {prompt} and `prompt_embeds`: {prompt_embeds}. Please make sure to\"\n+                \" only forward one of the two.\"\n+            )\n+        elif negative_prompt is not None and negative_prompt_embeds is not None:\n+            raise ValueError(\n+                f\"Cannot forward both `negative_prompt`: {negative_prompt} and `negative_prompt_embeds`: {negative_prompt_embeds}. Please make sure to\"\n+                \" only forward one of the two.\"\n+            )\n+        elif prompt is None and prompt_embeds is None:\n+            raise ValueError(\n+                \"Provide either `prompt` or `prompt_embeds`. Cannot leave both `prompt` and `prompt_embeds` undefined.\"\n+            )\n+        elif prompt is not None and (not isinstance(prompt, str) and not isinstance(prompt, list)):\n+            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n+        elif negative_prompt is not None and (\n+            not isinstance(negative_prompt, str) and not isinstance(negative_prompt, list)\n+        ):\n+            raise ValueError(f\"`negative_prompt` has to be of type `str` or `list` but is {type(negative_prompt)}\")\n+\n+        if mode is not None and (not isinstance(mode, str) or mode not in (\"animate\", \"replace\")):\n+            raise ValueError(\n+                f\"`mode` has to be of type `str` and in ('animate', 'replace') but its type is {type(mode)} and value is {mode}\"\n+            )\n+\n+        if prev_segment_conditioning_frames is not None and (\n+            not isinstance(prev_segment_conditioning_frames, int) or prev_segment_conditioning_frames not in (1, 5)\n+        ):\n+            raise ValueError(\n+                f\"`prev_segment_conditioning_frames` has to be of type `int` and 1 or 5 but its type is\"\n+                f\" {type(prev_segment_conditioning_frames)} and value is {prev_segment_conditioning_frames}\"\n+            )\n+\n+    def get_i2v_mask(\n+        self,\n+        batch_size: int,\n+        latent_t: int,\n+        latent_h: int,\n+        latent_w: int,\n+        mask_len: int = 1,\n+        mask_pixel_values: Optional[torch.Tensor] = None,\n+        dtype: Optional[torch.dtype] = None,\n+        device: Union[str, torch.device] = \"cuda\",\n+    ) -> torch.Tensor:\n+        # mask_pixel_values shape (if supplied): [B, C = 1, T, latent_h, latent_w]\n+        if mask_pixel_values is None:\n+            mask_lat_size = torch.zeros(\n+                batch_size, 1, (latent_t - 1) * 4 + 1, latent_h, latent_w, dtype=dtype, device=device\n+            )\n+        else:\n+            mask_lat_size = mask_pixel_values.clone().to(device=device, dtype=dtype)\n+        mask_lat_size[:, :, :mask_len] = 1\n+        first_frame_mask = mask_lat_size[:, :, 0:1]\n+        # Repeat first frame mask self.vae_scale_factor_temporal (= 4) times in the frame dimension\n+        first_frame_mask = torch.repeat_interleave(first_frame_mask, dim=2, repeats=self.vae_scale_factor_temporal)\n+        mask_lat_size = torch.concat([first_frame_mask, mask_lat_size[:, :, 1:]], dim=2)\n+        mask_lat_size = mask_lat_size.view(\n+            batch_size, -1, self.vae_scale_factor_temporal, latent_h, latent_w\n+        ).transpose(1, 2)  # [B, C = 1, 4 * T_lat, H_lat, W_lat] --> [B, C = 4, T_lat, H_lat, W_lat]\n+\n+        return mask_lat_size\n+\n+    def prepare_reference_image_latents(\n+        self,\n+        image: torch.Tensor,\n+        batch_size: int = 1,\n+        sample_mode: int = \"argmax\",\n+        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n+        dtype: Optional[torch.dtype] = None,\n+        device: Optional[torch.device] = None,\n+    ) -> torch.Tensor:\n+        # image shape: (B, C, H, W) or (B, C, T, H, W)\n+        dtype = dtype or self.vae.dtype\n+        if image.ndim == 4:\n+            # Add a singleton frame dimension after the channels dimension\n+            image = image.unsqueeze(2)\n+\n+        _, _, _, height, width = image.shape\n+        latent_height = height // self.vae_scale_factor_spatial\n+        latent_width = width // self.vae_scale_factor_spatial\n+\n+        # Encode image to latents using VAE\n+        image = image.to(device=device, dtype=dtype)\n+        if isinstance(generator, list):\n+            # Like in prepare_latents, assume len(generator) == batch_size\n+            ref_image_latents = [\n+                retrieve_latents(self.vae.encode(image), generator=g, sample_mode=sample_mode) for g in generator\n+            ]\n+            ref_image_latents = torch.cat(ref_image_latents)\n+        else:\n+            ref_image_latents = retrieve_latents(self.vae.encode(image), generator, sample_mode)\n+        # Standardize latents in preparation for Wan VAE encode\n+        latents_mean = (\n+            torch.tensor(self.vae.config.latents_mean)\n+            .view(1, self.vae.config.z_dim, 1, 1, 1)\n+            .to(ref_image_latents.device, ref_image_latents.dtype)\n+        )\n+        latents_recip_std = 1.0 / torch.tensor(self.vae.config.latents_std).view(1, self.vae.config.z_dim, 1, 1, 1).to(\n+            ref_image_latents.device, ref_image_latents.dtype\n+        )\n+        ref_image_latents = (ref_image_latents - latents_mean) * latents_recip_std\n+        # Handle the case where we supply one image and one generator, but batch_size > 1 (e.g. generating multiple\n+        # videos per prompt)\n+        if ref_image_latents.shape[0] == 1 and batch_size > 1:\n+            ref_image_latents = ref_image_latents.expand(batch_size, -1, -1, -1, -1)\n+\n+        # Prepare I2V mask in latent space and prepend to the reference image latents along channel dim\n+        reference_image_mask = self.get_i2v_mask(batch_size, 1, latent_height, latent_width, 1, None, dtype, device)\n+        reference_image_latents = torch.cat([reference_image_mask, ref_image_latents], dim=1)\n+\n+        return reference_image_latents\n+\n+    def prepare_prev_segment_cond_latents(\n+        self,\n+        prev_segment_cond_video: Optional[torch.Tensor] = None,\n+        background_video: Optional[torch.Tensor] = None,\n+        mask_video: Optional[torch.Tensor] = None,\n+        batch_size: int = 1,\n+        segment_frame_length: int = 77,\n+        start_frame: int = 0,\n+        height: int = 720,\n+        width: int = 1280,\n+        prev_segment_cond_frames: int = 1,\n+        task: str = \"animate\",\n+        interpolation_mode: str = \"bicubic\",\n+        sample_mode: str = \"argmax\",\n+        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n+        dtype: Optional[torch.dtype] = None,\n+        device: Optional[torch.device] = None,\n+    ) -> torch.Tensor:\n+        # prev_segment_cond_video shape: (B, C, T, H, W) in pixel space if supplied\n+        # background_video shape: (B, C, T, H, W) (same as prev_segment_cond_video shape)\n+        # mask_video shape: (B, 1, T, H, W) (same as prev_segment_cond_video, but with only 1 channel)\n+        dtype = dtype or self.vae.dtype\n+        if prev_segment_cond_video is None:\n+            if task == \"replace\":\n+                prev_segment_cond_video = background_video[:, :, :prev_segment_cond_frames].to(dtype)\n+            else:\n+                cond_frames_shape = (batch_size, 3, prev_segment_cond_frames, height, width)  # In pixel space\n+                prev_segment_cond_video = torch.zeros(cond_frames_shape, dtype=dtype, device=device)\n+\n+        data_batch_size, channels, _, segment_height, segment_width = prev_segment_cond_video.shape\n+        num_latent_frames = (segment_frame_length - 1) // self.vae_scale_factor_temporal + 1\n+        latent_height = height // self.vae_scale_factor_spatial\n+        latent_width = width // self.vae_scale_factor_spatial\n+        if segment_height != height or segment_width != width:\n+            print(\n+                f\"Interpolating prev segment cond video from ({segment_width}, {segment_height}) to ({width}, {height})\"\n+            )\n+            # Perform a 4D (spatial) rather than a 5D (spatiotemporal) reshape, following the original code\n+            prev_segment_cond_video = prev_segment_cond_video.transpose(1, 2).flatten(0, 1)  # [B * T, C, H, W]\n+            prev_segment_cond_video = F.interpolate(\n+                prev_segment_cond_video, size=(height, width), mode=interpolation_mode\n+            )\n+            prev_segment_cond_video = prev_segment_cond_video.unflatten(0, (batch_size, -1)).transpose(1, 2)\n+\n+        # Fill the remaining part of the cond video segment with zeros (if animating) or the background video (if\n+        # replacing).\n+        if task == \"replace\":\n+            remaining_segment = background_video[:, :, prev_segment_cond_frames:].to(dtype)\n+        else:\n+            remaining_segment_frames = segment_frame_length - prev_segment_cond_frames\n+            remaining_segment = torch.zeros(\n+                batch_size, channels, remaining_segment_frames, height, width, dtype=dtype, device=device\n+            )\n+\n+        # Prepend the conditioning frames from the previous segment to the remaining segment video in the frame dim\n+        prev_segment_cond_video = prev_segment_cond_video.to(dtype=dtype)\n+        full_segment_cond_video = torch.cat([prev_segment_cond_video, remaining_segment], dim=2)\n+\n+        if isinstance(generator, list):\n+            if data_batch_size == len(generator):\n+                prev_segment_cond_latents = [\n+                    retrieve_latents(self.vae.encode(full_segment_cond_video[i].unsqueeze(0)), g, sample_mode)\n+                    for i, g in enumerate(generator)\n+                ]\n+            elif data_batch_size == 1:\n+                # Like prepare_latents, assume len(generator) == batch_size\n+                prev_segment_cond_latents = [\n+                    retrieve_latents(self.vae.encode(full_segment_cond_video), g, sample_mode) for g in generator\n+                ]\n+            else:\n+                raise ValueError(\n+                    f\"The batch size of the prev segment video should be either {len(generator)} or 1 but is\"\n+                    f\" {data_batch_size}\"\n+                )\n+            prev_segment_cond_latents = torch.cat(prev_segment_cond_latents)\n+        else:\n+            prev_segment_cond_latents = retrieve_latents(\n+                self.vae.encode(full_segment_cond_video), generator, sample_mode\n+            )\n+        # Standardize latents in preparation for Wan VAE encode\n+        latents_mean = (\n+            torch.tensor(self.vae.config.latents_mean)\n+            .view(1, self.vae.config.z_dim, 1, 1, 1)\n+            .to(prev_segment_cond_latents.device, prev_segment_cond_latents.dtype)\n+        )\n+        latents_recip_std = 1.0 / torch.tensor(self.vae.config.latents_std).view(1, self.vae.config.z_dim, 1, 1, 1).to(\n+            prev_segment_cond_latents.device, prev_segment_cond_latents.dtype\n+        )\n+        prev_segment_cond_latents = (prev_segment_cond_latents - latents_mean) * latents_recip_std\n+\n+        # Prepare I2V mask\n+        if task == \"replace\":\n+            mask_video = 1 - mask_video\n+            mask_video = mask_video.permute(0, 2, 1, 3, 4)\n+            mask_video = mask_video.flatten(0, 1)\n+            mask_video = F.interpolate(mask_video, size=(latent_height, latent_width), mode=\"nearest\")\n+            mask_pixel_values = mask_video.unflatten(0, (batch_size, -1))\n+            mask_pixel_values = mask_pixel_values.permute(0, 2, 1, 3, 4)  # output shape: [B, C = 1, T, H_lat, W_lat]\n+        else:\n+            mask_pixel_values = None\n+        prev_segment_cond_mask = self.get_i2v_mask(\n+            batch_size,\n+            num_latent_frames,\n+            latent_height,\n+            latent_width,\n+            mask_len=prev_segment_cond_frames if start_frame > 0 else 0,\n+            mask_pixel_values=mask_pixel_values,\n+            dtype=dtype,\n+            device=device,\n+        )\n+\n+        # Prepend cond I2V mask to prev segment cond latents along channel dimension\n+        prev_segment_cond_latents = torch.cat([prev_segment_cond_mask, prev_segment_cond_latents], dim=1)\n+        return prev_segment_cond_latents\n+\n+    def prepare_pose_latents(\n+        self,\n+        pose_video: torch.Tensor,\n+        batch_size: int = 1,\n+        sample_mode: int = \"argmax\",\n+        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n+        dtype: Optional[torch.dtype] = None,\n+        device: Optional[torch.device] = None,\n+    ) -> torch.Tensor:\n+        # pose_video shape: (B, C, T, H, W)\n+        pose_video = pose_video.to(device=device, dtype=dtype if dtype is not None else self.vae.dtype)\n+        if isinstance(generator, list):\n+            pose_latents = [\n+                retrieve_latents(self.vae.encode(pose_video), generator=g, sample_mode=sample_mode) for g in generator\n+            ]\n+            pose_latents = torch.cat(pose_latents)\n+        else:\n+            pose_latents = retrieve_latents(self.vae.encode(pose_video), generator, sample_mode)\n+        # Standardize latents in preparation for Wan VAE encode\n+        latents_mean = (\n+            torch.tensor(self.vae.config.latents_mean)\n+            .view(1, self.vae.config.z_dim, 1, 1, 1)\n+            .to(pose_latents.device, pose_latents.dtype)\n+        )\n+        latents_recip_std = 1.0 / torch.tensor(self.vae.config.latents_std).view(1, self.vae.config.z_dim, 1, 1, 1).to(\n+            pose_latents.device, pose_latents.dtype\n+        )\n+        pose_latents = (pose_latents - latents_mean) * latents_recip_std\n+        if pose_latents.shape[0] == 1 and batch_size > 1:\n+            pose_latents = pose_latents.expand(batch_size, -1, -1, -1, -1)\n+        return pose_latents\n+\n+    def prepare_latents(\n+        self,\n+        batch_size: int,\n+        num_channels_latents: int = 16,\n+        height: int = 720,\n+        width: int = 1280,\n+        num_frames: int = 77,\n+        dtype: Optional[torch.dtype] = None,\n+        device: Optional[torch.device] = None,\n+        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n+        latents: Optional[torch.Tensor] = None,\n+    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n+        num_latent_frames = (num_frames - 1) // self.vae_scale_factor_temporal + 1\n+        latent_height = height // self.vae_scale_factor_spatial\n+        latent_width = width // self.vae_scale_factor_spatial\n+\n+        shape = (batch_size, num_channels_latents, num_latent_frames + 1, latent_height, latent_width)\n+        if isinstance(generator, list) and len(generator) != batch_size:\n+            raise ValueError(\n+                f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n+                f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n+            )\n+\n+        if latents is None:\n+            latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n+        else:\n+            latents = latents.to(device=device, dtype=dtype)\n+\n+        return latents\n+\n+    def pad_video_frames(self, frames: List[Any], num_target_frames: int) -> List[Any]:\n+        \"\"\"\n+        Pads an array-like video `frames` to `num_target_frames` using a \"reflect\"-like strategy. The frame dimension\n+        is assumed to be the first dimension. In the 1D case, we can visualize this strategy as follows:\n+\n+        pad_video_frames([1, 2, 3, 4, 5], 10) -> [1, 2, 3, 4, 5, 4, 3, 2, 1, 2]\n+        \"\"\"\n+        idx = 0\n+        flip = False\n+        target_frames = []\n+        while len(target_frames) < num_target_frames:\n+            target_frames.append(deepcopy(frames[idx]))\n+            if flip:\n+                idx -= 1\n+            else:\n+                idx += 1\n+            if idx == 0 or idx == len(frames) - 1:\n+                flip = not flip\n+\n+        return target_frames\n+\n+    @property\n+    def guidance_scale(self):\n+        return self._guidance_scale\n+\n+    @property\n+    def do_classifier_free_guidance(self):\n+        return self._guidance_scale > 1\n+\n+    @property\n+    def num_timesteps(self):\n+        return self._num_timesteps\n+\n+    @property\n+    def current_timestep(self):\n+        return self._current_timestep\n+\n+    @property\n+    def interrupt(self):\n+        return self._interrupt\n+\n+    @property\n+    def attention_kwargs(self):\n+        return self._attention_kwargs\n+\n+    @torch.no_grad()\n+    @replace_example_docstring(EXAMPLE_DOC_STRING)\n+    def __call__(\n+        self,\n+        image: PipelineImageInput,\n+        pose_video: List[PIL.Image.Image],\n+        face_video: List[PIL.Image.Image],\n+        background_video: Optional[List[PIL.Image.Image]] = None,\n+        mask_video: Optional[List[PIL.Image.Image]] = None,\n+        prompt: Union[str, List[str]] = None,\n+        negative_prompt: Union[str, List[str]] = None,\n+        height: int = 720,\n+        width: int = 1280,\n+        segment_frame_length: int = 77,\n+        num_inference_steps: int = 20,\n+        mode: str = \"animate\",\n+        prev_segment_conditioning_frames: int = 1,\n+        motion_encode_batch_size: Optional[int] = None,\n+        guidance_scale: float = 1.0,\n+        num_videos_per_prompt: Optional[int] = 1,\n+        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n+        latents: Optional[torch.Tensor] = None,\n+        prompt_embeds: Optional[torch.Tensor] = None,\n+        negative_prompt_embeds: Optional[torch.Tensor] = None,\n+        image_embeds: Optional[torch.Tensor] = None,\n+        output_type: Optional[str] = \"np\",\n+        return_dict: bool = True,\n+        attention_kwargs: Optional[Dict[str, Any]] = None,\n+        callback_on_step_end: Optional[\n+            Union[Callable[[int, int, Dict], None], PipelineCallback, MultiPipelineCallbacks]\n+        ] = None,\n+        callback_on_step_end_tensor_inputs: List[str] = [\"latents\"],\n+        max_sequence_length: int = 512,\n+    ):\n+        r\"\"\"\n+        The call function to the pipeline for generation.\n+\n+        Args:\n+            image (`PipelineImageInput`):\n+                The input character image to condition the generation on. Must be an image, a list of images or a\n+                `torch.Tensor`.\n+            pose_video (`List[PIL.Image.Image]`):\n+                The input pose video to condition the generation on. Must be a list of PIL images.\n+            face_video (`List[PIL.Image.Image]`):\n+                The input face video to condition the generation on. Must be a list of PIL images.\n+            background_video (`List[PIL.Image.Image]`, *optional*):\n+                When mode is `\"replace\"`, the input background video to condition the generation on. Must be a list of\n+                PIL images.\n+            mask_video (`List[PIL.Image.Image]`, *optional*):\n+                When mode is `\"replace\"`, the input mask video to condition the generation on. Must be a list of PIL\n+                images.\n+            prompt (`str` or `List[str]`, *optional*):\n+                The prompt or prompts to guide the image generation. If not defined, one has to pass `prompt_embeds`.\n+                instead.\n+            negative_prompt (`str` or `List[str]`, *optional*):\n+                The prompt or prompts not to guide the image generation. If not defined, one has to pass\n+                `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is\n+                less than `1`).\n+            mode (`str`, defaults to `\"animation\"`):\n+                The mode of the generation. Choose between `\"animate\"` and `\"replace\"`.\n+            prev_segment_conditioning_frames (`int`, defaults to `1`):\n+                The number of frames from the previous video segment to be used for temporal guidance. Recommended to\n+                be 1 or 5. In general, should be 4N + 1, where N is a non-negative integer.\n+            motion_encode_batch_size (`int`, *optional*):\n+                The batch size for batched encoding of the face video via the motion encoder. This allows trading off\n+                inference speed for lower memory usage by setting a smaller batch size. Will default to\n+                `self.transformer.config.motion_encoder_batch_size` if not set.\n+            height (`int`, defaults to `720`):\n+                The height of the generated video.\n+            width (`int`, defaults to `1280`):\n+                The width of the generated video.\n+            segment_frame_length (`int`, defaults to `77`):\n+                The number of frames in each generated video segment. The total frames of video generated will be equal\n+                to the number of frames in `pose_video`; we will generate the video in segments until we have hit this\n+                length. In general, should be 4N + 1, where N is a non-negative integer.\n+            num_inference_steps (`int`, defaults to `20`):\n+                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n+                expense of slower inference.\n+            guidance_scale (`float`, defaults to `1.0`):\n+                Guidance scale as defined in [Classifier-Free Diffusion\n+                Guidance](https://huggingface.co/papers/2207.12598). `guidance_scale` is defined as `w` of equation 2.\n+                of [Imagen Paper](https://huggingface.co/papers/2205.11487). Guidance scale is enabled by setting\n+                `guidance_scale > 1`. Higher guidance scale encourages to generate images that are closely linked to\n+                the text `prompt`, usually at the expense of lower image quality. By default, CFG is not used in Wan\n+                Animate inference.\n+            num_videos_per_prompt (`int`, *optional*, defaults to 1):\n+                The number of images to generate per prompt.\n+            generator (`torch.Generator` or `List[torch.Generator]`, *optional*):\n+                A [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html) to make\n+                generation deterministic.\n+            latents (`torch.Tensor`, *optional*):\n+                Pre-generated noisy latents sampled from a Gaussian distribution, to be used as inputs for image\n+                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents\n+                tensor is generated by sampling using the supplied random `generator`.\n+            prompt_embeds (`torch.Tensor`, *optional*):\n+                Pre-generated text embeddings. Can be used to easily tweak text inputs (prompt weighting). If not\n+                provided, text embeddings are generated from the `prompt` input argument.\n+            negative_prompt_embeds (`torch.Tensor`, *optional*):\n+                Pre-generated text embeddings. Can be used to easily tweak text inputs (prompt weighting). If not\n+                provided, text embeddings are generated from the `negative_prompt` input argument.\n+            image_embeds (`torch.Tensor`, *optional*):\n+                Pre-generated image embeddings. Can be used to easily tweak image inputs (weighting). If not provided,\n+                image embeddings are generated from the `image` input argument.\n+            output_type (`str`, *optional*, defaults to `\"np\"`):\n+                The output format of the generated image. Choose between `PIL.Image` or `np.array`.\n+            return_dict (`bool`, *optional*, defaults to `True`):\n+                Whether or not to return a [`WanPipelineOutput`] instead of a plain tuple.\n+            attention_kwargs (`dict`, *optional*):\n+                A kwargs dictionary that if specified is passed along to the `AttentionProcessor` as defined under\n+                `self.processor` in\n+                [diffusers.models.attention_processor](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).\n+            callback_on_step_end (`Callable`, `PipelineCallback`, `MultiPipelineCallbacks`, *optional*):\n+                A function or a subclass of `PipelineCallback` or `MultiPipelineCallbacks` that is called at the end of\n+                each denoising step during the inference. with the following arguments: `callback_on_step_end(self:\n+                DiffusionPipeline, step: int, timestep: int, callback_kwargs: Dict)`. `callback_kwargs` will include a\n+                list of all tensors as specified by `callback_on_step_end_tensor_inputs`.\n+            callback_on_step_end_tensor_inputs (`List`, *optional*):\n+                The list of tensor inputs for the `callback_on_step_end` function. The tensors specified in the list\n+                will be passed as `callback_kwargs` argument. You will only be able to include variables listed in the\n+                `._callback_tensor_inputs` attribute of your pipeline class.\n+            max_sequence_length (`int`, defaults to `512`):\n+                The maximum sequence length of the text encoder. If the prompt is longer than this, it will be\n+                truncated. If the prompt is shorter, it will be padded to this length.\n+\n+        Examples:\n+\n+        Returns:\n+            [`~WanPipelineOutput`] or `tuple`:\n+                If `return_dict` is `True`, [`WanPipelineOutput`] is returned, otherwise a `tuple` is returned where\n+                the first element is a list with the generated images and the second element is a list of `bool`s\n+                indicating whether the corresponding generated image contains \"not-safe-for-work\" (nsfw) content.\n+        \"\"\"\n+\n+        if isinstance(callback_on_step_end, (PipelineCallback, MultiPipelineCallbacks)):\n+            callback_on_step_end_tensor_inputs = callback_on_step_end.tensor_inputs\n+\n+        # 1. Check inputs. Raise error if not correct\n+        self.check_inputs(\n+            prompt,\n+            negative_prompt,\n+            image,\n+            pose_video,\n+            face_video,\n+            background_video,\n+            mask_video,\n+            height,\n+            width,\n+            prompt_embeds,\n+            negative_prompt_embeds,\n+            image_embeds,\n+            callback_on_step_end_tensor_inputs,\n+            mode,\n+            prev_segment_conditioning_frames,\n+        )\n+\n+        if segment_frame_length % self.vae_scale_factor_temporal != 1:\n+            logger.warning(\n+                f\"`segment_frame_length - 1` has to be divisible by {self.vae_scale_factor_temporal}. Rounding to the\"\n+                f\" nearest number.\"\n+            )\n+            segment_frame_length = (\n+                segment_frame_length // self.vae_scale_factor_temporal * self.vae_scale_factor_temporal + 1\n+            )\n+        segment_frame_length = max(segment_frame_length, 1)\n+\n+        self._guidance_scale = guidance_scale\n+        self._attention_kwargs = attention_kwargs\n+        self._current_timestep = None\n+        self._interrupt = False\n+\n+        device = self._execution_device\n+\n+        # 2. Define call parameters\n+        if prompt is not None and isinstance(prompt, str):\n+            batch_size = 1\n+        elif prompt is not None and isinstance(prompt, list):\n+            batch_size = len(prompt)\n+        else:\n+            batch_size = prompt_embeds.shape[0]\n+\n+        # As we generate in segments of `segment_frame_length`, set the target frame length to be the least multiple\n+        # of the effective segment length greater than or equal to the length of `pose_video`.\n+        cond_video_frames = len(pose_video)\n+        effective_segment_length = segment_frame_length - prev_segment_conditioning_frames\n+        last_segment_frames = (cond_video_frames - prev_segment_conditioning_frames) % effective_segment_length\n+        if last_segment_frames == 0:\n+            num_padding_frames = 0\n+        else:\n+            num_padding_frames = effective_segment_length - last_segment_frames\n+        num_target_frames = cond_video_frames + num_padding_frames\n+        num_segments = num_target_frames // effective_segment_length\n+\n+        # 3. Encode input prompt\n+        prompt_embeds, negative_prompt_embeds = self.encode_prompt(\n+            prompt=prompt,\n+            negative_prompt=negative_prompt,\n+            do_classifier_free_guidance=self.do_classifier_free_guidance,\n+            num_videos_per_prompt=num_videos_per_prompt,\n+            prompt_embeds=prompt_embeds,\n+            negative_prompt_embeds=negative_prompt_embeds,\n+            max_sequence_length=max_sequence_length,\n+            device=device,\n+        )\n+\n+        transformer_dtype = self.transformer.dtype\n+        prompt_embeds = prompt_embeds.to(transformer_dtype)\n+        if negative_prompt_embeds is not None:\n+            negative_prompt_embeds = negative_prompt_embeds.to(transformer_dtype)\n+\n+        # 4. Preprocess and encode the reference (character) image\n+        image_height, image_width = self.video_processor.get_default_height_width(image)\n+        if image_height != height or image_width != width:\n+            logger.warning(f\"Reshaping reference image from ({image_width}, {image_height}) to ({width}, {height})\")\n+        image_pixels = self.vae_image_processor.preprocess(image, height=height, width=width, resize_mode=\"fill\").to(\n+            device, dtype=torch.float32\n+        )\n+\n+        # Get CLIP features from the reference image\n+        if image_embeds is None:\n+            image_embeds = self.encode_image(image, device)\n+        image_embeds = image_embeds.repeat(batch_size * num_videos_per_prompt, 1, 1)\n+        image_embeds = image_embeds.to(transformer_dtype)\n+\n+        # 5. Encode conditioning videos (pose, face)\n+        pose_video = self.pad_video_frames(pose_video, num_target_frames)\n+        face_video = self.pad_video_frames(face_video, num_target_frames)\n+\n+        # TODO: also support np.ndarray input (e.g. from decord like the original implementation?)\n+        pose_video_width, pose_video_height = pose_video[0].size\n+        if pose_video_height != height or pose_video_width != width:\n+            logger.warning(\n+                f\"Reshaping pose video from ({pose_video_width}, {pose_video_height}) to ({width}, {height})\"\n+            )\n+        pose_video = self.video_processor.preprocess_video(pose_video, height=height, width=width).to(\n+            device, dtype=torch.float32\n+        )\n+\n+        face_video_width, face_video_height = face_video[0].size\n+        expected_face_size = self.transformer.config.motion_encoder_size\n+        if face_video_width != expected_face_size or face_video_height != expected_face_size:\n+            logger.warning(\n+                f\"Reshaping face video from ({face_video_width}, {face_video_height}) to ({expected_face_size},\"\n+                f\" {expected_face_size})\"\n+            )\n+        face_video = self.video_processor.preprocess_video(\n+            face_video, height=expected_face_size, width=expected_face_size\n+        ).to(device, dtype=torch.float32)\n+\n+        if mode == \"replace\":\n+            background_video = self.pad_video_frames(background_video, num_target_frames)\n+            mask_video = self.pad_video_frames(mask_video, num_target_frames)\n+\n+            background_video = self.video_processor.preprocess_video(background_video, height=height, width=width).to(\n+                device, dtype=torch.float32\n+            )\n+            mask_video = self.video_processor_for_mask.preprocess_video(mask_video, height=height, width=width).to(\n+                device, dtype=torch.float32\n+            )\n+\n+        # 6. Prepare timesteps\n+        self.scheduler.set_timesteps(num_inference_steps, device=device)\n+        timesteps = self.scheduler.timesteps\n+\n+        # 7. Prepare latent variables which stay constant for all inference segments\n+        num_channels_latents = self.vae.config.z_dim\n+\n+        # Get VAE-encoded latents of the reference (character) image\n+        reference_image_latents = self.prepare_reference_image_latents(\n+            image_pixels, batch_size * num_videos_per_prompt, generator=generator, device=device\n+        )\n+\n+        # 8. Loop over video inference segments\n+        start = 0\n+        end = segment_frame_length  # Data space frames, not latent frames\n+        all_out_frames = []\n+        out_frames = None\n+\n+        for _ in range(num_segments):\n+            assert start + prev_segment_conditioning_frames < cond_video_frames\n+\n+            # Sample noisy latents from prior for the current inference segment\n+            latents = self.prepare_latents(\n+                batch_size * num_videos_per_prompt,\n+                num_channels_latents=num_channels_latents,\n+                height=height,\n+                width=width,\n+                num_frames=segment_frame_length,\n+                dtype=torch.float32,\n+                device=device,\n+                generator=generator,\n+                latents=latents if start == 0 else None,  # Only use pre-calculated latents for first segment\n+            )\n+\n+            pose_video_segment = pose_video[:, :, start:end]\n+            face_video_segment = face_video[:, :, start:end]\n+\n+            face_video_segment = face_video_segment.expand(batch_size * num_videos_per_prompt, -1, -1, -1, -1)\n+            face_video_segment = face_video_segment.to(dtype=transformer_dtype)\n+\n+            if start > 0:\n+                prev_segment_cond_video = out_frames[:, :, -prev_segment_conditioning_frames:].clone().detach()\n+            else:\n+                prev_segment_cond_video = None\n+\n+            if mode == \"replace\":\n+                background_video_segment = background_video[:, :, start:end]\n+                mask_video_segment = mask_video[:, :, start:end]\n+\n+                background_video_segment = background_video_segment.expand(\n+                    batch_size * num_videos_per_prompt, -1, -1, -1, -1\n+                )\n+                mask_video_segment = mask_video_segment.expand(batch_size * num_videos_per_prompt, -1, -1, -1, -1)\n+            else:\n+                background_video_segment = None\n+                mask_video_segment = None\n+\n+            pose_latents = self.prepare_pose_latents(\n+                pose_video_segment, batch_size * num_videos_per_prompt, generator=generator, device=device\n+            )\n+            pose_latents = pose_latents.to(dtype=transformer_dtype)\n+\n+            prev_segment_cond_latents = self.prepare_prev_segment_cond_latents(\n+                prev_segment_cond_video,\n+                background_video=background_video_segment,\n+                mask_video=mask_video_segment,\n+                batch_size=batch_size * num_videos_per_prompt,\n+                segment_frame_length=segment_frame_length,\n+                start_frame=start,\n+                height=height,\n+                width=width,\n+                prev_segment_cond_frames=prev_segment_conditioning_frames,\n+                task=mode,\n+                generator=generator,\n+                device=device,\n+            )\n+\n+            # Concatenate the reference latents in the frame dimension\n+            reference_latents = torch.cat([reference_image_latents, prev_segment_cond_latents], dim=2)\n+\n+            # 8.1 Denoising loop\n+            num_warmup_steps = len(timesteps) - num_inference_steps * self.scheduler.order\n+            self._num_timesteps = len(timesteps)\n+\n+            with self.progress_bar(total=num_inference_steps) as progress_bar:\n+                for i, t in enumerate(timesteps):\n+                    if self.interrupt:\n+                        continue\n+\n+                    self._current_timestep = t\n+\n+                    # Concatenate the reference image + prev segment conditioning in the channel dim\n+                    latent_model_input = torch.cat([latents, reference_latents], dim=1).to(transformer_dtype)\n+                    timestep = t.expand(latents.shape[0])\n+\n+                    with self.transformer.cache_context(\"cond\"):\n+                        noise_pred = self.transformer(\n+                            hidden_states=latent_model_input,\n+                            timestep=timestep,\n+                            encoder_hidden_states=prompt_embeds,\n+                            encoder_hidden_states_image=image_embeds,\n+                            pose_hidden_states=pose_latents,\n+                            face_pixel_values=face_video_segment,\n+                            motion_encode_batch_size=motion_encode_batch_size,\n+                            attention_kwargs=attention_kwargs,\n+                            return_dict=False,\n+                        )[0]\n+\n+                    if self.do_classifier_free_guidance:\n+                        # Blank out face for unconditional guidance (set all pixels to -1)\n+                        face_pixel_values_uncond = face_video_segment * 0 - 1\n+                        with self.transformer.cache_context(\"uncond\"):\n+                            noise_uncond = self.transformer(\n+                                hidden_states=latent_model_input,\n+                                timestep=timestep,\n+                                encoder_hidden_states=negative_prompt_embeds,\n+                                encoder_hidden_states_image=image_embeds,\n+                                pose_hidden_states=pose_latents,\n+                                face_pixel_values=face_pixel_values_uncond,\n+                                motion_encode_batch_size=motion_encode_batch_size,\n+                                attention_kwargs=attention_kwargs,\n+                                return_dict=False,\n+                            )[0]\n+                            noise_pred = noise_uncond + guidance_scale * (noise_pred - noise_uncond)\n+\n+                    # compute the previous noisy sample x_t -> x_t-1\n+                    latents = self.scheduler.step(noise_pred, t, latents, return_dict=False)[0]\n+\n+                    if callback_on_step_end is not None:\n+                        callback_kwargs = {}\n+                        for k in callback_on_step_end_tensor_inputs:\n+                            callback_kwargs[k] = locals()[k]\n+                        callback_outputs = callback_on_step_end(self, i, t, callback_kwargs)\n+\n+                        latents = callback_outputs.pop(\"latents\", latents)\n+                        prompt_embeds = callback_outputs.pop(\"prompt_embeds\", prompt_embeds)\n+                        negative_prompt_embeds = callback_outputs.pop(\"negative_prompt_embeds\", negative_prompt_embeds)\n+\n+                    # call the callback, if provided\n+                    if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n+                        progress_bar.update()\n+\n+                    if XLA_AVAILABLE:\n+                        xm.mark_step()\n+\n+            latents = latents.to(self.vae.dtype)\n+            # Destandardize latents in preparation for Wan VAE decoding\n+            latents_mean = (\n+                torch.tensor(self.vae.config.latents_mean)\n+                .view(1, self.vae.config.z_dim, 1, 1, 1)\n+                .to(latents.device, latents.dtype)\n+            )\n+            latents_recip_std = 1.0 / torch.tensor(self.vae.config.latents_std).view(\n+                1, self.vae.config.z_dim, 1, 1, 1\n+            ).to(latents.device, latents.dtype)\n+            latents = latents / latents_recip_std + latents_mean\n+            # Skip the first latent frame (used for conditioning)\n+            out_frames = self.vae.decode(latents[:, :, 1:], return_dict=False)[0]\n+\n+            if start > 0:\n+                out_frames = out_frames[:, :, prev_segment_conditioning_frames:]\n+            all_out_frames.append(out_frames)\n+\n+            start += effective_segment_length\n+            end += effective_segment_length\n+\n+            # Reset scheduler timesteps / state for next denoising loop\n+            self.scheduler.set_timesteps(num_inference_steps, device=device)\n+            timesteps = self.scheduler.timesteps\n+\n+        self._current_timestep = None\n+        assert start + prev_segment_conditioning_frames >= cond_video_frames\n+\n+        if not output_type == \"latent\":\n+            video = torch.cat(all_out_frames, dim=2)[:, :, :cond_video_frames]\n+            video = self.video_processor.postprocess_video(video, output_type=output_type)\n+        else:\n+            video = latents\n+\n+        # Offload all models\n+        self.maybe_free_model_hooks()\n+\n+        if not return_dict:\n+            return (video,)\n+\n+        return WanPipelineOutput(frames=video)"
        },
        {
          "filename": "src/diffusers/utils/dummy_pt_objects.py",
          "status": "modified",
          "additions": 15,
          "deletions": 0,
          "changes": 15,
          "patch": "@@ -1623,6 +1623,21 @@ def from_pretrained(cls, *args, **kwargs):\n         requires_backends(cls, [\"torch\"])\n \n \n+class WanAnimateTransformer3DModel(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+    @classmethod\n+    def from_config(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\"])\n+\n+    @classmethod\n+    def from_pretrained(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\"])\n+\n+\n class WanTransformer3DModel(metaclass=DummyObject):\n     _backends = [\"torch\"]\n "
        },
        {
          "filename": "src/diffusers/utils/dummy_torch_and_transformers_objects.py",
          "status": "modified",
          "additions": 15,
          "deletions": 0,
          "changes": 15,
          "patch": "@@ -3512,6 +3512,21 @@ def from_pretrained(cls, *args, **kwargs):\n         requires_backends(cls, [\"torch\", \"transformers\"])\n \n \n+class WanAnimatePipeline(metaclass=DummyObject):\n+    _backends = [\"torch\", \"transformers\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_config(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_pretrained(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+\n class WanImageToVideoPipeline(metaclass=DummyObject):\n     _backends = [\"torch\", \"transformers\"]\n "
        },
        {
          "filename": "tests/models/transformers/test_models_transformer_wan_animate.py",
          "status": "added",
          "additions": 126,
          "deletions": 0,
          "changes": 126,
          "patch": "@@ -0,0 +1,126 @@\n+# Copyright 2025 HuggingFace Inc.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import unittest\n+\n+import torch\n+\n+from diffusers import WanAnimateTransformer3DModel\n+\n+from ...testing_utils import (\n+    enable_full_determinism,\n+    torch_device,\n+)\n+from ..test_modeling_common import ModelTesterMixin, TorchCompileTesterMixin\n+\n+\n+enable_full_determinism()\n+\n+\n+class WanAnimateTransformer3DTests(ModelTesterMixin, unittest.TestCase):\n+    model_class = WanAnimateTransformer3DModel\n+    main_input_name = \"hidden_states\"\n+    uses_custom_attn_processor = True\n+\n+    @property\n+    def dummy_input(self):\n+        batch_size = 1\n+        num_channels = 4\n+        num_frames = 20  # To make the shapes work out; for complicated reasons we want 21 to divide num_frames + 1\n+        height = 16\n+        width = 16\n+        text_encoder_embedding_dim = 16\n+        sequence_length = 12\n+\n+        clip_seq_len = 12\n+        clip_dim = 16\n+\n+        inference_segment_length = 77  # The inference segment length in the full Wan2.2-Animate-14B model\n+        face_height = 16  # Should be square and match `motion_encoder_size` below\n+        face_width = 16\n+\n+        hidden_states = torch.randn((batch_size, 2 * num_channels + 4, num_frames + 1, height, width)).to(torch_device)\n+        timestep = torch.randint(0, 1000, size=(batch_size,)).to(torch_device)\n+        encoder_hidden_states = torch.randn((batch_size, sequence_length, text_encoder_embedding_dim)).to(torch_device)\n+        clip_ref_features = torch.randn((batch_size, clip_seq_len, clip_dim)).to(torch_device)\n+        pose_latents = torch.randn((batch_size, num_channels, num_frames, height, width)).to(torch_device)\n+        face_pixel_values = torch.randn((batch_size, 3, inference_segment_length, face_height, face_width)).to(\n+            torch_device\n+        )\n+\n+        return {\n+            \"hidden_states\": hidden_states,\n+            \"timestep\": timestep,\n+            \"encoder_hidden_states\": encoder_hidden_states,\n+            \"encoder_hidden_states_image\": clip_ref_features,\n+            \"pose_hidden_states\": pose_latents,\n+            \"face_pixel_values\": face_pixel_values,\n+        }\n+\n+    @property\n+    def input_shape(self):\n+        return (12, 1, 16, 16)\n+\n+    @property\n+    def output_shape(self):\n+        return (4, 1, 16, 16)\n+\n+    def prepare_init_args_and_inputs_for_common(self):\n+        # Use custom channel sizes since the default Wan Animate channel sizes will cause the motion encoder to\n+        # contain the vast majority of the parameters in the test model\n+        channel_sizes = {\"4\": 16, \"8\": 16, \"16\": 16}\n+\n+        init_dict = {\n+            \"patch_size\": (1, 2, 2),\n+            \"num_attention_heads\": 2,\n+            \"attention_head_dim\": 12,\n+            \"in_channels\": 12,  # 2 * C + 4 = 2 * 4 + 4 = 12\n+            \"latent_channels\": 4,\n+            \"out_channels\": 4,\n+            \"text_dim\": 16,\n+            \"freq_dim\": 256,\n+            \"ffn_dim\": 32,\n+            \"num_layers\": 2,\n+            \"cross_attn_norm\": True,\n+            \"qk_norm\": \"rms_norm_across_heads\",\n+            \"image_dim\": 16,\n+            \"rope_max_seq_len\": 32,\n+            \"motion_encoder_channel_sizes\": channel_sizes,  # Start of Wan Animate-specific config\n+            \"motion_encoder_size\": 16,  # Ensures that there will be 2 motion encoder resblocks\n+            \"motion_style_dim\": 8,\n+            \"motion_dim\": 4,\n+            \"motion_encoder_dim\": 16,\n+            \"face_encoder_hidden_dim\": 16,\n+            \"face_encoder_num_heads\": 2,\n+            \"inject_face_latents_blocks\": 2,\n+        }\n+        inputs_dict = self.dummy_input\n+        return init_dict, inputs_dict\n+\n+    def test_gradient_checkpointing_is_applied(self):\n+        expected_set = {\"WanAnimateTransformer3DModel\"}\n+        super().test_gradient_checkpointing_is_applied(expected_set=expected_set)\n+\n+    # Override test_output because the transformer output is expected to have less channels than the main transformer\n+    # input.\n+    def test_output(self):\n+        expected_output_shape = (1, 4, 21, 16, 16)\n+        super().test_output(expected_output_shape=expected_output_shape)\n+\n+\n+class WanAnimateTransformerCompileTests(TorchCompileTesterMixin, unittest.TestCase):\n+    model_class = WanAnimateTransformer3DModel\n+\n+    def prepare_init_args_and_inputs_for_common(self):\n+        return WanAnimateTransformer3DTests().prepare_init_args_and_inputs_for_common()"
        },
        {
          "filename": "tests/pipelines/wan/test_wan_animate.py",
          "status": "added",
          "additions": 239,
          "deletions": 0,
          "changes": 239,
          "patch": "@@ -0,0 +1,239 @@\n+# Copyright 2025 The HuggingFace Team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import gc\n+import unittest\n+\n+import numpy as np\n+import torch\n+from PIL import Image\n+from transformers import (\n+    AutoTokenizer,\n+    CLIPImageProcessor,\n+    CLIPVisionConfig,\n+    CLIPVisionModelWithProjection,\n+    T5EncoderModel,\n+)\n+\n+from diffusers import (\n+    AutoencoderKLWan,\n+    FlowMatchEulerDiscreteScheduler,\n+    WanAnimatePipeline,\n+    WanAnimateTransformer3DModel,\n+)\n+\n+from ...testing_utils import (\n+    backend_empty_cache,\n+    enable_full_determinism,\n+    require_torch_accelerator,\n+    slow,\n+    torch_device,\n+)\n+from ..pipeline_params import TEXT_TO_IMAGE_BATCH_PARAMS, TEXT_TO_IMAGE_IMAGE_PARAMS, TEXT_TO_IMAGE_PARAMS\n+from ..test_pipelines_common import PipelineTesterMixin\n+\n+\n+enable_full_determinism()\n+\n+\n+class WanAnimatePipelineFastTests(PipelineTesterMixin, unittest.TestCase):\n+    pipeline_class = WanAnimatePipeline\n+    params = TEXT_TO_IMAGE_PARAMS - {\"cross_attention_kwargs\"}\n+    batch_params = TEXT_TO_IMAGE_BATCH_PARAMS\n+    image_params = TEXT_TO_IMAGE_IMAGE_PARAMS\n+    image_latents_params = TEXT_TO_IMAGE_IMAGE_PARAMS\n+    required_optional_params = frozenset(\n+        [\n+            \"num_inference_steps\",\n+            \"generator\",\n+            \"latents\",\n+            \"return_dict\",\n+            \"callback_on_step_end\",\n+            \"callback_on_step_end_tensor_inputs\",\n+        ]\n+    )\n+    test_xformers_attention = False\n+    supports_dduf = False\n+\n+    def get_dummy_components(self):\n+        torch.manual_seed(0)\n+        vae = AutoencoderKLWan(\n+            base_dim=3,\n+            z_dim=16,\n+            dim_mult=[1, 1, 1, 1],\n+            num_res_blocks=1,\n+            temperal_downsample=[False, True, True],\n+        )\n+\n+        torch.manual_seed(0)\n+        scheduler = FlowMatchEulerDiscreteScheduler(shift=7.0)\n+        text_encoder = T5EncoderModel.from_pretrained(\"hf-internal-testing/tiny-random-t5\")\n+        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-t5\")\n+\n+        torch.manual_seed(0)\n+        channel_sizes = {\"4\": 16, \"8\": 16, \"16\": 16}\n+        transformer = WanAnimateTransformer3DModel(\n+            patch_size=(1, 2, 2),\n+            num_attention_heads=2,\n+            attention_head_dim=12,\n+            in_channels=36,\n+            latent_channels=16,\n+            out_channels=16,\n+            text_dim=32,\n+            freq_dim=256,\n+            ffn_dim=32,\n+            num_layers=2,\n+            cross_attn_norm=True,\n+            qk_norm=\"rms_norm_across_heads\",\n+            image_dim=4,\n+            rope_max_seq_len=32,\n+            motion_encoder_channel_sizes=channel_sizes,\n+            motion_encoder_size=16,\n+            motion_style_dim=8,\n+            motion_dim=4,\n+            motion_encoder_dim=16,\n+            face_encoder_hidden_dim=16,\n+            face_encoder_num_heads=2,\n+            inject_face_latents_blocks=2,\n+        )\n+\n+        torch.manual_seed(0)\n+        image_encoder_config = CLIPVisionConfig(\n+            hidden_size=4,\n+            projection_dim=4,\n+            num_hidden_layers=2,\n+            num_attention_heads=2,\n+            image_size=4,\n+            intermediate_size=16,\n+            patch_size=1,\n+        )\n+        image_encoder = CLIPVisionModelWithProjection(image_encoder_config)\n+\n+        torch.manual_seed(0)\n+        image_processor = CLIPImageProcessor(crop_size=4, size=4)\n+\n+        components = {\n+            \"transformer\": transformer,\n+            \"vae\": vae,\n+            \"scheduler\": scheduler,\n+            \"text_encoder\": text_encoder,\n+            \"tokenizer\": tokenizer,\n+            \"image_encoder\": image_encoder,\n+            \"image_processor\": image_processor,\n+        }\n+        return components\n+\n+    def get_dummy_inputs(self, device, seed=0):\n+        if str(device).startswith(\"mps\"):\n+            generator = torch.manual_seed(seed)\n+        else:\n+            generator = torch.Generator(device=device).manual_seed(seed)\n+\n+        num_frames = 17\n+        height = 16\n+        width = 16\n+        face_height = 16\n+        face_width = 16\n+\n+        image = Image.new(\"RGB\", (height, width))\n+        pose_video = [Image.new(\"RGB\", (height, width))] * num_frames\n+        face_video = [Image.new(\"RGB\", (face_height, face_width))] * num_frames\n+\n+        inputs = {\n+            \"image\": image,\n+            \"pose_video\": pose_video,\n+            \"face_video\": face_video,\n+            \"prompt\": \"dance monkey\",\n+            \"negative_prompt\": \"negative\",\n+            \"height\": height,\n+            \"width\": width,\n+            \"segment_frame_length\": 77,  # TODO: can we set this to num_frames?\n+            \"num_inference_steps\": 2,\n+            \"mode\": \"animate\",\n+            \"prev_segment_conditioning_frames\": 1,\n+            \"generator\": generator,\n+            \"guidance_scale\": 1.0,\n+            \"output_type\": \"pt\",\n+            \"max_sequence_length\": 16,\n+        }\n+        return inputs\n+\n+    def test_inference(self):\n+        \"\"\"Test basic inference in animation mode.\"\"\"\n+        device = \"cpu\"\n+\n+        components = self.get_dummy_components()\n+        pipe = self.pipeline_class(**components)\n+        pipe.to(device)\n+        pipe.set_progress_bar_config(disable=None)\n+\n+        inputs = self.get_dummy_inputs(device)\n+        video = pipe(**inputs).frames[0]\n+        self.assertEqual(video.shape, (17, 3, 16, 16))\n+\n+        expected_video = torch.randn(17, 3, 16, 16)\n+        max_diff = np.abs(video - expected_video).max()\n+        self.assertLessEqual(max_diff, 1e10)\n+\n+    def test_inference_replacement(self):\n+        \"\"\"Test the pipeline in replacement mode with background and mask videos.\"\"\"\n+        device = \"cpu\"\n+\n+        components = self.get_dummy_components()\n+        pipe = self.pipeline_class(**components)\n+        pipe.to(device)\n+        pipe.set_progress_bar_config(disable=None)\n+\n+        inputs = self.get_dummy_inputs(device)\n+        inputs[\"mode\"] = \"replace\"\n+        num_frames = 17\n+        height = 16\n+        width = 16\n+        inputs[\"background_video\"] = [Image.new(\"RGB\", (height, width))] * num_frames\n+        inputs[\"mask_video\"] = [Image.new(\"L\", (height, width))] * num_frames\n+\n+        video = pipe(**inputs).frames[0]\n+        self.assertEqual(video.shape, (17, 3, 16, 16))\n+\n+    @unittest.skip(\"Test not supported\")\n+    def test_attention_slicing_forward_pass(self):\n+        pass\n+\n+    @unittest.skip(\n+        \"Setting the Wan Animate latents to zero at the last denoising step does not guarantee that the output will be\"\n+        \" zero. I believe this is because the latents are further processed in the outer loop where we loop over\"\n+        \" inference segments.\"\n+    )\n+    def test_callback_inputs(self):\n+        pass\n+\n+\n+@slow\n+@require_torch_accelerator\n+class WanAnimatePipelineIntegrationTests(unittest.TestCase):\n+    prompt = \"A painting of a squirrel eating a burger.\"\n+\n+    def setUp(self):\n+        super().setUp()\n+        gc.collect()\n+        backend_empty_cache(torch_device)\n+\n+    def tearDown(self):\n+        super().tearDown()\n+        gc.collect()\n+        backend_empty_cache(torch_device)\n+\n+    @unittest.skip(\"TODO: test needs to be implemented\")\n+    def test_wan_animate(self):\n+        pass"
        },
        {
          "filename": "tests/quantization/gguf/test_gguf.py",
          "status": "modified",
          "additions": 28,
          "deletions": 0,
          "changes": 28,
          "patch": "@@ -16,6 +16,7 @@\n     HiDreamImageTransformer2DModel,\n     SD3Transformer2DModel,\n     StableDiffusion3Pipeline,\n+    WanAnimateTransformer3DModel,\n     WanTransformer3DModel,\n     WanVACETransformer3DModel,\n )\n@@ -721,6 +722,33 @@ def get_dummy_inputs(self):\n         }\n \n \n+class WanAnimateGGUFSingleFileTests(GGUFSingleFileTesterMixin, unittest.TestCase):\n+    ckpt_path = \"https://huggingface.co/QuantStack/Wan2.2-Animate-14B-GGUF/blob/main/Wan2.2-Animate-14B-Q3_K_S.gguf\"\n+    torch_dtype = torch.bfloat16\n+    model_cls = WanAnimateTransformer3DModel\n+    expected_memory_use_in_gb = 9\n+\n+    def get_dummy_inputs(self):\n+        return {\n+            \"hidden_states\": torch.randn((1, 16, 2, 64, 64), generator=torch.Generator(\"cpu\").manual_seed(0)).to(\n+                torch_device, self.torch_dtype\n+            ),\n+            \"encoder_hidden_states\": torch.randn(\n+                (1, 512, 4096),\n+                generator=torch.Generator(\"cpu\").manual_seed(0),\n+            ).to(torch_device, self.torch_dtype),\n+            \"control_hidden_states\": torch.randn(\n+                (1, 96, 2, 64, 64),\n+                generator=torch.Generator(\"cpu\").manual_seed(0),\n+            ).to(torch_device, self.torch_dtype),\n+            \"control_hidden_states_scale\": torch.randn(\n+                (8,),\n+                generator=torch.Generator(\"cpu\").manual_seed(0),\n+            ).to(torch_device, self.torch_dtype),\n+            \"timestep\": torch.tensor([1]).to(torch_device, self.torch_dtype),\n+        }\n+\n+\n @require_torch_version_greater(\"2.7.1\")\n class GGUFCompileTests(QuantCompileTests, unittest.TestCase):\n     torch_dtype = torch.bfloat16"
        }
      ],
      "num_files": 19,
      "scraped_at": "2025-11-16T21:18:53.396885"
    },
    {
      "pr_number": 12525,
      "title": "Prx",
      "body": "# What does this PR do:\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\n Rename Photon into PRX\r\n\r\n\r\n## Before submitting\r\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/diffusers/blob/main/CONTRIBUTING.md)?\r\n- [x] Did you read our [philosophy doc](https://github.com/huggingface/diffusers/blob/main/PHILOSOPHY.md) (important for complex PRs)?\r\n- [ ] Was this discussed/approved via a GitHub issue or the [forum](https://discuss.huggingface.co/c/discussion-related-to-httpsgithubcomhuggingfacediffusers/63)? Please add a link to it if that's the case.\r\n- [x] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/diffusers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/diffusers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n@sayakpaul, @dg845, @stevhliu, \r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @.\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nCore library:\r\n\r\n- Schedulers: @yiyixuxu\r\n- Pipelines and pipeline callbacks: @yiyixuxu and @asomoza\r\n- Training examples: @sayakpaul\r\n- Docs: @stevhliu and @sayakpaul\r\n- JAX and MPS: @pcuenca\r\n- Audio: @sanchit-gandhi\r\n- General functionalities: @sayakpaul @yiyixuxu @DN6\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @SunMarc\r\n- PEFT: @sayakpaul @BenjaminBossan\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- transformers: [different repo](https://github.com/huggingface/transformers)\r\n- safetensors: [different repo](https://github.com/huggingface/safetensors)\r\n\r\n-->\r\n",
      "html_url": "https://github.com/huggingface/diffusers/pull/12525",
      "created_at": "2025-10-21T21:02:11Z",
      "merged_at": "2025-10-22T00:09:23Z",
      "merge_commit_sha": "dd07b19e27b737d844f62a8107228591f8d7bca8",
      "base_ref": "main",
      "head_sha": "3b80dcd3f482ad2af6e74818ce55c52dd6e0fd3e",
      "user": "DavidBert",
      "files": [
        {
          "filename": "docs/source/en/_toctree.yml",
          "status": "modified",
          "additions": 2,
          "deletions": 2,
          "changes": 4,
          "patch": "@@ -541,12 +541,12 @@\n         title: PAG\n       - local: api/pipelines/paint_by_example\n         title: Paint by Example\n-      - local: api/pipelines/photon\n-        title: Photon\n       - local: api/pipelines/pixart\n         title: PixArt-\u03b1\n       - local: api/pipelines/pixart_sigma\n         title: PixArt-\u03a3\n+      - local: api/pipelines/prx\n+        title: PRX\n       - local: api/pipelines/qwenimage\n         title: QwenImage\n       - local: api/pipelines/sana"
        },
        {
          "filename": "docs/source/en/api/pipelines/photon.md",
          "status": "removed",
          "additions": 0,
          "deletions": 131,
          "changes": 131,
          "patch": "@@ -1,131 +0,0 @@\n-<!-- Copyright 2025 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License. -->\n-\n-# Photon\n-\n-\n-Photon generates high-quality images from text using a simplified MMDIT architecture where text tokens don't update through transformer blocks. It employs flow matching with discrete scheduling for efficient sampling and uses Google's T5Gemma-2B-2B-UL2 model for multi-language text encoding. The ~1.3B parameter transformer delivers fast inference without sacrificing quality. You can choose between Flux VAE (8x compression, 16 latent channels) for balanced quality and speed or DC-AE (32x compression, 32 latent channels) for latent compression and faster processing.\n-\n-## Available models\n-\n-Photon offers multiple variants with different VAE configurations, each optimized for specific resolutions. Base models excel with detailed prompts, capturing complex compositions and subtle details. Fine-tuned models trained on the [Alchemist dataset](https://huggingface.co/datasets/yandex/alchemist) improve aesthetic quality, especially with simpler prompts.\n-\n-\n-| Model | Resolution | Fine-tuned | Distilled | Description | Suggested prompts | Suggested parameters | Recommended dtype |\n-|:-----:|:-----------------:|:----------:|:----------:|:----------:|:----------:|:----------:|:----------:|\n-| [`Photoroom/photon-256-t2i`](https://huggingface.co/Photoroom/photon-256-t2i)| 256 | No | No | Base model pre-trained at 256 with Flux VAE|Works best with detailed prompts in natural language|28 steps, cfg=5.0| `torch.bfloat16` |\n-| [`Photoroom/photon-256-t2i-sft`](https://huggingface.co/Photoroom/photon-256-t2i-sft)| 512 | Yes | No | Fine-tuned on the [Alchemist dataset](https://huggingface.co/datasets/yandex/alchemist) dataset with Flux VAE | Can handle less detailed prompts|28 steps, cfg=5.0| `torch.bfloat16` |\n-| [`Photoroom/photon-512-t2i`](https://huggingface.co/Photoroom/photon-512-t2i)| 512 | No | No | Base model pre-trained at 512 with Flux VAE |Works best with detailed prompts in natural language|28 steps, cfg=5.0| `torch.bfloat16` |\n-| [`Photoroom/photon-512-t2i-sft`](https://huggingface.co/Photoroom/photon-512-t2i-sft)| 512 | Yes | No | Fine-tuned on the [Alchemist dataset](https://huggingface.co/datasets/yandex/alchemist) dataset with Flux VAE | Can handle less detailed prompts in natural language|28 steps, cfg=5.0| `torch.bfloat16` |\n-| [`Photoroom/photon-512-t2i-sft-distilled`](https://huggingface.co/Photoroom/photon-512-t2i-sft-distilled)| 512 | Yes | Yes | 8-step distilled model from [`Photoroom/photon-512-t2i-sft`](https://huggingface.co/Photoroom/photon-512-t2i-sft) | Can handle less detailed prompts in natural language|8 steps, cfg=1.0| `torch.bfloat16` |\n-| [`Photoroom/photon-512-t2i-dc-ae`](https://huggingface.co/Photoroom/photon-512-t2i-dc-ae)| 512 | No | No | Base model pre-trained at 512 with [Deep Compression Autoencoder (DC-AE)](https://hanlab.mit.edu/projects/dc-ae)|Works best with detailed prompts in natural language|28 steps, cfg=5.0| `torch.bfloat16` |\n-| [`Photoroom/photon-512-t2i-dc-ae-sft`](https://huggingface.co/Photoroom/photon-512-t2i-dc-ae-sft)| 512 | Yes | No | Fine-tuned on the [Alchemist dataset](https://huggingface.co/datasets/yandex/alchemist) dataset with [Deep Compression Autoencoder (DC-AE)](https://hanlab.mit.edu/projects/dc-ae) | Can handle less detailed prompts in natural language|28 steps, cfg=5.0| `torch.bfloat16` |\n-| [`Photoroom/photon-512-t2i-dc-ae-sft-distilled`](https://huggingface.co/Photoroom/photon-512-t2i-dc-ae-sft-distilled)| 512 | Yes | Yes | 8-step distilled model from [`Photoroom/photon-512-t2i-dc-ae-sft-distilled`](https://huggingface.co/Photoroom/photon-512-t2i-dc-ae-sft-distilled) | Can handle less detailed prompts in natural language|8 steps, cfg=1.0| `torch.bfloat16` |s\n-\n-Refer to [this](https://huggingface.co/collections/Photoroom/photon-models-68e66254c202ebfab99ad38e) collection for more information.\n-\n-## Loading the pipeline\n-\n-Load the pipeline with [`~DiffusionPipeline.from_pretrained`].\n-\n-```py\n-from diffusers.pipelines.photon import PhotonPipeline\n-\n-# Load pipeline - VAE and text encoder will be loaded from HuggingFace\n-pipe = PhotonPipeline.from_pretrained(\"Photoroom/photon-512-t2i-sft\", torch_dtype=torch.bfloat16)\n-pipe.to(\"cuda\")\n-\n-prompt = \"A front-facing portrait of a lion the golden savanna at sunset.\"\n-image = pipe(prompt, num_inference_steps=28, guidance_scale=5.0).images[0]\n-image.save(\"photon_output.png\")\n-```\n-\n-### Manual Component Loading\n-\n-Load components individually to customize the pipeline for instance to use quantized models.\n-\n-```py\n-import torch\n-from diffusers.pipelines.photon import PhotonPipeline\n-from diffusers.models import AutoencoderKL, AutoencoderDC\n-from diffusers.models.transformers.transformer_photon import PhotonTransformer2DModel\n-from diffusers.schedulers import FlowMatchEulerDiscreteScheduler\n-from transformers import T5GemmaModel, GemmaTokenizerFast\n-from diffusers import BitsAndBytesConfig as DiffusersBitsAndBytesConfig\n-from transformers import BitsAndBytesConfig as BitsAndBytesConfig\n-\n-quant_config = DiffusersBitsAndBytesConfig(load_in_8bit=True)\n-# Load transformer\n-transformer = PhotonTransformer2DModel.from_pretrained(\n-    \"checkpoints/photon-512-t2i-sft\",\n-    subfolder=\"transformer\",\n-    quantization_config=quant_config,\n-    torch_dtype=torch.bfloat16,\n-)\n-\n-# Load scheduler\n-scheduler = FlowMatchEulerDiscreteScheduler.from_pretrained(\n-    \"checkpoints/photon-512-t2i-sft\", subfolder=\"scheduler\"\n-)\n-\n-# Load T5Gemma text encoder\n-t5gemma_model = T5GemmaModel.from_pretrained(\"google/t5gemma-2b-2b-ul2\",\n-                                            quantization_config=quant_config,\n-                                            torch_dtype=torch.bfloat16)\n-text_encoder = t5gemma_model.encoder.to(dtype=torch.bfloat16)\n-tokenizer = GemmaTokenizerFast.from_pretrained(\"google/t5gemma-2b-2b-ul2\")\n-tokenizer.model_max_length = 256\n-\n-# Load VAE - choose either Flux VAE or DC-AE\n-# Flux VAE\n-vae = AutoencoderKL.from_pretrained(\"black-forest-labs/FLUX.1-dev\",\n-                                    subfolder=\"vae\",\n-                                    quantization_config=quant_config,\n-                                    torch_dtype=torch.bfloat16)\n-\n-pipe = PhotonPipeline(\n-    transformer=transformer,\n-    scheduler=scheduler,\n-    text_encoder=text_encoder,\n-    tokenizer=tokenizer,\n-    vae=vae\n-)\n-pipe.to(\"cuda\")\n-```\n-\n-\n-## Memory Optimization\n-\n-For memory-constrained environments:\n-\n-```py\n-import torch\n-from diffusers.pipelines.photon import PhotonPipeline\n-\n-pipe = PhotonPipeline.from_pretrained(\"Photoroom/photon-512-t2i-sft\", torch_dtype=torch.bfloat16)\n-pipe.enable_model_cpu_offload()  # Offload components to CPU when not in use\n-\n-# Or use sequential CPU offload for even lower memory\n-pipe.enable_sequential_cpu_offload()\n-```\n-\n-## PhotonPipeline\n-\n-[[autodoc]] PhotonPipeline\n-  - all\n-  - __call__\n-\n-## PhotonPipelineOutput\n-\n-[[autodoc]] pipelines.photon.pipeline_output.PhotonPipelineOutput"
        },
        {
          "filename": "docs/source/en/api/pipelines/prx.md",
          "status": "added",
          "additions": 131,
          "deletions": 0,
          "changes": 131,
          "patch": "@@ -0,0 +1,131 @@\n+<!-- Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License. -->\n+\n+# PRX\n+\n+\n+PRX generates high-quality images from text using a simplified MMDIT architecture where text tokens don't update through transformer blocks. It employs flow matching with discrete scheduling for efficient sampling and uses Google's T5Gemma-2B-2B-UL2 model for multi-language text encoding. The ~1.3B parameter transformer delivers fast inference without sacrificing quality. You can choose between Flux VAE (8x compression, 16 latent channels) for balanced quality and speed or DC-AE (32x compression, 32 latent channels) for latent compression and faster processing.\n+\n+## Available models\n+\n+PRX offers multiple variants with different VAE configurations, each optimized for specific resolutions. Base models excel with detailed prompts, capturing complex compositions and subtle details. Fine-tuned models trained on the [Alchemist dataset](https://huggingface.co/datasets/yandex/alchemist) improve aesthetic quality, especially with simpler prompts.\n+\n+\n+| Model | Resolution | Fine-tuned | Distilled | Description | Suggested prompts | Suggested parameters | Recommended dtype |\n+|:-----:|:-----------------:|:----------:|:----------:|:----------:|:----------:|:----------:|:----------:|\n+| [`Photoroom/prx-256-t2i`](https://huggingface.co/Photoroom/prx-256-t2i)| 256 | No | No | Base model pre-trained at 256 with Flux VAE|Works best with detailed prompts in natural language|28 steps, cfg=5.0| `torch.bfloat16` |\n+| [`Photoroom/prx-256-t2i-sft`](https://huggingface.co/Photoroom/prx-256-t2i-sft)| 512 | Yes | No | Fine-tuned on the [Alchemist dataset](https://huggingface.co/datasets/yandex/alchemist) dataset with Flux VAE | Can handle less detailed prompts|28 steps, cfg=5.0| `torch.bfloat16` |\n+| [`Photoroom/prx-512-t2i`](https://huggingface.co/Photoroom/prx-512-t2i)| 512 | No | No | Base model pre-trained at 512 with Flux VAE |Works best with detailed prompts in natural language|28 steps, cfg=5.0| `torch.bfloat16` |\n+| [`Photoroom/prx-512-t2i-sft`](https://huggingface.co/Photoroom/prx-512-t2i-sft)| 512 | Yes | No | Fine-tuned on the [Alchemist dataset](https://huggingface.co/datasets/yandex/alchemist) dataset with Flux VAE | Can handle less detailed prompts in natural language|28 steps, cfg=5.0| `torch.bfloat16` |\n+| [`Photoroom/prx-512-t2i-sft-distilled`](https://huggingface.co/Photoroom/prx-512-t2i-sft-distilled)| 512 | Yes | Yes | 8-step distilled model from [`Photoroom/prx-512-t2i-sft`](https://huggingface.co/Photoroom/prx-512-t2i-sft) | Can handle less detailed prompts in natural language|8 steps, cfg=1.0| `torch.bfloat16` |\n+| [`Photoroom/prx-512-t2i-dc-ae`](https://huggingface.co/Photoroom/prx-512-t2i-dc-ae)| 512 | No | No | Base model pre-trained at 512 with [Deep Compression Autoencoder (DC-AE)](https://hanlab.mit.edu/projects/dc-ae)|Works best with detailed prompts in natural language|28 steps, cfg=5.0| `torch.bfloat16` |\n+| [`Photoroom/prx-512-t2i-dc-ae-sft`](https://huggingface.co/Photoroom/prx-512-t2i-dc-ae-sft)| 512 | Yes | No | Fine-tuned on the [Alchemist dataset](https://huggingface.co/datasets/yandex/alchemist) dataset with [Deep Compression Autoencoder (DC-AE)](https://hanlab.mit.edu/projects/dc-ae) | Can handle less detailed prompts in natural language|28 steps, cfg=5.0| `torch.bfloat16` |\n+| [`Photoroom/prx-512-t2i-dc-ae-sft-distilled`](https://huggingface.co/Photoroom/prx-512-t2i-dc-ae-sft-distilled)| 512 | Yes | Yes | 8-step distilled model from [`Photoroom/prx-512-t2i-dc-ae-sft-distilled`](https://huggingface.co/Photoroom/prx-512-t2i-dc-ae-sft-distilled) | Can handle less detailed prompts in natural language|8 steps, cfg=1.0| `torch.bfloat16` |s\n+\n+Refer to [this](https://huggingface.co/collections/Photoroom/prx-models-68e66254c202ebfab99ad38e) collection for more information.\n+\n+## Loading the pipeline\n+\n+Load the pipeline with [`~DiffusionPipeline.from_pretrained`].\n+\n+```py\n+from diffusers.pipelines.prx import PRXPipeline\n+\n+# Load pipeline - VAE and text encoder will be loaded from HuggingFace\n+pipe = PRXPipeline.from_pretrained(\"Photoroom/prx-512-t2i-sft\", torch_dtype=torch.bfloat16)\n+pipe.to(\"cuda\")\n+\n+prompt = \"A front-facing portrait of a lion the golden savanna at sunset.\"\n+image = pipe(prompt, num_inference_steps=28, guidance_scale=5.0).images[0]\n+image.save(\"prx_output.png\")\n+```\n+\n+### Manual Component Loading\n+\n+Load components individually to customize the pipeline for instance to use quantized models.\n+\n+```py\n+import torch\n+from diffusers.pipelines.prx import PRXPipeline\n+from diffusers.models import AutoencoderKL, AutoencoderDC\n+from diffusers.models.transformers.transformer_prx import PRXTransformer2DModel\n+from diffusers.schedulers import FlowMatchEulerDiscreteScheduler\n+from transformers import T5GemmaModel, GemmaTokenizerFast\n+from diffusers import BitsAndBytesConfig as DiffusersBitsAndBytesConfig\n+from transformers import BitsAndBytesConfig as BitsAndBytesConfig\n+\n+quant_config = DiffusersBitsAndBytesConfig(load_in_8bit=True)\n+# Load transformer\n+transformer = PRXTransformer2DModel.from_pretrained(\n+    \"checkpoints/prx-512-t2i-sft\",\n+    subfolder=\"transformer\",\n+    quantization_config=quant_config,\n+    torch_dtype=torch.bfloat16,\n+)\n+\n+# Load scheduler\n+scheduler = FlowMatchEulerDiscreteScheduler.from_pretrained(\n+    \"checkpoints/prx-512-t2i-sft\", subfolder=\"scheduler\"\n+)\n+\n+# Load T5Gemma text encoder\n+t5gemma_model = T5GemmaModel.from_pretrained(\"google/t5gemma-2b-2b-ul2\",\n+                                            quantization_config=quant_config,\n+                                            torch_dtype=torch.bfloat16)\n+text_encoder = t5gemma_model.encoder.to(dtype=torch.bfloat16)\n+tokenizer = GemmaTokenizerFast.from_pretrained(\"google/t5gemma-2b-2b-ul2\")\n+tokenizer.model_max_length = 256\n+\n+# Load VAE - choose either Flux VAE or DC-AE\n+# Flux VAE\n+vae = AutoencoderKL.from_pretrained(\"black-forest-labs/FLUX.1-dev\",\n+                                    subfolder=\"vae\",\n+                                    quantization_config=quant_config,\n+                                    torch_dtype=torch.bfloat16)\n+\n+pipe = PRXPipeline(\n+    transformer=transformer,\n+    scheduler=scheduler,\n+    text_encoder=text_encoder,\n+    tokenizer=tokenizer,\n+    vae=vae\n+)\n+pipe.to(\"cuda\")\n+```\n+\n+\n+## Memory Optimization\n+\n+For memory-constrained environments:\n+\n+```py\n+import torch\n+from diffusers.pipelines.prx import PRXPipeline\n+\n+pipe = PRXPipeline.from_pretrained(\"Photoroom/prx-512-t2i-sft\", torch_dtype=torch.bfloat16)\n+pipe.enable_model_cpu_offload()  # Offload components to CPU when not in use\n+\n+# Or use sequential CPU offload for even lower memory\n+pipe.enable_sequential_cpu_offload()\n+```\n+\n+## PRXPipeline\n+\n+[[autodoc]] PRXPipeline\n+  - all\n+  - __call__\n+\n+## PRXPipelineOutput\n+\n+[[autodoc]] pipelines.prx.pipeline_output.PRXPipelineOutput"
        },
        {
          "filename": "scripts/convert_prx_to_diffusers.py",
          "status": "renamed",
          "additions": 19,
          "deletions": 19,
          "changes": 38,
          "patch": "@@ -1,6 +1,6 @@\n #!/usr/bin/env python3\n \"\"\"\n-Script to convert Photon checkpoint from original codebase to diffusers format.\n+Script to convert PRX checkpoint from original codebase to diffusers format.\n \"\"\"\n \n import argparse\n@@ -13,15 +13,15 @@\n import torch\n from safetensors.torch import save_file\n \n-from diffusers.models.transformers.transformer_photon import PhotonTransformer2DModel\n-from diffusers.pipelines.photon import PhotonPipeline\n+from diffusers.models.transformers.transformer_prx import PRXTransformer2DModel\n+from diffusers.pipelines.prx import PRXPipeline\n \n \n DEFAULT_RESOLUTION = 512\n \n \n @dataclass(frozen=True)\n-class PhotonBase:\n+class PRXBase:\n     context_in_dim: int = 2304\n     hidden_size: int = 1792\n     mlp_ratio: float = 3.5\n@@ -34,22 +34,22 @@ class PhotonBase:\n \n \n @dataclass(frozen=True)\n-class PhotonFlux(PhotonBase):\n+class PRXFlux(PRXBase):\n     in_channels: int = 16\n     patch_size: int = 2\n \n \n @dataclass(frozen=True)\n-class PhotonDCAE(PhotonBase):\n+class PRXDCAE(PRXBase):\n     in_channels: int = 32\n     patch_size: int = 1\n \n \n def build_config(vae_type: str) -> Tuple[dict, int]:\n     if vae_type == \"flux\":\n-        cfg = PhotonFlux()\n+        cfg = PRXFlux()\n     elif vae_type == \"dc-ae\":\n-        cfg = PhotonDCAE()\n+        cfg = PRXDCAE()\n     else:\n         raise ValueError(f\"Unsupported VAE type: {vae_type}. Use 'flux' or 'dc-ae'\")\n \n@@ -64,7 +64,7 @@ def create_parameter_mapping(depth: int) -> dict:\n     # Key mappings for structural changes\n     mapping = {}\n \n-    # Map old structure (layers in PhotonBlock) to new structure (layers in PhotonAttention)\n+    # Map old structure (layers in PRXBlock) to new structure (layers in PRXAttention)\n     for i in range(depth):\n         # QKV projections moved to attention module\n         mapping[f\"blocks.{i}.img_qkv_proj.weight\"] = f\"blocks.{i}.attention.img_qkv_proj.weight\"\n@@ -108,8 +108,8 @@ def convert_checkpoint_parameters(old_state_dict: Dict[str, torch.Tensor], depth\n     return converted_state_dict\n \n \n-def create_transformer_from_checkpoint(checkpoint_path: str, config: dict) -> PhotonTransformer2DModel:\n-    \"\"\"Create and load PhotonTransformer2DModel from old checkpoint.\"\"\"\n+def create_transformer_from_checkpoint(checkpoint_path: str, config: dict) -> PRXTransformer2DModel:\n+    \"\"\"Create and load PRXTransformer2DModel from old checkpoint.\"\"\"\n \n     print(f\"Loading checkpoint from: {checkpoint_path}\")\n \n@@ -137,8 +137,8 @@ def create_transformer_from_checkpoint(checkpoint_path: str, config: dict) -> Ph\n     converted_state_dict = convert_checkpoint_parameters(state_dict, depth=model_depth)\n \n     # Create transformer with config\n-    print(\"Creating PhotonTransformer2DModel...\")\n-    transformer = PhotonTransformer2DModel(**config)\n+    print(\"Creating PRXTransformer2DModel...\")\n+    transformer = PRXTransformer2DModel(**config)\n \n     # Load state dict\n     print(\"Loading converted parameters...\")\n@@ -221,14 +221,14 @@ def create_model_index(vae_type: str, default_image_size: int, output_path: str)\n         vae_class = \"AutoencoderDC\"\n \n     model_index = {\n-        \"_class_name\": \"PhotonPipeline\",\n+        \"_class_name\": \"PRXPipeline\",\n         \"_diffusers_version\": \"0.31.0.dev0\",\n         \"_name_or_path\": os.path.basename(output_path),\n         \"default_sample_size\": default_image_size,\n         \"scheduler\": [\"diffusers\", \"FlowMatchEulerDiscreteScheduler\"],\n-        \"text_encoder\": [\"photon\", \"T5GemmaEncoder\"],\n+        \"text_encoder\": [\"prx\", \"T5GemmaEncoder\"],\n         \"tokenizer\": [\"transformers\", \"GemmaTokenizerFast\"],\n-        \"transformer\": [\"diffusers\", \"PhotonTransformer2DModel\"],\n+        \"transformer\": [\"diffusers\", \"PRXTransformer2DModel\"],\n         \"vae\": [\"diffusers\", vae_class],\n     }\n \n@@ -275,7 +275,7 @@ def main(args):\n \n     # Verify the pipeline can be loaded\n     try:\n-        pipeline = PhotonPipeline.from_pretrained(args.output_path)\n+        pipeline = PRXPipeline.from_pretrained(args.output_path)\n         print(\"Pipeline loaded successfully!\")\n         print(f\"Transformer: {type(pipeline.transformer).__name__}\")\n         print(f\"VAE: {type(pipeline.vae).__name__}\")\n@@ -298,10 +298,10 @@ def main(args):\n \n \n if __name__ == \"__main__\":\n-    parser = argparse.ArgumentParser(description=\"Convert Photon checkpoint to diffusers format\")\n+    parser = argparse.ArgumentParser(description=\"Convert PRX checkpoint to diffusers format\")\n \n     parser.add_argument(\n-        \"--checkpoint_path\", type=str, required=True, help=\"Path to the original Photon checkpoint (.pth file )\"\n+        \"--checkpoint_path\", type=str, required=True, help=\"Path to the original PRX checkpoint (.pth file )\"\n     )\n \n     parser.add_argument("
        },
        {
          "filename": "src/diffusers/__init__.py",
          "status": "modified",
          "additions": 4,
          "deletions": 4,
          "changes": 8,
          "patch": "@@ -232,9 +232,9 @@\n             \"MultiControlNetModel\",\n             \"OmniGenTransformer2DModel\",\n             \"ParallelConfig\",\n-            \"PhotonTransformer2DModel\",\n             \"PixArtTransformer2DModel\",\n             \"PriorTransformer\",\n+            \"PRXTransformer2DModel\",\n             \"QwenImageControlNetModel\",\n             \"QwenImageMultiControlNetModel\",\n             \"QwenImageTransformer2DModel\",\n@@ -516,11 +516,11 @@\n             \"MusicLDMPipeline\",\n             \"OmniGenPipeline\",\n             \"PaintByExamplePipeline\",\n-            \"PhotonPipeline\",\n             \"PIAPipeline\",\n             \"PixArtAlphaPipeline\",\n             \"PixArtSigmaPAGPipeline\",\n             \"PixArtSigmaPipeline\",\n+            \"PRXPipeline\",\n             \"QwenImageControlNetInpaintPipeline\",\n             \"QwenImageControlNetPipeline\",\n             \"QwenImageEditInpaintPipeline\",\n@@ -928,9 +928,9 @@\n             MultiControlNetModel,\n             OmniGenTransformer2DModel,\n             ParallelConfig,\n-            PhotonTransformer2DModel,\n             PixArtTransformer2DModel,\n             PriorTransformer,\n+            PRXTransformer2DModel,\n             QwenImageControlNetModel,\n             QwenImageMultiControlNetModel,\n             QwenImageTransformer2DModel,\n@@ -1182,11 +1182,11 @@\n             MusicLDMPipeline,\n             OmniGenPipeline,\n             PaintByExamplePipeline,\n-            PhotonPipeline,\n             PIAPipeline,\n             PixArtAlphaPipeline,\n             PixArtSigmaPAGPipeline,\n             PixArtSigmaPipeline,\n+            PRXPipeline,\n             QwenImageControlNetInpaintPipeline,\n             QwenImageControlNetPipeline,\n             QwenImageEditInpaintPipeline,"
        },
        {
          "filename": "src/diffusers/models/__init__.py",
          "status": "modified",
          "additions": 2,
          "deletions": 2,
          "changes": 4,
          "patch": "@@ -96,7 +96,7 @@\n     _import_structure[\"transformers.transformer_lumina2\"] = [\"Lumina2Transformer2DModel\"]\n     _import_structure[\"transformers.transformer_mochi\"] = [\"MochiTransformer3DModel\"]\n     _import_structure[\"transformers.transformer_omnigen\"] = [\"OmniGenTransformer2DModel\"]\n-    _import_structure[\"transformers.transformer_photon\"] = [\"PhotonTransformer2DModel\"]\n+    _import_structure[\"transformers.transformer_prx\"] = [\"PRXTransformer2DModel\"]\n     _import_structure[\"transformers.transformer_qwenimage\"] = [\"QwenImageTransformer2DModel\"]\n     _import_structure[\"transformers.transformer_sd3\"] = [\"SD3Transformer2DModel\"]\n     _import_structure[\"transformers.transformer_skyreels_v2\"] = [\"SkyReelsV2Transformer3DModel\"]\n@@ -191,9 +191,9 @@\n             LuminaNextDiT2DModel,\n             MochiTransformer3DModel,\n             OmniGenTransformer2DModel,\n-            PhotonTransformer2DModel,\n             PixArtTransformer2DModel,\n             PriorTransformer,\n+            PRXTransformer2DModel,\n             QwenImageTransformer2DModel,\n             SanaTransformer2DModel,\n             SD3Transformer2DModel,"
        },
        {
          "filename": "src/diffusers/models/transformers/__init__.py",
          "status": "modified",
          "additions": 1,
          "deletions": 1,
          "changes": 2,
          "patch": "@@ -32,7 +32,7 @@\n     from .transformer_lumina2 import Lumina2Transformer2DModel\n     from .transformer_mochi import MochiTransformer3DModel\n     from .transformer_omnigen import OmniGenTransformer2DModel\n-    from .transformer_photon import PhotonTransformer2DModel\n+    from .transformer_prx import PRXTransformer2DModel\n     from .transformer_qwenimage import QwenImageTransformer2DModel\n     from .transformer_sd3 import SD3Transformer2DModel\n     from .transformer_skyreels_v2 import SkyReelsV2Transformer3DModel"
        },
        {
          "filename": "src/diffusers/models/transformers/transformer_prx.py",
          "status": "renamed",
          "additions": 23,
          "deletions": 23,
          "changes": 46,
          "patch": "@@ -80,9 +80,9 @@ def apply_rope(xq: torch.Tensor, freqs_cis: torch.Tensor) -> torch.Tensor:\n     return xq_out.reshape(*xq.shape).type_as(xq)\n \n \n-class PhotonAttnProcessor2_0:\n+class PRXAttnProcessor2_0:\n     r\"\"\"\n-    Processor for implementing Photon-style attention with multi-source tokens and RoPE. Supports multiple attention\n+    Processor for implementing PRX-style attention with multi-source tokens and RoPE. Supports multiple attention\n     backends (Flash Attention, Sage Attention, etc.) via dispatch_attention_fn.\n     \"\"\"\n \n@@ -91,30 +91,30 @@ class PhotonAttnProcessor2_0:\n \n     def __init__(self):\n         if not hasattr(torch.nn.functional, \"scaled_dot_product_attention\"):\n-            raise ImportError(\"PhotonAttnProcessor2_0 requires PyTorch 2.0, please upgrade PyTorch to 2.0.\")\n+            raise ImportError(\"PRXAttnProcessor2_0 requires PyTorch 2.0, please upgrade PyTorch to 2.0.\")\n \n     def __call__(\n         self,\n-        attn: \"PhotonAttention\",\n+        attn: \"PRXAttention\",\n         hidden_states: torch.Tensor,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         image_rotary_emb: Optional[torch.Tensor] = None,\n         **kwargs,\n     ) -> torch.Tensor:\n         \"\"\"\n-        Apply Photon attention using PhotonAttention module.\n+        Apply PRX attention using PRXAttention module.\n \n         Args:\n-            attn: PhotonAttention module containing projection layers\n+            attn: PRXAttention module containing projection layers\n             hidden_states: Image tokens [B, L_img, D]\n             encoder_hidden_states: Text tokens [B, L_txt, D]\n             attention_mask: Boolean mask for text tokens [B, L_txt]\n             image_rotary_emb: Rotary positional embeddings [B, 1, L_img, head_dim//2, 2, 2]\n         \"\"\"\n \n         if encoder_hidden_states is None:\n-            raise ValueError(\"PhotonAttnProcessor2_0 requires 'encoder_hidden_states' containing text tokens.\")\n+            raise ValueError(\"PRXAttnProcessor2_0 requires 'encoder_hidden_states' containing text tokens.\")\n \n         # Project image tokens to Q, K, V\n         img_qkv = attn.img_qkv_proj(hidden_states)\n@@ -190,14 +190,14 @@ def __call__(\n         return attn_output\n \n \n-class PhotonAttention(nn.Module, AttentionModuleMixin):\n+class PRXAttention(nn.Module, AttentionModuleMixin):\n     r\"\"\"\n-    Photon-style attention module that handles multi-source tokens and RoPE. Similar to FluxAttention but adapted for\n-    Photon's architecture.\n+    PRX-style attention module that handles multi-source tokens and RoPE. Similar to FluxAttention but adapted for\n+    PRX's architecture.\n     \"\"\"\n \n-    _default_processor_cls = PhotonAttnProcessor2_0\n-    _available_processors = [PhotonAttnProcessor2_0]\n+    _default_processor_cls = PRXAttnProcessor2_0\n+    _available_processors = [PRXAttnProcessor2_0]\n \n     def __init__(\n         self,\n@@ -251,7 +251,7 @@ def forward(\n \n \n # inspired from https://github.com/black-forest-labs/flux/blob/main/src/flux/modules/layers.py\n-class PhotonEmbedND(nn.Module):\n+class PRXEmbedND(nn.Module):\n     r\"\"\"\n     N-dimensional rotary positional embedding.\n \n@@ -347,7 +347,7 @@ def forward(\n         return tuple(out[:3]), tuple(out[3:])\n \n \n-class PhotonBlock(nn.Module):\n+class PRXBlock(nn.Module):\n     r\"\"\"\n     Multimodal transformer block with text\u2013image cross-attention, modulation, and MLP.\n \n@@ -364,7 +364,7 @@ class PhotonBlock(nn.Module):\n     Attributes:\n         img_pre_norm (`nn.LayerNorm`):\n             Pre-normalization applied to image tokens before attention.\n-        attention (`PhotonAttention`):\n+        attention (`PRXAttention`):\n             Multi-head attention module with built-in QKV projections and normalizations for cross-attention between\n             image and text tokens.\n         post_attention_layernorm (`nn.LayerNorm`):\n@@ -400,15 +400,15 @@ def __init__(\n         # Pre-attention normalization for image tokens\n         self.img_pre_norm = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)\n \n-        # PhotonAttention module with built-in projections and norms\n-        self.attention = PhotonAttention(\n+        # PRXAttention module with built-in projections and norms\n+        self.attention = PRXAttention(\n             query_dim=hidden_size,\n             heads=num_heads,\n             dim_head=self.head_dim,\n             bias=False,\n             out_bias=False,\n             eps=1e-6,\n-            processor=PhotonAttnProcessor2_0(),\n+            processor=PRXAttnProcessor2_0(),\n         )\n \n         # mlp\n@@ -557,7 +557,7 @@ def seq2img(seq: torch.Tensor, patch_size: int, shape: torch.Tensor) -> torch.Te\n     return fold(seq.transpose(1, 2), shape, kernel_size=patch_size, stride=patch_size)\n \n \n-class PhotonTransformer2DModel(ModelMixin, ConfigMixin, AttentionMixin):\n+class PRXTransformer2DModel(ModelMixin, ConfigMixin, AttentionMixin):\n     r\"\"\"\n     Transformer-based 2D model for text to image generation.\n \n@@ -595,7 +595,7 @@ class PhotonTransformer2DModel(ModelMixin, ConfigMixin, AttentionMixin):\n         txt_in (`nn.Linear`):\n             Projection layer for text conditioning.\n         blocks (`nn.ModuleList`):\n-            Stack of transformer blocks (`PhotonBlock`).\n+            Stack of transformer blocks (`PRXBlock`).\n         final_layer (`LastLayer`):\n             Projection layer mapping hidden tokens back to patch outputs.\n \n@@ -661,14 +661,14 @@ def __init__(\n \n         self.hidden_size = hidden_size\n         self.num_heads = num_heads\n-        self.pe_embedder = PhotonEmbedND(dim=pe_dim, theta=theta, axes_dim=axes_dim)\n+        self.pe_embedder = PRXEmbedND(dim=pe_dim, theta=theta, axes_dim=axes_dim)\n         self.img_in = nn.Linear(self.in_channels * self.patch_size**2, self.hidden_size, bias=True)\n         self.time_in = MLPEmbedder(in_dim=256, hidden_dim=self.hidden_size)\n         self.txt_in = nn.Linear(context_in_dim, self.hidden_size)\n \n         self.blocks = nn.ModuleList(\n             [\n-                PhotonBlock(\n+                PRXBlock(\n                     self.hidden_size,\n                     self.num_heads,\n                     mlp_ratio=mlp_ratio,\n@@ -702,7 +702,7 @@ def forward(\n         return_dict: bool = True,\n     ) -> Union[Tuple[torch.Tensor, ...], Transformer2DModelOutput]:\n         r\"\"\"\n-        Forward pass of the PhotonTransformer2DModel.\n+        Forward pass of the PRXTransformer2DModel.\n \n         The latent image is split into patch tokens, combined with text conditioning, and processed through a stack of\n         transformer blocks modulated by the timestep. The output is reconstructed into the latent image space."
        },
        {
          "filename": "src/diffusers/pipelines/__init__.py",
          "status": "modified",
          "additions": 2,
          "deletions": 2,
          "changes": 4,
          "patch": "@@ -144,7 +144,7 @@\n         \"FluxKontextPipeline\",\n         \"FluxKontextInpaintPipeline\",\n     ]\n-    _import_structure[\"photon\"] = [\"PhotonPipeline\"]\n+    _import_structure[\"prx\"] = [\"PRXPipeline\"]\n     _import_structure[\"audioldm\"] = [\"AudioLDMPipeline\"]\n     _import_structure[\"audioldm2\"] = [\n         \"AudioLDM2Pipeline\",\n@@ -718,9 +718,9 @@\n             StableDiffusionXLPAGPipeline,\n         )\n         from .paint_by_example import PaintByExamplePipeline\n-        from .photon import PhotonPipeline\n         from .pia import PIAPipeline\n         from .pixart_alpha import PixArtAlphaPipeline, PixArtSigmaPipeline\n+        from .prx import PRXPipeline\n         from .qwenimage import (\n             QwenImageControlNetInpaintPipeline,\n             QwenImageControlNetPipeline,"
        },
        {
          "filename": "src/diffusers/pipelines/prx/__init__.py",
          "status": "renamed",
          "additions": 4,
          "deletions": 4,
          "changes": 8,
          "patch": "@@ -12,7 +12,7 @@\n \n _dummy_objects = {}\n _additional_imports = {}\n-_import_structure = {\"pipeline_output\": [\"PhotonPipelineOutput\"]}\n+_import_structure = {\"pipeline_output\": [\"PRXPipelineOutput\"]}\n \n try:\n     if not (is_transformers_available() and is_torch_available()):\n@@ -22,7 +22,7 @@\n \n     _dummy_objects.update(get_objects_from_module(dummy_torch_and_transformers_objects))\n else:\n-    _import_structure[\"pipeline_photon\"] = [\"PhotonPipeline\"]\n+    _import_structure[\"pipeline_prx\"] = [\"PRXPipeline\"]\n \n # Import T5GemmaEncoder for pipeline loading compatibility\n try:\n@@ -44,8 +44,8 @@\n     except OptionalDependencyNotAvailable:\n         from ...utils.dummy_torch_and_transformers_objects import *  # noqa F403\n     else:\n-        from .pipeline_output import PhotonPipelineOutput\n-        from .pipeline_photon import PhotonPipeline\n+        from .pipeline_output import PRXPipelineOutput\n+        from .pipeline_prx import PRXPipeline\n \n else:\n     import sys"
        },
        {
          "filename": "src/diffusers/pipelines/prx/pipeline_output.py",
          "status": "renamed",
          "additions": 2,
          "deletions": 2,
          "changes": 4,
          "patch": "@@ -22,9 +22,9 @@\n \n \n @dataclass\n-class PhotonPipelineOutput(BaseOutput):\n+class PRXPipelineOutput(BaseOutput):\n     \"\"\"\n-    Output class for Photon pipelines.\n+    Output class for PRX pipelines.\n \n     Args:\n         images (`List[PIL.Image.Image]` or `np.ndarray`)"
        },
        {
          "filename": "src/diffusers/pipelines/prx/pipeline_prx.py",
          "status": "renamed",
          "additions": 17,
          "deletions": 18,
          "changes": 35,
          "patch": "@@ -30,9 +30,9 @@\n from diffusers.image_processor import PixArtImageProcessor\n from diffusers.loaders import FromSingleFileMixin, LoraLoaderMixin, TextualInversionLoaderMixin\n from diffusers.models import AutoencoderDC, AutoencoderKL\n-from diffusers.models.transformers.transformer_photon import PhotonTransformer2DModel\n-from diffusers.pipelines.photon.pipeline_output import PhotonPipelineOutput\n+from diffusers.models.transformers.transformer_prx import PRXTransformer2DModel\n from diffusers.pipelines.pipeline_utils import DiffusionPipeline\n+from diffusers.pipelines.prx.pipeline_output import PRXPipelineOutput\n from diffusers.schedulers import FlowMatchEulerDiscreteScheduler\n from diffusers.utils import (\n     logging,\n@@ -73,7 +73,7 @@\n \n \n class TextPreprocessor:\n-    \"\"\"Text preprocessing utility for PhotonPipeline.\"\"\"\n+    \"\"\"Text preprocessing utility for PRXPipeline.\"\"\"\n \n     def __init__(self):\n         \"\"\"Initialize text preprocessor.\"\"\"\n@@ -203,34 +203,34 @@ def clean_text(self, text: str) -> str:\n     Examples:\n         ```py\n         >>> import torch\n-        >>> from diffusers import PhotonPipeline\n+        >>> from diffusers import PRXPipeline\n \n         >>> # Load pipeline with from_pretrained\n-        >>> pipe = PhotonPipeline.from_pretrained(\"Photoroom/photon-512-t2i-sft\")\n+        >>> pipe = PRXPipeline.from_pretrained(\"Photoroom/prx-512-t2i-sft\")\n         >>> pipe.to(\"cuda\")\n \n         >>> prompt = \"A digital painting of a rusty, vintage tram on a sandy beach\"\n         >>> image = pipe(prompt, num_inference_steps=28, guidance_scale=5.0).images[0]\n-        >>> image.save(\"photon_output.png\")\n+        >>> image.save(\"prx_output.png\")\n         ```\n \"\"\"\n \n \n-class PhotonPipeline(\n+class PRXPipeline(\n     DiffusionPipeline,\n     LoraLoaderMixin,\n     FromSingleFileMixin,\n     TextualInversionLoaderMixin,\n ):\n     r\"\"\"\n-    Pipeline for text-to-image generation using Photon Transformer.\n+    Pipeline for text-to-image generation using PRX Transformer.\n \n     This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n     library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n \n     Args:\n-        transformer ([`PhotonTransformer2DModel`]):\n-            The Photon transformer model to denoise the encoded image latents.\n+        transformer ([`PRXTransformer2DModel`]):\n+            The PRX transformer model to denoise the encoded image latents.\n         scheduler ([`FlowMatchEulerDiscreteScheduler`]):\n             A scheduler to be used in combination with `transformer` to denoise the encoded image latents.\n         text_encoder ([`T5GemmaEncoder`]):\n@@ -248,7 +248,7 @@ class PhotonPipeline(\n \n     def __init__(\n         self,\n-        transformer: PhotonTransformer2DModel,\n+        transformer: PRXTransformer2DModel,\n         scheduler: FlowMatchEulerDiscreteScheduler,\n         text_encoder: T5GemmaEncoder,\n         tokenizer: Union[T5TokenizerFast, GemmaTokenizerFast, AutoTokenizer],\n@@ -257,9 +257,9 @@ def __init__(\n     ):\n         super().__init__()\n \n-        if PhotonTransformer2DModel is None:\n+        if PRXTransformer2DModel is None:\n             raise ImportError(\n-                \"PhotonTransformer2DModel is not available. Please ensure the transformer_photon module is properly installed.\"\n+                \"PRXTransformer2DModel is not available. Please ensure the transformer_prx module is properly installed.\"\n             )\n \n         self.text_preprocessor = TextPreprocessor()\n@@ -567,7 +567,7 @@ def __call__(\n                 The output format of the generate image. Choose between\n                 [PIL](https://pillow.readthedocs.io/en/stable/): `PIL.Image.Image` or `np.array`.\n             return_dict (`bool`, *optional*, defaults to `True`):\n-                Whether or not to return a [`~pipelines.photon.PhotonPipelineOutput`] instead of a plain tuple.\n+                Whether or not to return a [`~pipelines.prx.PRXPipelineOutput`] instead of a plain tuple.\n             use_resolution_binning (`bool`, *optional*, defaults to `True`):\n                 If set to `True`, the requested height and width are first mapped to the closest resolutions using\n                 predefined aspect ratio bins. After the produced latents are decoded into images, they are resized back\n@@ -585,9 +585,8 @@ def __call__(\n         Examples:\n \n         Returns:\n-            [`~pipelines.photon.PhotonPipelineOutput`] or `tuple`: [`~pipelines.photon.PhotonPipelineOutput`] if\n-            `return_dict` is True, otherwise a `tuple. When returning a tuple, the first element is a list with the\n-            generated images.\n+            [`~pipelines.prx.PRXPipelineOutput`] or `tuple`: [`~pipelines.prx.PRXPipelineOutput`] if `return_dict` is\n+            True, otherwise a `tuple. When returning a tuple, the first element is a list with the generated images.\n         \"\"\"\n \n         # 0. Set height and width\n@@ -765,4 +764,4 @@ def __call__(\n         if not return_dict:\n             return (image,)\n \n-        return PhotonPipelineOutput(images=image)\n+        return PRXPipelineOutput(images=image)"
        },
        {
          "filename": "src/diffusers/utils/dummy_pt_objects.py",
          "status": "modified",
          "additions": 3,
          "deletions": 3,
          "changes": 6,
          "patch": "@@ -1098,7 +1098,7 @@ def from_pretrained(cls, *args, **kwargs):\n         requires_backends(cls, [\"torch\"])\n \n \n-class PhotonTransformer2DModel(metaclass=DummyObject):\n+class PixArtTransformer2DModel(metaclass=DummyObject):\n     _backends = [\"torch\"]\n \n     def __init__(self, *args, **kwargs):\n@@ -1113,7 +1113,7 @@ def from_pretrained(cls, *args, **kwargs):\n         requires_backends(cls, [\"torch\"])\n \n \n-class PixArtTransformer2DModel(metaclass=DummyObject):\n+class PriorTransformer(metaclass=DummyObject):\n     _backends = [\"torch\"]\n \n     def __init__(self, *args, **kwargs):\n@@ -1128,7 +1128,7 @@ def from_pretrained(cls, *args, **kwargs):\n         requires_backends(cls, [\"torch\"])\n \n \n-class PriorTransformer(metaclass=DummyObject):\n+class PRXTransformer2DModel(metaclass=DummyObject):\n     _backends = [\"torch\"]\n \n     def __init__(self, *args, **kwargs):"
        },
        {
          "filename": "src/diffusers/utils/dummy_torch_and_transformers_objects.py",
          "status": "modified",
          "additions": 5,
          "deletions": 5,
          "changes": 10,
          "patch": "@@ -1847,7 +1847,7 @@ def from_pretrained(cls, *args, **kwargs):\n         requires_backends(cls, [\"torch\", \"transformers\"])\n \n \n-class PhotonPipeline(metaclass=DummyObject):\n+class PIAPipeline(metaclass=DummyObject):\n     _backends = [\"torch\", \"transformers\"]\n \n     def __init__(self, *args, **kwargs):\n@@ -1862,7 +1862,7 @@ def from_pretrained(cls, *args, **kwargs):\n         requires_backends(cls, [\"torch\", \"transformers\"])\n \n \n-class PIAPipeline(metaclass=DummyObject):\n+class PixArtAlphaPipeline(metaclass=DummyObject):\n     _backends = [\"torch\", \"transformers\"]\n \n     def __init__(self, *args, **kwargs):\n@@ -1877,7 +1877,7 @@ def from_pretrained(cls, *args, **kwargs):\n         requires_backends(cls, [\"torch\", \"transformers\"])\n \n \n-class PixArtAlphaPipeline(metaclass=DummyObject):\n+class PixArtSigmaPAGPipeline(metaclass=DummyObject):\n     _backends = [\"torch\", \"transformers\"]\n \n     def __init__(self, *args, **kwargs):\n@@ -1892,7 +1892,7 @@ def from_pretrained(cls, *args, **kwargs):\n         requires_backends(cls, [\"torch\", \"transformers\"])\n \n \n-class PixArtSigmaPAGPipeline(metaclass=DummyObject):\n+class PixArtSigmaPipeline(metaclass=DummyObject):\n     _backends = [\"torch\", \"transformers\"]\n \n     def __init__(self, *args, **kwargs):\n@@ -1907,7 +1907,7 @@ def from_pretrained(cls, *args, **kwargs):\n         requires_backends(cls, [\"torch\", \"transformers\"])\n \n \n-class PixArtSigmaPipeline(metaclass=DummyObject):\n+class PRXPipeline(metaclass=DummyObject):\n     _backends = [\"torch\", \"transformers\"]\n \n     def __init__(self, *args, **kwargs):"
        },
        {
          "filename": "tests/models/transformers/test_models_transformer_prx.py",
          "status": "renamed",
          "additions": 4,
          "deletions": 4,
          "changes": 8,
          "patch": "@@ -17,7 +17,7 @@\n \n import torch\n \n-from diffusers.models.transformers.transformer_photon import PhotonTransformer2DModel\n+from diffusers.models.transformers.transformer_prx import PRXTransformer2DModel\n \n from ...testing_utils import enable_full_determinism, torch_device\n from ..test_modeling_common import ModelTesterMixin\n@@ -26,8 +26,8 @@\n enable_full_determinism()\n \n \n-class PhotonTransformerTests(ModelTesterMixin, unittest.TestCase):\n-    model_class = PhotonTransformer2DModel\n+class PRXTransformerTests(ModelTesterMixin, unittest.TestCase):\n+    model_class = PRXTransformer2DModel\n     main_input_name = \"hidden_states\"\n     uses_custom_attn_processor = True\n \n@@ -75,7 +75,7 @@ def prepare_init_args_and_inputs_for_common(self):\n         return init_dict, inputs_dict\n \n     def test_gradient_checkpointing_is_applied(self):\n-        expected_set = {\"PhotonTransformer2DModel\"}\n+        expected_set = {\"PRXTransformer2DModel\"}\n         super().test_gradient_checkpointing_is_applied(expected_set=expected_set)\n \n "
        },
        {
          "filename": "tests/pipelines/prx/__init__.py",
          "status": "renamed",
          "additions": 0,
          "deletions": 0,
          "changes": 0,
          "patch": ""
        },
        {
          "filename": "tests/pipelines/prx/test_pipeline_prx.py",
          "status": "renamed",
          "additions": 13,
          "deletions": 13,
          "changes": 26,
          "patch": "@@ -8,8 +8,8 @@\n from transformers.models.t5gemma.modeling_t5gemma import T5GemmaEncoder\n \n from diffusers.models import AutoencoderDC, AutoencoderKL\n-from diffusers.models.transformers.transformer_photon import PhotonTransformer2DModel\n-from diffusers.pipelines.photon.pipeline_photon import PhotonPipeline\n+from diffusers.models.transformers.transformer_prx import PRXTransformer2DModel\n+from diffusers.pipelines.prx.pipeline_prx import PRXPipeline\n from diffusers.schedulers import FlowMatchEulerDiscreteScheduler\n from diffusers.utils import is_transformers_version\n \n@@ -22,8 +22,8 @@\n     reason=\"See https://github.com/huggingface/diffusers/pull/12456#issuecomment-3424228544\",\n     strict=False,\n )\n-class PhotonPipelineFastTests(PipelineTesterMixin, unittest.TestCase):\n-    pipeline_class = PhotonPipeline\n+class PRXPipelineFastTests(PipelineTesterMixin, unittest.TestCase):\n+    pipeline_class = PRXPipeline\n     params = TEXT_TO_IMAGE_PARAMS - {\"cross_attention_kwargs\"}\n     batch_params = frozenset([\"prompt\", \"negative_prompt\", \"num_images_per_prompt\"])\n     test_xformers_attention = False\n@@ -32,16 +32,16 @@ class PhotonPipelineFastTests(PipelineTesterMixin, unittest.TestCase):\n \n     @classmethod\n     def setUpClass(cls):\n-        # Ensure PhotonPipeline has an _execution_device property expected by __call__\n-        if not isinstance(getattr(PhotonPipeline, \"_execution_device\", None), property):\n+        # Ensure PRXPipeline has an _execution_device property expected by __call__\n+        if not isinstance(getattr(PRXPipeline, \"_execution_device\", None), property):\n             try:\n-                setattr(PhotonPipeline, \"_execution_device\", property(lambda self: torch.device(\"cpu\")))\n+                setattr(PRXPipeline, \"_execution_device\", property(lambda self: torch.device(\"cpu\")))\n             except Exception:\n                 pass\n \n     def get_dummy_components(self):\n         torch.manual_seed(0)\n-        transformer = PhotonTransformer2DModel(\n+        transformer = PRXTransformer2DModel(\n             patch_size=1,\n             in_channels=4,\n             context_in_dim=8,\n@@ -129,7 +129,7 @@ def get_dummy_inputs(self, device, seed=0):\n     def test_inference(self):\n         device = \"cpu\"\n         components = self.get_dummy_components()\n-        pipe = PhotonPipeline(**components)\n+        pipe = PRXPipeline(**components)\n         pipe.to(device)\n         pipe.set_progress_bar_config(disable=None)\n         try:\n@@ -148,7 +148,7 @@ def test_inference(self):\n \n     def test_callback_inputs(self):\n         components = self.get_dummy_components()\n-        pipe = PhotonPipeline(**components)\n+        pipe = PRXPipeline(**components)\n         pipe = pipe.to(\"cpu\")\n         pipe.set_progress_bar_config(disable=None)\n         try:\n@@ -157,7 +157,7 @@ def test_callback_inputs(self):\n             pass\n         self.assertTrue(\n             hasattr(pipe, \"_callback_tensor_inputs\"),\n-            f\" {PhotonPipeline} should have `_callback_tensor_inputs` that defines a list of tensor variables its callback function can use as inputs\",\n+            f\" {PRXPipeline} should have `_callback_tensor_inputs` that defines a list of tensor variables its callback function can use as inputs\",\n         )\n \n         def callback_inputs_subset(pipe, i, t, callback_kwargs):\n@@ -216,7 +216,7 @@ def to_np_local(tensor):\n         self.assertLess(max(max_diff1, max_diff2), expected_max_diff)\n \n     def test_inference_with_autoencoder_dc(self):\n-        \"\"\"Test PhotonPipeline with AutoencoderDC (DCAE) instead of AutoencoderKL.\"\"\"\n+        \"\"\"Test PRXPipeline with AutoencoderDC (DCAE) instead of AutoencoderKL.\"\"\"\n         device = \"cpu\"\n \n         components = self.get_dummy_components()\n@@ -248,7 +248,7 @@ def test_inference_with_autoencoder_dc(self):\n \n         components[\"vae\"] = vae_dc\n \n-        pipe = PhotonPipeline(**components)\n+        pipe = PRXPipeline(**components)\n         pipe.to(device)\n         pipe.set_progress_bar_config(disable=None)\n "
        }
      ],
      "num_files": 17,
      "scraped_at": "2025-11-16T21:18:53.714852"
    },
    {
      "pr_number": 12520,
      "title": "Kandinsky 5 10 sec (NABLA suport)",
      "body": "This PR adds support for 10 sec Kandinsky 5.0 model herd.\r\n\r\n```python\r\nimport torch\r\nfrom diffusers import Kandinsky5T2VPipeline\r\nfrom diffusers.utils import export_to_video\r\n\r\n# Load the pipeline\r\npipe = Kandinsky5T2VPipeline.from_pretrained(\r\n    \"ai-forever/Kandinsky-5.0-T2V-Lite-sft-10s-Diffusers\", \r\n    torch_dtype=torch.bfloat16\r\n)\r\npipe = pipe.to(\"cuda\")\r\n\r\n# Generate video\r\nprompt = [\r\n    \"Photorealistic closeup video of two intricately detailed pirate ships locked in a fierce battle, complete with cannon fire and billowing sails, as they sail through the swirling waters of a steaming cup of coffee. The ships are miniature but highly realistic, with wooden textures and flags fluttering in the liquid breeze. Coffee splashes and foam ripple around them as they maneuver through the turbulent surface, dodging each other's attacks. A detailed reflection of the battle appears on the glossy surface of the coffee, adding to the dynamic realism. The camera pans and zooms to capture every dramatic moment of the high-seas clash within this tiny, unexpected world.\",\r\n    \"Bad quality\",\r\n]\r\nnegative_prompt = \"Static, 2D cartoon, cartoon, 2d animation, paintings, images, worst quality, low quality, ugly, deformed, walking backwards\"\r\n\r\npipe.transformer.set_attention_backend(\"flex\")\r\n\r\noutput = pipe(\r\n    prompt=prompt,\r\n    negative_prompt=negative_prompt,\r\n    height=512,\r\n    width=768,\r\n    num_frames=241,\r\n    num_inference_steps=50,\r\n    guidance_scale=5.0,\r\n    num_videos_per_prompt=1,\r\n    generator=torch.Generator(42)\r\n)\r\n```\r\n\r\nhttps://github.com/user-attachments/assets/52647681-0178-4797-88f3-de0506db5a3d\r\n\r\n",
      "html_url": "https://github.com/huggingface/diffusers/pull/12520",
      "created_at": "2025-10-21T10:44:16Z",
      "merged_at": "2025-10-28T02:17:18Z",
      "merge_commit_sha": "5afbcce176cd4e8ec08f43ee9fae2d6562edf54c",
      "base_ref": "main",
      "head_sha": "861f787edeca6ce926accae8e5c09e887e213d48",
      "user": "leffff",
      "files": [
        {
          "filename": "docs/source/en/_toctree.yml",
          "status": "modified",
          "additions": 2,
          "deletions": 0,
          "changes": 2,
          "patch": "@@ -525,6 +525,8 @@\n         title: Kandinsky 2.2\n       - local: api/pipelines/kandinsky3\n         title: Kandinsky 3\n+      - local: api/pipelines/kandinsky5\n+        title: Kandinsky 5\n       - local: api/pipelines/kolors\n         title: Kolors\n       - local: api/pipelines/latent_consistency_models"
        },
        {
          "filename": "docs/source/en/api/pipelines/kandinsky5.md",
          "status": "added",
          "additions": 149,
          "deletions": 0,
          "changes": 149,
          "patch": "@@ -0,0 +1,149 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+http://www.apache.org/licenses/LICENSE-2.0\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+-->\n+\n+# Kandinsky 5.0\n+\n+Kandinsky 5.0 is created by the Kandinsky team: Alexey Letunovskiy, Maria Kovaleva, Ivan Kirillov, Lev Novitskiy, Denis Koposov, Dmitrii Mikhailov, Anna Averchenkova, Andrey Shutkin, Julia Agafonova, Olga Kim, Anastasiia Kargapoltseva, Nikita Kiselev, Anna Dmitrienko,  Anastasia Maltseva, Kirill Chernyshev, Ilia Vasiliev, Viacheslav Vasilev, Vladimir Polovnikov, Yury Kolabushin, Alexander Belykh, Mikhail Mamaev, Anastasia Aliaskina, Tatiana Nikulina, Polina Gavrilova, Vladimir Arkhipkin, Vladimir Korviakov, Nikolai Gerasimenko, Denis Parkhomenko, Denis Dimitrov\n+\n+\n+Kandinsky 5.0 is a family of diffusion models for Video & Image generation. Kandinsky 5.0 T2V Lite is a lightweight video generation model (2B parameters) that ranks #1 among open-source models in its class. It outperforms larger models and offers the best understanding of Russian concepts in the open-source ecosystem.\n+\n+The model introduces several key innovations:\n+- **Latent diffusion pipeline** with **Flow Matching** for improved training stability\n+- **Diffusion Transformer (DiT)** as the main generative backbone with cross-attention to text embeddings\n+- Dual text encoding using **Qwen2.5-VL** and **CLIP** for comprehensive text understanding\n+- **HunyuanVideo 3D VAE** for efficient video encoding and decoding\n+- **Sparse attention mechanisms** (NABLA) for efficient long-sequence processing\n+\n+The original codebase can be found at [ai-forever/Kandinsky-5](https://github.com/ai-forever/Kandinsky-5).\n+\n+> [!TIP]\n+> Check out the [AI Forever](https://huggingface.co/ai-forever) organization on the Hub for the official model checkpoints for text-to-video generation, including pretrained, SFT, no-CFG, and distilled variants.\n+\n+## Available Models\n+\n+Kandinsky 5.0 T2V Lite comes in several variants optimized for different use cases:\n+\n+| model_id | Description | Use Cases |\n+|------------|-------------|-----------|\n+| **ai-forever/Kandinsky-5.0-T2V-Lite-sft-5s-Diffusers** | 5 second Supervised Fine-Tuned model | Highest generation quality |\n+| **ai-forever/Kandinsky-5.0-T2V-Lite-sft-10s-Diffusers** | 10 second Supervised Fine-Tuned model | Highest generation quality |\n+| **ai-forever/Kandinsky-5.0-T2V-Lite-nocfg-5s-Diffusers** | 5 second Classifier-Free Guidance distilled | 2\u00d7 faster inference |\n+| **ai-forever/Kandinsky-5.0-T2V-Lite-nocfg-10s-Diffusers** | 10 second Classifier-Free Guidance distilled | 2\u00d7 faster inference |\n+| **ai-forever/Kandinsky-5.0-T2V-Lite-distilled16steps-5s-Diffusers** | 5 second Diffusion distilled to 16 steps | 6\u00d7 faster inference, minimal quality loss |\n+| **ai-forever/Kandinsky-5.0-T2V-Lite-distilled16steps-10s-Diffusers** | 10 second Diffusion distilled to 16 steps | 6\u00d7 faster inference, minimal quality loss |\n+| **ai-forever/Kandinsky-5.0-T2V-Lite-pretrain-5s-Diffusers** | 5 second Base pretrained model | Research and fine-tuning |\n+| **ai-forever/Kandinsky-5.0-T2V-Lite-pretrain-10s-Diffusers** | 10 second Base pretrained model | Research and fine-tuning |\n+\n+All models are available in 5-second and 10-second video generation versions.\n+\n+## Kandinsky5T2VPipeline\n+\n+[[autodoc]] Kandinsky5T2VPipeline\n+    - all\n+    - __call__\n+\n+## Usage Examples\n+\n+### Basic Text-to-Video Generation\n+\n+```python\n+import torch\n+from diffusers import Kandinsky5T2VPipeline\n+from diffusers.utils import export_to_video\n+\n+# Load the pipeline\n+model_id = \"ai-forever/Kandinsky-5.0-T2V-Lite-sft-5s-Diffusers\"\n+pipe = Kandinsky5T2VPipeline.from_pretrained(model_id, torch_dtype=torch.bfloat16)\n+pipe = pipe.to(\"cuda\")\n+\n+# Generate video\n+prompt = \"A cat and a dog baking a cake together in a kitchen.\"\n+negative_prompt = \"Static, 2D cartoon, cartoon, 2d animation, paintings, images, worst quality, low quality, ugly, deformed, walking backwards\"\n+\n+output = pipe(\n+    prompt=prompt,\n+    negative_prompt=negative_prompt,\n+    height=512,\n+    width=768,\n+    num_frames=121,  # ~5 seconds at 24fps\n+    num_inference_steps=50,\n+    guidance_scale=5.0,\n+).frames[0]\n+\n+export_to_video(output, \"output.mp4\", fps=24, quality=9)\n+```\n+\n+### 10 second Models\n+**\u26a0\ufe0f Warning!** all 10 second models should be used with Flex attention and max-autotune-no-cudagraphs compilation:\n+\n+```python\n+pipe = Kandinsky5T2VPipeline.from_pretrained(\n+    \"ai-forever/Kandinsky-5.0-T2V-Lite-sft-10s-Diffusers\", \n+    torch_dtype=torch.bfloat16\n+)\n+pipe = pipe.to(\"cuda\")\n+\n+pipe.transformer.set_attention_backend(\n+    \"flex\"\n+)                                       # <--- Sett attention bakend to Flex\n+pipe.transformer.compile(\n+    mode=\"max-autotune-no-cudagraphs\", \n+    dynamic=True\n+)                                       # <--- Compile with max-autotune-no-cudagraphs\n+\n+prompt = \"A cat and a dog baking a cake together in a kitchen.\"\n+negative_prompt = \"Static, 2D cartoon, cartoon, 2d animation, paintings, images, worst quality, low quality, ugly, deformed, walking backwards\"\n+\n+output = pipe(\n+    prompt=prompt,\n+    negative_prompt=negative_prompt,\n+    height=512,\n+    width=768,\n+    num_frames=241,\n+    num_inference_steps=50,\n+    guidance_scale=5.0,\n+).frames[0]\n+\n+export_to_video(output, \"output.mp4\", fps=24, quality=9)\n+```\n+\n+### Diffusion Distilled model\n+**\u26a0\ufe0f Warning!** all nocfg and diffusion distilled models should be infered wothout CFG (```guidance_scale=1.0```):\n+\n+```python\n+model_id = \"ai-forever/Kandinsky-5.0-T2V-Lite-distilled16steps-5s-Diffusers\"\n+pipe = Kandinsky5T2VPipeline.from_pretrained(model_id, torch_dtype=torch.bfloat16)\n+pipe = pipe.to(\"cuda\")\n+\n+output = pipe(\n+    prompt=\"A beautiful sunset over mountains\",\n+    num_inference_steps=16,  # <--- Model is distilled in 16 steps\n+    guidance_scale=1.0,      # <--- no CFG\n+).frames[0]\n+\n+export_to_video(output, \"output.mp4\", fps=24, quality=9)\n+```\n+\n+\n+## Citation\n+```bibtex\n+@misc{kandinsky2025,\n+    author = {Alexey Letunovskiy and Maria Kovaleva and Ivan Kirillov and Lev Novitskiy and Denis Koposov and\n+              Dmitrii Mikhailov and Anna Averchenkova and Andrey Shutkin and Julia Agafonova and Olga Kim and\n+              Anastasiia Kargapoltseva and Nikita Kiselev and Vladimir Arkhipkin and Vladimir Korviakov and\n+              Nikolai Gerasimenko and Denis Parkhomenko and Anna Dmitrienko and Anastasia Maltseva and\n+              Kirill Chernyshev and Ilia Vasiliev and Viacheslav Vasilev and Vladimir Polovnikov and\n+              Yury Kolabushin and Alexander Belykh and Mikhail Mamaev and Anastasia Aliaskina and\n+              Tatiana Nikulina and Polina Gavrilova and Denis Dimitrov},\n+    title = {Kandinsky 5.0: A family of diffusion models for Video & Image generation},\n+    howpublished = {\\url{https://github.com/ai-forever/Kandinsky-5}},\n+    year = 2025\n+}\n+```\n\\ No newline at end of file"
        },
        {
          "filename": "src/diffusers/models/transformers/transformer_kandinsky.py",
          "status": "modified",
          "additions": 2,
          "deletions": 0,
          "changes": 2,
          "patch": "@@ -324,6 +324,7 @@ def apply_rotary(x, rope):\n                 sparse_params[\"sta_mask\"],\n                 thr=sparse_params[\"P\"],\n             )\n+\n         else:\n             attn_mask = None\n \n@@ -335,6 +336,7 @@ def apply_rotary(x, rope):\n             backend=self._attention_backend,\n             parallel_config=self._parallel_config,\n         )\n+\n         hidden_states = hidden_states.flatten(-2, -1)\n \n         attn_out = attn.out_layer(hidden_states)"
        },
        {
          "filename": "src/diffusers/pipelines/kandinsky5/pipeline_kandinsky.py",
          "status": "modified",
          "additions": 7,
          "deletions": 2,
          "changes": 9,
          "patch": "@@ -173,8 +173,10 @@ def __init__(\n         )\n         self.prompt_template_encode_start_idx = 129\n \n-        self.vae_scale_factor_temporal = vae.config.temporal_compression_ratio\n-        self.vae_scale_factor_spatial = vae.config.spatial_compression_ratio\n+        self.vae_scale_factor_temporal = (\n+            self.vae.config.temporal_compression_ratio if getattr(self, \"vae\", None) else 4\n+        )\n+        self.vae_scale_factor_spatial = self.vae.config.spatial_compression_ratio if getattr(self, \"vae\", None) else 8\n         self.video_processor = VideoProcessor(vae_scale_factor=self.vae_scale_factor_spatial)\n \n     @staticmethod\n@@ -384,6 +386,9 @@ def encode_prompt(\n         device = device or self._execution_device\n         dtype = dtype or self.text_encoder.dtype\n \n+        if not isinstance(prompt, list):\n+            prompt = [prompt]\n+\n         batch_size = len(prompt)\n \n         prompt = [prompt_clean(p) for p in prompt]"
        },
        {
          "filename": "tests/pipelines/kandinsky5/__init__.py",
          "status": "added",
          "additions": 0,
          "deletions": 0,
          "changes": 0,
          "patch": ""
        },
        {
          "filename": "tests/pipelines/kandinsky5/test_kandinsky5.py",
          "status": "added",
          "additions": 306,
          "deletions": 0,
          "changes": 306,
          "patch": "@@ -0,0 +1,306 @@\n+# Copyright 2025 The Kandinsky Team and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import unittest\n+\n+import torch\n+from transformers import (\n+    CLIPTextConfig,\n+    CLIPTextModel,\n+    CLIPTokenizer,\n+    Qwen2_5_VLConfig,\n+    Qwen2_5_VLForConditionalGeneration,\n+    Qwen2VLProcessor,\n+)\n+\n+from diffusers import (\n+    AutoencoderKLHunyuanVideo,\n+    FlowMatchEulerDiscreteScheduler,\n+    Kandinsky5T2VPipeline,\n+    Kandinsky5Transformer3DModel,\n+)\n+\n+from ...testing_utils import (\n+    enable_full_determinism,\n+    torch_device,\n+)\n+from ..pipeline_params import TEXT_TO_IMAGE_BATCH_PARAMS, TEXT_TO_IMAGE_PARAMS\n+from ..test_pipelines_common import PipelineTesterMixin\n+\n+\n+enable_full_determinism()\n+\n+\n+class Kandinsky5T2VPipelineFastTests(PipelineTesterMixin, unittest.TestCase):\n+    pipeline_class = Kandinsky5T2VPipeline\n+    params = TEXT_TO_IMAGE_PARAMS - {\"cross_attention_kwargs\", \"prompt_embeds\", \"negative_prompt_embeds\"}\n+    batch_params = TEXT_TO_IMAGE_BATCH_PARAMS\n+\n+    # Define required optional parameters for your pipeline\n+    required_optional_params = frozenset(\n+        [\n+            \"num_inference_steps\",\n+            \"generator\",\n+            \"latents\",\n+            \"return_dict\",\n+            \"callback_on_step_end\",\n+            \"callback_on_step_end_tensor_inputs\",\n+            \"max_sequence_length\",\n+        ]\n+    )\n+\n+    test_xformers_attention = False\n+    supports_dduf = False\n+\n+    def get_dummy_components(self):\n+        torch.manual_seed(0)\n+        vae = AutoencoderKLHunyuanVideo(\n+            in_channels=3,\n+            out_channels=3,\n+            spatial_compression_ratio=8,\n+            temporal_compression_ratio=4,\n+            latent_channels=4,\n+            block_out_channels=(8, 8, 8, 8),\n+            layers_per_block=1,\n+            norm_num_groups=4,\n+        )\n+\n+        torch.manual_seed(0)\n+        scheduler = FlowMatchEulerDiscreteScheduler(shift=7.0)\n+\n+        # Dummy Qwen2.5-VL model\n+        config = Qwen2_5_VLConfig(\n+            text_config={\n+                \"hidden_size\": 16,\n+                \"intermediate_size\": 16,\n+                \"num_hidden_layers\": 2,\n+                \"num_attention_heads\": 2,\n+                \"num_key_value_heads\": 2,\n+                \"rope_scaling\": {\n+                    \"mrope_section\": [1, 1, 2],\n+                    \"rope_type\": \"default\",\n+                    \"type\": \"default\",\n+                },\n+                \"rope_theta\": 1000000.0,\n+            },\n+            vision_config={\n+                \"depth\": 2,\n+                \"hidden_size\": 16,\n+                \"intermediate_size\": 16,\n+                \"num_heads\": 2,\n+                \"out_hidden_size\": 16,\n+            },\n+            hidden_size=16,\n+            vocab_size=152064,\n+            vision_end_token_id=151653,\n+            vision_start_token_id=151652,\n+            vision_token_id=151654,\n+        )\n+        text_encoder = Qwen2_5_VLForConditionalGeneration(config)\n+        tokenizer = Qwen2VLProcessor.from_pretrained(\"hf-internal-testing/tiny-random-Qwen2VLForConditionalGeneration\")\n+\n+        # Dummy CLIP model\n+        clip_text_encoder_config = CLIPTextConfig(\n+            bos_token_id=0,\n+            eos_token_id=2,\n+            hidden_size=32,\n+            intermediate_size=37,\n+            layer_norm_eps=1e-05,\n+            num_attention_heads=4,\n+            num_hidden_layers=5,\n+            pad_token_id=1,\n+            vocab_size=1000,\n+            hidden_act=\"gelu\",\n+            projection_dim=32,\n+        )\n+\n+        torch.manual_seed(0)\n+        text_encoder_2 = CLIPTextModel(clip_text_encoder_config)\n+        tokenizer_2 = CLIPTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-clip\")\n+\n+        torch.manual_seed(0)\n+        transformer = Kandinsky5Transformer3DModel(\n+            in_visual_dim=4,\n+            in_text_dim=16,  # Match tiny Qwen2.5-VL hidden size\n+            in_text_dim2=32,  # Match tiny CLIP hidden size\n+            time_dim=32,\n+            out_visual_dim=4,\n+            patch_size=(1, 2, 2),\n+            model_dim=48,\n+            ff_dim=128,\n+            num_text_blocks=1,\n+            num_visual_blocks=1,\n+            axes_dims=(8, 8, 8),\n+            visual_cond=False,\n+        )\n+\n+        components = {\n+            \"transformer\": transformer.eval(),\n+            \"vae\": vae.eval(),\n+            \"scheduler\": scheduler,\n+            \"text_encoder\": text_encoder.eval(),\n+            \"tokenizer\": tokenizer,\n+            \"text_encoder_2\": text_encoder_2.eval(),\n+            \"tokenizer_2\": tokenizer_2,\n+        }\n+        return components\n+\n+    def get_dummy_inputs(self, device, seed=0):\n+        if str(device).startswith(\"mps\"):\n+            generator = torch.manual_seed(seed)\n+        else:\n+            generator = torch.Generator(device=device).manual_seed(seed)\n+        inputs = {\n+            \"prompt\": \"A cat dancing\",\n+            \"negative_prompt\": \"blurry, low quality\",\n+            \"generator\": generator,\n+            \"num_inference_steps\": 2,\n+            \"guidance_scale\": 5.0,\n+            \"height\": 32,\n+            \"width\": 32,\n+            \"num_frames\": 5,\n+            \"max_sequence_length\": 16,\n+            \"output_type\": \"pt\",\n+        }\n+        return inputs\n+\n+    def test_inference(self):\n+        device = \"cpu\"\n+\n+        components = self.get_dummy_components()\n+        pipe = self.pipeline_class(**components)\n+        pipe.to(device)\n+        pipe.set_progress_bar_config(disable=None)\n+\n+        inputs = self.get_dummy_inputs(device)\n+        video = pipe(**inputs).frames\n+\n+        # Check video shape: (batch, frames, channel, height, width)\n+        expected_shape = (1, 5, 3, 32, 32)\n+        self.assertEqual(video.shape, expected_shape)\n+\n+        # Check specific values\n+        expected_slice = torch.tensor(\n+            [\n+                0.4330,\n+                0.4254,\n+                0.4285,\n+                0.3835,\n+                0.4253,\n+                0.4196,\n+                0.3704,\n+                0.3714,\n+                0.4999,\n+                0.5346,\n+                0.4795,\n+                0.4637,\n+                0.4930,\n+                0.5124,\n+                0.4902,\n+                0.4570,\n+            ]\n+        )\n+\n+        generated_slice = video.flatten()\n+        # Take first 8 and last 8 values for comparison\n+        video_slice = torch.cat([generated_slice[:8], generated_slice[-8:]])\n+        self.assertTrue(\n+            torch.allclose(video_slice, expected_slice, atol=1e-3),\n+            f\"video_slice: {video_slice}, expected_slice: {expected_slice}\",\n+        )\n+\n+    def test_inference_batch_single_identical(self):\n+        # Override to test batch single identical with video\n+        super().test_inference_batch_single_identical(batch_size=2, expected_max_diff=1e-2)\n+\n+    def test_encode_prompt_works_in_isolation(self, extra_required_param_value_dict=None, atol=1e-3, rtol=1e-3):\n+        components = self.get_dummy_components()\n+\n+        text_component_names = [\"text_encoder\", \"text_encoder_2\", \"tokenizer\", \"tokenizer_2\"]\n+        text_components = {k: (v if k in text_component_names else None) for k, v in components.items()}\n+        non_text_components = {k: (v if k not in text_component_names else None) for k, v in components.items()}\n+\n+        pipe_with_just_text_encoder = self.pipeline_class(**text_components)\n+        pipe_with_just_text_encoder = pipe_with_just_text_encoder.to(torch_device)\n+\n+        pipe_without_text_encoders = self.pipeline_class(**non_text_components)\n+        pipe_without_text_encoders = pipe_without_text_encoders.to(torch_device)\n+\n+        pipe = self.pipeline_class(**components)\n+        pipe = pipe.to(torch_device)\n+\n+        # Compute `encode_prompt()`.\n+\n+        # Test single prompt\n+        prompt = \"A cat dancing\"\n+        with torch.no_grad():\n+            prompt_embeds_qwen, prompt_embeds_clip, prompt_cu_seqlens = pipe_with_just_text_encoder.encode_prompt(\n+                prompt, device=torch_device, max_sequence_length=16\n+            )\n+\n+        # Check shapes\n+        self.assertEqual(prompt_embeds_qwen.shape, (1, 4, 16))  # [batch, seq_len, embed_dim]\n+        self.assertEqual(prompt_embeds_clip.shape, (1, 32))  # [batch, embed_dim]\n+        self.assertEqual(prompt_cu_seqlens.shape, (2,))  # [batch + 1]\n+\n+        # Test batch of prompts\n+        prompts = [\"A cat dancing\", \"A dog running\"]\n+        with torch.no_grad():\n+            batch_embeds_qwen, batch_embeds_clip, batch_cu_seqlens = pipe_with_just_text_encoder.encode_prompt(\n+                prompts, device=torch_device, max_sequence_length=16\n+            )\n+\n+        # Check batch size\n+        self.assertEqual(batch_embeds_qwen.shape, (len(prompts), 4, 16))\n+        self.assertEqual(batch_embeds_clip.shape, (len(prompts), 32))\n+        self.assertEqual(len(batch_cu_seqlens), len(prompts) + 1)  # [0, len1, len1+len2]\n+\n+        inputs = self.get_dummy_inputs(torch_device)\n+        inputs[\"guidance_scale\"] = 1.0\n+\n+        # baseline output: full pipeline\n+        pipe_out = pipe(**inputs).frames\n+\n+        # test against pipeline call with pre-computed prompt embeds\n+        inputs = self.get_dummy_inputs(torch_device)\n+        inputs[\"guidance_scale\"] = 1.0\n+\n+        with torch.no_grad():\n+            prompt_embeds_qwen, prompt_embeds_clip, prompt_cu_seqlens = pipe_with_just_text_encoder.encode_prompt(\n+                inputs[\"prompt\"], device=torch_device, max_sequence_length=inputs[\"max_sequence_length\"]\n+            )\n+\n+        inputs[\"prompt\"] = None\n+        inputs[\"prompt_embeds_qwen\"] = prompt_embeds_qwen\n+        inputs[\"prompt_embeds_clip\"] = prompt_embeds_clip\n+        inputs[\"prompt_cu_seqlens\"] = prompt_cu_seqlens\n+\n+        pipe_out_2 = pipe_without_text_encoders(**inputs)[0]\n+\n+        self.assertTrue(\n+            torch.allclose(pipe_out, pipe_out_2, atol=atol, rtol=rtol),\n+            f\"max diff: {torch.max(torch.abs(pipe_out - pipe_out_2))}\",\n+        )\n+\n+    @unittest.skip(\"Kandinsky5T2VPipeline does not support attention slicing\")\n+    def test_attention_slicing_forward_pass(self):\n+        pass\n+\n+    @unittest.skip(\"Kandinsky5T2VPipeline does not support xformers\")\n+    def test_xformers_attention_forwardGenerator_pass(self):\n+        pass\n+\n+    @unittest.skip(\"Kandinsky5T2VPipeline does not support VAE slicing\")\n+    def test_vae_slicing(self):\n+        pass"
        },
        {
          "filename": "tests/pipelines/test_pipelines_common.py",
          "status": "modified",
          "additions": 2,
          "deletions": 0,
          "changes": 2,
          "patch": "@@ -1461,6 +1461,8 @@ def test_save_load_float16(self, expected_max_diff=1e-2):\n     def test_save_load_optional_components(self, expected_max_difference=1e-4):\n         if not hasattr(self.pipeline_class, \"_optional_components\"):\n             return\n+        if not self.pipeline_class._optional_components:\n+            return\n         components = self.get_dummy_components()\n         pipe = self.pipeline_class(**components)\n         for component in pipe.components.values():"
        }
      ],
      "num_files": 7,
      "scraped_at": "2025-11-16T21:18:54.843550"
    },
    {
      "pr_number": 12508,
      "title": "Fix Chroma attention padding order and update docs to use `lodestones/Chroma1-HD`",
      "body": "# What does this PR do?\r\nChroma inference is currently incorrect, since the padding token should be added for the transformer forward pass, not for the T5 encoder forward pass. The T5 embedding step should use the regular attention mask.\r\n\r\nThis change fixes that to align with [official code](https://github.com/lodestone-rock/flow/blob/1845f16c62b0355fe1c27ddc49c51a74b1309847/src/trainer/train_chroma.py#L532C50-L532C61) and ComfyUI.\r\n\r\nI've also snuck in an update to use the final checkpoint in the docs/comments: https://huggingface.co/lodestones/Chroma1-HD\r\n\r\n---\r\n\r\nTop is before fix, bottom is after fix. I used `lodestones/Chroma1-Base`, since it's what I had on hand. Doesn't seem to be a huge difference, except for first column. Might have a stronger effect for shorter prompts, but I didn't test.\r\n\r\n![chroma_diffusers_t5_comparison](https://github.com/user-attachments/assets/1b6e39e0-f201-429e-a1e8-66c04b1a9a1d)\r\n\r\n",
      "html_url": "https://github.com/huggingface/diffusers/pull/12508",
      "created_at": "2025-10-19T04:14:10Z",
      "merged_at": "2025-10-27T10:55:21Z",
      "merge_commit_sha": "dc6bd1511a4948ebca35b22609002bba58e71c83",
      "base_ref": "main",
      "head_sha": "d33f10298fb5afd085d2f8c409fa88cbf0615db3",
      "user": "josephrocca",
      "files": [
        {
          "filename": "docs/source/en/api/models/chroma_transformer.md",
          "status": "modified",
          "additions": 1,
          "deletions": 1,
          "changes": 2,
          "patch": "@@ -12,7 +12,7 @@ specific language governing permissions and limitations under the License.\n \n # ChromaTransformer2DModel\n \n-A modified flux Transformer model from [Chroma](https://huggingface.co/lodestones/Chroma)\n+A modified flux Transformer model from [Chroma](https://huggingface.co/lodestones/Chroma1-HD)\n \n ## ChromaTransformer2DModel\n "
        },
        {
          "filename": "docs/source/en/api/pipelines/chroma.md",
          "status": "modified",
          "additions": 7,
          "deletions": 6,
          "changes": 13,
          "patch": "@@ -19,20 +19,21 @@ specific language governing permissions and limitations under the License.\n \n Chroma is a text to image generation model based on Flux.\n \n-Original model checkpoints for Chroma can be found [here](https://huggingface.co/lodestones/Chroma).\n+Original model checkpoints for Chroma can be found here:\n+* High-resolution finetune: [lodestones/Chroma1-HD](https://huggingface.co/lodestones/Chroma1-HD)\n+* Base model: [lodestones/Chroma1-Base](https://huggingface.co/lodestones/Chroma1-Base)\n+* Original repo with progress checkpoints: [lodestones/Chroma](https://huggingface.co/lodestones/Chroma) (loading this repo with `from_pretrained` will load a Diffusers-compatible version of the `unlocked-v37` checkpoint)\n \n > [!TIP]\n > Chroma can use all the same optimizations as Flux.\n \n ## Inference\n \n-The Diffusers version of Chroma is based on the [`unlocked-v37`](https://huggingface.co/lodestones/Chroma/blob/main/chroma-unlocked-v37.safetensors) version of the original model, which is available in the [Chroma repository](https://huggingface.co/lodestones/Chroma).\n-\n ```python\n import torch\n from diffusers import ChromaPipeline\n \n-pipe = ChromaPipeline.from_pretrained(\"lodestones/Chroma\", torch_dtype=torch.bfloat16)\n+pipe = ChromaPipeline.from_pretrained(\"lodestones/Chroma1-HD\", torch_dtype=torch.bfloat16)\n pipe.enable_model_cpu_offload()\n \n prompt = [\n@@ -63,10 +64,10 @@ Then run the following example\n import torch\n from diffusers import ChromaTransformer2DModel, ChromaPipeline\n \n-model_id = \"lodestones/Chroma\"\n+model_id = \"lodestones/Chroma1-HD\"\n dtype = torch.bfloat16\n \n-transformer = ChromaTransformer2DModel.from_single_file(\"https://huggingface.co/lodestones/Chroma/blob/main/chroma-unlocked-v37.safetensors\", torch_dtype=dtype)\n+transformer = ChromaTransformer2DModel.from_single_file(\"https://huggingface.co/lodestones/Chroma1-HD/blob/main/Chroma1-HD.safetensors\", torch_dtype=dtype)\n \n pipe = ChromaPipeline.from_pretrained(model_id, transformer=transformer, torch_dtype=dtype)\n pipe.enable_model_cpu_offload()"
        },
        {
          "filename": "src/diffusers/models/transformers/transformer_chroma.py",
          "status": "modified",
          "additions": 1,
          "deletions": 1,
          "changes": 2,
          "patch": "@@ -379,7 +379,7 @@ class ChromaTransformer2DModel(\n     \"\"\"\n     The Transformer model introduced in Flux, modified for Chroma.\n \n-    Reference: https://huggingface.co/lodestones/Chroma\n+    Reference: https://huggingface.co/lodestones/Chroma1-HD\n \n     Args:\n         patch_size (`int`, defaults to `1`):"
        },
        {
          "filename": "src/diffusers/pipelines/chroma/pipeline_chroma.py",
          "status": "modified",
          "additions": 14,
          "deletions": 11,
          "changes": 25,
          "patch": "@@ -53,8 +53,8 @@\n         >>> import torch\n         >>> from diffusers import ChromaPipeline\n \n-        >>> model_id = \"lodestones/Chroma\"\n-        >>> ckpt_path = \"https://huggingface.co/lodestones/Chroma/blob/main/chroma-unlocked-v37.safetensors\"\n+        >>> model_id = \"lodestones/Chroma1-HD\"\n+        >>> ckpt_path = \"https://huggingface.co/lodestones/Chroma1-HD/blob/main/Chroma1-HD.safetensors\"\n         >>> transformer = ChromaTransformer2DModel.from_single_file(ckpt_path, torch_dtype=torch.bfloat16)\n         >>> pipe = ChromaPipeline.from_pretrained(\n         ...     model_id,\n@@ -158,7 +158,7 @@ class ChromaPipeline(\n     r\"\"\"\n     The Chroma pipeline for text-to-image generation.\n \n-    Reference: https://huggingface.co/lodestones/Chroma/\n+    Reference: https://huggingface.co/lodestones/Chroma1-HD/\n \n     Args:\n         transformer ([`ChromaTransformer2DModel`]):\n@@ -233,20 +233,23 @@ def _get_t5_prompt_embeds(\n             return_tensors=\"pt\",\n         )\n         text_input_ids = text_inputs.input_ids\n-        attention_mask = text_inputs.attention_mask.clone()\n+        tokenizer_mask = text_inputs.attention_mask\n \n-        # Chroma requires the attention mask to include one padding token\n-        seq_lengths = attention_mask.sum(dim=1)\n-        mask_indices = torch.arange(attention_mask.size(1)).unsqueeze(0).expand(batch_size, -1)\n-        attention_mask = (mask_indices <= seq_lengths.unsqueeze(1)).bool()\n+        tokenizer_mask_device = tokenizer_mask.to(device)\n \n+        # unlike FLUX, Chroma uses the attention mask when generating the T5 embedding\n         prompt_embeds = self.text_encoder(\n-            text_input_ids.to(device), output_hidden_states=False, attention_mask=attention_mask.to(device)\n+            text_input_ids.to(device),\n+            output_hidden_states=False,\n+            attention_mask=tokenizer_mask_device,\n         )[0]\n \n-        dtype = self.text_encoder.dtype\n         prompt_embeds = prompt_embeds.to(dtype=dtype, device=device)\n-        attention_mask = attention_mask.to(device=device)\n+\n+        # for the text tokens, chroma requires that all except the first padding token are masked out during the forward pass through the transformer\n+        seq_lengths = tokenizer_mask_device.sum(dim=1)\n+        mask_indices = torch.arange(tokenizer_mask_device.size(1), device=device).unsqueeze(0).expand(batch_size, -1)\n+        attention_mask = (mask_indices <= seq_lengths.unsqueeze(1)).to(dtype=dtype, device=device)\n \n         _, seq_len, _ = prompt_embeds.shape\n "
        },
        {
          "filename": "src/diffusers/pipelines/chroma/pipeline_chroma_img2img.py",
          "status": "modified",
          "additions": 12,
          "deletions": 11,
          "changes": 23,
          "patch": "@@ -53,8 +53,8 @@\n         >>> import torch\n         >>> from diffusers import ChromaTransformer2DModel, ChromaImg2ImgPipeline\n \n-        >>> model_id = \"lodestones/Chroma\"\n-        >>> ckpt_path = \"https://huggingface.co/lodestones/Chroma/blob/main/chroma-unlocked-v37.safetensors\"\n+        >>> model_id = \"lodestones/Chroma1-HD\"\n+        >>> ckpt_path = \"https://huggingface.co/lodestones/Chroma1-HD/blob/main/Chroma1-HD.safetensors\"\n         >>> pipe = ChromaImg2ImgPipeline.from_pretrained(\n         ...     model_id,\n         ...     transformer=transformer,\n@@ -170,7 +170,7 @@ class ChromaImg2ImgPipeline(\n     r\"\"\"\n     The Chroma pipeline for image-to-image generation.\n \n-    Reference: https://huggingface.co/lodestones/Chroma/\n+    Reference: https://huggingface.co/lodestones/Chroma1-HD/\n \n     Args:\n         transformer ([`ChromaTransformer2DModel`]):\n@@ -247,20 +247,21 @@ def _get_t5_prompt_embeds(\n             return_tensors=\"pt\",\n         )\n         text_input_ids = text_inputs.input_ids\n-        attention_mask = text_inputs.attention_mask.clone()\n+        tokenizer_mask = text_inputs.attention_mask\n \n-        # Chroma requires the attention mask to include one padding token\n-        seq_lengths = attention_mask.sum(dim=1)\n-        mask_indices = torch.arange(attention_mask.size(1)).unsqueeze(0).expand(batch_size, -1)\n-        attention_mask = (mask_indices <= seq_lengths.unsqueeze(1)).long()\n+        tokenizer_mask_device = tokenizer_mask.to(device)\n \n         prompt_embeds = self.text_encoder(\n-            text_input_ids.to(device), output_hidden_states=False, attention_mask=attention_mask.to(device)\n+            text_input_ids.to(device),\n+            output_hidden_states=False,\n+            attention_mask=tokenizer_mask_device,\n         )[0]\n \n-        dtype = self.text_encoder.dtype\n         prompt_embeds = prompt_embeds.to(dtype=dtype, device=device)\n-        attention_mask = attention_mask.to(dtype=dtype, device=device)\n+\n+        seq_lengths = tokenizer_mask_device.sum(dim=1)\n+        mask_indices = torch.arange(tokenizer_mask_device.size(1), device=device).unsqueeze(0).expand(batch_size, -1)\n+        attention_mask = (mask_indices <= seq_lengths.unsqueeze(1)).to(dtype=dtype, device=device)\n \n         _, seq_len, _ = prompt_embeds.shape\n "
        }
      ],
      "num_files": 5,
      "scraped_at": "2025-11-16T21:18:56.472323"
    },
    {
      "pr_number": 12478,
      "title": "Kandinsky 5 is finally in Diffusers!",
      "body": "# What does this PR do?\r\n\r\nThis PR adds Kandinsky5T2VPipeline and Kandinsky5Transformer3DModel as well as several layer classes neede for Kandinsky 5.0 Lite T2V model\r\n\r\n@sayakpaul Please review",
      "html_url": "https://github.com/huggingface/diffusers/pull/12478",
      "created_at": "2025-10-13T22:43:57Z",
      "merged_at": "2025-10-18T04:34:30Z",
      "merge_commit_sha": "23ebbb4bc81a17ebea17cb7cb94f301199e49a7f",
      "base_ref": "main",
      "head_sha": "ecbe522399e61b61b2ff26658bd5090d849bb190",
      "user": "leffff",
      "files": [
        {
          "filename": "docs/source/en/api/loaders/lora.md",
          "status": "modified",
          "additions": 3,
          "deletions": 0,
          "changes": 3,
          "patch": "@@ -107,6 +107,9 @@ LoRA is a fast and lightweight training method that inserts and trains a signifi\n \n [[autodoc]] loaders.lora_pipeline.QwenImageLoraLoaderMixin\n \n+## KandinskyLoraLoaderMixin\n+[[autodoc]] loaders.lora_pipeline.KandinskyLoraLoaderMixin\n+\n ## LoraBaseMixin\n \n [[autodoc]] loaders.lora_base.LoraBaseMixin\n\\ No newline at end of file"
        },
        {
          "filename": "src/diffusers/__init__.py",
          "status": "modified",
          "additions": 4,
          "deletions": 0,
          "changes": 4,
          "patch": "@@ -220,6 +220,7 @@\n             \"HunyuanVideoTransformer3DModel\",\n             \"I2VGenXLUNet\",\n             \"Kandinsky3UNet\",\n+            \"Kandinsky5Transformer3DModel\",\n             \"LatteTransformer3DModel\",\n             \"LTXVideoTransformer3DModel\",\n             \"Lumina2Transformer2DModel\",\n@@ -474,6 +475,7 @@\n             \"ImageTextPipelineOutput\",\n             \"Kandinsky3Img2ImgPipeline\",\n             \"Kandinsky3Pipeline\",\n+            \"Kandinsky5T2VPipeline\",\n             \"KandinskyCombinedPipeline\",\n             \"KandinskyImg2ImgCombinedPipeline\",\n             \"KandinskyImg2ImgPipeline\",\n@@ -912,6 +914,7 @@\n             HunyuanVideoTransformer3DModel,\n             I2VGenXLUNet,\n             Kandinsky3UNet,\n+            Kandinsky5Transformer3DModel,\n             LatteTransformer3DModel,\n             LTXVideoTransformer3DModel,\n             Lumina2Transformer2DModel,\n@@ -1136,6 +1139,7 @@\n             ImageTextPipelineOutput,\n             Kandinsky3Img2ImgPipeline,\n             Kandinsky3Pipeline,\n+            Kandinsky5T2VPipeline,\n             KandinskyCombinedPipeline,\n             KandinskyImg2ImgCombinedPipeline,\n             KandinskyImg2ImgPipeline,"
        },
        {
          "filename": "src/diffusers/loaders/__init__.py",
          "status": "modified",
          "additions": 2,
          "deletions": 0,
          "changes": 2,
          "patch": "@@ -77,6 +77,7 @@ def text_encoder_attn_modules(text_encoder):\n             \"SanaLoraLoaderMixin\",\n             \"Lumina2LoraLoaderMixin\",\n             \"WanLoraLoaderMixin\",\n+            \"KandinskyLoraLoaderMixin\",\n             \"HiDreamImageLoraLoaderMixin\",\n             \"SkyReelsV2LoraLoaderMixin\",\n             \"QwenImageLoraLoaderMixin\",\n@@ -115,6 +116,7 @@ def text_encoder_attn_modules(text_encoder):\n                 FluxLoraLoaderMixin,\n                 HiDreamImageLoraLoaderMixin,\n                 HunyuanVideoLoraLoaderMixin,\n+                KandinskyLoraLoaderMixin,\n                 LoraLoaderMixin,\n                 LTXVideoLoraLoaderMixin,\n                 Lumina2LoraLoaderMixin,"
        },
        {
          "filename": "src/diffusers/loaders/lora_pipeline.py",
          "status": "modified",
          "additions": 285,
          "deletions": 0,
          "changes": 285,
          "patch": "@@ -3639,6 +3639,291 @@ def unfuse_lora(self, components: List[str] = [\"transformer\"], **kwargs):\n         super().unfuse_lora(components=components, **kwargs)\n \n \n+class KandinskyLoraLoaderMixin(LoraBaseMixin):\n+    r\"\"\"\n+    Load LoRA layers into [`Kandinsky5Transformer3DModel`],\n+    \"\"\"\n+\n+    _lora_loadable_modules = [\"transformer\"]\n+    transformer_name = TRANSFORMER_NAME\n+\n+    @classmethod\n+    @validate_hf_hub_args\n+    def lora_state_dict(\n+        cls,\n+        pretrained_model_name_or_path_or_dict: Union[str, Dict[str, torch.Tensor]],\n+        **kwargs,\n+    ):\n+        r\"\"\"\n+        Return state dict for lora weights and the network alphas.\n+\n+        Parameters:\n+            pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):\n+                Can be either:\n+                    - A string, the *model id* of a pretrained model hosted on the Hub.\n+                    - A path to a *directory* containing the model weights.\n+                    - A [torch state\n+                      dict](https://pytorch.org/tutorials/beginner/saving_loading_models.html#what-is-a-state-dict).\n+\n+            cache_dir (`Union[str, os.PathLike]`, *optional*):\n+                Path to a directory where a downloaded pretrained model configuration is cached.\n+            force_download (`bool`, *optional*, defaults to `False`):\n+                Whether or not to force the (re-)download of the model weights.\n+            proxies (`Dict[str, str]`, *optional*):\n+                A dictionary of proxy servers to use by protocol or endpoint.\n+            local_files_only (`bool`, *optional*, defaults to `False`):\n+                Whether to only load local model weights and configuration files.\n+            token (`str` or *bool*, *optional*):\n+                The token to use as HTTP bearer authorization for remote files.\n+            revision (`str`, *optional*, defaults to `\"main\"`):\n+                The specific model version to use.\n+            subfolder (`str`, *optional*, defaults to `\"\"`):\n+                The subfolder location of a model file within a larger model repository.\n+            weight_name (`str`, *optional*, defaults to None):\n+                Name of the serialized state dict file.\n+            use_safetensors (`bool`, *optional*):\n+                Whether to use safetensors for loading.\n+            return_lora_metadata (`bool`, *optional*, defaults to False):\n+                When enabled, additionally return the LoRA adapter metadata.\n+        \"\"\"\n+        # Load the main state dict first which has the LoRA layers\n+        cache_dir = kwargs.pop(\"cache_dir\", None)\n+        force_download = kwargs.pop(\"force_download\", False)\n+        proxies = kwargs.pop(\"proxies\", None)\n+        local_files_only = kwargs.pop(\"local_files_only\", None)\n+        token = kwargs.pop(\"token\", None)\n+        revision = kwargs.pop(\"revision\", None)\n+        subfolder = kwargs.pop(\"subfolder\", None)\n+        weight_name = kwargs.pop(\"weight_name\", None)\n+        use_safetensors = kwargs.pop(\"use_safetensors\", None)\n+        return_lora_metadata = kwargs.pop(\"return_lora_metadata\", False)\n+\n+        allow_pickle = False\n+        if use_safetensors is None:\n+            use_safetensors = True\n+            allow_pickle = True\n+\n+        user_agent = {\"file_type\": \"attn_procs_weights\", \"framework\": \"pytorch\"}\n+\n+        state_dict, metadata = _fetch_state_dict(\n+            pretrained_model_name_or_path_or_dict=pretrained_model_name_or_path_or_dict,\n+            weight_name=weight_name,\n+            use_safetensors=use_safetensors,\n+            local_files_only=local_files_only,\n+            cache_dir=cache_dir,\n+            force_download=force_download,\n+            proxies=proxies,\n+            token=token,\n+            revision=revision,\n+            subfolder=subfolder,\n+            user_agent=user_agent,\n+            allow_pickle=allow_pickle,\n+        )\n+\n+        is_dora_scale_present = any(\"dora_scale\" in k for k in state_dict)\n+        if is_dora_scale_present:\n+            warn_msg = \"It seems like you are using a DoRA checkpoint that is not compatible in Diffusers at the moment. So, we are going to filter out the keys associated to 'dora_scale` from the state dict. If you think this is a mistake please open an issue https://github.com/huggingface/diffusers/issues/new.\"\n+            logger.warning(warn_msg)\n+            state_dict = {k: v for k, v in state_dict.items() if \"dora_scale\" not in k}\n+\n+        out = (state_dict, metadata) if return_lora_metadata else state_dict\n+        return out\n+\n+    def load_lora_weights(\n+        self,\n+        pretrained_model_name_or_path_or_dict: Union[str, Dict[str, torch.Tensor]],\n+        adapter_name: Optional[str] = None,\n+        hotswap: bool = False,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Load LoRA weights specified in `pretrained_model_name_or_path_or_dict` into `self.transformer`\n+\n+        Parameters:\n+            pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):\n+                See [`~loaders.KandinskyLoraLoaderMixin.lora_state_dict`].\n+            adapter_name (`str`, *optional*):\n+                Adapter name to be used for referencing the loaded adapter model.\n+            hotswap (`bool`, *optional*):\n+                Whether to substitute an existing (LoRA) adapter with the newly loaded adapter in-place.\n+            low_cpu_mem_usage (`bool`, *optional*):\n+                Speed up model loading by only loading the pretrained LoRA weights and not initializing the random\n+                weights.\n+            kwargs (`dict`, *optional*):\n+                See [`~loaders.KandinskyLoraLoaderMixin.lora_state_dict`].\n+        \"\"\"\n+        if not USE_PEFT_BACKEND:\n+            raise ValueError(\"PEFT backend is required for this method.\")\n+\n+        low_cpu_mem_usage = kwargs.pop(\"low_cpu_mem_usage\", _LOW_CPU_MEM_USAGE_DEFAULT_LORA)\n+        if low_cpu_mem_usage and not is_peft_version(\">=\", \"0.13.1\"):\n+            raise ValueError(\n+                \"`low_cpu_mem_usage=True` is not compatible with this `peft` version. Please update it with `pip install -U peft`.\"\n+            )\n+\n+        # if a dict is passed, copy it instead of modifying it inplace\n+        if isinstance(pretrained_model_name_or_path_or_dict, dict):\n+            pretrained_model_name_or_path_or_dict = pretrained_model_name_or_path_or_dict.copy()\n+\n+        # First, ensure that the checkpoint is a compatible one and can be successfully loaded.\n+        kwargs[\"return_lora_metadata\"] = True\n+        state_dict, metadata = self.lora_state_dict(pretrained_model_name_or_path_or_dict, **kwargs)\n+\n+        is_correct_format = all(\"lora\" in key for key in state_dict.keys())\n+        if not is_correct_format:\n+            raise ValueError(\"Invalid LoRA checkpoint.\")\n+\n+        # Load LoRA into transformer\n+        self.load_lora_into_transformer(\n+            state_dict,\n+            transformer=getattr(self, self.transformer_name) if not hasattr(self, \"transformer\") else self.transformer,\n+            adapter_name=adapter_name,\n+            metadata=metadata,\n+            _pipeline=self,\n+            low_cpu_mem_usage=low_cpu_mem_usage,\n+            hotswap=hotswap,\n+        )\n+\n+    @classmethod\n+    def load_lora_into_transformer(\n+        cls,\n+        state_dict,\n+        transformer,\n+        adapter_name=None,\n+        _pipeline=None,\n+        low_cpu_mem_usage=False,\n+        hotswap: bool = False,\n+        metadata=None,\n+    ):\n+        \"\"\"\n+        Load the LoRA layers specified in `state_dict` into `transformer`.\n+\n+        Parameters:\n+            state_dict (`dict`):\n+                A standard state dict containing the lora layer parameters.\n+            transformer (`Kandinsky5Transformer3DModel`):\n+                The transformer model to load the LoRA layers into.\n+            adapter_name (`str`, *optional*):\n+                Adapter name to be used for referencing the loaded adapter model.\n+            low_cpu_mem_usage (`bool`, *optional*):\n+                Speed up model loading by only loading the pretrained LoRA weights.\n+            hotswap (`bool`, *optional*):\n+                See [`~loaders.KandinskyLoraLoaderMixin.load_lora_weights`].\n+            metadata (`dict`):\n+                Optional LoRA adapter metadata.\n+        \"\"\"\n+        if low_cpu_mem_usage and not is_peft_version(\">=\", \"0.13.1\"):\n+            raise ValueError(\n+                \"`low_cpu_mem_usage=True` is not compatible with this `peft` version. Please update it with `pip install -U peft`.\"\n+            )\n+\n+        # Load the layers corresponding to transformer.\n+        logger.info(f\"Loading {cls.transformer_name}.\")\n+        transformer.load_lora_adapter(\n+            state_dict,\n+            network_alphas=None,\n+            adapter_name=adapter_name,\n+            metadata=metadata,\n+            _pipeline=_pipeline,\n+            low_cpu_mem_usage=low_cpu_mem_usage,\n+            hotswap=hotswap,\n+        )\n+\n+    @classmethod\n+    def save_lora_weights(\n+        cls,\n+        save_directory: Union[str, os.PathLike],\n+        transformer_lora_layers: Dict[str, Union[torch.nn.Module, torch.Tensor]] = None,\n+        is_main_process: bool = True,\n+        weight_name: str = None,\n+        save_function: Callable = None,\n+        safe_serialization: bool = True,\n+        transformer_lora_adapter_metadata=None,\n+    ):\n+        r\"\"\"\n+        Save the LoRA parameters corresponding to the transformer and text encoders.\n+\n+        Arguments:\n+            save_directory (`str` or `os.PathLike`):\n+                Directory to save LoRA parameters to.\n+            transformer_lora_layers (`Dict[str, torch.nn.Module]` or `Dict[str, torch.Tensor]`):\n+                State dict of the LoRA layers corresponding to the `transformer`.\n+            is_main_process (`bool`, *optional*, defaults to `True`):\n+                Whether the process calling this is the main process.\n+            save_function (`Callable`):\n+                The function to use to save the state dictionary.\n+            safe_serialization (`bool`, *optional*, defaults to `True`):\n+                Whether to save the model using `safetensors` or the traditional PyTorch way.\n+            transformer_lora_adapter_metadata:\n+                LoRA adapter metadata associated with the transformer.\n+        \"\"\"\n+        lora_layers = {}\n+        lora_metadata = {}\n+\n+        if transformer_lora_layers:\n+            lora_layers[cls.transformer_name] = transformer_lora_layers\n+            lora_metadata[cls.transformer_name] = transformer_lora_adapter_metadata\n+\n+        if not lora_layers:\n+            raise ValueError(\"You must pass at least one of `transformer_lora_layers`\")\n+\n+        cls._save_lora_weights(\n+            save_directory=save_directory,\n+            lora_layers=lora_layers,\n+            lora_metadata=lora_metadata,\n+            is_main_process=is_main_process,\n+            weight_name=weight_name,\n+            save_function=save_function,\n+            safe_serialization=safe_serialization,\n+        )\n+\n+    def fuse_lora(\n+        self,\n+        components: List[str] = [\"transformer\"],\n+        lora_scale: float = 1.0,\n+        safe_fusing: bool = False,\n+        adapter_names: Optional[List[str]] = None,\n+        **kwargs,\n+    ):\n+        r\"\"\"\n+        Fuses the LoRA parameters into the original parameters of the corresponding blocks.\n+\n+        Args:\n+            components: (`List[str]`): List of LoRA-injectable components to fuse the LoRAs into.\n+            lora_scale (`float`, defaults to 1.0):\n+                Controls how much to influence the outputs with the LoRA parameters.\n+            safe_fusing (`bool`, defaults to `False`):\n+                Whether to check fused weights for NaN values before fusing.\n+            adapter_names (`List[str]`, *optional*):\n+                Adapter names to be used for fusing.\n+\n+        Example:\n+        ```py\n+        from diffusers import Kandinsky5T2VPipeline\n+\n+        pipeline = Kandinsky5T2VPipeline.from_pretrained(\"ai-forever/Kandinsky-5.0-T2V\")\n+        pipeline.load_lora_weights(\"path/to/lora.safetensors\")\n+        pipeline.fuse_lora(lora_scale=0.7)\n+        ```\n+        \"\"\"\n+        super().fuse_lora(\n+            components=components,\n+            lora_scale=lora_scale,\n+            safe_fusing=safe_fusing,\n+            adapter_names=adapter_names,\n+            **kwargs,\n+        )\n+\n+    def unfuse_lora(self, components: List[str] = [\"transformer\"], **kwargs):\n+        r\"\"\"\n+        Reverses the effect of [`pipe.fuse_lora()`].\n+\n+        Args:\n+            components (`List[str]`): List of LoRA-injectable components to unfuse LoRA from.\n+        \"\"\"\n+        super().unfuse_lora(components=components, **kwargs)\n+\n+\n class WanLoraLoaderMixin(LoraBaseMixin):\n     r\"\"\"\n     Load LoRA layers into [`WanTransformer3DModel`]. Specific to [`WanPipeline`] and `[WanImageToVideoPipeline`]."
        },
        {
          "filename": "src/diffusers/models/__init__.py",
          "status": "modified",
          "additions": 2,
          "deletions": 0,
          "changes": 2,
          "patch": "@@ -91,6 +91,7 @@\n     _import_structure[\"transformers.transformer_hidream_image\"] = [\"HiDreamImageTransformer2DModel\"]\n     _import_structure[\"transformers.transformer_hunyuan_video\"] = [\"HunyuanVideoTransformer3DModel\"]\n     _import_structure[\"transformers.transformer_hunyuan_video_framepack\"] = [\"HunyuanVideoFramepackTransformer3DModel\"]\n+    _import_structure[\"transformers.transformer_kandinsky\"] = [\"Kandinsky5Transformer3DModel\"]\n     _import_structure[\"transformers.transformer_ltx\"] = [\"LTXVideoTransformer3DModel\"]\n     _import_structure[\"transformers.transformer_lumina2\"] = [\"Lumina2Transformer2DModel\"]\n     _import_structure[\"transformers.transformer_mochi\"] = [\"MochiTransformer3DModel\"]\n@@ -182,6 +183,7 @@\n             HunyuanDiT2DModel,\n             HunyuanVideoFramepackTransformer3DModel,\n             HunyuanVideoTransformer3DModel,\n+            Kandinsky5Transformer3DModel,\n             LatteTransformer3DModel,\n             LTXVideoTransformer3DModel,\n             Lumina2Transformer2DModel,"
        },
        {
          "filename": "src/diffusers/models/transformers/__init__.py",
          "status": "modified",
          "additions": 1,
          "deletions": 0,
          "changes": 1,
          "patch": "@@ -27,6 +27,7 @@\n     from .transformer_hidream_image import HiDreamImageTransformer2DModel\n     from .transformer_hunyuan_video import HunyuanVideoTransformer3DModel\n     from .transformer_hunyuan_video_framepack import HunyuanVideoFramepackTransformer3DModel\n+    from .transformer_kandinsky import Kandinsky5Transformer3DModel\n     from .transformer_ltx import LTXVideoTransformer3DModel\n     from .transformer_lumina2 import Lumina2Transformer2DModel\n     from .transformer_mochi import MochiTransformer3DModel"
        },
        {
          "filename": "src/diffusers/models/transformers/transformer_kandinsky.py",
          "status": "added",
          "additions": 667,
          "deletions": 0,
          "changes": 667,
          "patch": "@@ -0,0 +1,667 @@\n+# Copyright 2025 The Kandinsky Team and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import inspect\n+import math\n+from typing import Any, Dict, Optional, Tuple, Union\n+\n+import torch\n+import torch.nn as nn\n+import torch.nn.functional as F\n+from torch import Tensor\n+\n+from ...configuration_utils import ConfigMixin, register_to_config\n+from ...loaders import FromOriginalModelMixin, PeftAdapterMixin\n+from ...utils import (\n+    logging,\n+)\n+from ..attention import AttentionMixin, AttentionModuleMixin\n+from ..attention_dispatch import _CAN_USE_FLEX_ATTN, dispatch_attention_fn\n+from ..cache_utils import CacheMixin\n+from ..modeling_outputs import Transformer2DModelOutput\n+from ..modeling_utils import ModelMixin\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+def get_freqs(dim, max_period=10000.0):\n+    freqs = torch.exp(-math.log(max_period) * torch.arange(start=0, end=dim, dtype=torch.float32) / dim)\n+    return freqs\n+\n+\n+def fractal_flatten(x, rope, shape, block_mask=False):\n+    if block_mask:\n+        pixel_size = 8\n+        x = local_patching(x, shape, (1, pixel_size, pixel_size), dim=1)\n+        rope = local_patching(rope, shape, (1, pixel_size, pixel_size), dim=1)\n+        x = x.flatten(1, 2)\n+        rope = rope.flatten(1, 2)\n+    else:\n+        x = x.flatten(1, 3)\n+        rope = rope.flatten(1, 3)\n+    return x, rope\n+\n+\n+def fractal_unflatten(x, shape, block_mask=False):\n+    if block_mask:\n+        pixel_size = 8\n+        x = x.reshape(x.shape[0], -1, pixel_size**2, *x.shape[2:])\n+        x = local_merge(x, shape, (1, pixel_size, pixel_size), dim=1)\n+    else:\n+        x = x.reshape(*shape, *x.shape[2:])\n+    return x\n+\n+\n+def local_patching(x, shape, group_size, dim=0):\n+    batch_size, duration, height, width = shape\n+    g1, g2, g3 = group_size\n+    x = x.reshape(\n+        *x.shape[:dim],\n+        duration // g1,\n+        g1,\n+        height // g2,\n+        g2,\n+        width // g3,\n+        g3,\n+        *x.shape[dim + 3 :],\n+    )\n+    x = x.permute(\n+        *range(len(x.shape[:dim])),\n+        dim,\n+        dim + 2,\n+        dim + 4,\n+        dim + 1,\n+        dim + 3,\n+        dim + 5,\n+        *range(dim + 6, len(x.shape)),\n+    )\n+    x = x.flatten(dim, dim + 2).flatten(dim + 1, dim + 3)\n+    return x\n+\n+\n+def local_merge(x, shape, group_size, dim=0):\n+    batch_size, duration, height, width = shape\n+    g1, g2, g3 = group_size\n+    x = x.reshape(\n+        *x.shape[:dim],\n+        duration // g1,\n+        height // g2,\n+        width // g3,\n+        g1,\n+        g2,\n+        g3,\n+        *x.shape[dim + 2 :],\n+    )\n+    x = x.permute(\n+        *range(len(x.shape[:dim])),\n+        dim,\n+        dim + 3,\n+        dim + 1,\n+        dim + 4,\n+        dim + 2,\n+        dim + 5,\n+        *range(dim + 6, len(x.shape)),\n+    )\n+    x = x.flatten(dim, dim + 1).flatten(dim + 1, dim + 2).flatten(dim + 2, dim + 3)\n+    return x\n+\n+\n+def nablaT_v2(\n+    q: Tensor,\n+    k: Tensor,\n+    sta: Tensor,\n+    thr: float = 0.9,\n+):\n+    if _CAN_USE_FLEX_ATTN:\n+        from torch.nn.attention.flex_attention import BlockMask\n+    else:\n+        raise ValueError(\"Nabla attention is not supported with this version of PyTorch\")\n+\n+    q = q.transpose(1, 2).contiguous()\n+    k = k.transpose(1, 2).contiguous()\n+\n+    # Map estimation\n+    B, h, S, D = q.shape\n+    s1 = S // 64\n+    qa = q.reshape(B, h, s1, 64, D).mean(-2)\n+    ka = k.reshape(B, h, s1, 64, D).mean(-2).transpose(-2, -1)\n+    map = qa @ ka\n+\n+    map = torch.softmax(map / math.sqrt(D), dim=-1)\n+    # Map binarization\n+    vals, inds = map.sort(-1)\n+    cvals = vals.cumsum_(-1)\n+    mask = (cvals >= 1 - thr).int()\n+    mask = mask.gather(-1, inds.argsort(-1))\n+\n+    mask = torch.logical_or(mask, sta)\n+\n+    # BlockMask creation\n+    kv_nb = mask.sum(-1).to(torch.int32)\n+    kv_inds = mask.argsort(dim=-1, descending=True).to(torch.int32)\n+    return BlockMask.from_kv_blocks(torch.zeros_like(kv_nb), kv_inds, kv_nb, kv_inds, BLOCK_SIZE=64, mask_mod=None)\n+\n+\n+class Kandinsky5TimeEmbeddings(nn.Module):\n+    def __init__(self, model_dim, time_dim, max_period=10000.0):\n+        super().__init__()\n+        assert model_dim % 2 == 0\n+        self.model_dim = model_dim\n+        self.max_period = max_period\n+        self.freqs = get_freqs(self.model_dim // 2, self.max_period)\n+        self.in_layer = nn.Linear(model_dim, time_dim, bias=True)\n+        self.activation = nn.SiLU()\n+        self.out_layer = nn.Linear(time_dim, time_dim, bias=True)\n+\n+    @torch.autocast(device_type=\"cuda\", dtype=torch.float32)\n+    def forward(self, time):\n+        args = torch.outer(time, self.freqs.to(device=time.device))\n+        time_embed = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n+        time_embed = self.out_layer(self.activation(self.in_layer(time_embed)))\n+        return time_embed\n+\n+\n+class Kandinsky5TextEmbeddings(nn.Module):\n+    def __init__(self, text_dim, model_dim):\n+        super().__init__()\n+        self.in_layer = nn.Linear(text_dim, model_dim, bias=True)\n+        self.norm = nn.LayerNorm(model_dim, elementwise_affine=True)\n+\n+    def forward(self, text_embed):\n+        text_embed = self.in_layer(text_embed)\n+        return self.norm(text_embed).type_as(text_embed)\n+\n+\n+class Kandinsky5VisualEmbeddings(nn.Module):\n+    def __init__(self, visual_dim, model_dim, patch_size):\n+        super().__init__()\n+        self.patch_size = patch_size\n+        self.in_layer = nn.Linear(math.prod(patch_size) * visual_dim, model_dim)\n+\n+    def forward(self, x):\n+        batch_size, duration, height, width, dim = x.shape\n+        x = (\n+            x.view(\n+                batch_size,\n+                duration // self.patch_size[0],\n+                self.patch_size[0],\n+                height // self.patch_size[1],\n+                self.patch_size[1],\n+                width // self.patch_size[2],\n+                self.patch_size[2],\n+                dim,\n+            )\n+            .permute(0, 1, 3, 5, 2, 4, 6, 7)\n+            .flatten(4, 7)\n+        )\n+        return self.in_layer(x)\n+\n+\n+class Kandinsky5RoPE1D(nn.Module):\n+    def __init__(self, dim, max_pos=1024, max_period=10000.0):\n+        super().__init__()\n+        self.max_period = max_period\n+        self.dim = dim\n+        self.max_pos = max_pos\n+        freq = get_freqs(dim // 2, max_period)\n+        pos = torch.arange(max_pos, dtype=freq.dtype)\n+        self.register_buffer(\"args\", torch.outer(pos, freq), persistent=False)\n+\n+    def forward(self, pos):\n+        args = self.args[pos]\n+        cosine = torch.cos(args)\n+        sine = torch.sin(args)\n+        rope = torch.stack([cosine, -sine, sine, cosine], dim=-1)\n+        rope = rope.view(*rope.shape[:-1], 2, 2)\n+        return rope.unsqueeze(-4)\n+\n+\n+class Kandinsky5RoPE3D(nn.Module):\n+    def __init__(self, axes_dims, max_pos=(128, 128, 128), max_period=10000.0):\n+        super().__init__()\n+        self.axes_dims = axes_dims\n+        self.max_pos = max_pos\n+        self.max_period = max_period\n+\n+        for i, (axes_dim, ax_max_pos) in enumerate(zip(axes_dims, max_pos)):\n+            freq = get_freqs(axes_dim // 2, max_period)\n+            pos = torch.arange(ax_max_pos, dtype=freq.dtype)\n+            self.register_buffer(f\"args_{i}\", torch.outer(pos, freq), persistent=False)\n+\n+    def forward(self, shape, pos, scale_factor=(1.0, 1.0, 1.0)):\n+        batch_size, duration, height, width = shape\n+        args_t = self.args_0[pos[0]] / scale_factor[0]\n+        args_h = self.args_1[pos[1]] / scale_factor[1]\n+        args_w = self.args_2[pos[2]] / scale_factor[2]\n+\n+        args = torch.cat(\n+            [\n+                args_t.view(1, duration, 1, 1, -1).repeat(batch_size, 1, height, width, 1),\n+                args_h.view(1, 1, height, 1, -1).repeat(batch_size, duration, 1, width, 1),\n+                args_w.view(1, 1, 1, width, -1).repeat(batch_size, duration, height, 1, 1),\n+            ],\n+            dim=-1,\n+        )\n+        cosine = torch.cos(args)\n+        sine = torch.sin(args)\n+        rope = torch.stack([cosine, -sine, sine, cosine], dim=-1)\n+        rope = rope.view(*rope.shape[:-1], 2, 2)\n+        return rope.unsqueeze(-4)\n+\n+\n+class Kandinsky5Modulation(nn.Module):\n+    def __init__(self, time_dim, model_dim, num_params):\n+        super().__init__()\n+        self.activation = nn.SiLU()\n+        self.out_layer = nn.Linear(time_dim, num_params * model_dim)\n+        self.out_layer.weight.data.zero_()\n+        self.out_layer.bias.data.zero_()\n+\n+    @torch.autocast(device_type=\"cuda\", dtype=torch.float32)\n+    def forward(self, x):\n+        return self.out_layer(self.activation(x))\n+\n+\n+class Kandinsky5AttnProcessor:\n+    _attention_backend = None\n+    _parallel_config = None\n+\n+    def __init__(self):\n+        if not hasattr(F, \"scaled_dot_product_attention\"):\n+            raise ImportError(f\"{self.__class__.__name__} requires PyTorch 2.0. Please upgrade your pytorch version.\")\n+\n+    def __call__(self, attn, hidden_states, encoder_hidden_states=None, rotary_emb=None, sparse_params=None):\n+        # query, key, value = self.get_qkv(x)\n+        query = attn.to_query(hidden_states)\n+\n+        if encoder_hidden_states is not None:\n+            key = attn.to_key(encoder_hidden_states)\n+            value = attn.to_value(encoder_hidden_states)\n+\n+            shape, cond_shape = query.shape[:-1], key.shape[:-1]\n+            query = query.reshape(*shape, attn.num_heads, -1)\n+            key = key.reshape(*cond_shape, attn.num_heads, -1)\n+            value = value.reshape(*cond_shape, attn.num_heads, -1)\n+\n+        else:\n+            key = attn.to_key(hidden_states)\n+            value = attn.to_value(hidden_states)\n+\n+            shape = query.shape[:-1]\n+            query = query.reshape(*shape, attn.num_heads, -1)\n+            key = key.reshape(*shape, attn.num_heads, -1)\n+            value = value.reshape(*shape, attn.num_heads, -1)\n+\n+        # query, key = self.norm_qk(query, key)\n+        query = attn.query_norm(query.float()).type_as(query)\n+        key = attn.key_norm(key.float()).type_as(key)\n+\n+        def apply_rotary(x, rope):\n+            x_ = x.reshape(*x.shape[:-1], -1, 1, 2).to(torch.float32)\n+            x_out = (rope * x_).sum(dim=-1)\n+            return x_out.reshape(*x.shape).to(torch.bfloat16)\n+\n+        if rotary_emb is not None:\n+            query = apply_rotary(query, rotary_emb).type_as(query)\n+            key = apply_rotary(key, rotary_emb).type_as(key)\n+\n+        if sparse_params is not None:\n+            attn_mask = nablaT_v2(\n+                query,\n+                key,\n+                sparse_params[\"sta_mask\"],\n+                thr=sparse_params[\"P\"],\n+            )\n+        else:\n+            attn_mask = None\n+\n+        hidden_states = dispatch_attention_fn(\n+            query,\n+            key,\n+            value,\n+            attn_mask=attn_mask,\n+            backend=self._attention_backend,\n+            parallel_config=self._parallel_config,\n+        )\n+        hidden_states = hidden_states.flatten(-2, -1)\n+\n+        attn_out = attn.out_layer(hidden_states)\n+        return attn_out\n+\n+\n+class Kandinsky5Attention(nn.Module, AttentionModuleMixin):\n+    _default_processor_cls = Kandinsky5AttnProcessor\n+    _available_processors = [\n+        Kandinsky5AttnProcessor,\n+    ]\n+\n+    def __init__(self, num_channels, head_dim, processor=None):\n+        super().__init__()\n+        assert num_channels % head_dim == 0\n+        self.num_heads = num_channels // head_dim\n+\n+        self.to_query = nn.Linear(num_channels, num_channels, bias=True)\n+        self.to_key = nn.Linear(num_channels, num_channels, bias=True)\n+        self.to_value = nn.Linear(num_channels, num_channels, bias=True)\n+        self.query_norm = nn.RMSNorm(head_dim)\n+        self.key_norm = nn.RMSNorm(head_dim)\n+\n+        self.out_layer = nn.Linear(num_channels, num_channels, bias=True)\n+        if processor is None:\n+            processor = self._default_processor_cls()\n+        self.set_processor(processor)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        encoder_hidden_states: Optional[torch.Tensor] = None,\n+        sparse_params: Optional[torch.Tensor] = None,\n+        rotary_emb: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n+        **kwargs,\n+    ) -> torch.Tensor:\n+        attn_parameters = set(inspect.signature(self.processor.__call__).parameters.keys())\n+        quiet_attn_parameters = {}\n+        unused_kwargs = [k for k, _ in kwargs.items() if k not in attn_parameters and k not in quiet_attn_parameters]\n+        if len(unused_kwargs) > 0:\n+            logger.warning(\n+                f\"attention_processor_kwargs {unused_kwargs} are not expected by {self.processor.__class__.__name__} and will be ignored.\"\n+            )\n+        kwargs = {k: w for k, w in kwargs.items() if k in attn_parameters}\n+\n+        return self.processor(\n+            self,\n+            hidden_states,\n+            encoder_hidden_states=encoder_hidden_states,\n+            sparse_params=sparse_params,\n+            rotary_emb=rotary_emb,\n+            **kwargs,\n+        )\n+\n+\n+class Kandinsky5FeedForward(nn.Module):\n+    def __init__(self, dim, ff_dim):\n+        super().__init__()\n+        self.in_layer = nn.Linear(dim, ff_dim, bias=False)\n+        self.activation = nn.GELU()\n+        self.out_layer = nn.Linear(ff_dim, dim, bias=False)\n+\n+    def forward(self, x):\n+        return self.out_layer(self.activation(self.in_layer(x)))\n+\n+\n+class Kandinsky5OutLayer(nn.Module):\n+    def __init__(self, model_dim, time_dim, visual_dim, patch_size):\n+        super().__init__()\n+        self.patch_size = patch_size\n+        self.modulation = Kandinsky5Modulation(time_dim, model_dim, 2)\n+        self.norm = nn.LayerNorm(model_dim, elementwise_affine=False)\n+        self.out_layer = nn.Linear(model_dim, math.prod(patch_size) * visual_dim, bias=True)\n+\n+    def forward(self, visual_embed, text_embed, time_embed):\n+        shift, scale = torch.chunk(self.modulation(time_embed).unsqueeze(dim=1), 2, dim=-1)\n+\n+        visual_embed = (\n+            self.norm(visual_embed.float()) * (scale.float()[:, None, None] + 1.0) + shift.float()[:, None, None]\n+        ).type_as(visual_embed)\n+\n+        x = self.out_layer(visual_embed)\n+\n+        batch_size, duration, height, width, _ = x.shape\n+        x = (\n+            x.view(\n+                batch_size,\n+                duration,\n+                height,\n+                width,\n+                -1,\n+                self.patch_size[0],\n+                self.patch_size[1],\n+                self.patch_size[2],\n+            )\n+            .permute(0, 1, 5, 2, 6, 3, 7, 4)\n+            .flatten(1, 2)\n+            .flatten(2, 3)\n+            .flatten(3, 4)\n+        )\n+        return x\n+\n+\n+class Kandinsky5TransformerEncoderBlock(nn.Module):\n+    def __init__(self, model_dim, time_dim, ff_dim, head_dim):\n+        super().__init__()\n+        self.text_modulation = Kandinsky5Modulation(time_dim, model_dim, 6)\n+\n+        self.self_attention_norm = nn.LayerNorm(model_dim, elementwise_affine=False)\n+        self.self_attention = Kandinsky5Attention(model_dim, head_dim, processor=Kandinsky5AttnProcessor())\n+\n+        self.feed_forward_norm = nn.LayerNorm(model_dim, elementwise_affine=False)\n+        self.feed_forward = Kandinsky5FeedForward(model_dim, ff_dim)\n+\n+    def forward(self, x, time_embed, rope):\n+        self_attn_params, ff_params = torch.chunk(self.text_modulation(time_embed).unsqueeze(dim=1), 2, dim=-1)\n+        shift, scale, gate = torch.chunk(self_attn_params, 3, dim=-1)\n+        out = (self.self_attention_norm(x.float()) * (scale.float() + 1.0) + shift.float()).type_as(x)\n+        out = self.self_attention(out, rotary_emb=rope)\n+        x = (x.float() + gate.float() * out.float()).type_as(x)\n+\n+        shift, scale, gate = torch.chunk(ff_params, 3, dim=-1)\n+        out = (self.feed_forward_norm(x.float()) * (scale.float() + 1.0) + shift.float()).type_as(x)\n+        out = self.feed_forward(out)\n+        x = (x.float() + gate.float() * out.float()).type_as(x)\n+\n+        return x\n+\n+\n+class Kandinsky5TransformerDecoderBlock(nn.Module):\n+    def __init__(self, model_dim, time_dim, ff_dim, head_dim):\n+        super().__init__()\n+        self.visual_modulation = Kandinsky5Modulation(time_dim, model_dim, 9)\n+\n+        self.self_attention_norm = nn.LayerNorm(model_dim, elementwise_affine=False)\n+        self.self_attention = Kandinsky5Attention(model_dim, head_dim, processor=Kandinsky5AttnProcessor())\n+\n+        self.cross_attention_norm = nn.LayerNorm(model_dim, elementwise_affine=False)\n+        self.cross_attention = Kandinsky5Attention(model_dim, head_dim, processor=Kandinsky5AttnProcessor())\n+\n+        self.feed_forward_norm = nn.LayerNorm(model_dim, elementwise_affine=False)\n+        self.feed_forward = Kandinsky5FeedForward(model_dim, ff_dim)\n+\n+    def forward(self, visual_embed, text_embed, time_embed, rope, sparse_params):\n+        self_attn_params, cross_attn_params, ff_params = torch.chunk(\n+            self.visual_modulation(time_embed).unsqueeze(dim=1), 3, dim=-1\n+        )\n+\n+        shift, scale, gate = torch.chunk(self_attn_params, 3, dim=-1)\n+        visual_out = (self.self_attention_norm(visual_embed.float()) * (scale.float() + 1.0) + shift.float()).type_as(\n+            visual_embed\n+        )\n+        visual_out = self.self_attention(visual_out, rotary_emb=rope, sparse_params=sparse_params)\n+        visual_embed = (visual_embed.float() + gate.float() * visual_out.float()).type_as(visual_embed)\n+\n+        shift, scale, gate = torch.chunk(cross_attn_params, 3, dim=-1)\n+        visual_out = (self.cross_attention_norm(visual_embed.float()) * (scale.float() + 1.0) + shift.float()).type_as(\n+            visual_embed\n+        )\n+        visual_out = self.cross_attention(visual_out, encoder_hidden_states=text_embed)\n+        visual_embed = (visual_embed.float() + gate.float() * visual_out.float()).type_as(visual_embed)\n+\n+        shift, scale, gate = torch.chunk(ff_params, 3, dim=-1)\n+        visual_out = (self.feed_forward_norm(visual_embed.float()) * (scale.float() + 1.0) + shift.float()).type_as(\n+            visual_embed\n+        )\n+        visual_out = self.feed_forward(visual_out)\n+        visual_embed = (visual_embed.float() + gate.float() * visual_out.float()).type_as(visual_embed)\n+\n+        return visual_embed\n+\n+\n+class Kandinsky5Transformer3DModel(\n+    ModelMixin,\n+    ConfigMixin,\n+    PeftAdapterMixin,\n+    FromOriginalModelMixin,\n+    CacheMixin,\n+    AttentionMixin,\n+):\n+    \"\"\"\n+    A 3D Diffusion Transformer model for video-like data.\n+    \"\"\"\n+\n+    _repeated_blocks = [\n+        \"Kandinsky5TransformerEncoderBlock\",\n+        \"Kandinsky5TransformerDecoderBlock\",\n+    ]\n+    _supports_gradient_checkpointing = True\n+\n+    @register_to_config\n+    def __init__(\n+        self,\n+        in_visual_dim=4,\n+        in_text_dim=3584,\n+        in_text_dim2=768,\n+        time_dim=512,\n+        out_visual_dim=4,\n+        patch_size=(1, 2, 2),\n+        model_dim=2048,\n+        ff_dim=5120,\n+        num_text_blocks=2,\n+        num_visual_blocks=32,\n+        axes_dims=(16, 24, 24),\n+        visual_cond=False,\n+        attention_type: str = \"regular\",\n+        attention_causal: bool = None,\n+        attention_local: bool = None,\n+        attention_glob: bool = None,\n+        attention_window: int = None,\n+        attention_P: float = None,\n+        attention_wT: int = None,\n+        attention_wW: int = None,\n+        attention_wH: int = None,\n+        attention_add_sta: bool = None,\n+        attention_method: str = None,\n+    ):\n+        super().__init__()\n+\n+        head_dim = sum(axes_dims)\n+        self.in_visual_dim = in_visual_dim\n+        self.model_dim = model_dim\n+        self.patch_size = patch_size\n+        self.visual_cond = visual_cond\n+        self.attention_type = attention_type\n+\n+        visual_embed_dim = 2 * in_visual_dim + 1 if visual_cond else in_visual_dim\n+\n+        # Initialize embeddings\n+        self.time_embeddings = Kandinsky5TimeEmbeddings(model_dim, time_dim)\n+        self.text_embeddings = Kandinsky5TextEmbeddings(in_text_dim, model_dim)\n+        self.pooled_text_embeddings = Kandinsky5TextEmbeddings(in_text_dim2, time_dim)\n+        self.visual_embeddings = Kandinsky5VisualEmbeddings(visual_embed_dim, model_dim, patch_size)\n+\n+        # Initialize positional embeddings\n+        self.text_rope_embeddings = Kandinsky5RoPE1D(head_dim)\n+        self.visual_rope_embeddings = Kandinsky5RoPE3D(axes_dims)\n+\n+        # Initialize transformer blocks\n+        self.text_transformer_blocks = nn.ModuleList(\n+            [Kandinsky5TransformerEncoderBlock(model_dim, time_dim, ff_dim, head_dim) for _ in range(num_text_blocks)]\n+        )\n+\n+        self.visual_transformer_blocks = nn.ModuleList(\n+            [\n+                Kandinsky5TransformerDecoderBlock(model_dim, time_dim, ff_dim, head_dim)\n+                for _ in range(num_visual_blocks)\n+            ]\n+        )\n+\n+        # Initialize output layer\n+        self.out_layer = Kandinsky5OutLayer(model_dim, time_dim, out_visual_dim, patch_size)\n+        self.gradient_checkpointing = False\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,  # x\n+        encoder_hidden_states: torch.Tensor,  # text_embed\n+        timestep: torch.Tensor,  # time\n+        pooled_projections: torch.Tensor,  # pooled_text_embed\n+        visual_rope_pos: Tuple[int, int, int],\n+        text_rope_pos: torch.LongTensor,\n+        scale_factor: Tuple[float, float, float] = (1.0, 1.0, 1.0),\n+        sparse_params: Optional[Dict[str, Any]] = None,\n+        return_dict: bool = True,\n+    ) -> Union[Transformer2DModelOutput, torch.FloatTensor]:\n+        \"\"\"\n+        Forward pass of the Kandinsky5 3D Transformer.\n+\n+        Args:\n+            hidden_states (`torch.FloatTensor`): Input visual states\n+            encoder_hidden_states (`torch.FloatTensor`): Text embeddings\n+            timestep (`torch.Tensor` or `float` or `int`): Current timestep\n+            pooled_projections (`torch.FloatTensor`): Pooled text embeddings\n+            visual_rope_pos (`Tuple[int, int, int]`): Position for visual RoPE\n+            text_rope_pos (`torch.LongTensor`): Position for text RoPE\n+            scale_factor (`Tuple[float, float, float]`, optional): Scale factor for RoPE\n+            sparse_params (`Dict[str, Any]`, optional): Parameters for sparse attention\n+            return_dict (`bool`, optional): Whether to return a dictionary\n+\n+        Returns:\n+            [`~models.transformer_2d.Transformer2DModelOutput`] or `torch.FloatTensor`: The output of the transformer\n+        \"\"\"\n+        x = hidden_states\n+        text_embed = encoder_hidden_states\n+        time = timestep\n+        pooled_text_embed = pooled_projections\n+\n+        text_embed = self.text_embeddings(text_embed)\n+        time_embed = self.time_embeddings(time)\n+        time_embed = time_embed + self.pooled_text_embeddings(pooled_text_embed)\n+        visual_embed = self.visual_embeddings(x)\n+        text_rope = self.text_rope_embeddings(text_rope_pos)\n+        text_rope = text_rope.unsqueeze(dim=0)\n+\n+        for text_transformer_block in self.text_transformer_blocks:\n+            if torch.is_grad_enabled() and self.gradient_checkpointing:\n+                text_embed = self._gradient_checkpointing_func(\n+                    text_transformer_block, text_embed, time_embed, text_rope\n+                )\n+            else:\n+                text_embed = text_transformer_block(text_embed, time_embed, text_rope)\n+\n+        visual_shape = visual_embed.shape[:-1]\n+        visual_rope = self.visual_rope_embeddings(visual_shape, visual_rope_pos, scale_factor)\n+        to_fractal = sparse_params[\"to_fractal\"] if sparse_params is not None else False\n+        visual_embed, visual_rope = fractal_flatten(visual_embed, visual_rope, visual_shape, block_mask=to_fractal)\n+\n+        for visual_transformer_block in self.visual_transformer_blocks:\n+            if torch.is_grad_enabled() and self.gradient_checkpointing:\n+                visual_embed = self._gradient_checkpointing_func(\n+                    visual_transformer_block,\n+                    visual_embed,\n+                    text_embed,\n+                    time_embed,\n+                    visual_rope,\n+                    sparse_params,\n+                )\n+            else:\n+                visual_embed = visual_transformer_block(\n+                    visual_embed, text_embed, time_embed, visual_rope, sparse_params\n+                )\n+\n+        visual_embed = fractal_unflatten(visual_embed, visual_shape, block_mask=to_fractal)\n+        x = self.out_layer(visual_embed, text_embed, time_embed)\n+\n+        if not return_dict:\n+            return x\n+\n+        return Transformer2DModelOutput(sample=x)"
        },
        {
          "filename": "src/diffusers/pipelines/__init__.py",
          "status": "modified",
          "additions": 2,
          "deletions": 0,
          "changes": 2,
          "patch": "@@ -382,6 +382,7 @@\n         \"WuerstchenPriorPipeline\",\n     ]\n     _import_structure[\"wan\"] = [\"WanPipeline\", \"WanImageToVideoPipeline\", \"WanVideoToVideoPipeline\", \"WanVACEPipeline\"]\n+    _import_structure[\"kandinsky5\"] = [\"Kandinsky5T2VPipeline\"]\n     _import_structure[\"skyreels_v2\"] = [\n         \"SkyReelsV2DiffusionForcingPipeline\",\n         \"SkyReelsV2DiffusionForcingImageToVideoPipeline\",\n@@ -671,6 +672,7 @@\n             Kandinsky3Img2ImgPipeline,\n             Kandinsky3Pipeline,\n         )\n+        from .kandinsky5 import Kandinsky5T2VPipeline\n         from .latent_consistency_models import (\n             LatentConsistencyModelImg2ImgPipeline,\n             LatentConsistencyModelPipeline,"
        },
        {
          "filename": "src/diffusers/pipelines/kandinsky5/__init__.py",
          "status": "added",
          "additions": 48,
          "deletions": 0,
          "changes": 48,
          "patch": "@@ -0,0 +1,48 @@\n+from typing import TYPE_CHECKING\n+\n+from ...utils import (\n+    DIFFUSERS_SLOW_IMPORT,\n+    OptionalDependencyNotAvailable,\n+    _LazyModule,\n+    get_objects_from_module,\n+    is_torch_available,\n+    is_transformers_available,\n+)\n+\n+\n+_dummy_objects = {}\n+_import_structure = {}\n+\n+\n+try:\n+    if not (is_transformers_available() and is_torch_available()):\n+        raise OptionalDependencyNotAvailable()\n+except OptionalDependencyNotAvailable:\n+    from ...utils import dummy_torch_and_transformers_objects  # noqa F403\n+\n+    _dummy_objects.update(get_objects_from_module(dummy_torch_and_transformers_objects))\n+else:\n+    _import_structure[\"pipeline_kandinsky\"] = [\"Kandinsky5T2VPipeline\"]\n+\n+if TYPE_CHECKING or DIFFUSERS_SLOW_IMPORT:\n+    try:\n+        if not (is_transformers_available() and is_torch_available()):\n+            raise OptionalDependencyNotAvailable()\n+\n+    except OptionalDependencyNotAvailable:\n+        from ...utils.dummy_torch_and_transformers_objects import *\n+    else:\n+        from .pipeline_kandinsky import Kandinsky5T2VPipeline\n+\n+else:\n+    import sys\n+\n+    sys.modules[__name__] = _LazyModule(\n+        __name__,\n+        globals()[\"__file__\"],\n+        _import_structure,\n+        module_spec=__spec__,\n+    )\n+\n+    for name, value in _dummy_objects.items():\n+        setattr(sys.modules[__name__], name, value)"
        },
        {
          "filename": "src/diffusers/pipelines/kandinsky5/pipeline_kandinsky.py",
          "status": "added",
          "additions": 893,
          "deletions": 0,
          "changes": 893,
          "patch": "@@ -0,0 +1,893 @@\n+# Copyright 2025 The Kandinsky Team and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import html\n+from typing import Callable, Dict, List, Optional, Union\n+\n+import regex as re\n+import torch\n+from torch.nn import functional as F\n+from transformers import CLIPTextModel, CLIPTokenizer, Qwen2_5_VLForConditionalGeneration, Qwen2VLProcessor\n+\n+from ...callbacks import MultiPipelineCallbacks, PipelineCallback\n+from ...loaders import KandinskyLoraLoaderMixin\n+from ...models import AutoencoderKLHunyuanVideo\n+from ...models.transformers import Kandinsky5Transformer3DModel\n+from ...schedulers import FlowMatchEulerDiscreteScheduler\n+from ...utils import is_ftfy_available, is_torch_xla_available, logging, replace_example_docstring\n+from ...utils.torch_utils import randn_tensor\n+from ...video_processor import VideoProcessor\n+from ..pipeline_utils import DiffusionPipeline\n+from .pipeline_output import KandinskyPipelineOutput\n+\n+\n+if is_torch_xla_available():\n+    import torch_xla.core.xla_model as xm\n+\n+    XLA_AVAILABLE = True\n+else:\n+    XLA_AVAILABLE = False\n+\n+logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n+\n+if is_ftfy_available():\n+    import ftfy\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+EXAMPLE_DOC_STRING = \"\"\"\n+    Examples:\n+\n+        ```python\n+        >>> import torch\n+        >>> from diffusers import Kandinsky5T2VPipeline\n+        >>> from diffusers.utils import export_to_video\n+\n+        >>> # Available models:\n+        >>> # ai-forever/Kandinsky-5.0-T2V-Lite-sft-5s-Diffusers\n+        >>> # ai-forever/Kandinsky-5.0-T2V-Lite-nocfg-5s-Diffusers\n+        >>> # ai-forever/Kandinsky-5.0-T2V-Lite-distilled16steps-5s-Diffusers\n+        >>> # ai-forever/Kandinsky-5.0-T2V-Lite-pretrain-5s-Diffusers\n+\n+        >>> model_id = \"ai-forever/Kandinsky-5.0-T2V-Lite-sft-5s-Diffusers\"\n+        >>> pipe = Kandinsky5T2VPipeline.from_pretrained(model_id, torch_dtype=torch.bfloat16)\n+        >>> pipe = pipe.to(\"cuda\")\n+\n+        >>> prompt = \"A cat and a dog baking a cake together in a kitchen.\"\n+        >>> negative_prompt = \"Static, 2D cartoon, cartoon, 2d animation, paintings, images, worst quality, low quality, ugly, deformed, walking backwards\"\n+\n+        >>> output = pipe(\n+        ...     prompt=prompt,\n+        ...     negative_prompt=negative_prompt,\n+        ...     height=512,\n+        ...     width=768,\n+        ...     num_frames=121,\n+        ...     num_inference_steps=50,\n+        ...     guidance_scale=5.0,\n+        ... ).frames[0]\n+\n+        >>> export_to_video(output, \"output.mp4\", fps=24, quality=9)\n+        ```\n+\"\"\"\n+\n+\n+def basic_clean(text):\n+    \"\"\"Clean text using ftfy if available and unescape HTML entities.\"\"\"\n+    if is_ftfy_available():\n+        text = ftfy.fix_text(text)\n+    text = html.unescape(html.unescape(text))\n+    return text.strip()\n+\n+\n+def whitespace_clean(text):\n+    \"\"\"Normalize whitespace in text by replacing multiple spaces with single space.\"\"\"\n+    text = re.sub(r\"\\s+\", \" \", text)\n+    text = text.strip()\n+    return text\n+\n+\n+def prompt_clean(text):\n+    \"\"\"Apply both basic cleaning and whitespace normalization to prompts.\"\"\"\n+    text = whitespace_clean(basic_clean(text))\n+    return text\n+\n+\n+class Kandinsky5T2VPipeline(DiffusionPipeline, KandinskyLoraLoaderMixin):\n+    r\"\"\"\n+    Pipeline for text-to-video generation using Kandinsky 5.0.\n+\n+    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods\n+    implemented for all pipelines (downloading, saving, running on a particular device, etc.).\n+\n+    Args:\n+        transformer ([`Kandinsky5Transformer3DModel`]):\n+            Conditional Transformer to denoise the encoded video latents.\n+        vae ([`AutoencoderKLHunyuanVideo`]):\n+            Variational Auto-Encoder (VAE) Model to encode and decode videos to and from latent representations.\n+        text_encoder ([`Qwen2_5_VLForConditionalGeneration`]):\n+            Frozen text-encoder (Qwen2.5-VL).\n+        tokenizer ([`AutoProcessor`]):\n+            Tokenizer for Qwen2.5-VL.\n+        text_encoder_2 ([`CLIPTextModel`]):\n+            Frozen CLIP text encoder.\n+        tokenizer_2 ([`CLIPTokenizer`]):\n+            Tokenizer for CLIP.\n+        scheduler ([`FlowMatchEulerDiscreteScheduler`]):\n+            A scheduler to be used in combination with `transformer` to denoise the encoded video latents.\n+    \"\"\"\n+\n+    model_cpu_offload_seq = \"text_encoder->text_encoder_2->transformer->vae\"\n+    _callback_tensor_inputs = [\n+        \"latents\",\n+        \"prompt_embeds_qwen\",\n+        \"prompt_embeds_clip\",\n+        \"negative_prompt_embeds_qwen\",\n+        \"negative_prompt_embeds_clip\",\n+    ]\n+\n+    def __init__(\n+        self,\n+        transformer: Kandinsky5Transformer3DModel,\n+        vae: AutoencoderKLHunyuanVideo,\n+        text_encoder: Qwen2_5_VLForConditionalGeneration,\n+        tokenizer: Qwen2VLProcessor,\n+        text_encoder_2: CLIPTextModel,\n+        tokenizer_2: CLIPTokenizer,\n+        scheduler: FlowMatchEulerDiscreteScheduler,\n+    ):\n+        super().__init__()\n+\n+        self.register_modules(\n+            transformer=transformer,\n+            vae=vae,\n+            text_encoder=text_encoder,\n+            tokenizer=tokenizer,\n+            text_encoder_2=text_encoder_2,\n+            tokenizer_2=tokenizer_2,\n+            scheduler=scheduler,\n+        )\n+\n+        self.prompt_template = \"\\n\".join(\n+            [\n+                \"<|im_start|>system\\nYou are a promt engineer. Describe the video in detail.\",\n+                \"Describe how the camera moves or shakes, describe the zoom and view angle, whether it follows the objects.\",\n+                \"Describe the location of the video, main characters or objects and their action.\",\n+                \"Describe the dynamism of the video and presented actions.\",\n+                \"Name the visual style of the video: whether it is a professional footage, user generated content, some kind of animation, video game or scren content.\",\n+                \"Describe the visual effects, postprocessing and transitions if they are presented in the video.\",\n+                \"Pay attention to the order of key actions shown in the scene.<|im_end|>\",\n+                \"<|im_start|>user\\n{}<|im_end|>\",\n+            ]\n+        )\n+        self.prompt_template_encode_start_idx = 129\n+\n+        self.vae_scale_factor_temporal = vae.config.temporal_compression_ratio\n+        self.vae_scale_factor_spatial = vae.config.spatial_compression_ratio\n+        self.video_processor = VideoProcessor(vae_scale_factor=self.vae_scale_factor_spatial)\n+\n+    @staticmethod\n+    def fast_sta_nabla(T: int, H: int, W: int, wT: int = 3, wH: int = 3, wW: int = 3, device=\"cuda\") -> torch.Tensor:\n+        \"\"\"\n+        Create a sparse temporal attention (STA) mask for efficient video generation.\n+\n+        This method generates a mask that limits attention to nearby frames and spatial positions, reducing\n+        computational complexity for video generation.\n+\n+        Args:\n+            T (int): Number of temporal frames\n+            H (int): Height in latent space\n+            W (int): Width in latent space\n+            wT (int): Temporal attention window size\n+            wH (int): Height attention window size\n+            wW (int): Width attention window size\n+            device (str): Device to create tensor on\n+\n+        Returns:\n+            torch.Tensor: Sparse attention mask of shape (T*H*W, T*H*W)\n+        \"\"\"\n+        l = torch.Tensor([T, H, W]).amax()\n+        r = torch.arange(0, l, 1, dtype=torch.int16, device=device)\n+        mat = (r.unsqueeze(1) - r.unsqueeze(0)).abs()\n+        sta_t, sta_h, sta_w = (\n+            mat[:T, :T].flatten(),\n+            mat[:H, :H].flatten(),\n+            mat[:W, :W].flatten(),\n+        )\n+        sta_t = sta_t <= wT // 2\n+        sta_h = sta_h <= wH // 2\n+        sta_w = sta_w <= wW // 2\n+        sta_hw = (sta_h.unsqueeze(1) * sta_w.unsqueeze(0)).reshape(H, H, W, W).transpose(1, 2).flatten()\n+        sta = (sta_t.unsqueeze(1) * sta_hw.unsqueeze(0)).reshape(T, T, H * W, H * W).transpose(1, 2)\n+        return sta.reshape(T * H * W, T * H * W)\n+\n+    def get_sparse_params(self, sample, device):\n+        \"\"\"\n+        Generate sparse attention parameters for the transformer based on sample dimensions.\n+\n+        This method computes the sparse attention configuration needed for efficient video processing in the\n+        transformer model.\n+\n+        Args:\n+            sample (torch.Tensor): Input sample tensor\n+            device (torch.device): Device to place tensors on\n+\n+        Returns:\n+            Dict: Dictionary containing sparse attention parameters\n+        \"\"\"\n+        assert self.transformer.config.patch_size[0] == 1\n+        B, T, H, W, _ = sample.shape\n+        T, H, W = (\n+            T // self.transformer.config.patch_size[0],\n+            H // self.transformer.config.patch_size[1],\n+            W // self.transformer.config.patch_size[2],\n+        )\n+        if self.transformer.config.attention_type == \"nabla\":\n+            sta_mask = self.fast_sta_nabla(\n+                T,\n+                H // 8,\n+                W // 8,\n+                self.transformer.config.attention_wT,\n+                self.transformer.config.attention_wH,\n+                self.transformer.config.attention_wW,\n+                device=device,\n+            )\n+\n+            sparse_params = {\n+                \"sta_mask\": sta_mask.unsqueeze_(0).unsqueeze_(0),\n+                \"attention_type\": self.transformer.config.attention_type,\n+                \"to_fractal\": True,\n+                \"P\": self.transformer.config.attention_P,\n+                \"wT\": self.transformer.config.attention_wT,\n+                \"wW\": self.transformer.config.attention_wW,\n+                \"wH\": self.transformer.config.attention_wH,\n+                \"add_sta\": self.transformer.config.attention_add_sta,\n+                \"visual_shape\": (T, H, W),\n+                \"method\": self.transformer.config.attention_method,\n+            }\n+        else:\n+            sparse_params = None\n+\n+        return sparse_params\n+\n+    def _encode_prompt_qwen(\n+        self,\n+        prompt: Union[str, List[str]],\n+        device: Optional[torch.device] = None,\n+        max_sequence_length: int = 256,\n+        dtype: Optional[torch.dtype] = None,\n+    ):\n+        \"\"\"\n+        Encode prompt using Qwen2.5-VL text encoder.\n+\n+        This method processes the input prompt through the Qwen2.5-VL model to generate text embeddings suitable for\n+        video generation.\n+\n+        Args:\n+            prompt (Union[str, List[str]]): Input prompt or list of prompts\n+            device (torch.device): Device to run encoding on\n+            num_videos_per_prompt (int): Number of videos to generate per prompt\n+            max_sequence_length (int): Maximum sequence length for tokenization\n+            dtype (torch.dtype): Data type for embeddings\n+\n+        Returns:\n+            Tuple[torch.Tensor, torch.Tensor]: Text embeddings and cumulative sequence lengths\n+        \"\"\"\n+        device = device or self._execution_device\n+        dtype = dtype or self.text_encoder.dtype\n+\n+        full_texts = [self.prompt_template.format(p) for p in prompt]\n+\n+        inputs = self.tokenizer(\n+            text=full_texts,\n+            images=None,\n+            videos=None,\n+            max_length=max_sequence_length + self.prompt_template_encode_start_idx,\n+            truncation=True,\n+            return_tensors=\"pt\",\n+            padding=True,\n+        ).to(device)\n+\n+        embeds = self.text_encoder(\n+            input_ids=inputs[\"input_ids\"],\n+            return_dict=True,\n+            output_hidden_states=True,\n+        )[\"hidden_states\"][-1][:, self.prompt_template_encode_start_idx :]\n+\n+        attention_mask = inputs[\"attention_mask\"][:, self.prompt_template_encode_start_idx :]\n+        cu_seqlens = torch.cumsum(attention_mask.sum(1), dim=0)\n+        cu_seqlens = F.pad(cu_seqlens, (1, 0), value=0).to(dtype=torch.int32)\n+\n+        return embeds.to(dtype), cu_seqlens\n+\n+    def _encode_prompt_clip(\n+        self,\n+        prompt: Union[str, List[str]],\n+        device: Optional[torch.device] = None,\n+        dtype: Optional[torch.dtype] = None,\n+    ):\n+        \"\"\"\n+        Encode prompt using CLIP text encoder.\n+\n+        This method processes the input prompt through the CLIP model to generate pooled embeddings that capture\n+        semantic information.\n+\n+        Args:\n+            prompt (Union[str, List[str]]): Input prompt or list of prompts\n+            device (torch.device): Device to run encoding on\n+            num_videos_per_prompt (int): Number of videos to generate per prompt\n+            dtype (torch.dtype): Data type for embeddings\n+\n+        Returns:\n+            torch.Tensor: Pooled text embeddings from CLIP\n+        \"\"\"\n+        device = device or self._execution_device\n+        dtype = dtype or self.text_encoder_2.dtype\n+\n+        inputs = self.tokenizer_2(\n+            prompt,\n+            max_length=77,\n+            truncation=True,\n+            add_special_tokens=True,\n+            padding=\"max_length\",\n+            return_tensors=\"pt\",\n+        ).to(device)\n+\n+        pooled_embed = self.text_encoder_2(**inputs)[\"pooler_output\"]\n+\n+        return pooled_embed.to(dtype)\n+\n+    def encode_prompt(\n+        self,\n+        prompt: Union[str, List[str]],\n+        num_videos_per_prompt: int = 1,\n+        max_sequence_length: int = 512,\n+        device: Optional[torch.device] = None,\n+        dtype: Optional[torch.dtype] = None,\n+    ):\n+        r\"\"\"\n+        Encodes a single prompt (positive or negative) into text encoder hidden states.\n+\n+        This method combines embeddings from both Qwen2.5-VL and CLIP text encoders to create comprehensive text\n+        representations for video generation.\n+\n+        Args:\n+            prompt (`str` or `List[str]`):\n+                Prompt to be encoded.\n+            num_videos_per_prompt (`int`, *optional*, defaults to 1):\n+                Number of videos to generate per prompt.\n+            max_sequence_length (`int`, *optional*, defaults to 512):\n+                Maximum sequence length for text encoding.\n+            device (`torch.device`, *optional*):\n+                Torch device.\n+            dtype (`torch.dtype`, *optional*):\n+                Torch dtype.\n+\n+        Returns:\n+            Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n+                - Qwen text embeddings of shape (batch_size * num_videos_per_prompt, sequence_length, embedding_dim)\n+                - CLIP pooled embeddings of shape (batch_size * num_videos_per_prompt, clip_embedding_dim)\n+                - Cumulative sequence lengths (`cu_seqlens`) for Qwen embeddings of shape (batch_size *\n+                  num_videos_per_prompt + 1,)\n+        \"\"\"\n+        device = device or self._execution_device\n+        dtype = dtype or self.text_encoder.dtype\n+\n+        batch_size = len(prompt)\n+\n+        prompt = [prompt_clean(p) for p in prompt]\n+\n+        # Encode with Qwen2.5-VL\n+        prompt_embeds_qwen, prompt_cu_seqlens = self._encode_prompt_qwen(\n+            prompt=prompt,\n+            device=device,\n+            max_sequence_length=max_sequence_length,\n+            dtype=dtype,\n+        )\n+        # prompt_embeds_qwen shape: [batch_size, seq_len, embed_dim]\n+\n+        # Encode with CLIP\n+        prompt_embeds_clip = self._encode_prompt_clip(\n+            prompt=prompt,\n+            device=device,\n+            dtype=dtype,\n+        )\n+        # prompt_embeds_clip shape: [batch_size, clip_embed_dim]\n+\n+        # Repeat embeddings for num_videos_per_prompt\n+        # Qwen embeddings: repeat sequence for each video, then reshape\n+        prompt_embeds_qwen = prompt_embeds_qwen.repeat(\n+            1, num_videos_per_prompt, 1\n+        )  # [batch_size, seq_len * num_videos_per_prompt, embed_dim]\n+        # Reshape to [batch_size * num_videos_per_prompt, seq_len, embed_dim]\n+        prompt_embeds_qwen = prompt_embeds_qwen.view(\n+            batch_size * num_videos_per_prompt, -1, prompt_embeds_qwen.shape[-1]\n+        )\n+\n+        # CLIP embeddings: repeat for each video\n+        prompt_embeds_clip = prompt_embeds_clip.repeat(\n+            1, num_videos_per_prompt, 1\n+        )  # [batch_size, num_videos_per_prompt, clip_embed_dim]\n+        # Reshape to [batch_size * num_videos_per_prompt, clip_embed_dim]\n+        prompt_embeds_clip = prompt_embeds_clip.view(batch_size * num_videos_per_prompt, -1)\n+\n+        # Repeat cumulative sequence lengths for num_videos_per_prompt\n+        # Original cu_seqlens: [0, len1, len1+len2, ...]\n+        # Need to repeat the differences and reconstruct for repeated prompts\n+        # Original differences (lengths) for each prompt in the batch\n+        original_lengths = prompt_cu_seqlens.diff()  # [len1, len2, ...]\n+        # Repeat the lengths for num_videos_per_prompt\n+        repeated_lengths = original_lengths.repeat_interleave(\n+            num_videos_per_prompt\n+        )  # [len1, len1, ..., len2, len2, ...]\n+        # Reconstruct the cumulative lengths\n+        repeated_cu_seqlens = torch.cat(\n+            [torch.tensor([0], device=device, dtype=torch.int32), repeated_lengths.cumsum(0)]\n+        )\n+\n+        return prompt_embeds_qwen, prompt_embeds_clip, repeated_cu_seqlens\n+\n+    def check_inputs(\n+        self,\n+        prompt,\n+        negative_prompt,\n+        height,\n+        width,\n+        prompt_embeds_qwen=None,\n+        prompt_embeds_clip=None,\n+        negative_prompt_embeds_qwen=None,\n+        negative_prompt_embeds_clip=None,\n+        prompt_cu_seqlens=None,\n+        negative_prompt_cu_seqlens=None,\n+        callback_on_step_end_tensor_inputs=None,\n+    ):\n+        \"\"\"\n+        Validate input parameters for the pipeline.\n+\n+        Args:\n+            prompt: Input prompt\n+            negative_prompt: Negative prompt for guidance\n+            height: Video height\n+            width: Video width\n+            prompt_embeds_qwen: Pre-computed Qwen prompt embeddings\n+            prompt_embeds_clip: Pre-computed CLIP prompt embeddings\n+            negative_prompt_embeds_qwen: Pre-computed Qwen negative prompt embeddings\n+            negative_prompt_embeds_clip: Pre-computed CLIP negative prompt embeddings\n+            prompt_cu_seqlens: Pre-computed cumulative sequence lengths for Qwen positive prompt\n+            negative_prompt_cu_seqlens: Pre-computed cumulative sequence lengths for Qwen negative prompt\n+            callback_on_step_end_tensor_inputs: Callback tensor inputs\n+\n+        Raises:\n+            ValueError: If inputs are invalid\n+        \"\"\"\n+        if height % 16 != 0 or width % 16 != 0:\n+            raise ValueError(f\"`height` and `width` have to be divisible by 16 but are {height} and {width}.\")\n+\n+        if callback_on_step_end_tensor_inputs is not None and not all(\n+            k in self._callback_tensor_inputs for k in callback_on_step_end_tensor_inputs\n+        ):\n+            raise ValueError(\n+                f\"`callback_on_step_end_tensor_inputs` has to be in {self._callback_tensor_inputs}, but found {[k for k in callback_on_step_end_tensor_inputs if k not in self._callback_tensor_inputs]}\"\n+            )\n+\n+        # Check for consistency within positive prompt embeddings and sequence lengths\n+        if prompt_embeds_qwen is not None or prompt_embeds_clip is not None or prompt_cu_seqlens is not None:\n+            if prompt_embeds_qwen is None or prompt_embeds_clip is None or prompt_cu_seqlens is None:\n+                raise ValueError(\n+                    \"If any of `prompt_embeds_qwen`, `prompt_embeds_clip`, or `prompt_cu_seqlens` is provided, \"\n+                    \"all three must be provided.\"\n+                )\n+\n+        # Check for consistency within negative prompt embeddings and sequence lengths\n+        if (\n+            negative_prompt_embeds_qwen is not None\n+            or negative_prompt_embeds_clip is not None\n+            or negative_prompt_cu_seqlens is not None\n+        ):\n+            if (\n+                negative_prompt_embeds_qwen is None\n+                or negative_prompt_embeds_clip is None\n+                or negative_prompt_cu_seqlens is None\n+            ):\n+                raise ValueError(\n+                    \"If any of `negative_prompt_embeds_qwen`, `negative_prompt_embeds_clip`, or `negative_prompt_cu_seqlens` is provided, \"\n+                    \"all three must be provided.\"\n+                )\n+\n+        # Check if prompt or embeddings are provided (either prompt or all required embedding components for positive)\n+        if prompt is None and prompt_embeds_qwen is None:\n+            raise ValueError(\n+                \"Provide either `prompt` or `prompt_embeds_qwen` (and corresponding `prompt_embeds_clip` and `prompt_cu_seqlens`). Cannot leave all undefined.\"\n+            )\n+\n+        # Validate types for prompt and negative_prompt if provided\n+        if prompt is not None and (not isinstance(prompt, str) and not isinstance(prompt, list)):\n+            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n+        if negative_prompt is not None and (\n+            not isinstance(negative_prompt, str) and not isinstance(negative_prompt, list)\n+        ):\n+            raise ValueError(f\"`negative_prompt` has to be of type `str` or `list` but is {type(negative_prompt)}\")\n+\n+    def prepare_latents(\n+        self,\n+        batch_size: int,\n+        num_channels_latents: int = 16,\n+        height: int = 480,\n+        width: int = 832,\n+        num_frames: int = 81,\n+        dtype: Optional[torch.dtype] = None,\n+        device: Optional[torch.device] = None,\n+        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n+        latents: Optional[torch.Tensor] = None,\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        Prepare initial latent variables for video generation.\n+\n+        This method creates random noise latents or uses provided latents as starting point for the denoising process.\n+\n+        Args:\n+            batch_size (int): Number of videos to generate\n+            num_channels_latents (int): Number of channels in latent space\n+            height (int): Height of generated video\n+            width (int): Width of generated video\n+            num_frames (int): Number of frames in video\n+            dtype (torch.dtype): Data type for latents\n+            device (torch.device): Device to create latents on\n+            generator (torch.Generator): Random number generator\n+            latents (torch.Tensor): Pre-existing latents to use\n+\n+        Returns:\n+            torch.Tensor: Prepared latent tensor\n+        \"\"\"\n+        if latents is not None:\n+            return latents.to(device=device, dtype=dtype)\n+\n+        num_latent_frames = (num_frames - 1) // self.vae_scale_factor_temporal + 1\n+        shape = (\n+            batch_size,\n+            num_latent_frames,\n+            int(height) // self.vae_scale_factor_spatial,\n+            int(width) // self.vae_scale_factor_spatial,\n+            num_channels_latents,\n+        )\n+        if isinstance(generator, list) and len(generator) != batch_size:\n+            raise ValueError(\n+                f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n+                f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n+            )\n+\n+        latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n+\n+        if self.transformer.visual_cond:\n+            # For visual conditioning, concatenate with zeros and mask\n+            visual_cond = torch.zeros_like(latents)\n+            visual_cond_mask = torch.zeros(\n+                [\n+                    batch_size,\n+                    num_latent_frames,\n+                    int(height) // self.vae_scale_factor_spatial,\n+                    int(width) // self.vae_scale_factor_spatial,\n+                    1,\n+                ],\n+                dtype=latents.dtype,\n+                device=latents.device,\n+            )\n+            latents = torch.cat([latents, visual_cond, visual_cond_mask], dim=-1)\n+\n+        return latents\n+\n+    @property\n+    def guidance_scale(self):\n+        \"\"\"Get the current guidance scale value.\"\"\"\n+        return self._guidance_scale\n+\n+    @property\n+    def do_classifier_free_guidance(self):\n+        \"\"\"Check if classifier-free guidance is enabled.\"\"\"\n+        return self._guidance_scale > 1.0\n+\n+    @property\n+    def num_timesteps(self):\n+        \"\"\"Get the number of denoising timesteps.\"\"\"\n+        return self._num_timesteps\n+\n+    @property\n+    def interrupt(self):\n+        \"\"\"Check if generation has been interrupted.\"\"\"\n+        return self._interrupt\n+\n+    @torch.no_grad()\n+    @replace_example_docstring(EXAMPLE_DOC_STRING)\n+    def __call__(\n+        self,\n+        prompt: Union[str, List[str]] = None,\n+        negative_prompt: Optional[Union[str, List[str]]] = None,\n+        height: int = 512,\n+        width: int = 768,\n+        num_frames: int = 121,\n+        num_inference_steps: int = 50,\n+        guidance_scale: float = 5.0,\n+        num_videos_per_prompt: Optional[int] = 1,\n+        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n+        latents: Optional[torch.Tensor] = None,\n+        prompt_embeds_qwen: Optional[torch.Tensor] = None,\n+        prompt_embeds_clip: Optional[torch.Tensor] = None,\n+        negative_prompt_embeds_qwen: Optional[torch.Tensor] = None,\n+        negative_prompt_embeds_clip: Optional[torch.Tensor] = None,\n+        prompt_cu_seqlens: Optional[torch.Tensor] = None,\n+        negative_prompt_cu_seqlens: Optional[torch.Tensor] = None,\n+        output_type: Optional[str] = \"pil\",\n+        return_dict: bool = True,\n+        callback_on_step_end: Optional[\n+            Union[Callable[[int, int, Dict], None], PipelineCallback, MultiPipelineCallbacks]\n+        ] = None,\n+        callback_on_step_end_tensor_inputs: List[str] = [\"latents\"],\n+        max_sequence_length: int = 512,\n+        **kwargs,\n+    ):\n+        r\"\"\"\n+        The call function to the pipeline for generation.\n+\n+        Args:\n+            prompt (`str` or `List[str]`, *optional*):\n+                The prompt or prompts to guide the video generation. If not defined, pass `prompt_embeds` instead.\n+            negative_prompt (`str` or `List[str]`, *optional*):\n+                The prompt or prompts to avoid during video generation. If not defined, pass `negative_prompt_embeds`\n+                instead. Ignored when not using guidance (`guidance_scale` < `1`).\n+            height (`int`, defaults to `512`):\n+                The height in pixels of the generated video.\n+            width (`int`, defaults to `768`):\n+                The width in pixels of the generated video.\n+            num_frames (`int`, defaults to `25`):\n+                The number of frames in the generated video.\n+            num_inference_steps (`int`, defaults to `50`):\n+                The number of denoising steps.\n+            guidance_scale (`float`, defaults to `5.0`):\n+                Guidance scale as defined in classifier-free guidance.\n+            num_videos_per_prompt (`int`, *optional*, defaults to 1):\n+                The number of videos to generate per prompt.\n+            generator (`torch.Generator` or `List[torch.Generator]`, *optional*):\n+                A torch generator to make generation deterministic.\n+            latents (`torch.Tensor`, *optional*):\n+                Pre-generated noisy latents.\n+            prompt_embeds (`torch.Tensor`, *optional*):\n+                Pre-generated text embeddings.\n+            negative_prompt_embeds (`torch.Tensor`, *optional*):\n+                Pre-generated negative text embeddings.\n+            output_type (`str`, *optional*, defaults to `\"pil\"`):\n+                The output format of the generated video.\n+            return_dict (`bool`, *optional*, defaults to `True`):\n+                Whether or not to return a [`KandinskyPipelineOutput`].\n+            callback_on_step_end (`Callable`, `PipelineCallback`, `MultiPipelineCallbacks`, *optional*):\n+                A function that is called at the end of each denoising step.\n+            callback_on_step_end_tensor_inputs (`List`, *optional*):\n+                The list of tensor inputs for the `callback_on_step_end` function.\n+            max_sequence_length (`int`, defaults to `512`):\n+                The maximum sequence length for text encoding.\n+\n+        Examples:\n+\n+        Returns:\n+            [`~KandinskyPipelineOutput`] or `tuple`:\n+                If `return_dict` is `True`, [`KandinskyPipelineOutput`] is returned, otherwise a `tuple` is returned\n+                where the first element is a list with the generated images.\n+        \"\"\"\n+        if isinstance(callback_on_step_end, (PipelineCallback, MultiPipelineCallbacks)):\n+            callback_on_step_end_tensor_inputs = callback_on_step_end.tensor_inputs\n+\n+        # 1. Check inputs. Raise error if not correct\n+        self.check_inputs(\n+            prompt=prompt,\n+            negative_prompt=negative_prompt,\n+            height=height,\n+            width=width,\n+            prompt_embeds_qwen=prompt_embeds_qwen,\n+            prompt_embeds_clip=prompt_embeds_clip,\n+            negative_prompt_embeds_qwen=negative_prompt_embeds_qwen,\n+            negative_prompt_embeds_clip=negative_prompt_embeds_clip,\n+            prompt_cu_seqlens=prompt_cu_seqlens,\n+            negative_prompt_cu_seqlens=negative_prompt_cu_seqlens,\n+            callback_on_step_end_tensor_inputs=callback_on_step_end_tensor_inputs,\n+        )\n+\n+        if num_frames % self.vae_scale_factor_temporal != 1:\n+            logger.warning(\n+                f\"`num_frames - 1` has to be divisible by {self.vae_scale_factor_temporal}. Rounding to the nearest number.\"\n+            )\n+            num_frames = num_frames // self.vae_scale_factor_temporal * self.vae_scale_factor_temporal + 1\n+        num_frames = max(num_frames, 1)\n+\n+        self._guidance_scale = guidance_scale\n+        self._interrupt = False\n+\n+        device = self._execution_device\n+        dtype = self.transformer.dtype\n+\n+        # 2. Define call parameters\n+        if prompt is not None and isinstance(prompt, str):\n+            batch_size = 1\n+            prompt = [prompt]\n+        elif prompt is not None and isinstance(prompt, list):\n+            batch_size = len(prompt)\n+        else:\n+            batch_size = prompt_embeds_qwen.shape[0]\n+\n+        # 3. Encode input prompt\n+        if prompt_embeds_qwen is None:\n+            prompt_embeds_qwen, prompt_embeds_clip, prompt_cu_seqlens = self.encode_prompt(\n+                prompt=prompt,\n+                max_sequence_length=max_sequence_length,\n+                device=device,\n+                dtype=dtype,\n+            )\n+\n+        if self.do_classifier_free_guidance:\n+            if negative_prompt is None:\n+                negative_prompt = \"Static, 2D cartoon, cartoon, 2d animation, paintings, images, worst quality, low quality, ugly, deformed, walking backwards\"\n+\n+            if isinstance(negative_prompt, str):\n+                negative_prompt = [negative_prompt] * len(prompt) if prompt is not None else [negative_prompt]\n+            elif len(negative_prompt) != len(prompt):\n+                raise ValueError(\n+                    f\"`negative_prompt` must have same length as `prompt`. Got {len(negative_prompt)} vs {len(prompt)}.\"\n+                )\n+\n+            if negative_prompt_embeds_qwen is None:\n+                negative_prompt_embeds_qwen, negative_prompt_embeds_clip, negative_cu_seqlens = self.encode_prompt(\n+                    prompt=negative_prompt,\n+                    max_sequence_length=max_sequence_length,\n+                    device=device,\n+                    dtype=dtype,\n+                )\n+\n+        # 4. Prepare timesteps\n+        self.scheduler.set_timesteps(num_inference_steps, device=device)\n+        timesteps = self.scheduler.timesteps\n+\n+        # 5. Prepare latent variables\n+        num_channels_latents = self.transformer.config.in_visual_dim\n+        latents = self.prepare_latents(\n+            batch_size * num_videos_per_prompt,\n+            num_channels_latents,\n+            height,\n+            width,\n+            num_frames,\n+            dtype,\n+            device,\n+            generator,\n+            latents,\n+        )\n+\n+        # 6. Prepare rope positions for positional encoding\n+        num_latent_frames = (num_frames - 1) // self.vae_scale_factor_temporal + 1\n+        visual_rope_pos = [\n+            torch.arange(num_latent_frames, device=device),\n+            torch.arange(height // self.vae_scale_factor_spatial // 2, device=device),\n+            torch.arange(width // self.vae_scale_factor_spatial // 2, device=device),\n+        ]\n+\n+        text_rope_pos = torch.arange(prompt_cu_seqlens.diff().max().item(), device=device)\n+\n+        negative_text_rope_pos = (\n+            torch.arange(negative_cu_seqlens.diff().max().item(), device=device)\n+            if negative_cu_seqlens is not None\n+            else None\n+        )\n+\n+        # 7. Sparse Params for efficient attention\n+        sparse_params = self.get_sparse_params(latents, device)\n+\n+        # 8. Denoising loop\n+        num_warmup_steps = len(timesteps) - num_inference_steps * self.scheduler.order\n+        self._num_timesteps = len(timesteps)\n+\n+        with self.progress_bar(total=num_inference_steps) as progress_bar:\n+            for i, t in enumerate(timesteps):\n+                if self.interrupt:\n+                    continue\n+\n+                timestep = t.unsqueeze(0).repeat(batch_size * num_videos_per_prompt)\n+\n+                # Predict noise residual\n+                pred_velocity = self.transformer(\n+                    hidden_states=latents.to(dtype),\n+                    encoder_hidden_states=prompt_embeds_qwen.to(dtype),\n+                    pooled_projections=prompt_embeds_clip.to(dtype),\n+                    timestep=timestep.to(dtype),\n+                    visual_rope_pos=visual_rope_pos,\n+                    text_rope_pos=text_rope_pos,\n+                    scale_factor=(1, 2, 2),\n+                    sparse_params=sparse_params,\n+                    return_dict=True,\n+                ).sample\n+\n+                if self.do_classifier_free_guidance and negative_prompt_embeds_qwen is not None:\n+                    uncond_pred_velocity = self.transformer(\n+                        hidden_states=latents.to(dtype),\n+                        encoder_hidden_states=negative_prompt_embeds_qwen.to(dtype),\n+                        pooled_projections=negative_prompt_embeds_clip.to(dtype),\n+                        timestep=timestep.to(dtype),\n+                        visual_rope_pos=visual_rope_pos,\n+                        text_rope_pos=negative_text_rope_pos,\n+                        scale_factor=(1, 2, 2),\n+                        sparse_params=sparse_params,\n+                        return_dict=True,\n+                    ).sample\n+\n+                    pred_velocity = uncond_pred_velocity + guidance_scale * (pred_velocity - uncond_pred_velocity)\n+                # Compute previous sample using the scheduler\n+                latents[:, :, :, :, :num_channels_latents] = self.scheduler.step(\n+                    pred_velocity, t, latents[:, :, :, :, :num_channels_latents], return_dict=False\n+                )[0]\n+\n+                if callback_on_step_end is not None:\n+                    callback_kwargs = {}\n+                    for k in callback_on_step_end_tensor_inputs:\n+                        callback_kwargs[k] = locals()[k]\n+                    callback_outputs = callback_on_step_end(self, i, t, callback_kwargs)\n+\n+                    latents = callback_outputs.pop(\"latents\", latents)\n+                    prompt_embeds_qwen = callback_outputs.pop(\"prompt_embeds_qwen\", prompt_embeds_qwen)\n+                    prompt_embeds_clip = callback_outputs.pop(\"prompt_embeds_clip\", prompt_embeds_clip)\n+                    negative_prompt_embeds_qwen = callback_outputs.pop(\n+                        \"negative_prompt_embeds_qwen\", negative_prompt_embeds_qwen\n+                    )\n+                    negative_prompt_embeds_clip = callback_outputs.pop(\n+                        \"negative_prompt_embeds_clip\", negative_prompt_embeds_clip\n+                    )\n+\n+                if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n+                    progress_bar.update()\n+\n+                if XLA_AVAILABLE:\n+                    xm.mark_step()\n+\n+        # 8. Post-processing - extract main latents\n+        latents = latents[:, :, :, :, :num_channels_latents]\n+\n+        # 9. Decode latents to video\n+        if output_type != \"latent\":\n+            latents = latents.to(self.vae.dtype)\n+            # Reshape and normalize latents\n+            video = latents.reshape(\n+                batch_size,\n+                num_videos_per_prompt,\n+                (num_frames - 1) // self.vae_scale_factor_temporal + 1,\n+                height // self.vae_scale_factor_spatial,\n+                width // self.vae_scale_factor_spatial,\n+                num_channels_latents,\n+            )\n+            video = video.permute(0, 1, 5, 2, 3, 4)  # [batch, num_videos, channels, frames, height, width]\n+            video = video.reshape(\n+                batch_size * num_videos_per_prompt,\n+                num_channels_latents,\n+                (num_frames - 1) // self.vae_scale_factor_temporal + 1,\n+                height // self.vae_scale_factor_spatial,\n+                width // self.vae_scale_factor_spatial,\n+            )\n+\n+            # Normalize and decode through VAE\n+            video = video / self.vae.config.scaling_factor\n+            video = self.vae.decode(video).sample\n+            video = self.video_processor.postprocess_video(video, output_type=output_type)\n+        else:\n+            video = latents\n+\n+        # Offload all models\n+        self.maybe_free_model_hooks()\n+\n+        if not return_dict:\n+            return (video,)\n+\n+        return KandinskyPipelineOutput(frames=video)"
        },
        {
          "filename": "src/diffusers/pipelines/kandinsky5/pipeline_output.py",
          "status": "added",
          "additions": 20,
          "deletions": 0,
          "changes": 20,
          "patch": "@@ -0,0 +1,20 @@\n+from dataclasses import dataclass\n+\n+import torch\n+\n+from diffusers.utils import BaseOutput\n+\n+\n+@dataclass\n+class KandinskyPipelineOutput(BaseOutput):\n+    r\"\"\"\n+    Output class for Wan pipelines.\n+\n+    Args:\n+        frames (`torch.Tensor`, `np.ndarray`, or List[List[PIL.Image.Image]]):\n+            List of video outputs - It can be a nested list of length `batch_size,` with each sub-list containing\n+            denoised PIL image sequences of length `num_frames.` It can also be a NumPy array or Torch tensor of shape\n+            `(batch_size, num_frames, channels, height, width)`.\n+    \"\"\"\n+\n+    frames: torch.Tensor"
        },
        {
          "filename": "src/diffusers/utils/dummy_pt_objects.py",
          "status": "modified",
          "additions": 15,
          "deletions": 0,
          "changes": 15,
          "patch": "@@ -918,6 +918,21 @@ def from_pretrained(cls, *args, **kwargs):\n         requires_backends(cls, [\"torch\"])\n \n \n+class Kandinsky5Transformer3DModel(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+    @classmethod\n+    def from_config(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\"])\n+\n+    @classmethod\n+    def from_pretrained(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\"])\n+\n+\n class LatteTransformer3DModel(metaclass=DummyObject):\n     _backends = [\"torch\"]\n "
        },
        {
          "filename": "src/diffusers/utils/dummy_torch_and_transformers_objects.py",
          "status": "modified",
          "additions": 15,
          "deletions": 0,
          "changes": 15,
          "patch": "@@ -1247,6 +1247,21 @@ def from_pretrained(cls, *args, **kwargs):\n         requires_backends(cls, [\"torch\", \"transformers\"])\n \n \n+class Kandinsky5T2VPipeline(metaclass=DummyObject):\n+    _backends = [\"torch\", \"transformers\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_config(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_pretrained(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+\n class KandinskyCombinedPipeline(metaclass=DummyObject):\n     _backends = [\"torch\", \"transformers\"]\n "
        }
      ],
      "num_files": 13,
      "scraped_at": "2025-11-16T21:19:00.538001"
    },
    {
      "pr_number": 12473,
      "title": "[core] `AutoencoderMixin` to abstract common methods",
      "body": "# What does this PR do?\r\n\r\nAsbtracts common methods like `enable_slicing()`, `disable_slicing()`, `enable_tiling()`, and `disable_tiling()` to a class `AutoencoderMixin`. Not all VAEs implement slicing and tiling and that has been addressed accordingly.\r\n\r\nAs a consequence, we also reduce a bit of code bloat.",
      "html_url": "https://github.com/huggingface/diffusers/pull/12473",
      "created_at": "2025-10-13T04:53:56Z",
      "merged_at": "2025-10-22T03:22:07Z",
      "merge_commit_sha": "a5a0ccf86a8b2468709f964704dd3667cbb7ac8f",
      "base_ref": "main",
      "head_sha": "234ffa4cbed4c941d87c7450d06d8d29e9da6915",
      "user": "sayakpaul",
      "files": [
        {
          "filename": "src/diffusers/models/autoencoders/autoencoder_asym_kl.py",
          "status": "modified",
          "additions": 2,
          "deletions": 5,
          "changes": 7,
          "patch": "@@ -20,10 +20,10 @@\n from ...utils.accelerate_utils import apply_forward_hook\n from ..modeling_outputs import AutoencoderKLOutput\n from ..modeling_utils import ModelMixin\n-from .vae import DecoderOutput, DiagonalGaussianDistribution, Encoder, MaskConditionDecoder\n+from .vae import AutoencoderMixin, DecoderOutput, DiagonalGaussianDistribution, Encoder, MaskConditionDecoder\n \n \n-class AsymmetricAutoencoderKL(ModelMixin, ConfigMixin):\n+class AsymmetricAutoencoderKL(ModelMixin, AutoencoderMixin, ConfigMixin):\n     r\"\"\"\n     Designing a Better Asymmetric VQGAN for StableDiffusion https://huggingface.co/papers/2306.04632 . A VAE model with\n     KL loss for encoding images into latents and decoding latent representations into images.\n@@ -107,9 +107,6 @@ def __init__(\n         self.quant_conv = nn.Conv2d(2 * latent_channels, 2 * latent_channels, 1)\n         self.post_quant_conv = nn.Conv2d(latent_channels, latent_channels, 1)\n \n-        self.use_slicing = False\n-        self.use_tiling = False\n-\n         self.register_to_config(block_out_channels=up_block_out_channels)\n         self.register_to_config(force_upcast=False)\n "
        },
        {
          "filename": "src/diffusers/models/autoencoders/autoencoder_dc.py",
          "status": "modified",
          "additions": 2,
          "deletions": 23,
          "changes": 25,
          "patch": "@@ -27,7 +27,7 @@\n from ..modeling_utils import ModelMixin\n from ..normalization import RMSNorm, get_normalization\n from ..transformers.sana_transformer import GLUMBConv\n-from .vae import DecoderOutput, EncoderOutput\n+from .vae import AutoencoderMixin, DecoderOutput, EncoderOutput\n \n \n class ResBlock(nn.Module):\n@@ -378,7 +378,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return hidden_states\n \n \n-class AutoencoderDC(ModelMixin, ConfigMixin, FromOriginalModelMixin):\n+class AutoencoderDC(ModelMixin, AutoencoderMixin, ConfigMixin, FromOriginalModelMixin):\n     r\"\"\"\n     An Autoencoder model introduced in [DCAE](https://huggingface.co/papers/2410.10733) and used in\n     [SANA](https://huggingface.co/papers/2410.10629).\n@@ -536,27 +536,6 @@ def enable_tiling(\n         self.tile_latent_min_height = self.tile_sample_min_height // self.spatial_compression_ratio\n         self.tile_latent_min_width = self.tile_sample_min_width // self.spatial_compression_ratio\n \n-    def disable_tiling(self) -> None:\n-        r\"\"\"\n-        Disable tiled AE decoding. If `enable_tiling` was previously enabled, this method will go back to computing\n-        decoding in one step.\n-        \"\"\"\n-        self.use_tiling = False\n-\n-    def enable_slicing(self) -> None:\n-        r\"\"\"\n-        Enable sliced AE decoding. When this option is enabled, the AE will split the input tensor in slices to compute\n-        decoding in several steps. This is useful to save some memory and allow larger batch sizes.\n-        \"\"\"\n-        self.use_slicing = True\n-\n-    def disable_slicing(self) -> None:\n-        r\"\"\"\n-        Disable sliced AE decoding. If `enable_slicing` was previously enabled, this method will go back to computing\n-        decoding in one step.\n-        \"\"\"\n-        self.use_slicing = False\n-\n     def _encode(self, x: torch.Tensor) -> torch.Tensor:\n         batch_size, num_channels, height, width = x.shape\n "
        },
        {
          "filename": "src/diffusers/models/autoencoders/autoencoder_kl.py",
          "status": "modified",
          "additions": 2,
          "deletions": 31,
          "changes": 33,
          "patch": "@@ -32,10 +32,10 @@\n )\n from ..modeling_outputs import AutoencoderKLOutput\n from ..modeling_utils import ModelMixin\n-from .vae import Decoder, DecoderOutput, DiagonalGaussianDistribution, Encoder\n+from .vae import AutoencoderMixin, Decoder, DecoderOutput, DiagonalGaussianDistribution, Encoder\n \n \n-class AutoencoderKL(ModelMixin, ConfigMixin, FromOriginalModelMixin, PeftAdapterMixin):\n+class AutoencoderKL(ModelMixin, AutoencoderMixin, ConfigMixin, FromOriginalModelMixin, PeftAdapterMixin):\n     r\"\"\"\n     A VAE model with KL loss for encoding images into latents and decoding latent representations into images.\n \n@@ -138,35 +138,6 @@ def __init__(\n         self.tile_latent_min_size = int(sample_size / (2 ** (len(self.config.block_out_channels) - 1)))\n         self.tile_overlap_factor = 0.25\n \n-    def enable_tiling(self, use_tiling: bool = True):\n-        r\"\"\"\n-        Enable tiled VAE decoding. When this option is enabled, the VAE will split the input tensor into tiles to\n-        compute decoding and encoding in several steps. This is useful for saving a large amount of memory and to allow\n-        processing larger images.\n-        \"\"\"\n-        self.use_tiling = use_tiling\n-\n-    def disable_tiling(self):\n-        r\"\"\"\n-        Disable tiled VAE decoding. If `enable_tiling` was previously enabled, this method will go back to computing\n-        decoding in one step.\n-        \"\"\"\n-        self.enable_tiling(False)\n-\n-    def enable_slicing(self):\n-        r\"\"\"\n-        Enable sliced VAE decoding. When this option is enabled, the VAE will split the input tensor in slices to\n-        compute decoding in several steps. This is useful to save some memory and allow larger batch sizes.\n-        \"\"\"\n-        self.use_slicing = True\n-\n-    def disable_slicing(self):\n-        r\"\"\"\n-        Disable sliced VAE decoding. If `enable_slicing` was previously enabled, this method will go back to computing\n-        decoding in one step.\n-        \"\"\"\n-        self.use_slicing = False\n-\n     @property\n     # Copied from diffusers.models.unets.unet_2d_condition.UNet2DConditionModel.attn_processors\n     def attn_processors(self) -> Dict[str, AttentionProcessor]:"
        },
        {
          "filename": "src/diffusers/models/autoencoders/autoencoder_kl_allegro.py",
          "status": "modified",
          "additions": 2,
          "deletions": 30,
          "changes": 32,
          "patch": "@@ -28,6 +28,7 @@\n from ..modeling_utils import ModelMixin\n from ..resnet import ResnetBlock2D\n from ..upsampling import Upsample2D\n+from .vae import AutoencoderMixin\n \n \n class AllegroTemporalConvLayer(nn.Module):\n@@ -673,7 +674,7 @@ def forward(self, sample: torch.Tensor) -> torch.Tensor:\n         return sample\n \n \n-class AutoencoderKLAllegro(ModelMixin, ConfigMixin):\n+class AutoencoderKLAllegro(ModelMixin, AutoencoderMixin, ConfigMixin):\n     r\"\"\"\n     A VAE model with KL loss for encoding videos into latents and decoding latent representations into videos. Used in\n     [Allegro](https://github.com/rhymes-ai/Allegro).\n@@ -795,35 +796,6 @@ def __init__(\n             sample_size - self.tile_overlap_w,\n         )\n \n-    def enable_tiling(self) -> None:\n-        r\"\"\"\n-        Enable tiled VAE decoding. When this option is enabled, the VAE will split the input tensor into tiles to\n-        compute decoding and encoding in several steps. This is useful for saving a large amount of memory and to allow\n-        processing larger images.\n-        \"\"\"\n-        self.use_tiling = True\n-\n-    def disable_tiling(self) -> None:\n-        r\"\"\"\n-        Disable tiled VAE decoding. If `enable_tiling` was previously enabled, this method will go back to computing\n-        decoding in one step.\n-        \"\"\"\n-        self.use_tiling = False\n-\n-    def enable_slicing(self) -> None:\n-        r\"\"\"\n-        Enable sliced VAE decoding. When this option is enabled, the VAE will split the input tensor in slices to\n-        compute decoding in several steps. This is useful to save some memory and allow larger batch sizes.\n-        \"\"\"\n-        self.use_slicing = True\n-\n-    def disable_slicing(self) -> None:\n-        r\"\"\"\n-        Disable sliced VAE decoding. If `enable_slicing` was previously enabled, this method will go back to computing\n-        decoding in one step.\n-        \"\"\"\n-        self.use_slicing = False\n-\n     def _encode(self, x: torch.Tensor) -> torch.Tensor:\n         # TODO(aryan)\n         # if self.use_tiling and (width > self.tile_sample_min_width or height > self.tile_sample_min_height):"
        },
        {
          "filename": "src/diffusers/models/autoencoders/autoencoder_kl_cogvideox.py",
          "status": "modified",
          "additions": 2,
          "deletions": 23,
          "changes": 25,
          "patch": "@@ -29,7 +29,7 @@\n from ..modeling_outputs import AutoencoderKLOutput\n from ..modeling_utils import ModelMixin\n from ..upsampling import CogVideoXUpsample3D\n-from .vae import DecoderOutput, DiagonalGaussianDistribution\n+from .vae import AutoencoderMixin, DecoderOutput, DiagonalGaussianDistribution\n \n \n logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n@@ -955,7 +955,7 @@ def forward(\n         return hidden_states, new_conv_cache\n \n \n-class AutoencoderKLCogVideoX(ModelMixin, ConfigMixin, FromOriginalModelMixin):\n+class AutoencoderKLCogVideoX(ModelMixin, AutoencoderMixin, ConfigMixin, FromOriginalModelMixin):\n     r\"\"\"\n     A VAE model with KL loss for encoding images into latents and decoding latent representations into images. Used in\n     [CogVideoX](https://github.com/THUDM/CogVideo).\n@@ -1124,27 +1124,6 @@ def enable_tiling(\n         self.tile_overlap_factor_height = tile_overlap_factor_height or self.tile_overlap_factor_height\n         self.tile_overlap_factor_width = tile_overlap_factor_width or self.tile_overlap_factor_width\n \n-    def disable_tiling(self) -> None:\n-        r\"\"\"\n-        Disable tiled VAE decoding. If `enable_tiling` was previously enabled, this method will go back to computing\n-        decoding in one step.\n-        \"\"\"\n-        self.use_tiling = False\n-\n-    def enable_slicing(self) -> None:\n-        r\"\"\"\n-        Enable sliced VAE decoding. When this option is enabled, the VAE will split the input tensor in slices to\n-        compute decoding in several steps. This is useful to save some memory and allow larger batch sizes.\n-        \"\"\"\n-        self.use_slicing = True\n-\n-    def disable_slicing(self) -> None:\n-        r\"\"\"\n-        Disable sliced VAE decoding. If `enable_slicing` was previously enabled, this method will go back to computing\n-        decoding in one step.\n-        \"\"\"\n-        self.use_slicing = False\n-\n     def _encode(self, x: torch.Tensor) -> torch.Tensor:\n         batch_size, num_channels, num_frames, height, width = x.shape\n "
        },
        {
          "filename": "src/diffusers/models/autoencoders/autoencoder_kl_cosmos.py",
          "status": "modified",
          "additions": 2,
          "deletions": 23,
          "changes": 25,
          "patch": "@@ -24,7 +24,7 @@\n from ...utils.accelerate_utils import apply_forward_hook\n from ..modeling_outputs import AutoencoderKLOutput\n from ..modeling_utils import ModelMixin\n-from .vae import DecoderOutput, IdentityDistribution\n+from .vae import AutoencoderMixin, DecoderOutput, IdentityDistribution\n \n \n logger = get_logger(__name__)\n@@ -875,7 +875,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return hidden_states\n \n \n-class AutoencoderKLCosmos(ModelMixin, ConfigMixin):\n+class AutoencoderKLCosmos(ModelMixin, AutoencoderMixin, ConfigMixin):\n     r\"\"\"\n     Autoencoder used in [Cosmos](https://huggingface.co/papers/2501.03575).\n \n@@ -1031,27 +1031,6 @@ def enable_tiling(\n         self.tile_sample_stride_width = tile_sample_stride_width or self.tile_sample_stride_width\n         self.tile_sample_stride_num_frames = tile_sample_stride_num_frames or self.tile_sample_stride_num_frames\n \n-    def disable_tiling(self) -> None:\n-        r\"\"\"\n-        Disable tiled VAE decoding. If `enable_tiling` was previously enabled, this method will go back to computing\n-        decoding in one step.\n-        \"\"\"\n-        self.use_tiling = False\n-\n-    def enable_slicing(self) -> None:\n-        r\"\"\"\n-        Enable sliced VAE decoding. When this option is enabled, the VAE will split the input tensor in slices to\n-        compute decoding in several steps. This is useful to save some memory and allow larger batch sizes.\n-        \"\"\"\n-        self.use_slicing = True\n-\n-    def disable_slicing(self) -> None:\n-        r\"\"\"\n-        Disable sliced VAE decoding. If `enable_slicing` was previously enabled, this method will go back to computing\n-        decoding in one step.\n-        \"\"\"\n-        self.use_slicing = False\n-\n     def _encode(self, x: torch.Tensor) -> torch.Tensor:\n         x = self.encoder(x)\n         enc = self.quant_conv(x)"
        },
        {
          "filename": "src/diffusers/models/autoencoders/autoencoder_kl_hunyuan_video.py",
          "status": "modified",
          "additions": 2,
          "deletions": 23,
          "changes": 25,
          "patch": "@@ -26,7 +26,7 @@\n from ..attention_processor import Attention\n from ..modeling_outputs import AutoencoderKLOutput\n from ..modeling_utils import ModelMixin\n-from .vae import DecoderOutput, DiagonalGaussianDistribution\n+from .vae import AutoencoderMixin, DecoderOutput, DiagonalGaussianDistribution\n \n \n logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n@@ -624,7 +624,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return hidden_states\n \n \n-class AutoencoderKLHunyuanVideo(ModelMixin, ConfigMixin):\n+class AutoencoderKLHunyuanVideo(ModelMixin, AutoencoderMixin, ConfigMixin):\n     r\"\"\"\n     A VAE model with KL loss for encoding videos into latents and decoding latent representations into videos.\n     Introduced in [HunyuanVideo](https://huggingface.co/papers/2412.03603).\n@@ -763,27 +763,6 @@ def enable_tiling(\n         self.tile_sample_stride_width = tile_sample_stride_width or self.tile_sample_stride_width\n         self.tile_sample_stride_num_frames = tile_sample_stride_num_frames or self.tile_sample_stride_num_frames\n \n-    def disable_tiling(self) -> None:\n-        r\"\"\"\n-        Disable tiled VAE decoding. If `enable_tiling` was previously enabled, this method will go back to computing\n-        decoding in one step.\n-        \"\"\"\n-        self.use_tiling = False\n-\n-    def enable_slicing(self) -> None:\n-        r\"\"\"\n-        Enable sliced VAE decoding. When this option is enabled, the VAE will split the input tensor in slices to\n-        compute decoding in several steps. This is useful to save some memory and allow larger batch sizes.\n-        \"\"\"\n-        self.use_slicing = True\n-\n-    def disable_slicing(self) -> None:\n-        r\"\"\"\n-        Disable sliced VAE decoding. If `enable_slicing` was previously enabled, this method will go back to computing\n-        decoding in one step.\n-        \"\"\"\n-        self.use_slicing = False\n-\n     def _encode(self, x: torch.Tensor) -> torch.Tensor:\n         batch_size, num_channels, num_frames, height, width = x.shape\n "
        },
        {
          "filename": "src/diffusers/models/autoencoders/autoencoder_kl_ltx.py",
          "status": "modified",
          "additions": 2,
          "deletions": 23,
          "changes": 25,
          "patch": "@@ -26,7 +26,7 @@\n from ..modeling_outputs import AutoencoderKLOutput\n from ..modeling_utils import ModelMixin\n from ..normalization import RMSNorm\n-from .vae import DecoderOutput, DiagonalGaussianDistribution\n+from .vae import AutoencoderMixin, DecoderOutput, DiagonalGaussianDistribution\n \n \n class LTXVideoCausalConv3d(nn.Module):\n@@ -1034,7 +1034,7 @@ def forward(self, hidden_states: torch.Tensor, temb: Optional[torch.Tensor] = No\n         return hidden_states\n \n \n-class AutoencoderKLLTXVideo(ModelMixin, ConfigMixin, FromOriginalModelMixin):\n+class AutoencoderKLLTXVideo(ModelMixin, AutoencoderMixin, ConfigMixin, FromOriginalModelMixin):\n     r\"\"\"\n     A VAE model with KL loss for encoding images into latents and decoding latent representations into images. Used in\n     [LTX](https://huggingface.co/Lightricks/LTX-Video).\n@@ -1219,27 +1219,6 @@ def enable_tiling(\n         self.tile_sample_stride_width = tile_sample_stride_width or self.tile_sample_stride_width\n         self.tile_sample_stride_num_frames = tile_sample_stride_num_frames or self.tile_sample_stride_num_frames\n \n-    def disable_tiling(self) -> None:\n-        r\"\"\"\n-        Disable tiled VAE decoding. If `enable_tiling` was previously enabled, this method will go back to computing\n-        decoding in one step.\n-        \"\"\"\n-        self.use_tiling = False\n-\n-    def enable_slicing(self) -> None:\n-        r\"\"\"\n-        Enable sliced VAE decoding. When this option is enabled, the VAE will split the input tensor in slices to\n-        compute decoding in several steps. This is useful to save some memory and allow larger batch sizes.\n-        \"\"\"\n-        self.use_slicing = True\n-\n-    def disable_slicing(self) -> None:\n-        r\"\"\"\n-        Disable sliced VAE decoding. If `enable_slicing` was previously enabled, this method will go back to computing\n-        decoding in one step.\n-        \"\"\"\n-        self.use_slicing = False\n-\n     def _encode(self, x: torch.Tensor) -> torch.Tensor:\n         batch_size, num_channels, num_frames, height, width = x.shape\n "
        },
        {
          "filename": "src/diffusers/models/autoencoders/autoencoder_kl_magvit.py",
          "status": "modified",
          "additions": 2,
          "deletions": 23,
          "changes": 25,
          "patch": "@@ -26,7 +26,7 @@\n from ..activations import get_activation\n from ..modeling_outputs import AutoencoderKLOutput\n from ..modeling_utils import ModelMixin\n-from .vae import DecoderOutput, DiagonalGaussianDistribution\n+from .vae import AutoencoderMixin, DecoderOutput, DiagonalGaussianDistribution\n \n \n logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n@@ -663,7 +663,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return hidden_states\n \n \n-class AutoencoderKLMagvit(ModelMixin, ConfigMixin):\n+class AutoencoderKLMagvit(ModelMixin, AutoencoderMixin, ConfigMixin):\n     r\"\"\"\n     A VAE model with KL loss for encoding images into latents and decoding latent representations into images. This\n     model is used in [EasyAnimate](https://huggingface.co/papers/2405.18991).\n@@ -805,27 +805,6 @@ def enable_tiling(\n         self.tile_sample_stride_width = tile_sample_stride_width or self.tile_sample_stride_width\n         self.tile_sample_stride_num_frames = tile_sample_stride_num_frames or self.tile_sample_stride_num_frames\n \n-    def disable_tiling(self) -> None:\n-        r\"\"\"\n-        Disable tiled VAE decoding. If `enable_tiling` was previously enabled, this method will go back to computing\n-        decoding in one step.\n-        \"\"\"\n-        self.use_tiling = False\n-\n-    def enable_slicing(self) -> None:\n-        r\"\"\"\n-        Enable sliced VAE decoding. When this option is enabled, the VAE will split the input tensor in slices to\n-        compute decoding in several steps. This is useful to save some memory and allow larger batch sizes.\n-        \"\"\"\n-        self.use_slicing = True\n-\n-    def disable_slicing(self) -> None:\n-        r\"\"\"\n-        Disable sliced VAE decoding. If `enable_slicing` was previously enabled, this method will go back to computing\n-        decoding in one step.\n-        \"\"\"\n-        self.use_slicing = False\n-\n     @apply_forward_hook\n     def _encode(\n         self, x: torch.Tensor, return_dict: bool = True"
        },
        {
          "filename": "src/diffusers/models/autoencoders/autoencoder_kl_mochi.py",
          "status": "modified",
          "additions": 2,
          "deletions": 23,
          "changes": 25,
          "patch": "@@ -27,7 +27,7 @@\n from ..modeling_outputs import AutoencoderKLOutput\n from ..modeling_utils import ModelMixin\n from .autoencoder_kl_cogvideox import CogVideoXCausalConv3d\n-from .vae import DecoderOutput, DiagonalGaussianDistribution\n+from .vae import AutoencoderMixin, DecoderOutput, DiagonalGaussianDistribution\n \n \n logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n@@ -657,7 +657,7 @@ def forward(\n         return hidden_states, new_conv_cache\n \n \n-class AutoencoderKLMochi(ModelMixin, ConfigMixin):\n+class AutoencoderKLMochi(ModelMixin, AutoencoderMixin, ConfigMixin):\n     r\"\"\"\n     A VAE model with KL loss for encoding images into latents and decoding latent representations into images. Used in\n     [Mochi 1 preview](https://github.com/genmoai/models).\n@@ -818,27 +818,6 @@ def enable_tiling(\n         self.tile_sample_stride_height = tile_sample_stride_height or self.tile_sample_stride_height\n         self.tile_sample_stride_width = tile_sample_stride_width or self.tile_sample_stride_width\n \n-    def disable_tiling(self) -> None:\n-        r\"\"\"\n-        Disable tiled VAE decoding. If `enable_tiling` was previously enabled, this method will go back to computing\n-        decoding in one step.\n-        \"\"\"\n-        self.use_tiling = False\n-\n-    def enable_slicing(self) -> None:\n-        r\"\"\"\n-        Enable sliced VAE decoding. When this option is enabled, the VAE will split the input tensor in slices to\n-        compute decoding in several steps. This is useful to save some memory and allow larger batch sizes.\n-        \"\"\"\n-        self.use_slicing = True\n-\n-    def disable_slicing(self) -> None:\n-        r\"\"\"\n-        Disable sliced VAE decoding. If `enable_slicing` was previously enabled, this method will go back to computing\n-        decoding in one step.\n-        \"\"\"\n-        self.use_slicing = False\n-\n     def _enable_framewise_encoding(self):\n         r\"\"\"\n         Enables the framewise VAE encoding implementation with past latent padding. By default, Diffusers uses the"
        },
        {
          "filename": "src/diffusers/models/autoencoders/autoencoder_kl_qwenimage.py",
          "status": "modified",
          "additions": 2,
          "deletions": 23,
          "changes": 25,
          "patch": "@@ -31,7 +31,7 @@\n from ..activations import get_activation\n from ..modeling_outputs import AutoencoderKLOutput\n from ..modeling_utils import ModelMixin\n-from .vae import DecoderOutput, DiagonalGaussianDistribution\n+from .vae import AutoencoderMixin, DecoderOutput, DiagonalGaussianDistribution\n \n \n logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n@@ -663,7 +663,7 @@ def forward(self, x, feat_cache=None, feat_idx=[0]):\n         return x\n \n \n-class AutoencoderKLQwenImage(ModelMixin, ConfigMixin, FromOriginalModelMixin):\n+class AutoencoderKLQwenImage(ModelMixin, AutoencoderMixin, ConfigMixin, FromOriginalModelMixin):\n     r\"\"\"\n     A VAE model with KL loss for encoding videos into latents and decoding latent representations into videos.\n \n@@ -763,27 +763,6 @@ def enable_tiling(\n         self.tile_sample_stride_height = tile_sample_stride_height or self.tile_sample_stride_height\n         self.tile_sample_stride_width = tile_sample_stride_width or self.tile_sample_stride_width\n \n-    def disable_tiling(self) -> None:\n-        r\"\"\"\n-        Disable tiled VAE decoding. If `enable_tiling` was previously enabled, this method will go back to computing\n-        decoding in one step.\n-        \"\"\"\n-        self.use_tiling = False\n-\n-    def enable_slicing(self) -> None:\n-        r\"\"\"\n-        Enable sliced VAE decoding. When this option is enabled, the VAE will split the input tensor in slices to\n-        compute decoding in several steps. This is useful to save some memory and allow larger batch sizes.\n-        \"\"\"\n-        self.use_slicing = True\n-\n-    def disable_slicing(self) -> None:\n-        r\"\"\"\n-        Disable sliced VAE decoding. If `enable_slicing` was previously enabled, this method will go back to computing\n-        decoding in one step.\n-        \"\"\"\n-        self.use_slicing = False\n-\n     def clear_cache(self):\n         def _count_conv3d(model):\n             count = 0"
        },
        {
          "filename": "src/diffusers/models/autoencoders/autoencoder_kl_temporal_decoder.py",
          "status": "modified",
          "additions": 2,
          "deletions": 2,
          "changes": 4,
          "patch": "@@ -23,7 +23,7 @@\n from ..modeling_outputs import AutoencoderKLOutput\n from ..modeling_utils import ModelMixin\n from ..unets.unet_3d_blocks import MidBlockTemporalDecoder, UpBlockTemporalDecoder\n-from .vae import DecoderOutput, DiagonalGaussianDistribution, Encoder\n+from .vae import AutoencoderMixin, DecoderOutput, DiagonalGaussianDistribution, Encoder\n \n \n class TemporalDecoder(nn.Module):\n@@ -135,7 +135,7 @@ def forward(\n         return sample\n \n \n-class AutoencoderKLTemporalDecoder(ModelMixin, ConfigMixin):\n+class AutoencoderKLTemporalDecoder(ModelMixin, AutoencoderMixin, ConfigMixin):\n     r\"\"\"\n     A VAE model with KL loss for encoding images into latents and decoding latent representations into images.\n "
        },
        {
          "filename": "src/diffusers/models/autoencoders/autoencoder_kl_wan.py",
          "status": "modified",
          "additions": 2,
          "deletions": 23,
          "changes": 25,
          "patch": "@@ -25,7 +25,7 @@\n from ..activations import get_activation\n from ..modeling_outputs import AutoencoderKLOutput\n from ..modeling_utils import ModelMixin\n-from .vae import DecoderOutput, DiagonalGaussianDistribution\n+from .vae import AutoencoderMixin, DecoderOutput, DiagonalGaussianDistribution\n \n \n logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n@@ -951,7 +951,7 @@ def unpatchify(x, patch_size):\n     return x\n \n \n-class AutoencoderKLWan(ModelMixin, ConfigMixin, FromOriginalModelMixin):\n+class AutoencoderKLWan(ModelMixin, AutoencoderMixin, ConfigMixin, FromOriginalModelMixin):\n     r\"\"\"\n     A VAE model with KL loss for encoding videos into latents and decoding latent representations into videos.\n     Introduced in [Wan 2.1].\n@@ -1110,27 +1110,6 @@ def enable_tiling(\n         self.tile_sample_stride_height = tile_sample_stride_height or self.tile_sample_stride_height\n         self.tile_sample_stride_width = tile_sample_stride_width or self.tile_sample_stride_width\n \n-    def disable_tiling(self) -> None:\n-        r\"\"\"\n-        Disable tiled VAE decoding. If `enable_tiling` was previously enabled, this method will go back to computing\n-        decoding in one step.\n-        \"\"\"\n-        self.use_tiling = False\n-\n-    def enable_slicing(self) -> None:\n-        r\"\"\"\n-        Enable sliced VAE decoding. When this option is enabled, the VAE will split the input tensor in slices to\n-        compute decoding in several steps. This is useful to save some memory and allow larger batch sizes.\n-        \"\"\"\n-        self.use_slicing = True\n-\n-    def disable_slicing(self) -> None:\n-        r\"\"\"\n-        Disable sliced VAE decoding. If `enable_slicing` was previously enabled, this method will go back to computing\n-        decoding in one step.\n-        \"\"\"\n-        self.use_slicing = False\n-\n     def clear_cache(self):\n         # Use cached conv counts for decoder and encoder to avoid re-iterating modules each call\n         self._conv_num = self._cached_conv_counts[\"decoder\"]"
        },
        {
          "filename": "src/diffusers/models/autoencoders/autoencoder_oobleck.py",
          "status": "modified",
          "additions": 2,
          "deletions": 15,
          "changes": 17,
          "patch": "@@ -25,6 +25,7 @@\n from ...utils.accelerate_utils import apply_forward_hook\n from ...utils.torch_utils import randn_tensor\n from ..modeling_utils import ModelMixin\n+from .vae import AutoencoderMixin\n \n \n class Snake1d(nn.Module):\n@@ -291,7 +292,7 @@ def forward(self, hidden_state):\n         return hidden_state\n \n \n-class AutoencoderOobleck(ModelMixin, ConfigMixin):\n+class AutoencoderOobleck(ModelMixin, AutoencoderMixin, ConfigMixin):\n     r\"\"\"\n     An autoencoder for encoding waveforms into latents and decoding latent representations into waveforms. First\n     introduced in Stable Audio.\n@@ -356,20 +357,6 @@ def __init__(\n \n         self.use_slicing = False\n \n-    def enable_slicing(self):\n-        r\"\"\"\n-        Enable sliced VAE decoding. When this option is enabled, the VAE will split the input tensor in slices to\n-        compute decoding in several steps. This is useful to save some memory and allow larger batch sizes.\n-        \"\"\"\n-        self.use_slicing = True\n-\n-    def disable_slicing(self):\n-        r\"\"\"\n-        Disable sliced VAE decoding. If `enable_slicing` was previously enabled, this method will go back to computing\n-        decoding in one step.\n-        \"\"\"\n-        self.use_slicing = False\n-\n     @apply_forward_hook\n     def encode(\n         self, x: torch.Tensor, return_dict: bool = True"
        },
        {
          "filename": "src/diffusers/models/autoencoders/autoencoder_tiny.py",
          "status": "modified",
          "additions": 2,
          "deletions": 31,
          "changes": 33,
          "patch": "@@ -22,7 +22,7 @@\n from ...utils import BaseOutput\n from ...utils.accelerate_utils import apply_forward_hook\n from ..modeling_utils import ModelMixin\n-from .vae import DecoderOutput, DecoderTiny, EncoderTiny\n+from .vae import AutoencoderMixin, DecoderOutput, DecoderTiny, EncoderTiny\n \n \n @dataclass\n@@ -38,7 +38,7 @@ class AutoencoderTinyOutput(BaseOutput):\n     latents: torch.Tensor\n \n \n-class AutoencoderTiny(ModelMixin, ConfigMixin):\n+class AutoencoderTiny(ModelMixin, AutoencoderMixin, ConfigMixin):\n     r\"\"\"\n     A tiny distilled VAE model for encoding images into latents and decoding latent representations into images.\n \n@@ -162,35 +162,6 @@ def unscale_latents(self, x: torch.Tensor) -> torch.Tensor:\n         \"\"\"[0, 1] -> raw latents\"\"\"\n         return x.sub(self.latent_shift).mul(2 * self.latent_magnitude)\n \n-    def enable_slicing(self) -> None:\n-        r\"\"\"\n-        Enable sliced VAE decoding. When this option is enabled, the VAE will split the input tensor in slices to\n-        compute decoding in several steps. This is useful to save some memory and allow larger batch sizes.\n-        \"\"\"\n-        self.use_slicing = True\n-\n-    def disable_slicing(self) -> None:\n-        r\"\"\"\n-        Disable sliced VAE decoding. If `enable_slicing` was previously enabled, this method will go back to computing\n-        decoding in one step.\n-        \"\"\"\n-        self.use_slicing = False\n-\n-    def enable_tiling(self, use_tiling: bool = True) -> None:\n-        r\"\"\"\n-        Enable tiled VAE decoding. When this option is enabled, the VAE will split the input tensor into tiles to\n-        compute decoding and encoding in several steps. This is useful for saving a large amount of memory and to allow\n-        processing larger images.\n-        \"\"\"\n-        self.use_tiling = use_tiling\n-\n-    def disable_tiling(self) -> None:\n-        r\"\"\"\n-        Disable tiled VAE decoding. If `enable_tiling` was previously enabled, this method will go back to computing\n-        decoding in one step.\n-        \"\"\"\n-        self.enable_tiling(False)\n-\n     def _tiled_encode(self, x: torch.Tensor) -> torch.Tensor:\n         r\"\"\"Encode a batch of images using a tiled encoder.\n "
        },
        {
          "filename": "src/diffusers/models/autoencoders/consistency_decoder_vae.py",
          "status": "modified",
          "additions": 2,
          "deletions": 35,
          "changes": 37,
          "patch": "@@ -32,7 +32,7 @@\n )\n from ..modeling_utils import ModelMixin\n from ..unets.unet_2d import UNet2DModel\n-from .vae import DecoderOutput, DiagonalGaussianDistribution, Encoder\n+from .vae import AutoencoderMixin, DecoderOutput, DiagonalGaussianDistribution, Encoder\n \n \n @dataclass\n@@ -49,7 +49,7 @@ class ConsistencyDecoderVAEOutput(BaseOutput):\n     latent_dist: \"DiagonalGaussianDistribution\"\n \n \n-class ConsistencyDecoderVAE(ModelMixin, ConfigMixin):\n+class ConsistencyDecoderVAE(ModelMixin, AutoencoderMixin, ConfigMixin):\n     r\"\"\"\n     The consistency decoder used with DALL-E 3.\n \n@@ -167,39 +167,6 @@ def __init__(\n         self.tile_latent_min_size = int(sample_size / (2 ** (len(self.config.block_out_channels) - 1)))\n         self.tile_overlap_factor = 0.25\n \n-    # Copied from diffusers.models.autoencoders.autoencoder_kl.AutoencoderKL.enable_tiling\n-    def enable_tiling(self, use_tiling: bool = True):\n-        r\"\"\"\n-        Enable tiled VAE decoding. When this option is enabled, the VAE will split the input tensor into tiles to\n-        compute decoding and encoding in several steps. This is useful for saving a large amount of memory and to allow\n-        processing larger images.\n-        \"\"\"\n-        self.use_tiling = use_tiling\n-\n-    # Copied from diffusers.models.autoencoders.autoencoder_kl.AutoencoderKL.disable_tiling\n-    def disable_tiling(self):\n-        r\"\"\"\n-        Disable tiled VAE decoding. If `enable_tiling` was previously enabled, this method will go back to computing\n-        decoding in one step.\n-        \"\"\"\n-        self.enable_tiling(False)\n-\n-    # Copied from diffusers.models.autoencoders.autoencoder_kl.AutoencoderKL.enable_slicing\n-    def enable_slicing(self):\n-        r\"\"\"\n-        Enable sliced VAE decoding. When this option is enabled, the VAE will split the input tensor in slices to\n-        compute decoding in several steps. This is useful to save some memory and allow larger batch sizes.\n-        \"\"\"\n-        self.use_slicing = True\n-\n-    # Copied from diffusers.models.autoencoders.autoencoder_kl.AutoencoderKL.disable_slicing\n-    def disable_slicing(self):\n-        r\"\"\"\n-        Disable sliced VAE decoding. If `enable_slicing` was previously enabled, this method will go back to computing\n-        decoding in one step.\n-        \"\"\"\n-        self.use_slicing = False\n-\n     @property\n     # Copied from diffusers.models.unets.unet_2d_condition.UNet2DConditionModel.attn_processors\n     def attn_processors(self) -> Dict[str, AttentionProcessor]:"
        },
        {
          "filename": "src/diffusers/models/autoencoders/vae.py",
          "status": "modified",
          "additions": 35,
          "deletions": 0,
          "changes": 35,
          "patch": "@@ -894,3 +894,38 @@ def forward(self, x: torch.Tensor) -> torch.Tensor:\n \n         # scale image from [0, 1] to [-1, 1] to match diffusers convention\n         return x.mul(2).sub(1)\n+\n+\n+class AutoencoderMixin:\n+    def enable_tiling(self):\n+        r\"\"\"\n+        Enable tiled VAE decoding. When this option is enabled, the VAE will split the input tensor into tiles to\n+        compute decoding and encoding in several steps. This is useful for saving a large amount of memory and to allow\n+        processing larger images.\n+        \"\"\"\n+        if not hasattr(self, \"use_tiling\"):\n+            raise NotImplementedError(f\"Tiling doesn't seem to be implemented for {self.__class__.__name__}.\")\n+        self.use_tiling = True\n+\n+    def disable_tiling(self):\n+        r\"\"\"\n+        Disable tiled VAE decoding. If `enable_tiling` was previously enabled, this method will go back to computing\n+        decoding in one step.\n+        \"\"\"\n+        self.use_tiling = False\n+\n+    def enable_slicing(self):\n+        r\"\"\"\n+        Enable sliced VAE decoding. When this option is enabled, the VAE will split the input tensor in slices to\n+        compute decoding in several steps. This is useful to save some memory and allow larger batch sizes.\n+        \"\"\"\n+        if not hasattr(self, \"use_slicing\"):\n+            raise NotImplementedError(f\"Slicing doesn't seem to be implemented for {self.__class__.__name__}.\")\n+        self.use_slicing = True\n+\n+    def disable_slicing(self):\n+        r\"\"\"\n+        Disable sliced VAE decoding. If `enable_slicing` was previously enabled, this method will go back to computing\n+        decoding in one step.\n+        \"\"\"\n+        self.use_slicing = False"
        },
        {
          "filename": "src/diffusers/models/autoencoders/vq_model.py",
          "status": "modified",
          "additions": 2,
          "deletions": 1,
          "changes": 3,
          "patch": "@@ -22,6 +22,7 @@\n from ...utils.accelerate_utils import apply_forward_hook\n from ..autoencoders.vae import Decoder, DecoderOutput, Encoder, VectorQuantizer\n from ..modeling_utils import ModelMixin\n+from .vae import AutoencoderMixin\n \n \n @dataclass\n@@ -37,7 +38,7 @@ class VQEncoderOutput(BaseOutput):\n     latents: torch.Tensor\n \n \n-class VQModel(ModelMixin, ConfigMixin):\n+class VQModel(ModelMixin, AutoencoderMixin, ConfigMixin):\n     r\"\"\"\n     A VQ-VAE model for decoding latent representations.\n "
        },
        {
          "filename": "tests/models/autoencoders/testing_utils.py",
          "status": "modified",
          "additions": 5,
          "deletions": 0,
          "changes": 5,
          "patch": "@@ -57,6 +57,9 @@ def test_enable_disable_tiling(self):\n         torch.manual_seed(0)\n         model = self.model_class(**init_dict).to(torch_device)\n \n+        if not hasattr(model, \"use_tiling\"):\n+            pytest.skip(f\"Skipping test as {self.model_class.__name__} doesn't support tiling.\")\n+\n         inputs_dict.update({\"return_dict\": False})\n         _ = inputs_dict.pop(\"generator\", None)\n         accepts_generator = self._accepts_generator(model)\n@@ -102,6 +105,8 @@ def test_enable_disable_slicing(self):\n \n         torch.manual_seed(0)\n         model = self.model_class(**init_dict).to(torch_device)\n+        if not hasattr(model, \"use_slicing\"):\n+            pytest.skip(f\"Skipping test as {self.model_class.__name__} doesn't support tiling.\")\n \n         inputs_dict.update({\"return_dict\": False})\n         _ = inputs_dict.pop(\"generator\", None)"
        }
      ],
      "num_files": 19,
      "scraped_at": "2025-11-16T21:19:01.144916"
    },
    {
      "pr_number": 12464,
      "title": "[docs] Fix syntax",
      "body": "Fixes some syntax issues causing the doc-builder to break",
      "html_url": "https://github.com/huggingface/diffusers/pull/12464",
      "created_at": "2025-10-10T16:10:00Z",
      "merged_at": "2025-10-11T02:43:30Z",
      "merge_commit_sha": "8abc7aeb715c0149ee0a9982b2d608ce97f55215",
      "base_ref": "main",
      "head_sha": "31162ff52052788fdcf0280bae83c757a96d49df",
      "user": "stevhliu",
      "files": [
        {
          "filename": "docs/source/en/api/pipelines/marigold.md",
          "status": "modified",
          "additions": 1,
          "deletions": 1,
          "changes": 2,
          "patch": "@@ -75,7 +75,7 @@ The following is a summary of the recommended checkpoints, all of which produce\n | [prs-eth/marigold-depth-v1-1](https://huggingface.co/prs-eth/marigold-depth-v1-1)                   | Depth        | Affine-invariant depth prediction assigns each pixel a value between 0 (near plane) and 1 (far plane), with both planes determined by the model during inference.                    |\n | [prs-eth/marigold-normals-v0-1](https://huggingface.co/prs-eth/marigold-normals-v0-1)               | Normals      | The surface normals predictions are unit-length 3D vectors in the screen space camera, with values in the range from -1 to 1.                                                        |\n | [prs-eth/marigold-iid-appearance-v1-1](https://huggingface.co/prs-eth/marigold-iid-appearance-v1-1) | Intrinsics   | InteriorVerse decomposition is comprised of Albedo and two BRDF material properties: Roughness and Metallicity.                                                                      | \n-| [prs-eth/marigold-iid-lighting-v1-1](https://huggingface.co/prs-eth/marigold-iid-lighting-v1-1)     | Intrinsics   | HyperSim decomposition of an image &nbsp\\\\(I\\\\)&nbsp is comprised of Albedo &nbsp\\\\(A\\\\), Diffuse shading &nbsp\\\\(S\\\\), and Non-diffuse residual &nbsp\\\\(R\\\\): &nbsp\\\\(I = A*S+R\\\\). |\n+| [prs-eth/marigold-iid-lighting-v1-1](https://huggingface.co/prs-eth/marigold-iid-lighting-v1-1)     | Intrinsics   | HyperSim decomposition of an image $I$ is comprised of Albedo $A$, Diffuse shading $S$, and Non-diffuse residual $R$: $I = A*S+R$. |\n \n > [!TIP]\n > Make sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff "
        },
        {
          "filename": "src/diffusers/pipelines/marigold/pipeline_marigold_depth.py",
          "status": "modified",
          "additions": 5,
          "deletions": 6,
          "changes": 11,
          "patch": "@@ -86,15 +86,14 @@ class MarigoldDepthOutput(BaseOutput):\n \n     Args:\n         prediction (`np.ndarray`, `torch.Tensor`):\n-            Predicted depth maps with values in the range [0, 1]. The shape is $numimages \\times 1 \\times height \\times\n-            width$ for `torch.Tensor` or $numimages \\times height \\times width \\times 1$ for `np.ndarray`.\n+            Predicted depth maps with values in the range [0, 1]. The shape is `numimages \u00d7 1 \u00d7 height \u00d7 width` for\n+            `torch.Tensor` or `numimages \u00d7 height \u00d7 width \u00d7 1` for `np.ndarray`.\n         uncertainty (`None`, `np.ndarray`, `torch.Tensor`):\n-            Uncertainty maps computed from the ensemble, with values in the range [0, 1]. The shape is $numimages\n-            \\times 1 \\times height \\times width$ for `torch.Tensor` or $numimages \\times height \\times width \\times 1$\n-            for `np.ndarray`.\n+            Uncertainty maps computed from the ensemble, with values in the range [0, 1]. The shape is `numimages \u00d7 1 \u00d7\n+            height \u00d7 width` for `torch.Tensor` or `numimages \u00d7 height \u00d7 width \u00d7 1` for `np.ndarray`.\n         latent (`None`, `torch.Tensor`):\n             Latent features corresponding to the predictions, compatible with the `latents` argument of the pipeline.\n-            The shape is $numimages * numensemble \\times 4 \\times latentheight \\times latentwidth$.\n+            The shape is `numimages * numensemble \u00d7 4 \u00d7 latentheight \u00d7 latentwidth`.\n     \"\"\"\n \n     prediction: Union[np.ndarray, torch.Tensor]"
        },
        {
          "filename": "src/diffusers/pipelines/marigold/pipeline_marigold_intrinsics.py",
          "status": "modified",
          "additions": 8,
          "deletions": 8,
          "changes": 16,
          "patch": "@@ -99,17 +99,17 @@ class MarigoldIntrinsicsOutput(BaseOutput):\n \n     Args:\n         prediction (`np.ndarray`, `torch.Tensor`):\n-            Predicted image intrinsics with values in the range [0, 1]. The shape is $(numimages * numtargets) \\times 3\n-            \\times height \\times width$ for `torch.Tensor` or $(numimages * numtargets) \\times height \\times width\n-            \\times 3$ for `np.ndarray`, where `numtargets` corresponds to the number of predicted target modalities of\n-            the intrinsic image decomposition.\n+            Predicted image intrinsics with values in the range [0, 1]. The shape is `(numimages * numtargets) \u00d7 3 \u00d7\n+            height \u00d7 width` for `torch.Tensor` or `(numimages * numtargets) \u00d7 height \u00d7 width \u00d7 3` for `np.ndarray`,\n+            where `numtargets` corresponds to the number of predicted target modalities of the intrinsic image\n+            decomposition.\n         uncertainty (`None`, `np.ndarray`, `torch.Tensor`):\n-            Uncertainty maps computed from the ensemble, with values in the range [0, 1]. The shape is $(numimages *\n-            numtargets) \\times 3 \\times height \\times width$ for `torch.Tensor` or $(numimages * numtargets) \\times\n-            height \\times width \\times 3$ for `np.ndarray`.\n+            Uncertainty maps computed from the ensemble, with values in the range [0, 1]. The shape is `(numimages *\n+            numtargets) \u00d7 3 \u00d7 height \u00d7 width` for `torch.Tensor` or `(numimages * numtargets) \u00d7 height \u00d7 width \u00d7 3` for\n+            `np.ndarray`.\n         latent (`None`, `torch.Tensor`):\n             Latent features corresponding to the predictions, compatible with the `latents` argument of the pipeline.\n-            The shape is $(numimages * numensemble) \\times (numtargets * 4) \\times latentheight \\times latentwidth$.\n+            The shape is `(numimages * numensemble) \u00d7 (numtargets * 4) \u00d7 latentheight \u00d7 latentwidth`.\n     \"\"\"\n \n     prediction: Union[np.ndarray, torch.Tensor]"
        },
        {
          "filename": "src/diffusers/pipelines/marigold/pipeline_marigold_normals.py",
          "status": "modified",
          "additions": 5,
          "deletions": 6,
          "changes": 11,
          "patch": "@@ -81,15 +81,14 @@ class MarigoldNormalsOutput(BaseOutput):\n \n     Args:\n         prediction (`np.ndarray`, `torch.Tensor`):\n-            Predicted normals with values in the range [-1, 1]. The shape is $numimages \\times 3 \\times height \\times\n-            width$ for `torch.Tensor` or $numimages \\times height \\times width \\times 3$ for `np.ndarray`.\n+            Predicted normals with values in the range [-1, 1]. The shape is `numimages \u00d7 3 \u00d7 height \u00d7 width` for\n+            `torch.Tensor` or `numimages \u00d7 height \u00d7 width \u00d7 3` for `np.ndarray`.\n         uncertainty (`None`, `np.ndarray`, `torch.Tensor`):\n-            Uncertainty maps computed from the ensemble, with values in the range [0, 1]. The shape is $numimages\n-            \\times 1 \\times height \\times width$ for `torch.Tensor` or $numimages \\times height \\times width \\times 1$\n-            for `np.ndarray`.\n+            Uncertainty maps computed from the ensemble, with values in the range [0, 1]. The shape is `numimages \u00d7 1 \u00d7\n+            height \u00d7 width` for `torch.Tensor` or `numimages \u00d7 height \u00d7 width \u00d7 1` for `np.ndarray`.\n         latent (`None`, `torch.Tensor`):\n             Latent features corresponding to the predictions, compatible with the `latents` argument of the pipeline.\n-            The shape is $numimages * numensemble \\times 4 \\times latentheight \\times latentwidth$.\n+            The shape is `numimages * numensemble \u00d7 4 \u00d7 latentheight \u00d7 latentwidth`.\n     \"\"\"\n \n     prediction: Union[np.ndarray, torch.Tensor]"
        }
      ],
      "num_files": 4,
      "scraped_at": "2025-11-16T21:19:01.745246"
    },
    {
      "pr_number": 12456,
      "title": "Add Photon model and pipeline support",
      "body": "This commit adds support for the Photon image generation model:\r\n- PhotonTransformer2DModel: Core transformer architecture\r\n- PhotonPipeline: Text-to-image generation pipeline\r\n- Attention processor updates for Photon-specific attention mechanism\r\n- Conversion script for loading Photon checkpoints\r\n- Documentation and tests\r\n\r\nSome exemples below with the 512 model fine-tuned on the Alchemist dataset and distilled with PAG\r\n\r\n<img width=\"1000\" height=\"1000\" alt=\"image_10\" src=\"https://github.com/user-attachments/assets/254d4438-2c26-4efc-8da2-ed57983116da\" />\r\n<img width=\"1000\" height=\"1000\" alt=\"image_4\" src=\"https://github.com/user-attachments/assets/ff9b6691-90a8-4d14-ae6d-abcc9c6dadc6\" />\r\n<img width=\"1000\" height=\"1000\" alt=\"image_0\" src=\"https://github.com/user-attachments/assets/c3db9c19-975a-4d3f-9462-f07a2127b046\" />\r\n<img width=\"1000\" height=\"1000\" alt=\"image_1\" src=\"https://github.com/user-attachments/assets/fcfa00e3-ce1b-4889-a65b-b921caaa9cd7\" />\r\n\r\n\r\n\r\n\r\n# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes # (issue)\r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/diffusers/blob/main/CONTRIBUTING.md)?\r\n- [x] Did you read our [philosophy doc](https://github.com/huggingface/diffusers/blob/main/PHILOSOPHY.md) (important for complex PRs)?\r\n- [ ] Was this discussed/approved via a GitHub issue or the [forum](https://discuss.huggingface.co/c/discussion-related-to-httpsgithubcomhuggingfacediffusers/63)? Please add a link to it if that's the case.\r\n- [x] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/diffusers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/diffusers/tree/main/docs#writing-source-documentation).\r\n- [x] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @.\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nCore library:\r\n\r\n- Schedulers: @yiyixuxu\r\n- Pipelines and pipeline callbacks: @yiyixuxu and @asomoza\r\n- Training examples: @sayakpaul\r\n- Docs: @stevhliu and @sayakpaul\r\n- JAX and MPS: @pcuenca\r\n- Audio: @sanchit-gandhi\r\n- General functionalities: @sayakpaul @yiyixuxu @DN6\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @SunMarc\r\n- PEFT: @sayakpaul @BenjaminBossan\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- transformers: [different repo](https://github.com/huggingface/transformers)\r\n- safetensors: [different repo](https://github.com/huggingface/safetensors)\r\n\r\n-->\r\n",
      "html_url": "https://github.com/huggingface/diffusers/pull/12456",
      "created_at": "2025-10-09T13:21:05Z",
      "merged_at": "2025-10-21T15:25:55Z",
      "merge_commit_sha": "cefc2cf82dbdb5e4f725374420f0f6a91eb69048",
      "base_ref": "main",
      "head_sha": "803d0d1e7efb4a7e998f20ce963ad633e3fb4cab",
      "user": "DavidBert",
      "files": [
        {
          "filename": "docs/source/en/_toctree.yml",
          "status": "modified",
          "additions": 2,
          "deletions": 0,
          "changes": 2,
          "patch": "@@ -541,6 +541,8 @@\n         title: PAG\n       - local: api/pipelines/paint_by_example\n         title: Paint by Example\n+      - local: api/pipelines/photon\n+        title: Photon\n       - local: api/pipelines/pixart\n         title: PixArt-\u03b1\n       - local: api/pipelines/pixart_sigma"
        },
        {
          "filename": "docs/source/en/api/pipelines/photon.md",
          "status": "added",
          "additions": 131,
          "deletions": 0,
          "changes": 131,
          "patch": "@@ -0,0 +1,131 @@\n+<!-- Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License. -->\n+\n+# Photon\n+\n+\n+Photon generates high-quality images from text using a simplified MMDIT architecture where text tokens don't update through transformer blocks. It employs flow matching with discrete scheduling for efficient sampling and uses Google's T5Gemma-2B-2B-UL2 model for multi-language text encoding. The ~1.3B parameter transformer delivers fast inference without sacrificing quality. You can choose between Flux VAE (8x compression, 16 latent channels) for balanced quality and speed or DC-AE (32x compression, 32 latent channels) for latent compression and faster processing.\n+\n+## Available models\n+\n+Photon offers multiple variants with different VAE configurations, each optimized for specific resolutions. Base models excel with detailed prompts, capturing complex compositions and subtle details. Fine-tuned models trained on the [Alchemist dataset](https://huggingface.co/datasets/yandex/alchemist) improve aesthetic quality, especially with simpler prompts.\n+\n+\n+| Model | Resolution | Fine-tuned | Distilled | Description | Suggested prompts | Suggested parameters | Recommended dtype |\n+|:-----:|:-----------------:|:----------:|:----------:|:----------:|:----------:|:----------:|:----------:|\n+| [`Photoroom/photon-256-t2i`](https://huggingface.co/Photoroom/photon-256-t2i)| 256 | No | No | Base model pre-trained at 256 with Flux VAE|Works best with detailed prompts in natural language|28 steps, cfg=5.0| `torch.bfloat16` |\n+| [`Photoroom/photon-256-t2i-sft`](https://huggingface.co/Photoroom/photon-256-t2i-sft)| 512 | Yes | No | Fine-tuned on the [Alchemist dataset](https://huggingface.co/datasets/yandex/alchemist) dataset with Flux VAE | Can handle less detailed prompts|28 steps, cfg=5.0| `torch.bfloat16` |\n+| [`Photoroom/photon-512-t2i`](https://huggingface.co/Photoroom/photon-512-t2i)| 512 | No | No | Base model pre-trained at 512 with Flux VAE |Works best with detailed prompts in natural language|28 steps, cfg=5.0| `torch.bfloat16` |\n+| [`Photoroom/photon-512-t2i-sft`](https://huggingface.co/Photoroom/photon-512-t2i-sft)| 512 | Yes | No | Fine-tuned on the [Alchemist dataset](https://huggingface.co/datasets/yandex/alchemist) dataset with Flux VAE | Can handle less detailed prompts in natural language|28 steps, cfg=5.0| `torch.bfloat16` |\n+| [`Photoroom/photon-512-t2i-sft-distilled`](https://huggingface.co/Photoroom/photon-512-t2i-sft-distilled)| 512 | Yes | Yes | 8-step distilled model from [`Photoroom/photon-512-t2i-sft`](https://huggingface.co/Photoroom/photon-512-t2i-sft) | Can handle less detailed prompts in natural language|8 steps, cfg=1.0| `torch.bfloat16` |\n+| [`Photoroom/photon-512-t2i-dc-ae`](https://huggingface.co/Photoroom/photon-512-t2i-dc-ae)| 512 | No | No | Base model pre-trained at 512 with [Deep Compression Autoencoder (DC-AE)](https://hanlab.mit.edu/projects/dc-ae)|Works best with detailed prompts in natural language|28 steps, cfg=5.0| `torch.bfloat16` |\n+| [`Photoroom/photon-512-t2i-dc-ae-sft`](https://huggingface.co/Photoroom/photon-512-t2i-dc-ae-sft)| 512 | Yes | No | Fine-tuned on the [Alchemist dataset](https://huggingface.co/datasets/yandex/alchemist) dataset with [Deep Compression Autoencoder (DC-AE)](https://hanlab.mit.edu/projects/dc-ae) | Can handle less detailed prompts in natural language|28 steps, cfg=5.0| `torch.bfloat16` |\n+| [`Photoroom/photon-512-t2i-dc-ae-sft-distilled`](https://huggingface.co/Photoroom/photon-512-t2i-dc-ae-sft-distilled)| 512 | Yes | Yes | 8-step distilled model from [`Photoroom/photon-512-t2i-dc-ae-sft-distilled`](https://huggingface.co/Photoroom/photon-512-t2i-dc-ae-sft-distilled) | Can handle less detailed prompts in natural language|8 steps, cfg=1.0| `torch.bfloat16` |s\n+\n+Refer to [this](https://huggingface.co/collections/Photoroom/photon-models-68e66254c202ebfab99ad38e) collection for more information.\n+\n+## Loading the pipeline\n+\n+Load the pipeline with [`~DiffusionPipeline.from_pretrained`].\n+\n+```py\n+from diffusers.pipelines.photon import PhotonPipeline\n+\n+# Load pipeline - VAE and text encoder will be loaded from HuggingFace\n+pipe = PhotonPipeline.from_pretrained(\"Photoroom/photon-512-t2i-sft\", torch_dtype=torch.bfloat16)\n+pipe.to(\"cuda\")\n+\n+prompt = \"A front-facing portrait of a lion the golden savanna at sunset.\"\n+image = pipe(prompt, num_inference_steps=28, guidance_scale=5.0).images[0]\n+image.save(\"photon_output.png\")\n+```\n+\n+### Manual Component Loading\n+\n+Load components individually to customize the pipeline for instance to use quantized models.\n+\n+```py\n+import torch\n+from diffusers.pipelines.photon import PhotonPipeline\n+from diffusers.models import AutoencoderKL, AutoencoderDC\n+from diffusers.models.transformers.transformer_photon import PhotonTransformer2DModel\n+from diffusers.schedulers import FlowMatchEulerDiscreteScheduler\n+from transformers import T5GemmaModel, GemmaTokenizerFast\n+from diffusers import BitsAndBytesConfig as DiffusersBitsAndBytesConfig\n+from transformers import BitsAndBytesConfig as BitsAndBytesConfig\n+\n+quant_config = DiffusersBitsAndBytesConfig(load_in_8bit=True)\n+# Load transformer\n+transformer = PhotonTransformer2DModel.from_pretrained(\n+    \"checkpoints/photon-512-t2i-sft\",\n+    subfolder=\"transformer\",\n+    quantization_config=quant_config,\n+    torch_dtype=torch.bfloat16,\n+)\n+\n+# Load scheduler\n+scheduler = FlowMatchEulerDiscreteScheduler.from_pretrained(\n+    \"checkpoints/photon-512-t2i-sft\", subfolder=\"scheduler\"\n+)\n+\n+# Load T5Gemma text encoder\n+t5gemma_model = T5GemmaModel.from_pretrained(\"google/t5gemma-2b-2b-ul2\",\n+                                            quantization_config=quant_config,\n+                                            torch_dtype=torch.bfloat16)\n+text_encoder = t5gemma_model.encoder.to(dtype=torch.bfloat16)\n+tokenizer = GemmaTokenizerFast.from_pretrained(\"google/t5gemma-2b-2b-ul2\")\n+tokenizer.model_max_length = 256\n+\n+# Load VAE - choose either Flux VAE or DC-AE\n+# Flux VAE\n+vae = AutoencoderKL.from_pretrained(\"black-forest-labs/FLUX.1-dev\",\n+                                    subfolder=\"vae\",\n+                                    quantization_config=quant_config,\n+                                    torch_dtype=torch.bfloat16)\n+\n+pipe = PhotonPipeline(\n+    transformer=transformer,\n+    scheduler=scheduler,\n+    text_encoder=text_encoder,\n+    tokenizer=tokenizer,\n+    vae=vae\n+)\n+pipe.to(\"cuda\")\n+```\n+\n+\n+## Memory Optimization\n+\n+For memory-constrained environments:\n+\n+```py\n+import torch\n+from diffusers.pipelines.photon import PhotonPipeline\n+\n+pipe = PhotonPipeline.from_pretrained(\"Photoroom/photon-512-t2i-sft\", torch_dtype=torch.bfloat16)\n+pipe.enable_model_cpu_offload()  # Offload components to CPU when not in use\n+\n+# Or use sequential CPU offload for even lower memory\n+pipe.enable_sequential_cpu_offload()\n+```\n+\n+## PhotonPipeline\n+\n+[[autodoc]] PhotonPipeline\n+  - all\n+  - __call__\n+\n+## PhotonPipelineOutput\n+\n+[[autodoc]] pipelines.photon.pipeline_output.PhotonPipelineOutput"
        },
        {
          "filename": "scripts/convert_photon_to_diffusers.py",
          "status": "added",
          "additions": 345,
          "deletions": 0,
          "changes": 345,
          "patch": "@@ -0,0 +1,345 @@\n+#!/usr/bin/env python3\n+\"\"\"\n+Script to convert Photon checkpoint from original codebase to diffusers format.\n+\"\"\"\n+\n+import argparse\n+import json\n+import os\n+import sys\n+from dataclasses import asdict, dataclass\n+from typing import Dict, Tuple\n+\n+import torch\n+from safetensors.torch import save_file\n+\n+from diffusers.models.transformers.transformer_photon import PhotonTransformer2DModel\n+from diffusers.pipelines.photon import PhotonPipeline\n+\n+\n+DEFAULT_RESOLUTION = 512\n+\n+\n+@dataclass(frozen=True)\n+class PhotonBase:\n+    context_in_dim: int = 2304\n+    hidden_size: int = 1792\n+    mlp_ratio: float = 3.5\n+    num_heads: int = 28\n+    depth: int = 16\n+    axes_dim: Tuple[int, int] = (32, 32)\n+    theta: int = 10_000\n+    time_factor: float = 1000.0\n+    time_max_period: int = 10_000\n+\n+\n+@dataclass(frozen=True)\n+class PhotonFlux(PhotonBase):\n+    in_channels: int = 16\n+    patch_size: int = 2\n+\n+\n+@dataclass(frozen=True)\n+class PhotonDCAE(PhotonBase):\n+    in_channels: int = 32\n+    patch_size: int = 1\n+\n+\n+def build_config(vae_type: str) -> Tuple[dict, int]:\n+    if vae_type == \"flux\":\n+        cfg = PhotonFlux()\n+    elif vae_type == \"dc-ae\":\n+        cfg = PhotonDCAE()\n+    else:\n+        raise ValueError(f\"Unsupported VAE type: {vae_type}. Use 'flux' or 'dc-ae'\")\n+\n+    config_dict = asdict(cfg)\n+    config_dict[\"axes_dim\"] = list(config_dict[\"axes_dim\"])  # type: ignore[index]\n+    return config_dict\n+\n+\n+def create_parameter_mapping(depth: int) -> dict:\n+    \"\"\"Create mapping from old parameter names to new diffusers names.\"\"\"\n+\n+    # Key mappings for structural changes\n+    mapping = {}\n+\n+    # Map old structure (layers in PhotonBlock) to new structure (layers in PhotonAttention)\n+    for i in range(depth):\n+        # QKV projections moved to attention module\n+        mapping[f\"blocks.{i}.img_qkv_proj.weight\"] = f\"blocks.{i}.attention.img_qkv_proj.weight\"\n+        mapping[f\"blocks.{i}.txt_kv_proj.weight\"] = f\"blocks.{i}.attention.txt_kv_proj.weight\"\n+\n+        # QK norm moved to attention module and renamed to match Attention's qk_norm structure\n+        mapping[f\"blocks.{i}.qk_norm.query_norm.scale\"] = f\"blocks.{i}.attention.norm_q.weight\"\n+        mapping[f\"blocks.{i}.qk_norm.key_norm.scale\"] = f\"blocks.{i}.attention.norm_k.weight\"\n+        mapping[f\"blocks.{i}.qk_norm.query_norm.weight\"] = f\"blocks.{i}.attention.norm_q.weight\"\n+        mapping[f\"blocks.{i}.qk_norm.key_norm.weight\"] = f\"blocks.{i}.attention.norm_k.weight\"\n+\n+        # K norm for text tokens moved to attention module\n+        mapping[f\"blocks.{i}.k_norm.scale\"] = f\"blocks.{i}.attention.norm_added_k.weight\"\n+        mapping[f\"blocks.{i}.k_norm.weight\"] = f\"blocks.{i}.attention.norm_added_k.weight\"\n+\n+        # Attention output projection\n+        mapping[f\"blocks.{i}.attn_out.weight\"] = f\"blocks.{i}.attention.to_out.0.weight\"\n+\n+    return mapping\n+\n+\n+def convert_checkpoint_parameters(old_state_dict: Dict[str, torch.Tensor], depth: int) -> Dict[str, torch.Tensor]:\n+    \"\"\"Convert old checkpoint parameters to new diffusers format.\"\"\"\n+\n+    print(\"Converting checkpoint parameters...\")\n+\n+    mapping = create_parameter_mapping(depth)\n+    converted_state_dict = {}\n+\n+    for key, value in old_state_dict.items():\n+        new_key = key\n+\n+        # Apply specific mappings if needed\n+        if key in mapping:\n+            new_key = mapping[key]\n+            print(f\"  Mapped: {key} -> {new_key}\")\n+\n+        converted_state_dict[new_key] = value\n+\n+    print(f\"\u2713 Converted {len(converted_state_dict)} parameters\")\n+    return converted_state_dict\n+\n+\n+def create_transformer_from_checkpoint(checkpoint_path: str, config: dict) -> PhotonTransformer2DModel:\n+    \"\"\"Create and load PhotonTransformer2DModel from old checkpoint.\"\"\"\n+\n+    print(f\"Loading checkpoint from: {checkpoint_path}\")\n+\n+    # Load old checkpoint\n+    if not os.path.exists(checkpoint_path):\n+        raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n+\n+    old_checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n+\n+    # Handle different checkpoint formats\n+    if isinstance(old_checkpoint, dict):\n+        if \"model\" in old_checkpoint:\n+            state_dict = old_checkpoint[\"model\"]\n+        elif \"state_dict\" in old_checkpoint:\n+            state_dict = old_checkpoint[\"state_dict\"]\n+        else:\n+            state_dict = old_checkpoint\n+    else:\n+        state_dict = old_checkpoint\n+\n+    print(f\"\u2713 Loaded checkpoint with {len(state_dict)} parameters\")\n+\n+    # Convert parameter names if needed\n+    model_depth = int(config.get(\"depth\", 16))\n+    converted_state_dict = convert_checkpoint_parameters(state_dict, depth=model_depth)\n+\n+    # Create transformer with config\n+    print(\"Creating PhotonTransformer2DModel...\")\n+    transformer = PhotonTransformer2DModel(**config)\n+\n+    # Load state dict\n+    print(\"Loading converted parameters...\")\n+    missing_keys, unexpected_keys = transformer.load_state_dict(converted_state_dict, strict=False)\n+\n+    if missing_keys:\n+        print(f\"\u26a0 Missing keys: {missing_keys}\")\n+    if unexpected_keys:\n+        print(f\"\u26a0 Unexpected keys: {unexpected_keys}\")\n+\n+    if not missing_keys and not unexpected_keys:\n+        print(\"\u2713 All parameters loaded successfully!\")\n+\n+    return transformer\n+\n+\n+def create_scheduler_config(output_path: str, shift: float):\n+    \"\"\"Create FlowMatchEulerDiscreteScheduler config.\"\"\"\n+\n+    scheduler_config = {\"_class_name\": \"FlowMatchEulerDiscreteScheduler\", \"num_train_timesteps\": 1000, \"shift\": shift}\n+\n+    scheduler_path = os.path.join(output_path, \"scheduler\")\n+    os.makedirs(scheduler_path, exist_ok=True)\n+\n+    with open(os.path.join(scheduler_path, \"scheduler_config.json\"), \"w\") as f:\n+        json.dump(scheduler_config, f, indent=2)\n+\n+    print(\"\u2713 Created scheduler config\")\n+\n+\n+def download_and_save_vae(vae_type: str, output_path: str):\n+    \"\"\"Download and save VAE to local directory.\"\"\"\n+    from diffusers import AutoencoderDC, AutoencoderKL\n+\n+    vae_path = os.path.join(output_path, \"vae\")\n+    os.makedirs(vae_path, exist_ok=True)\n+\n+    if vae_type == \"flux\":\n+        print(\"Downloading FLUX VAE from black-forest-labs/FLUX.1-dev...\")\n+        vae = AutoencoderKL.from_pretrained(\"black-forest-labs/FLUX.1-dev\", subfolder=\"vae\")\n+    else:  # dc-ae\n+        print(\"Downloading DC-AE VAE from mit-han-lab/dc-ae-f32c32-sana-1.1-diffusers...\")\n+        vae = AutoencoderDC.from_pretrained(\"mit-han-lab/dc-ae-f32c32-sana-1.1-diffusers\")\n+\n+    vae.save_pretrained(vae_path)\n+    print(f\"\u2713 Saved VAE to {vae_path}\")\n+\n+\n+def download_and_save_text_encoder(output_path: str):\n+    \"\"\"Download and save T5Gemma text encoder and tokenizer.\"\"\"\n+    from transformers import GemmaTokenizerFast\n+    from transformers.models.t5gemma.modeling_t5gemma import T5GemmaModel\n+\n+    text_encoder_path = os.path.join(output_path, \"text_encoder\")\n+    tokenizer_path = os.path.join(output_path, \"tokenizer\")\n+    os.makedirs(text_encoder_path, exist_ok=True)\n+    os.makedirs(tokenizer_path, exist_ok=True)\n+\n+    print(\"Downloading T5Gemma model from google/t5gemma-2b-2b-ul2...\")\n+    t5gemma_model = T5GemmaModel.from_pretrained(\"google/t5gemma-2b-2b-ul2\")\n+\n+    # Extract and save only the encoder\n+    t5gemma_encoder = t5gemma_model.encoder\n+    t5gemma_encoder.save_pretrained(text_encoder_path)\n+    print(f\"\u2713 Saved T5GemmaEncoder to {text_encoder_path}\")\n+\n+    print(\"Downloading tokenizer from google/t5gemma-2b-2b-ul2...\")\n+    tokenizer = GemmaTokenizerFast.from_pretrained(\"google/t5gemma-2b-2b-ul2\")\n+    tokenizer.model_max_length = 256\n+    tokenizer.save_pretrained(tokenizer_path)\n+    print(f\"\u2713 Saved tokenizer to {tokenizer_path}\")\n+\n+\n+def create_model_index(vae_type: str, default_image_size: int, output_path: str):\n+    \"\"\"Create model_index.json for the pipeline.\"\"\"\n+\n+    if vae_type == \"flux\":\n+        vae_class = \"AutoencoderKL\"\n+    else:  # dc-ae\n+        vae_class = \"AutoencoderDC\"\n+\n+    model_index = {\n+        \"_class_name\": \"PhotonPipeline\",\n+        \"_diffusers_version\": \"0.31.0.dev0\",\n+        \"_name_or_path\": os.path.basename(output_path),\n+        \"default_sample_size\": default_image_size,\n+        \"scheduler\": [\"diffusers\", \"FlowMatchEulerDiscreteScheduler\"],\n+        \"text_encoder\": [\"photon\", \"T5GemmaEncoder\"],\n+        \"tokenizer\": [\"transformers\", \"GemmaTokenizerFast\"],\n+        \"transformer\": [\"diffusers\", \"PhotonTransformer2DModel\"],\n+        \"vae\": [\"diffusers\", vae_class],\n+    }\n+\n+    model_index_path = os.path.join(output_path, \"model_index.json\")\n+    with open(model_index_path, \"w\") as f:\n+        json.dump(model_index, f, indent=2)\n+\n+\n+def main(args):\n+    # Validate inputs\n+    if not os.path.exists(args.checkpoint_path):\n+        raise FileNotFoundError(f\"Checkpoint not found: {args.checkpoint_path}\")\n+\n+    config = build_config(args.vae_type)\n+\n+    # Create output directory\n+    os.makedirs(args.output_path, exist_ok=True)\n+    print(f\"\u2713 Output directory: {args.output_path}\")\n+\n+    # Create transformer from checkpoint\n+    transformer = create_transformer_from_checkpoint(args.checkpoint_path, config)\n+\n+    # Save transformer\n+    transformer_path = os.path.join(args.output_path, \"transformer\")\n+    os.makedirs(transformer_path, exist_ok=True)\n+\n+    # Save config\n+    with open(os.path.join(transformer_path, \"config.json\"), \"w\") as f:\n+        json.dump(config, f, indent=2)\n+\n+    # Save model weights as safetensors\n+    state_dict = transformer.state_dict()\n+    save_file(state_dict, os.path.join(transformer_path, \"diffusion_pytorch_model.safetensors\"))\n+    print(f\"\u2713 Saved transformer to {transformer_path}\")\n+\n+    # Create scheduler config\n+    create_scheduler_config(args.output_path, args.shift)\n+\n+    download_and_save_vae(args.vae_type, args.output_path)\n+    download_and_save_text_encoder(args.output_path)\n+\n+    # Create model_index.json\n+    create_model_index(args.vae_type, args.resolution, args.output_path)\n+\n+    # Verify the pipeline can be loaded\n+    try:\n+        pipeline = PhotonPipeline.from_pretrained(args.output_path)\n+        print(\"Pipeline loaded successfully!\")\n+        print(f\"Transformer: {type(pipeline.transformer).__name__}\")\n+        print(f\"VAE: {type(pipeline.vae).__name__}\")\n+        print(f\"Text Encoder: {type(pipeline.text_encoder).__name__}\")\n+        print(f\"Scheduler: {type(pipeline.scheduler).__name__}\")\n+\n+        # Display model info\n+        num_params = sum(p.numel() for p in pipeline.transformer.parameters())\n+        print(f\"\u2713 Transformer parameters: {num_params:,}\")\n+\n+    except Exception as e:\n+        print(f\"Pipeline verification failed: {e}\")\n+        return False\n+\n+    print(\"Conversion completed successfully!\")\n+    print(f\"Converted pipeline saved to: {args.output_path}\")\n+    print(f\"VAE type: {args.vae_type}\")\n+\n+    return True\n+\n+\n+if __name__ == \"__main__\":\n+    parser = argparse.ArgumentParser(description=\"Convert Photon checkpoint to diffusers format\")\n+\n+    parser.add_argument(\n+        \"--checkpoint_path\", type=str, required=True, help=\"Path to the original Photon checkpoint (.pth file )\"\n+    )\n+\n+    parser.add_argument(\n+        \"--output_path\", type=str, required=True, help=\"Output directory for the converted diffusers pipeline\"\n+    )\n+\n+    parser.add_argument(\n+        \"--vae_type\",\n+        type=str,\n+        choices=[\"flux\", \"dc-ae\"],\n+        required=True,\n+        help=\"VAE type to use: 'flux' for AutoencoderKL (16 channels) or 'dc-ae' for AutoencoderDC (32 channels)\",\n+    )\n+\n+    parser.add_argument(\n+        \"--resolution\",\n+        type=int,\n+        choices=[256, 512, 1024],\n+        default=DEFAULT_RESOLUTION,\n+        help=\"Target resolution for the model (256, 512, or 1024). Affects the transformer's sample_size.\",\n+    )\n+\n+    parser.add_argument(\n+        \"--shift\",\n+        type=float,\n+        default=3.0,\n+        help=\"Shift for the scheduler\",\n+    )\n+\n+    args = parser.parse_args()\n+\n+    try:\n+        success = main(args)\n+        if not success:\n+            sys.exit(1)\n+    except Exception as e:\n+        print(f\"Conversion failed: {e}\")\n+        import traceback\n+\n+        traceback.print_exc()\n+        sys.exit(1)"
        },
        {
          "filename": "src/diffusers/__init__.py",
          "status": "modified",
          "additions": 4,
          "deletions": 0,
          "changes": 4,
          "patch": "@@ -232,6 +232,7 @@\n             \"MultiControlNetModel\",\n             \"OmniGenTransformer2DModel\",\n             \"ParallelConfig\",\n+            \"PhotonTransformer2DModel\",\n             \"PixArtTransformer2DModel\",\n             \"PriorTransformer\",\n             \"QwenImageControlNetModel\",\n@@ -515,6 +516,7 @@\n             \"MusicLDMPipeline\",\n             \"OmniGenPipeline\",\n             \"PaintByExamplePipeline\",\n+            \"PhotonPipeline\",\n             \"PIAPipeline\",\n             \"PixArtAlphaPipeline\",\n             \"PixArtSigmaPAGPipeline\",\n@@ -926,6 +928,7 @@\n             MultiControlNetModel,\n             OmniGenTransformer2DModel,\n             ParallelConfig,\n+            PhotonTransformer2DModel,\n             PixArtTransformer2DModel,\n             PriorTransformer,\n             QwenImageControlNetModel,\n@@ -1179,6 +1182,7 @@\n             MusicLDMPipeline,\n             OmniGenPipeline,\n             PaintByExamplePipeline,\n+            PhotonPipeline,\n             PIAPipeline,\n             PixArtAlphaPipeline,\n             PixArtSigmaPAGPipeline,"
        },
        {
          "filename": "src/diffusers/models/__init__.py",
          "status": "modified",
          "additions": 2,
          "deletions": 0,
          "changes": 2,
          "patch": "@@ -96,6 +96,7 @@\n     _import_structure[\"transformers.transformer_lumina2\"] = [\"Lumina2Transformer2DModel\"]\n     _import_structure[\"transformers.transformer_mochi\"] = [\"MochiTransformer3DModel\"]\n     _import_structure[\"transformers.transformer_omnigen\"] = [\"OmniGenTransformer2DModel\"]\n+    _import_structure[\"transformers.transformer_photon\"] = [\"PhotonTransformer2DModel\"]\n     _import_structure[\"transformers.transformer_qwenimage\"] = [\"QwenImageTransformer2DModel\"]\n     _import_structure[\"transformers.transformer_sd3\"] = [\"SD3Transformer2DModel\"]\n     _import_structure[\"transformers.transformer_skyreels_v2\"] = [\"SkyReelsV2Transformer3DModel\"]\n@@ -190,6 +191,7 @@\n             LuminaNextDiT2DModel,\n             MochiTransformer3DModel,\n             OmniGenTransformer2DModel,\n+            PhotonTransformer2DModel,\n             PixArtTransformer2DModel,\n             PriorTransformer,\n             QwenImageTransformer2DModel,"
        },
        {
          "filename": "src/diffusers/models/transformers/__init__.py",
          "status": "modified",
          "additions": 1,
          "deletions": 0,
          "changes": 1,
          "patch": "@@ -32,6 +32,7 @@\n     from .transformer_lumina2 import Lumina2Transformer2DModel\n     from .transformer_mochi import MochiTransformer3DModel\n     from .transformer_omnigen import OmniGenTransformer2DModel\n+    from .transformer_photon import PhotonTransformer2DModel\n     from .transformer_qwenimage import QwenImageTransformer2DModel\n     from .transformer_sd3 import SD3Transformer2DModel\n     from .transformer_skyreels_v2 import SkyReelsV2Transformer3DModel"
        },
        {
          "filename": "src/diffusers/models/transformers/transformer_photon.py",
          "status": "added",
          "additions": 770,
          "deletions": 0,
          "changes": 770,
          "patch": "@@ -0,0 +1,770 @@\n+# Copyright 2025 The Photoroom and The HuggingFace Teams. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import Any, Dict, List, Optional, Tuple, Union\n+\n+import torch\n+from torch import nn\n+from torch.nn.functional import fold, unfold\n+\n+from ...configuration_utils import ConfigMixin, register_to_config\n+from ...utils import logging\n+from ..attention import AttentionMixin, AttentionModuleMixin\n+from ..attention_dispatch import dispatch_attention_fn\n+from ..embeddings import get_timestep_embedding\n+from ..modeling_outputs import Transformer2DModelOutput\n+from ..modeling_utils import ModelMixin\n+from ..normalization import RMSNorm\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+def get_image_ids(batch_size: int, height: int, width: int, patch_size: int, device: torch.device) -> torch.Tensor:\n+    r\"\"\"\n+    Generates 2D patch coordinate indices for a batch of images.\n+\n+    Args:\n+        batch_size (`int`):\n+            Number of images in the batch.\n+        height (`int`):\n+            Height of the input images (in pixels).\n+        width (`int`):\n+            Width of the input images (in pixels).\n+        patch_size (`int`):\n+            Size of the square patches that the image is divided into.\n+        device (`torch.device`):\n+            The device on which to create the tensor.\n+\n+    Returns:\n+        `torch.Tensor`:\n+            Tensor of shape `(batch_size, num_patches, 2)` containing the (row, col) coordinates of each patch in the\n+            image grid.\n+    \"\"\"\n+\n+    img_ids = torch.zeros(height // patch_size, width // patch_size, 2, device=device)\n+    img_ids[..., 0] = torch.arange(height // patch_size, device=device)[:, None]\n+    img_ids[..., 1] = torch.arange(width // patch_size, device=device)[None, :]\n+    return img_ids.reshape((height // patch_size) * (width // patch_size), 2).unsqueeze(0).repeat(batch_size, 1, 1)\n+\n+\n+def apply_rope(xq: torch.Tensor, freqs_cis: torch.Tensor) -> torch.Tensor:\n+    r\"\"\"\n+    Applies rotary positional embeddings (RoPE) to a query tensor.\n+\n+    Args:\n+        xq (`torch.Tensor`):\n+            Input tensor of shape `(..., dim)` representing the queries.\n+        freqs_cis (`torch.Tensor`):\n+            Precomputed rotary frequency components of shape `(..., dim/2, 2)` containing cosine and sine pairs.\n+\n+    Returns:\n+        `torch.Tensor`:\n+            Tensor of the same shape as `xq` with rotary embeddings applied.\n+    \"\"\"\n+    xq_ = xq.float().reshape(*xq.shape[:-1], -1, 1, 2)\n+    # Ensure freqs_cis is on the same device as queries to avoid device mismatches with offloading\n+    freqs_cis = freqs_cis.to(device=xq.device, dtype=xq_.dtype)\n+    xq_out = freqs_cis[..., 0] * xq_[..., 0] + freqs_cis[..., 1] * xq_[..., 1]\n+    return xq_out.reshape(*xq.shape).type_as(xq)\n+\n+\n+class PhotonAttnProcessor2_0:\n+    r\"\"\"\n+    Processor for implementing Photon-style attention with multi-source tokens and RoPE. Supports multiple attention\n+    backends (Flash Attention, Sage Attention, etc.) via dispatch_attention_fn.\n+    \"\"\"\n+\n+    _attention_backend = None\n+    _parallel_config = None\n+\n+    def __init__(self):\n+        if not hasattr(torch.nn.functional, \"scaled_dot_product_attention\"):\n+            raise ImportError(\"PhotonAttnProcessor2_0 requires PyTorch 2.0, please upgrade PyTorch to 2.0.\")\n+\n+    def __call__(\n+        self,\n+        attn: \"PhotonAttention\",\n+        hidden_states: torch.Tensor,\n+        encoder_hidden_states: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        image_rotary_emb: Optional[torch.Tensor] = None,\n+        **kwargs,\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        Apply Photon attention using PhotonAttention module.\n+\n+        Args:\n+            attn: PhotonAttention module containing projection layers\n+            hidden_states: Image tokens [B, L_img, D]\n+            encoder_hidden_states: Text tokens [B, L_txt, D]\n+            attention_mask: Boolean mask for text tokens [B, L_txt]\n+            image_rotary_emb: Rotary positional embeddings [B, 1, L_img, head_dim//2, 2, 2]\n+        \"\"\"\n+\n+        if encoder_hidden_states is None:\n+            raise ValueError(\"PhotonAttnProcessor2_0 requires 'encoder_hidden_states' containing text tokens.\")\n+\n+        # Project image tokens to Q, K, V\n+        img_qkv = attn.img_qkv_proj(hidden_states)\n+        B, L_img, _ = img_qkv.shape\n+        img_qkv = img_qkv.reshape(B, L_img, 3, attn.heads, attn.head_dim)\n+        img_qkv = img_qkv.permute(2, 0, 3, 1, 4)  # [3, B, H, L_img, D]\n+        img_q, img_k, img_v = img_qkv[0], img_qkv[1], img_qkv[2]\n+\n+        # Apply QK normalization to image tokens\n+        img_q = attn.norm_q(img_q)\n+        img_k = attn.norm_k(img_k)\n+\n+        # Project text tokens to K, V\n+        txt_kv = attn.txt_kv_proj(encoder_hidden_states)\n+        B, L_txt, _ = txt_kv.shape\n+        txt_kv = txt_kv.reshape(B, L_txt, 2, attn.heads, attn.head_dim)\n+        txt_kv = txt_kv.permute(2, 0, 3, 1, 4)  # [2, B, H, L_txt, D]\n+        txt_k, txt_v = txt_kv[0], txt_kv[1]\n+\n+        # Apply K normalization to text tokens\n+        txt_k = attn.norm_added_k(txt_k)\n+\n+        # Apply RoPE to image queries and keys\n+        if image_rotary_emb is not None:\n+            img_q = apply_rope(img_q, image_rotary_emb)\n+            img_k = apply_rope(img_k, image_rotary_emb)\n+\n+        # Concatenate text and image keys/values\n+        k = torch.cat((txt_k, img_k), dim=2)  # [B, H, L_txt + L_img, D]\n+        v = torch.cat((txt_v, img_v), dim=2)  # [B, H, L_txt + L_img, D]\n+\n+        # Build attention mask if provided\n+        attn_mask_tensor = None\n+        if attention_mask is not None:\n+            bs, _, l_img, _ = img_q.shape\n+            l_txt = txt_k.shape[2]\n+\n+            if attention_mask.dim() != 2:\n+                raise ValueError(f\"Unsupported attention_mask shape: {attention_mask.shape}\")\n+            if attention_mask.shape[-1] != l_txt:\n+                raise ValueError(f\"attention_mask last dim {attention_mask.shape[-1]} must equal text length {l_txt}\")\n+\n+            device = img_q.device\n+            ones_img = torch.ones((bs, l_img), dtype=torch.bool, device=device)\n+            attention_mask = attention_mask.to(device=device, dtype=torch.bool)\n+            joint_mask = torch.cat([attention_mask, ones_img], dim=-1)\n+            attn_mask_tensor = joint_mask[:, None, None, :].expand(-1, attn.heads, l_img, -1)\n+\n+        # Apply attention using dispatch_attention_fn for backend support\n+        # Reshape to match dispatch_attention_fn expectations: [B, L, H, D]\n+        query = img_q.transpose(1, 2)  # [B, L_img, H, D]\n+        key = k.transpose(1, 2)  # [B, L_txt + L_img, H, D]\n+        value = v.transpose(1, 2)  # [B, L_txt + L_img, H, D]\n+\n+        attn_output = dispatch_attention_fn(\n+            query,\n+            key,\n+            value,\n+            attn_mask=attn_mask_tensor,\n+            backend=self._attention_backend,\n+            parallel_config=self._parallel_config,\n+        )\n+\n+        # Reshape from [B, L_img, H, D] to [B, L_img, H*D]\n+        batch_size, seq_len, num_heads, head_dim = attn_output.shape\n+        attn_output = attn_output.reshape(batch_size, seq_len, num_heads * head_dim)\n+\n+        # Apply output projection\n+        attn_output = attn.to_out[0](attn_output)\n+        if len(attn.to_out) > 1:\n+            attn_output = attn.to_out[1](attn_output)  # dropout if present\n+\n+        return attn_output\n+\n+\n+class PhotonAttention(nn.Module, AttentionModuleMixin):\n+    r\"\"\"\n+    Photon-style attention module that handles multi-source tokens and RoPE. Similar to FluxAttention but adapted for\n+    Photon's architecture.\n+    \"\"\"\n+\n+    _default_processor_cls = PhotonAttnProcessor2_0\n+    _available_processors = [PhotonAttnProcessor2_0]\n+\n+    def __init__(\n+        self,\n+        query_dim: int,\n+        heads: int = 8,\n+        dim_head: int = 64,\n+        bias: bool = False,\n+        out_bias: bool = False,\n+        eps: float = 1e-6,\n+        processor=None,\n+    ):\n+        super().__init__()\n+\n+        self.heads = heads\n+        self.head_dim = dim_head\n+        self.inner_dim = dim_head * heads\n+        self.query_dim = query_dim\n+\n+        self.img_qkv_proj = nn.Linear(query_dim, query_dim * 3, bias=bias)\n+\n+        self.norm_q = RMSNorm(self.head_dim, eps=eps, elementwise_affine=True)\n+        self.norm_k = RMSNorm(self.head_dim, eps=eps, elementwise_affine=True)\n+\n+        self.txt_kv_proj = nn.Linear(query_dim, query_dim * 2, bias=bias)\n+        self.norm_added_k = RMSNorm(self.head_dim, eps=eps, elementwise_affine=True)\n+\n+        self.to_out = nn.ModuleList([])\n+        self.to_out.append(nn.Linear(self.inner_dim, query_dim, bias=out_bias))\n+        self.to_out.append(nn.Dropout(0.0))\n+\n+        if processor is None:\n+            processor = self._default_processor_cls()\n+        self.set_processor(processor)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        encoder_hidden_states: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        image_rotary_emb: Optional[torch.Tensor] = None,\n+        **kwargs,\n+    ) -> torch.Tensor:\n+        return self.processor(\n+            self,\n+            hidden_states,\n+            encoder_hidden_states=encoder_hidden_states,\n+            attention_mask=attention_mask,\n+            image_rotary_emb=image_rotary_emb,\n+            **kwargs,\n+        )\n+\n+\n+# inspired from https://github.com/black-forest-labs/flux/blob/main/src/flux/modules/layers.py\n+class PhotonEmbedND(nn.Module):\n+    r\"\"\"\n+    N-dimensional rotary positional embedding.\n+\n+    This module creates rotary embeddings (RoPE) across multiple axes, where each axis can have its own embedding\n+    dimension. The embeddings are combined and returned as a single tensor\n+\n+    Args:\n+        dim (int):\n+        Base embedding dimension (must be even).\n+        theta (int):\n+        Scaling factor that controls the frequency spectrum of the rotary embeddings.\n+        axes_dim (list[int]):\n+        List of embedding dimensions for each axis (each must be even).\n+    \"\"\"\n+\n+    def __init__(self, dim: int, theta: int, axes_dim: List[int]):\n+        super().__init__()\n+        self.dim = dim\n+        self.theta = theta\n+        self.axes_dim = axes_dim\n+\n+    def rope(self, pos: torch.Tensor, dim: int, theta: int) -> torch.Tensor:\n+        assert dim % 2 == 0\n+        scale = torch.arange(0, dim, 2, dtype=torch.float64, device=pos.device) / dim\n+        omega = 1.0 / (theta**scale)\n+        out = pos.unsqueeze(-1) * omega.unsqueeze(0)\n+        out = torch.stack([torch.cos(out), -torch.sin(out), torch.sin(out), torch.cos(out)], dim=-1)\n+        # Native PyTorch equivalent of: Rearrange(\"b n d (i j) -> b n d i j\", i=2, j=2)\n+        # out shape: (b, n, d, 4) -> reshape to (b, n, d, 2, 2)\n+        out = out.reshape(*out.shape[:-1], 2, 2)\n+        return out.float()\n+\n+    def forward(self, ids: torch.Tensor) -> torch.Tensor:\n+        n_axes = ids.shape[-1]\n+        emb = torch.cat(\n+            [self.rope(ids[:, :, i], self.axes_dim[i], self.theta) for i in range(n_axes)],\n+            dim=-3,\n+        )\n+        return emb.unsqueeze(1)\n+\n+\n+class MLPEmbedder(nn.Module):\n+    r\"\"\"\n+    A simple 2-layer MLP used for embedding inputs.\n+\n+    Args:\n+        in_dim (`int`):\n+            Dimensionality of the input features.\n+        hidden_dim (`int`):\n+            Dimensionality of the hidden and output embedding space.\n+\n+    Returns:\n+        `torch.Tensor`:\n+            Tensor of shape `(..., hidden_dim)` containing the embedded representations.\n+    \"\"\"\n+\n+    def __init__(self, in_dim: int, hidden_dim: int):\n+        super().__init__()\n+        self.in_layer = nn.Linear(in_dim, hidden_dim, bias=True)\n+        self.silu = nn.SiLU()\n+        self.out_layer = nn.Linear(hidden_dim, hidden_dim, bias=True)\n+\n+    def forward(self, x: torch.Tensor) -> torch.Tensor:\n+        return self.out_layer(self.silu(self.in_layer(x)))\n+\n+\n+class Modulation(nn.Module):\n+    r\"\"\"\n+    Modulation network that generates scale, shift, and gating parameters.\n+\n+    Given an input vector, the module projects it through a linear layer to produce six chunks, which are grouped into\n+    two tuples `(shift, scale, gate)`.\n+\n+    Args:\n+        dim (`int`):\n+            Dimensionality of the input vector. The output will have `6 * dim` features internally.\n+\n+    Returns:\n+        ((`torch.Tensor`, `torch.Tensor`, `torch.Tensor`), (`torch.Tensor`, `torch.Tensor`, `torch.Tensor`)):\n+            Two tuples `(shift, scale, gate)`.\n+    \"\"\"\n+\n+    def __init__(self, dim: int):\n+        super().__init__()\n+        self.lin = nn.Linear(dim, 6 * dim, bias=True)\n+        nn.init.constant_(self.lin.weight, 0)\n+        nn.init.constant_(self.lin.bias, 0)\n+\n+    def forward(\n+        self, vec: torch.Tensor\n+    ) -> Tuple[Tuple[torch.Tensor, torch.Tensor, torch.Tensor], Tuple[torch.Tensor, torch.Tensor, torch.Tensor]]:\n+        out = self.lin(nn.functional.silu(vec))[:, None, :].chunk(6, dim=-1)\n+        return tuple(out[:3]), tuple(out[3:])\n+\n+\n+class PhotonBlock(nn.Module):\n+    r\"\"\"\n+    Multimodal transformer block with text\u2013image cross-attention, modulation, and MLP.\n+\n+    Args:\n+        hidden_size (`int`):\n+            Dimension of the hidden representations.\n+        num_heads (`int`):\n+            Number of attention heads.\n+        mlp_ratio (`float`, *optional*, defaults to 4.0):\n+            Expansion ratio for the hidden dimension inside the MLP.\n+        qk_scale (`float`, *optional*):\n+            Scale factor for queries and keys. If not provided, defaults to ``head_dim**-0.5``.\n+\n+    Attributes:\n+        img_pre_norm (`nn.LayerNorm`):\n+            Pre-normalization applied to image tokens before attention.\n+        attention (`PhotonAttention`):\n+            Multi-head attention module with built-in QKV projections and normalizations for cross-attention between\n+            image and text tokens.\n+        post_attention_layernorm (`nn.LayerNorm`):\n+            Normalization applied after attention.\n+        gate_proj / up_proj / down_proj (`nn.Linear`):\n+            Feedforward layers forming the gated MLP.\n+        mlp_act (`nn.GELU`):\n+            Nonlinear activation used in the MLP.\n+        modulation (`Modulation`):\n+            Produces scale/shift/gating parameters for modulated layers.\n+\n+        Methods:\n+            The forward method performs cross-attention and the MLP with modulation.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        hidden_size: int,\n+        num_heads: int,\n+        mlp_ratio: float = 4.0,\n+        qk_scale: Optional[float] = None,\n+    ):\n+        super().__init__()\n+\n+        self.hidden_dim = hidden_size\n+        self.num_heads = num_heads\n+        self.head_dim = hidden_size // num_heads\n+        self.scale = qk_scale or self.head_dim**-0.5\n+\n+        self.mlp_hidden_dim = int(hidden_size * mlp_ratio)\n+        self.hidden_size = hidden_size\n+\n+        # Pre-attention normalization for image tokens\n+        self.img_pre_norm = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)\n+\n+        # PhotonAttention module with built-in projections and norms\n+        self.attention = PhotonAttention(\n+            query_dim=hidden_size,\n+            heads=num_heads,\n+            dim_head=self.head_dim,\n+            bias=False,\n+            out_bias=False,\n+            eps=1e-6,\n+            processor=PhotonAttnProcessor2_0(),\n+        )\n+\n+        # mlp\n+        self.post_attention_layernorm = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)\n+        self.gate_proj = nn.Linear(hidden_size, self.mlp_hidden_dim, bias=False)\n+        self.up_proj = nn.Linear(hidden_size, self.mlp_hidden_dim, bias=False)\n+        self.down_proj = nn.Linear(self.mlp_hidden_dim, hidden_size, bias=False)\n+        self.mlp_act = nn.GELU(approximate=\"tanh\")\n+\n+        self.modulation = Modulation(hidden_size)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        encoder_hidden_states: torch.Tensor,\n+        temb: torch.Tensor,\n+        image_rotary_emb: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        **kwargs: Dict[str, Any],\n+    ) -> torch.Tensor:\n+        r\"\"\"\n+        Runs modulation-gated cross-attention and MLP, with residual connections.\n+\n+        Args:\n+            hidden_states (`torch.Tensor`):\n+                Image tokens of shape `(B, L_img, hidden_size)`.\n+            encoder_hidden_states (`torch.Tensor`):\n+                Text tokens of shape `(B, L_txt, hidden_size)`.\n+            temb (`torch.Tensor`):\n+                Conditioning vector used by `Modulation` to produce scale/shift/gates, shape `(B, hidden_size)` (or\n+                broadcastable).\n+            image_rotary_emb (`torch.Tensor`):\n+                Rotary positional embeddings applied inside attention.\n+            attention_mask (`torch.Tensor`, *optional*):\n+                Boolean mask for text tokens of shape `(B, L_txt)`, where `0` marks padding.\n+            **kwargs:\n+                Additional keyword arguments for API compatibility.\n+\n+        Returns:\n+            `torch.Tensor`:\n+                Updated image tokens of shape `(B, L_img, hidden_size)`.\n+        \"\"\"\n+\n+        mod_attn, mod_mlp = self.modulation(temb)\n+        attn_shift, attn_scale, attn_gate = mod_attn\n+        mlp_shift, mlp_scale, mlp_gate = mod_mlp\n+\n+        hidden_states_mod = (1 + attn_scale) * self.img_pre_norm(hidden_states) + attn_shift\n+\n+        attn_out = self.attention(\n+            hidden_states=hidden_states_mod,\n+            encoder_hidden_states=encoder_hidden_states,\n+            attention_mask=attention_mask,\n+            image_rotary_emb=image_rotary_emb,\n+        )\n+\n+        hidden_states = hidden_states + attn_gate * attn_out\n+\n+        x = (1 + mlp_scale) * self.post_attention_layernorm(hidden_states) + mlp_shift\n+        hidden_states = hidden_states + mlp_gate * (self.down_proj(self.mlp_act(self.gate_proj(x)) * self.up_proj(x)))\n+        return hidden_states\n+\n+\n+class FinalLayer(nn.Module):\n+    r\"\"\"\n+    Final projection layer with adaptive LayerNorm modulation.\n+\n+    This layer applies a normalized and modulated transformation to input tokens and projects them into patch-level\n+    outputs.\n+\n+    Args:\n+        hidden_size (`int`):\n+            Dimensionality of the input tokens.\n+        patch_size (`int`):\n+            Size of the square image patches.\n+        out_channels (`int`):\n+            Number of output channels per pixel (e.g. RGB = 3).\n+\n+    Forward Inputs:\n+        x (`torch.Tensor`):\n+            Input tokens of shape `(B, L, hidden_size)`, where `L` is the number of patches.\n+        vec (`torch.Tensor`):\n+            Conditioning vector of shape `(B, hidden_size)` used to generate shift and scale parameters for adaptive\n+            LayerNorm.\n+\n+    Returns:\n+        `torch.Tensor`:\n+            Projected patch outputs of shape `(B, L, patch_size * patch_size * out_channels)`.\n+    \"\"\"\n+\n+    def __init__(self, hidden_size: int, patch_size: int, out_channels: int):\n+        super().__init__()\n+        self.norm_final = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)\n+        self.linear = nn.Linear(hidden_size, patch_size * patch_size * out_channels, bias=True)\n+        self.adaLN_modulation = nn.Sequential(nn.SiLU(), nn.Linear(hidden_size, 2 * hidden_size, bias=True))\n+\n+    def forward(self, x: torch.Tensor, vec: torch.Tensor) -> torch.Tensor:\n+        shift, scale = self.adaLN_modulation(vec).chunk(2, dim=1)\n+        x = (1 + scale[:, None, :]) * self.norm_final(x) + shift[:, None, :]\n+        x = self.linear(x)\n+        return x\n+\n+\n+def img2seq(img: torch.Tensor, patch_size: int) -> torch.Tensor:\n+    r\"\"\"\n+    Flattens an image tensor into a sequence of non-overlapping patches.\n+\n+    Args:\n+        img (`torch.Tensor`):\n+            Input image tensor of shape `(B, C, H, W)`.\n+        patch_size (`int`):\n+            Size of each square patch. Must evenly divide both `H` and `W`.\n+\n+    Returns:\n+        `torch.Tensor`:\n+            Flattened patch sequence of shape `(B, L, C * patch_size * patch_size)`, where `L = (H // patch_size) * (W\n+            // patch_size)` is the number of patches.\n+    \"\"\"\n+    return unfold(img, kernel_size=patch_size, stride=patch_size).transpose(1, 2)\n+\n+\n+def seq2img(seq: torch.Tensor, patch_size: int, shape: torch.Tensor) -> torch.Tensor:\n+    r\"\"\"\n+    Reconstructs an image tensor from a sequence of patches (inverse of `img2seq`).\n+\n+    Args:\n+        seq (`torch.Tensor`):\n+            Patch sequence of shape `(B, L, C * patch_size * patch_size)`, where `L = (H // patch_size) * (W //\n+            patch_size)`.\n+        patch_size (`int`):\n+            Size of each square patch.\n+        shape (`tuple` or `torch.Tensor`):\n+            The original image spatial shape `(H, W)`. If a tensor is provided, the first two values are interpreted as\n+            height and width.\n+\n+    Returns:\n+        `torch.Tensor`:\n+            Reconstructed image tensor of shape `(B, C, H, W)`.\n+    \"\"\"\n+    if isinstance(shape, tuple):\n+        shape = shape[-2:]\n+    elif isinstance(shape, torch.Tensor):\n+        shape = (int(shape[0]), int(shape[1]))\n+    else:\n+        raise NotImplementedError(f\"shape type {type(shape)} not supported\")\n+    return fold(seq.transpose(1, 2), shape, kernel_size=patch_size, stride=patch_size)\n+\n+\n+class PhotonTransformer2DModel(ModelMixin, ConfigMixin, AttentionMixin):\n+    r\"\"\"\n+    Transformer-based 2D model for text to image generation.\n+\n+    Args:\n+        in_channels (`int`, *optional*, defaults to 16):\n+            Number of input channels in the latent image.\n+        patch_size (`int`, *optional*, defaults to 2):\n+            Size of the square patches used to flatten the input image.\n+        context_in_dim (`int`, *optional*, defaults to 2304):\n+            Dimensionality of the text conditioning input.\n+        hidden_size (`int`, *optional*, defaults to 1792):\n+            Dimension of the hidden representation.\n+        mlp_ratio (`float`, *optional*, defaults to 3.5):\n+            Expansion ratio for the hidden dimension inside MLP blocks.\n+        num_heads (`int`, *optional*, defaults to 28):\n+            Number of attention heads.\n+        depth (`int`, *optional*, defaults to 16):\n+            Number of transformer blocks.\n+        axes_dim (`list[int]`, *optional*):\n+            List of dimensions for each positional embedding axis. Defaults to `[32, 32]`.\n+        theta (`int`, *optional*, defaults to 10000):\n+            Frequency scaling factor for rotary embeddings.\n+        time_factor (`float`, *optional*, defaults to 1000.0):\n+            Scaling factor applied in timestep embeddings.\n+        time_max_period (`int`, *optional*, defaults to 10000):\n+            Maximum frequency period for timestep embeddings.\n+\n+    Attributes:\n+        pe_embedder (`EmbedND`):\n+            Multi-axis rotary embedding generator for positional encodings.\n+        img_in (`nn.Linear`):\n+            Projection layer for image patch tokens.\n+        time_in (`MLPEmbedder`):\n+            Embedding layer for timestep embeddings.\n+        txt_in (`nn.Linear`):\n+            Projection layer for text conditioning.\n+        blocks (`nn.ModuleList`):\n+            Stack of transformer blocks (`PhotonBlock`).\n+        final_layer (`LastLayer`):\n+            Projection layer mapping hidden tokens back to patch outputs.\n+\n+    Methods:\n+        attn_processors:\n+            Returns a dictionary of all attention processors in the model.\n+        set_attn_processor(processor):\n+            Replaces attention processors across all attention layers.\n+        process_inputs(image_latent, txt):\n+            Converts inputs into patch tokens, encodes text, and produces positional encodings.\n+        compute_timestep_embedding(timestep, dtype):\n+            Creates a timestep embedding of dimension 256, scaled and projected.\n+        forward_transformers(image_latent, cross_attn_conditioning, timestep, time_embedding, attention_mask,\n+        **block_kwargs):\n+            Runs the sequence of transformer blocks over image and text tokens.\n+        forward(image_latent, timestep, cross_attn_conditioning, micro_conditioning, cross_attn_mask=None,\n+        attention_kwargs=None, return_dict=True):\n+            Full forward pass from latent input to reconstructed output image.\n+\n+    Returns:\n+        `Transformer2DModelOutput` if `return_dict=True` (default), otherwise a tuple containing:\n+            - `sample` (`torch.Tensor`): Reconstructed image of shape `(B, C, H, W)`.\n+    \"\"\"\n+\n+    config_name = \"config.json\"\n+    _supports_gradient_checkpointing = True\n+\n+    @register_to_config\n+    def __init__(\n+        self,\n+        in_channels: int = 16,\n+        patch_size: int = 2,\n+        context_in_dim: int = 2304,\n+        hidden_size: int = 1792,\n+        mlp_ratio: float = 3.5,\n+        num_heads: int = 28,\n+        depth: int = 16,\n+        axes_dim: list = None,\n+        theta: int = 10000,\n+        time_factor: float = 1000.0,\n+        time_max_period: int = 10000,\n+    ):\n+        super().__init__()\n+\n+        if axes_dim is None:\n+            axes_dim = [32, 32]\n+\n+        # Store parameters directly\n+        self.in_channels = in_channels\n+        self.patch_size = patch_size\n+        self.out_channels = self.in_channels * self.patch_size**2\n+\n+        self.time_factor = time_factor\n+        self.time_max_period = time_max_period\n+\n+        if hidden_size % num_heads != 0:\n+            raise ValueError(f\"Hidden size {hidden_size} must be divisible by num_heads {num_heads}\")\n+\n+        pe_dim = hidden_size // num_heads\n+\n+        if sum(axes_dim) != pe_dim:\n+            raise ValueError(f\"Got {axes_dim} but expected positional dim {pe_dim}\")\n+\n+        self.hidden_size = hidden_size\n+        self.num_heads = num_heads\n+        self.pe_embedder = PhotonEmbedND(dim=pe_dim, theta=theta, axes_dim=axes_dim)\n+        self.img_in = nn.Linear(self.in_channels * self.patch_size**2, self.hidden_size, bias=True)\n+        self.time_in = MLPEmbedder(in_dim=256, hidden_dim=self.hidden_size)\n+        self.txt_in = nn.Linear(context_in_dim, self.hidden_size)\n+\n+        self.blocks = nn.ModuleList(\n+            [\n+                PhotonBlock(\n+                    self.hidden_size,\n+                    self.num_heads,\n+                    mlp_ratio=mlp_ratio,\n+                )\n+                for i in range(depth)\n+            ]\n+        )\n+\n+        self.final_layer = FinalLayer(self.hidden_size, 1, self.out_channels)\n+\n+        self.gradient_checkpointing = False\n+\n+    def _compute_timestep_embedding(self, timestep: torch.Tensor, dtype: torch.dtype) -> torch.Tensor:\n+        return self.time_in(\n+            get_timestep_embedding(\n+                timesteps=timestep,\n+                embedding_dim=256,\n+                max_period=self.time_max_period,\n+                scale=self.time_factor,\n+                flip_sin_to_cos=True,  # Match original cos, sin order\n+            ).to(dtype)\n+        )\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        timestep: torch.Tensor,\n+        encoder_hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        attention_kwargs: Optional[Dict[str, Any]] = None,\n+        return_dict: bool = True,\n+    ) -> Union[Tuple[torch.Tensor, ...], Transformer2DModelOutput]:\n+        r\"\"\"\n+        Forward pass of the PhotonTransformer2DModel.\n+\n+        The latent image is split into patch tokens, combined with text conditioning, and processed through a stack of\n+        transformer blocks modulated by the timestep. The output is reconstructed into the latent image space.\n+\n+        Args:\n+            hidden_states (`torch.Tensor`):\n+                Input latent image tensor of shape `(B, C, H, W)`.\n+            timestep (`torch.Tensor`):\n+                Timestep tensor of shape `(B,)` or `(1,)`, used for temporal conditioning.\n+            encoder_hidden_states (`torch.Tensor`):\n+                Text conditioning tensor of shape `(B, L_txt, context_in_dim)`.\n+            attention_mask (`torch.Tensor`, *optional*):\n+                Boolean mask of shape `(B, L_txt)`, where `0` marks padding in the text sequence.\n+            attention_kwargs (`dict`, *optional*):\n+                Additional arguments passed to attention layers.\n+            return_dict (`bool`, *optional*, defaults to `True`):\n+                Whether to return a `Transformer2DModelOutput` or a tuple.\n+\n+        Returns:\n+            `Transformer2DModelOutput` if `return_dict=True`, otherwise a tuple:\n+\n+                - `sample` (`torch.Tensor`): Output latent image of shape `(B, C, H, W)`.\n+        \"\"\"\n+        # Process text conditioning\n+        txt = self.txt_in(encoder_hidden_states)\n+\n+        # Convert image to sequence and embed\n+        img = img2seq(hidden_states, self.patch_size)\n+        img = self.img_in(img)\n+\n+        # Generate positional embeddings\n+        bs, _, h, w = hidden_states.shape\n+        img_ids = get_image_ids(bs, h, w, patch_size=self.patch_size, device=hidden_states.device)\n+        pe = self.pe_embedder(img_ids)\n+\n+        # Compute time embedding\n+        vec = self._compute_timestep_embedding(timestep, dtype=img.dtype)\n+\n+        # Apply transformer blocks\n+        for block in self.blocks:\n+            if torch.is_grad_enabled() and self.gradient_checkpointing:\n+                img = self._gradient_checkpointing_func(\n+                    block.__call__,\n+                    img,\n+                    txt,\n+                    vec,\n+                    pe,\n+                    attention_mask,\n+                )\n+            else:\n+                img = block(\n+                    hidden_states=img,\n+                    encoder_hidden_states=txt,\n+                    temb=vec,\n+                    image_rotary_emb=pe,\n+                    attention_mask=attention_mask,\n+                )\n+\n+        # Final layer and convert back to image\n+        img = self.final_layer(img, vec)\n+        output = seq2img(img, self.patch_size, hidden_states.shape)\n+\n+        if not return_dict:\n+            return (output,)\n+        return Transformer2DModelOutput(sample=output)"
        },
        {
          "filename": "src/diffusers/pipelines/__init__.py",
          "status": "modified",
          "additions": 2,
          "deletions": 0,
          "changes": 2,
          "patch": "@@ -144,6 +144,7 @@\n         \"FluxKontextPipeline\",\n         \"FluxKontextInpaintPipeline\",\n     ]\n+    _import_structure[\"photon\"] = [\"PhotonPipeline\"]\n     _import_structure[\"audioldm\"] = [\"AudioLDMPipeline\"]\n     _import_structure[\"audioldm2\"] = [\n         \"AudioLDM2Pipeline\",\n@@ -717,6 +718,7 @@\n             StableDiffusionXLPAGPipeline,\n         )\n         from .paint_by_example import PaintByExamplePipeline\n+        from .photon import PhotonPipeline\n         from .pia import PIAPipeline\n         from .pixart_alpha import PixArtAlphaPipeline, PixArtSigmaPipeline\n         from .qwenimage import ("
        },
        {
          "filename": "src/diffusers/pipelines/photon/__init__.py",
          "status": "added",
          "additions": 63,
          "deletions": 0,
          "changes": 63,
          "patch": "@@ -0,0 +1,63 @@\n+from typing import TYPE_CHECKING\n+\n+from ...utils import (\n+    DIFFUSERS_SLOW_IMPORT,\n+    OptionalDependencyNotAvailable,\n+    _LazyModule,\n+    get_objects_from_module,\n+    is_torch_available,\n+    is_transformers_available,\n+)\n+\n+\n+_dummy_objects = {}\n+_additional_imports = {}\n+_import_structure = {\"pipeline_output\": [\"PhotonPipelineOutput\"]}\n+\n+try:\n+    if not (is_transformers_available() and is_torch_available()):\n+        raise OptionalDependencyNotAvailable()\n+except OptionalDependencyNotAvailable:\n+    from ...utils import dummy_torch_and_transformers_objects  # noqa F403\n+\n+    _dummy_objects.update(get_objects_from_module(dummy_torch_and_transformers_objects))\n+else:\n+    _import_structure[\"pipeline_photon\"] = [\"PhotonPipeline\"]\n+\n+# Import T5GemmaEncoder for pipeline loading compatibility\n+try:\n+    if is_transformers_available():\n+        import transformers\n+        from transformers.models.t5gemma.modeling_t5gemma import T5GemmaEncoder\n+\n+        _additional_imports[\"T5GemmaEncoder\"] = T5GemmaEncoder\n+        # Patch transformers module directly for serialization\n+        if not hasattr(transformers, \"T5GemmaEncoder\"):\n+            transformers.T5GemmaEncoder = T5GemmaEncoder\n+except ImportError:\n+    pass\n+\n+if TYPE_CHECKING or DIFFUSERS_SLOW_IMPORT:\n+    try:\n+        if not (is_transformers_available() and is_torch_available()):\n+            raise OptionalDependencyNotAvailable()\n+    except OptionalDependencyNotAvailable:\n+        from ...utils.dummy_torch_and_transformers_objects import *  # noqa F403\n+    else:\n+        from .pipeline_output import PhotonPipelineOutput\n+        from .pipeline_photon import PhotonPipeline\n+\n+else:\n+    import sys\n+\n+    sys.modules[__name__] = _LazyModule(\n+        __name__,\n+        globals()[\"__file__\"],\n+        _import_structure,\n+        module_spec=__spec__,\n+    )\n+\n+    for name, value in _dummy_objects.items():\n+        setattr(sys.modules[__name__], name, value)\n+    for name, value in _additional_imports.items():\n+        setattr(sys.modules[__name__], name, value)"
        },
        {
          "filename": "src/diffusers/pipelines/photon/pipeline_output.py",
          "status": "added",
          "additions": 35,
          "deletions": 0,
          "changes": 35,
          "patch": "@@ -0,0 +1,35 @@\n+# Copyright 2025 The Photoroom and the HuggingFace Teams. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from dataclasses import dataclass\n+from typing import List, Union\n+\n+import numpy as np\n+import PIL.Image\n+\n+from ...utils import BaseOutput\n+\n+\n+@dataclass\n+class PhotonPipelineOutput(BaseOutput):\n+    \"\"\"\n+    Output class for Photon pipelines.\n+\n+    Args:\n+        images (`List[PIL.Image.Image]` or `np.ndarray`)\n+            List of denoised PIL images of length `batch_size` or numpy array of shape `(batch_size, height, width,\n+            num_channels)`. PIL images or numpy array present the denoised images of the diffusion pipeline.\n+    \"\"\"\n+\n+    images: Union[List[PIL.Image.Image], np.ndarray]"
        },
        {
          "filename": "src/diffusers/pipelines/photon/pipeline_photon.py",
          "status": "added",
          "additions": 768,
          "deletions": 0,
          "changes": 768,
          "patch": "@@ -0,0 +1,768 @@\n+# Copyright 2025 The Photoroom and The HuggingFace Teams. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import html\n+import inspect\n+import re\n+import urllib.parse as ul\n+from typing import Callable, Dict, List, Optional, Union\n+\n+import ftfy\n+import torch\n+from transformers import (\n+    AutoTokenizer,\n+    GemmaTokenizerFast,\n+    T5TokenizerFast,\n+)\n+from transformers.models.t5gemma.modeling_t5gemma import T5GemmaEncoder\n+\n+from diffusers.image_processor import PixArtImageProcessor\n+from diffusers.loaders import FromSingleFileMixin, LoraLoaderMixin, TextualInversionLoaderMixin\n+from diffusers.models import AutoencoderDC, AutoencoderKL\n+from diffusers.models.transformers.transformer_photon import PhotonTransformer2DModel\n+from diffusers.pipelines.photon.pipeline_output import PhotonPipelineOutput\n+from diffusers.pipelines.pipeline_utils import DiffusionPipeline\n+from diffusers.schedulers import FlowMatchEulerDiscreteScheduler\n+from diffusers.utils import (\n+    logging,\n+    replace_example_docstring,\n+)\n+from diffusers.utils.torch_utils import randn_tensor\n+\n+\n+DEFAULT_RESOLUTION = 512\n+\n+ASPECT_RATIO_256_BIN = {\n+    \"0.46\": [160, 352],\n+    \"0.6\": [192, 320],\n+    \"0.78\": [224, 288],\n+    \"1.0\": [256, 256],\n+    \"1.29\": [288, 224],\n+    \"1.67\": [320, 192],\n+    \"2.2\": [352, 160],\n+}\n+\n+ASPECT_RATIO_512_BIN = {\n+    \"0.5\": [352, 704],\n+    \"0.57\": [384, 672],\n+    \"0.6\": [384, 640],\n+    \"0.68\": [416, 608],\n+    \"0.78\": [448, 576],\n+    \"0.88\": [480, 544],\n+    \"1.0\": [512, 512],\n+    \"1.13\": [544, 480],\n+    \"1.29\": [576, 448],\n+    \"1.46\": [608, 416],\n+    \"1.67\": [640, 384],\n+    \"1.75\": [672, 384],\n+    \"2.0\": [704, 352],\n+}\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class TextPreprocessor:\n+    \"\"\"Text preprocessing utility for PhotonPipeline.\"\"\"\n+\n+    def __init__(self):\n+        \"\"\"Initialize text preprocessor.\"\"\"\n+        self.bad_punct_regex = re.compile(\n+            r\"[\"\n+            + \"#\u00ae\u2022\u00a9\u2122&@\u00b7\u00ba\u00bd\u00be\u00bf\u00a1\u00a7~\"\n+            + r\"\\)\"\n+            + r\"\\(\"\n+            + r\"\\]\"\n+            + r\"\\[\"\n+            + r\"\\}\"\n+            + r\"\\{\"\n+            + r\"\\|\"\n+            + r\"\\\\\"\n+            + r\"\\/\"\n+            + r\"\\*\"\n+            + r\"]{1,}\"\n+        )\n+\n+    def clean_text(self, text: str) -> str:\n+        \"\"\"Clean text using comprehensive text processing logic.\"\"\"\n+        # See Deepfloyd https://github.com/deep-floyd/IF/blob/develop/deepfloyd_if/modules/t5.py\n+        text = str(text)\n+        text = ul.unquote_plus(text)\n+        text = text.strip().lower()\n+        text = re.sub(\"<person>\", \"person\", text)\n+\n+        # Remove all urls:\n+        text = re.sub(\n+            r\"\\b((?:https?|www):(?:\\/{1,3}|[a-zA-Z0-9%])|[a-zA-Z0-9.\\-]+[.](?:com|co|ru|net|org|edu|gov|it)[\\w/-]*\\b\\/?(?!@))\",\n+            \"\",\n+            text,\n+        )  # regex for urls\n+\n+        # @<nickname>\n+        text = re.sub(r\"@[\\w\\d]+\\b\", \"\", text)\n+\n+        # 31C0\u201431EF CJK Strokes through 4E00\u20149FFF CJK Unified Ideographs\n+        text = re.sub(r\"[\\u31c0-\\u31ef]+\", \"\", text)\n+        text = re.sub(r\"[\\u31f0-\\u31ff]+\", \"\", text)\n+        text = re.sub(r\"[\\u3200-\\u32ff]+\", \"\", text)\n+        text = re.sub(r\"[\\u3300-\\u33ff]+\", \"\", text)\n+        text = re.sub(r\"[\\u3400-\\u4dbf]+\", \"\", text)\n+        text = re.sub(r\"[\\u4dc0-\\u4dff]+\", \"\", text)\n+        text = re.sub(r\"[\\u4e00-\\u9fff]+\", \"\", text)\n+\n+        # \u0432\u0441\u0435 \u0432\u0438\u0434\u044b \u0442\u0438\u0440\u0435 / all types of dash --> \"-\"\n+        text = re.sub(\n+            r\"[\\u002D\\u058A\\u05BE\\u1400\\u1806\\u2010-\\u2015\\u2E17\\u2E1A\\u2E3A\\u2E3B\\u2E40\\u301C\\u3030\\u30A0\\uFE31\\uFE32\\uFE58\\uFE63\\uFF0D]+\",\n+            \"-\",\n+            text,\n+        )\n+\n+        # \u043a\u0430\u0432\u044b\u0447\u043a\u0438 \u043a \u043e\u0434\u043d\u043e\u043c\u0443 \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u0443\n+        text = re.sub(r\"[`\u00b4\u00ab\u00bb\" \"\u00a8]\", '\"', text)\n+        text = re.sub(r\"['']\", \"'\", text)\n+\n+        # &quot; and &amp\n+        text = re.sub(r\"&quot;?\", \"\", text)\n+        text = re.sub(r\"&amp\", \"\", text)\n+\n+        # ip addresses:\n+        text = re.sub(r\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\", \" \", text)\n+\n+        # article ids:\n+        text = re.sub(r\"\\d:\\d\\d\\s+$\", \"\", text)\n+\n+        # \\n\n+        text = re.sub(r\"\\\\n\", \" \", text)\n+\n+        # \"#123\", \"#12345..\", \"123456..\"\n+        text = re.sub(r\"#\\d{1,3}\\b\", \"\", text)\n+        text = re.sub(r\"#\\d{5,}\\b\", \"\", text)\n+        text = re.sub(r\"\\b\\d{6,}\\b\", \"\", text)\n+\n+        # filenames:\n+        text = re.sub(r\"[\\S]+\\.(?:png|jpg|jpeg|bmp|webp|eps|pdf|apk|mp4)\", \"\", text)\n+\n+        # Clean punctuation\n+        text = re.sub(r\"[\\\"\\']{2,}\", r'\"', text)  # \"\"\"AUSVERKAUFT\"\"\"\n+        text = re.sub(r\"[\\.]{2,}\", r\" \", text)\n+\n+        text = re.sub(self.bad_punct_regex, r\" \", text)  # ***AUSVERKAUFT***, #AUSVERKAUFT\n+        text = re.sub(r\"\\s+\\.\\s+\", r\" \", text)  # \" . \"\n+\n+        # this-is-my-cute-cat / this_is_my_cute_cat\n+        regex2 = re.compile(r\"(?:\\-|\\_)\")\n+        if len(re.findall(regex2, text)) > 3:\n+            text = re.sub(regex2, \" \", text)\n+\n+        # Basic cleaning\n+        text = ftfy.fix_text(text)\n+        text = html.unescape(html.unescape(text))\n+        text = text.strip()\n+\n+        # Clean alphanumeric patterns\n+        text = re.sub(r\"\\b[a-zA-Z]{1,3}\\d{3,15}\\b\", \"\", text)  # jc6640\n+        text = re.sub(r\"\\b[a-zA-Z]+\\d+[a-zA-Z]+\\b\", \"\", text)  # jc6640vc\n+        text = re.sub(r\"\\b\\d+[a-zA-Z]+\\d+\\b\", \"\", text)  # 6640vc231\n+\n+        # Common spam patterns\n+        text = re.sub(r\"(worldwide\\s+)?(free\\s+)?shipping\", \"\", text)\n+        text = re.sub(r\"(free\\s)?download(\\sfree)?\", \"\", text)\n+        text = re.sub(r\"\\bclick\\b\\s(?:for|on)\\s\\w+\", \"\", text)\n+        text = re.sub(r\"\\b(?:png|jpg|jpeg|bmp|webp|eps|pdf|apk|mp4)(\\simage[s]?)?\", \"\", text)\n+        text = re.sub(r\"\\bpage\\s+\\d+\\b\", \"\", text)\n+\n+        text = re.sub(r\"\\b\\d*[a-zA-Z]+\\d+[a-zA-Z]+\\d+[a-zA-Z\\d]*\\b\", r\" \", text)  # j2d1a2a...\n+        text = re.sub(r\"\\b\\d+\\.?\\d*[x\u0445\u00d7]\\d+\\.?\\d*\\b\", \"\", text)\n+\n+        # Final cleanup\n+        text = re.sub(r\"\\b\\s+\\:\\s+\", r\": \", text)\n+        text = re.sub(r\"(\\D[,\\./])\\b\", r\"\\1 \", text)\n+        text = re.sub(r\"\\s+\", \" \", text)\n+\n+        text.strip()\n+\n+        text = re.sub(r\"^[\\\"\\']([\\w\\W]+)[\\\"\\']$\", r\"\\1\", text)\n+        text = re.sub(r\"^[\\'\\_,\\-\\:;]\", r\"\", text)\n+        text = re.sub(r\"[\\'\\_,\\-\\:\\-\\+]$\", r\"\", text)\n+        text = re.sub(r\"^\\.\\S+$\", \"\", text)\n+\n+        return text.strip()\n+\n+\n+EXAMPLE_DOC_STRING = \"\"\"\n+    Examples:\n+        ```py\n+        >>> import torch\n+        >>> from diffusers import PhotonPipeline\n+\n+        >>> # Load pipeline with from_pretrained\n+        >>> pipe = PhotonPipeline.from_pretrained(\"Photoroom/photon-512-t2i-sft\")\n+        >>> pipe.to(\"cuda\")\n+\n+        >>> prompt = \"A digital painting of a rusty, vintage tram on a sandy beach\"\n+        >>> image = pipe(prompt, num_inference_steps=28, guidance_scale=5.0).images[0]\n+        >>> image.save(\"photon_output.png\")\n+        ```\n+\"\"\"\n+\n+\n+class PhotonPipeline(\n+    DiffusionPipeline,\n+    LoraLoaderMixin,\n+    FromSingleFileMixin,\n+    TextualInversionLoaderMixin,\n+):\n+    r\"\"\"\n+    Pipeline for text-to-image generation using Photon Transformer.\n+\n+    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n+    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n+\n+    Args:\n+        transformer ([`PhotonTransformer2DModel`]):\n+            The Photon transformer model to denoise the encoded image latents.\n+        scheduler ([`FlowMatchEulerDiscreteScheduler`]):\n+            A scheduler to be used in combination with `transformer` to denoise the encoded image latents.\n+        text_encoder ([`T5GemmaEncoder`]):\n+            Text encoder model for encoding prompts.\n+        tokenizer ([`T5TokenizerFast` or `GemmaTokenizerFast`]):\n+            Tokenizer for the text encoder.\n+        vae ([`AutoencoderKL`] or [`AutoencoderDC`]):\n+            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n+            Supports both AutoencoderKL (8x compression) and AutoencoderDC (32x compression).\n+    \"\"\"\n+\n+    model_cpu_offload_seq = \"text_encoder->transformer->vae\"\n+    _callback_tensor_inputs = [\"latents\", \"prompt_embeds\"]\n+    _optional_components = [\"vae\"]\n+\n+    def __init__(\n+        self,\n+        transformer: PhotonTransformer2DModel,\n+        scheduler: FlowMatchEulerDiscreteScheduler,\n+        text_encoder: T5GemmaEncoder,\n+        tokenizer: Union[T5TokenizerFast, GemmaTokenizerFast, AutoTokenizer],\n+        vae: Optional[Union[AutoencoderKL, AutoencoderDC]] = None,\n+        default_sample_size: Optional[int] = DEFAULT_RESOLUTION,\n+    ):\n+        super().__init__()\n+\n+        if PhotonTransformer2DModel is None:\n+            raise ImportError(\n+                \"PhotonTransformer2DModel is not available. Please ensure the transformer_photon module is properly installed.\"\n+            )\n+\n+        self.text_preprocessor = TextPreprocessor()\n+        self.default_sample_size = default_sample_size\n+        self._guidance_scale = 1.0\n+\n+        self.register_modules(\n+            transformer=transformer,\n+            scheduler=scheduler,\n+            text_encoder=text_encoder,\n+            tokenizer=tokenizer,\n+            vae=vae,\n+        )\n+\n+        self.register_to_config(default_sample_size=self.default_sample_size)\n+\n+        if vae is not None:\n+            self.image_processor = PixArtImageProcessor(vae_scale_factor=self.vae_scale_factor)\n+        else:\n+            self.image_processor = None\n+\n+    @property\n+    def vae_scale_factor(self):\n+        if self.vae is None:\n+            return 8\n+        if hasattr(self.vae, \"spatial_compression_ratio\"):\n+            return self.vae.spatial_compression_ratio\n+        else:  # Flux VAE\n+            return 2 ** (len(self.vae.config.block_out_channels) - 1)\n+\n+    @property\n+    def do_classifier_free_guidance(self):\n+        \"\"\"Check if classifier-free guidance is enabled based on guidance scale.\"\"\"\n+        return self._guidance_scale > 1.0\n+\n+    @property\n+    def guidance_scale(self):\n+        return self._guidance_scale\n+\n+    def get_default_resolution(self):\n+        \"\"\"Determine the default resolution based on the loaded VAE and config.\n+\n+        Returns:\n+            int: The default sample size (height/width) to use for generation.\n+        \"\"\"\n+        default_from_config = getattr(self.config, \"default_sample_size\", None)\n+        if default_from_config is not None:\n+            return default_from_config\n+\n+        return DEFAULT_RESOLUTION\n+\n+    def prepare_latents(\n+        self,\n+        batch_size: int,\n+        num_channels_latents: int,\n+        height: int,\n+        width: int,\n+        dtype: torch.dtype,\n+        device: torch.device,\n+        generator: Optional[torch.Generator] = None,\n+        latents: Optional[torch.Tensor] = None,\n+    ):\n+        \"\"\"Prepare initial latents for the diffusion process.\"\"\"\n+        if latents is None:\n+            spatial_compression = self.vae_scale_factor\n+            latent_height, latent_width = (\n+                height // spatial_compression,\n+                width // spatial_compression,\n+            )\n+            shape = (batch_size, num_channels_latents, latent_height, latent_width)\n+            latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n+        else:\n+            latents = latents.to(device)\n+        return latents\n+\n+    def encode_prompt(\n+        self,\n+        prompt: Union[str, List[str]],\n+        device: Optional[torch.device] = None,\n+        do_classifier_free_guidance: bool = True,\n+        negative_prompt: str = \"\",\n+        num_images_per_prompt: int = 1,\n+        prompt_embeds: Optional[torch.FloatTensor] = None,\n+        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n+        prompt_attention_mask: Optional[torch.BoolTensor] = None,\n+        negative_prompt_attention_mask: Optional[torch.BoolTensor] = None,\n+    ):\n+        \"\"\"Encode text prompt using standard text encoder and tokenizer, or use precomputed embeddings.\"\"\"\n+        if device is None:\n+            device = self._execution_device\n+\n+        if prompt_embeds is None:\n+            if isinstance(prompt, str):\n+                prompt = [prompt]\n+            # Encode the prompts\n+            prompt_embeds, prompt_attention_mask, negative_prompt_embeds, negative_prompt_attention_mask = (\n+                self._encode_prompt_standard(prompt, device, do_classifier_free_guidance, negative_prompt)\n+            )\n+\n+        # Duplicate embeddings for each generation per prompt\n+        if num_images_per_prompt > 1:\n+            # Repeat prompt embeddings\n+            bs_embed, seq_len, _ = prompt_embeds.shape\n+            prompt_embeds = prompt_embeds.repeat(1, num_images_per_prompt, 1)\n+            prompt_embeds = prompt_embeds.view(bs_embed * num_images_per_prompt, seq_len, -1)\n+\n+            if prompt_attention_mask is not None:\n+                prompt_attention_mask = prompt_attention_mask.view(bs_embed, -1)\n+                prompt_attention_mask = prompt_attention_mask.repeat(num_images_per_prompt, 1)\n+\n+            # Repeat negative embeddings if using CFG\n+            if do_classifier_free_guidance and negative_prompt_embeds is not None:\n+                bs_embed, seq_len, _ = negative_prompt_embeds.shape\n+                negative_prompt_embeds = negative_prompt_embeds.repeat(1, num_images_per_prompt, 1)\n+                negative_prompt_embeds = negative_prompt_embeds.view(bs_embed * num_images_per_prompt, seq_len, -1)\n+\n+                if negative_prompt_attention_mask is not None:\n+                    negative_prompt_attention_mask = negative_prompt_attention_mask.view(bs_embed, -1)\n+                    negative_prompt_attention_mask = negative_prompt_attention_mask.repeat(num_images_per_prompt, 1)\n+\n+        return (\n+            prompt_embeds,\n+            prompt_attention_mask,\n+            negative_prompt_embeds if do_classifier_free_guidance else None,\n+            negative_prompt_attention_mask if do_classifier_free_guidance else None,\n+        )\n+\n+    def _tokenize_prompts(self, prompts: List[str], device: torch.device):\n+        \"\"\"Tokenize and clean prompts.\"\"\"\n+        cleaned = [self.text_preprocessor.clean_text(text) for text in prompts]\n+        tokens = self.tokenizer(\n+            cleaned,\n+            padding=\"max_length\",\n+            max_length=self.tokenizer.model_max_length,\n+            truncation=True,\n+            return_attention_mask=True,\n+            return_tensors=\"pt\",\n+        )\n+        return tokens[\"input_ids\"].to(device), tokens[\"attention_mask\"].bool().to(device)\n+\n+    def _encode_prompt_standard(\n+        self,\n+        prompt: List[str],\n+        device: torch.device,\n+        do_classifier_free_guidance: bool = True,\n+        negative_prompt: str = \"\",\n+    ):\n+        \"\"\"Encode prompt using standard text encoder and tokenizer with batch processing.\"\"\"\n+        batch_size = len(prompt)\n+\n+        if do_classifier_free_guidance:\n+            if isinstance(negative_prompt, str):\n+                negative_prompt = [negative_prompt] * batch_size\n+\n+            prompts_to_encode = negative_prompt + prompt\n+        else:\n+            prompts_to_encode = prompt\n+\n+        input_ids, attention_mask = self._tokenize_prompts(prompts_to_encode, device)\n+\n+        with torch.no_grad():\n+            embeddings = self.text_encoder(\n+                input_ids=input_ids,\n+                attention_mask=attention_mask,\n+                output_hidden_states=True,\n+            )[\"last_hidden_state\"]\n+\n+        if do_classifier_free_guidance:\n+            uncond_text_embeddings, text_embeddings = embeddings.split(batch_size, dim=0)\n+            uncond_cross_attn_mask, cross_attn_mask = attention_mask.split(batch_size, dim=0)\n+        else:\n+            text_embeddings = embeddings\n+            cross_attn_mask = attention_mask\n+            uncond_text_embeddings = None\n+            uncond_cross_attn_mask = None\n+\n+        return text_embeddings, cross_attn_mask, uncond_text_embeddings, uncond_cross_attn_mask\n+\n+    def check_inputs(\n+        self,\n+        prompt: Union[str, List[str]],\n+        height: int,\n+        width: int,\n+        guidance_scale: float,\n+        callback_on_step_end_tensor_inputs: Optional[List[str]] = None,\n+        prompt_embeds: Optional[torch.FloatTensor] = None,\n+        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n+    ):\n+        \"\"\"Check that all inputs are in correct format.\"\"\"\n+        if prompt is not None and prompt_embeds is not None:\n+            raise ValueError(\n+                f\"Cannot forward both `prompt`: {prompt} and `prompt_embeds`: {prompt_embeds}. Please make sure to\"\n+                \" only forward one of the two.\"\n+            )\n+\n+        if prompt is None and prompt_embeds is None:\n+            raise ValueError(\n+                \"Provide either `prompt` or `prompt_embeds`. Cannot leave both `prompt` and `prompt_embeds` undefined.\"\n+            )\n+\n+        if prompt is not None and (not isinstance(prompt, str) and not isinstance(prompt, list)):\n+            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n+\n+        if prompt_embeds is not None and guidance_scale > 1.0 and negative_prompt_embeds is None:\n+            raise ValueError(\n+                \"When `prompt_embeds` is provided and `guidance_scale > 1.0`, \"\n+                \"`negative_prompt_embeds` must also be provided for classifier-free guidance.\"\n+            )\n+\n+        spatial_compression = self.vae_scale_factor\n+        if height % spatial_compression != 0 or width % spatial_compression != 0:\n+            raise ValueError(\n+                f\"`height` and `width` have to be divisible by {spatial_compression} but are {height} and {width}.\"\n+            )\n+\n+        if guidance_scale < 1.0:\n+            raise ValueError(f\"guidance_scale has to be >= 1.0 but is {guidance_scale}\")\n+\n+        if callback_on_step_end_tensor_inputs is not None and not isinstance(callback_on_step_end_tensor_inputs, list):\n+            raise ValueError(\n+                f\"`callback_on_step_end_tensor_inputs` has to be a list but is {callback_on_step_end_tensor_inputs}\"\n+            )\n+\n+        if callback_on_step_end_tensor_inputs is not None and not all(\n+            k in self._callback_tensor_inputs for k in callback_on_step_end_tensor_inputs\n+        ):\n+            raise ValueError(\n+                f\"`callback_on_step_end_tensor_inputs` has to be in {self._callback_tensor_inputs}, but found {[k for k in callback_on_step_end_tensor_inputs if k not in self._callback_tensor_inputs]}\"\n+            )\n+\n+    @torch.no_grad()\n+    @replace_example_docstring(EXAMPLE_DOC_STRING)\n+    def __call__(\n+        self,\n+        prompt: Union[str, List[str]] = None,\n+        negative_prompt: str = \"\",\n+        height: Optional[int] = None,\n+        width: Optional[int] = None,\n+        num_inference_steps: int = 28,\n+        timesteps: List[int] = None,\n+        guidance_scale: float = 4.0,\n+        num_images_per_prompt: Optional[int] = 1,\n+        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n+        latents: Optional[torch.Tensor] = None,\n+        prompt_embeds: Optional[torch.FloatTensor] = None,\n+        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n+        prompt_attention_mask: Optional[torch.BoolTensor] = None,\n+        negative_prompt_attention_mask: Optional[torch.BoolTensor] = None,\n+        output_type: Optional[str] = \"pil\",\n+        return_dict: bool = True,\n+        use_resolution_binning: bool = True,\n+        callback_on_step_end: Optional[Callable[[int, int, Dict], None]] = None,\n+        callback_on_step_end_tensor_inputs: List[str] = [\"latents\"],\n+    ):\n+        \"\"\"\n+        Function invoked when calling the pipeline for generation.\n+\n+        Args:\n+            prompt (`str` or `List[str]`, *optional*):\n+                The prompt or prompts to guide the image generation. If not defined, one has to pass `prompt_embeds`\n+                instead.\n+            negative_prompt (`str`, *optional*, defaults to `\"\"`):\n+                The prompt or prompts not to guide the image generation. Ignored when not using guidance (i.e., ignored\n+                if `guidance_scale` is less than `1`).\n+            height (`int`, *optional*, defaults to self.transformer.config.sample_size * self.vae_scale_factor):\n+                The height in pixels of the generated image.\n+            width (`int`, *optional*, defaults to self.transformer.config.sample_size * self.vae_scale_factor):\n+                The width in pixels of the generated image.\n+            num_inference_steps (`int`, *optional*, defaults to 28):\n+                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n+                expense of slower inference.\n+            timesteps (`List[int]`, *optional*):\n+                Custom timesteps to use for the denoising process with schedulers which support a `timesteps` argument\n+                in their `set_timesteps` method. If not defined, the default behavior when `num_inference_steps` is\n+                passed will be used. Must be in descending order.\n+            guidance_scale (`float`, *optional*, defaults to 4.0):\n+                Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).\n+                `guidance_scale` is defined as `w` of equation 2. of [Imagen\n+                Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale >\n+                1`. Higher guidance scale encourages to generate images that are closely linked to the text `prompt`,\n+                usually at the expense of lower image quality.\n+            num_images_per_prompt (`int`, *optional*, defaults to 1):\n+                The number of images to generate per prompt.\n+            generator (`torch.Generator` or `List[torch.Generator]`, *optional*):\n+                One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)\n+                to make generation deterministic.\n+            latents (`torch.Tensor`, *optional*):\n+                Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for image\n+                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents\n+                tensor will be generated by sampling using the supplied random `generator`.\n+            prompt_embeds (`torch.FloatTensor`, *optional*):\n+                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n+                provided, text embeddings will be generated from `prompt` input argument.\n+            negative_prompt_embeds (`torch.FloatTensor`, *optional*):\n+                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt\n+                weighting. If not provided and `guidance_scale > 1`, negative embeddings will be generated from an\n+                empty string.\n+            prompt_attention_mask (`torch.BoolTensor`, *optional*):\n+                Pre-generated attention mask for `prompt_embeds`. If not provided, attention mask will be generated\n+                from `prompt` input argument.\n+            negative_prompt_attention_mask (`torch.BoolTensor`, *optional*):\n+                Pre-generated attention mask for `negative_prompt_embeds`. If not provided and `guidance_scale > 1`,\n+                attention mask will be generated from an empty string.\n+            output_type (`str`, *optional*, defaults to `\"pil\"`):\n+                The output format of the generate image. Choose between\n+                [PIL](https://pillow.readthedocs.io/en/stable/): `PIL.Image.Image` or `np.array`.\n+            return_dict (`bool`, *optional*, defaults to `True`):\n+                Whether or not to return a [`~pipelines.photon.PhotonPipelineOutput`] instead of a plain tuple.\n+            use_resolution_binning (`bool`, *optional*, defaults to `True`):\n+                If set to `True`, the requested height and width are first mapped to the closest resolutions using\n+                predefined aspect ratio bins. After the produced latents are decoded into images, they are resized back\n+                to the requested resolution. Useful for generating non-square images at optimal resolutions.\n+            callback_on_step_end (`Callable`, *optional*):\n+                A function that calls at the end of each denoising steps during the inference. The function is called\n+                with the following arguments: `callback_on_step_end(self, step, timestep, callback_kwargs)`.\n+                `callback_kwargs` will include a list of all tensors as specified by\n+                `callback_on_step_end_tensor_inputs`.\n+            callback_on_step_end_tensor_inputs (`List`, *optional*):\n+                The list of tensor inputs for the `callback_on_step_end` function. The tensors specified in the list\n+                will be passed as `callback_kwargs` argument. You will only be able to include tensors that are listed\n+                in the `._callback_tensor_inputs` attribute.\n+\n+        Examples:\n+\n+        Returns:\n+            [`~pipelines.photon.PhotonPipelineOutput`] or `tuple`: [`~pipelines.photon.PhotonPipelineOutput`] if\n+            `return_dict` is True, otherwise a `tuple. When returning a tuple, the first element is a list with the\n+            generated images.\n+        \"\"\"\n+\n+        # 0. Set height and width\n+        default_resolution = self.get_default_resolution()\n+        height = height or default_resolution\n+        width = width or default_resolution\n+\n+        if use_resolution_binning:\n+            if self.image_processor is None:\n+                raise ValueError(\n+                    \"Resolution binning requires a VAE with image_processor, but VAE is not available. \"\n+                    \"Set use_resolution_binning=False or provide a VAE.\"\n+                )\n+            if self.default_sample_size <= 256:\n+                aspect_ratio_bin = ASPECT_RATIO_256_BIN\n+            else:\n+                aspect_ratio_bin = ASPECT_RATIO_512_BIN\n+\n+            # Store original dimensions\n+            orig_height, orig_width = height, width\n+            # Map to closest resolution in the bin\n+            height, width = self.image_processor.classify_height_width_bin(height, width, ratios=aspect_ratio_bin)\n+\n+        # 1. Check inputs\n+        self.check_inputs(\n+            prompt,\n+            height,\n+            width,\n+            guidance_scale,\n+            callback_on_step_end_tensor_inputs,\n+            prompt_embeds,\n+            negative_prompt_embeds,\n+        )\n+\n+        if self.vae is None and output_type not in [\"latent\", \"pt\"]:\n+            raise ValueError(\n+                f\"VAE is required for output_type='{output_type}' but it is not available. \"\n+                \"Either provide a VAE or set output_type='latent' or 'pt' to get latent outputs.\"\n+            )\n+\n+        if prompt is not None and isinstance(prompt, str):\n+            batch_size = 1\n+        elif prompt is not None and isinstance(prompt, list):\n+            batch_size = len(prompt)\n+        else:\n+            batch_size = prompt_embeds.shape[0]\n+\n+        # Use execution device (handles offloading scenarios including group offloading)\n+        device = self._execution_device\n+\n+        self._guidance_scale = guidance_scale\n+\n+        # 2. Encode input prompt\n+        text_embeddings, cross_attn_mask, uncond_text_embeddings, uncond_cross_attn_mask = self.encode_prompt(\n+            prompt,\n+            device,\n+            do_classifier_free_guidance=self.do_classifier_free_guidance,\n+            negative_prompt=negative_prompt,\n+            num_images_per_prompt=num_images_per_prompt,\n+            prompt_embeds=prompt_embeds,\n+            negative_prompt_embeds=negative_prompt_embeds,\n+            prompt_attention_mask=prompt_attention_mask,\n+            negative_prompt_attention_mask=negative_prompt_attention_mask,\n+        )\n+        # Expose standard names for callbacks parity\n+        prompt_embeds = text_embeddings\n+        negative_prompt_embeds = uncond_text_embeddings\n+\n+        # 3. Prepare timesteps\n+        if timesteps is not None:\n+            self.scheduler.set_timesteps(timesteps=timesteps, device=device)\n+            timesteps = self.scheduler.timesteps\n+            num_inference_steps = len(timesteps)\n+        else:\n+            self.scheduler.set_timesteps(num_inference_steps, device=device)\n+            timesteps = self.scheduler.timesteps\n+\n+        self.num_timesteps = len(timesteps)\n+\n+        # 4. Prepare latent variables\n+        if self.vae is not None:\n+            num_channels_latents = self.vae.config.latent_channels\n+        else:\n+            # When vae is None, get latent channels from transformer\n+            num_channels_latents = self.transformer.config.in_channels\n+        latents = self.prepare_latents(\n+            batch_size * num_images_per_prompt,\n+            num_channels_latents,\n+            height,\n+            width,\n+            text_embeddings.dtype,\n+            device,\n+            generator,\n+            latents,\n+        )\n+\n+        # 5. Prepare extra step kwargs\n+        extra_step_kwargs = {}\n+        accepts_eta = \"eta\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n+        if accepts_eta:\n+            extra_step_kwargs[\"eta\"] = 0.0\n+\n+        # 6. Prepare cross-attention embeddings and masks\n+        if self.do_classifier_free_guidance:\n+            ca_embed = torch.cat([uncond_text_embeddings, text_embeddings], dim=0)\n+            ca_mask = None\n+            if cross_attn_mask is not None and uncond_cross_attn_mask is not None:\n+                ca_mask = torch.cat([uncond_cross_attn_mask, cross_attn_mask], dim=0)\n+        else:\n+            ca_embed = text_embeddings\n+            ca_mask = cross_attn_mask\n+\n+        # 7. Denoising loop\n+        num_warmup_steps = max(len(timesteps) - num_inference_steps * self.scheduler.order, 0)\n+\n+        with self.progress_bar(total=num_inference_steps) as progress_bar:\n+            for i, t in enumerate(timesteps):\n+                # Duplicate latents if using classifier-free guidance\n+                if self.do_classifier_free_guidance:\n+                    latents_in = torch.cat([latents, latents], dim=0)\n+                    # Normalize timestep for the transformer\n+                    t_cont = (t.float() / self.scheduler.config.num_train_timesteps).view(1).repeat(2).to(device)\n+                else:\n+                    latents_in = latents\n+                    # Normalize timestep for the transformer\n+                    t_cont = (t.float() / self.scheduler.config.num_train_timesteps).view(1).to(device)\n+\n+                # Forward through transformer\n+                noise_pred = self.transformer(\n+                    hidden_states=latents_in,\n+                    timestep=t_cont,\n+                    encoder_hidden_states=ca_embed,\n+                    attention_mask=ca_mask,\n+                    return_dict=False,\n+                )[0]\n+\n+                # Apply CFG\n+                if self.do_classifier_free_guidance:\n+                    noise_uncond, noise_text = noise_pred.chunk(2, dim=0)\n+                    noise_pred = noise_uncond + guidance_scale * (noise_text - noise_uncond)\n+\n+                # Compute the previous noisy sample x_t -> x_t-1\n+                latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs).prev_sample\n+\n+                if callback_on_step_end is not None:\n+                    callback_kwargs = {}\n+                    for k in callback_on_step_end_tensor_inputs:\n+                        callback_kwargs[k] = locals()[k]\n+                    callback_on_step_end(self, i, t, callback_kwargs)\n+\n+                # Call the callback, if provided\n+                if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n+                    progress_bar.update()\n+\n+        # 8. Post-processing\n+        if output_type == \"latent\" or (output_type == \"pt\" and self.vae is None):\n+            image = latents\n+        else:\n+            # Unscale latents for VAE (supports both AutoencoderKL and AutoencoderDC)\n+            scaling_factor = getattr(self.vae.config, \"scaling_factor\", 0.18215)\n+            shift_factor = getattr(self.vae.config, \"shift_factor\", 0.0)\n+            latents = (latents / scaling_factor) + shift_factor\n+            # Decode using VAE (AutoencoderKL or AutoencoderDC)\n+            image = self.vae.decode(latents, return_dict=False)[0]\n+            # Resize back to original resolution if using binning\n+            if use_resolution_binning:\n+                image = self.image_processor.resize_and_crop_tensor(image, orig_width, orig_height)\n+\n+            # Use standard image processor for post-processing\n+            image = self.image_processor.postprocess(image, output_type=output_type)\n+\n+        # Offload all models\n+        self.maybe_free_model_hooks()\n+\n+        if not return_dict:\n+            return (image,)\n+\n+        return PhotonPipelineOutput(images=image)"
        },
        {
          "filename": "src/diffusers/utils/dummy_pt_objects.py",
          "status": "modified",
          "additions": 15,
          "deletions": 0,
          "changes": 15,
          "patch": "@@ -1098,6 +1098,21 @@ def from_pretrained(cls, *args, **kwargs):\n         requires_backends(cls, [\"torch\"])\n \n \n+class PhotonTransformer2DModel(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+    @classmethod\n+    def from_config(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\"])\n+\n+    @classmethod\n+    def from_pretrained(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\"])\n+\n+\n class PixArtTransformer2DModel(metaclass=DummyObject):\n     _backends = [\"torch\"]\n "
        },
        {
          "filename": "src/diffusers/utils/dummy_torch_and_transformers_objects.py",
          "status": "modified",
          "additions": 15,
          "deletions": 0,
          "changes": 15,
          "patch": "@@ -1847,6 +1847,21 @@ def from_pretrained(cls, *args, **kwargs):\n         requires_backends(cls, [\"torch\", \"transformers\"])\n \n \n+class PhotonPipeline(metaclass=DummyObject):\n+    _backends = [\"torch\", \"transformers\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_config(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_pretrained(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+\n class PIAPipeline(metaclass=DummyObject):\n     _backends = [\"torch\", \"transformers\"]\n "
        },
        {
          "filename": "tests/models/transformers/test_models_transformer_photon.py",
          "status": "added",
          "additions": 83,
          "deletions": 0,
          "changes": 83,
          "patch": "@@ -0,0 +1,83 @@\n+# coding=utf-8\n+# Copyright 2025 HuggingFace Inc.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import unittest\n+\n+import torch\n+\n+from diffusers.models.transformers.transformer_photon import PhotonTransformer2DModel\n+\n+from ...testing_utils import enable_full_determinism, torch_device\n+from ..test_modeling_common import ModelTesterMixin\n+\n+\n+enable_full_determinism()\n+\n+\n+class PhotonTransformerTests(ModelTesterMixin, unittest.TestCase):\n+    model_class = PhotonTransformer2DModel\n+    main_input_name = \"hidden_states\"\n+    uses_custom_attn_processor = True\n+\n+    @property\n+    def dummy_input(self):\n+        return self.prepare_dummy_input()\n+\n+    @property\n+    def input_shape(self):\n+        return (16, 16, 16)\n+\n+    @property\n+    def output_shape(self):\n+        return (16, 16, 16)\n+\n+    def prepare_dummy_input(self, height=16, width=16):\n+        batch_size = 1\n+        num_latent_channels = 16\n+        sequence_length = 16\n+        embedding_dim = 1792\n+\n+        hidden_states = torch.randn((batch_size, num_latent_channels, height, width)).to(torch_device)\n+        encoder_hidden_states = torch.randn((batch_size, sequence_length, embedding_dim)).to(torch_device)\n+        timestep = torch.tensor([1.0]).to(torch_device).expand(batch_size)\n+\n+        return {\n+            \"hidden_states\": hidden_states,\n+            \"timestep\": timestep,\n+            \"encoder_hidden_states\": encoder_hidden_states,\n+        }\n+\n+    def prepare_init_args_and_inputs_for_common(self):\n+        init_dict = {\n+            \"in_channels\": 16,\n+            \"patch_size\": 2,\n+            \"context_in_dim\": 1792,\n+            \"hidden_size\": 1792,\n+            \"mlp_ratio\": 3.5,\n+            \"num_heads\": 28,\n+            \"depth\": 4,  # Smaller depth for testing\n+            \"axes_dim\": [32, 32],\n+            \"theta\": 10_000,\n+        }\n+        inputs_dict = self.prepare_dummy_input()\n+        return init_dict, inputs_dict\n+\n+    def test_gradient_checkpointing_is_applied(self):\n+        expected_set = {\"PhotonTransformer2DModel\"}\n+        super().test_gradient_checkpointing_is_applied(expected_set=expected_set)\n+\n+\n+if __name__ == \"__main__\":\n+    unittest.main()"
        },
        {
          "filename": "tests/pipelines/photon/__init__.py",
          "status": "added",
          "additions": 0,
          "deletions": 0,
          "changes": 0,
          "patch": ""
        },
        {
          "filename": "tests/pipelines/photon/test_pipeline_photon.py",
          "status": "added",
          "additions": 265,
          "deletions": 0,
          "changes": 265,
          "patch": "@@ -0,0 +1,265 @@\n+import unittest\n+\n+import numpy as np\n+import pytest\n+import torch\n+from transformers import AutoTokenizer\n+from transformers.models.t5gemma.configuration_t5gemma import T5GemmaConfig, T5GemmaModuleConfig\n+from transformers.models.t5gemma.modeling_t5gemma import T5GemmaEncoder\n+\n+from diffusers.models import AutoencoderDC, AutoencoderKL\n+from diffusers.models.transformers.transformer_photon import PhotonTransformer2DModel\n+from diffusers.pipelines.photon.pipeline_photon import PhotonPipeline\n+from diffusers.schedulers import FlowMatchEulerDiscreteScheduler\n+from diffusers.utils import is_transformers_version\n+\n+from ..pipeline_params import TEXT_TO_IMAGE_PARAMS\n+from ..test_pipelines_common import PipelineTesterMixin\n+\n+\n+@pytest.mark.xfail(\n+    condition=is_transformers_version(\">\", \"4.57.1\"),\n+    reason=\"See https://github.com/huggingface/diffusers/pull/12456#issuecomment-3424228544\",\n+    strict=False,\n+)\n+class PhotonPipelineFastTests(PipelineTesterMixin, unittest.TestCase):\n+    pipeline_class = PhotonPipeline\n+    params = TEXT_TO_IMAGE_PARAMS - {\"cross_attention_kwargs\"}\n+    batch_params = frozenset([\"prompt\", \"negative_prompt\", \"num_images_per_prompt\"])\n+    test_xformers_attention = False\n+    test_layerwise_casting = True\n+    test_group_offloading = True\n+\n+    @classmethod\n+    def setUpClass(cls):\n+        # Ensure PhotonPipeline has an _execution_device property expected by __call__\n+        if not isinstance(getattr(PhotonPipeline, \"_execution_device\", None), property):\n+            try:\n+                setattr(PhotonPipeline, \"_execution_device\", property(lambda self: torch.device(\"cpu\")))\n+            except Exception:\n+                pass\n+\n+    def get_dummy_components(self):\n+        torch.manual_seed(0)\n+        transformer = PhotonTransformer2DModel(\n+            patch_size=1,\n+            in_channels=4,\n+            context_in_dim=8,\n+            hidden_size=8,\n+            mlp_ratio=2.0,\n+            num_heads=2,\n+            depth=1,\n+            axes_dim=[2, 2],\n+        )\n+\n+        torch.manual_seed(0)\n+        vae = AutoencoderKL(\n+            sample_size=32,\n+            in_channels=3,\n+            out_channels=3,\n+            block_out_channels=(4,),\n+            layers_per_block=1,\n+            latent_channels=4,\n+            norm_num_groups=1,\n+            use_quant_conv=False,\n+            use_post_quant_conv=False,\n+            shift_factor=0.0,\n+            scaling_factor=1.0,\n+        ).eval()\n+\n+        torch.manual_seed(0)\n+        scheduler = FlowMatchEulerDiscreteScheduler()\n+\n+        torch.manual_seed(0)\n+        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/dummy-gemma\")\n+        tokenizer.model_max_length = 64\n+\n+        torch.manual_seed(0)\n+\n+        encoder_params = {\n+            \"vocab_size\": tokenizer.vocab_size,\n+            \"hidden_size\": 8,\n+            \"intermediate_size\": 16,\n+            \"num_hidden_layers\": 1,\n+            \"num_attention_heads\": 2,\n+            \"num_key_value_heads\": 1,\n+            \"head_dim\": 4,\n+            \"max_position_embeddings\": 64,\n+            \"layer_types\": [\"full_attention\"],\n+            \"attention_bias\": False,\n+            \"attention_dropout\": 0.0,\n+            \"dropout_rate\": 0.0,\n+            \"hidden_activation\": \"gelu_pytorch_tanh\",\n+            \"rms_norm_eps\": 1e-06,\n+            \"attn_logit_softcapping\": 50.0,\n+            \"final_logit_softcapping\": 30.0,\n+            \"query_pre_attn_scalar\": 4,\n+            \"rope_theta\": 10000.0,\n+            \"sliding_window\": 4096,\n+        }\n+        encoder_config = T5GemmaModuleConfig(**encoder_params)\n+        text_encoder_config = T5GemmaConfig(encoder=encoder_config, is_encoder_decoder=False, **encoder_params)\n+        text_encoder = T5GemmaEncoder(text_encoder_config)\n+\n+        return {\n+            \"transformer\": transformer,\n+            \"vae\": vae,\n+            \"scheduler\": scheduler,\n+            \"text_encoder\": text_encoder,\n+            \"tokenizer\": tokenizer,\n+        }\n+\n+    def get_dummy_inputs(self, device, seed=0):\n+        if str(device).startswith(\"mps\"):\n+            generator = torch.manual_seed(seed)\n+        else:\n+            generator = torch.Generator(device=device).manual_seed(seed)\n+        return {\n+            \"prompt\": \"\",\n+            \"negative_prompt\": \"\",\n+            \"generator\": generator,\n+            \"num_inference_steps\": 2,\n+            \"guidance_scale\": 1.0,\n+            \"height\": 32,\n+            \"width\": 32,\n+            \"output_type\": \"pt\",\n+            \"use_resolution_binning\": False,\n+        }\n+\n+    def test_inference(self):\n+        device = \"cpu\"\n+        components = self.get_dummy_components()\n+        pipe = PhotonPipeline(**components)\n+        pipe.to(device)\n+        pipe.set_progress_bar_config(disable=None)\n+        try:\n+            pipe.register_to_config(_execution_device=\"cpu\")\n+        except Exception:\n+            pass\n+\n+        inputs = self.get_dummy_inputs(device)\n+        image = pipe(**inputs)[0]\n+        generated_image = image[0]\n+\n+        self.assertEqual(generated_image.shape, (3, 32, 32))\n+        expected_image = torch.zeros(3, 32, 32)\n+        max_diff = np.abs(generated_image - expected_image).max()\n+        self.assertLessEqual(max_diff, 1e10)\n+\n+    def test_callback_inputs(self):\n+        components = self.get_dummy_components()\n+        pipe = PhotonPipeline(**components)\n+        pipe = pipe.to(\"cpu\")\n+        pipe.set_progress_bar_config(disable=None)\n+        try:\n+            pipe.register_to_config(_execution_device=\"cpu\")\n+        except Exception:\n+            pass\n+        self.assertTrue(\n+            hasattr(pipe, \"_callback_tensor_inputs\"),\n+            f\" {PhotonPipeline} should have `_callback_tensor_inputs` that defines a list of tensor variables its callback function can use as inputs\",\n+        )\n+\n+        def callback_inputs_subset(pipe, i, t, callback_kwargs):\n+            for tensor_name in callback_kwargs.keys():\n+                assert tensor_name in pipe._callback_tensor_inputs\n+            return callback_kwargs\n+\n+        def callback_inputs_all(pipe, i, t, callback_kwargs):\n+            for tensor_name in pipe._callback_tensor_inputs:\n+                assert tensor_name in callback_kwargs\n+            for tensor_name in callback_kwargs.keys():\n+                assert tensor_name in pipe._callback_tensor_inputs\n+            return callback_kwargs\n+\n+        inputs = self.get_dummy_inputs(\"cpu\")\n+\n+        inputs[\"callback_on_step_end\"] = callback_inputs_subset\n+        inputs[\"callback_on_step_end_tensor_inputs\"] = [\"latents\"]\n+        _ = pipe(**inputs)[0]\n+\n+        inputs[\"callback_on_step_end\"] = callback_inputs_all\n+        inputs[\"callback_on_step_end_tensor_inputs\"] = pipe._callback_tensor_inputs\n+        _ = pipe(**inputs)[0]\n+\n+    def test_attention_slicing_forward_pass(self, expected_max_diff=1e-3):\n+        if not self.test_attention_slicing:\n+            return\n+\n+        components = self.get_dummy_components()\n+        pipe = self.pipeline_class(**components)\n+        for component in pipe.components.values():\n+            if hasattr(component, \"set_default_attn_processor\"):\n+                component.set_default_attn_processor()\n+        pipe.to(\"cpu\")\n+        pipe.set_progress_bar_config(disable=None)\n+\n+        def to_np_local(tensor):\n+            if isinstance(tensor, torch.Tensor):\n+                return tensor.detach().cpu().numpy()\n+            return tensor\n+\n+        generator_device = \"cpu\"\n+        inputs = self.get_dummy_inputs(generator_device)\n+        output_without_slicing = pipe(**inputs)[0]\n+\n+        pipe.enable_attention_slicing(slice_size=1)\n+        inputs = self.get_dummy_inputs(generator_device)\n+        output_with_slicing1 = pipe(**inputs)[0]\n+\n+        pipe.enable_attention_slicing(slice_size=2)\n+        inputs = self.get_dummy_inputs(generator_device)\n+        output_with_slicing2 = pipe(**inputs)[0]\n+\n+        max_diff1 = np.abs(to_np_local(output_with_slicing1) - to_np_local(output_without_slicing)).max()\n+        max_diff2 = np.abs(to_np_local(output_with_slicing2) - to_np_local(output_without_slicing)).max()\n+        self.assertLess(max(max_diff1, max_diff2), expected_max_diff)\n+\n+    def test_inference_with_autoencoder_dc(self):\n+        \"\"\"Test PhotonPipeline with AutoencoderDC (DCAE) instead of AutoencoderKL.\"\"\"\n+        device = \"cpu\"\n+\n+        components = self.get_dummy_components()\n+\n+        torch.manual_seed(0)\n+        vae_dc = AutoencoderDC(\n+            in_channels=3,\n+            latent_channels=4,\n+            attention_head_dim=2,\n+            encoder_block_types=(\n+                \"ResBlock\",\n+                \"EfficientViTBlock\",\n+            ),\n+            decoder_block_types=(\n+                \"ResBlock\",\n+                \"EfficientViTBlock\",\n+            ),\n+            encoder_block_out_channels=(8, 8),\n+            decoder_block_out_channels=(8, 8),\n+            encoder_qkv_multiscales=((), (5,)),\n+            decoder_qkv_multiscales=((), (5,)),\n+            encoder_layers_per_block=(1, 1),\n+            decoder_layers_per_block=(1, 1),\n+            upsample_block_type=\"interpolate\",\n+            downsample_block_type=\"stride_conv\",\n+            decoder_norm_types=\"rms_norm\",\n+            decoder_act_fns=\"silu\",\n+        ).eval()\n+\n+        components[\"vae\"] = vae_dc\n+\n+        pipe = PhotonPipeline(**components)\n+        pipe.to(device)\n+        pipe.set_progress_bar_config(disable=None)\n+\n+        expected_scale_factor = vae_dc.spatial_compression_ratio\n+        self.assertEqual(pipe.vae_scale_factor, expected_scale_factor)\n+\n+        inputs = self.get_dummy_inputs(device)\n+        image = pipe(**inputs)[0]\n+        generated_image = image[0]\n+\n+        self.assertEqual(generated_image.shape, (3, 32, 32))\n+        expected_image = torch.zeros(3, 32, 32)\n+        max_diff = np.abs(generated_image - expected_image).max()\n+        self.assertLessEqual(max_diff, 1e10)"
        }
      ],
      "num_files": 16,
      "scraped_at": "2025-11-16T21:19:02.652218"
    },
    {
      "pr_number": 12454,
      "title": "[modular] i2i and t2i support for kontext modular",
      "body": "# What does this PR do?\r\n\r\nSubcedes https://github.com/huggingface/diffusers/pull/12269.\r\n\r\n<details>\r\n<summary>Test code:</summary>\r\n\r\n```py\r\nimport torch \r\nfrom diffusers import ModularPipeline\r\nfrom diffusers.utils import load_image\r\n\r\nrepo_id = \"black-forest-labs/FLUX.1-Kontext-dev\"\r\n\r\npipe = ModularPipeline.from_pretrained(repo_id)\r\npipe.load_components(torch_dtype=torch.bfloat16)\r\npipe = pipe.to(\"cuda\")\r\n\r\nimage = load_image(\r\n    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/yarn-art-pikachu.png\" \r\n).convert(\"RGB\")\r\nprompt = \"Make Pikachu hold a sign that says 'Black Forest Labs is awesome', yarn art style, detailed, vibrant colors\"\r\n\r\noutput = pipe(\r\n    image=image,\r\n    prompt=prompt,\r\n    guidance_scale=2.5,\r\n    num_inference_steps=28,\r\n    max_sequence_length=512,\r\n    generator=torch.manual_seed(0)\r\n)\r\noutput.values[\"images\"][0].save(\"modular_flux_kontext_image.png\")\r\n\r\nprompt = \"A cat and a dog baking a cake together in a kitchen. The cat is carefully measuring flour, while the dog is stirring the batter with a wooden spoon. The kitchen is cozy, with sunlight streaming through the window.\"\r\noutput = pipe(\r\n    prompt=prompt, \r\n    num_inference_steps=28, \r\n    guidance_scale=3.5, \r\n    generator=torch.manual_seed(0),\r\n    max_sequence_length=512,\r\n)\r\noutput.values[\"images\"][0].save(\"modular_flux.png\")\r\n```\r\n\r\n</details>\r\n\r\nResults:\r\n\r\n| T2I | I2I |\r\n|---|---|\r\n| ![alt text](https://github.com/user-attachments/assets/779881f9-678e-4a6b-9dde-fa49d83e9b2d) | ![alt text](https://github.com/user-attachments/assets/302b3183-c789-4ab4-847a-ae22790b931c) |",
      "html_url": "https://github.com/huggingface/diffusers/pull/12454",
      "created_at": "2025-10-09T09:51:30Z",
      "merged_at": "2025-10-10T12:40:17Z",
      "merge_commit_sha": "693d8a3a52252153dc0f1503ea87db89d2364693",
      "base_ref": "main",
      "head_sha": "b1ae489e8bc7f5d814ce593ab82f5307f7ecbbde",
      "user": "sayakpaul",
      "files": [
        {
          "filename": "src/diffusers/__init__.py",
          "status": "modified",
          "additions": 4,
          "deletions": 0,
          "changes": 4,
          "patch": "@@ -386,6 +386,8 @@\n     _import_structure[\"modular_pipelines\"].extend(\n         [\n             \"FluxAutoBlocks\",\n+            \"FluxKontextAutoBlocks\",\n+            \"FluxKontextModularPipeline\",\n             \"FluxModularPipeline\",\n             \"QwenImageAutoBlocks\",\n             \"QwenImageEditAutoBlocks\",\n@@ -1050,6 +1052,8 @@\n     else:\n         from .modular_pipelines import (\n             FluxAutoBlocks,\n+            FluxKontextAutoBlocks,\n+            FluxKontextModularPipeline,\n             FluxModularPipeline,\n             QwenImageAutoBlocks,\n             QwenImageEditAutoBlocks,"
        },
        {
          "filename": "src/diffusers/modular_pipelines/__init__.py",
          "status": "modified",
          "additions": 7,
          "deletions": 2,
          "changes": 9,
          "patch": "@@ -46,7 +46,12 @@\n     ]\n     _import_structure[\"stable_diffusion_xl\"] = [\"StableDiffusionXLAutoBlocks\", \"StableDiffusionXLModularPipeline\"]\n     _import_structure[\"wan\"] = [\"WanAutoBlocks\", \"WanModularPipeline\"]\n-    _import_structure[\"flux\"] = [\"FluxAutoBlocks\", \"FluxModularPipeline\"]\n+    _import_structure[\"flux\"] = [\n+        \"FluxAutoBlocks\",\n+        \"FluxModularPipeline\",\n+        \"FluxKontextAutoBlocks\",\n+        \"FluxKontextModularPipeline\",\n+    ]\n     _import_structure[\"qwenimage\"] = [\n         \"QwenImageAutoBlocks\",\n         \"QwenImageModularPipeline\",\n@@ -65,7 +70,7 @@\n         from ..utils.dummy_pt_objects import *  # noqa F403\n     else:\n         from .components_manager import ComponentsManager\n-        from .flux import FluxAutoBlocks, FluxModularPipeline\n+        from .flux import FluxAutoBlocks, FluxKontextAutoBlocks, FluxKontextModularPipeline, FluxModularPipeline\n         from .modular_pipeline import (\n             AutoPipelineBlocks,\n             BlockState,"
        },
        {
          "filename": "src/diffusers/modular_pipelines/flux/__init__.py",
          "status": "modified",
          "additions": 12,
          "deletions": 3,
          "changes": 15,
          "patch": "@@ -25,14 +25,18 @@\n     _import_structure[\"modular_blocks\"] = [\n         \"ALL_BLOCKS\",\n         \"AUTO_BLOCKS\",\n+        \"AUTO_BLOCKS_KONTEXT\",\n+        \"FLUX_KONTEXT_BLOCKS\",\n         \"TEXT2IMAGE_BLOCKS\",\n         \"FluxAutoBeforeDenoiseStep\",\n         \"FluxAutoBlocks\",\n-        \"FluxAutoBlocks\",\n         \"FluxAutoDecodeStep\",\n         \"FluxAutoDenoiseStep\",\n+        \"FluxKontextAutoBlocks\",\n+        \"FluxKontextAutoDenoiseStep\",\n+        \"FluxKontextBeforeDenoiseStep\",\n     ]\n-    _import_structure[\"modular_pipeline\"] = [\"FluxModularPipeline\"]\n+    _import_structure[\"modular_pipeline\"] = [\"FluxKontextModularPipeline\", \"FluxModularPipeline\"]\n \n if TYPE_CHECKING or DIFFUSERS_SLOW_IMPORT:\n     try:\n@@ -45,13 +49,18 @@\n         from .modular_blocks import (\n             ALL_BLOCKS,\n             AUTO_BLOCKS,\n+            AUTO_BLOCKS_KONTEXT,\n+            FLUX_KONTEXT_BLOCKS,\n             TEXT2IMAGE_BLOCKS,\n             FluxAutoBeforeDenoiseStep,\n             FluxAutoBlocks,\n             FluxAutoDecodeStep,\n             FluxAutoDenoiseStep,\n+            FluxKontextAutoBlocks,\n+            FluxKontextAutoDenoiseStep,\n+            FluxKontextBeforeDenoiseStep,\n         )\n-        from .modular_pipeline import FluxModularPipeline\n+        from .modular_pipeline import FluxKontextModularPipeline, FluxModularPipeline\n else:\n     import sys\n "
        },
        {
          "filename": "src/diffusers/modular_pipelines/flux/before_denoise.py",
          "status": "modified",
          "additions": 72,
          "deletions": 12,
          "changes": 84,
          "patch": "@@ -118,15 +118,6 @@ def retrieve_latents(\n         raise AttributeError(\"Could not access latents of provided encoder_output\")\n \n \n-# TODO: align this with Qwen patchifier\n-def _pack_latents(latents, batch_size, num_channels_latents, height, width):\n-    latents = latents.view(batch_size, num_channels_latents, height // 2, 2, width // 2, 2)\n-    latents = latents.permute(0, 2, 4, 1, 3, 5)\n-    latents = latents.reshape(batch_size, (height // 2) * (width // 2), num_channels_latents * 4)\n-\n-    return latents\n-\n-\n def _get_initial_timesteps_and_optionals(\n     transformer,\n     scheduler,\n@@ -398,16 +389,15 @@ def prepare_latents(\n                 f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n             )\n \n-        # TODO: move packing latents code to a patchifier\n+        # TODO: move packing latents code to a patchifier similar to Qwen\n         latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n-        latents = _pack_latents(latents, batch_size, num_channels_latents, height, width)\n+        latents = FluxPipeline._pack_latents(latents, batch_size, num_channels_latents, height, width)\n \n         return latents\n \n     @torch.no_grad()\n     def __call__(self, components: FluxModularPipeline, state: PipelineState) -> PipelineState:\n         block_state = self.get_block_state(state)\n-\n         block_state.height = block_state.height or components.default_height\n         block_state.width = block_state.width or components.default_width\n         block_state.device = components._execution_device\n@@ -557,3 +547,73 @@ def __call__(self, components: FluxModularPipeline, state: PipelineState) -> Pip\n         self.set_block_state(state, block_state)\n \n         return components, state\n+\n+\n+class FluxKontextRoPEInputsStep(ModularPipelineBlocks):\n+    model_name = \"flux-kontext\"\n+\n+    @property\n+    def description(self) -> str:\n+        return \"Step that prepares the RoPE inputs for the denoising process of Flux Kontext. Should be placed after text encoder and latent preparation steps.\"\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        return [\n+            InputParam(name=\"image_height\"),\n+            InputParam(name=\"image_width\"),\n+            InputParam(name=\"height\"),\n+            InputParam(name=\"width\"),\n+            InputParam(name=\"prompt_embeds\"),\n+        ]\n+\n+    @property\n+    def intermediate_outputs(self) -> List[OutputParam]:\n+        return [\n+            OutputParam(\n+                name=\"txt_ids\",\n+                kwargs_type=\"denoiser_input_fields\",\n+                type_hint=List[int],\n+                description=\"The sequence lengths of the prompt embeds, used for RoPE calculation.\",\n+            ),\n+            OutputParam(\n+                name=\"img_ids\",\n+                kwargs_type=\"denoiser_input_fields\",\n+                type_hint=List[int],\n+                description=\"The sequence lengths of the image latents, used for RoPE calculation.\",\n+            ),\n+        ]\n+\n+    def __call__(self, components: FluxModularPipeline, state: PipelineState) -> PipelineState:\n+        block_state = self.get_block_state(state)\n+\n+        prompt_embeds = block_state.prompt_embeds\n+        device, dtype = prompt_embeds.device, prompt_embeds.dtype\n+        block_state.txt_ids = torch.zeros(prompt_embeds.shape[1], 3).to(\n+            device=prompt_embeds.device, dtype=prompt_embeds.dtype\n+        )\n+\n+        img_ids = None\n+        if (\n+            getattr(block_state, \"image_height\", None) is not None\n+            and getattr(block_state, \"image_width\", None) is not None\n+        ):\n+            image_latent_height = 2 * (int(block_state.image_height) // (components.vae_scale_factor * 2))\n+            image_latent_width = 2 * (int(block_state.width) // (components.vae_scale_factor * 2))\n+            img_ids = FluxPipeline._prepare_latent_image_ids(\n+                None, image_latent_height // 2, image_latent_width // 2, device, dtype\n+            )\n+            # image ids are the same as latent ids with the first dimension set to 1 instead of 0\n+            img_ids[..., 0] = 1\n+\n+        height = 2 * (int(block_state.height) // (components.vae_scale_factor * 2))\n+        width = 2 * (int(block_state.width) // (components.vae_scale_factor * 2))\n+        latent_ids = FluxPipeline._prepare_latent_image_ids(None, height // 2, width // 2, device, dtype)\n+\n+        if img_ids is not None:\n+            latent_ids = torch.cat([latent_ids, img_ids], dim=0)\n+\n+        block_state.img_ids = latent_ids\n+\n+        self.set_block_state(state, block_state)\n+\n+        return components, state"
        },
        {
          "filename": "src/diffusers/modular_pipelines/flux/denoise.py",
          "status": "modified",
          "additions": 107,
          "deletions": 0,
          "changes": 107,
          "patch": "@@ -109,6 +109,96 @@ def __call__(\n         return components, block_state\n \n \n+class FluxKontextLoopDenoiser(ModularPipelineBlocks):\n+    model_name = \"flux-kontext\"\n+\n+    @property\n+    def expected_components(self) -> List[ComponentSpec]:\n+        return [ComponentSpec(\"transformer\", FluxTransformer2DModel)]\n+\n+    @property\n+    def description(self) -> str:\n+        return (\n+            \"Step within the denoising loop that denoise the latents for Flux Kontext. \"\n+            \"This block should be used to compose the `sub_blocks` attribute of a `LoopSequentialPipelineBlocks` \"\n+            \"object (e.g. `FluxDenoiseLoopWrapper`)\"\n+        )\n+\n+    @property\n+    def inputs(self) -> List[Tuple[str, Any]]:\n+        return [\n+            InputParam(\"joint_attention_kwargs\"),\n+            InputParam(\n+                \"latents\",\n+                required=True,\n+                type_hint=torch.Tensor,\n+                description=\"The initial latents to use for the denoising process. Can be generated in prepare_latent step.\",\n+            ),\n+            InputParam(\n+                \"image_latents\",\n+                type_hint=torch.Tensor,\n+                description=\"Image latents to use for the denoising process. Can be generated in prepare_latent step.\",\n+            ),\n+            InputParam(\n+                \"guidance\",\n+                required=True,\n+                type_hint=torch.Tensor,\n+                description=\"Guidance scale as a tensor\",\n+            ),\n+            InputParam(\n+                \"prompt_embeds\",\n+                required=True,\n+                type_hint=torch.Tensor,\n+                description=\"Prompt embeddings\",\n+            ),\n+            InputParam(\n+                \"pooled_prompt_embeds\",\n+                required=True,\n+                type_hint=torch.Tensor,\n+                description=\"Pooled prompt embeddings\",\n+            ),\n+            InputParam(\n+                \"txt_ids\",\n+                required=True,\n+                type_hint=torch.Tensor,\n+                description=\"IDs computed from text sequence needed for RoPE\",\n+            ),\n+            InputParam(\n+                \"img_ids\",\n+                required=True,\n+                type_hint=torch.Tensor,\n+                description=\"IDs computed from latent sequence needed for RoPE\",\n+            ),\n+        ]\n+\n+    @torch.no_grad()\n+    def __call__(\n+        self, components: FluxModularPipeline, block_state: BlockState, i: int, t: torch.Tensor\n+    ) -> PipelineState:\n+        latents = block_state.latents\n+        latent_model_input = latents\n+        image_latents = block_state.image_latents\n+        if image_latents is not None:\n+            latent_model_input = torch.cat([latent_model_input, image_latents], dim=1)\n+\n+        timestep = t.expand(latents.shape[0]).to(latents.dtype)\n+        noise_pred = components.transformer(\n+            hidden_states=latent_model_input,\n+            timestep=timestep / 1000,\n+            guidance=block_state.guidance,\n+            encoder_hidden_states=block_state.prompt_embeds,\n+            pooled_projections=block_state.pooled_prompt_embeds,\n+            joint_attention_kwargs=block_state.joint_attention_kwargs,\n+            txt_ids=block_state.txt_ids,\n+            img_ids=block_state.img_ids,\n+            return_dict=False,\n+        )[0]\n+        noise_pred = noise_pred[:, : latents.size(1)]\n+        block_state.noise_pred = noise_pred\n+\n+        return components, block_state\n+\n+\n class FluxLoopAfterDenoiser(ModularPipelineBlocks):\n     model_name = \"flux\"\n \n@@ -221,3 +311,20 @@ def description(self) -> str:\n             \" - `FluxLoopAfterDenoiser`\\n\"\n             \"This block supports both text2image and img2img tasks.\"\n         )\n+\n+\n+class FluxKontextDenoiseStep(FluxDenoiseLoopWrapper):\n+    model_name = \"flux-kontext\"\n+    block_classes = [FluxKontextLoopDenoiser, FluxLoopAfterDenoiser]\n+    block_names = [\"denoiser\", \"after_denoiser\"]\n+\n+    @property\n+    def description(self) -> str:\n+        return (\n+            \"Denoise step that iteratively denoise the latents. \\n\"\n+            \"Its loop logic is defined in `FluxDenoiseLoopWrapper.__call__` method \\n\"\n+            \"At each iteration, it runs blocks defined in `sub_blocks` sequentially:\\n\"\n+            \" - `FluxKontextLoopDenoiser`\\n\"\n+            \" - `FluxLoopAfterDenoiser`\\n\"\n+            \"This block supports both text2image and img2img tasks.\"\n+        )"
        },
        {
          "filename": "src/diffusers/modular_pipelines/flux/encoders.py",
          "status": "modified",
          "additions": 89,
          "deletions": 18,
          "changes": 107,
          "patch": "@@ -20,7 +20,7 @@\n from transformers import CLIPTextModel, CLIPTokenizer, T5EncoderModel, T5TokenizerFast\n \n from ...configuration_utils import FrozenDict\n-from ...image_processor import VaeImageProcessor\n+from ...image_processor import VaeImageProcessor, is_valid_image, is_valid_image_imagelist\n from ...loaders import FluxLoraLoaderMixin, TextualInversionLoaderMixin\n from ...models import AutoencoderKL\n from ...utils import USE_PEFT_BACKEND, is_ftfy_available, logging, scale_lora_layers, unscale_lora_layers\n@@ -83,11 +83,11 @@ def encode_vae_image(vae: AutoencoderKL, image: torch.Tensor, generator: torch.G\n \n \n class FluxProcessImagesInputStep(ModularPipelineBlocks):\n-    model_name = \"Flux\"\n+    model_name = \"flux\"\n \n     @property\n     def description(self) -> str:\n-        return \"Image Preprocess step. Resizing is needed in Flux Kontext (will be implemented later.)\"\n+        return \"Image Preprocess step.\"\n \n     @property\n     def expected_components(self) -> List[ComponentSpec]:\n@@ -106,9 +106,7 @@ def inputs(self) -> List[InputParam]:\n \n     @property\n     def intermediate_outputs(self) -> List[OutputParam]:\n-        return [\n-            OutputParam(name=\"processed_image\"),\n-        ]\n+        return [OutputParam(name=\"processed_image\")]\n \n     @staticmethod\n     def check_inputs(height, width, vae_scale_factor):\n@@ -142,13 +140,80 @@ def __call__(self, components: FluxModularPipeline, state: PipelineState):\n         return components, state\n \n \n+class FluxKontextProcessImagesInputStep(ModularPipelineBlocks):\n+    model_name = \"flux-kontext\"\n+\n+    def __init__(self, _auto_resize=True):\n+        self._auto_resize = _auto_resize\n+        super().__init__()\n+\n+    @property\n+    def description(self) -> str:\n+        return (\n+            \"Image preprocess step for Flux Kontext. The preprocessed image goes to the VAE.\\n\"\n+            \"Kontext works as a T2I model, too, in case no input image is provided.\"\n+        )\n+\n+    @property\n+    def expected_components(self) -> List[ComponentSpec]:\n+        return [\n+            ComponentSpec(\n+                \"image_processor\",\n+                VaeImageProcessor,\n+                config=FrozenDict({\"vae_scale_factor\": 16}),\n+                default_creation_method=\"from_config\",\n+            ),\n+        ]\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        return [InputParam(\"image\")]\n+\n+    @property\n+    def intermediate_outputs(self) -> List[OutputParam]:\n+        return [OutputParam(name=\"processed_image\")]\n+\n+    @torch.no_grad()\n+    def __call__(self, components: FluxModularPipeline, state: PipelineState):\n+        from ...pipelines.flux.pipeline_flux_kontext import PREFERRED_KONTEXT_RESOLUTIONS\n+\n+        block_state = self.get_block_state(state)\n+        images = block_state.image\n+\n+        if images is None:\n+            block_state.processed_image = None\n+\n+        else:\n+            multiple_of = components.image_processor.config.vae_scale_factor\n+\n+            if not is_valid_image_imagelist(images):\n+                raise ValueError(f\"Images must be image or list of images but are {type(images)}\")\n+\n+            if is_valid_image(images):\n+                images = [images]\n+\n+            img = images[0]\n+            image_height, image_width = components.image_processor.get_default_height_width(img)\n+            aspect_ratio = image_width / image_height\n+            if self._auto_resize:\n+                # Kontext is trained on specific resolutions, using one of them is recommended\n+                _, image_width, image_height = min(\n+                    (abs(aspect_ratio - w / h), w, h) for w, h in PREFERRED_KONTEXT_RESOLUTIONS\n+                )\n+            image_width = image_width // multiple_of * multiple_of\n+            image_height = image_height // multiple_of * multiple_of\n+            images = components.image_processor.resize(images, image_height, image_width)\n+            block_state.processed_image = components.image_processor.preprocess(images, image_height, image_width)\n+\n+        self.set_block_state(state, block_state)\n+        return components, state\n+\n+\n class FluxVaeEncoderDynamicStep(ModularPipelineBlocks):\n     model_name = \"flux\"\n \n     def __init__(\n-        self,\n-        input_name: str = \"processed_image\",\n-        output_name: str = \"image_latents\",\n+        self, input_name: str = \"processed_image\", output_name: str = \"image_latents\", sample_mode: str = \"sample\"\n     ):\n         \"\"\"Initialize a VAE encoder step for converting images to latent representations.\n \n@@ -160,6 +225,7 @@ def __init__(\n                 Examples: \"processed_image\" or \"processed_control_image\"\n             output_name (str, optional): Name of the output latent tensor. Defaults to \"image_latents\".\n                 Examples: \"image_latents\" or \"control_image_latents\"\n+            sample_mode (str, optional): Sampling mode to be used.\n \n         Examples:\n             # Basic usage with default settings (includes image processor): # FluxImageVaeEncoderDynamicStep()\n@@ -170,6 +236,7 @@ def __init__(\n         \"\"\"\n         self._image_input_name = input_name\n         self._image_latents_output_name = output_name\n+        self.sample_mode = sample_mode\n         super().__init__()\n \n     @property\n@@ -183,7 +250,7 @@ def expected_components(self) -> List[ComponentSpec]:\n \n     @property\n     def inputs(self) -> List[InputParam]:\n-        inputs = [InputParam(self._image_input_name, required=True), InputParam(\"generator\")]\n+        inputs = [InputParam(self._image_input_name), InputParam(\"generator\")]\n         return inputs\n \n     @property\n@@ -199,16 +266,20 @@ def intermediate_outputs(self) -> List[OutputParam]:\n     @torch.no_grad()\n     def __call__(self, components: FluxModularPipeline, state: PipelineState) -> PipelineState:\n         block_state = self.get_block_state(state)\n-\n-        device = components._execution_device\n-        dtype = components.vae.dtype\n-\n         image = getattr(block_state, self._image_input_name)\n-        image = image.to(device=device, dtype=dtype)\n \n-        # Encode image into latents\n-        image_latents = encode_vae_image(image=image, vae=components.vae, generator=block_state.generator)\n-        setattr(block_state, self._image_latents_output_name, image_latents)\n+        if image is None:\n+            setattr(block_state, self._image_latents_output_name, None)\n+        else:\n+            device = components._execution_device\n+            dtype = components.vae.dtype\n+            image = image.to(device=device, dtype=dtype)\n+\n+            # Encode image into latents\n+            image_latents = encode_vae_image(\n+                image=image, vae=components.vae, generator=block_state.generator, sample_mode=self.sample_mode\n+            )\n+            setattr(block_state, self._image_latents_output_name, image_latents)\n \n         self.set_block_state(state, block_state)\n "
        },
        {
          "filename": "src/diffusers/modular_pipelines/flux/inputs.py",
          "status": "modified",
          "additions": 123,
          "deletions": 0,
          "changes": 123,
          "patch": "@@ -17,6 +17,7 @@\n import torch\n \n from ...pipelines import FluxPipeline\n+from ...utils import logging\n from ..modular_pipeline import ModularPipelineBlocks, PipelineState\n from ..modular_pipeline_utils import InputParam, OutputParam\n \n@@ -25,6 +26,9 @@\n from .modular_pipeline import FluxModularPipeline\n \n \n+logger = logging.get_logger(__name__)\n+\n+\n class FluxTextInputStep(ModularPipelineBlocks):\n     model_name = \"flux\"\n \n@@ -234,3 +238,122 @@ def __call__(self, components: FluxModularPipeline, state: PipelineState) -> Pip\n \n         self.set_block_state(state, block_state)\n         return components, state\n+\n+\n+class FluxKontextInputsDynamicStep(FluxInputsDynamicStep):\n+    model_name = \"flux-kontext\"\n+\n+    def __call__(self, components: FluxModularPipeline, state: PipelineState) -> PipelineState:\n+        block_state = self.get_block_state(state)\n+\n+        # Process image latent inputs (height/width calculation, patchify, and batch expansion)\n+        for image_latent_input_name in self._image_latent_inputs:\n+            image_latent_tensor = getattr(block_state, image_latent_input_name)\n+            if image_latent_tensor is None:\n+                continue\n+\n+            # 1. Calculate height/width from latents\n+            # Unlike the `FluxInputsDynamicStep`, we don't overwrite the `block.height` and `block.width`\n+            height, width = calculate_dimension_from_latents(image_latent_tensor, components.vae_scale_factor)\n+            if not hasattr(block_state, \"image_height\"):\n+                block_state.image_height = height\n+            if not hasattr(block_state, \"image_width\"):\n+                block_state.image_width = width\n+\n+            # 2. Patchify the image latent tensor\n+            # TODO: Implement patchifier for Flux.\n+            latent_height, latent_width = image_latent_tensor.shape[2:]\n+            image_latent_tensor = FluxPipeline._pack_latents(\n+                image_latent_tensor, block_state.batch_size, image_latent_tensor.shape[1], latent_height, latent_width\n+            )\n+\n+            # 3. Expand batch size\n+            image_latent_tensor = repeat_tensor_to_batch_size(\n+                input_name=image_latent_input_name,\n+                input_tensor=image_latent_tensor,\n+                num_images_per_prompt=block_state.num_images_per_prompt,\n+                batch_size=block_state.batch_size,\n+            )\n+\n+            setattr(block_state, image_latent_input_name, image_latent_tensor)\n+\n+        # Process additional batch inputs (only batch expansion)\n+        for input_name in self._additional_batch_inputs:\n+            input_tensor = getattr(block_state, input_name)\n+            if input_tensor is None:\n+                continue\n+\n+            # Only expand batch size\n+            input_tensor = repeat_tensor_to_batch_size(\n+                input_name=input_name,\n+                input_tensor=input_tensor,\n+                num_images_per_prompt=block_state.num_images_per_prompt,\n+                batch_size=block_state.batch_size,\n+            )\n+\n+            setattr(block_state, input_name, input_tensor)\n+\n+        self.set_block_state(state, block_state)\n+        return components, state\n+\n+\n+class FluxKontextSetResolutionStep(ModularPipelineBlocks):\n+    model_name = \"flux-kontext\"\n+\n+    def description(self):\n+        return (\n+            \"Determines the height and width to be used during the subsequent computations.\\n\"\n+            \"It should always be placed _before_ the latent preparation step.\"\n+        )\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        inputs = [\n+            InputParam(name=\"height\"),\n+            InputParam(name=\"width\"),\n+            InputParam(name=\"max_area\", type_hint=int, default=1024**2),\n+        ]\n+        return inputs\n+\n+    @property\n+    def intermediate_outputs(self) -> List[OutputParam]:\n+        return [\n+            OutputParam(name=\"height\", type_hint=int, description=\"The height of the initial noisy latents\"),\n+            OutputParam(name=\"width\", type_hint=int, description=\"The width of the initial noisy latents\"),\n+        ]\n+\n+    @staticmethod\n+    def check_inputs(height, width, vae_scale_factor):\n+        if height is not None and height % (vae_scale_factor * 2) != 0:\n+            raise ValueError(f\"Height must be divisible by {vae_scale_factor * 2} but is {height}\")\n+\n+        if width is not None and width % (vae_scale_factor * 2) != 0:\n+            raise ValueError(f\"Width must be divisible by {vae_scale_factor * 2} but is {width}\")\n+\n+    def __call__(self, components: FluxModularPipeline, state: PipelineState) -> PipelineState:\n+        block_state = self.get_block_state(state)\n+\n+        height = block_state.height or components.default_height\n+        width = block_state.width or components.default_width\n+        self.check_inputs(height, width, components.vae_scale_factor)\n+\n+        original_height, original_width = height, width\n+        max_area = block_state.max_area\n+        aspect_ratio = width / height\n+        width = round((max_area * aspect_ratio) ** 0.5)\n+        height = round((max_area / aspect_ratio) ** 0.5)\n+\n+        multiple_of = components.vae_scale_factor * 2\n+        width = width // multiple_of * multiple_of\n+        height = height // multiple_of * multiple_of\n+\n+        if height != original_height or width != original_width:\n+            logger.warning(\n+                f\"Generation `height` and `width` have been adjusted to {height} and {width} to fit the model requirements.\"\n+            )\n+\n+        block_state.height = height\n+        block_state.width = width\n+\n+        self.set_block_state(state, block_state)\n+        return components, state"
        },
        {
          "filename": "src/diffusers/modular_pipelines/flux/modular_blocks.py",
          "status": "modified",
          "additions": 204,
          "deletions": 11,
          "changes": 215,
          "patch": "@@ -18,25 +18,33 @@\n from .before_denoise import (\n     FluxImg2ImgPrepareLatentsStep,\n     FluxImg2ImgSetTimestepsStep,\n+    FluxKontextRoPEInputsStep,\n     FluxPrepareLatentsStep,\n     FluxRoPEInputsStep,\n     FluxSetTimestepsStep,\n )\n from .decoders import FluxDecodeStep\n-from .denoise import FluxDenoiseStep\n-from .encoders import FluxProcessImagesInputStep, FluxTextEncoderStep, FluxVaeEncoderDynamicStep\n-from .inputs import FluxInputsDynamicStep, FluxTextInputStep\n+from .denoise import FluxDenoiseStep, FluxKontextDenoiseStep\n+from .encoders import (\n+    FluxKontextProcessImagesInputStep,\n+    FluxProcessImagesInputStep,\n+    FluxTextEncoderStep,\n+    FluxVaeEncoderDynamicStep,\n+)\n+from .inputs import (\n+    FluxInputsDynamicStep,\n+    FluxKontextInputsDynamicStep,\n+    FluxKontextSetResolutionStep,\n+    FluxTextInputStep,\n+)\n \n \n logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n \n \n # vae encoder (run before before_denoise)\n FluxImg2ImgVaeEncoderBlocks = InsertableDict(\n-    [\n-        (\"preprocess\", FluxProcessImagesInputStep()),\n-        (\"encode\", FluxVaeEncoderDynamicStep()),\n-    ]\n+    [(\"preprocess\", FluxProcessImagesInputStep()), (\"encode\", FluxVaeEncoderDynamicStep())]\n )\n \n \n@@ -66,6 +74,39 @@ def description(self):\n         )\n \n \n+# Flux Kontext vae encoder (run before before_denoise)\n+\n+FluxKontextVaeEncoderBlocks = InsertableDict(\n+    [(\"preprocess\", FluxKontextProcessImagesInputStep()), (\"encode\", FluxVaeEncoderDynamicStep(sample_mode=\"argmax\"))]\n+)\n+\n+\n+class FluxKontextVaeEncoderStep(SequentialPipelineBlocks):\n+    model_name = \"flux-kontext\"\n+\n+    block_classes = FluxKontextVaeEncoderBlocks.values()\n+    block_names = FluxKontextVaeEncoderBlocks.keys()\n+\n+    @property\n+    def description(self) -> str:\n+        return \"Vae encoder step that preprocess andencode the image inputs into their latent representations.\"\n+\n+\n+class FluxKontextAutoVaeEncoderStep(AutoPipelineBlocks):\n+    block_classes = [FluxKontextVaeEncoderStep]\n+    block_names = [\"img2img\"]\n+    block_trigger_inputs = [\"image\"]\n+\n+    @property\n+    def description(self):\n+        return (\n+            \"Vae encoder step that encode the image inputs into their latent representations.\\n\"\n+            + \"This is an auto pipeline block that works for img2img tasks.\\n\"\n+            + \" - `FluxKontextVaeEncoderStep` (img2img) is used when only `image` is provided.\"\n+            + \" - if `image` is not provided, step will be skipped.\"\n+        )\n+\n+\n # before_denoise: text2img\n FluxBeforeDenoiseBlocks = InsertableDict(\n     [\n@@ -107,6 +148,7 @@ def description(self):\n \n # before_denoise: all task (text2img, img2img)\n class FluxAutoBeforeDenoiseStep(AutoPipelineBlocks):\n+    model_name = \"flux-kontext\"\n     block_classes = [FluxImg2ImgBeforeDenoiseStep, FluxBeforeDenoiseStep]\n     block_names = [\"img2img\", \"text2image\"]\n     block_trigger_inputs = [\"image_latents\", None]\n@@ -121,6 +163,44 @@ def description(self):\n         )\n \n \n+# before_denoise: FluxKontext\n+\n+FluxKontextBeforeDenoiseBlocks = InsertableDict(\n+    [\n+        (\"prepare_latents\", FluxPrepareLatentsStep()),\n+        (\"set_timesteps\", FluxSetTimestepsStep()),\n+        (\"prepare_rope_inputs\", FluxKontextRoPEInputsStep()),\n+    ]\n+)\n+\n+\n+class FluxKontextBeforeDenoiseStep(SequentialPipelineBlocks):\n+    block_classes = FluxKontextBeforeDenoiseBlocks.values()\n+    block_names = FluxKontextBeforeDenoiseBlocks.keys()\n+\n+    @property\n+    def description(self):\n+        return (\n+            \"Before denoise step that prepare the inputs for the denoise step\\n\"\n+            \"for img2img/text2img task for Flux Kontext.\"\n+        )\n+\n+\n+class FluxKontextAutoBeforeDenoiseStep(AutoPipelineBlocks):\n+    block_classes = [FluxKontextBeforeDenoiseStep, FluxBeforeDenoiseStep]\n+    block_names = [\"img2img\", \"text2image\"]\n+    block_trigger_inputs = [\"image_latents\", None]\n+\n+    @property\n+    def description(self):\n+        return (\n+            \"Before denoise step that prepare the inputs for the denoise step.\\n\"\n+            + \"This is an auto pipeline block that works for text2image.\\n\"\n+            + \" - `FluxBeforeDenoiseStep` (text2image) is used.\\n\"\n+            + \" - `FluxKontextBeforeDenoiseStep` (img2img) is used when only `image_latents` is provided.\\n\"\n+        )\n+\n+\n # denoise: text2image\n class FluxAutoDenoiseStep(AutoPipelineBlocks):\n     block_classes = [FluxDenoiseStep]\n@@ -136,6 +216,23 @@ def description(self) -> str:\n         )\n \n \n+# denoise: Flux Kontext\n+\n+\n+class FluxKontextAutoDenoiseStep(AutoPipelineBlocks):\n+    block_classes = [FluxKontextDenoiseStep]\n+    block_names = [\"denoise\"]\n+    block_trigger_inputs = [None]\n+\n+    @property\n+    def description(self) -> str:\n+        return (\n+            \"Denoise step that iteratively denoise the latents for Flux Kontext. \"\n+            \"This is a auto pipeline block that works for text2image and img2img tasks.\"\n+            \" - `FluxDenoiseStep` (denoise) for text2image and img2img tasks.\"\n+        )\n+\n+\n # decode: all task (text2img, img2img)\n class FluxAutoDecodeStep(AutoPipelineBlocks):\n     block_classes = [FluxDecodeStep]\n@@ -165,7 +262,7 @@ def description(self):\n         \" - update height/width based `image_latents`, patchify `image_latents`.\"\n \n \n-class FluxImageAutoInputStep(AutoPipelineBlocks):\n+class FluxAutoInputStep(AutoPipelineBlocks):\n     block_classes = [FluxImg2ImgInputStep, FluxTextInputStep]\n     block_names = [\"img2img\", \"text2image\"]\n     block_trigger_inputs = [\"image_latents\", None]\n@@ -180,16 +277,59 @@ def description(self):\n         )\n \n \n+# inputs: Flux Kontext\n+\n+FluxKontextBlocks = InsertableDict(\n+    [\n+        (\"set_resolution\", FluxKontextSetResolutionStep()),\n+        (\"text_inputs\", FluxTextInputStep()),\n+        (\"additional_inputs\", FluxKontextInputsDynamicStep()),\n+    ]\n+)\n+\n+\n+class FluxKontextInputStep(SequentialPipelineBlocks):\n+    model_name = \"flux-kontext\"\n+    block_classes = FluxKontextBlocks.values()\n+    block_names = FluxKontextBlocks.keys()\n+\n+    @property\n+    def description(self):\n+        return (\n+            \"Input step that prepares the inputs for the both text2img and img2img denoising step. It:\\n\"\n+            \" - make sure the text embeddings have consistent batch size as well as the additional inputs (`image_latents`).\\n\"\n+            \" - update height/width based `image_latents`, patchify `image_latents`.\"\n+        )\n+\n+\n+class FluxKontextAutoInputStep(AutoPipelineBlocks):\n+    block_classes = [FluxKontextInputStep, FluxTextInputStep]\n+    # block_classes = [FluxKontextInputStep]\n+    block_names = [\"img2img\", \"text2img\"]\n+    # block_names = [\"img2img\"]\n+    block_trigger_inputs = [\"image_latents\", None]\n+    # block_trigger_inputs = [\"image_latents\"]\n+\n+    @property\n+    def description(self):\n+        return (\n+            \"Input step that standardize the inputs for the denoising step, e.g. make sure inputs have consistent batch size, and patchified. \\n\"\n+            \" This is an auto pipeline block that works for text2image/img2img tasks.\\n\"\n+            + \" - `FluxKontextInputStep` (img2img) is used when `image_latents` is provided.\\n\"\n+            + \" - `FluxKontextInputStep` is also capable of handling text2image task when `image_latent` isn't present.\"\n+        )\n+\n+\n class FluxCoreDenoiseStep(SequentialPipelineBlocks):\n     model_name = \"flux\"\n-    block_classes = [FluxImageAutoInputStep, FluxAutoBeforeDenoiseStep, FluxAutoDenoiseStep]\n+    block_classes = [FluxAutoInputStep, FluxAutoBeforeDenoiseStep, FluxAutoDenoiseStep]\n     block_names = [\"input\", \"before_denoise\", \"denoise\"]\n \n     @property\n     def description(self):\n         return (\n             \"Core step that performs the denoising process. \\n\"\n-            + \" - `FluxImageAutoInputStep` (input) standardizes the inputs for the denoising step.\\n\"\n+            + \" - `FluxAutoInputStep` (input) standardizes the inputs for the denoising step.\\n\"\n             + \" - `FluxAutoBeforeDenoiseStep` (before_denoise) prepares the inputs for the denoising step.\\n\"\n             + \" - `FluxAutoDenoiseStep` (denoise) iteratively denoises the latents.\\n\"\n             + \"This step supports text-to-image and image-to-image tasks for Flux:\\n\"\n@@ -198,6 +338,24 @@ def description(self):\n         )\n \n \n+class FluxKontextCoreDenoiseStep(SequentialPipelineBlocks):\n+    model_name = \"flux-kontext\"\n+    block_classes = [FluxKontextAutoInputStep, FluxKontextAutoBeforeDenoiseStep, FluxKontextAutoDenoiseStep]\n+    block_names = [\"input\", \"before_denoise\", \"denoise\"]\n+\n+    @property\n+    def description(self):\n+        return (\n+            \"Core step that performs the denoising process. \\n\"\n+            + \" - `FluxKontextAutoInputStep` (input) standardizes the inputs for the denoising step.\\n\"\n+            + \" - `FluxKontextAutoBeforeDenoiseStep` (before_denoise) prepares the inputs for the denoising step.\\n\"\n+            + \" - `FluxKontextAutoDenoiseStep` (denoise) iteratively denoises the latents.\\n\"\n+            + \"This step supports text-to-image and image-to-image tasks for Flux:\\n\"\n+            + \" - for image-to-image generation, you need to provide `image_latents`\\n\"\n+            + \" - for text-to-image generation, all you need to provide is prompt embeddings.\"\n+        )\n+\n+\n # Auto blocks (text2image and img2img)\n AUTO_BLOCKS = InsertableDict(\n     [\n@@ -208,6 +366,15 @@ def description(self):\n     ]\n )\n \n+AUTO_BLOCKS_KONTEXT = InsertableDict(\n+    [\n+        (\"text_encoder\", FluxTextEncoderStep()),\n+        (\"image_encoder\", FluxKontextAutoVaeEncoderStep()),\n+        (\"denoise\", FluxKontextCoreDenoiseStep()),\n+        (\"decode\", FluxDecodeStep()),\n+    ]\n+)\n+\n \n class FluxAutoBlocks(SequentialPipelineBlocks):\n     model_name = \"flux\"\n@@ -224,6 +391,13 @@ def description(self):\n         )\n \n \n+class FluxKontextAutoBlocks(FluxAutoBlocks):\n+    model_name = \"flux-kontext\"\n+\n+    block_classes = AUTO_BLOCKS_KONTEXT.values()\n+    block_names = AUTO_BLOCKS_KONTEXT.keys()\n+\n+\n TEXT2IMAGE_BLOCKS = InsertableDict(\n     [\n         (\"text_encoder\", FluxTextEncoderStep()),\n@@ -250,4 +424,23 @@ def description(self):\n     ]\n )\n \n-ALL_BLOCKS = {\"text2image\": TEXT2IMAGE_BLOCKS, \"img2img\": IMAGE2IMAGE_BLOCKS, \"auto\": AUTO_BLOCKS}\n+FLUX_KONTEXT_BLOCKS = InsertableDict(\n+    [\n+        (\"text_encoder\", FluxTextEncoderStep()),\n+        (\"vae_encoder\", FluxVaeEncoderDynamicStep(sample_mode=\"argmax\")),\n+        (\"input\", FluxKontextInputStep()),\n+        (\"prepare_latents\", FluxPrepareLatentsStep()),\n+        (\"set_timesteps\", FluxSetTimestepsStep()),\n+        (\"prepare_rope_inputs\", FluxKontextRoPEInputsStep()),\n+        (\"denoise\", FluxKontextDenoiseStep()),\n+        (\"decode\", FluxDecodeStep()),\n+    ]\n+)\n+\n+ALL_BLOCKS = {\n+    \"text2image\": TEXT2IMAGE_BLOCKS,\n+    \"img2img\": IMAGE2IMAGE_BLOCKS,\n+    \"auto\": AUTO_BLOCKS,\n+    \"auto_kontext\": AUTO_BLOCKS_KONTEXT,\n+    \"kontext\": FLUX_KONTEXT_BLOCKS,\n+}"
        },
        {
          "filename": "src/diffusers/modular_pipelines/flux/modular_pipeline.py",
          "status": "modified",
          "additions": 10,
          "deletions": 0,
          "changes": 10,
          "patch": "@@ -55,3 +55,13 @@ def num_channels_latents(self):\n         if getattr(self, \"transformer\", None):\n             num_channels_latents = self.transformer.config.in_channels // 4\n         return num_channels_latents\n+\n+\n+class FluxKontextModularPipeline(FluxModularPipeline):\n+    \"\"\"\n+    A ModularPipeline for Flux Kontext.\n+\n+    > [!WARNING] > This is an experimental feature and is likely to change in the future.\n+    \"\"\"\n+\n+    default_blocks_name = \"FluxKontextAutoBlocks\""
        },
        {
          "filename": "src/diffusers/modular_pipelines/modular_pipeline.py",
          "status": "modified",
          "additions": 1,
          "deletions": 0,
          "changes": 1,
          "patch": "@@ -57,6 +57,7 @@\n         (\"stable-diffusion-xl\", \"StableDiffusionXLModularPipeline\"),\n         (\"wan\", \"WanModularPipeline\"),\n         (\"flux\", \"FluxModularPipeline\"),\n+        (\"flux-kontext\", \"FluxKontextModularPipeline\"),\n         (\"qwenimage\", \"QwenImageModularPipeline\"),\n         (\"qwenimage-edit\", \"QwenImageEditModularPipeline\"),\n         (\"qwenimage-edit-plus\", \"QwenImageEditPlusModularPipeline\"),"
        },
        {
          "filename": "src/diffusers/utils/dummy_torch_and_transformers_objects.py",
          "status": "modified",
          "additions": 30,
          "deletions": 0,
          "changes": 30,
          "patch": "@@ -17,6 +17,36 @@ def from_pretrained(cls, *args, **kwargs):\n         requires_backends(cls, [\"torch\", \"transformers\"])\n \n \n+class FluxKontextAutoBlocks(metaclass=DummyObject):\n+    _backends = [\"torch\", \"transformers\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_config(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_pretrained(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+\n+class FluxKontextModularPipeline(metaclass=DummyObject):\n+    _backends = [\"torch\", \"transformers\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_config(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_pretrained(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+\n class FluxModularPipeline(metaclass=DummyObject):\n     _backends = [\"torch\", \"transformers\"]\n "
        }
      ],
      "num_files": 11,
      "scraped_at": "2025-11-16T21:19:03.299854"
    },
    {
      "pr_number": 12445,
      "title": "Align Flux modular more and more with Qwen modular",
      "body": "# What does this PR do?\r\n\r\nAs the title goes. \r\n\r\nNext plan is to incorporate these changes in https://github.com/huggingface/diffusers/pull/12269/. \r\n\r\nI tested with the code snippet from https://github.com/huggingface/diffusers/pull/12419#issuecomment-3369922849. \r\n\r\nFor img2img, I did:\r\n\r\n```py\r\nimport torch \r\nfrom diffusers import ModularPipeline\r\nfrom diffusers.utils import load_image\r\n\r\nrepo_id = \"black-forest-labs/FLUX.1-dev\"\r\n\r\npipe = ModularPipeline.from_pretrained(repo_id)\r\npipe.load_components(torch_dtype=torch.bfloat16)\r\npipe = pipe.to(\"cuda\")\r\n# print(pipe)\r\n\r\nurl = \"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg\"\r\ninit_image = load_image(url).resize((1024, 1024))\r\n\r\noutput = pipe(\r\n    prompt=\"a dog sitting by the see waiting for its companion to come\",\r\n    image=init_image,\r\n    guidance_scale=3.5,\r\n    num_inference_steps=28,\r\n    max_sequence_length=512,\r\n    strength=0.95,\r\n    generator=torch.manual_seed(0)\r\n)\r\noutput.values[\"images\"][0].save(\"modular_flux_image.png\")\r\n```",
      "html_url": "https://github.com/huggingface/diffusers/pull/12445",
      "created_at": "2025-10-07T07:34:44Z",
      "merged_at": "2025-10-08T03:52:34Z",
      "merge_commit_sha": "2dc31677e12fe175950f28fd5a0c0703594e7ce4",
      "base_ref": "main",
      "head_sha": "0252edcab750faccfa91c1795780a6c379b9a549",
      "user": "sayakpaul",
      "files": [
        {
          "filename": "src/diffusers/modular_pipelines/flux/before_denoise.py",
          "status": "modified",
          "additions": 97,
          "deletions": 232,
          "changes": 329,
          "patch": "@@ -13,12 +13,12 @@\n # limitations under the License.\n \n import inspect\n-from typing import Any, List, Optional, Tuple, Union\n+from typing import List, Optional, Union\n \n import numpy as np\n import torch\n \n-from ...models import AutoencoderKL\n+from ...pipelines import FluxPipeline\n from ...schedulers import FlowMatchEulerDiscreteScheduler\n from ...utils import logging\n from ...utils.torch_utils import randn_tensor\n@@ -104,48 +104,6 @@ def calculate_shift(\n     return mu\n \n \n-# Adapted from the original implementation.\n-def prepare_latents_img2img(\n-    vae, scheduler, image, timestep, batch_size, num_channels_latents, height, width, dtype, device, generator\n-):\n-    if isinstance(generator, list) and len(generator) != batch_size:\n-        raise ValueError(\n-            f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n-            f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n-        )\n-\n-    vae_scale_factor = 2 ** (len(vae.config.block_out_channels) - 1)\n-    latent_channels = vae.config.latent_channels\n-\n-    # VAE applies 8x compression on images but we must also account for packing which requires\n-    # latent height and width to be divisible by 2.\n-    height = 2 * (int(height) // (vae_scale_factor * 2))\n-    width = 2 * (int(width) // (vae_scale_factor * 2))\n-    shape = (batch_size, num_channels_latents, height, width)\n-    latent_image_ids = _prepare_latent_image_ids(batch_size, height // 2, width // 2, device, dtype)\n-\n-    image = image.to(device=device, dtype=dtype)\n-    if image.shape[1] != latent_channels:\n-        image_latents = _encode_vae_image(image=image, generator=generator)\n-    else:\n-        image_latents = image\n-    if batch_size > image_latents.shape[0] and batch_size % image_latents.shape[0] == 0:\n-        # expand init_latents for batch_size\n-        additional_image_per_prompt = batch_size // image_latents.shape[0]\n-        image_latents = torch.cat([image_latents] * additional_image_per_prompt, dim=0)\n-    elif batch_size > image_latents.shape[0] and batch_size % image_latents.shape[0] != 0:\n-        raise ValueError(\n-            f\"Cannot duplicate `image` of batch size {image_latents.shape[0]} to {batch_size} text prompts.\"\n-        )\n-    else:\n-        image_latents = torch.cat([image_latents], dim=0)\n-\n-    noise = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n-    latents = scheduler.scale_noise(image_latents, timestep, noise)\n-    latents = _pack_latents(latents, batch_size, num_channels_latents, height, width)\n-    return latents, latent_image_ids\n-\n-\n # Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_img2img.retrieve_latents\n def retrieve_latents(\n     encoder_output: torch.Tensor, generator: Optional[torch.Generator] = None, sample_mode: str = \"sample\"\n@@ -160,6 +118,7 @@ def retrieve_latents(\n         raise AttributeError(\"Could not access latents of provided encoder_output\")\n \n \n+# TODO: align this with Qwen patchifier\n def _pack_latents(latents, batch_size, num_channels_latents, height, width):\n     latents = latents.view(batch_size, num_channels_latents, height // 2, 2, width // 2, 2)\n     latents = latents.permute(0, 2, 4, 1, 3, 5)\n@@ -168,35 +127,6 @@ def _pack_latents(latents, batch_size, num_channels_latents, height, width):\n     return latents\n \n \n-def _prepare_latent_image_ids(batch_size, height, width, device, dtype):\n-    latent_image_ids = torch.zeros(height, width, 3)\n-    latent_image_ids[..., 1] = latent_image_ids[..., 1] + torch.arange(height)[:, None]\n-    latent_image_ids[..., 2] = latent_image_ids[..., 2] + torch.arange(width)[None, :]\n-\n-    latent_image_id_height, latent_image_id_width, latent_image_id_channels = latent_image_ids.shape\n-\n-    latent_image_ids = latent_image_ids.reshape(\n-        latent_image_id_height * latent_image_id_width, latent_image_id_channels\n-    )\n-\n-    return latent_image_ids.to(device=device, dtype=dtype)\n-\n-\n-# Cannot use \"# Copied from\" because it introduces weird indentation errors.\n-def _encode_vae_image(vae, image: torch.Tensor, generator: torch.Generator):\n-    if isinstance(generator, list):\n-        image_latents = [\n-            retrieve_latents(vae.encode(image[i : i + 1]), generator=generator[i]) for i in range(image.shape[0])\n-        ]\n-        image_latents = torch.cat(image_latents, dim=0)\n-    else:\n-        image_latents = retrieve_latents(vae.encode(image), generator=generator)\n-\n-    image_latents = (image_latents - vae.config.shift_factor) * vae.config.scaling_factor\n-\n-    return image_latents\n-\n-\n def _get_initial_timesteps_and_optionals(\n     transformer,\n     scheduler,\n@@ -231,96 +161,6 @@ def _get_initial_timesteps_and_optionals(\n     return timesteps, num_inference_steps, sigmas, guidance\n \n \n-class FluxInputStep(ModularPipelineBlocks):\n-    model_name = \"flux\"\n-\n-    @property\n-    def description(self) -> str:\n-        return (\n-            \"Input processing step that:\\n\"\n-            \"  1. Determines `batch_size` and `dtype` based on `prompt_embeds`\\n\"\n-            \"  2. Adjusts input tensor shapes based on `batch_size` (number of prompts) and `num_images_per_prompt`\\n\\n\"\n-            \"All input tensors are expected to have either batch_size=1 or match the batch_size\\n\"\n-            \"of prompt_embeds. The tensors will be duplicated across the batch dimension to\\n\"\n-            \"have a final batch_size of batch_size * num_images_per_prompt.\"\n-        )\n-\n-    @property\n-    def inputs(self) -> List[InputParam]:\n-        return [\n-            InputParam(\"num_images_per_prompt\", default=1),\n-            InputParam(\n-                \"prompt_embeds\",\n-                required=True,\n-                kwargs_type=\"denoiser_input_fields\",\n-                type_hint=torch.Tensor,\n-                description=\"Pre-generated text embeddings. Can be generated from text_encoder step.\",\n-            ),\n-            InputParam(\n-                \"pooled_prompt_embeds\",\n-                kwargs_type=\"denoiser_input_fields\",\n-                type_hint=torch.Tensor,\n-                description=\"Pre-generated pooled text embeddings. Can be generated from text_encoder step.\",\n-            ),\n-            # TODO: support negative embeddings?\n-        ]\n-\n-    @property\n-    def intermediate_outputs(self) -> List[str]:\n-        return [\n-            OutputParam(\n-                \"batch_size\",\n-                type_hint=int,\n-                description=\"Number of prompts, the final batch size of model inputs should be batch_size * num_images_per_prompt\",\n-            ),\n-            OutputParam(\n-                \"dtype\",\n-                type_hint=torch.dtype,\n-                description=\"Data type of model tensor inputs (determined by `prompt_embeds`)\",\n-            ),\n-            OutputParam(\n-                \"prompt_embeds\",\n-                type_hint=torch.Tensor,\n-                kwargs_type=\"denoiser_input_fields\",\n-                description=\"text embeddings used to guide the image generation\",\n-            ),\n-            OutputParam(\n-                \"pooled_prompt_embeds\",\n-                type_hint=torch.Tensor,\n-                kwargs_type=\"denoiser_input_fields\",\n-                description=\"pooled text embeddings used to guide the image generation\",\n-            ),\n-            # TODO: support negative embeddings?\n-        ]\n-\n-    def check_inputs(self, components, block_state):\n-        if block_state.prompt_embeds is not None and block_state.pooled_prompt_embeds is not None:\n-            if block_state.prompt_embeds.shape[0] != block_state.pooled_prompt_embeds.shape[0]:\n-                raise ValueError(\n-                    \"`prompt_embeds` and `pooled_prompt_embeds` must have the same batch size when passed directly, but\"\n-                    f\" got: `prompt_embeds` {block_state.prompt_embeds.shape} != `pooled_prompt_embeds`\"\n-                    f\" {block_state.pooled_prompt_embeds.shape}.\"\n-                )\n-\n-    @torch.no_grad()\n-    def __call__(self, components: FluxModularPipeline, state: PipelineState) -> PipelineState:\n-        # TODO: consider adding negative embeddings?\n-        block_state = self.get_block_state(state)\n-        self.check_inputs(components, block_state)\n-\n-        block_state.batch_size = block_state.prompt_embeds.shape[0]\n-        block_state.dtype = block_state.prompt_embeds.dtype\n-\n-        _, seq_len, _ = block_state.prompt_embeds.shape\n-        block_state.prompt_embeds = block_state.prompt_embeds.repeat(1, block_state.num_images_per_prompt, 1)\n-        block_state.prompt_embeds = block_state.prompt_embeds.view(\n-            block_state.batch_size * block_state.num_images_per_prompt, seq_len, -1\n-        )\n-        self.set_block_state(state, block_state)\n-\n-        return components, state\n-\n-\n class FluxSetTimestepsStep(ModularPipelineBlocks):\n     model_name = \"flux\"\n \n@@ -389,6 +229,10 @@ def __call__(self, components: FluxModularPipeline, state: PipelineState) -> Pip\n         block_state.sigmas = sigmas\n         block_state.guidance = guidance\n \n+        # We set the index here to remove DtoH sync, helpful especially during compilation.\n+        # Check out more details here: https://github.com/huggingface/diffusers/pull/11696\n+        components.scheduler.set_begin_index(0)\n+\n         self.set_block_state(state, block_state)\n         return components, state\n \n@@ -432,11 +276,6 @@ def intermediate_outputs(self) -> List[OutputParam]:\n                 type_hint=int,\n                 description=\"The number of denoising steps to perform at inference time\",\n             ),\n-            OutputParam(\n-                \"latent_timestep\",\n-                type_hint=torch.Tensor,\n-                description=\"The timestep that represents the initial noise level for image-to-image generation\",\n-            ),\n             OutputParam(\"guidance\", type_hint=torch.Tensor, description=\"Optional guidance to be used.\"),\n         ]\n \n@@ -484,8 +323,6 @@ def __call__(self, components: FluxModularPipeline, state: PipelineState) -> Pip\n         block_state.sigmas = sigmas\n         block_state.guidance = guidance\n \n-        block_state.latent_timestep = timesteps[:1].repeat(batch_size)\n-\n         self.set_block_state(state, block_state)\n         return components, state\n \n@@ -524,11 +361,6 @@ def intermediate_outputs(self) -> List[OutputParam]:\n             OutputParam(\n                 \"latents\", type_hint=torch.Tensor, description=\"The initial latents to use for the denoising process\"\n             ),\n-            OutputParam(\n-                \"latent_image_ids\",\n-                type_hint=torch.Tensor,\n-                description=\"IDs computed from the image sequence needed for RoPE\",\n-            ),\n         ]\n \n     @staticmethod\n@@ -552,33 +384,25 @@ def prepare_latents(\n         generator,\n         latents=None,\n     ):\n-        # Couldn't use the `prepare_latents` method directly from Flux because I decided to copy over\n-        # the packing methods here. So, for example, `comp._pack_latents()` won't work if we were\n-        # to go with the \"# Copied from ...\" approach. Or maybe there's a way?\n-\n-        # VAE applies 8x compression on images but we must also account for packing which requires\n-        # latent height and width to be divisible by 2.\n         height = 2 * (int(height) // (comp.vae_scale_factor * 2))\n         width = 2 * (int(width) // (comp.vae_scale_factor * 2))\n \n         shape = (batch_size, num_channels_latents, height, width)\n \n         if latents is not None:\n-            latent_image_ids = _prepare_latent_image_ids(batch_size, height // 2, width // 2, device, dtype)\n-            return latents.to(device=device, dtype=dtype), latent_image_ids\n+            return latents.to(device=device, dtype=dtype)\n \n         if isinstance(generator, list) and len(generator) != batch_size:\n             raise ValueError(\n                 f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n                 f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n             )\n \n+        # TODO: move packing latents code to a patchifier\n         latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n         latents = _pack_latents(latents, batch_size, num_channels_latents, height, width)\n \n-        latent_image_ids = _prepare_latent_image_ids(batch_size, height // 2, width // 2, device, dtype)\n-\n-        return latents, latent_image_ids\n+        return latents\n \n     @torch.no_grad()\n     def __call__(self, components: FluxModularPipeline, state: PipelineState) -> PipelineState:\n@@ -587,12 +411,11 @@ def __call__(self, components: FluxModularPipeline, state: PipelineState) -> Pip\n         block_state.height = block_state.height or components.default_height\n         block_state.width = block_state.width or components.default_width\n         block_state.device = components._execution_device\n-        block_state.dtype = torch.bfloat16  # TODO: okay to hardcode this?\n         block_state.num_channels_latents = components.num_channels_latents\n \n         self.check_inputs(components, block_state)\n         batch_size = block_state.batch_size * block_state.num_images_per_prompt\n-        block_state.latents, block_state.latent_image_ids = self.prepare_latents(\n+        block_state.latents = self.prepare_latents(\n             components,\n             batch_size,\n             block_state.num_channels_latents,\n@@ -613,81 +436,123 @@ class FluxImg2ImgPrepareLatentsStep(ModularPipelineBlocks):\n     model_name = \"flux\"\n \n     @property\n-    def expected_components(self) -> List[ComponentSpec]:\n-        return [ComponentSpec(\"vae\", AutoencoderKL), ComponentSpec(\"scheduler\", FlowMatchEulerDiscreteScheduler)]\n+    def description(self) -> str:\n+        return \"Step that adds noise to image latents for image-to-image. Should be run after `set_timesteps`,\"\n+        \" `prepare_latents`. Both noise and image latents should already be patchified.\"\n \n     @property\n-    def description(self) -> str:\n-        return \"Step that prepares the latents for the image-to-image generation process\"\n+    def expected_components(self) -> List[ComponentSpec]:\n+        return [ComponentSpec(\"scheduler\", FlowMatchEulerDiscreteScheduler)]\n \n     @property\n-    def inputs(self) -> List[Tuple[str, Any]]:\n+    def inputs(self) -> List[InputParam]:\n         return [\n-            InputParam(\"height\", type_hint=int),\n-            InputParam(\"width\", type_hint=int),\n-            InputParam(\"latents\", type_hint=Optional[torch.Tensor]),\n-            InputParam(\"num_images_per_prompt\", type_hint=int, default=1),\n-            InputParam(\"generator\"),\n             InputParam(\n-                \"image_latents\",\n+                name=\"latents\",\n                 required=True,\n                 type_hint=torch.Tensor,\n-                description=\"The latents representing the reference image for image-to-image/inpainting generation. Can be generated in vae_encode step.\",\n+                description=\"The initial random noised, can be generated in prepare latent step.\",\n             ),\n             InputParam(\n-                \"latent_timestep\",\n+                name=\"image_latents\",\n                 required=True,\n                 type_hint=torch.Tensor,\n-                description=\"The timestep that represents the initial noise level for image-to-image/inpainting generation. Can be generated in set_timesteps step.\",\n+                description=\"The image latents to use for the denoising process. Can be generated in vae encoder and packed in input step.\",\n             ),\n             InputParam(\n-                \"batch_size\",\n+                name=\"timesteps\",\n                 required=True,\n-                type_hint=int,\n-                description=\"Number of prompts, the final batch size of model inputs should be batch_size * num_images_per_prompt. Can be generated in input step.\",\n+                type_hint=torch.Tensor,\n+                description=\"The timesteps to use for the denoising process. Can be generated in set_timesteps step.\",\n             ),\n-            InputParam(\"dtype\", required=True, type_hint=torch.dtype, description=\"The dtype of the model inputs\"),\n         ]\n \n     @property\n     def intermediate_outputs(self) -> List[OutputParam]:\n         return [\n             OutputParam(\n-                \"latents\", type_hint=torch.Tensor, description=\"The initial latents to use for the denoising process\"\n-            ),\n-            OutputParam(\n-                \"latent_image_ids\",\n+                name=\"initial_noise\",\n                 type_hint=torch.Tensor,\n-                description=\"IDs computed from the image sequence needed for RoPE\",\n+                description=\"The initial random noised used for inpainting denoising.\",\n             ),\n         ]\n \n+    @staticmethod\n+    def check_inputs(image_latents, latents):\n+        if image_latents.shape[0] != latents.shape[0]:\n+            raise ValueError(\n+                f\"`image_latents` must have have same batch size as `latents`, but got {image_latents.shape[0]} and {latents.shape[0]}\"\n+            )\n+\n+        if image_latents.ndim != 3:\n+            raise ValueError(f\"`image_latents` must have 3 dimensions (patchified), but got {image_latents.ndim}\")\n+\n     @torch.no_grad()\n     def __call__(self, components: FluxModularPipeline, state: PipelineState) -> PipelineState:\n         block_state = self.get_block_state(state)\n \n-        block_state.device = components._execution_device\n-        block_state.dtype = torch.bfloat16  # TODO: okay to hardcode this?\n-        block_state.num_channels_latents = components.num_channels_latents\n-        block_state.dtype = block_state.dtype if block_state.dtype is not None else components.vae.dtype\n-        block_state.device = components._execution_device\n+        self.check_inputs(image_latents=block_state.image_latents, latents=block_state.latents)\n \n-        # TODO: implement `check_inputs`\n-        batch_size = block_state.batch_size * block_state.num_images_per_prompt\n-        if block_state.latents is None:\n-            block_state.latents, block_state.latent_image_ids = prepare_latents_img2img(\n-                components.vae,\n-                components.scheduler,\n-                block_state.image_latents,\n-                block_state.latent_timestep,\n-                batch_size,\n-                block_state.num_channels_latents,\n-                block_state.height,\n-                block_state.width,\n-                block_state.dtype,\n-                block_state.device,\n-                block_state.generator,\n-            )\n+        # prepare latent timestep\n+        latent_timestep = block_state.timesteps[:1].repeat(block_state.latents.shape[0])\n+\n+        # make copy of initial_noise\n+        block_state.initial_noise = block_state.latents\n+\n+        # scale noise\n+        block_state.latents = components.scheduler.scale_noise(\n+            block_state.image_latents, latent_timestep, block_state.latents\n+        )\n+\n+        self.set_block_state(state, block_state)\n+\n+        return components, state\n+\n+\n+class FluxRoPEInputsStep(ModularPipelineBlocks):\n+    model_name = \"flux\"\n+\n+    @property\n+    def description(self) -> str:\n+        return \"Step that prepares the RoPE inputs for the denoising process. Should be placed after text encoder and latent preparation steps.\"\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        return [\n+            InputParam(name=\"height\", required=True),\n+            InputParam(name=\"width\", required=True),\n+            InputParam(name=\"prompt_embeds\"),\n+        ]\n+\n+    @property\n+    def intermediate_outputs(self) -> List[OutputParam]:\n+        return [\n+            OutputParam(\n+                name=\"txt_ids\",\n+                kwargs_type=\"denoiser_input_fields\",\n+                type_hint=List[int],\n+                description=\"The sequence lengths of the prompt embeds, used for RoPE calculation.\",\n+            ),\n+            OutputParam(\n+                name=\"img_ids\",\n+                kwargs_type=\"denoiser_input_fields\",\n+                type_hint=List[int],\n+                description=\"The sequence lengths of the image latents, used for RoPE calculation.\",\n+            ),\n+        ]\n+\n+    def __call__(self, components: FluxModularPipeline, state: PipelineState) -> PipelineState:\n+        block_state = self.get_block_state(state)\n+\n+        prompt_embeds = block_state.prompt_embeds\n+        device, dtype = prompt_embeds.device, prompt_embeds.dtype\n+        block_state.txt_ids = torch.zeros(prompt_embeds.shape[1], 3).to(\n+            device=prompt_embeds.device, dtype=prompt_embeds.dtype\n+        )\n+\n+        height = 2 * (int(block_state.height) // (components.vae_scale_factor * 2))\n+        width = 2 * (int(block_state.width) // (components.vae_scale_factor * 2))\n+        block_state.img_ids = FluxPipeline._prepare_latent_image_ids(None, height // 2, width // 2, device, dtype)\n \n         self.set_block_state(state, block_state)\n "
        },
        {
          "filename": "src/diffusers/modular_pipelines/flux/denoise.py",
          "status": "modified",
          "additions": 4,
          "deletions": 8,
          "changes": 12,
          "patch": "@@ -76,18 +76,17 @@ def inputs(self) -> List[Tuple[str, Any]]:\n                 description=\"Pooled prompt embeddings\",\n             ),\n             InputParam(\n-                \"text_ids\",\n+                \"txt_ids\",\n                 required=True,\n                 type_hint=torch.Tensor,\n                 description=\"IDs computed from text sequence needed for RoPE\",\n             ),\n             InputParam(\n-                \"latent_image_ids\",\n+                \"img_ids\",\n                 required=True,\n                 type_hint=torch.Tensor,\n                 description=\"IDs computed from image sequence needed for RoPE\",\n             ),\n-            # TODO: guidance\n         ]\n \n     @torch.no_grad()\n@@ -101,8 +100,8 @@ def __call__(\n             encoder_hidden_states=block_state.prompt_embeds,\n             pooled_projections=block_state.pooled_prompt_embeds,\n             joint_attention_kwargs=block_state.joint_attention_kwargs,\n-            txt_ids=block_state.text_ids,\n-            img_ids=block_state.latent_image_ids,\n+            txt_ids=block_state.txt_ids,\n+            img_ids=block_state.img_ids,\n             return_dict=False,\n         )[0]\n         block_state.noise_pred = noise_pred\n@@ -195,9 +194,6 @@ def __call__(self, components: FluxModularPipeline, state: PipelineState) -> Pip\n         block_state.num_warmup_steps = max(\n             len(block_state.timesteps) - block_state.num_inference_steps * components.scheduler.order, 0\n         )\n-        # We set the index here to remove DtoH sync, helpful especially during compilation.\n-        # Check out more details here: https://github.com/huggingface/diffusers/pull/11696\n-        components.scheduler.set_begin_index(0)\n         with self.progress_bar(total=block_state.num_inference_steps) as progress_bar:\n             for i, t in enumerate(block_state.timesteps):\n                 components, block_state = self.loop_step(components, block_state, i=i, t=t)"
        },
        {
          "filename": "src/diffusers/modular_pipelines/flux/encoders.py",
          "status": "modified",
          "additions": 115,
          "deletions": 117,
          "changes": 232,
          "patch": "@@ -25,7 +25,7 @@\n from ...models import AutoencoderKL\n from ...utils import USE_PEFT_BACKEND, is_ftfy_available, logging, scale_lora_layers, unscale_lora_layers\n from ..modular_pipeline import ModularPipelineBlocks, PipelineState\n-from ..modular_pipeline_utils import ComponentSpec, ConfigSpec, InputParam, OutputParam\n+from ..modular_pipeline_utils import ComponentSpec, InputParam, OutputParam\n from .modular_pipeline import FluxModularPipeline\n \n \n@@ -67,89 +67,148 @@ def retrieve_latents(\n         raise AttributeError(\"Could not access latents of provided encoder_output\")\n \n \n-class FluxVaeEncoderStep(ModularPipelineBlocks):\n-    model_name = \"flux\"\n+def encode_vae_image(vae: AutoencoderKL, image: torch.Tensor, generator: torch.Generator, sample_mode=\"sample\"):\n+    if isinstance(generator, list):\n+        image_latents = [\n+            retrieve_latents(vae.encode(image[i : i + 1]), generator=generator[i], sample_mode=sample_mode)\n+            for i in range(image.shape[0])\n+        ]\n+        image_latents = torch.cat(image_latents, dim=0)\n+    else:\n+        image_latents = retrieve_latents(vae.encode(image), generator=generator, sample_mode=sample_mode)\n+\n+    image_latents = (image_latents - vae.config.shift_factor) * vae.config.scaling_factor\n+\n+    return image_latents\n+\n+\n+class FluxProcessImagesInputStep(ModularPipelineBlocks):\n+    model_name = \"Flux\"\n \n     @property\n     def description(self) -> str:\n-        return \"Vae Encoder step that encode the input image into a latent representation\"\n+        return \"Image Preprocess step. Resizing is needed in Flux Kontext (will be implemented later.)\"\n \n     @property\n     def expected_components(self) -> List[ComponentSpec]:\n         return [\n-            ComponentSpec(\"vae\", AutoencoderKL),\n             ComponentSpec(\n                 \"image_processor\",\n                 VaeImageProcessor,\n-                config=FrozenDict({\"vae_scale_factor\": 16, \"vae_latent_channels\": 16}),\n+                config=FrozenDict({\"vae_scale_factor\": 16}),\n                 default_creation_method=\"from_config\",\n             ),\n         ]\n \n     @property\n     def inputs(self) -> List[InputParam]:\n-        return [\n-            InputParam(\"image\", required=True),\n-            InputParam(\"height\"),\n-            InputParam(\"width\"),\n-            InputParam(\"generator\"),\n-            InputParam(\"dtype\", type_hint=torch.dtype, description=\"Data type of model tensor inputs\"),\n-            InputParam(\n-                \"preprocess_kwargs\",\n-                type_hint=Optional[dict],\n-                description=\"A kwargs dictionary that if specified is passed along to the `ImageProcessor` as defined under `self.image_processor` in [diffusers.image_processor.VaeImageProcessor]\",\n-            ),\n-        ]\n+        return [InputParam(\"resized_image\"), InputParam(\"image\"), InputParam(\"height\"), InputParam(\"width\")]\n \n     @property\n     def intermediate_outputs(self) -> List[OutputParam]:\n         return [\n-            OutputParam(\n-                \"image_latents\",\n-                type_hint=torch.Tensor,\n-                description=\"The latents representing the reference image for image-to-image/inpainting generation\",\n-            )\n+            OutputParam(name=\"processed_image\"),\n         ]\n \n     @staticmethod\n-    # Copied from diffusers.pipelines.stable_diffusion_3.pipeline_stable_diffusion_3_inpaint.StableDiffusion3InpaintPipeline._encode_vae_image with self.vae->vae\n-    def _encode_vae_image(vae, image: torch.Tensor, generator: torch.Generator):\n-        if isinstance(generator, list):\n-            image_latents = [\n-                retrieve_latents(vae.encode(image[i : i + 1]), generator=generator[i]) for i in range(image.shape[0])\n-            ]\n-            image_latents = torch.cat(image_latents, dim=0)\n+    def check_inputs(height, width, vae_scale_factor):\n+        if height is not None and height % (vae_scale_factor * 2) != 0:\n+            raise ValueError(f\"Height must be divisible by {vae_scale_factor * 2} but is {height}\")\n+\n+        if width is not None and width % (vae_scale_factor * 2) != 0:\n+            raise ValueError(f\"Width must be divisible by {vae_scale_factor * 2} but is {width}\")\n+\n+    @torch.no_grad()\n+    def __call__(self, components: FluxModularPipeline, state: PipelineState):\n+        block_state = self.get_block_state(state)\n+\n+        if block_state.resized_image is None and block_state.image is None:\n+            raise ValueError(\"`resized_image` and `image` cannot be None at the same time\")\n+\n+        if block_state.resized_image is None:\n+            image = block_state.image\n+            self.check_inputs(\n+                height=block_state.height, width=block_state.width, vae_scale_factor=components.vae_scale_factor\n+            )\n+            height = block_state.height or components.default_height\n+            width = block_state.width or components.default_width\n         else:\n-            image_latents = retrieve_latents(vae.encode(image), generator=generator)\n+            width, height = block_state.resized_image[0].size\n+            image = block_state.resized_image\n+\n+        block_state.processed_image = components.image_processor.preprocess(image=image, height=height, width=width)\n+\n+        self.set_block_state(state, block_state)\n+        return components, state\n \n-        image_latents = (image_latents - vae.config.shift_factor) * vae.config.scaling_factor\n \n-        return image_latents\n+class FluxVaeEncoderDynamicStep(ModularPipelineBlocks):\n+    model_name = \"flux\"\n+\n+    def __init__(\n+        self,\n+        input_name: str = \"processed_image\",\n+        output_name: str = \"image_latents\",\n+    ):\n+        \"\"\"Initialize a VAE encoder step for converting images to latent representations.\n+\n+        Both the input and output names are configurable so this block can be configured to process to different image\n+        inputs (e.g., \"processed_image\" -> \"image_latents\", \"processed_control_image\" -> \"control_image_latents\").\n+\n+        Args:\n+            input_name (str, optional): Name of the input image tensor. Defaults to \"processed_image\".\n+                Examples: \"processed_image\" or \"processed_control_image\"\n+            output_name (str, optional): Name of the output latent tensor. Defaults to \"image_latents\".\n+                Examples: \"image_latents\" or \"control_image_latents\"\n+\n+        Examples:\n+            # Basic usage with default settings (includes image processor): # FluxImageVaeEncoderDynamicStep()\n+\n+            # Custom input/output names for control image: # FluxImageVaeEncoderDynamicStep(\n+                input_name=\"processed_control_image\", output_name=\"control_image_latents\"\n+            )\n+        \"\"\"\n+        self._image_input_name = input_name\n+        self._image_latents_output_name = output_name\n+        super().__init__()\n+\n+    @property\n+    def description(self) -> str:\n+        return f\"Dynamic VAE Encoder step that converts {self._image_input_name} into latent representations {self._image_latents_output_name}.\\n\"\n+\n+    @property\n+    def expected_components(self) -> List[ComponentSpec]:\n+        components = [ComponentSpec(\"vae\", AutoencoderKL)]\n+        return components\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        inputs = [InputParam(self._image_input_name, required=True), InputParam(\"generator\")]\n+        return inputs\n+\n+    @property\n+    def intermediate_outputs(self) -> List[OutputParam]:\n+        return [\n+            OutputParam(\n+                self._image_latents_output_name,\n+                type_hint=torch.Tensor,\n+                description=\"The latents representing the reference image\",\n+            )\n+        ]\n \n     @torch.no_grad()\n     def __call__(self, components: FluxModularPipeline, state: PipelineState) -> PipelineState:\n         block_state = self.get_block_state(state)\n-        block_state.preprocess_kwargs = block_state.preprocess_kwargs or {}\n-        block_state.device = components._execution_device\n-        block_state.dtype = block_state.dtype if block_state.dtype is not None else components.vae.dtype\n \n-        block_state.image = components.image_processor.preprocess(\n-            block_state.image, height=block_state.height, width=block_state.width, **block_state.preprocess_kwargs\n-        )\n-        block_state.image = block_state.image.to(device=block_state.device, dtype=block_state.dtype)\n+        device = components._execution_device\n+        dtype = components.vae.dtype\n \n-        block_state.batch_size = block_state.image.shape[0]\n+        image = getattr(block_state, self._image_input_name)\n+        image = image.to(device=device, dtype=dtype)\n \n-        # if generator is a list, make sure the length of it matches the length of images (both should be batch_size)\n-        if isinstance(block_state.generator, list) and len(block_state.generator) != block_state.batch_size:\n-            raise ValueError(\n-                f\"You have passed a list of generators of length {len(block_state.generator)}, but requested an effective batch\"\n-                f\" size of {block_state.batch_size}. Make sure the batch size matches the length of the generators.\"\n-            )\n-\n-        block_state.image_latents = self._encode_vae_image(\n-            components.vae, image=block_state.image, generator=block_state.generator\n-        )\n+        # Encode image into latents\n+        image_latents = encode_vae_image(image=image, vae=components.vae, generator=block_state.generator)\n+        setattr(block_state, self._image_latents_output_name, image_latents)\n \n         self.set_block_state(state, block_state)\n \n@@ -161,7 +220,7 @@ class FluxTextEncoderStep(ModularPipelineBlocks):\n \n     @property\n     def description(self) -> str:\n-        return \"Text Encoder step that generate text_embeddings to guide the video generation\"\n+        return \"Text Encoder step that generate text_embeddings to guide the image generation\"\n \n     @property\n     def expected_components(self) -> List[ComponentSpec]:\n@@ -172,10 +231,6 @@ def expected_components(self) -> List[ComponentSpec]:\n             ComponentSpec(\"tokenizer_2\", T5TokenizerFast),\n         ]\n \n-    @property\n-    def expected_configs(self) -> List[ConfigSpec]:\n-        return []\n-\n     @property\n     def inputs(self) -> List[InputParam]:\n         return [\n@@ -200,12 +255,6 @@ def intermediate_outputs(self) -> List[OutputParam]:\n                 type_hint=torch.Tensor,\n                 description=\"pooled text embeddings used to guide the image generation\",\n             ),\n-            OutputParam(\n-                \"text_ids\",\n-                kwargs_type=\"denoiser_input_fields\",\n-                type_hint=torch.Tensor,\n-                description=\"ids from the text sequence for RoPE\",\n-            ),\n         ]\n \n     @staticmethod\n@@ -216,16 +265,10 @@ def check_inputs(block_state):\n \n     @staticmethod\n     def _get_t5_prompt_embeds(\n-        components,\n-        prompt: Union[str, List[str]],\n-        num_images_per_prompt: int,\n-        max_sequence_length: int,\n-        device: torch.device,\n+        components, prompt: Union[str, List[str]], max_sequence_length: int, device: torch.device\n     ):\n         dtype = components.text_encoder_2.dtype\n-\n         prompt = [prompt] if isinstance(prompt, str) else prompt\n-        batch_size = len(prompt)\n \n         if isinstance(components, TextualInversionLoaderMixin):\n             prompt = components.maybe_convert_prompt(prompt, components.tokenizer_2)\n@@ -251,23 +294,11 @@ def _get_t5_prompt_embeds(\n \n         prompt_embeds = components.text_encoder_2(text_input_ids.to(device), output_hidden_states=False)[0]\n         prompt_embeds = prompt_embeds.to(dtype=dtype, device=device)\n-        _, seq_len, _ = prompt_embeds.shape\n-\n-        # duplicate text embeddings and attention mask for each generation per prompt, using mps friendly method\n-        prompt_embeds = prompt_embeds.repeat(1, num_images_per_prompt, 1)\n-        prompt_embeds = prompt_embeds.view(batch_size * num_images_per_prompt, seq_len, -1)\n-\n         return prompt_embeds\n \n     @staticmethod\n-    def _get_clip_prompt_embeds(\n-        components,\n-        prompt: Union[str, List[str]],\n-        num_images_per_prompt: int,\n-        device: torch.device,\n-    ):\n+    def _get_clip_prompt_embeds(components, prompt: Union[str, List[str]], device: torch.device):\n         prompt = [prompt] if isinstance(prompt, str) else prompt\n-        batch_size = len(prompt)\n \n         if isinstance(components, TextualInversionLoaderMixin):\n             prompt = components.maybe_convert_prompt(prompt, components.tokenizer)\n@@ -297,10 +328,6 @@ def _get_clip_prompt_embeds(\n         prompt_embeds = prompt_embeds.pooler_output\n         prompt_embeds = prompt_embeds.to(dtype=components.text_encoder.dtype, device=device)\n \n-        # duplicate text embeddings for each generation per prompt, using mps friendly method\n-        prompt_embeds = prompt_embeds.repeat(1, num_images_per_prompt)\n-        prompt_embeds = prompt_embeds.view(batch_size * num_images_per_prompt, -1)\n-\n         return prompt_embeds\n \n     @staticmethod\n@@ -309,34 +336,11 @@ def encode_prompt(\n         prompt: Union[str, List[str]],\n         prompt_2: Union[str, List[str]],\n         device: Optional[torch.device] = None,\n-        num_images_per_prompt: int = 1,\n         prompt_embeds: Optional[torch.FloatTensor] = None,\n         pooled_prompt_embeds: Optional[torch.FloatTensor] = None,\n         max_sequence_length: int = 512,\n         lora_scale: Optional[float] = None,\n     ):\n-        r\"\"\"\n-        Encodes the prompt into text encoder hidden states.\n-\n-        Args:\n-            prompt (`str` or `List[str]`, *optional*):\n-                prompt to be encoded\n-            prompt_2 (`str` or `List[str]`, *optional*):\n-                The prompt or prompts to be sent to the `tokenizer_2` and `text_encoder_2`. If not defined, `prompt` is\n-                used in all text-encoders\n-            device: (`torch.device`):\n-                torch device\n-            num_images_per_prompt (`int`):\n-                number of images that should be generated per prompt\n-            prompt_embeds (`torch.FloatTensor`, *optional*):\n-                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n-                provided, text embeddings will be generated from `prompt` input argument.\n-            pooled_prompt_embeds (`torch.FloatTensor`, *optional*):\n-                Pre-generated pooled text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting.\n-                If not provided, pooled text embeddings will be generated from `prompt` input argument.\n-            lora_scale (`float`, *optional*):\n-                A lora scale that will be applied to all LoRA layers of the text encoder if LoRA layers are loaded.\n-        \"\"\"\n         device = device or components._execution_device\n \n         # set lora scale so that monkey patched LoRA\n@@ -361,12 +365,10 @@ def encode_prompt(\n                 components,\n                 prompt=prompt,\n                 device=device,\n-                num_images_per_prompt=num_images_per_prompt,\n             )\n             prompt_embeds = FluxTextEncoderStep._get_t5_prompt_embeds(\n                 components,\n                 prompt=prompt_2,\n-                num_images_per_prompt=num_images_per_prompt,\n                 max_sequence_length=max_sequence_length,\n                 device=device,\n             )\n@@ -381,10 +383,7 @@ def encode_prompt(\n                 # Retrieve the original scale by scaling back the LoRA layers\n                 unscale_lora_layers(components.text_encoder_2, lora_scale)\n \n-        dtype = components.text_encoder.dtype if components.text_encoder is not None else torch.bfloat16\n-        text_ids = torch.zeros(prompt_embeds.shape[1], 3).to(device=device, dtype=dtype)\n-\n-        return prompt_embeds, pooled_prompt_embeds, text_ids\n+        return prompt_embeds, pooled_prompt_embeds\n \n     @torch.no_grad()\n     def __call__(self, components: FluxModularPipeline, state: PipelineState) -> PipelineState:\n@@ -400,14 +399,13 @@ def __call__(self, components: FluxModularPipeline, state: PipelineState) -> Pip\n             if block_state.joint_attention_kwargs is not None\n             else None\n         )\n-        (block_state.prompt_embeds, block_state.pooled_prompt_embeds, block_state.text_ids) = self.encode_prompt(\n+        block_state.prompt_embeds, block_state.pooled_prompt_embeds = self.encode_prompt(\n             components,\n             prompt=block_state.prompt,\n             prompt_2=None,\n             prompt_embeds=None,\n             pooled_prompt_embeds=None,\n             device=block_state.device,\n-            num_images_per_prompt=1,  # TODO: hardcoded for now.\n             max_sequence_length=block_state.max_sequence_length,\n             lora_scale=block_state.text_encoder_lora_scale,\n         )"
        },
        {
          "filename": "src/diffusers/modular_pipelines/flux/inputs.py",
          "status": "added",
          "additions": 236,
          "deletions": 0,
          "changes": 236,
          "patch": "@@ -0,0 +1,236 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import List\n+\n+import torch\n+\n+from ...pipelines import FluxPipeline\n+from ..modular_pipeline import ModularPipelineBlocks, PipelineState\n+from ..modular_pipeline_utils import InputParam, OutputParam\n+\n+# TODO: consider making these common utilities for modular if they are not pipeline-specific.\n+from ..qwenimage.inputs import calculate_dimension_from_latents, repeat_tensor_to_batch_size\n+from .modular_pipeline import FluxModularPipeline\n+\n+\n+class FluxTextInputStep(ModularPipelineBlocks):\n+    model_name = \"flux\"\n+\n+    @property\n+    def description(self) -> str:\n+        return (\n+            \"Text input processing step that standardizes text embeddings for the pipeline.\\n\"\n+            \"This step:\\n\"\n+            \"  1. Determines `batch_size` and `dtype` based on `prompt_embeds`\\n\"\n+            \"  2. Ensures all text embeddings have consistent batch sizes (batch_size * num_images_per_prompt)\"\n+        )\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        return [\n+            InputParam(\"num_images_per_prompt\", default=1),\n+            InputParam(\n+                \"prompt_embeds\",\n+                required=True,\n+                kwargs_type=\"denoiser_input_fields\",\n+                type_hint=torch.Tensor,\n+                description=\"Pre-generated text embeddings. Can be generated from text_encoder step.\",\n+            ),\n+            InputParam(\n+                \"pooled_prompt_embeds\",\n+                kwargs_type=\"denoiser_input_fields\",\n+                type_hint=torch.Tensor,\n+                description=\"Pre-generated pooled text embeddings. Can be generated from text_encoder step.\",\n+            ),\n+            # TODO: support negative embeddings?\n+        ]\n+\n+    @property\n+    def intermediate_outputs(self) -> List[str]:\n+        return [\n+            OutputParam(\n+                \"batch_size\",\n+                type_hint=int,\n+                description=\"Number of prompts, the final batch size of model inputs should be batch_size * num_images_per_prompt\",\n+            ),\n+            OutputParam(\n+                \"dtype\",\n+                type_hint=torch.dtype,\n+                description=\"Data type of model tensor inputs (determined by `prompt_embeds`)\",\n+            ),\n+            OutputParam(\n+                \"prompt_embeds\",\n+                type_hint=torch.Tensor,\n+                kwargs_type=\"denoiser_input_fields\",\n+                description=\"text embeddings used to guide the image generation\",\n+            ),\n+            OutputParam(\n+                \"pooled_prompt_embeds\",\n+                type_hint=torch.Tensor,\n+                kwargs_type=\"denoiser_input_fields\",\n+                description=\"pooled text embeddings used to guide the image generation\",\n+            ),\n+            # TODO: support negative embeddings?\n+        ]\n+\n+    def check_inputs(self, components, block_state):\n+        if block_state.prompt_embeds is not None and block_state.pooled_prompt_embeds is not None:\n+            if block_state.prompt_embeds.shape[0] != block_state.pooled_prompt_embeds.shape[0]:\n+                raise ValueError(\n+                    \"`prompt_embeds` and `pooled_prompt_embeds` must have the same batch size when passed directly, but\"\n+                    f\" got: `prompt_embeds` {block_state.prompt_embeds.shape} != `pooled_prompt_embeds`\"\n+                    f\" {block_state.pooled_prompt_embeds.shape}.\"\n+                )\n+\n+    @torch.no_grad()\n+    def __call__(self, components: FluxModularPipeline, state: PipelineState) -> PipelineState:\n+        # TODO: consider adding negative embeddings?\n+        block_state = self.get_block_state(state)\n+        self.check_inputs(components, block_state)\n+\n+        block_state.batch_size = block_state.prompt_embeds.shape[0]\n+        block_state.dtype = block_state.prompt_embeds.dtype\n+\n+        _, seq_len, _ = block_state.prompt_embeds.shape\n+        block_state.prompt_embeds = block_state.prompt_embeds.repeat(1, block_state.num_images_per_prompt, 1)\n+        block_state.prompt_embeds = block_state.prompt_embeds.view(\n+            block_state.batch_size * block_state.num_images_per_prompt, seq_len, -1\n+        )\n+        self.set_block_state(state, block_state)\n+\n+        return components, state\n+\n+\n+# Adapted from `QwenImageInputsDynamicStep`\n+class FluxInputsDynamicStep(ModularPipelineBlocks):\n+    model_name = \"flux\"\n+\n+    def __init__(\n+        self,\n+        image_latent_inputs: List[str] = [\"image_latents\"],\n+        additional_batch_inputs: List[str] = [],\n+    ):\n+        if not isinstance(image_latent_inputs, list):\n+            image_latent_inputs = [image_latent_inputs]\n+        if not isinstance(additional_batch_inputs, list):\n+            additional_batch_inputs = [additional_batch_inputs]\n+\n+        self._image_latent_inputs = image_latent_inputs\n+        self._additional_batch_inputs = additional_batch_inputs\n+        super().__init__()\n+\n+    @property\n+    def description(self) -> str:\n+        # Functionality section\n+        summary_section = (\n+            \"Input processing step that:\\n\"\n+            \"  1. For image latent inputs: Updates height/width if None, patchifies latents, and expands batch size\\n\"\n+            \"  2. For additional batch inputs: Expands batch dimensions to match final batch size\"\n+        )\n+\n+        # Inputs info\n+        inputs_info = \"\"\n+        if self._image_latent_inputs or self._additional_batch_inputs:\n+            inputs_info = \"\\n\\nConfigured inputs:\"\n+            if self._image_latent_inputs:\n+                inputs_info += f\"\\n  - Image latent inputs: {self._image_latent_inputs}\"\n+            if self._additional_batch_inputs:\n+                inputs_info += f\"\\n  - Additional batch inputs: {self._additional_batch_inputs}\"\n+\n+        # Placement guidance\n+        placement_section = \"\\n\\nThis block should be placed after the encoder steps and the text input step.\"\n+\n+        return summary_section + inputs_info + placement_section\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        inputs = [\n+            InputParam(name=\"num_images_per_prompt\", default=1),\n+            InputParam(name=\"batch_size\", required=True),\n+            InputParam(name=\"height\"),\n+            InputParam(name=\"width\"),\n+        ]\n+\n+        # Add image latent inputs\n+        for image_latent_input_name in self._image_latent_inputs:\n+            inputs.append(InputParam(name=image_latent_input_name))\n+\n+        # Add additional batch inputs\n+        for input_name in self._additional_batch_inputs:\n+            inputs.append(InputParam(name=input_name))\n+\n+        return inputs\n+\n+    @property\n+    def intermediate_outputs(self) -> List[OutputParam]:\n+        return [\n+            OutputParam(name=\"image_height\", type_hint=int, description=\"The height of the image latents\"),\n+            OutputParam(name=\"image_width\", type_hint=int, description=\"The width of the image latents\"),\n+        ]\n+\n+    def __call__(self, components: FluxModularPipeline, state: PipelineState) -> PipelineState:\n+        block_state = self.get_block_state(state)\n+\n+        # Process image latent inputs (height/width calculation, patchify, and batch expansion)\n+        for image_latent_input_name in self._image_latent_inputs:\n+            image_latent_tensor = getattr(block_state, image_latent_input_name)\n+            if image_latent_tensor is None:\n+                continue\n+\n+            # 1. Calculate height/width from latents\n+            height, width = calculate_dimension_from_latents(image_latent_tensor, components.vae_scale_factor)\n+            block_state.height = block_state.height or height\n+            block_state.width = block_state.width or width\n+\n+            if not hasattr(block_state, \"image_height\"):\n+                block_state.image_height = height\n+            if not hasattr(block_state, \"image_width\"):\n+                block_state.image_width = width\n+\n+            # 2. Patchify the image latent tensor\n+            # TODO: Implement patchifier for Flux.\n+            latent_height, latent_width = image_latent_tensor.shape[2:]\n+            image_latent_tensor = FluxPipeline._pack_latents(\n+                image_latent_tensor, block_state.batch_size, image_latent_tensor.shape[1], latent_height, latent_width\n+            )\n+\n+            # 3. Expand batch size\n+            image_latent_tensor = repeat_tensor_to_batch_size(\n+                input_name=image_latent_input_name,\n+                input_tensor=image_latent_tensor,\n+                num_images_per_prompt=block_state.num_images_per_prompt,\n+                batch_size=block_state.batch_size,\n+            )\n+\n+            setattr(block_state, image_latent_input_name, image_latent_tensor)\n+\n+        # Process additional batch inputs (only batch expansion)\n+        for input_name in self._additional_batch_inputs:\n+            input_tensor = getattr(block_state, input_name)\n+            if input_tensor is None:\n+                continue\n+\n+            # Only expand batch size\n+            input_tensor = repeat_tensor_to_batch_size(\n+                input_name=input_name,\n+                input_tensor=input_tensor,\n+                num_images_per_prompt=block_state.num_images_per_prompt,\n+                batch_size=block_state.batch_size,\n+            )\n+\n+            setattr(block_state, input_name, input_tensor)\n+\n+        self.set_block_state(state, block_state)\n+        return components, state"
        },
        {
          "filename": "src/diffusers/modular_pipelines/flux/modular_blocks.py",
          "status": "modified",
          "additions": 121,
          "deletions": 64,
          "changes": 185,
          "patch": "@@ -18,21 +18,41 @@\n from .before_denoise import (\n     FluxImg2ImgPrepareLatentsStep,\n     FluxImg2ImgSetTimestepsStep,\n-    FluxInputStep,\n     FluxPrepareLatentsStep,\n+    FluxRoPEInputsStep,\n     FluxSetTimestepsStep,\n )\n from .decoders import FluxDecodeStep\n from .denoise import FluxDenoiseStep\n-from .encoders import FluxTextEncoderStep, FluxVaeEncoderStep\n+from .encoders import FluxProcessImagesInputStep, FluxTextEncoderStep, FluxVaeEncoderDynamicStep\n+from .inputs import FluxInputsDynamicStep, FluxTextInputStep\n \n \n logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n \n \n # vae encoder (run before before_denoise)\n+FluxImg2ImgVaeEncoderBlocks = InsertableDict(\n+    [\n+        (\"preprocess\", FluxProcessImagesInputStep()),\n+        (\"encode\", FluxVaeEncoderDynamicStep()),\n+    ]\n+)\n+\n+\n+class FluxImg2ImgVaeEncoderStep(SequentialPipelineBlocks):\n+    model_name = \"flux\"\n+\n+    block_classes = FluxImg2ImgVaeEncoderBlocks.values()\n+    block_names = FluxImg2ImgVaeEncoderBlocks.keys()\n+\n+    @property\n+    def description(self) -> str:\n+        return \"Vae encoder step that preprocess andencode the image inputs into their latent representations.\"\n+\n+\n class FluxAutoVaeEncoderStep(AutoPipelineBlocks):\n-    block_classes = [FluxVaeEncoderStep]\n+    block_classes = [FluxImg2ImgVaeEncoderStep]\n     block_names = [\"img2img\"]\n     block_trigger_inputs = [\"image\"]\n \n@@ -41,45 +61,48 @@ def description(self):\n         return (\n             \"Vae encoder step that encode the image inputs into their latent representations.\\n\"\n             + \"This is an auto pipeline block that works for img2img tasks.\\n\"\n-            + \" - `FluxVaeEncoderStep` (img2img) is used when only `image` is provided.\"\n-            + \" - if `image` is provided, step will be skipped.\"\n+            + \" - `FluxImg2ImgVaeEncoderStep` (img2img) is used when only `image` is provided.\"\n+            + \" - if `image` is not provided, step will be skipped.\"\n         )\n \n \n-# before_denoise: text2img, img2img\n-class FluxBeforeDenoiseStep(SequentialPipelineBlocks):\n-    block_classes = [\n-        FluxInputStep,\n-        FluxPrepareLatentsStep,\n-        FluxSetTimestepsStep,\n+# before_denoise: text2img\n+FluxBeforeDenoiseBlocks = InsertableDict(\n+    [\n+        (\"prepare_latents\", FluxPrepareLatentsStep()),\n+        (\"set_timesteps\", FluxSetTimestepsStep()),\n+        (\"prepare_rope_inputs\", FluxRoPEInputsStep()),\n     ]\n-    block_names = [\"input\", \"prepare_latents\", \"set_timesteps\"]\n+)\n+\n+\n+class FluxBeforeDenoiseStep(SequentialPipelineBlocks):\n+    block_classes = FluxBeforeDenoiseBlocks.values()\n+    block_names = FluxBeforeDenoiseBlocks.keys()\n \n     @property\n     def description(self):\n-        return (\n-            \"Before denoise step that prepare the inputs for the denoise step.\\n\"\n-            + \"This is a sequential pipeline blocks:\\n\"\n-            + \" - `FluxInputStep` is used to adjust the batch size of the model inputs\\n\"\n-            + \" - `FluxPrepareLatentsStep` is used to prepare the latents\\n\"\n-            + \" - `FluxSetTimestepsStep` is used to set the timesteps\\n\"\n-        )\n+        return \"Before denoise step that prepares the inputs for the denoise step in text-to-image generation.\"\n \n \n # before_denoise: img2img\n+FluxImg2ImgBeforeDenoiseBlocks = InsertableDict(\n+    [\n+        (\"prepare_latents\", FluxPrepareLatentsStep()),\n+        (\"set_timesteps\", FluxImg2ImgSetTimestepsStep()),\n+        (\"prepare_img2img_latents\", FluxImg2ImgPrepareLatentsStep()),\n+        (\"prepare_rope_inputs\", FluxRoPEInputsStep()),\n+    ]\n+)\n+\n+\n class FluxImg2ImgBeforeDenoiseStep(SequentialPipelineBlocks):\n-    block_classes = [FluxInputStep, FluxImg2ImgSetTimestepsStep, FluxImg2ImgPrepareLatentsStep]\n-    block_names = [\"input\", \"set_timesteps\", \"prepare_latents\"]\n+    block_classes = FluxImg2ImgBeforeDenoiseBlocks.values()\n+    block_names = FluxImg2ImgBeforeDenoiseBlocks.keys()\n \n     @property\n     def description(self):\n-        return (\n-            \"Before denoise step that prepare the inputs for the denoise step for img2img task.\\n\"\n-            + \"This is a sequential pipeline blocks:\\n\"\n-            + \" - `FluxInputStep` is used to adjust the batch size of the model inputs\\n\"\n-            + \" - `FluxImg2ImgSetTimestepsStep` is used to set the timesteps\\n\"\n-            + \" - `FluxImg2ImgPrepareLatentsStep` is used to prepare the latents\\n\"\n-        )\n+        return \"Before denoise step that prepare the inputs for the denoise step for img2img task.\"\n \n \n # before_denoise: all task (text2img, img2img)\n@@ -113,7 +136,7 @@ def description(self) -> str:\n         )\n \n \n-# decode: all task (text2img, img2img, inpainting)\n+# decode: all task (text2img, img2img)\n class FluxAutoDecodeStep(AutoPipelineBlocks):\n     block_classes = [FluxDecodeStep]\n     block_names = [\"non-inpaint\"]\n@@ -124,32 +147,73 @@ def description(self):\n         return \"Decode step that decode the denoised latents into image outputs.\\n - `FluxDecodeStep`\"\n \n \n+# inputs: text2image/img2img\n+FluxImg2ImgBlocks = InsertableDict(\n+    [(\"text_inputs\", FluxTextInputStep()), (\"additional_inputs\", FluxInputsDynamicStep())]\n+)\n+\n+\n+class FluxImg2ImgInputStep(SequentialPipelineBlocks):\n+    model_name = \"flux\"\n+    block_classes = FluxImg2ImgBlocks.values()\n+    block_names = FluxImg2ImgBlocks.keys()\n+\n+    @property\n+    def description(self):\n+        return \"Input step that prepares the inputs for the img2img denoising step. It:\\n\"\n+        \" - make sure the text embeddings have consistent batch size as well as the additional inputs (`image_latents`).\\n\"\n+        \" - update height/width based `image_latents`, patchify `image_latents`.\"\n+\n+\n+class FluxImageAutoInputStep(AutoPipelineBlocks):\n+    block_classes = [FluxImg2ImgInputStep, FluxTextInputStep]\n+    block_names = [\"img2img\", \"text2image\"]\n+    block_trigger_inputs = [\"image_latents\", None]\n+\n+    @property\n+    def description(self):\n+        return (\n+            \"Input step that standardize the inputs for the denoising step, e.g. make sure inputs have consistent batch size, and patchified. \\n\"\n+            \" This is an auto pipeline block that works for text2image/img2img tasks.\\n\"\n+            + \" - `FluxImg2ImgInputStep` (img2img) is used when `image_latents` is provided.\\n\"\n+            + \" - `FluxTextInputStep` (text2image) is used when `image_latents` are not provided.\\n\"\n+        )\n+\n+\n class FluxCoreDenoiseStep(SequentialPipelineBlocks):\n-    block_classes = [FluxInputStep, FluxAutoBeforeDenoiseStep, FluxAutoDenoiseStep]\n+    model_name = \"flux\"\n+    block_classes = [FluxImageAutoInputStep, FluxAutoBeforeDenoiseStep, FluxAutoDenoiseStep]\n     block_names = [\"input\", \"before_denoise\", \"denoise\"]\n \n     @property\n     def description(self):\n         return (\n             \"Core step that performs the denoising process. \\n\"\n-            + \" - `FluxInputStep` (input) standardizes the inputs for the denoising step.\\n\"\n+            + \" - `FluxImageAutoInputStep` (input) standardizes the inputs for the denoising step.\\n\"\n             + \" - `FluxAutoBeforeDenoiseStep` (before_denoise) prepares the inputs for the denoising step.\\n\"\n             + \" - `FluxAutoDenoiseStep` (denoise) iteratively denoises the latents.\\n\"\n-            + \"This step support text-to-image and image-to-image tasks for Flux:\\n\"\n+            + \"This step supports text-to-image and image-to-image tasks for Flux:\\n\"\n             + \" - for image-to-image generation, you need to provide `image_latents`\\n\"\n-            + \" - for text-to-image generation, all you need to provide is prompt embeddings\"\n+            + \" - for text-to-image generation, all you need to provide is prompt embeddings.\"\n         )\n \n \n-# text2image\n-class FluxAutoBlocks(SequentialPipelineBlocks):\n-    block_classes = [\n-        FluxTextEncoderStep,\n-        FluxAutoVaeEncoderStep,\n-        FluxCoreDenoiseStep,\n-        FluxAutoDecodeStep,\n+# Auto blocks (text2image and img2img)\n+AUTO_BLOCKS = InsertableDict(\n+    [\n+        (\"text_encoder\", FluxTextEncoderStep()),\n+        (\"image_encoder\", FluxAutoVaeEncoderStep()),\n+        (\"denoise\", FluxCoreDenoiseStep()),\n+        (\"decode\", FluxDecodeStep()),\n     ]\n-    block_names = [\"text_encoder\", \"image_encoder\", \"denoise\", \"decode\"]\n+)\n+\n+\n+class FluxAutoBlocks(SequentialPipelineBlocks):\n+    model_name = \"flux\"\n+\n+    block_classes = AUTO_BLOCKS.values()\n+    block_names = AUTO_BLOCKS.keys()\n \n     @property\n     def description(self):\n@@ -162,35 +226,28 @@ def description(self):\n \n TEXT2IMAGE_BLOCKS = InsertableDict(\n     [\n-        (\"text_encoder\", FluxTextEncoderStep),\n-        (\"input\", FluxInputStep),\n-        (\"prepare_latents\", FluxPrepareLatentsStep),\n-        (\"set_timesteps\", FluxSetTimestepsStep),\n-        (\"denoise\", FluxDenoiseStep),\n-        (\"decode\", FluxDecodeStep),\n+        (\"text_encoder\", FluxTextEncoderStep()),\n+        (\"input\", FluxTextInputStep()),\n+        (\"prepare_latents\", FluxPrepareLatentsStep()),\n+        (\"set_timesteps\", FluxSetTimestepsStep()),\n+        (\"prepare_rope_inputs\", FluxRoPEInputsStep()),\n+        (\"denoise\", FluxDenoiseStep()),\n+        (\"decode\", FluxDecodeStep()),\n     ]\n )\n \n IMAGE2IMAGE_BLOCKS = InsertableDict(\n     [\n-        (\"text_encoder\", FluxTextEncoderStep),\n-        (\"image_encoder\", FluxVaeEncoderStep),\n-        (\"input\", FluxInputStep),\n-        (\"set_timesteps\", FluxImg2ImgSetTimestepsStep),\n-        (\"prepare_latents\", FluxImg2ImgPrepareLatentsStep),\n-        (\"denoise\", FluxDenoiseStep),\n-        (\"decode\", FluxDecodeStep),\n+        (\"text_encoder\", FluxTextEncoderStep()),\n+        (\"vae_encoder\", FluxVaeEncoderDynamicStep()),\n+        (\"input\", FluxImg2ImgInputStep()),\n+        (\"prepare_latents\", FluxPrepareLatentsStep()),\n+        (\"set_timesteps\", FluxImg2ImgSetTimestepsStep()),\n+        (\"prepare_img2img_latents\", FluxImg2ImgPrepareLatentsStep()),\n+        (\"prepare_rope_inputs\", FluxRoPEInputsStep()),\n+        (\"denoise\", FluxDenoiseStep()),\n+        (\"decode\", FluxDecodeStep()),\n     ]\n )\n \n-AUTO_BLOCKS = InsertableDict(\n-    [\n-        (\"text_encoder\", FluxTextEncoderStep),\n-        (\"image_encoder\", FluxAutoVaeEncoderStep),\n-        (\"denoise\", FluxCoreDenoiseStep),\n-        (\"decode\", FluxAutoDecodeStep),\n-    ]\n-)\n-\n-\n ALL_BLOCKS = {\"text2image\": TEXT2IMAGE_BLOCKS, \"img2img\": IMAGE2IMAGE_BLOCKS, \"auto\": AUTO_BLOCKS}"
        }
      ],
      "num_files": 5,
      "scraped_at": "2025-11-16T21:19:05.012939"
    },
    {
      "pr_number": 12424,
      "title": "fix dockerfile definitions.",
      "body": "# What does this PR do?\r\n\r\nDocker build is broken from the past two days:\r\nhttps://github.com/huggingface/diffusers/actions/runs/18208988147/job/51845669366#step:5:1173\r\n\r\nOnly changes the Dockerfiles that we use in `.github/workflows/build_docker_images.yml`.",
      "html_url": "https://github.com/huggingface/diffusers/pull/12424",
      "created_at": "2025-10-03T04:16:10Z",
      "merged_at": "2025-10-08T04:16:18Z",
      "merge_commit_sha": "35e538d46a32e6ef588678f478437d594c32f949",
      "base_ref": "main",
      "head_sha": "7a731d7b750dfa352eabeb54680b794f22f8a56e",
      "user": "sayakpaul",
      "files": [
        {
          "filename": ".github/workflows/build_docker_images.yml",
          "status": "modified",
          "additions": 0,
          "deletions": 1,
          "changes": 1,
          "patch": "@@ -72,7 +72,6 @@ jobs:\n         image-name:\n           - diffusers-pytorch-cpu\n           - diffusers-pytorch-cuda\n-          - diffusers-pytorch-cuda\n           - diffusers-pytorch-xformers-cuda\n           - diffusers-pytorch-minimum-cuda\n           - diffusers-doc-builder"
        },
        {
          "filename": ".github/workflows/pr_tests.yml",
          "status": "modified",
          "additions": 0,
          "deletions": 1,
          "changes": 1,
          "patch": "@@ -286,4 +286,3 @@ jobs:\n       with:\n         name: pr_main_test_reports\n         path: reports\n-"
        },
        {
          "filename": "docker/diffusers-doc-builder/Dockerfile",
          "status": "modified",
          "additions": 33,
          "deletions": 47,
          "changes": 80,
          "patch": "@@ -1,56 +1,42 @@\n-FROM ubuntu:20.04\n+FROM python:3.10-slim\n+ENV PYTHONDONTWRITEBYTECODE=1\n LABEL maintainer=\"Hugging Face\"\n LABEL repository=\"diffusers\"\n \n ENV DEBIAN_FRONTEND=noninteractive\n \n-RUN apt-get -y update \\\n-    && apt-get install -y software-properties-common \\\n-    && add-apt-repository ppa:deadsnakes/ppa\n-\n-RUN apt install -y bash \\\n-                   build-essential \\\n-                   git \\\n-                   git-lfs \\\n-                   curl \\\n-                   ca-certificates \\\n-                   libsndfile1-dev \\\n-                   python3.10 \\\n-                   python3-pip \\\n-                   libgl1 \\\n-                   zip \\\n-                   wget \\\n-                   python3.10-venv && \\\n-    rm -rf /var/lib/apt/lists\n-\n-# make sure to use venv\n-RUN python3.10 -m venv /opt/venv\n-ENV PATH=\"/opt/venv/bin:$PATH\"\n+RUN apt-get -y update && apt-get install -y bash \\\n+    build-essential \\\n+    git \\\n+    git-lfs \\\n+    curl \\\n+    ca-certificates \\\n+    libsndfile1-dev \\\n+    libgl1\n+\n+ENV UV_PYTHON=/usr/local/bin/python\n \n # pre-install the heavy dependencies (these can later be overridden by the deps from setup.py)\n-RUN python3.10 -m pip install --no-cache-dir --upgrade pip uv==0.1.11 && \\\n-    python3.10 -m uv pip install --no-cache-dir \\\n-        torch \\\n-        torchvision \\\n-        torchaudio \\\n-        invisible_watermark \\\n-        --extra-index-url https://download.pytorch.org/whl/cpu && \\\n-    python3.10 -m uv pip install --no-cache-dir \\\n-        accelerate \\\n-        datasets \\\n-        hf-doc-builder \\\n-        huggingface-hub \\\n-        Jinja2 \\\n-        librosa \\\n-        numpy==1.26.4 \\\n-        scipy \\\n-        tensorboard \\\n-        transformers \\\n-        matplotlib \\\n-        setuptools==69.5.1 \\\n-        bitsandbytes \\\n-        torchao \\\n-        gguf \\\n-        optimum-quanto\n+RUN pip install uv\n+RUN uv pip install --no-cache-dir \\\n+    torch \\\n+    torchvision \\\n+    torchaudio \\\n+    --extra-index-url https://download.pytorch.org/whl/cpu\n+\n+RUN uv pip install --no-cache-dir \"git+https://github.com/huggingface/diffusers.git@main#egg=diffusers[test]\"\n+\n+# Extra dependencies\n+RUN uv pip install --no-cache-dir \\\n+    accelerate \\\n+    numpy==1.26.4 \\\n+    hf_transfer \\\n+    setuptools==69.5.1 \\\n+    bitsandbytes \\\n+    torchao \\\n+    gguf \\\n+    optimum-quanto\n+\n+RUN apt-get clean && rm -rf /var/lib/apt/lists/* && apt-get autoremove && apt-get autoclean\n \n CMD [\"/bin/bash\"]"
        },
        {
          "filename": "docker/diffusers-pytorch-cpu/Dockerfile",
          "status": "modified",
          "additions": 28,
          "deletions": 41,
          "changes": 69,
          "patch": "@@ -1,50 +1,37 @@\n-FROM ubuntu:20.04\n+FROM python:3.10-slim\n+ENV PYTHONDONTWRITEBYTECODE=1\n LABEL maintainer=\"Hugging Face\"\n LABEL repository=\"diffusers\"\n \n ENV DEBIAN_FRONTEND=noninteractive\n \n-RUN apt-get -y update \\\n-    && apt-get install -y software-properties-common \\\n-    && add-apt-repository ppa:deadsnakes/ppa\n-\n-RUN apt install -y bash \\\n-                   build-essential \\\n-                   git \\\n-                   git-lfs \\\n-                   curl \\\n-                   ca-certificates \\\n-                   libsndfile1-dev \\\n-                   python3.10 \\\n-                   python3.10-dev \\\n-                   python3-pip \\\n-                   libgl1 \\\n-                   python3.10-venv && \\\n-    rm -rf /var/lib/apt/lists\n-\n-# make sure to use venv\n-RUN python3.10 -m venv /opt/venv\n-ENV PATH=\"/opt/venv/bin:$PATH\"\n+RUN apt-get -y update && apt-get install -y bash \\\n+    build-essential \\\n+    git \\\n+    git-lfs \\\n+    curl \\\n+    ca-certificates \\\n+    libsndfile1-dev \\\n+    libgl1\n+\n+ENV UV_PYTHON=/usr/local/bin/python\n \n # pre-install the heavy dependencies (these can later be overridden by the deps from setup.py)\n-RUN python3.10 -m pip install --no-cache-dir --upgrade pip uv==0.1.11 && \\\n-    python3.10 -m uv pip install --no-cache-dir \\\n-        torch \\\n-        torchvision \\\n-        torchaudio \\\n-        invisible_watermark \\\n-        --extra-index-url https://download.pytorch.org/whl/cpu && \\\n-    python3.10 -m uv pip install --no-cache-dir \\\n-        accelerate \\\n-        datasets \\\n-        hf-doc-builder \\\n-        huggingface-hub \\\n-        Jinja2 \\\n-        librosa \\\n-        numpy==1.26.4 \\\n-        scipy \\\n-        tensorboard \\\n-        transformers matplotlib  \\\n-        hf_transfer\n+RUN pip install uv\n+RUN uv pip install --no-cache-dir \\\n+    torch \\\n+    torchvision \\\n+    torchaudio \\\n+    --extra-index-url https://download.pytorch.org/whl/cpu\n+\n+RUN uv pip install --no-cache-dir \"git+https://github.com/huggingface/diffusers.git@main#egg=diffusers[test]\"\n+\n+# Extra dependencies\n+RUN uv pip install --no-cache-dir \\\n+    accelerate \\\n+    numpy==1.26.4 \\\n+    hf_transfer\n+\n+RUN apt-get clean && rm -rf /var/lib/apt/lists/* && apt-get autoremove && apt-get autoclean\n \n CMD [\"/bin/bash\"]"
        },
        {
          "filename": "docker/diffusers-pytorch-cuda/Dockerfile",
          "status": "modified",
          "additions": 20,
          "deletions": 23,
          "changes": 43,
          "patch": "@@ -2,11 +2,13 @@ FROM nvidia/cuda:12.1.0-runtime-ubuntu20.04\n LABEL maintainer=\"Hugging Face\"\n LABEL repository=\"diffusers\"\n \n+ARG PYTHON_VERSION=3.12\n ENV DEBIAN_FRONTEND=noninteractive\n \n RUN apt-get -y update \\\n     && apt-get install -y software-properties-common \\\n-    && add-apt-repository ppa:deadsnakes/ppa\n+    && add-apt-repository ppa:deadsnakes/ppa && \\\n+    apt-get update\n \n RUN apt install -y bash \\\n     build-essential \\\n@@ -16,36 +18,31 @@ RUN apt install -y bash \\\n     ca-certificates \\\n     libsndfile1-dev \\\n     libgl1 \\\n-    python3.10 \\\n-    python3.10-dev \\\n+    python3 \\\n     python3-pip \\\n-    python3.10-venv && \\\n-    rm -rf /var/lib/apt/lists\n+    && apt-get clean \\\n+    && rm -rf /var/lib/apt/lists/*\n \n-# make sure to use venv\n-RUN python3.10 -m venv /opt/venv\n-ENV PATH=\"/opt/venv/bin:$PATH\"\n+RUN curl -LsSf https://astral.sh/uv/install.sh | sh\n+ENV PATH=\"/root/.local/bin:$PATH\"\n+ENV VIRTUAL_ENV=\"/opt/venv\"\n+ENV UV_PYTHON_INSTALL_DIR=/opt/uv/python\n+RUN uv venv --python ${PYTHON_VERSION} --seed ${VIRTUAL_ENV}\n+ENV PATH=\"$VIRTUAL_ENV/bin:$PATH\"\n \n # pre-install the heavy dependencies (these can later be overridden by the deps from setup.py)\n-RUN python3.10 -m pip install --no-cache-dir --upgrade pip uv==0.1.11 && \\\n-    python3.10 -m uv pip install --no-cache-dir \\\n+RUN uv pip install --no-cache-dir \\\n     torch \\\n     torchvision \\\n-    torchaudio \\\n-    invisible_watermark && \\\n-    python3.10 -m pip install --no-cache-dir \\\n+    torchaudio\n+\n+RUN uv pip install --no-cache-dir \"git+https://github.com/huggingface/diffusers.git@main#egg=diffusers[test]\"\n+\n+# Extra dependencies\n+RUN uv pip install --no-cache-dir \\\n     accelerate \\\n-    datasets \\\n-    hf-doc-builder \\\n-    huggingface-hub \\\n-    hf_transfer \\\n-    Jinja2 \\\n-    librosa \\\n     numpy==1.26.4 \\\n-    scipy \\\n-    tensorboard \\\n-    transformers \\\n-    pytorch-lightning  \\\n+    pytorch-lightning \\\n     hf_transfer\n \n CMD [\"/bin/bash\"]"
        },
        {
          "filename": "docker/diffusers-pytorch-minimum-cuda/Dockerfile",
          "status": "modified",
          "additions": 20,
          "deletions": 22,
          "changes": 42,
          "patch": "@@ -2,14 +2,16 @@ FROM nvidia/cuda:12.1.0-runtime-ubuntu20.04\n LABEL maintainer=\"Hugging Face\"\n LABEL repository=\"diffusers\"\n \n+ARG PYTHON_VERSION=3.10\n ENV DEBIAN_FRONTEND=noninteractive\n ENV MINIMUM_SUPPORTED_TORCH_VERSION=\"2.1.0\"\n ENV MINIMUM_SUPPORTED_TORCHVISION_VERSION=\"0.16.0\"\n ENV MINIMUM_SUPPORTED_TORCHAUDIO_VERSION=\"2.1.0\"\n \n RUN apt-get -y update \\\n     && apt-get install -y software-properties-common \\\n-    && add-apt-repository ppa:deadsnakes/ppa\n+    && add-apt-repository ppa:deadsnakes/ppa && \\\n+    apt-get update\n \n RUN apt install -y bash \\\n     build-essential \\\n@@ -19,35 +21,31 @@ RUN apt install -y bash \\\n     ca-certificates \\\n     libsndfile1-dev \\\n     libgl1 \\\n-    python3.10 \\\n-    python3.10-dev \\\n+    python3 \\\n     python3-pip \\\n-    python3.10-venv && \\\n-    rm -rf /var/lib/apt/lists\n+    && apt-get clean \\\n+    && rm -rf /var/lib/apt/lists/*\n \n-# make sure to use venv\n-RUN python3.10 -m venv /opt/venv\n-ENV PATH=\"/opt/venv/bin:$PATH\"\n+RUN curl -LsSf https://astral.sh/uv/install.sh | sh\n+ENV PATH=\"/root/.local/bin:$PATH\"\n+ENV VIRTUAL_ENV=\"/opt/venv\"\n+ENV UV_PYTHON_INSTALL_DIR=/opt/uv/python\n+RUN uv venv --python ${PYTHON_VERSION} --seed ${VIRTUAL_ENV}\n+ENV PATH=\"$VIRTUAL_ENV/bin:$PATH\"\n \n # pre-install the heavy dependencies (these can later be overridden by the deps from setup.py)\n-RUN python3.10 -m pip install --no-cache-dir --upgrade pip uv==0.1.11 && \\\n-    python3.10 -m uv pip install --no-cache-dir \\\n+RUN uv pip install --no-cache-dir \\\n     torch==$MINIMUM_SUPPORTED_TORCH_VERSION \\\n     torchvision==$MINIMUM_SUPPORTED_TORCHVISION_VERSION \\\n-    torchaudio==$MINIMUM_SUPPORTED_TORCHAUDIO_VERSION \\\n-    invisible_watermark && \\\n-    python3.10 -m pip install --no-cache-dir \\\n+    torchaudio==$MINIMUM_SUPPORTED_TORCHAUDIO_VERSION\n+\n+RUN uv pip install --no-cache-dir \"git+https://github.com/huggingface/diffusers.git@main#egg=diffusers[test]\"\n+\n+# Extra dependencies\n+RUN uv pip install --no-cache-dir \\\n     accelerate \\\n-    datasets \\\n-    hf-doc-builder \\\n-    huggingface-hub \\\n-    hf_transfer \\\n-    Jinja2 \\\n-    librosa \\\n     numpy==1.26.4 \\\n-    scipy \\\n-    tensorboard \\\n-    transformers \\\n+    pytorch-lightning \\\n     hf_transfer\n \n CMD [\"/bin/bash\"]"
        },
        {
          "filename": "docker/diffusers-pytorch-xformers-cuda/Dockerfile",
          "status": "modified",
          "additions": 35,
          "deletions": 37,
          "changes": 72,
          "patch": "@@ -2,50 +2,48 @@ FROM nvidia/cuda:12.1.0-runtime-ubuntu20.04\n LABEL maintainer=\"Hugging Face\"\n LABEL repository=\"diffusers\"\n \n+ARG PYTHON_VERSION=3.12\n ENV DEBIAN_FRONTEND=noninteractive\n \n RUN apt-get -y update \\\n     && apt-get install -y software-properties-common \\\n-    && add-apt-repository ppa:deadsnakes/ppa\n+    && add-apt-repository ppa:deadsnakes/ppa && \\\n+    apt-get update\n \n RUN apt install -y bash \\\n-                   build-essential \\\n-                   git \\\n-                   git-lfs \\\n-                   curl \\\n-                   ca-certificates \\\n-                   libsndfile1-dev \\\n-                   libgl1 \\\n-                   python3.10 \\\n-                   python3.10-dev \\\n-                   python3-pip \\\n-                   python3.10-venv && \\\n-    rm -rf /var/lib/apt/lists\n-\n-# make sure to use venv\n-RUN python3.10 -m venv /opt/venv\n-ENV PATH=\"/opt/venv/bin:$PATH\"\n+    build-essential \\\n+    git \\\n+    git-lfs \\\n+    curl \\\n+    ca-certificates \\\n+    libsndfile1-dev \\\n+    libgl1 \\\n+    python3 \\\n+    python3-pip \\\n+    && apt-get clean \\\n+    && rm -rf /var/lib/apt/lists/*\n+\n+RUN curl -LsSf https://astral.sh/uv/install.sh | sh\n+ENV PATH=\"/root/.local/bin:$PATH\"\n+ENV VIRTUAL_ENV=\"/opt/venv\"\n+ENV UV_PYTHON_INSTALL_DIR=/opt/uv/python\n+RUN uv venv --python ${PYTHON_VERSION} --seed ${VIRTUAL_ENV}\n+ENV PATH=\"$VIRTUAL_ENV/bin:$PATH\"\n \n # pre-install the heavy dependencies (these can later be overridden by the deps from setup.py)\n-RUN python3.10 -m pip install --no-cache-dir --upgrade pip uv==0.1.11 && \\\n-    python3.10 -m pip install --no-cache-dir \\\n-        torch \\\n-        torchvision \\\n-        torchaudio \\\n-        invisible_watermark && \\\n-    python3.10 -m uv pip install --no-cache-dir \\\n-        accelerate \\\n-        datasets \\\n-        hf-doc-builder \\\n-        huggingface-hub \\\n-        hf_transfer \\\n-        Jinja2 \\\n-        librosa \\\n-        numpy==1.26.4 \\\n-        scipy \\\n-        tensorboard \\\n-        transformers \\\n-        xformers  \\\n-        hf_transfer\n+RUN uv pip install --no-cache-dir \\\n+    torch \\\n+    torchvision \\\n+    torchaudio\n+\n+RUN uv pip install --no-cache-dir \"git+https://github.com/huggingface/diffusers.git@main#egg=diffusers[test]\"\n+\n+# Extra dependencies\n+RUN uv pip install --no-cache-dir \\\n+    accelerate \\\n+    numpy==1.26.4 \\\n+    pytorch-lightning \\\n+    hf_transfer \\\n+    xformers\n \n CMD [\"/bin/bash\"]"
        }
      ],
      "num_files": 7,
      "scraped_at": "2025-11-16T21:19:09.849406"
    },
    {
      "pr_number": 12416,
      "title": "[core] support QwenImage Edit Plus in modular",
      "body": "# What does this PR do?\r\n\r\n<details>\r\n<summary>Test code:</summary>\r\n\r\n```py\r\nimport torch\r\nfrom diffusers import ModularPipeline\r\nfrom diffusers.utils import load_image\r\n\r\n# repo_id = \"Qwen/Qwen-Image-Edit\"\r\nrepo_id = \"Qwen/Qwen-Image-Edit-2509\"\r\n\r\npipeline = ModularPipeline.from_pretrained(repo_id)\r\npipeline.load_components(torch_dtype=torch.bfloat16)\r\npipeline.to(\"cuda\")\r\n\r\nguider_spec = pipeline.get_component_spec(\"guider\")\r\nguider = guider_spec.create(guidance_scale=4.5)\r\npipeline.update_components(guider=guider)\r\n\r\nimage = load_image(\r\n    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/yarn-art-pikachu.png\"\r\n).convert(\"RGB\")\r\nprompt = (\r\n    \"Make Pikachu hold a sign that says 'Qwen is awesome', yarn art style, detailed, vibrant colors\"\r\n)\r\nimage = pipeline(\r\n    image=image, \r\n    prompt=prompt, \r\n    negative_prompt=\" \",\r\n    num_inference_steps=40,\r\n    generator=torch.manual_seed(0),\r\n).images[0]\r\nimage.save(\"qwenimage_edit_plus_modular.png\")\r\n```\r\n\r\n</details>\r\n\r\nResult:\r\n\r\n<img width=\"1024\" height=\"1024\" alt=\"image\" src=\"https://github.com/user-attachments/assets/e2c94afb-bfc4-4b2e-a49a-d0ab43325387\" />",
      "html_url": "https://github.com/huggingface/diffusers/pull/12416",
      "created_at": "2025-10-01T15:15:36Z",
      "merged_at": "2025-10-05T16:27:13Z",
      "merge_commit_sha": "c3675d4c9bb9c02521cd2c1aec198460c1657256",
      "base_ref": "main",
      "head_sha": "8359fa1015aaa376f34ed6481b24969520e91cad",
      "user": "sayakpaul",
      "files": [
        {
          "filename": "src/diffusers/__init__.py",
          "status": "modified",
          "additions": 4,
          "deletions": 0,
          "changes": 4,
          "patch": "@@ -390,6 +390,8 @@\n             \"QwenImageAutoBlocks\",\n             \"QwenImageEditAutoBlocks\",\n             \"QwenImageEditModularPipeline\",\n+            \"QwenImageEditPlusAutoBlocks\",\n+            \"QwenImageEditPlusModularPipeline\",\n             \"QwenImageModularPipeline\",\n             \"StableDiffusionXLAutoBlocks\",\n             \"StableDiffusionXLModularPipeline\",\n@@ -1052,6 +1054,8 @@\n             QwenImageAutoBlocks,\n             QwenImageEditAutoBlocks,\n             QwenImageEditModularPipeline,\n+            QwenImageEditPlusAutoBlocks,\n+            QwenImageEditPlusModularPipeline,\n             QwenImageModularPipeline,\n             StableDiffusionXLAutoBlocks,\n             StableDiffusionXLModularPipeline,"
        },
        {
          "filename": "src/diffusers/modular_pipelines/__init__.py",
          "status": "modified",
          "additions": 4,
          "deletions": 0,
          "changes": 4,
          "patch": "@@ -52,6 +52,8 @@\n         \"QwenImageModularPipeline\",\n         \"QwenImageEditModularPipeline\",\n         \"QwenImageEditAutoBlocks\",\n+        \"QwenImageEditPlusModularPipeline\",\n+        \"QwenImageEditPlusAutoBlocks\",\n     ]\n     _import_structure[\"components_manager\"] = [\"ComponentsManager\"]\n \n@@ -78,6 +80,8 @@\n             QwenImageAutoBlocks,\n             QwenImageEditAutoBlocks,\n             QwenImageEditModularPipeline,\n+            QwenImageEditPlusAutoBlocks,\n+            QwenImageEditPlusModularPipeline,\n             QwenImageModularPipeline,\n         )\n         from .stable_diffusion_xl import StableDiffusionXLAutoBlocks, StableDiffusionXLModularPipeline"
        },
        {
          "filename": "src/diffusers/modular_pipelines/modular_pipeline.py",
          "status": "modified",
          "additions": 3,
          "deletions": 1,
          "changes": 4,
          "patch": "@@ -59,6 +59,7 @@\n         (\"flux\", \"FluxModularPipeline\"),\n         (\"qwenimage\", \"QwenImageModularPipeline\"),\n         (\"qwenimage-edit\", \"QwenImageEditModularPipeline\"),\n+        (\"qwenimage-edit-plus\", \"QwenImageEditPlusModularPipeline\"),\n     ]\n )\n \n@@ -1628,7 +1629,8 @@ def from_pretrained(\n             blocks = ModularPipelineBlocks.from_pretrained(\n                 pretrained_model_name_or_path, trust_remote_code=trust_remote_code, **kwargs\n             )\n-        except EnvironmentError:\n+        except EnvironmentError as e:\n+            logger.debug(f\"EnvironmentError: {e}\")\n             blocks = None\n \n         cache_dir = kwargs.pop(\"cache_dir\", None)"
        },
        {
          "filename": "src/diffusers/modular_pipelines/qwenimage/__init__.py",
          "status": "modified",
          "additions": 16,
          "deletions": 2,
          "changes": 18,
          "patch": "@@ -29,13 +29,20 @@\n         \"EDIT_AUTO_BLOCKS\",\n         \"EDIT_BLOCKS\",\n         \"EDIT_INPAINT_BLOCKS\",\n+        \"EDIT_PLUS_AUTO_BLOCKS\",\n+        \"EDIT_PLUS_BLOCKS\",\n         \"IMAGE2IMAGE_BLOCKS\",\n         \"INPAINT_BLOCKS\",\n         \"TEXT2IMAGE_BLOCKS\",\n         \"QwenImageAutoBlocks\",\n         \"QwenImageEditAutoBlocks\",\n+        \"QwenImageEditPlusAutoBlocks\",\n+    ]\n+    _import_structure[\"modular_pipeline\"] = [\n+        \"QwenImageEditModularPipeline\",\n+        \"QwenImageEditPlusModularPipeline\",\n+        \"QwenImageModularPipeline\",\n     ]\n-    _import_structure[\"modular_pipeline\"] = [\"QwenImageEditModularPipeline\", \"QwenImageModularPipeline\"]\n \n if TYPE_CHECKING or DIFFUSERS_SLOW_IMPORT:\n     try:\n@@ -54,13 +61,20 @@\n             EDIT_AUTO_BLOCKS,\n             EDIT_BLOCKS,\n             EDIT_INPAINT_BLOCKS,\n+            EDIT_PLUS_AUTO_BLOCKS,\n+            EDIT_PLUS_BLOCKS,\n             IMAGE2IMAGE_BLOCKS,\n             INPAINT_BLOCKS,\n             TEXT2IMAGE_BLOCKS,\n             QwenImageAutoBlocks,\n             QwenImageEditAutoBlocks,\n+            QwenImageEditPlusAutoBlocks,\n+        )\n+        from .modular_pipeline import (\n+            QwenImageEditModularPipeline,\n+            QwenImageEditPlusModularPipeline,\n+            QwenImageModularPipeline,\n         )\n-        from .modular_pipeline import QwenImageEditModularPipeline, QwenImageModularPipeline\n else:\n     import sys\n "
        },
        {
          "filename": "src/diffusers/modular_pipelines/qwenimage/before_denoise.py",
          "status": "modified",
          "additions": 1,
          "deletions": 2,
          "changes": 3,
          "patch": "@@ -203,7 +203,6 @@ def __call__(self, components: QwenImageModularPipeline, state: PipelineState) -\n         block_state.latents = components.pachifier.pack_latents(block_state.latents)\n \n         self.set_block_state(state, block_state)\n-\n         return components, state\n \n \n@@ -571,7 +570,7 @@ class QwenImageEditRoPEInputsStep(ModularPipelineBlocks):\n \n     @property\n     def description(self) -> str:\n-        return \"Step that prepares the RoPE inputs for denoising process. This is used in QwenImage Edit. Should be place after prepare_latents step\"\n+        return \"Step that prepares the RoPE inputs for denoising process. This is used in QwenImage Edit. Should be placed after prepare_latents step\"\n \n     @property\n     def inputs(self) -> List[InputParam]:"
        },
        {
          "filename": "src/diffusers/modular_pipelines/qwenimage/encoders.py",
          "status": "modified",
          "additions": 229,
          "deletions": 7,
          "changes": 236,
          "patch": "@@ -128,6 +128,61 @@ def get_qwen_prompt_embeds_edit(\n     return prompt_embeds, encoder_attention_mask\n \n \n+def get_qwen_prompt_embeds_edit_plus(\n+    text_encoder,\n+    processor,\n+    prompt: Union[str, List[str]] = None,\n+    image: Optional[Union[torch.Tensor, List[PIL.Image.Image], PIL.Image.Image]] = None,\n+    prompt_template_encode: str = \"<|im_start|>system\\nDescribe the key features of the input image (color, shape, size, texture, objects, background), then explain how the user's text instruction should alter or modify the image. Generate a new image that meets the user's requirements while maintaining consistency with the original input where appropriate.<|im_end|>\\n<|im_start|>user\\n{}<|im_end|>\\n<|im_start|>assistant\\n\",\n+    img_template_encode: str = \"Picture {}: <|vision_start|><|image_pad|><|vision_end|>\",\n+    prompt_template_encode_start_idx: int = 64,\n+    device: Optional[torch.device] = None,\n+):\n+    prompt = [prompt] if isinstance(prompt, str) else prompt\n+    if isinstance(image, list):\n+        base_img_prompt = \"\"\n+        for i, img in enumerate(image):\n+            base_img_prompt += img_template_encode.format(i + 1)\n+    elif image is not None:\n+        base_img_prompt = img_template_encode.format(1)\n+    else:\n+        base_img_prompt = \"\"\n+\n+    template = prompt_template_encode\n+\n+    drop_idx = prompt_template_encode_start_idx\n+    txt = [template.format(base_img_prompt + e) for e in prompt]\n+\n+    model_inputs = processor(\n+        text=txt,\n+        images=image,\n+        padding=True,\n+        return_tensors=\"pt\",\n+    ).to(device)\n+    outputs = text_encoder(\n+        input_ids=model_inputs.input_ids,\n+        attention_mask=model_inputs.attention_mask,\n+        pixel_values=model_inputs.pixel_values,\n+        image_grid_thw=model_inputs.image_grid_thw,\n+        output_hidden_states=True,\n+    )\n+\n+    hidden_states = outputs.hidden_states[-1]\n+    split_hidden_states = _extract_masked_hidden(hidden_states, model_inputs.attention_mask)\n+    split_hidden_states = [e[drop_idx:] for e in split_hidden_states]\n+    attn_mask_list = [torch.ones(e.size(0), dtype=torch.long, device=e.device) for e in split_hidden_states]\n+    max_seq_len = max([e.size(0) for e in split_hidden_states])\n+    prompt_embeds = torch.stack(\n+        [torch.cat([u, u.new_zeros(max_seq_len - u.size(0), u.size(1))]) for u in split_hidden_states]\n+    )\n+    encoder_attention_mask = torch.stack(\n+        [torch.cat([u, u.new_zeros(max_seq_len - u.size(0))]) for u in attn_mask_list]\n+    )\n+\n+    prompt_embeds = prompt_embeds.to(device=device)\n+    return prompt_embeds, encoder_attention_mask\n+\n+\n # Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_img2img.retrieve_latents\n def retrieve_latents(\n     encoder_output: torch.Tensor, generator: Optional[torch.Generator] = None, sample_mode: str = \"sample\"\n@@ -266,6 +321,83 @@ def __call__(self, components: QwenImageModularPipeline, state: PipelineState):\n         return components, state\n \n \n+class QwenImageEditPlusResizeDynamicStep(QwenImageEditResizeDynamicStep):\n+    model_name = \"qwenimage\"\n+\n+    def __init__(\n+        self,\n+        input_name: str = \"image\",\n+        output_name: str = \"resized_image\",\n+        vae_image_output_name: str = \"vae_image\",\n+    ):\n+        \"\"\"Create a configurable step for resizing images to the target area (1024 * 1024) while maintaining the aspect ratio.\n+\n+        This block resizes an input image or a list input images and exposes the resized result under configurable\n+        input and output names. Use this when you need to wire the resize step to different image fields (e.g.,\n+        \"image\", \"control_image\")\n+\n+        Args:\n+            input_name (str, optional): Name of the image field to read from the\n+                pipeline state. Defaults to \"image\".\n+            output_name (str, optional): Name of the resized image field to write\n+                back to the pipeline state. Defaults to \"resized_image\".\n+            vae_image_output_name (str, optional): Name of the image field\n+                to write back to the pipeline state. This is used by the VAE encoder step later on. QwenImage Edit Plus\n+                processes the input image(s) differently for the VL and the VAE.\n+        \"\"\"\n+        if not isinstance(input_name, str) or not isinstance(output_name, str):\n+            raise ValueError(\n+                f\"input_name and output_name must be strings but are {type(input_name)} and {type(output_name)}\"\n+            )\n+        self.condition_image_size = 384 * 384\n+        self._image_input_name = input_name\n+        self._resized_image_output_name = output_name\n+        self._vae_image_output_name = vae_image_output_name\n+        super().__init__()\n+\n+    @property\n+    def intermediate_outputs(self) -> List[OutputParam]:\n+        return super().intermediate_outputs + [\n+            OutputParam(\n+                name=self._vae_image_output_name,\n+                type_hint=List[PIL.Image.Image],\n+                description=\"The images to be processed which will be further used by the VAE encoder.\",\n+            ),\n+        ]\n+\n+    @torch.no_grad()\n+    def __call__(self, components: QwenImageModularPipeline, state: PipelineState):\n+        block_state = self.get_block_state(state)\n+\n+        images = getattr(block_state, self._image_input_name)\n+\n+        if not is_valid_image_imagelist(images):\n+            raise ValueError(f\"Images must be image or list of images but are {type(images)}\")\n+\n+        if (\n+            not isinstance(images, torch.Tensor)\n+            and isinstance(images, PIL.Image.Image)\n+            and not isinstance(images, list)\n+        ):\n+            images = [images]\n+\n+        # TODO (sayakpaul): revisit this when the inputs are `torch.Tensor`s\n+        condition_images = []\n+        vae_images = []\n+        for img in images:\n+            image_width, image_height = img.size\n+            condition_width, condition_height, _ = calculate_dimensions(\n+                self.condition_image_size, image_width / image_height\n+            )\n+            condition_images.append(components.image_resize_processor.resize(img, condition_height, condition_width))\n+            vae_images.append(img)\n+\n+        setattr(block_state, self._resized_image_output_name, condition_images)\n+        setattr(block_state, self._vae_image_output_name, vae_images)\n+        self.set_block_state(state, block_state)\n+        return components, state\n+\n+\n class QwenImageTextEncoderStep(ModularPipelineBlocks):\n     model_name = \"qwenimage\"\n \n@@ -511,6 +643,61 @@ def __call__(self, components: QwenImageModularPipeline, state: PipelineState):\n         return components, state\n \n \n+class QwenImageEditPlusTextEncoderStep(QwenImageEditTextEncoderStep):\n+    model_name = \"qwenimage\"\n+\n+    @property\n+    def expected_configs(self) -> List[ConfigSpec]:\n+        return [\n+            ConfigSpec(\n+                name=\"prompt_template_encode\",\n+                default=\"<|im_start|>system\\nDescribe the key features of the input image (color, shape, size, texture, objects, background), then explain how the user's text instruction should alter or modify the image. Generate a new image that meets the user's requirements while maintaining consistency with the original input where appropriate.<|im_end|>\\n<|im_start|>user\\n{}<|im_end|>\\n<|im_start|>assistant\\n\",\n+            ),\n+            ConfigSpec(\n+                name=\"img_template_encode\",\n+                default=\"Picture {}: <|vision_start|><|image_pad|><|vision_end|>\",\n+            ),\n+            ConfigSpec(name=\"prompt_template_encode_start_idx\", default=64),\n+        ]\n+\n+    @torch.no_grad()\n+    def __call__(self, components: QwenImageModularPipeline, state: PipelineState):\n+        block_state = self.get_block_state(state)\n+\n+        self.check_inputs(block_state.prompt, block_state.negative_prompt)\n+\n+        device = components._execution_device\n+\n+        block_state.prompt_embeds, block_state.prompt_embeds_mask = get_qwen_prompt_embeds_edit_plus(\n+            components.text_encoder,\n+            components.processor,\n+            prompt=block_state.prompt,\n+            image=block_state.resized_image,\n+            prompt_template_encode=components.config.prompt_template_encode,\n+            img_template_encode=components.config.img_template_encode,\n+            prompt_template_encode_start_idx=components.config.prompt_template_encode_start_idx,\n+            device=device,\n+        )\n+\n+        if components.requires_unconditional_embeds:\n+            negative_prompt = block_state.negative_prompt or \" \"\n+            block_state.negative_prompt_embeds, block_state.negative_prompt_embeds_mask = (\n+                get_qwen_prompt_embeds_edit_plus(\n+                    components.text_encoder,\n+                    components.processor,\n+                    prompt=negative_prompt,\n+                    image=block_state.resized_image,\n+                    prompt_template_encode=components.config.prompt_template_encode,\n+                    img_template_encode=components.config.img_template_encode,\n+                    prompt_template_encode_start_idx=components.config.prompt_template_encode_start_idx,\n+                    device=device,\n+                )\n+            )\n+\n+        self.set_block_state(state, block_state)\n+        return components, state\n+\n+\n class QwenImageInpaintProcessImagesInputStep(ModularPipelineBlocks):\n     model_name = \"qwenimage\"\n \n@@ -612,12 +799,7 @@ def expected_components(self) -> List[ComponentSpec]:\n \n     @property\n     def inputs(self) -> List[InputParam]:\n-        return [\n-            InputParam(\"resized_image\"),\n-            InputParam(\"image\"),\n-            InputParam(\"height\"),\n-            InputParam(\"width\"),\n-        ]\n+        return [InputParam(\"resized_image\"), InputParam(\"image\"), InputParam(\"height\"), InputParam(\"width\")]\n \n     @property\n     def intermediate_outputs(self) -> List[OutputParam]:\n@@ -661,6 +843,47 @@ def __call__(self, components: QwenImageModularPipeline, state: PipelineState):\n         return components, state\n \n \n+class QwenImageEditPlusProcessImagesInputStep(QwenImageProcessImagesInputStep):\n+    model_name = \"qwenimage-edit-plus\"\n+    vae_image_size = 1024 * 1024\n+\n+    @property\n+    def description(self) -> str:\n+        return \"Image Preprocess step for QwenImage Edit Plus. Unlike QwenImage Edit, QwenImage Edit Plus doesn't use the same resized image for further preprocessing.\"\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        return [InputParam(\"vae_image\"), InputParam(\"image\"), InputParam(\"height\"), InputParam(\"width\")]\n+\n+    @torch.no_grad()\n+    def __call__(self, components: QwenImageModularPipeline, state: PipelineState):\n+        block_state = self.get_block_state(state)\n+\n+        if block_state.vae_image is None and block_state.image is None:\n+            raise ValueError(\"`vae_image` and `image` cannot be None at the same time\")\n+\n+        if block_state.vae_image is None:\n+            image = block_state.image\n+            self.check_inputs(\n+                height=block_state.height, width=block_state.width, vae_scale_factor=components.vae_scale_factor\n+            )\n+            height = block_state.height or components.default_height\n+            width = block_state.width or components.default_width\n+            block_state.processed_image = components.image_processor.preprocess(\n+                image=image, height=height, width=width\n+            )\n+        else:\n+            width, height = block_state.vae_image[0].size\n+            image = block_state.vae_image\n+\n+            block_state.processed_image = components.image_processor.preprocess(\n+                image=image, height=height, width=width\n+            )\n+\n+        self.set_block_state(state, block_state)\n+        return components, state\n+\n+\n class QwenImageVaeEncoderDynamicStep(ModularPipelineBlocks):\n     model_name = \"qwenimage\"\n \n@@ -738,7 +961,6 @@ def __call__(self, components: QwenImageModularPipeline, state: PipelineState) -\n             dtype=dtype,\n             latent_channels=components.num_channels_latents,\n         )\n-\n         setattr(block_state, self._image_latents_output_name, image_latents)\n \n         self.set_block_state(state, block_state)"
        },
        {
          "filename": "src/diffusers/modular_pipelines/qwenimage/modular_blocks.py",
          "status": "modified",
          "additions": 150,
          "deletions": 1,
          "changes": 151,
          "patch": "@@ -37,6 +37,9 @@\n )\n from .encoders import (\n     QwenImageControlNetVaeEncoderStep,\n+    QwenImageEditPlusProcessImagesInputStep,\n+    QwenImageEditPlusResizeDynamicStep,\n+    QwenImageEditPlusTextEncoderStep,\n     QwenImageEditResizeDynamicStep,\n     QwenImageEditTextEncoderStep,\n     QwenImageInpaintProcessImagesInputStep,\n@@ -872,16 +875,162 @@ def description(self):\n         )\n \n \n-# 3. all block presets supported in QwenImage & QwenImage-Edit\n+#################### QwenImage Edit Plus #####################\n+\n+# 3. QwenImage-Edit Plus\n+\n+## 3.1 QwenImage-Edit Plus / edit\n+\n+#### QwenImage-Edit Plus vl encoder: take both image and text prompts\n+QwenImageEditPlusVLEncoderBlocks = InsertableDict(\n+    [\n+        (\"resize\", QwenImageEditPlusResizeDynamicStep()),\n+        (\"encode\", QwenImageEditPlusTextEncoderStep()),\n+    ]\n+)\n+\n+\n+class QwenImageEditPlusVLEncoderStep(SequentialPipelineBlocks):\n+    model_name = \"qwenimage\"\n+    block_classes = QwenImageEditPlusVLEncoderBlocks.values()\n+    block_names = QwenImageEditPlusVLEncoderBlocks.keys()\n+\n+    @property\n+    def description(self) -> str:\n+        return \"QwenImage-Edit Plus VL encoder step that encode the image an text prompts together.\"\n+\n+\n+#### QwenImage-Edit Plus vae encoder\n+QwenImageEditPlusVaeEncoderBlocks = InsertableDict(\n+    [\n+        (\"resize\", QwenImageEditPlusResizeDynamicStep()),  # edit plus has a different resize step\n+        (\"preprocess\", QwenImageEditPlusProcessImagesInputStep()),  # vae_image -> processed_image\n+        (\"encode\", QwenImageVaeEncoderDynamicStep()),  # processed_image -> image_latents\n+    ]\n+)\n+\n+\n+class QwenImageEditPlusVaeEncoderStep(SequentialPipelineBlocks):\n+    model_name = \"qwenimage\"\n+    block_classes = QwenImageEditPlusVaeEncoderBlocks.values()\n+    block_names = QwenImageEditPlusVaeEncoderBlocks.keys()\n+\n+    @property\n+    def description(self) -> str:\n+        return \"Vae encoder step that encode the image inputs into their latent representations.\"\n+\n+\n+#### QwenImage Edit Plus presets\n+EDIT_PLUS_BLOCKS = InsertableDict(\n+    [\n+        (\"text_encoder\", QwenImageEditPlusVLEncoderStep()),\n+        (\"vae_encoder\", QwenImageEditPlusVaeEncoderStep()),\n+        (\"input\", QwenImageEditInputStep()),\n+        (\"prepare_latents\", QwenImagePrepareLatentsStep()),\n+        (\"set_timesteps\", QwenImageSetTimestepsStep()),\n+        (\"prepare_rope_inputs\", QwenImageEditRoPEInputsStep()),\n+        (\"denoise\", QwenImageEditDenoiseStep()),\n+        (\"decode\", QwenImageDecodeStep()),\n+    ]\n+)\n+\n+\n+# auto before_denoise step for edit tasks\n+class QwenImageEditPlusAutoBeforeDenoiseStep(AutoPipelineBlocks):\n+    model_name = \"qwenimage-edit-plus\"\n+    block_classes = [QwenImageEditBeforeDenoiseStep]\n+    block_names = [\"edit\"]\n+    block_trigger_inputs = [\"image_latents\"]\n+\n+    @property\n+    def description(self):\n+        return (\n+            \"Before denoise step that prepare the inputs (timesteps, latents, rope inputs etc.) for the denoise step.\\n\"\n+            + \"This is an auto pipeline block that works for edit (img2img) task.\\n\"\n+            + \" - `QwenImageEditBeforeDenoiseStep` (edit) is used when `image_latents` is provided and `processed_mask_image` is not provided.\\n\"\n+            + \" - if `image_latents` is not provided, step will be skipped.\"\n+        )\n+\n+\n+## 3.2 QwenImage-Edit Plus/auto encoders\n+\n+\n+class QwenImageEditPlusAutoVaeEncoderStep(AutoPipelineBlocks):\n+    block_classes = [\n+        QwenImageEditPlusVaeEncoderStep,\n+    ]\n+    block_names = [\"edit\"]\n+    block_trigger_inputs = [\"image\"]\n+\n+    @property\n+    def description(self):\n+        return (\n+            \"Vae encoder step that encode the image inputs into their latent representations. \\n\"\n+            \" This is an auto pipeline block that works for edit task.\\n\"\n+            + \" - `QwenImageEditPlusVaeEncoderStep` (edit) is used when `image` is provided.\\n\"\n+            + \" - if `image` is not provided, step will be skipped.\"\n+        )\n+\n+\n+## 3.3 QwenImage-Edit/auto blocks & presets\n+\n+\n+class QwenImageEditPlusCoreDenoiseStep(SequentialPipelineBlocks):\n+    model_name = \"qwenimage-edit-plus\"\n+    block_classes = [\n+        QwenImageEditAutoInputStep,\n+        QwenImageEditPlusAutoBeforeDenoiseStep,\n+        QwenImageEditAutoDenoiseStep,\n+    ]\n+    block_names = [\"input\", \"before_denoise\", \"denoise\"]\n+\n+    @property\n+    def description(self):\n+        return (\n+            \"Core step that performs the denoising process. \\n\"\n+            + \" - `QwenImageEditAutoInputStep` (input) standardizes the inputs for the denoising step.\\n\"\n+            + \" - `QwenImageEditPlusAutoBeforeDenoiseStep` (before_denoise) prepares the inputs for the denoising step.\\n\"\n+            + \" - `QwenImageEditAutoDenoiseStep` (denoise) iteratively denoises the latents.\\n\\n\"\n+            + \"This step support edit (img2img) workflow for QwenImage Edit Plus:\\n\"\n+            + \" - When `image_latents` is provided, it will be used for edit (img2img) task.\\n\"\n+        )\n+\n+\n+EDIT_PLUS_AUTO_BLOCKS = InsertableDict(\n+    [\n+        (\"text_encoder\", QwenImageEditPlusVLEncoderStep()),\n+        (\"vae_encoder\", QwenImageEditPlusAutoVaeEncoderStep()),\n+        (\"denoise\", QwenImageEditPlusCoreDenoiseStep()),\n+        (\"decode\", QwenImageAutoDecodeStep()),\n+    ]\n+)\n+\n+\n+class QwenImageEditPlusAutoBlocks(SequentialPipelineBlocks):\n+    model_name = \"qwenimage-edit-plus\"\n+    block_classes = EDIT_PLUS_AUTO_BLOCKS.values()\n+    block_names = EDIT_PLUS_AUTO_BLOCKS.keys()\n+\n+    @property\n+    def description(self):\n+        return (\n+            \"Auto Modular pipeline for edit (img2img) and edit tasks using QwenImage-Edit Plus.\\n\"\n+            + \"- for edit (img2img) generation, you need to provide `image`\\n\"\n+        )\n+\n+\n+# 3. all block presets supported in QwenImage, QwenImage-Edit, QwenImage-Edit Plus\n \n \n ALL_BLOCKS = {\n     \"text2image\": TEXT2IMAGE_BLOCKS,\n     \"img2img\": IMAGE2IMAGE_BLOCKS,\n     \"edit\": EDIT_BLOCKS,\n     \"edit_inpaint\": EDIT_INPAINT_BLOCKS,\n+    \"edit_plus\": EDIT_PLUS_BLOCKS,\n     \"inpaint\": INPAINT_BLOCKS,\n     \"controlnet\": CONTROLNET_BLOCKS,\n     \"auto\": AUTO_BLOCKS,\n     \"edit_auto\": EDIT_AUTO_BLOCKS,\n+    \"edit_plus_auto\": EDIT_PLUS_AUTO_BLOCKS,\n }"
        },
        {
          "filename": "src/diffusers/modular_pipelines/qwenimage/modular_pipeline.py",
          "status": "modified",
          "additions": 10,
          "deletions": 0,
          "changes": 10,
          "patch": "@@ -196,3 +196,13 @@ def requires_unconditional_embeds(self):\n             requires_unconditional_embeds = self.guider._enabled and self.guider.num_conditions > 1\n \n         return requires_unconditional_embeds\n+\n+\n+class QwenImageEditPlusModularPipeline(QwenImageEditModularPipeline):\n+    \"\"\"\n+    A ModularPipeline for QwenImage-Edit Plus.\n+\n+    > [!WARNING] > This is an experimental feature and is likely to change in the future.\n+    \"\"\"\n+\n+    default_blocks_name = \"QwenImageEditPlusAutoBlocks\""
        },
        {
          "filename": "src/diffusers/pipelines/auto_pipeline.py",
          "status": "modified",
          "additions": 2,
          "deletions": 0,
          "changes": 2,
          "patch": "@@ -95,6 +95,7 @@\n     QwenImageControlNetPipeline,\n     QwenImageEditInpaintPipeline,\n     QwenImageEditPipeline,\n+    QwenImageEditPlusPipeline,\n     QwenImageImg2ImgPipeline,\n     QwenImageInpaintPipeline,\n     QwenImagePipeline,\n@@ -186,6 +187,7 @@\n         (\"flux-kontext\", FluxKontextPipeline),\n         (\"qwenimage\", QwenImageImg2ImgPipeline),\n         (\"qwenimage-edit\", QwenImageEditPipeline),\n+        (\"qwenimage-edit-plus\", QwenImageEditPlusPipeline),\n     ]\n )\n "
        },
        {
          "filename": "src/diffusers/utils/dummy_torch_and_transformers_objects.py",
          "status": "modified",
          "additions": 30,
          "deletions": 0,
          "changes": 30,
          "patch": "@@ -77,6 +77,36 @@ def from_pretrained(cls, *args, **kwargs):\n         requires_backends(cls, [\"torch\", \"transformers\"])\n \n \n+class QwenImageEditPlusAutoBlocks(metaclass=DummyObject):\n+    _backends = [\"torch\", \"transformers\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_config(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_pretrained(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+\n+class QwenImageEditPlusModularPipeline(metaclass=DummyObject):\n+    _backends = [\"torch\", \"transformers\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_config(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_pretrained(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+\n class QwenImageModularPipeline(metaclass=DummyObject):\n     _backends = [\"torch\", \"transformers\"]\n "
        }
      ],
      "num_files": 10,
      "scraped_at": "2025-11-16T21:19:11.271193"
    },
    {
      "pr_number": 12407,
      "title": "[training-scripts] Make more examples UV-compatible (follow up on #12000)",
      "body": "make kontext and qwenimage also uv compatible, follow up on #12000 ",
      "html_url": "https://github.com/huggingface/diffusers/pull/12407",
      "created_at": "2025-09-29T10:02:10Z",
      "merged_at": "2025-10-03T14:46:48Z",
      "merge_commit_sha": "941ac9c3d9aab9c36fc33c58dac1980442928082",
      "base_ref": "main",
      "head_sha": "6b19dd0079d4f6e72c5b19895f0706dcc66c1838",
      "user": "linoytsaban",
      "files": [
        {
          "filename": "examples/advanced_diffusion_training/train_dreambooth_lora_flux_advanced.py",
          "status": "modified",
          "additions": 4,
          "deletions": 0,
          "changes": 4,
          "patch": "@@ -25,6 +25,10 @@\n #     \"Jinja2\",\n #     \"peft>=0.11.1\",\n #     \"sentencepiece\",\n+#     \"torchvision\",\n+#     \"datasets\",\n+#     \"bitsandbytes\",\n+#     \"prodigyopt\",\n # ]\n # ///\n "
        },
        {
          "filename": "examples/dreambooth/train_dreambooth_lora_flux.py",
          "status": "modified",
          "additions": 4,
          "deletions": 0,
          "changes": 4,
          "patch": "@@ -25,6 +25,10 @@\n #     \"Jinja2\",\n #     \"peft>=0.11.1\",\n #     \"sentencepiece\",\n+#     \"torchvision\",\n+#     \"datasets\",\n+#     \"bitsandbytes\",\n+#     \"prodigyopt\",\n # ]\n # ///\n "
        },
        {
          "filename": "examples/dreambooth/train_dreambooth_lora_flux_kontext.py",
          "status": "modified",
          "additions": 18,
          "deletions": 0,
          "changes": 18,
          "patch": "@@ -14,6 +14,24 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n+# /// script\n+# dependencies = [\n+#     \"diffusers @ git+https://github.com/huggingface/diffusers.git\",\n+#     \"torch>=2.0.0\",\n+#     \"accelerate>=0.31.0\",\n+#     \"transformers>=4.41.2\",\n+#     \"ftfy\",\n+#     \"tensorboard\",\n+#     \"Jinja2\",\n+#     \"peft>=0.11.1\",\n+#     \"sentencepiece\",\n+#     \"torchvision\",\n+#     \"datasets\",\n+#     \"bitsandbytes\",\n+#     \"prodigyopt\",\n+# ]\n+# ///\n+\n import argparse\n import copy\n import itertools"
        },
        {
          "filename": "examples/dreambooth/train_dreambooth_lora_qwen_image.py",
          "status": "modified",
          "additions": 18,
          "deletions": 0,
          "changes": 18,
          "patch": "@@ -13,6 +13,24 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n \n+# /// script\n+# dependencies = [\n+#     \"diffusers @ git+https://github.com/huggingface/diffusers.git\",\n+#     \"torch>=2.0.0\",\n+#     \"accelerate>=0.31.0\",\n+#     \"transformers>=4.41.2\",\n+#     \"ftfy\",\n+#     \"tensorboard\",\n+#     \"Jinja2\",\n+#     \"peft>=0.11.1\",\n+#     \"sentencepiece\",\n+#     \"torchvision\",\n+#     \"datasets\",\n+#     \"bitsandbytes\",\n+#     \"prodigyopt\",\n+# ]\n+# ///\n+\n import argparse\n import copy\n import itertools"
        },
        {
          "filename": "examples/dreambooth/train_dreambooth_lora_sana.py",
          "status": "modified",
          "additions": 4,
          "deletions": 0,
          "changes": 4,
          "patch": "@@ -25,6 +25,10 @@\n #     \"Jinja2\",\n #     \"peft>=0.14.0\",\n #     \"sentencepiece\",\n+#     \"torchvision\",\n+#     \"datasets\",\n+#     \"bitsandbytes\",\n+#     \"prodigyopt\",\n # ]\n # ///\n "
        }
      ],
      "num_files": 5,
      "scraped_at": "2025-11-16T21:19:11.866177"
    },
    {
      "pr_number": 12397,
      "title": "[CI] disable installing transformers from main in ci for now.",
      "body": "# What does this PR do?\r\n\r\nTemporary solution to fix the current CI red situation. https://github.com/huggingface/diffusers/pull/12395 will eventually be merged.",
      "html_url": "https://github.com/huggingface/diffusers/pull/12397",
      "created_at": "2025-09-26T10:30:04Z",
      "merged_at": "2025-09-26T13:11:17Z",
      "merge_commit_sha": "4588bbeb4229fd307119257e273a424b370573b1",
      "base_ref": "main",
      "head_sha": "213014741228e4020271e62c3d3cd607501908ff",
      "user": "sayakpaul",
      "files": [
        {
          "filename": ".github/workflows/pr_modular_tests.yml",
          "status": "modified",
          "additions": 3,
          "deletions": 2,
          "changes": 5,
          "patch": "@@ -110,8 +110,9 @@ jobs:\n       run: |\n         python -m venv /opt/venv && export PATH=\"/opt/venv/bin:$PATH\"\n         python -m uv pip install -e [quality,test]\n-        pip uninstall transformers -y && python -m uv pip install -U transformers@git+https://github.com/huggingface/transformers.git --no-deps\n-        pip uninstall accelerate -y && python -m uv pip install -U accelerate@git+https://github.com/huggingface/accelerate.git --no-deps\n+        # Stopping this update temporarily until the Hub RC is fully shipped and integrated.\n+        # pip uninstall transformers -y && python -m uv pip install -U transformers@git+https://github.com/huggingface/transformers.git --no-deps\n+        # pip uninstall accelerate -y && python -m uv pip install -U accelerate@git+https://github.com/huggingface/accelerate.git --no-deps\n \n     - name: Environment\n       run: |"
        },
        {
          "filename": ".github/workflows/pr_tests.yml",
          "status": "modified",
          "additions": 7,
          "deletions": 5,
          "changes": 12,
          "patch": "@@ -116,8 +116,9 @@ jobs:\n       run: |\n         python -m venv /opt/venv && export PATH=\"/opt/venv/bin:$PATH\"\n         python -m uv pip install -e [quality,test]\n-        pip uninstall transformers -y && python -m uv pip install -U transformers@git+https://github.com/huggingface/transformers.git --no-deps\n-        pip uninstall accelerate -y && python -m uv pip install -U accelerate@git+https://github.com/huggingface/accelerate.git --no-deps\n+        # Stopping this update temporarily until the Hub RC is fully shipped and integrated.\n+        # pip uninstall transformers -y && python -m uv pip install -U transformers@git+https://github.com/huggingface/transformers.git --no-deps\n+        # pip uninstall accelerate -y && python -m uv pip install -U accelerate@git+https://github.com/huggingface/accelerate.git --no-deps\n \n     - name: Environment\n       run: |\n@@ -253,9 +254,10 @@ jobs:\n         python -m uv pip install -e [quality,test]\n         # TODO (sayakpaul, DN6): revisit `--no-deps`\n         python -m pip install -U peft@git+https://github.com/huggingface/peft.git --no-deps\n-        python -m uv pip install -U transformers@git+https://github.com/huggingface/transformers.git --no-deps\n-        python -m uv pip install -U tokenizers\n-        pip uninstall accelerate -y && python -m uv pip install -U accelerate@git+https://github.com/huggingface/accelerate.git --no-deps\n+        # Stopping this update temporarily until the Hub RC is fully shipped and integrated.\n+        # python -m uv pip install -U transformers@git+https://github.com/huggingface/transformers.git --no-deps\n+        # python -m uv pip install -U tokenizers\n+        # pip uninstall accelerate -y && python -m uv pip install -U accelerate@git+https://github.com/huggingface/accelerate.git --no-deps\n \n     - name: Environment\n       run: |"
        },
        {
          "filename": ".github/workflows/pr_tests_gpu.yml",
          "status": "modified",
          "additions": 8,
          "deletions": 5,
          "changes": 13,
          "patch": "@@ -132,8 +132,9 @@ jobs:\n         run: |\n           python -m venv /opt/venv && export PATH=\"/opt/venv/bin:$PATH\"\n           python -m uv pip install -e [quality,test]\n-          pip uninstall accelerate -y && python -m uv pip install -U accelerate@git+https://github.com/huggingface/accelerate.git\n-          pip uninstall transformers -y && python -m uv pip install -U transformers@git+https://github.com/huggingface/transformers.git --no-deps\n+          # Stopping this update temporarily until the Hub RC is fully shipped and integrated.\n+          # pip uninstall accelerate -y && python -m uv pip install -U accelerate@git+https://github.com/huggingface/accelerate.git\n+          # pip uninstall transformers -y && python -m uv pip install -U transformers@git+https://github.com/huggingface/transformers.git --no-deps\n \n       - name: Environment\n         run: |\n@@ -203,8 +204,9 @@ jobs:\n         python -m venv /opt/venv && export PATH=\"/opt/venv/bin:$PATH\"\n         python -m uv pip install -e [quality,test]\n         python -m uv pip install peft@git+https://github.com/huggingface/peft.git\n-        pip uninstall accelerate -y && python -m uv pip install -U accelerate@git+https://github.com/huggingface/accelerate.git\n-        pip uninstall transformers -y && python -m uv pip install -U transformers@git+https://github.com/huggingface/transformers.git --no-deps\n+        # Stopping this update temporarily until the Hub RC is fully shipped and integrated.\n+        # pip uninstall accelerate -y && python -m uv pip install -U accelerate@git+https://github.com/huggingface/accelerate.git\n+        # pip uninstall transformers -y && python -m uv pip install -U transformers@git+https://github.com/huggingface/transformers.git --no-deps\n \n     - name: Environment\n       run: |\n@@ -266,7 +268,8 @@ jobs:\n     - name: Install dependencies\n       run: |\n         python -m venv /opt/venv && export PATH=\"/opt/venv/bin:$PATH\"\n-        pip uninstall transformers -y && python -m uv pip install -U transformers@git+https://github.com/huggingface/transformers.git --no-deps\n+        # Stopping this update temporarily until the Hub RC is fully shipped and integrated.\n+        # pip uninstall transformers -y && python -m uv pip install -U transformers@git+https://github.com/huggingface/transformers.git --no-deps\n         python -m uv pip install -e [quality,test,training]\n \n     - name: Environment"
        },
        {
          "filename": "tests/pipelines/kandinsky/test_kandinsky.py",
          "status": "modified",
          "additions": 3,
          "deletions": 1,
          "changes": 4,
          "patch": "@@ -218,7 +218,9 @@ def get_dummy_inputs(self, device, seed=0):\n         return dummy.get_dummy_inputs(device=device, seed=seed)\n \n     @pytest.mark.xfail(\n-        condition=is_transformers_version(\">=\", \"4.56.2\"), reason=\"Latest transformers changes the slices\", strict=True\n+        condition=is_transformers_version(\">=\", \"4.56.2\"),\n+        reason=\"Latest transformers changes the slices\",\n+        strict=False,\n     )\n     def test_kandinsky(self):\n         device = \"cpu\""
        },
        {
          "filename": "tests/pipelines/kandinsky/test_kandinsky_combined.py",
          "status": "modified",
          "additions": 9,
          "deletions": 3,
          "changes": 12,
          "patch": "@@ -76,7 +76,9 @@ def get_dummy_inputs(self, device, seed=0):\n         return inputs\n \n     @pytest.mark.xfail(\n-        condition=is_transformers_version(\">=\", \"4.56.2\"), reason=\"Latest transformers changes the slices\", strict=True\n+        condition=is_transformers_version(\">=\", \"4.56.2\"),\n+        reason=\"Latest transformers changes the slices\",\n+        strict=False,\n     )\n     def test_kandinsky(self):\n         device = \"cpu\"\n@@ -187,7 +189,9 @@ def get_dummy_inputs(self, device, seed=0):\n         return inputs\n \n     @pytest.mark.xfail(\n-        condition=is_transformers_version(\">=\", \"4.56.2\"), reason=\"Latest transformers changes the slices\", strict=True\n+        condition=is_transformers_version(\">=\", \"4.56.2\"),\n+        reason=\"Latest transformers changes the slices\",\n+        strict=False,\n     )\n     def test_kandinsky(self):\n         device = \"cpu\"\n@@ -301,7 +305,9 @@ def get_dummy_inputs(self, device, seed=0):\n         return inputs\n \n     @pytest.mark.xfail(\n-        condition=is_transformers_version(\">=\", \"4.56.2\"), reason=\"Latest transformers changes the slices\", strict=True\n+        condition=is_transformers_version(\">=\", \"4.56.2\"),\n+        reason=\"Latest transformers changes the slices\",\n+        strict=False,\n     )\n     def test_kandinsky(self):\n         device = \"cpu\""
        },
        {
          "filename": "tests/pipelines/kandinsky/test_kandinsky_img2img.py",
          "status": "modified",
          "additions": 3,
          "deletions": 1,
          "changes": 4,
          "patch": "@@ -240,7 +240,9 @@ def get_dummy_inputs(self, device, seed=0):\n         return dummies.get_dummy_inputs(device=device, seed=seed)\n \n     @pytest.mark.xfail(\n-        condition=is_transformers_version(\">=\", \"4.56.2\"), reason=\"Latest transformers changes the slices\", strict=True\n+        condition=is_transformers_version(\">=\", \"4.56.2\"),\n+        reason=\"Latest transformers changes the slices\",\n+        strict=False,\n     )\n     def test_kandinsky_img2img(self):\n         device = \"cpu\""
        },
        {
          "filename": "tests/pipelines/kandinsky/test_kandinsky_inpaint.py",
          "status": "modified",
          "additions": 3,
          "deletions": 1,
          "changes": 4,
          "patch": "@@ -234,7 +234,9 @@ def get_dummy_inputs(self, device, seed=0):\n         return dummies.get_dummy_inputs(device=device, seed=seed)\n \n     @pytest.mark.xfail(\n-        condition=is_transformers_version(\">=\", \"4.56.2\"), reason=\"Latest transformers changes the slices\", strict=True\n+        condition=is_transformers_version(\">=\", \"4.56.2\"),\n+        reason=\"Latest transformers changes the slices\",\n+        strict=False,\n     )\n     def test_kandinsky_inpaint(self):\n         device = \"cpu\""
        }
      ],
      "num_files": 7,
      "scraped_at": "2025-11-16T21:19:12.764742"
    },
    {
      "pr_number": 12395,
      "title": "Install latest prerelease from huggingface_hub when installing transformers from main",
      "body": "This PR adds back deps update in CI (undo https://github.com/huggingface/diffusers/pull/12397) + fixes the related failing tests.",
      "html_url": "https://github.com/huggingface/diffusers/pull/12395",
      "created_at": "2025-09-26T08:50:10Z",
      "merged_at": "2025-09-30T11:32:33Z",
      "merge_commit_sha": "b59654544bbaf5c6040e670397351abe0e543a75",
      "base_ref": "main",
      "head_sha": "ae1c1d88edfb9097e89104fec465e9a128252f2b",
      "user": "Wauplin",
      "files": [
        {
          "filename": ".github/workflows/pr_modular_tests.yml",
          "status": "modified",
          "additions": 2,
          "deletions": 3,
          "changes": 5,
          "patch": "@@ -110,9 +110,8 @@ jobs:\n       run: |\n         python -m venv /opt/venv && export PATH=\"/opt/venv/bin:$PATH\"\n         python -m uv pip install -e [quality,test]\n-        # Stopping this update temporarily until the Hub RC is fully shipped and integrated.\n-        # pip uninstall transformers -y && python -m uv pip install -U transformers@git+https://github.com/huggingface/transformers.git --no-deps\n-        # pip uninstall accelerate -y && python -m uv pip install -U accelerate@git+https://github.com/huggingface/accelerate.git --no-deps\n+        pip uninstall transformers -y && pip uninstall huggingface_hub -y && python -m uv pip install --prerelease allow -U transformers@git+https://github.com/huggingface/transformers.git\n+        pip uninstall accelerate -y && python -m uv pip install -U accelerate@git+https://github.com/huggingface/accelerate.git --no-deps\n \n     - name: Environment\n       run: |"
        },
        {
          "filename": ".github/workflows/pr_tests.yml",
          "status": "modified",
          "additions": 5,
          "deletions": 7,
          "changes": 12,
          "patch": "@@ -116,9 +116,8 @@ jobs:\n       run: |\n         python -m venv /opt/venv && export PATH=\"/opt/venv/bin:$PATH\"\n         python -m uv pip install -e [quality,test]\n-        # Stopping this update temporarily until the Hub RC is fully shipped and integrated.\n-        # pip uninstall transformers -y && python -m uv pip install -U transformers@git+https://github.com/huggingface/transformers.git --no-deps\n-        # pip uninstall accelerate -y && python -m uv pip install -U accelerate@git+https://github.com/huggingface/accelerate.git --no-deps\n+        pip uninstall transformers -y && pip uninstall huggingface_hub -y && python -m uv pip install --prerelease allow -U transformers@git+https://github.com/huggingface/transformers.git\n+        pip uninstall accelerate -y && python -m uv pip install -U accelerate@git+https://github.com/huggingface/accelerate.git --no-deps\n \n     - name: Environment\n       run: |\n@@ -254,10 +253,9 @@ jobs:\n         python -m uv pip install -e [quality,test]\n         # TODO (sayakpaul, DN6): revisit `--no-deps`\n         python -m pip install -U peft@git+https://github.com/huggingface/peft.git --no-deps\n-        # Stopping this update temporarily until the Hub RC is fully shipped and integrated.\n-        # python -m uv pip install -U transformers@git+https://github.com/huggingface/transformers.git --no-deps\n-        # python -m uv pip install -U tokenizers\n-        # pip uninstall accelerate -y && python -m uv pip install -U accelerate@git+https://github.com/huggingface/accelerate.git --no-deps\n+        python -m uv pip install -U tokenizers\n+        pip uninstall accelerate -y && python -m uv pip install -U accelerate@git+https://github.com/huggingface/accelerate.git --no-deps\n+        pip uninstall transformers -y && pip uninstall huggingface_hub -y && python -m uv pip install --prerelease allow -U transformers@git+https://github.com/huggingface/transformers.git\n \n     - name: Environment\n       run: |"
        },
        {
          "filename": ".github/workflows/pr_tests_gpu.yml",
          "status": "modified",
          "additions": 5,
          "deletions": 8,
          "changes": 13,
          "patch": "@@ -132,9 +132,8 @@ jobs:\n         run: |\n           python -m venv /opt/venv && export PATH=\"/opt/venv/bin:$PATH\"\n           python -m uv pip install -e [quality,test]\n-          # Stopping this update temporarily until the Hub RC is fully shipped and integrated.\n-          # pip uninstall accelerate -y && python -m uv pip install -U accelerate@git+https://github.com/huggingface/accelerate.git\n-          # pip uninstall transformers -y && python -m uv pip install -U transformers@git+https://github.com/huggingface/transformers.git --no-deps\n+          pip uninstall accelerate -y && python -m uv pip install -U accelerate@git+https://github.com/huggingface/accelerate.git\n+          pip uninstall transformers -y && pip uninstall huggingface_hub -y && python -m uv pip install --prerelease allow -U transformers@git+https://github.com/huggingface/transformers.git\n \n       - name: Environment\n         run: |\n@@ -204,9 +203,8 @@ jobs:\n         python -m venv /opt/venv && export PATH=\"/opt/venv/bin:$PATH\"\n         python -m uv pip install -e [quality,test]\n         python -m uv pip install peft@git+https://github.com/huggingface/peft.git\n-        # Stopping this update temporarily until the Hub RC is fully shipped and integrated.\n-        # pip uninstall accelerate -y && python -m uv pip install -U accelerate@git+https://github.com/huggingface/accelerate.git\n-        # pip uninstall transformers -y && python -m uv pip install -U transformers@git+https://github.com/huggingface/transformers.git --no-deps\n+        pip uninstall accelerate -y && python -m uv pip install -U accelerate@git+https://github.com/huggingface/accelerate.git\n+        pip uninstall transformers -y && pip uninstall huggingface_hub -y && python -m uv pip install --prerelease allow -U transformers@git+https://github.com/huggingface/transformers.git\n \n     - name: Environment\n       run: |\n@@ -268,8 +266,7 @@ jobs:\n     - name: Install dependencies\n       run: |\n         python -m venv /opt/venv && export PATH=\"/opt/venv/bin:$PATH\"\n-        # Stopping this update temporarily until the Hub RC is fully shipped and integrated.\n-        # pip uninstall transformers -y && python -m uv pip install -U transformers@git+https://github.com/huggingface/transformers.git --no-deps\n+        pip uninstall transformers -y && pip uninstall huggingface_hub -y && python -m uv pip install --prerelease allow -U transformers@git+https://github.com/huggingface/transformers.git\n         python -m uv pip install -e [quality,test,training]\n \n     - name: Environment"
        },
        {
          "filename": "tests/models/test_modeling_common.py",
          "status": "modified",
          "additions": 5,
          "deletions": 3,
          "changes": 8,
          "patch": "@@ -243,8 +243,8 @@ def load_model(path):\n             else:\n                 _ = load_model(repo_id)\n \n-        warning_message = str(warning.warnings[0].message)\n-        self.assertIn(\"This serialization format is now deprecated to standardize the serialization\", warning_message)\n+        warning_messages = \" \".join(str(w.message) for w in warning.warnings)\n+        self.assertIn(\"This serialization format is now deprecated to standardize the serialization\", warning_messages)\n \n     # Local tests are already covered down below.\n     @parameterized.expand(\n@@ -298,11 +298,13 @@ def test_local_files_only_with_sharded_checkpoint(self):\n             raise_for_status=mock.Mock(side_effect=HfHubHTTPError(\"Server down\", response=mock.Mock())),\n             json=mock.Mock(return_value={}),\n         )\n+        client_mock = mock.Mock()\n+        client_mock.get.return_value = error_response\n \n         with tempfile.TemporaryDirectory() as tmpdir:\n             model = FluxTransformer2DModel.from_pretrained(repo_id, subfolder=\"transformer\", cache_dir=tmpdir)\n \n-            with mock.patch(\"requests.Session.get\", return_value=error_response):\n+            with mock.patch(\"huggingface_hub.hf_api.get_session\", return_value=client_mock):\n                 # Should fail with local_files_only=False (network required)\n                 # We would make a network call with model_info\n                 with self.assertRaises(OSError):"
        }
      ],
      "num_files": 4,
      "scraped_at": "2025-11-16T21:19:13.074871"
    },
    {
      "pr_number": 12389,
      "title": "Support both huggingface_hub `v0.x` and `v1.x`",
      "body": "Related to https://github.com/huggingface/huggingface_hub/issues/3340 https://github.com/huggingface/transformers/pull/40889\r\n\r\nWith the upcoming huggingface_hub `v1.0` release, we don't need many changes in `diffusers`. That's why it will be possible to support both 0.x and 1.x versions -and let users decide which one suits them better-. The main difference is the use of `httpx` instead of `requests` which requires to catch different errors compared to before. \r\n\r\n\r\nIn this PR I did not work on moving `diffusers` completely out of `requests` in favor of `httpx`. It would still be good to do it in ~2 months once the transformers v5 + hfh v1 will have been released. Doing that will definitely harmonize the HTTP backend but will require to remove hfh v0.x support (hence why it's best to wait until 1.0 is properly released).\r\n\r\n\r\n---\r\nNote that in this PR the CI will still be triggered on `huggingface_hub` v.0.x (and should pass). I opened https://github.com/huggingface/diffusers/pull/12384 in parallel that has more changes to trigger on v1.0 (requires some tricky stuff because of release candidates). \r\n\r\nPlan is to review/merge this PR and close https://github.com/huggingface/diffusers/pull/12384 afterwards. ",
      "html_url": "https://github.com/huggingface/diffusers/pull/12389",
      "created_at": "2025-09-25T15:00:20Z",
      "merged_at": "2025-09-25T16:28:54Z",
      "merge_commit_sha": "ec5449f3a1378df207df481bfa1ad7ff8057a58a",
      "base_ref": "main",
      "head_sha": "bfe038fb700586932e2ce62e6994457259a18e8c",
      "user": "Wauplin",
      "files": [
        {
          "filename": "setup.py",
          "status": "modified",
          "additions": 3,
          "deletions": 1,
          "changes": 4,
          "patch": "@@ -102,7 +102,8 @@\n     \"filelock\",\n     \"flax>=0.4.1\",\n     \"hf-doc-builder>=0.3.0\",\n-    \"huggingface-hub>=0.34.0\",\n+    \"httpx<1.0.0\",\n+    \"huggingface-hub>=0.34.0,<2.0\",\n     \"requests-mock==1.10.0\",\n     \"importlib_metadata\",\n     \"invisible-watermark>=0.2.0\",\n@@ -259,6 +260,7 @@ def run(self):\n install_requires = [\n     deps[\"importlib_metadata\"],\n     deps[\"filelock\"],\n+    deps[\"httpx\"],\n     deps[\"huggingface-hub\"],\n     deps[\"numpy\"],\n     deps[\"regex\"],"
        },
        {
          "filename": "src/diffusers/configuration_utils.py",
          "status": "modified",
          "additions": 2,
          "deletions": 2,
          "changes": 4,
          "patch": "@@ -30,11 +30,11 @@\n from huggingface_hub import DDUFEntry, create_repo, hf_hub_download\n from huggingface_hub.utils import (\n     EntryNotFoundError,\n+    HfHubHTTPError,\n     RepositoryNotFoundError,\n     RevisionNotFoundError,\n     validate_hf_hub_args,\n )\n-from requests import HTTPError\n from typing_extensions import Self\n \n from . import __version__\n@@ -419,7 +419,7 @@ def load_config(\n                 raise EnvironmentError(\n                     f\"{pretrained_model_name_or_path} does not appear to have a file named {cls.config_name}.\"\n                 )\n-            except HTTPError as err:\n+            except HfHubHTTPError as err:\n                 raise EnvironmentError(\n                     \"There was a specific connection error when trying to load\"\n                     f\" {pretrained_model_name_or_path}:\\n{err}\""
        },
        {
          "filename": "src/diffusers/dependency_versions_table.py",
          "status": "modified",
          "additions": 2,
          "deletions": 1,
          "changes": 3,
          "patch": "@@ -9,7 +9,8 @@\n     \"filelock\": \"filelock\",\n     \"flax\": \"flax>=0.4.1\",\n     \"hf-doc-builder\": \"hf-doc-builder>=0.3.0\",\n-    \"huggingface-hub\": \"huggingface-hub>=0.34.0\",\n+    \"httpx\": \"httpx<1.0.0\",\n+    \"huggingface-hub\": \"huggingface-hub>=0.34.0,<2.0\",\n     \"requests-mock\": \"requests-mock==1.10.0\",\n     \"importlib_metadata\": \"importlib_metadata\",\n     \"invisible-watermark\": \"invisible-watermark>=0.2.0\","
        },
        {
          "filename": "src/diffusers/models/modeling_flax_utils.py",
          "status": "modified",
          "additions": 2,
          "deletions": 2,
          "changes": 4,
          "patch": "@@ -26,11 +26,11 @@\n from huggingface_hub import create_repo, hf_hub_download\n from huggingface_hub.utils import (\n     EntryNotFoundError,\n+    HfHubHTTPError,\n     RepositoryNotFoundError,\n     RevisionNotFoundError,\n     validate_hf_hub_args,\n )\n-from requests import HTTPError\n \n from .. import __version__, is_torch_available\n from ..utils import (\n@@ -385,7 +385,7 @@ def from_pretrained(\n                 raise EnvironmentError(\n                     f\"{pretrained_model_name_or_path} does not appear to have a file named {FLAX_WEIGHTS_NAME}.\"\n                 )\n-            except HTTPError as err:\n+            except HfHubHTTPError as err:\n                 raise EnvironmentError(\n                     f\"There was a specific connection error when trying to load {pretrained_model_name_or_path}:\\n\"\n                     f\"{err}\""
        },
        {
          "filename": "src/diffusers/pipelines/pipeline_loading_utils.py",
          "status": "modified",
          "additions": 3,
          "deletions": 3,
          "changes": 6,
          "patch": "@@ -19,12 +19,12 @@\n from pathlib import Path\n from typing import Any, Callable, Dict, List, Optional, Union\n \n+import httpx\n import requests\n import torch\n from huggingface_hub import DDUFEntry, ModelCard, model_info, snapshot_download\n-from huggingface_hub.utils import OfflineModeIsEnabled, validate_hf_hub_args\n+from huggingface_hub.utils import HfHubHTTPError, OfflineModeIsEnabled, validate_hf_hub_args\n from packaging import version\n-from requests.exceptions import HTTPError\n \n from .. import __version__\n from ..utils import (\n@@ -1110,7 +1110,7 @@ def _download_dduf_file(\n     if not local_files_only:\n         try:\n             info = model_info(pretrained_model_name, token=token, revision=revision)\n-        except (HTTPError, OfflineModeIsEnabled, requests.ConnectionError) as e:\n+        except (HfHubHTTPError, OfflineModeIsEnabled, requests.ConnectionError, httpx.NetworkError) as e:\n             logger.warning(f\"Couldn't connect to the Hub: {e}.\\nWill try to load from local cache.\")\n             local_files_only = True\n             model_info_call_error = e  # save error to reraise it if model is not cached locally"
        },
        {
          "filename": "src/diffusers/pipelines/pipeline_utils.py",
          "status": "modified",
          "additions": 3,
          "deletions": 3,
          "changes": 6,
          "patch": "@@ -23,6 +23,7 @@\n from pathlib import Path\n from typing import Any, Callable, Dict, List, Optional, Union, get_args, get_origin\n \n+import httpx\n import numpy as np\n import PIL.Image\n import requests\n@@ -36,9 +37,8 @@\n     read_dduf_file,\n     snapshot_download,\n )\n-from huggingface_hub.utils import OfflineModeIsEnabled, validate_hf_hub_args\n+from huggingface_hub.utils import HfHubHTTPError, OfflineModeIsEnabled, validate_hf_hub_args\n from packaging import version\n-from requests.exceptions import HTTPError\n from tqdm.auto import tqdm\n from typing_extensions import Self\n \n@@ -1616,7 +1616,7 @@ def download(cls, pretrained_model_name, **kwargs) -> Union[str, os.PathLike]:\n         if not local_files_only:\n             try:\n                 info = model_info(pretrained_model_name, token=token, revision=revision)\n-            except (HTTPError, OfflineModeIsEnabled, requests.ConnectionError) as e:\n+            except (HfHubHTTPError, OfflineModeIsEnabled, requests.ConnectionError, httpx.NetworkError) as e:\n                 logger.warning(f\"Couldn't connect to the Hub: {e}.\\nWill try to load from local cache.\")\n                 local_files_only = True\n                 model_info_call_error = e  # save error to reraise it if model is not cached locally"
        },
        {
          "filename": "src/diffusers/utils/hub_utils.py",
          "status": "modified",
          "additions": 3,
          "deletions": 3,
          "changes": 6,
          "patch": "@@ -38,13 +38,13 @@\n from huggingface_hub.file_download import REGEX_COMMIT_HASH\n from huggingface_hub.utils import (\n     EntryNotFoundError,\n+    HfHubHTTPError,\n     RepositoryNotFoundError,\n     RevisionNotFoundError,\n     is_jinja_available,\n     validate_hf_hub_args,\n )\n from packaging import version\n-from requests import HTTPError\n \n from .. import __version__\n from .constants import (\n@@ -316,7 +316,7 @@ def _get_model_file(\n             raise EnvironmentError(\n                 f\"{pretrained_model_name_or_path} does not appear to have a file named {weights_name}.\"\n             ) from e\n-        except HTTPError as e:\n+        except HfHubHTTPError as e:\n             raise EnvironmentError(\n                 f\"There was a specific connection error when trying to load {pretrained_model_name_or_path}:\\n{e}\"\n             ) from e\n@@ -432,7 +432,7 @@ def _get_checkpoint_shard_files(\n \n     # We have already dealt with RepositoryNotFoundError and RevisionNotFoundError when getting the index, so\n     # we don't have to catch them here. We have also dealt with EntryNotFoundError.\n-    except HTTPError as e:\n+    except HfHubHTTPError as e:\n         raise EnvironmentError(\n             f\"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load {pretrained_model_name_or_path}. You should try\"\n             \" again after checking your internet connection.\""
        },
        {
          "filename": "tests/models/test_modeling_common.py",
          "status": "modified",
          "additions": 3,
          "deletions": 4,
          "changes": 7,
          "patch": "@@ -37,9 +37,8 @@\n import torch.nn as nn\n from accelerate.utils.modeling import _get_proper_dtype, compute_module_sizes, dtype_byte_size\n from huggingface_hub import ModelCard, delete_repo, snapshot_download, try_to_load_from_cache\n-from huggingface_hub.utils import is_jinja_available\n+from huggingface_hub.utils import HfHubHTTPError, is_jinja_available\n from parameterized import parameterized\n-from requests.exceptions import HTTPError\n \n from diffusers.models import FluxTransformer2DModel, SD3Transformer2DModel, UNet2DConditionModel\n from diffusers.models.attention_processor import (\n@@ -272,7 +271,7 @@ def test_cached_files_are_used_when_no_internet(self):\n         response_mock = mock.Mock()\n         response_mock.status_code = 500\n         response_mock.headers = {}\n-        response_mock.raise_for_status.side_effect = HTTPError\n+        response_mock.raise_for_status.side_effect = HfHubHTTPError(\"Server down\", response=mock.Mock())\n         response_mock.json.return_value = {}\n \n         # Download this model to make sure it's in the cache.\n@@ -296,7 +295,7 @@ def test_local_files_only_with_sharded_checkpoint(self):\n         error_response = mock.Mock(\n             status_code=500,\n             headers={},\n-            raise_for_status=mock.Mock(side_effect=HTTPError),\n+            raise_for_status=mock.Mock(side_effect=HfHubHTTPError(\"Server down\", response=mock.Mock())),\n             json=mock.Mock(return_value={}),\n         )\n "
        },
        {
          "filename": "tests/pipelines/test_pipelines.py",
          "status": "modified",
          "additions": 3,
          "deletions": 3,
          "changes": 6,
          "patch": "@@ -33,9 +33,9 @@\n import torch\n import torch.nn as nn\n from huggingface_hub import snapshot_download\n+from huggingface_hub.utils import HfHubHTTPError\n from parameterized import parameterized\n from PIL import Image\n-from requests.exceptions import HTTPError\n from transformers import CLIPImageProcessor, CLIPModel, CLIPTextConfig, CLIPTextModel, CLIPTokenizer\n \n from diffusers import (\n@@ -430,7 +430,7 @@ def test_cached_files_are_used_when_no_internet(self):\n         response_mock = mock.Mock()\n         response_mock.status_code = 500\n         response_mock.headers = {}\n-        response_mock.raise_for_status.side_effect = HTTPError\n+        response_mock.raise_for_status.side_effect = HfHubHTTPError(\"Server down\", response=mock.Mock())\n         response_mock.json.return_value = {}\n \n         # Download this model to make sure it's in the cache.\n@@ -457,7 +457,7 @@ def test_local_files_only_are_used_when_no_internet(self):\n         response_mock = mock.Mock()\n         response_mock.status_code = 500\n         response_mock.headers = {}\n-        response_mock.raise_for_status.side_effect = HTTPError\n+        response_mock.raise_for_status.side_effect = HfHubHTTPError(\"Server down\", response=mock.Mock())\n         response_mock.json.return_value = {}\n \n         # first check that with local files only the pipeline can only be used if cached"
        }
      ],
      "num_files": 9,
      "scraped_at": "2025-11-16T21:19:14.944843"
    },
    {
      "pr_number": 12374,
      "title": "[tests] introduce `VAETesterMixin` to consolidate tests for slicing and tiling",
      "body": "# What does this PR do?\r\n\r\nMore reductions in LoC.",
      "html_url": "https://github.com/huggingface/diffusers/pull/12374",
      "created_at": "2025-09-23T08:04:18Z",
      "merged_at": "2025-10-17T06:32:29Z",
      "merge_commit_sha": "af769881d37fe916afef2c47279f66c79f5f2714",
      "base_ref": "main",
      "head_sha": "137bf5af89b688d6461544c5f59a51e0d06fbb88",
      "user": "sayakpaul",
      "files": [
        {
          "filename": "tests/models/autoencoders/test_models_asymmetric_autoencoder_kl.py",
          "status": "modified",
          "additions": 3,
          "deletions": 2,
          "changes": 5,
          "patch": "@@ -35,13 +35,14 @@\n     torch_all_close,\n     torch_device,\n )\n-from ..test_modeling_common import ModelTesterMixin, UNetTesterMixin\n+from ..test_modeling_common import ModelTesterMixin\n+from .testing_utils import AutoencoderTesterMixin\n \n \n enable_full_determinism()\n \n \n-class AutoencoderKLTests(ModelTesterMixin, UNetTesterMixin, unittest.TestCase):\n+class AsymmetricAutoencoderKLTests(ModelTesterMixin, AutoencoderTesterMixin, unittest.TestCase):\n     model_class = AsymmetricAutoencoderKL\n     main_input_name = \"sample\"\n     base_precision = 1e-2"
        },
        {
          "filename": "tests/models/autoencoders/test_models_autoencoder_cosmos.py",
          "status": "modified",
          "additions": 3,
          "deletions": 6,
          "changes": 9,
          "patch": "@@ -17,13 +17,14 @@\n from diffusers import AutoencoderKLCosmos\n \n from ...testing_utils import enable_full_determinism, floats_tensor, torch_device\n-from ..test_modeling_common import ModelTesterMixin, UNetTesterMixin\n+from ..test_modeling_common import ModelTesterMixin\n+from .testing_utils import AutoencoderTesterMixin\n \n \n enable_full_determinism()\n \n \n-class AutoencoderKLCosmosTests(ModelTesterMixin, UNetTesterMixin, unittest.TestCase):\n+class AutoencoderKLCosmosTests(ModelTesterMixin, AutoencoderTesterMixin, unittest.TestCase):\n     model_class = AutoencoderKLCosmos\n     main_input_name = \"sample\"\n     base_precision = 1e-2\n@@ -80,7 +81,3 @@ def test_gradient_checkpointing_is_applied(self):\n     @unittest.skip(\"Not sure why this test fails. Investigate later.\")\n     def test_effective_gradient_checkpointing(self):\n         pass\n-\n-    @unittest.skip(\"Unsupported test.\")\n-    def test_forward_with_norm_groups(self):\n-        pass"
        },
        {
          "filename": "tests/models/autoencoders/test_models_autoencoder_dc.py",
          "status": "modified",
          "additions": 3,
          "deletions": 6,
          "changes": 9,
          "patch": "@@ -22,13 +22,14 @@\n     floats_tensor,\n     torch_device,\n )\n-from ..test_modeling_common import ModelTesterMixin, UNetTesterMixin\n+from ..test_modeling_common import ModelTesterMixin\n+from .testing_utils import AutoencoderTesterMixin\n \n \n enable_full_determinism()\n \n \n-class AutoencoderDCTests(ModelTesterMixin, UNetTesterMixin, unittest.TestCase):\n+class AutoencoderDCTests(ModelTesterMixin, AutoencoderTesterMixin, unittest.TestCase):\n     model_class = AutoencoderDC\n     main_input_name = \"sample\"\n     base_precision = 1e-2\n@@ -81,7 +82,3 @@ def prepare_init_args_and_inputs_for_common(self):\n         init_dict = self.get_autoencoder_dc_config()\n         inputs_dict = self.dummy_input\n         return init_dict, inputs_dict\n-\n-    @unittest.skip(\"AutoencoderDC does not support `norm_num_groups` because it does not use GroupNorm.\")\n-    def test_forward_with_norm_groups(self):\n-        pass"
        },
        {
          "filename": "tests/models/autoencoders/test_models_autoencoder_hunyuan_video.py",
          "status": "modified",
          "additions": 4,
          "deletions": 69,
          "changes": 73,
          "patch": "@@ -20,18 +20,15 @@\n from diffusers import AutoencoderKLHunyuanVideo\n from diffusers.models.autoencoders.autoencoder_kl_hunyuan_video import prepare_causal_attention_mask\n \n-from ...testing_utils import (\n-    enable_full_determinism,\n-    floats_tensor,\n-    torch_device,\n-)\n-from ..test_modeling_common import ModelTesterMixin, UNetTesterMixin\n+from ...testing_utils import enable_full_determinism, floats_tensor, torch_device\n+from ..test_modeling_common import ModelTesterMixin\n+from .testing_utils import AutoencoderTesterMixin\n \n \n enable_full_determinism()\n \n \n-class AutoencoderKLHunyuanVideoTests(ModelTesterMixin, UNetTesterMixin, unittest.TestCase):\n+class AutoencoderKLHunyuanVideoTests(ModelTesterMixin, AutoencoderTesterMixin, unittest.TestCase):\n     model_class = AutoencoderKLHunyuanVideo\n     main_input_name = \"sample\"\n     base_precision = 1e-2\n@@ -87,68 +84,6 @@ def prepare_init_args_and_inputs_for_common(self):\n         inputs_dict = self.dummy_input\n         return init_dict, inputs_dict\n \n-    def test_enable_disable_tiling(self):\n-        init_dict, inputs_dict = self.prepare_init_args_and_inputs_for_common()\n-\n-        torch.manual_seed(0)\n-        model = self.model_class(**init_dict).to(torch_device)\n-\n-        inputs_dict.update({\"return_dict\": False})\n-\n-        torch.manual_seed(0)\n-        output_without_tiling = model(**inputs_dict, generator=torch.manual_seed(0))[0]\n-\n-        torch.manual_seed(0)\n-        model.enable_tiling()\n-        output_with_tiling = model(**inputs_dict, generator=torch.manual_seed(0))[0]\n-\n-        self.assertLess(\n-            (output_without_tiling.detach().cpu().numpy() - output_with_tiling.detach().cpu().numpy()).max(),\n-            0.5,\n-            \"VAE tiling should not affect the inference results\",\n-        )\n-\n-        torch.manual_seed(0)\n-        model.disable_tiling()\n-        output_without_tiling_2 = model(**inputs_dict, generator=torch.manual_seed(0))[0]\n-\n-        self.assertEqual(\n-            output_without_tiling.detach().cpu().numpy().all(),\n-            output_without_tiling_2.detach().cpu().numpy().all(),\n-            \"Without tiling outputs should match with the outputs when tiling is manually disabled.\",\n-        )\n-\n-    def test_enable_disable_slicing(self):\n-        init_dict, inputs_dict = self.prepare_init_args_and_inputs_for_common()\n-\n-        torch.manual_seed(0)\n-        model = self.model_class(**init_dict).to(torch_device)\n-\n-        inputs_dict.update({\"return_dict\": False})\n-\n-        torch.manual_seed(0)\n-        output_without_slicing = model(**inputs_dict, generator=torch.manual_seed(0))[0]\n-\n-        torch.manual_seed(0)\n-        model.enable_slicing()\n-        output_with_slicing = model(**inputs_dict, generator=torch.manual_seed(0))[0]\n-\n-        self.assertLess(\n-            (output_without_slicing.detach().cpu().numpy() - output_with_slicing.detach().cpu().numpy()).max(),\n-            0.5,\n-            \"VAE slicing should not affect the inference results\",\n-        )\n-\n-        torch.manual_seed(0)\n-        model.disable_slicing()\n-        output_without_slicing_2 = model(**inputs_dict, generator=torch.manual_seed(0))[0]\n-\n-        self.assertEqual(\n-            output_without_slicing.detach().cpu().numpy().all(),\n-            output_without_slicing_2.detach().cpu().numpy().all(),\n-            \"Without slicing outputs should match with the outputs when slicing is manually disabled.\",\n-        )\n-\n     def test_gradient_checkpointing_is_applied(self):\n         expected_set = {\n             \"HunyuanVideoDecoder3D\","
        },
        {
          "filename": "tests/models/autoencoders/test_models_autoencoder_kl.py",
          "status": "modified",
          "additions": 3,
          "deletions": 64,
          "changes": 67,
          "patch": "@@ -35,13 +35,14 @@\n     torch_all_close,\n     torch_device,\n )\n-from ..test_modeling_common import ModelTesterMixin, UNetTesterMixin\n+from ..test_modeling_common import ModelTesterMixin\n+from .testing_utils import AutoencoderTesterMixin\n \n \n enable_full_determinism()\n \n \n-class AutoencoderKLTests(ModelTesterMixin, UNetTesterMixin, unittest.TestCase):\n+class AutoencoderKLTests(ModelTesterMixin, AutoencoderTesterMixin, unittest.TestCase):\n     model_class = AutoencoderKL\n     main_input_name = \"sample\"\n     base_precision = 1e-2\n@@ -83,68 +84,6 @@ def prepare_init_args_and_inputs_for_common(self):\n         inputs_dict = self.dummy_input\n         return init_dict, inputs_dict\n \n-    def test_enable_disable_tiling(self):\n-        init_dict, inputs_dict = self.prepare_init_args_and_inputs_for_common()\n-\n-        torch.manual_seed(0)\n-        model = self.model_class(**init_dict).to(torch_device)\n-\n-        inputs_dict.update({\"return_dict\": False})\n-\n-        torch.manual_seed(0)\n-        output_without_tiling = model(**inputs_dict, generator=torch.manual_seed(0))[0]\n-\n-        torch.manual_seed(0)\n-        model.enable_tiling()\n-        output_with_tiling = model(**inputs_dict, generator=torch.manual_seed(0))[0]\n-\n-        self.assertLess(\n-            (output_without_tiling.detach().cpu().numpy() - output_with_tiling.detach().cpu().numpy()).max(),\n-            0.5,\n-            \"VAE tiling should not affect the inference results\",\n-        )\n-\n-        torch.manual_seed(0)\n-        model.disable_tiling()\n-        output_without_tiling_2 = model(**inputs_dict, generator=torch.manual_seed(0))[0]\n-\n-        self.assertEqual(\n-            output_without_tiling.detach().cpu().numpy().all(),\n-            output_without_tiling_2.detach().cpu().numpy().all(),\n-            \"Without tiling outputs should match with the outputs when tiling is manually disabled.\",\n-        )\n-\n-    def test_enable_disable_slicing(self):\n-        init_dict, inputs_dict = self.prepare_init_args_and_inputs_for_common()\n-\n-        torch.manual_seed(0)\n-        model = self.model_class(**init_dict).to(torch_device)\n-\n-        inputs_dict.update({\"return_dict\": False})\n-\n-        torch.manual_seed(0)\n-        output_without_slicing = model(**inputs_dict, generator=torch.manual_seed(0))[0]\n-\n-        torch.manual_seed(0)\n-        model.enable_slicing()\n-        output_with_slicing = model(**inputs_dict, generator=torch.manual_seed(0))[0]\n-\n-        self.assertLess(\n-            (output_without_slicing.detach().cpu().numpy() - output_with_slicing.detach().cpu().numpy()).max(),\n-            0.5,\n-            \"VAE slicing should not affect the inference results\",\n-        )\n-\n-        torch.manual_seed(0)\n-        model.disable_slicing()\n-        output_without_slicing_2 = model(**inputs_dict, generator=torch.manual_seed(0))[0]\n-\n-        self.assertEqual(\n-            output_without_slicing.detach().cpu().numpy().all(),\n-            output_without_slicing_2.detach().cpu().numpy().all(),\n-            \"Without slicing outputs should match with the outputs when slicing is manually disabled.\",\n-        )\n-\n     def test_gradient_checkpointing_is_applied(self):\n         expected_set = {\"Decoder\", \"Encoder\", \"UNetMidBlock2D\"}\n         super().test_gradient_checkpointing_is_applied(expected_set=expected_set)"
        },
        {
          "filename": "tests/models/autoencoders/test_models_autoencoder_kl_cogvideox.py",
          "status": "modified",
          "additions": 3,
          "deletions": 64,
          "changes": 67,
          "patch": "@@ -24,13 +24,14 @@\n     floats_tensor,\n     torch_device,\n )\n-from ..test_modeling_common import ModelTesterMixin, UNetTesterMixin\n+from ..test_modeling_common import ModelTesterMixin\n+from .testing_utils import AutoencoderTesterMixin\n \n \n enable_full_determinism()\n \n \n-class AutoencoderKLCogVideoXTests(ModelTesterMixin, UNetTesterMixin, unittest.TestCase):\n+class AutoencoderKLCogVideoXTests(ModelTesterMixin, AutoencoderTesterMixin, unittest.TestCase):\n     model_class = AutoencoderKLCogVideoX\n     main_input_name = \"sample\"\n     base_precision = 1e-2\n@@ -82,68 +83,6 @@ def prepare_init_args_and_inputs_for_common(self):\n         inputs_dict = self.dummy_input\n         return init_dict, inputs_dict\n \n-    def test_enable_disable_tiling(self):\n-        init_dict, inputs_dict = self.prepare_init_args_and_inputs_for_common()\n-\n-        torch.manual_seed(0)\n-        model = self.model_class(**init_dict).to(torch_device)\n-\n-        inputs_dict.update({\"return_dict\": False})\n-\n-        torch.manual_seed(0)\n-        output_without_tiling = model(**inputs_dict, generator=torch.manual_seed(0))[0]\n-\n-        torch.manual_seed(0)\n-        model.enable_tiling()\n-        output_with_tiling = model(**inputs_dict, generator=torch.manual_seed(0))[0]\n-\n-        self.assertLess(\n-            (output_without_tiling.detach().cpu().numpy() - output_with_tiling.detach().cpu().numpy()).max(),\n-            0.5,\n-            \"VAE tiling should not affect the inference results\",\n-        )\n-\n-        torch.manual_seed(0)\n-        model.disable_tiling()\n-        output_without_tiling_2 = model(**inputs_dict, generator=torch.manual_seed(0))[0]\n-\n-        self.assertEqual(\n-            output_without_tiling.detach().cpu().numpy().all(),\n-            output_without_tiling_2.detach().cpu().numpy().all(),\n-            \"Without tiling outputs should match with the outputs when tiling is manually disabled.\",\n-        )\n-\n-    def test_enable_disable_slicing(self):\n-        init_dict, inputs_dict = self.prepare_init_args_and_inputs_for_common()\n-\n-        torch.manual_seed(0)\n-        model = self.model_class(**init_dict).to(torch_device)\n-\n-        inputs_dict.update({\"return_dict\": False})\n-\n-        torch.manual_seed(0)\n-        output_without_slicing = model(**inputs_dict, generator=torch.manual_seed(0))[0]\n-\n-        torch.manual_seed(0)\n-        model.enable_slicing()\n-        output_with_slicing = model(**inputs_dict, generator=torch.manual_seed(0))[0]\n-\n-        self.assertLess(\n-            (output_without_slicing.detach().cpu().numpy() - output_with_slicing.detach().cpu().numpy()).max(),\n-            0.5,\n-            \"VAE slicing should not affect the inference results\",\n-        )\n-\n-        torch.manual_seed(0)\n-        model.disable_slicing()\n-        output_without_slicing_2 = model(**inputs_dict, generator=torch.manual_seed(0))[0]\n-\n-        self.assertEqual(\n-            output_without_slicing.detach().cpu().numpy().all(),\n-            output_without_slicing_2.detach().cpu().numpy().all(),\n-            \"Without slicing outputs should match with the outputs when slicing is manually disabled.\",\n-        )\n-\n     def test_gradient_checkpointing_is_applied(self):\n         expected_set = {\n             \"CogVideoXDownBlock3D\","
        },
        {
          "filename": "tests/models/autoencoders/test_models_autoencoder_kl_temporal_decoder.py",
          "status": "modified",
          "additions": 3,
          "deletions": 6,
          "changes": 9,
          "patch": "@@ -22,13 +22,14 @@\n     floats_tensor,\n     torch_device,\n )\n-from ..test_modeling_common import ModelTesterMixin, UNetTesterMixin\n+from ..test_modeling_common import ModelTesterMixin\n+from .testing_utils import AutoencoderTesterMixin\n \n \n enable_full_determinism()\n \n \n-class AutoencoderKLTemporalDecoderTests(ModelTesterMixin, UNetTesterMixin, unittest.TestCase):\n+class AutoencoderKLTemporalDecoderTests(ModelTesterMixin, AutoencoderTesterMixin, unittest.TestCase):\n     model_class = AutoencoderKLTemporalDecoder\n     main_input_name = \"sample\"\n     base_precision = 1e-2\n@@ -67,7 +68,3 @@ def prepare_init_args_and_inputs_for_common(self):\n     def test_gradient_checkpointing_is_applied(self):\n         expected_set = {\"Encoder\", \"TemporalDecoder\", \"UNetMidBlock2D\"}\n         super().test_gradient_checkpointing_is_applied(expected_set=expected_set)\n-\n-    @unittest.skip(\"Test unsupported.\")\n-    def test_forward_with_norm_groups(self):\n-        pass"
        },
        {
          "filename": "tests/models/autoencoders/test_models_autoencoder_ltx_video.py",
          "status": "modified",
          "additions": 4,
          "deletions": 34,
          "changes": 38,
          "patch": "@@ -24,13 +24,14 @@\n     floats_tensor,\n     torch_device,\n )\n-from ..test_modeling_common import ModelTesterMixin, UNetTesterMixin\n+from ..test_modeling_common import ModelTesterMixin\n+from .testing_utils import AutoencoderTesterMixin\n \n \n enable_full_determinism()\n \n \n-class AutoencoderKLLTXVideo090Tests(ModelTesterMixin, UNetTesterMixin, unittest.TestCase):\n+class AutoencoderKLLTXVideo090Tests(ModelTesterMixin, AutoencoderTesterMixin, unittest.TestCase):\n     model_class = AutoencoderKLLTXVideo\n     main_input_name = \"sample\"\n     base_precision = 1e-2\n@@ -99,7 +100,7 @@ def test_forward_with_norm_groups(self):\n         pass\n \n \n-class AutoencoderKLLTXVideo091Tests(ModelTesterMixin, UNetTesterMixin, unittest.TestCase):\n+class AutoencoderKLLTXVideo091Tests(ModelTesterMixin, unittest.TestCase):\n     model_class = AutoencoderKLLTXVideo\n     main_input_name = \"sample\"\n     base_precision = 1e-2\n@@ -167,34 +168,3 @@ def test_outputs_equivalence(self):\n     @unittest.skip(\"AutoencoderKLLTXVideo does not support `norm_num_groups` because it does not use GroupNorm.\")\n     def test_forward_with_norm_groups(self):\n         pass\n-\n-    def test_enable_disable_tiling(self):\n-        init_dict, inputs_dict = self.prepare_init_args_and_inputs_for_common()\n-\n-        torch.manual_seed(0)\n-        model = self.model_class(**init_dict).to(torch_device)\n-\n-        inputs_dict.update({\"return_dict\": False})\n-\n-        torch.manual_seed(0)\n-        output_without_tiling = model(**inputs_dict, generator=torch.manual_seed(0))[0]\n-\n-        torch.manual_seed(0)\n-        model.enable_tiling()\n-        output_with_tiling = model(**inputs_dict, generator=torch.manual_seed(0))[0]\n-\n-        self.assertLess(\n-            (output_without_tiling.detach().cpu().numpy() - output_with_tiling.detach().cpu().numpy()).max(),\n-            0.5,\n-            \"VAE tiling should not affect the inference results\",\n-        )\n-\n-        torch.manual_seed(0)\n-        model.disable_tiling()\n-        output_without_tiling_2 = model(**inputs_dict, generator=torch.manual_seed(0))[0]\n-\n-        self.assertEqual(\n-            output_without_tiling.detach().cpu().numpy().all(),\n-            output_without_tiling_2.detach().cpu().numpy().all(),\n-            \"Without tiling outputs should match with the outputs when tiling is manually disabled.\",\n-        )"
        },
        {
          "filename": "tests/models/autoencoders/test_models_autoencoder_magvit.py",
          "status": "modified",
          "additions": 9,
          "deletions": 2,
          "changes": 11,
          "patch": "@@ -18,13 +18,14 @@\n from diffusers import AutoencoderKLMagvit\n \n from ...testing_utils import enable_full_determinism, floats_tensor, torch_device\n-from ..test_modeling_common import ModelTesterMixin, UNetTesterMixin\n+from ..test_modeling_common import ModelTesterMixin\n+from .testing_utils import AutoencoderTesterMixin\n \n \n enable_full_determinism()\n \n \n-class AutoencoderKLMagvitTests(ModelTesterMixin, UNetTesterMixin, unittest.TestCase):\n+class AutoencoderKLMagvitTests(ModelTesterMixin, AutoencoderTesterMixin, unittest.TestCase):\n     model_class = AutoencoderKLMagvit\n     main_input_name = \"sample\"\n     base_precision = 1e-2\n@@ -88,3 +89,9 @@ def test_effective_gradient_checkpointing(self):\n     @unittest.skip(\"Unsupported test.\")\n     def test_forward_with_norm_groups(self):\n         pass\n+\n+    @unittest.skip(\n+        \"Unsupported test. Error: RuntimeError: Sizes of tensors must match except in dimension 0. Expected size 9 but got size 12 for tensor number 1 in the list.\"\n+    )\n+    def test_enable_disable_slicing(self):\n+        pass"
        },
        {
          "filename": "tests/models/autoencoders/test_models_autoencoder_mochi.py",
          "status": "modified",
          "additions": 4,
          "deletions": 15,
          "changes": 19,
          "patch": "@@ -17,18 +17,15 @@\n \n from diffusers import AutoencoderKLMochi\n \n-from ...testing_utils import (\n-    enable_full_determinism,\n-    floats_tensor,\n-    torch_device,\n-)\n-from ..test_modeling_common import ModelTesterMixin, UNetTesterMixin\n+from ...testing_utils import enable_full_determinism, floats_tensor, torch_device\n+from ..test_modeling_common import ModelTesterMixin\n+from .testing_utils import AutoencoderTesterMixin\n \n \n enable_full_determinism()\n \n \n-class AutoencoderKLMochiTests(ModelTesterMixin, UNetTesterMixin, unittest.TestCase):\n+class AutoencoderKLMochiTests(ModelTesterMixin, AutoencoderTesterMixin, unittest.TestCase):\n     model_class = AutoencoderKLMochi\n     main_input_name = \"sample\"\n     base_precision = 1e-2\n@@ -79,14 +76,6 @@ def test_gradient_checkpointing_is_applied(self):\n         }\n         super().test_gradient_checkpointing_is_applied(expected_set=expected_set)\n \n-    @unittest.skip(\"Unsupported test.\")\n-    def test_forward_with_norm_groups(self):\n-        \"\"\"\n-        tests/models/autoencoders/test_models_autoencoder_mochi.py::AutoencoderKLMochiTests::test_forward_with_norm_groups -\n-        TypeError: AutoencoderKLMochi.__init__() got an unexpected keyword argument 'norm_num_groups'\n-        \"\"\"\n-        pass\n-\n     @unittest.skip(\"Unsupported test.\")\n     def test_model_parallelism(self):\n         \"\"\""
        },
        {
          "filename": "tests/models/autoencoders/test_models_autoencoder_oobleck.py",
          "status": "modified",
          "additions": 3,
          "deletions": 6,
          "changes": 9,
          "patch": "@@ -30,13 +30,14 @@\n     torch_all_close,\n     torch_device,\n )\n-from ..test_modeling_common import ModelTesterMixin, UNetTesterMixin\n+from ..test_modeling_common import ModelTesterMixin\n+from .testing_utils import AutoencoderTesterMixin\n \n \n enable_full_determinism()\n \n \n-class AutoencoderOobleckTests(ModelTesterMixin, UNetTesterMixin, unittest.TestCase):\n+class AutoencoderOobleckTests(ModelTesterMixin, AutoencoderTesterMixin, unittest.TestCase):\n     model_class = AutoencoderOobleck\n     main_input_name = \"sample\"\n     base_precision = 1e-2\n@@ -106,10 +107,6 @@ def test_enable_disable_slicing(self):\n             \"Without slicing outputs should match with the outputs when slicing is manually disabled.\",\n         )\n \n-    @unittest.skip(\"Test unsupported.\")\n-    def test_forward_with_norm_groups(self):\n-        pass\n-\n     @unittest.skip(\"No attention module used in this model\")\n     def test_set_attn_processor_for_determinism(self):\n         return"
        },
        {
          "filename": "tests/models/autoencoders/test_models_autoencoder_tiny.py",
          "status": "modified",
          "additions": 3,
          "deletions": 33,
          "changes": 36,
          "patch": "@@ -31,13 +31,14 @@\n     torch_all_close,\n     torch_device,\n )\n-from ..test_modeling_common import ModelTesterMixin, UNetTesterMixin\n+from ..test_modeling_common import ModelTesterMixin\n+from .testing_utils import AutoencoderTesterMixin\n \n \n enable_full_determinism()\n \n \n-class AutoencoderTinyTests(ModelTesterMixin, UNetTesterMixin, unittest.TestCase):\n+class AutoencoderTinyTests(ModelTesterMixin, AutoencoderTesterMixin, unittest.TestCase):\n     model_class = AutoencoderTiny\n     main_input_name = \"sample\"\n     base_precision = 1e-2\n@@ -81,37 +82,6 @@ def prepare_init_args_and_inputs_for_common(self):\n     def test_enable_disable_tiling(self):\n         pass\n \n-    def test_enable_disable_slicing(self):\n-        init_dict, inputs_dict = self.prepare_init_args_and_inputs_for_common()\n-\n-        torch.manual_seed(0)\n-        model = self.model_class(**init_dict).to(torch_device)\n-\n-        inputs_dict.update({\"return_dict\": False})\n-\n-        torch.manual_seed(0)\n-        output_without_slicing = model(**inputs_dict)[0]\n-\n-        torch.manual_seed(0)\n-        model.enable_slicing()\n-        output_with_slicing = model(**inputs_dict)[0]\n-\n-        self.assertLess(\n-            (output_without_slicing.detach().cpu().numpy() - output_with_slicing.detach().cpu().numpy()).max(),\n-            0.5,\n-            \"VAE slicing should not affect the inference results\",\n-        )\n-\n-        torch.manual_seed(0)\n-        model.disable_slicing()\n-        output_without_slicing_2 = model(**inputs_dict)[0]\n-\n-        self.assertEqual(\n-            output_without_slicing.detach().cpu().numpy().all(),\n-            output_without_slicing_2.detach().cpu().numpy().all(),\n-            \"Without slicing outputs should match with the outputs when slicing is manually disabled.\",\n-        )\n-\n     @unittest.skip(\"Test not supported.\")\n     def test_outputs_equivalence(self):\n         pass"
        },
        {
          "filename": "tests/models/autoencoders/test_models_autoencoder_wan.py",
          "status": "modified",
          "additions": 3,
          "deletions": 66,
          "changes": 69,
          "patch": "@@ -15,18 +15,17 @@\n \n import unittest\n \n-import torch\n-\n from diffusers import AutoencoderKLWan\n \n from ...testing_utils import enable_full_determinism, floats_tensor, torch_device\n-from ..test_modeling_common import ModelTesterMixin, UNetTesterMixin\n+from ..test_modeling_common import ModelTesterMixin\n+from .testing_utils import AutoencoderTesterMixin\n \n \n enable_full_determinism()\n \n \n-class AutoencoderKLWanTests(ModelTesterMixin, UNetTesterMixin, unittest.TestCase):\n+class AutoencoderKLWanTests(ModelTesterMixin, AutoencoderTesterMixin, unittest.TestCase):\n     model_class = AutoencoderKLWan\n     main_input_name = \"sample\"\n     base_precision = 1e-2\n@@ -76,68 +75,6 @@ def prepare_init_args_and_inputs_for_tiling(self):\n         inputs_dict = self.dummy_input_tiling\n         return init_dict, inputs_dict\n \n-    def test_enable_disable_tiling(self):\n-        init_dict, inputs_dict = self.prepare_init_args_and_inputs_for_tiling()\n-\n-        torch.manual_seed(0)\n-        model = self.model_class(**init_dict).to(torch_device)\n-\n-        inputs_dict.update({\"return_dict\": False})\n-\n-        torch.manual_seed(0)\n-        output_without_tiling = model(**inputs_dict, generator=torch.manual_seed(0))[0]\n-\n-        torch.manual_seed(0)\n-        model.enable_tiling(96, 96, 64, 64)\n-        output_with_tiling = model(**inputs_dict, generator=torch.manual_seed(0))[0]\n-\n-        self.assertLess(\n-            (output_without_tiling.detach().cpu().numpy() - output_with_tiling.detach().cpu().numpy()).max(),\n-            0.5,\n-            \"VAE tiling should not affect the inference results\",\n-        )\n-\n-        torch.manual_seed(0)\n-        model.disable_tiling()\n-        output_without_tiling_2 = model(**inputs_dict, generator=torch.manual_seed(0))[0]\n-\n-        self.assertEqual(\n-            output_without_tiling.detach().cpu().numpy().all(),\n-            output_without_tiling_2.detach().cpu().numpy().all(),\n-            \"Without tiling outputs should match with the outputs when tiling is manually disabled.\",\n-        )\n-\n-    def test_enable_disable_slicing(self):\n-        init_dict, inputs_dict = self.prepare_init_args_and_inputs_for_common()\n-\n-        torch.manual_seed(0)\n-        model = self.model_class(**init_dict).to(torch_device)\n-\n-        inputs_dict.update({\"return_dict\": False})\n-\n-        torch.manual_seed(0)\n-        output_without_slicing = model(**inputs_dict, generator=torch.manual_seed(0))[0]\n-\n-        torch.manual_seed(0)\n-        model.enable_slicing()\n-        output_with_slicing = model(**inputs_dict, generator=torch.manual_seed(0))[0]\n-\n-        self.assertLess(\n-            (output_without_slicing.detach().cpu().numpy() - output_with_slicing.detach().cpu().numpy()).max(),\n-            0.05,\n-            \"VAE slicing should not affect the inference results\",\n-        )\n-\n-        torch.manual_seed(0)\n-        model.disable_slicing()\n-        output_without_slicing_2 = model(**inputs_dict, generator=torch.manual_seed(0))[0]\n-\n-        self.assertEqual(\n-            output_without_slicing.detach().cpu().numpy().all(),\n-            output_without_slicing_2.detach().cpu().numpy().all(),\n-            \"Without slicing outputs should match with the outputs when slicing is manually disabled.\",\n-        )\n-\n     @unittest.skip(\"Gradient checkpointing has not been implemented yet\")\n     def test_gradient_checkpointing_is_applied(self):\n         pass"
        },
        {
          "filename": "tests/models/autoencoders/test_models_consistency_decoder_vae.py",
          "status": "modified",
          "additions": 2,
          "deletions": 65,
          "changes": 67,
          "patch": "@@ -31,12 +31,13 @@\n     torch_device,\n )\n from ..test_modeling_common import ModelTesterMixin\n+from .testing_utils import AutoencoderTesterMixin\n \n \n enable_full_determinism()\n \n \n-class ConsistencyDecoderVAETests(ModelTesterMixin, unittest.TestCase):\n+class ConsistencyDecoderVAETests(ModelTesterMixin, AutoencoderTesterMixin, unittest.TestCase):\n     model_class = ConsistencyDecoderVAE\n     main_input_name = \"sample\"\n     base_precision = 1e-2\n@@ -92,70 +93,6 @@ def init_dict(self):\n     def prepare_init_args_and_inputs_for_common(self):\n         return self.init_dict, self.inputs_dict()\n \n-    def test_enable_disable_tiling(self):\n-        init_dict, inputs_dict = self.prepare_init_args_and_inputs_for_common()\n-\n-        torch.manual_seed(0)\n-        model = self.model_class(**init_dict).to(torch_device)\n-\n-        inputs_dict.update({\"return_dict\": False})\n-        _ = inputs_dict.pop(\"generator\")\n-\n-        torch.manual_seed(0)\n-        output_without_tiling = model(**inputs_dict, generator=torch.manual_seed(0))[0]\n-\n-        torch.manual_seed(0)\n-        model.enable_tiling()\n-        output_with_tiling = model(**inputs_dict, generator=torch.manual_seed(0))[0]\n-\n-        self.assertLess(\n-            (output_without_tiling.detach().cpu().numpy() - output_with_tiling.detach().cpu().numpy()).max(),\n-            0.5,\n-            \"VAE tiling should not affect the inference results\",\n-        )\n-\n-        torch.manual_seed(0)\n-        model.disable_tiling()\n-        output_without_tiling_2 = model(**inputs_dict, generator=torch.manual_seed(0))[0]\n-\n-        self.assertEqual(\n-            output_without_tiling.detach().cpu().numpy().all(),\n-            output_without_tiling_2.detach().cpu().numpy().all(),\n-            \"Without tiling outputs should match with the outputs when tiling is manually disabled.\",\n-        )\n-\n-    def test_enable_disable_slicing(self):\n-        init_dict, inputs_dict = self.prepare_init_args_and_inputs_for_common()\n-\n-        torch.manual_seed(0)\n-        model = self.model_class(**init_dict).to(torch_device)\n-\n-        inputs_dict.update({\"return_dict\": False})\n-        _ = inputs_dict.pop(\"generator\")\n-\n-        torch.manual_seed(0)\n-        output_without_slicing = model(**inputs_dict, generator=torch.manual_seed(0))[0]\n-\n-        torch.manual_seed(0)\n-        model.enable_slicing()\n-        output_with_slicing = model(**inputs_dict, generator=torch.manual_seed(0))[0]\n-\n-        self.assertLess(\n-            (output_without_slicing.detach().cpu().numpy() - output_with_slicing.detach().cpu().numpy()).max(),\n-            0.5,\n-            \"VAE slicing should not affect the inference results\",\n-        )\n-\n-        torch.manual_seed(0)\n-        model.disable_slicing()\n-        output_without_slicing_2 = model(**inputs_dict, generator=torch.manual_seed(0))[0]\n-\n-        self.assertEqual(\n-            output_without_slicing.detach().cpu().numpy().all(),\n-            output_without_slicing_2.detach().cpu().numpy().all(),\n-            \"Without slicing outputs should match with the outputs when slicing is manually disabled.\",\n-        )\n-\n \n @slow\n class ConsistencyDecoderVAEIntegrationTests(unittest.TestCase):"
        },
        {
          "filename": "tests/models/autoencoders/test_models_vq.py",
          "status": "modified",
          "additions": 4,
          "deletions": 8,
          "changes": 12,
          "patch": "@@ -19,19 +19,15 @@\n \n from diffusers import VQModel\n \n-from ...testing_utils import (\n-    backend_manual_seed,\n-    enable_full_determinism,\n-    floats_tensor,\n-    torch_device,\n-)\n-from ..test_modeling_common import ModelTesterMixin, UNetTesterMixin\n+from ...testing_utils import backend_manual_seed, enable_full_determinism, floats_tensor, torch_device\n+from ..test_modeling_common import ModelTesterMixin\n+from .testing_utils import AutoencoderTesterMixin\n \n \n enable_full_determinism()\n \n \n-class VQModelTests(ModelTesterMixin, UNetTesterMixin, unittest.TestCase):\n+class VQModelTests(ModelTesterMixin, AutoencoderTesterMixin, unittest.TestCase):\n     model_class = VQModel\n     main_input_name = \"sample\"\n "
        },
        {
          "filename": "tests/models/autoencoders/testing_utils.py",
          "status": "added",
          "additions": 142,
          "deletions": 0,
          "changes": 142,
          "patch": "@@ -0,0 +1,142 @@\n+import inspect\n+\n+import numpy as np\n+import pytest\n+import torch\n+\n+from diffusers.models.autoencoders.vae import DecoderOutput\n+from diffusers.utils.torch_utils import torch_device\n+\n+\n+class AutoencoderTesterMixin:\n+    \"\"\"\n+    Test mixin class specific to VAEs to test for slicing and tiling. Diffusion networks\n+    usually don't do slicing and tiling.\n+    \"\"\"\n+\n+    @staticmethod\n+    def _accepts_generator(model):\n+        model_sig = inspect.signature(model.forward)\n+        accepts_generator = \"generator\" in model_sig.parameters\n+        return accepts_generator\n+\n+    @staticmethod\n+    def _accepts_norm_num_groups(model_class):\n+        model_sig = inspect.signature(model_class.__init__)\n+        accepts_norm_groups = \"norm_num_groups\" in model_sig.parameters\n+        return accepts_norm_groups\n+\n+    def test_forward_with_norm_groups(self):\n+        if not self._accepts_norm_num_groups(self.model_class):\n+            pytest.skip(f\"Test not supported for {self.model_class.__name__}\")\n+        init_dict, inputs_dict = self.prepare_init_args_and_inputs_for_common()\n+\n+        init_dict[\"norm_num_groups\"] = 16\n+        init_dict[\"block_out_channels\"] = (16, 32)\n+\n+        model = self.model_class(**init_dict)\n+        model.to(torch_device)\n+        model.eval()\n+\n+        with torch.no_grad():\n+            output = model(**inputs_dict)\n+\n+            if isinstance(output, dict):\n+                output = output.to_tuple()[0]\n+\n+        self.assertIsNotNone(output)\n+        expected_shape = inputs_dict[\"sample\"].shape\n+        self.assertEqual(output.shape, expected_shape, \"Input and output shapes do not match\")\n+\n+    def test_enable_disable_tiling(self):\n+        if not hasattr(self.model_class, \"enable_tiling\"):\n+            pytest.skip(f\"Skipping test as {self.model_class.__name__} doesn't support tiling.\")\n+\n+        init_dict, inputs_dict = self.prepare_init_args_and_inputs_for_common()\n+\n+        torch.manual_seed(0)\n+        model = self.model_class(**init_dict).to(torch_device)\n+\n+        inputs_dict.update({\"return_dict\": False})\n+        _ = inputs_dict.pop(\"generator\", None)\n+        accepts_generator = self._accepts_generator(model)\n+\n+        torch.manual_seed(0)\n+        if accepts_generator:\n+            inputs_dict[\"generator\"] = torch.manual_seed(0)\n+        output_without_tiling = model(**inputs_dict)[0]\n+        # Mochi-1\n+        if isinstance(output_without_tiling, DecoderOutput):\n+            output_without_tiling = output_without_tiling.sample\n+\n+        torch.manual_seed(0)\n+        model.enable_tiling()\n+        if accepts_generator:\n+            inputs_dict[\"generator\"] = torch.manual_seed(0)\n+        output_with_tiling = model(**inputs_dict)[0]\n+        if isinstance(output_with_tiling, DecoderOutput):\n+            output_with_tiling = output_with_tiling.sample\n+\n+        assert (\n+            output_without_tiling.detach().cpu().numpy() - output_with_tiling.detach().cpu().numpy()\n+        ).max() < 0.5, \"VAE tiling should not affect the inference results\"\n+\n+        torch.manual_seed(0)\n+        model.disable_tiling()\n+        if accepts_generator:\n+            inputs_dict[\"generator\"] = torch.manual_seed(0)\n+        output_without_tiling_2 = model(**inputs_dict)[0]\n+        if isinstance(output_without_tiling_2, DecoderOutput):\n+            output_without_tiling_2 = output_without_tiling_2.sample\n+\n+        assert np.allclose(\n+            output_without_tiling.detach().cpu().numpy().all(),\n+            output_without_tiling_2.detach().cpu().numpy().all(),\n+        ), \"Without tiling outputs should match with the outputs when tiling is manually disabled.\"\n+\n+    def test_enable_disable_slicing(self):\n+        if not hasattr(self.model_class, \"enable_slicing\"):\n+            pytest.skip(f\"Skipping test as {self.model_class.__name__} doesn't support slicing.\")\n+\n+        init_dict, inputs_dict = self.prepare_init_args_and_inputs_for_common()\n+\n+        torch.manual_seed(0)\n+        model = self.model_class(**init_dict).to(torch_device)\n+\n+        inputs_dict.update({\"return_dict\": False})\n+        _ = inputs_dict.pop(\"generator\", None)\n+        accepts_generator = self._accepts_generator(model)\n+\n+        if accepts_generator:\n+            inputs_dict[\"generator\"] = torch.manual_seed(0)\n+\n+        torch.manual_seed(0)\n+        output_without_slicing = model(**inputs_dict)[0]\n+        # Mochi-1\n+        if isinstance(output_without_slicing, DecoderOutput):\n+            output_without_slicing = output_without_slicing.sample\n+\n+        torch.manual_seed(0)\n+        model.enable_slicing()\n+        if accepts_generator:\n+            inputs_dict[\"generator\"] = torch.manual_seed(0)\n+        output_with_slicing = model(**inputs_dict)[0]\n+        if isinstance(output_with_slicing, DecoderOutput):\n+            output_with_slicing = output_with_slicing.sample\n+\n+        assert (\n+            output_without_slicing.detach().cpu().numpy() - output_with_slicing.detach().cpu().numpy()\n+        ).max() < 0.5, \"VAE slicing should not affect the inference results\"\n+\n+        torch.manual_seed(0)\n+        model.disable_slicing()\n+        if accepts_generator:\n+            inputs_dict[\"generator\"] = torch.manual_seed(0)\n+        output_without_slicing_2 = model(**inputs_dict)[0]\n+        if isinstance(output_without_slicing_2, DecoderOutput):\n+            output_without_slicing_2 = output_without_slicing_2.sample\n+\n+        assert np.allclose(\n+            output_without_slicing.detach().cpu().numpy().all(),\n+            output_without_slicing_2.detach().cpu().numpy().all(),\n+        ), \"Without slicing outputs should match with the outputs when slicing is manually disabled.\""
        },
        {
          "filename": "tests/models/test_modeling_common.py",
          "status": "modified",
          "additions": 8,
          "deletions": 0,
          "changes": 8,
          "patch": "@@ -450,7 +450,15 @@ def get_dummy_inputs():\n \n \n class UNetTesterMixin:\n+    @staticmethod\n+    def _accepts_norm_num_groups(model_class):\n+        model_sig = inspect.signature(model_class.__init__)\n+        accepts_norm_groups = \"norm_num_groups\" in model_sig.parameters\n+        return accepts_norm_groups\n+\n     def test_forward_with_norm_groups(self):\n+        if not self._accepts_norm_num_groups(self.model_class):\n+            pytest.skip(f\"Test not supported for {self.model_class.__name__}\")\n         init_dict, inputs_dict = self.prepare_init_args_and_inputs_for_common()\n \n         init_dict[\"norm_num_groups\"] = 16"
        }
      ],
      "num_files": 17,
      "scraped_at": "2025-11-16T21:19:16.633842"
    },
    {
      "pr_number": 12364,
      "title": "[tests] xfail some kandinsky tests.",
      "body": "# What does this PR do?\r\n\r\nLatest transformers renders these test slices ineffective. I think it's okay to xfail them for now as Kandinsky pipelines aren't used that much any longer.",
      "html_url": "https://github.com/huggingface/diffusers/pull/12364",
      "created_at": "2025-09-22T05:21:16Z",
      "merged_at": "2025-09-22T05:47:47Z",
      "merge_commit_sha": "843355f89fd043e82b3344d9259e6faa640da6f9",
      "base_ref": "main",
      "head_sha": "78f921bf33bda29e1a8a723f314aaaafc5f854d2",
      "user": "sayakpaul",
      "files": [
        {
          "filename": "src/diffusers/pipelines/qwenimage/__init__.py",
          "status": "modified",
          "additions": 1,
          "deletions": 1,
          "changes": 2,
          "patch": "@@ -27,8 +27,8 @@\n     _import_structure[\"pipeline_qwenimage_controlnet\"] = [\"QwenImageControlNetPipeline\"]\n     _import_structure[\"pipeline_qwenimage_controlnet_inpaint\"] = [\"QwenImageControlNetInpaintPipeline\"]\n     _import_structure[\"pipeline_qwenimage_edit\"] = [\"QwenImageEditPipeline\"]\n-    _import_structure[\"pipeline_qwenimage_edit_plus\"] = [\"QwenImageEditPlusPipeline\"]\n     _import_structure[\"pipeline_qwenimage_edit_inpaint\"] = [\"QwenImageEditInpaintPipeline\"]\n+    _import_structure[\"pipeline_qwenimage_edit_plus\"] = [\"QwenImageEditPlusPipeline\"]\n     _import_structure[\"pipeline_qwenimage_img2img\"] = [\"QwenImageImg2ImgPipeline\"]\n     _import_structure[\"pipeline_qwenimage_inpaint\"] = [\"QwenImageInpaintPipeline\"]\n "
        },
        {
          "filename": "tests/pipelines/kandinsky/test_kandinsky.py",
          "status": "modified",
          "additions": 5,
          "deletions": 0,
          "changes": 5,
          "patch": "@@ -18,11 +18,13 @@\n import unittest\n \n import numpy as np\n+import pytest\n import torch\n from transformers import XLMRobertaTokenizerFast\n \n from diffusers import DDIMScheduler, KandinskyPipeline, KandinskyPriorPipeline, UNet2DConditionModel, VQModel\n from diffusers.pipelines.kandinsky.text_encoder import MCLIPConfig, MultilingualCLIP\n+from diffusers.utils import is_transformers_version\n \n from ...testing_utils import (\n     backend_empty_cache,\n@@ -215,6 +217,9 @@ def get_dummy_inputs(self, device, seed=0):\n         dummy = Dummies()\n         return dummy.get_dummy_inputs(device=device, seed=seed)\n \n+    @pytest.mark.xfail(\n+        condition=is_transformers_version(\">=\", \"4.56.2\"), reason=\"Latest transformers changes the slices\", strict=True\n+    )\n     def test_kandinsky(self):\n         device = \"cpu\"\n "
        },
        {
          "filename": "tests/pipelines/kandinsky/test_kandinsky_combined.py",
          "status": "modified",
          "additions": 11,
          "deletions": 0,
          "changes": 11,
          "patch": "@@ -16,8 +16,10 @@\n import unittest\n \n import numpy as np\n+import pytest\n \n from diffusers import KandinskyCombinedPipeline, KandinskyImg2ImgCombinedPipeline, KandinskyInpaintCombinedPipeline\n+from diffusers.utils import is_transformers_version\n \n from ...testing_utils import enable_full_determinism, require_torch_accelerator, torch_device\n from ..test_pipelines_common import PipelineTesterMixin\n@@ -73,6 +75,9 @@ def get_dummy_inputs(self, device, seed=0):\n         )\n         return inputs\n \n+    @pytest.mark.xfail(\n+        condition=is_transformers_version(\">=\", \"4.56.2\"), reason=\"Latest transformers changes the slices\", strict=True\n+    )\n     def test_kandinsky(self):\n         device = \"cpu\"\n \n@@ -181,6 +186,9 @@ def get_dummy_inputs(self, device, seed=0):\n         inputs.pop(\"negative_image_embeds\")\n         return inputs\n \n+    @pytest.mark.xfail(\n+        condition=is_transformers_version(\">=\", \"4.56.2\"), reason=\"Latest transformers changes the slices\", strict=True\n+    )\n     def test_kandinsky(self):\n         device = \"cpu\"\n \n@@ -292,6 +300,9 @@ def get_dummy_inputs(self, device, seed=0):\n         inputs.pop(\"negative_image_embeds\")\n         return inputs\n \n+    @pytest.mark.xfail(\n+        condition=is_transformers_version(\">=\", \"4.56.2\"), reason=\"Latest transformers changes the slices\", strict=True\n+    )\n     def test_kandinsky(self):\n         device = \"cpu\"\n "
        },
        {
          "filename": "tests/pipelines/kandinsky/test_kandinsky_img2img.py",
          "status": "modified",
          "additions": 5,
          "deletions": 0,
          "changes": 5,
          "patch": "@@ -18,6 +18,7 @@\n import unittest\n \n import numpy as np\n+import pytest\n import torch\n from PIL import Image\n from transformers import XLMRobertaTokenizerFast\n@@ -31,6 +32,7 @@\n     VQModel,\n )\n from diffusers.pipelines.kandinsky.text_encoder import MCLIPConfig, MultilingualCLIP\n+from diffusers.utils import is_transformers_version\n \n from ...testing_utils import (\n     backend_empty_cache,\n@@ -237,6 +239,9 @@ def get_dummy_inputs(self, device, seed=0):\n         dummies = Dummies()\n         return dummies.get_dummy_inputs(device=device, seed=seed)\n \n+    @pytest.mark.xfail(\n+        condition=is_transformers_version(\">=\", \"4.56.2\"), reason=\"Latest transformers changes the slices\", strict=True\n+    )\n     def test_kandinsky_img2img(self):\n         device = \"cpu\"\n "
        },
        {
          "filename": "tests/pipelines/kandinsky/test_kandinsky_inpaint.py",
          "status": "modified",
          "additions": 5,
          "deletions": 0,
          "changes": 5,
          "patch": "@@ -18,12 +18,14 @@\n import unittest\n \n import numpy as np\n+import pytest\n import torch\n from PIL import Image\n from transformers import XLMRobertaTokenizerFast\n \n from diffusers import DDIMScheduler, KandinskyInpaintPipeline, KandinskyPriorPipeline, UNet2DConditionModel, VQModel\n from diffusers.pipelines.kandinsky.text_encoder import MCLIPConfig, MultilingualCLIP\n+from diffusers.utils import is_transformers_version\n \n from ...testing_utils import (\n     backend_empty_cache,\n@@ -231,6 +233,9 @@ def get_dummy_inputs(self, device, seed=0):\n         dummies = Dummies()\n         return dummies.get_dummy_inputs(device=device, seed=seed)\n \n+    @pytest.mark.xfail(\n+        condition=is_transformers_version(\">=\", \"4.56.2\"), reason=\"Latest transformers changes the slices\", strict=True\n+    )\n     def test_kandinsky_inpaint(self):\n         device = \"cpu\"\n "
        }
      ],
      "num_files": 5,
      "scraped_at": "2025-11-16T21:19:17.565966"
    },
    {
      "pr_number": 12357,
      "title": "feat: Add QwenImageEditPlus to support future feature upgrades",
      "body": "Introduces QwenImageEditPlusPipeline to support upcoming version upgrade.\r\n\r\ncc @yiyixuxu @sayakpaul ",
      "html_url": "https://github.com/huggingface/diffusers/pull/12357",
      "created_at": "2025-09-20T10:21:30Z",
      "merged_at": "2025-09-21T16:10:52Z",
      "merge_commit_sha": "df267ee4e8500a2ef5960879f6d1ea49cc8ec40d",
      "base_ref": "main",
      "head_sha": "6eeb3044707f804bea769dc954d4fde11d63b790",
      "user": "naykun",
      "files": [
        {
          "filename": "src/diffusers/__init__.py",
          "status": "modified",
          "additions": 2,
          "deletions": 0,
          "changes": 2,
          "patch": "@@ -514,6 +514,7 @@\n             \"QwenImageControlNetPipeline\",\n             \"QwenImageEditInpaintPipeline\",\n             \"QwenImageEditPipeline\",\n+            \"QwenImageEditPlusPipeline\",\n             \"QwenImageImg2ImgPipeline\",\n             \"QwenImageInpaintPipeline\",\n             \"QwenImagePipeline\",\n@@ -1168,6 +1169,7 @@\n             QwenImageControlNetPipeline,\n             QwenImageEditInpaintPipeline,\n             QwenImageEditPipeline,\n+            QwenImageEditPlusPipeline,\n             QwenImageImg2ImgPipeline,\n             QwenImageInpaintPipeline,\n             QwenImagePipeline,"
        },
        {
          "filename": "src/diffusers/pipelines/__init__.py",
          "status": "modified",
          "additions": 2,
          "deletions": 0,
          "changes": 2,
          "patch": "@@ -393,6 +393,7 @@\n         \"QwenImageImg2ImgPipeline\",\n         \"QwenImageInpaintPipeline\",\n         \"QwenImageEditPipeline\",\n+        \"QwenImageEditPlusPipeline\",\n         \"QwenImageEditInpaintPipeline\",\n         \"QwenImageControlNetInpaintPipeline\",\n         \"QwenImageControlNetPipeline\",\n@@ -719,6 +720,7 @@\n             QwenImageControlNetPipeline,\n             QwenImageEditInpaintPipeline,\n             QwenImageEditPipeline,\n+            QwenImageEditPlusPipeline,\n             QwenImageImg2ImgPipeline,\n             QwenImageInpaintPipeline,\n             QwenImagePipeline,"
        },
        {
          "filename": "src/diffusers/pipelines/qwenimage/__init__.py",
          "status": "modified",
          "additions": 2,
          "deletions": 0,
          "changes": 2,
          "patch": "@@ -27,6 +27,7 @@\n     _import_structure[\"pipeline_qwenimage_controlnet\"] = [\"QwenImageControlNetPipeline\"]\n     _import_structure[\"pipeline_qwenimage_controlnet_inpaint\"] = [\"QwenImageControlNetInpaintPipeline\"]\n     _import_structure[\"pipeline_qwenimage_edit\"] = [\"QwenImageEditPipeline\"]\n+    _import_structure[\"pipeline_qwenimage_edit_plus\"] = [\"QwenImageEditPlusPipeline\"]\n     _import_structure[\"pipeline_qwenimage_edit_inpaint\"] = [\"QwenImageEditInpaintPipeline\"]\n     _import_structure[\"pipeline_qwenimage_img2img\"] = [\"QwenImageImg2ImgPipeline\"]\n     _import_structure[\"pipeline_qwenimage_inpaint\"] = [\"QwenImageInpaintPipeline\"]\n@@ -43,6 +44,7 @@\n         from .pipeline_qwenimage_controlnet_inpaint import QwenImageControlNetInpaintPipeline\n         from .pipeline_qwenimage_edit import QwenImageEditPipeline\n         from .pipeline_qwenimage_edit_inpaint import QwenImageEditInpaintPipeline\n+        from .pipeline_qwenimage_edit_plus import QwenImageEditPlusPipeline\n         from .pipeline_qwenimage_img2img import QwenImageImg2ImgPipeline\n         from .pipeline_qwenimage_inpaint import QwenImageInpaintPipeline\n else:"
        },
        {
          "filename": "src/diffusers/pipelines/qwenimage/pipeline_qwenimage_edit.py",
          "status": "modified",
          "additions": 0,
          "deletions": 1,
          "changes": 1,
          "patch": "@@ -208,7 +208,6 @@ def __init__(\n         # QwenImage latents are turned into 2x2 patches and packed. This means the latent width and height has to be divisible\n         # by the patch size. So the vae scale factor is multiplied by the patch size to account for this\n         self.image_processor = VaeImageProcessor(vae_scale_factor=self.vae_scale_factor * 2)\n-        self.vl_processor = processor\n         self.tokenizer_max_length = 1024\n \n         self.prompt_template_encode = \"<|im_start|>system\\nDescribe the key features of the input image (color, shape, size, texture, objects, background), then explain how the user's text instruction should alter or modify the image. Generate a new image that meets the user's requirements while maintaining consistency with the original input where appropriate.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>{}<|im_end|>\\n<|im_start|>assistant\\n\""
        },
        {
          "filename": "src/diffusers/pipelines/qwenimage/pipeline_qwenimage_edit_plus.py",
          "status": "added",
          "additions": 883,
          "deletions": 0,
          "changes": 883,
          "patch": "@@ -0,0 +1,883 @@\n+# Copyright 2025 Qwen-Image Team and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import inspect\n+import math\n+from typing import Any, Callable, Dict, List, Optional, Union\n+\n+import numpy as np\n+import torch\n+from transformers import Qwen2_5_VLForConditionalGeneration, Qwen2Tokenizer, Qwen2VLProcessor\n+\n+from ...image_processor import PipelineImageInput, VaeImageProcessor\n+from ...loaders import QwenImageLoraLoaderMixin\n+from ...models import AutoencoderKLQwenImage, QwenImageTransformer2DModel\n+from ...schedulers import FlowMatchEulerDiscreteScheduler\n+from ...utils import is_torch_xla_available, logging, replace_example_docstring\n+from ...utils.torch_utils import randn_tensor\n+from ..pipeline_utils import DiffusionPipeline\n+from .pipeline_output import QwenImagePipelineOutput\n+\n+\n+if is_torch_xla_available():\n+    import torch_xla.core.xla_model as xm\n+\n+    XLA_AVAILABLE = True\n+else:\n+    XLA_AVAILABLE = False\n+\n+\n+logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n+\n+EXAMPLE_DOC_STRING = \"\"\"\n+    Examples:\n+        ```py\n+        >>> import torch\n+        >>> from PIL import Image\n+        >>> from diffusers import QwenImageEditPlusPipeline\n+        >>> from diffusers.utils import load_image\n+\n+        >>> pipe = QwenImageEditPlusPipeline.from_pretrained(\"Qwen/Qwen-Image-Edit-2509\", torch_dtype=torch.bfloat16)\n+        >>> pipe.to(\"cuda\")\n+        >>> image = load_image(\n+        ...     \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/yarn-art-pikachu.png\"\n+        ... ).convert(\"RGB\")\n+        >>> prompt = (\n+        ...     \"Make Pikachu hold a sign that says 'Qwen Edit is awesome', yarn art style, detailed, vibrant colors\"\n+        ... )\n+        >>> # Depending on the variant being used, the pipeline call will slightly vary.\n+        >>> # Refer to the pipeline documentation for more details.\n+        >>> image = pipe(image, prompt, num_inference_steps=50).images[0]\n+        >>> image.save(\"qwenimage_edit_plus.png\")\n+        ```\n+\"\"\"\n+\n+CONDITION_IMAGE_SIZE = 384 * 384\n+VAE_IMAGE_SIZE = 1024 * 1024\n+\n+\n+# Copied from diffusers.pipelines.qwenimage.pipeline_qwenimage.calculate_shift\n+def calculate_shift(\n+    image_seq_len,\n+    base_seq_len: int = 256,\n+    max_seq_len: int = 4096,\n+    base_shift: float = 0.5,\n+    max_shift: float = 1.15,\n+):\n+    m = (max_shift - base_shift) / (max_seq_len - base_seq_len)\n+    b = base_shift - m * base_seq_len\n+    mu = image_seq_len * m + b\n+    return mu\n+\n+\n+# Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.retrieve_timesteps\n+def retrieve_timesteps(\n+    scheduler,\n+    num_inference_steps: Optional[int] = None,\n+    device: Optional[Union[str, torch.device]] = None,\n+    timesteps: Optional[List[int]] = None,\n+    sigmas: Optional[List[float]] = None,\n+    **kwargs,\n+):\n+    r\"\"\"\n+    Calls the scheduler's `set_timesteps` method and retrieves timesteps from the scheduler after the call. Handles\n+    custom timesteps. Any kwargs will be supplied to `scheduler.set_timesteps`.\n+\n+    Args:\n+        scheduler (`SchedulerMixin`):\n+            The scheduler to get timesteps from.\n+        num_inference_steps (`int`):\n+            The number of diffusion steps used when generating samples with a pre-trained model. If used, `timesteps`\n+            must be `None`.\n+        device (`str` or `torch.device`, *optional*):\n+            The device to which the timesteps should be moved to. If `None`, the timesteps are not moved.\n+        timesteps (`List[int]`, *optional*):\n+            Custom timesteps used to override the timestep spacing strategy of the scheduler. If `timesteps` is passed,\n+            `num_inference_steps` and `sigmas` must be `None`.\n+        sigmas (`List[float]`, *optional*):\n+            Custom sigmas used to override the timestep spacing strategy of the scheduler. If `sigmas` is passed,\n+            `num_inference_steps` and `timesteps` must be `None`.\n+\n+    Returns:\n+        `Tuple[torch.Tensor, int]`: A tuple where the first element is the timestep schedule from the scheduler and the\n+        second element is the number of inference steps.\n+    \"\"\"\n+    if timesteps is not None and sigmas is not None:\n+        raise ValueError(\"Only one of `timesteps` or `sigmas` can be passed. Please choose one to set custom values\")\n+    if timesteps is not None:\n+        accepts_timesteps = \"timesteps\" in set(inspect.signature(scheduler.set_timesteps).parameters.keys())\n+        if not accepts_timesteps:\n+            raise ValueError(\n+                f\"The current scheduler class {scheduler.__class__}'s `set_timesteps` does not support custom\"\n+                f\" timestep schedules. Please check whether you are using the correct scheduler.\"\n+            )\n+        scheduler.set_timesteps(timesteps=timesteps, device=device, **kwargs)\n+        timesteps = scheduler.timesteps\n+        num_inference_steps = len(timesteps)\n+    elif sigmas is not None:\n+        accept_sigmas = \"sigmas\" in set(inspect.signature(scheduler.set_timesteps).parameters.keys())\n+        if not accept_sigmas:\n+            raise ValueError(\n+                f\"The current scheduler class {scheduler.__class__}'s `set_timesteps` does not support custom\"\n+                f\" sigmas schedules. Please check whether you are using the correct scheduler.\"\n+            )\n+        scheduler.set_timesteps(sigmas=sigmas, device=device, **kwargs)\n+        timesteps = scheduler.timesteps\n+        num_inference_steps = len(timesteps)\n+    else:\n+        scheduler.set_timesteps(num_inference_steps, device=device, **kwargs)\n+        timesteps = scheduler.timesteps\n+    return timesteps, num_inference_steps\n+\n+\n+# Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_img2img.retrieve_latents\n+def retrieve_latents(\n+    encoder_output: torch.Tensor, generator: Optional[torch.Generator] = None, sample_mode: str = \"sample\"\n+):\n+    if hasattr(encoder_output, \"latent_dist\") and sample_mode == \"sample\":\n+        return encoder_output.latent_dist.sample(generator)\n+    elif hasattr(encoder_output, \"latent_dist\") and sample_mode == \"argmax\":\n+        return encoder_output.latent_dist.mode()\n+    elif hasattr(encoder_output, \"latents\"):\n+        return encoder_output.latents\n+    else:\n+        raise AttributeError(\"Could not access latents of provided encoder_output\")\n+\n+\n+def calculate_dimensions(target_area, ratio):\n+    width = math.sqrt(target_area * ratio)\n+    height = width / ratio\n+\n+    width = round(width / 32) * 32\n+    height = round(height / 32) * 32\n+\n+    return width, height\n+\n+\n+class QwenImageEditPlusPipeline(DiffusionPipeline, QwenImageLoraLoaderMixin):\n+    r\"\"\"\n+    The Qwen-Image-Edit pipeline for image editing.\n+\n+    Args:\n+        transformer ([`QwenImageTransformer2DModel`]):\n+            Conditional Transformer (MMDiT) architecture to denoise the encoded image latents.\n+        scheduler ([`FlowMatchEulerDiscreteScheduler`]):\n+            A scheduler to be used in combination with `transformer` to denoise the encoded image latents.\n+        vae ([`AutoencoderKL`]):\n+            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n+        text_encoder ([`Qwen2.5-VL-7B-Instruct`]):\n+            [Qwen2.5-VL-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct), specifically the\n+            [Qwen2.5-VL-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct) variant.\n+        tokenizer (`QwenTokenizer`):\n+            Tokenizer of class\n+            [CLIPTokenizer](https://huggingface.co/docs/transformers/en/model_doc/clip#transformers.CLIPTokenizer).\n+    \"\"\"\n+\n+    model_cpu_offload_seq = \"text_encoder->transformer->vae\"\n+    _callback_tensor_inputs = [\"latents\", \"prompt_embeds\"]\n+\n+    def __init__(\n+        self,\n+        scheduler: FlowMatchEulerDiscreteScheduler,\n+        vae: AutoencoderKLQwenImage,\n+        text_encoder: Qwen2_5_VLForConditionalGeneration,\n+        tokenizer: Qwen2Tokenizer,\n+        processor: Qwen2VLProcessor,\n+        transformer: QwenImageTransformer2DModel,\n+    ):\n+        super().__init__()\n+\n+        self.register_modules(\n+            vae=vae,\n+            text_encoder=text_encoder,\n+            tokenizer=tokenizer,\n+            processor=processor,\n+            transformer=transformer,\n+            scheduler=scheduler,\n+        )\n+        self.vae_scale_factor = 2 ** len(self.vae.temperal_downsample) if getattr(self, \"vae\", None) else 8\n+        self.latent_channels = self.vae.config.z_dim if getattr(self, \"vae\", None) else 16\n+        # QwenImage latents are turned into 2x2 patches and packed. This means the latent width and height has to be divisible\n+        # by the patch size. So the vae scale factor is multiplied by the patch size to account for this\n+        self.image_processor = VaeImageProcessor(vae_scale_factor=self.vae_scale_factor * 2)\n+        self.tokenizer_max_length = 1024\n+\n+        self.prompt_template_encode = \"<|im_start|>system\\nDescribe the key features of the input image (color, shape, size, texture, objects, background), then explain how the user's text instruction should alter or modify the image. Generate a new image that meets the user's requirements while maintaining consistency with the original input where appropriate.<|im_end|>\\n<|im_start|>user\\n{}<|im_end|>\\n<|im_start|>assistant\\n\"\n+        self.prompt_template_encode_start_idx = 64\n+        self.default_sample_size = 128\n+\n+    # Copied from diffusers.pipelines.qwenimage.pipeline_qwenimage.QwenImagePipeline._extract_masked_hidden\n+    def _extract_masked_hidden(self, hidden_states: torch.Tensor, mask: torch.Tensor):\n+        bool_mask = mask.bool()\n+        valid_lengths = bool_mask.sum(dim=1)\n+        selected = hidden_states[bool_mask]\n+        split_result = torch.split(selected, valid_lengths.tolist(), dim=0)\n+\n+        return split_result\n+\n+    def _get_qwen_prompt_embeds(\n+        self,\n+        prompt: Union[str, List[str]] = None,\n+        image: Optional[torch.Tensor] = None,\n+        device: Optional[torch.device] = None,\n+        dtype: Optional[torch.dtype] = None,\n+    ):\n+        device = device or self._execution_device\n+        dtype = dtype or self.text_encoder.dtype\n+\n+        prompt = [prompt] if isinstance(prompt, str) else prompt\n+        img_prompt_template = \"Picture {}: <|vision_start|><|image_pad|><|vision_end|>\"\n+        if isinstance(image, list):\n+            base_img_prompt = \"\"\n+            for i, img in enumerate(image):\n+                base_img_prompt += img_prompt_template.format(i + 1)\n+        elif image is not None:\n+            base_img_prompt = img_prompt_template.format(1)\n+        else:\n+            base_img_prompt = \"\"\n+\n+        template = self.prompt_template_encode\n+\n+        drop_idx = self.prompt_template_encode_start_idx\n+        txt = [template.format(base_img_prompt + e) for e in prompt]\n+\n+        model_inputs = self.processor(\n+            text=txt,\n+            images=image,\n+            padding=True,\n+            return_tensors=\"pt\",\n+        ).to(device)\n+\n+        outputs = self.text_encoder(\n+            input_ids=model_inputs.input_ids,\n+            attention_mask=model_inputs.attention_mask,\n+            pixel_values=model_inputs.pixel_values,\n+            image_grid_thw=model_inputs.image_grid_thw,\n+            output_hidden_states=True,\n+        )\n+\n+        hidden_states = outputs.hidden_states[-1]\n+        split_hidden_states = self._extract_masked_hidden(hidden_states, model_inputs.attention_mask)\n+        split_hidden_states = [e[drop_idx:] for e in split_hidden_states]\n+        attn_mask_list = [torch.ones(e.size(0), dtype=torch.long, device=e.device) for e in split_hidden_states]\n+        max_seq_len = max([e.size(0) for e in split_hidden_states])\n+        prompt_embeds = torch.stack(\n+            [torch.cat([u, u.new_zeros(max_seq_len - u.size(0), u.size(1))]) for u in split_hidden_states]\n+        )\n+        encoder_attention_mask = torch.stack(\n+            [torch.cat([u, u.new_zeros(max_seq_len - u.size(0))]) for u in attn_mask_list]\n+        )\n+\n+        prompt_embeds = prompt_embeds.to(dtype=dtype, device=device)\n+\n+        return prompt_embeds, encoder_attention_mask\n+\n+    # Copied from diffusers.pipelines.qwenimage.pipeline_qwenimage_edit.QwenImageEditPipeline.encode_prompt\n+    def encode_prompt(\n+        self,\n+        prompt: Union[str, List[str]],\n+        image: Optional[torch.Tensor] = None,\n+        device: Optional[torch.device] = None,\n+        num_images_per_prompt: int = 1,\n+        prompt_embeds: Optional[torch.Tensor] = None,\n+        prompt_embeds_mask: Optional[torch.Tensor] = None,\n+        max_sequence_length: int = 1024,\n+    ):\n+        r\"\"\"\n+\n+        Args:\n+            prompt (`str` or `List[str]`, *optional*):\n+                prompt to be encoded\n+            image (`torch.Tensor`, *optional*):\n+                image to be encoded\n+            device: (`torch.device`):\n+                torch device\n+            num_images_per_prompt (`int`):\n+                number of images that should be generated per prompt\n+            prompt_embeds (`torch.Tensor`, *optional*):\n+                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n+                provided, text embeddings will be generated from `prompt` input argument.\n+        \"\"\"\n+        device = device or self._execution_device\n+\n+        prompt = [prompt] if isinstance(prompt, str) else prompt\n+        batch_size = len(prompt) if prompt_embeds is None else prompt_embeds.shape[0]\n+\n+        if prompt_embeds is None:\n+            prompt_embeds, prompt_embeds_mask = self._get_qwen_prompt_embeds(prompt, image, device)\n+\n+        _, seq_len, _ = prompt_embeds.shape\n+        prompt_embeds = prompt_embeds.repeat(1, num_images_per_prompt, 1)\n+        prompt_embeds = prompt_embeds.view(batch_size * num_images_per_prompt, seq_len, -1)\n+        prompt_embeds_mask = prompt_embeds_mask.repeat(1, num_images_per_prompt, 1)\n+        prompt_embeds_mask = prompt_embeds_mask.view(batch_size * num_images_per_prompt, seq_len)\n+\n+        return prompt_embeds, prompt_embeds_mask\n+\n+    # Copied from diffusers.pipelines.qwenimage.pipeline_qwenimage_edit.QwenImageEditPipeline.check_inputs\n+    def check_inputs(\n+        self,\n+        prompt,\n+        height,\n+        width,\n+        negative_prompt=None,\n+        prompt_embeds=None,\n+        negative_prompt_embeds=None,\n+        prompt_embeds_mask=None,\n+        negative_prompt_embeds_mask=None,\n+        callback_on_step_end_tensor_inputs=None,\n+        max_sequence_length=None,\n+    ):\n+        if height % (self.vae_scale_factor * 2) != 0 or width % (self.vae_scale_factor * 2) != 0:\n+            logger.warning(\n+                f\"`height` and `width` have to be divisible by {self.vae_scale_factor * 2} but are {height} and {width}. Dimensions will be resized accordingly\"\n+            )\n+\n+        if callback_on_step_end_tensor_inputs is not None and not all(\n+            k in self._callback_tensor_inputs for k in callback_on_step_end_tensor_inputs\n+        ):\n+            raise ValueError(\n+                f\"`callback_on_step_end_tensor_inputs` has to be in {self._callback_tensor_inputs}, but found {[k for k in callback_on_step_end_tensor_inputs if k not in self._callback_tensor_inputs]}\"\n+            )\n+\n+        if prompt is not None and prompt_embeds is not None:\n+            raise ValueError(\n+                f\"Cannot forward both `prompt`: {prompt} and `prompt_embeds`: {prompt_embeds}. Please make sure to\"\n+                \" only forward one of the two.\"\n+            )\n+        elif prompt is None and prompt_embeds is None:\n+            raise ValueError(\n+                \"Provide either `prompt` or `prompt_embeds`. Cannot leave both `prompt` and `prompt_embeds` undefined.\"\n+            )\n+        elif prompt is not None and (not isinstance(prompt, str) and not isinstance(prompt, list)):\n+            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n+\n+        if negative_prompt is not None and negative_prompt_embeds is not None:\n+            raise ValueError(\n+                f\"Cannot forward both `negative_prompt`: {negative_prompt} and `negative_prompt_embeds`:\"\n+                f\" {negative_prompt_embeds}. Please make sure to only forward one of the two.\"\n+            )\n+\n+        if prompt_embeds is not None and prompt_embeds_mask is None:\n+            raise ValueError(\n+                \"If `prompt_embeds` are provided, `prompt_embeds_mask` also have to be passed. Make sure to generate `prompt_embeds_mask` from the same text encoder that was used to generate `prompt_embeds`.\"\n+            )\n+        if negative_prompt_embeds is not None and negative_prompt_embeds_mask is None:\n+            raise ValueError(\n+                \"If `negative_prompt_embeds` are provided, `negative_prompt_embeds_mask` also have to be passed. Make sure to generate `negative_prompt_embeds_mask` from the same text encoder that was used to generate `negative_prompt_embeds`.\"\n+            )\n+\n+        if max_sequence_length is not None and max_sequence_length > 1024:\n+            raise ValueError(f\"`max_sequence_length` cannot be greater than 1024 but is {max_sequence_length}\")\n+\n+    @staticmethod\n+    # Copied from diffusers.pipelines.qwenimage.pipeline_qwenimage.QwenImagePipeline._pack_latents\n+    def _pack_latents(latents, batch_size, num_channels_latents, height, width):\n+        latents = latents.view(batch_size, num_channels_latents, height // 2, 2, width // 2, 2)\n+        latents = latents.permute(0, 2, 4, 1, 3, 5)\n+        latents = latents.reshape(batch_size, (height // 2) * (width // 2), num_channels_latents * 4)\n+\n+        return latents\n+\n+    @staticmethod\n+    # Copied from diffusers.pipelines.qwenimage.pipeline_qwenimage.QwenImagePipeline._unpack_latents\n+    def _unpack_latents(latents, height, width, vae_scale_factor):\n+        batch_size, num_patches, channels = latents.shape\n+\n+        # VAE applies 8x compression on images but we must also account for packing which requires\n+        # latent height and width to be divisible by 2.\n+        height = 2 * (int(height) // (vae_scale_factor * 2))\n+        width = 2 * (int(width) // (vae_scale_factor * 2))\n+\n+        latents = latents.view(batch_size, height // 2, width // 2, channels // 4, 2, 2)\n+        latents = latents.permute(0, 3, 1, 4, 2, 5)\n+\n+        latents = latents.reshape(batch_size, channels // (2 * 2), 1, height, width)\n+\n+        return latents\n+\n+    # Copied from diffusers.pipelines.qwenimage.pipeline_qwenimage_edit.QwenImageEditPipeline._encode_vae_image\n+    def _encode_vae_image(self, image: torch.Tensor, generator: torch.Generator):\n+        if isinstance(generator, list):\n+            image_latents = [\n+                retrieve_latents(self.vae.encode(image[i : i + 1]), generator=generator[i], sample_mode=\"argmax\")\n+                for i in range(image.shape[0])\n+            ]\n+            image_latents = torch.cat(image_latents, dim=0)\n+        else:\n+            image_latents = retrieve_latents(self.vae.encode(image), generator=generator, sample_mode=\"argmax\")\n+        latents_mean = (\n+            torch.tensor(self.vae.config.latents_mean)\n+            .view(1, self.latent_channels, 1, 1, 1)\n+            .to(image_latents.device, image_latents.dtype)\n+        )\n+        latents_std = (\n+            torch.tensor(self.vae.config.latents_std)\n+            .view(1, self.latent_channels, 1, 1, 1)\n+            .to(image_latents.device, image_latents.dtype)\n+        )\n+        image_latents = (image_latents - latents_mean) / latents_std\n+\n+        return image_latents\n+\n+    def prepare_latents(\n+        self,\n+        images,\n+        batch_size,\n+        num_channels_latents,\n+        height,\n+        width,\n+        dtype,\n+        device,\n+        generator,\n+        latents=None,\n+    ):\n+        # VAE applies 8x compression on images but we must also account for packing which requires\n+        # latent height and width to be divisible by 2.\n+        height = 2 * (int(height) // (self.vae_scale_factor * 2))\n+        width = 2 * (int(width) // (self.vae_scale_factor * 2))\n+\n+        shape = (batch_size, 1, num_channels_latents, height, width)\n+\n+        image_latents = None\n+        if images is not None:\n+            if not isinstance(images, list):\n+                images = [images]\n+            all_image_latents = []\n+            for image in images:\n+                image = image.to(device=device, dtype=dtype)\n+                if image.shape[1] != self.latent_channels:\n+                    image_latents = self._encode_vae_image(image=image, generator=generator)\n+                else:\n+                    image_latents = image\n+                if batch_size > image_latents.shape[0] and batch_size % image_latents.shape[0] == 0:\n+                    # expand init_latents for batch_size\n+                    additional_image_per_prompt = batch_size // image_latents.shape[0]\n+                    image_latents = torch.cat([image_latents] * additional_image_per_prompt, dim=0)\n+                elif batch_size > image_latents.shape[0] and batch_size % image_latents.shape[0] != 0:\n+                    raise ValueError(\n+                        f\"Cannot duplicate `image` of batch size {image_latents.shape[0]} to {batch_size} text prompts.\"\n+                    )\n+                else:\n+                    image_latents = torch.cat([image_latents], dim=0)\n+\n+                image_latent_height, image_latent_width = image_latents.shape[3:]\n+                image_latents = self._pack_latents(\n+                    image_latents, batch_size, num_channels_latents, image_latent_height, image_latent_width\n+                )\n+                all_image_latents.append(image_latents)\n+            image_latents = torch.cat(all_image_latents, dim=1)\n+\n+        if isinstance(generator, list) and len(generator) != batch_size:\n+            raise ValueError(\n+                f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n+                f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n+            )\n+        if latents is None:\n+            latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n+            latents = self._pack_latents(latents, batch_size, num_channels_latents, height, width)\n+        else:\n+            latents = latents.to(device=device, dtype=dtype)\n+\n+        return latents, image_latents\n+\n+    @property\n+    def guidance_scale(self):\n+        return self._guidance_scale\n+\n+    @property\n+    def attention_kwargs(self):\n+        return self._attention_kwargs\n+\n+    @property\n+    def num_timesteps(self):\n+        return self._num_timesteps\n+\n+    @property\n+    def current_timestep(self):\n+        return self._current_timestep\n+\n+    @property\n+    def interrupt(self):\n+        return self._interrupt\n+\n+    @torch.no_grad()\n+    @replace_example_docstring(EXAMPLE_DOC_STRING)\n+    def __call__(\n+        self,\n+        image: Optional[PipelineImageInput] = None,\n+        prompt: Union[str, List[str]] = None,\n+        negative_prompt: Union[str, List[str]] = None,\n+        true_cfg_scale: float = 4.0,\n+        height: Optional[int] = None,\n+        width: Optional[int] = None,\n+        num_inference_steps: int = 50,\n+        sigmas: Optional[List[float]] = None,\n+        guidance_scale: Optional[float] = None,\n+        num_images_per_prompt: int = 1,\n+        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n+        latents: Optional[torch.Tensor] = None,\n+        prompt_embeds: Optional[torch.Tensor] = None,\n+        prompt_embeds_mask: Optional[torch.Tensor] = None,\n+        negative_prompt_embeds: Optional[torch.Tensor] = None,\n+        negative_prompt_embeds_mask: Optional[torch.Tensor] = None,\n+        output_type: Optional[str] = \"pil\",\n+        return_dict: bool = True,\n+        attention_kwargs: Optional[Dict[str, Any]] = None,\n+        callback_on_step_end: Optional[Callable[[int, int, Dict], None]] = None,\n+        callback_on_step_end_tensor_inputs: List[str] = [\"latents\"],\n+        max_sequence_length: int = 512,\n+    ):\n+        r\"\"\"\n+        Function invoked when calling the pipeline for generation.\n+\n+        Args:\n+            image (`torch.Tensor`, `PIL.Image.Image`, `np.ndarray`, `List[torch.Tensor]`, `List[PIL.Image.Image]`, or `List[np.ndarray]`):\n+                `Image`, numpy array or tensor representing an image batch to be used as the starting point. For both\n+                numpy array and pytorch tensor, the expected value range is between `[0, 1]` If it's a tensor or a list\n+                or tensors, the expected shape should be `(B, C, H, W)` or `(C, H, W)`. If it is a numpy array or a\n+                list of arrays, the expected shape should be `(B, H, W, C)` or `(H, W, C)` It can also accept image\n+                latents as `image`, but if passing latents directly it is not encoded again.\n+            prompt (`str` or `List[str]`, *optional*):\n+                The prompt or prompts to guide the image generation. If not defined, one has to pass `prompt_embeds`.\n+                instead.\n+            negative_prompt (`str` or `List[str]`, *optional*):\n+                The prompt or prompts not to guide the image generation. If not defined, one has to pass\n+                `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `true_cfg_scale` is\n+                not greater than `1`).\n+            true_cfg_scale (`float`, *optional*, defaults to 1.0):\n+                true_cfg_scale (`float`, *optional*, defaults to 1.0): Guidance scale as defined in [Classifier-Free\n+                Diffusion Guidance](https://huggingface.co/papers/2207.12598). `true_cfg_scale` is defined as `w` of\n+                equation 2. of [Imagen Paper](https://huggingface.co/papers/2205.11487). Classifier-free guidance is\n+                enabled by setting `true_cfg_scale > 1` and a provided `negative_prompt`. Higher guidance scale\n+                encourages to generate images that are closely linked to the text `prompt`, usually at the expense of\n+                lower image quality.\n+            height (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n+                The height in pixels of the generated image. This is set to 1024 by default for the best results.\n+            width (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n+                The width in pixels of the generated image. This is set to 1024 by default for the best results.\n+            num_inference_steps (`int`, *optional*, defaults to 50):\n+                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n+                expense of slower inference.\n+            sigmas (`List[float]`, *optional*):\n+                Custom sigmas to use for the denoising process with schedulers which support a `sigmas` argument in\n+                their `set_timesteps` method. If not defined, the default behavior when `num_inference_steps` is passed\n+                will be used.\n+            guidance_scale (`float`, *optional*, defaults to None):\n+                A guidance scale value for guidance distilled models. Unlike the traditional classifier-free guidance\n+                where the guidance scale is applied during inference through noise prediction rescaling, guidance\n+                distilled models take the guidance scale directly as an input parameter during forward pass. Guidance\n+                scale is enabled by setting `guidance_scale > 1`. Higher guidance scale encourages to generate images\n+                that are closely linked to the text `prompt`, usually at the expense of lower image quality. This\n+                parameter in the pipeline is there to support future guidance-distilled models when they come up. It is\n+                ignored when not using guidance distilled models. To enable traditional classifier-free guidance,\n+                please pass `true_cfg_scale > 1.0` and `negative_prompt` (even an empty negative prompt like \" \" should\n+                enable classifier-free guidance computations).\n+            num_images_per_prompt (`int`, *optional*, defaults to 1):\n+                The number of images to generate per prompt.\n+            generator (`torch.Generator` or `List[torch.Generator]`, *optional*):\n+                One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)\n+                to make generation deterministic.\n+            latents (`torch.Tensor`, *optional*):\n+                Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for image\n+                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents\n+                tensor will be generated by sampling using the supplied random `generator`.\n+            prompt_embeds (`torch.Tensor`, *optional*):\n+                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n+                provided, text embeddings will be generated from `prompt` input argument.\n+            negative_prompt_embeds (`torch.Tensor`, *optional*):\n+                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt\n+                weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt` input\n+                argument.\n+            output_type (`str`, *optional*, defaults to `\"pil\"`):\n+                The output format of the generate image. Choose between\n+                [PIL](https://pillow.readthedocs.io/en/stable/): `PIL.Image.Image` or `np.array`.\n+            return_dict (`bool`, *optional*, defaults to `True`):\n+                Whether or not to return a [`~pipelines.qwenimage.QwenImagePipelineOutput`] instead of a plain tuple.\n+            attention_kwargs (`dict`, *optional*):\n+                A kwargs dictionary that if specified is passed along to the `AttentionProcessor` as defined under\n+                `self.processor` in\n+                [diffusers.models.attention_processor](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).\n+            callback_on_step_end (`Callable`, *optional*):\n+                A function that calls at the end of each denoising steps during the inference. The function is called\n+                with the following arguments: `callback_on_step_end(self: DiffusionPipeline, step: int, timestep: int,\n+                callback_kwargs: Dict)`. `callback_kwargs` will include a list of all tensors as specified by\n+                `callback_on_step_end_tensor_inputs`.\n+            callback_on_step_end_tensor_inputs (`List`, *optional*):\n+                The list of tensor inputs for the `callback_on_step_end` function. The tensors specified in the list\n+                will be passed as `callback_kwargs` argument. You will only be able to include variables listed in the\n+                `._callback_tensor_inputs` attribute of your pipeline class.\n+            max_sequence_length (`int` defaults to 512): Maximum sequence length to use with the `prompt`.\n+\n+        Examples:\n+\n+        Returns:\n+            [`~pipelines.qwenimage.QwenImagePipelineOutput`] or `tuple`:\n+            [`~pipelines.qwenimage.QwenImagePipelineOutput`] if `return_dict` is True, otherwise a `tuple`. When\n+            returning a tuple, the first element is a list with the generated images.\n+        \"\"\"\n+        image_size = image[-1].size if isinstance(image, list) else image.size\n+        calculated_width, calculated_height = calculate_dimensions(1024 * 1024, image_size[0] / image_size[1])\n+        height = height or calculated_height\n+        width = width or calculated_width\n+\n+        multiple_of = self.vae_scale_factor * 2\n+        width = width // multiple_of * multiple_of\n+        height = height // multiple_of * multiple_of\n+\n+        # 1. Check inputs. Raise error if not correct\n+        self.check_inputs(\n+            prompt,\n+            height,\n+            width,\n+            negative_prompt=negative_prompt,\n+            prompt_embeds=prompt_embeds,\n+            negative_prompt_embeds=negative_prompt_embeds,\n+            prompt_embeds_mask=prompt_embeds_mask,\n+            negative_prompt_embeds_mask=negative_prompt_embeds_mask,\n+            callback_on_step_end_tensor_inputs=callback_on_step_end_tensor_inputs,\n+            max_sequence_length=max_sequence_length,\n+        )\n+\n+        self._guidance_scale = guidance_scale\n+        self._attention_kwargs = attention_kwargs\n+        self._current_timestep = None\n+        self._interrupt = False\n+\n+        # 2. Define call parameters\n+        if prompt is not None and isinstance(prompt, str):\n+            batch_size = 1\n+        elif prompt is not None and isinstance(prompt, list):\n+            batch_size = len(prompt)\n+        else:\n+            batch_size = prompt_embeds.shape[0]\n+\n+        device = self._execution_device\n+        # 3. Preprocess image\n+        if image is not None and not (isinstance(image, torch.Tensor) and image.size(1) == self.latent_channels):\n+            if not isinstance(image, list):\n+                image = [image]\n+            condition_image_sizes = []\n+            condition_images = []\n+            vae_image_sizes = []\n+            vae_images = []\n+            for img in image:\n+                image_width, image_height = img.size\n+                condition_width, condition_height = calculate_dimensions(\n+                    CONDITION_IMAGE_SIZE, image_width / image_height\n+                )\n+                vae_width, vae_height = calculate_dimensions(VAE_IMAGE_SIZE, image_width / image_height)\n+                condition_image_sizes.append((condition_width, condition_height))\n+                vae_image_sizes.append((vae_width, vae_height))\n+                condition_images.append(self.image_processor.resize(img, condition_height, condition_width))\n+                vae_images.append(self.image_processor.preprocess(img, vae_height, vae_width).unsqueeze(2))\n+\n+        has_neg_prompt = negative_prompt is not None or (\n+            negative_prompt_embeds is not None and negative_prompt_embeds_mask is not None\n+        )\n+\n+        if true_cfg_scale > 1 and not has_neg_prompt:\n+            logger.warning(\n+                f\"true_cfg_scale is passed as {true_cfg_scale}, but classifier-free guidance is not enabled since no negative_prompt is provided.\"\n+            )\n+        elif true_cfg_scale <= 1 and has_neg_prompt:\n+            logger.warning(\n+                \" negative_prompt is passed but classifier-free guidance is not enabled since true_cfg_scale <= 1\"\n+            )\n+\n+        do_true_cfg = true_cfg_scale > 1 and has_neg_prompt\n+        prompt_embeds, prompt_embeds_mask = self.encode_prompt(\n+            image=condition_images,\n+            prompt=prompt,\n+            prompt_embeds=prompt_embeds,\n+            prompt_embeds_mask=prompt_embeds_mask,\n+            device=device,\n+            num_images_per_prompt=num_images_per_prompt,\n+            max_sequence_length=max_sequence_length,\n+        )\n+        if do_true_cfg:\n+            negative_prompt_embeds, negative_prompt_embeds_mask = self.encode_prompt(\n+                image=condition_images,\n+                prompt=negative_prompt,\n+                prompt_embeds=negative_prompt_embeds,\n+                prompt_embeds_mask=negative_prompt_embeds_mask,\n+                device=device,\n+                num_images_per_prompt=num_images_per_prompt,\n+                max_sequence_length=max_sequence_length,\n+            )\n+\n+        # 4. Prepare latent variables\n+        num_channels_latents = self.transformer.config.in_channels // 4\n+        latents, image_latents = self.prepare_latents(\n+            vae_images,\n+            batch_size * num_images_per_prompt,\n+            num_channels_latents,\n+            height,\n+            width,\n+            prompt_embeds.dtype,\n+            device,\n+            generator,\n+            latents,\n+        )\n+        img_shapes = [\n+            [\n+                (1, height // self.vae_scale_factor // 2, width // self.vae_scale_factor // 2),\n+                *[\n+                    (1, vae_height // self.vae_scale_factor // 2, vae_width // self.vae_scale_factor // 2)\n+                    for vae_width, vae_height in vae_image_sizes\n+                ],\n+            ]\n+        ] * batch_size\n+\n+        # 5. Prepare timesteps\n+        sigmas = np.linspace(1.0, 1 / num_inference_steps, num_inference_steps) if sigmas is None else sigmas\n+        image_seq_len = latents.shape[1]\n+        mu = calculate_shift(\n+            image_seq_len,\n+            self.scheduler.config.get(\"base_image_seq_len\", 256),\n+            self.scheduler.config.get(\"max_image_seq_len\", 4096),\n+            self.scheduler.config.get(\"base_shift\", 0.5),\n+            self.scheduler.config.get(\"max_shift\", 1.15),\n+        )\n+        timesteps, num_inference_steps = retrieve_timesteps(\n+            self.scheduler,\n+            num_inference_steps,\n+            device,\n+            sigmas=sigmas,\n+            mu=mu,\n+        )\n+        num_warmup_steps = max(len(timesteps) - num_inference_steps * self.scheduler.order, 0)\n+        self._num_timesteps = len(timesteps)\n+\n+        # handle guidance\n+        if self.transformer.config.guidance_embeds and guidance_scale is None:\n+            raise ValueError(\"guidance_scale is required for guidance-distilled model.\")\n+        elif self.transformer.config.guidance_embeds:\n+            guidance = torch.full([1], guidance_scale, device=device, dtype=torch.float32)\n+            guidance = guidance.expand(latents.shape[0])\n+        elif not self.transformer.config.guidance_embeds and guidance_scale is not None:\n+            logger.warning(\n+                f\"guidance_scale is passed as {guidance_scale}, but ignored since the model is not guidance-distilled.\"\n+            )\n+            guidance = None\n+        elif not self.transformer.config.guidance_embeds and guidance_scale is None:\n+            guidance = None\n+\n+        if self.attention_kwargs is None:\n+            self._attention_kwargs = {}\n+\n+        txt_seq_lens = prompt_embeds_mask.sum(dim=1).tolist() if prompt_embeds_mask is not None else None\n+        negative_txt_seq_lens = (\n+            negative_prompt_embeds_mask.sum(dim=1).tolist() if negative_prompt_embeds_mask is not None else None\n+        )\n+\n+        # 6. Denoising loop\n+        self.scheduler.set_begin_index(0)\n+        with self.progress_bar(total=num_inference_steps) as progress_bar:\n+            for i, t in enumerate(timesteps):\n+                if self.interrupt:\n+                    continue\n+\n+                self._current_timestep = t\n+\n+                latent_model_input = latents\n+                if image_latents is not None:\n+                    latent_model_input = torch.cat([latents, image_latents], dim=1)\n+\n+                # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n+                timestep = t.expand(latents.shape[0]).to(latents.dtype)\n+                with self.transformer.cache_context(\"cond\"):\n+                    noise_pred = self.transformer(\n+                        hidden_states=latent_model_input,\n+                        timestep=timestep / 1000,\n+                        guidance=guidance,\n+                        encoder_hidden_states_mask=prompt_embeds_mask,\n+                        encoder_hidden_states=prompt_embeds,\n+                        img_shapes=img_shapes,\n+                        txt_seq_lens=txt_seq_lens,\n+                        attention_kwargs=self.attention_kwargs,\n+                        return_dict=False,\n+                    )[0]\n+                    noise_pred = noise_pred[:, : latents.size(1)]\n+\n+                if do_true_cfg:\n+                    with self.transformer.cache_context(\"uncond\"):\n+                        neg_noise_pred = self.transformer(\n+                            hidden_states=latent_model_input,\n+                            timestep=timestep / 1000,\n+                            guidance=guidance,\n+                            encoder_hidden_states_mask=negative_prompt_embeds_mask,\n+                            encoder_hidden_states=negative_prompt_embeds,\n+                            img_shapes=img_shapes,\n+                            txt_seq_lens=negative_txt_seq_lens,\n+                            attention_kwargs=self.attention_kwargs,\n+                            return_dict=False,\n+                        )[0]\n+                    neg_noise_pred = neg_noise_pred[:, : latents.size(1)]\n+                    comb_pred = neg_noise_pred + true_cfg_scale * (noise_pred - neg_noise_pred)\n+\n+                    cond_norm = torch.norm(noise_pred, dim=-1, keepdim=True)\n+                    noise_norm = torch.norm(comb_pred, dim=-1, keepdim=True)\n+                    noise_pred = comb_pred * (cond_norm / noise_norm)\n+\n+                # compute the previous noisy sample x_t -> x_t-1\n+                latents_dtype = latents.dtype\n+                latents = self.scheduler.step(noise_pred, t, latents, return_dict=False)[0]\n+\n+                if latents.dtype != latents_dtype:\n+                    if torch.backends.mps.is_available():\n+                        # some platforms (eg. apple mps) misbehave due to a pytorch bug: https://github.com/pytorch/pytorch/pull/99272\n+                        latents = latents.to(latents_dtype)\n+\n+                if callback_on_step_end is not None:\n+                    callback_kwargs = {}\n+                    for k in callback_on_step_end_tensor_inputs:\n+                        callback_kwargs[k] = locals()[k]\n+                    callback_outputs = callback_on_step_end(self, i, t, callback_kwargs)\n+\n+                    latents = callback_outputs.pop(\"latents\", latents)\n+                    prompt_embeds = callback_outputs.pop(\"prompt_embeds\", prompt_embeds)\n+\n+                # call the callback, if provided\n+                if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n+                    progress_bar.update()\n+\n+                if XLA_AVAILABLE:\n+                    xm.mark_step()\n+\n+        self._current_timestep = None\n+        if output_type == \"latent\":\n+            image = latents\n+        else:\n+            latents = self._unpack_latents(latents, height, width, self.vae_scale_factor)\n+            latents = latents.to(self.vae.dtype)\n+            latents_mean = (\n+                torch.tensor(self.vae.config.latents_mean)\n+                .view(1, self.vae.config.z_dim, 1, 1, 1)\n+                .to(latents.device, latents.dtype)\n+            )\n+            latents_std = 1.0 / torch.tensor(self.vae.config.latents_std).view(1, self.vae.config.z_dim, 1, 1, 1).to(\n+                latents.device, latents.dtype\n+            )\n+            latents = latents / latents_std + latents_mean\n+            image = self.vae.decode(latents, return_dict=False)[0][:, :, 0]\n+            image = self.image_processor.postprocess(image, output_type=output_type)\n+\n+        # Offload all models\n+        self.maybe_free_model_hooks()\n+\n+        if not return_dict:\n+            return (image,)\n+\n+        return QwenImagePipelineOutput(images=image)"
        },
        {
          "filename": "src/diffusers/utils/dummy_torch_and_transformers_objects.py",
          "status": "modified",
          "additions": 15,
          "deletions": 0,
          "changes": 15,
          "patch": "@@ -1877,6 +1877,21 @@ def from_pretrained(cls, *args, **kwargs):\n         requires_backends(cls, [\"torch\", \"transformers\"])\n \n \n+class QwenImageEditPlusPipeline(metaclass=DummyObject):\n+    _backends = [\"torch\", \"transformers\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_config(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_pretrained(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+\n class QwenImageImg2ImgPipeline(metaclass=DummyObject):\n     _backends = [\"torch\", \"transformers\"]\n "
        }
      ],
      "num_files": 6,
      "scraped_at": "2025-11-16T21:19:18.770507"
    },
    {
      "pr_number": 12340,
      "title": "Added LucyEditPipeline",
      "body": "# What does this PR do?\r\nInitial implementation of LucyEditPipeline - a video editing pipeline.\r\n\r\n## Who can review?\r\n\r\n@yiyixuxu and @asomoza",
      "html_url": "https://github.com/huggingface/diffusers/pull/12340",
      "created_at": "2025-09-16T19:19:55Z",
      "merged_at": "2025-09-16T23:41:05Z",
      "merge_commit_sha": "8c72cd12ee65e420c86a0724f0182f966f339a7e",
      "base_ref": "main",
      "head_sha": "5bb798634366c782adc58240bc7525ed46d6d716",
      "user": "sarihl",
      "files": [
        {
          "filename": "src/diffusers/__init__.py",
          "status": "modified",
          "additions": 2,
          "deletions": 0,
          "changes": 2,
          "patch": "@@ -495,6 +495,7 @@\n             \"LTXImageToVideoPipeline\",\n             \"LTXLatentUpsamplePipeline\",\n             \"LTXPipeline\",\n+            \"LucyEditPipeline\",\n             \"Lumina2Pipeline\",\n             \"Lumina2Text2ImgPipeline\",\n             \"LuminaPipeline\",\n@@ -1149,6 +1150,7 @@\n             LTXImageToVideoPipeline,\n             LTXLatentUpsamplePipeline,\n             LTXPipeline,\n+            LucyEditPipeline,\n             Lumina2Pipeline,\n             Lumina2Text2ImgPipeline,\n             LuminaPipeline,"
        },
        {
          "filename": "src/diffusers/pipelines/__init__.py",
          "status": "modified",
          "additions": 2,
          "deletions": 0,
          "changes": 2,
          "patch": "@@ -285,6 +285,7 @@\n     ]\n     _import_structure[\"lumina\"] = [\"LuminaPipeline\", \"LuminaText2ImgPipeline\"]\n     _import_structure[\"lumina2\"] = [\"Lumina2Pipeline\", \"Lumina2Text2ImgPipeline\"]\n+    _import_structure[\"lucy\"] = [\"LucyEditPipeline\"]\n     _import_structure[\"marigold\"].extend(\n         [\n             \"MarigoldDepthPipeline\",\n@@ -682,6 +683,7 @@\n             LEditsPPPipelineStableDiffusionXL,\n         )\n         from .ltx import LTXConditionPipeline, LTXImageToVideoPipeline, LTXLatentUpsamplePipeline, LTXPipeline\n+        from .lucy import LucyEditPipeline\n         from .lumina import LuminaPipeline, LuminaText2ImgPipeline\n         from .lumina2 import Lumina2Pipeline, Lumina2Text2ImgPipeline\n         from .marigold import ("
        },
        {
          "filename": "src/diffusers/pipelines/lucy/__init__.py",
          "status": "added",
          "additions": 47,
          "deletions": 0,
          "changes": 47,
          "patch": "@@ -0,0 +1,47 @@\n+from typing import TYPE_CHECKING\n+\n+from ...utils import (\n+    DIFFUSERS_SLOW_IMPORT,\n+    OptionalDependencyNotAvailable,\n+    _LazyModule,\n+    get_objects_from_module,\n+    is_torch_available,\n+    is_transformers_available,\n+)\n+\n+\n+_dummy_objects = {}\n+_import_structure = {}\n+\n+\n+try:\n+    if not (is_transformers_available() and is_torch_available()):\n+        raise OptionalDependencyNotAvailable()\n+except OptionalDependencyNotAvailable:\n+    from ...utils import dummy_torch_and_transformers_objects  # noqa F403\n+\n+    _dummy_objects.update(get_objects_from_module(dummy_torch_and_transformers_objects))\n+else:\n+    _import_structure[\"pipeline_lucy_edit\"] = [\"LucyEditPipeline\"]\n+if TYPE_CHECKING or DIFFUSERS_SLOW_IMPORT:\n+    try:\n+        if not (is_transformers_available() and is_torch_available()):\n+            raise OptionalDependencyNotAvailable()\n+\n+    except OptionalDependencyNotAvailable:\n+        from ...utils.dummy_torch_and_transformers_objects import *\n+    else:\n+        from .pipeline_lucy_edit import LucyEditPipeline\n+\n+else:\n+    import sys\n+\n+    sys.modules[__name__] = _LazyModule(\n+        __name__,\n+        globals()[\"__file__\"],\n+        _import_structure,\n+        module_spec=__spec__,\n+    )\n+\n+    for name, value in _dummy_objects.items():\n+        setattr(sys.modules[__name__], name, value)"
        },
        {
          "filename": "src/diffusers/pipelines/lucy/pipeline_lucy_edit.py",
          "status": "added",
          "additions": 735,
          "deletions": 0,
          "changes": 735,
          "patch": "@@ -0,0 +1,735 @@\n+# Copyright 2025 The Wan Team and The HuggingFace Team. All rights reserved.\n+# Copyright 2025 The Decart AI Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+# Modifications by Decart AI Team:\n+# - Based on pipeline_wan.py, but with supports recieving a condition video appended to the channel dimension.\n+\n+import html\n+from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n+\n+import regex as re\n+import torch\n+from PIL import Image\n+from transformers import AutoTokenizer, UMT5EncoderModel\n+\n+from ...callbacks import MultiPipelineCallbacks, PipelineCallback\n+from ...loaders import WanLoraLoaderMixin\n+from ...models import AutoencoderKLWan, WanTransformer3DModel\n+from ...schedulers import FlowMatchEulerDiscreteScheduler\n+from ...utils import is_ftfy_available, is_torch_xla_available, logging, replace_example_docstring\n+from ...utils.torch_utils import randn_tensor\n+from ...video_processor import VideoProcessor\n+from ..pipeline_utils import DiffusionPipeline\n+from .pipeline_output import LucyPipelineOutput\n+\n+\n+if is_torch_xla_available():\n+    import torch_xla.core.xla_model as xm\n+\n+    XLA_AVAILABLE = True\n+else:\n+    XLA_AVAILABLE = False\n+\n+logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n+\n+if is_ftfy_available():\n+    import ftfy\n+\n+\n+EXAMPLE_DOC_STRING = \"\"\"\n+    Examples:\n+        ```python\n+        >>> from typing import List\n+\n+        >>> import torch\n+        >>> from PIL import Image\n+\n+        >>> from diffusers import AutoencoderKLWan, LucyEditPipeline\n+        >>> from diffusers.utils import export_to_video, load_video\n+\n+        >>> # Arguments\n+        >>> url = \"https://d2drjpuinn46lb.cloudfront.net/painter_original_edit.mp4\"\n+        >>> prompt = \"Change the apron and blouse to a classic clown costume: satin polka-dot jumpsuit in bright primary colors, ruffled white collar, oversized pom-pom buttons, white gloves, oversized red shoes, red foam nose; soft window light from left, eye-level medium shot, natural folds and fabric highlights.\"\n+        >>> negative_prompt = \"\"\n+        >>> num_frames = 81\n+        >>> height = 480\n+        >>> width = 832\n+\n+\n+        >>> # Load video\n+        >>> def convert_video(video: List[Image.Image]) -> List[Image.Image]:\n+        ...     video = load_video(url)[:num_frames]\n+        ...     video = [video[i].resize((width, height)) for i in range(num_frames)]\n+        ...     return video\n+\n+\n+        >>> video = load_video(url, convert_method=convert_video)\n+\n+        >>> # Load model\n+        >>> model_id = \"decart-ai/Lucy-Edit-Dev\"\n+        >>> vae = AutoencoderKLWan.from_pretrained(model_id, subfolder=\"vae\", torch_dtype=torch.float32)\n+        >>> pipe = LucyEditPipeline.from_pretrained(model_id, vae=vae, torch_dtype=torch.bfloat16)\n+        >>> pipe.to(\"cuda\")\n+\n+        >>> # Generate video\n+        >>> output = pipe(\n+        ...     prompt=prompt,\n+        ...     video=video,\n+        ...     negative_prompt=negative_prompt,\n+        ...     height=480,\n+        ...     width=832,\n+        ...     num_frames=81,\n+        ...     guidance_scale=5.0,\n+        ... ).frames[0]\n+\n+        >>> # Export video\n+        >>> export_to_video(output, \"output.mp4\", fps=24)\n+        ```\n+\"\"\"\n+\n+\n+def basic_clean(text):\n+    text = ftfy.fix_text(text)\n+    text = html.unescape(html.unescape(text))\n+    return text.strip()\n+\n+\n+def whitespace_clean(text):\n+    text = re.sub(r\"\\s+\", \" \", text)\n+    text = text.strip()\n+    return text\n+\n+\n+def prompt_clean(text):\n+    text = whitespace_clean(basic_clean(text))\n+    return text\n+\n+\n+# Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_img2img.retrieve_latents\n+def retrieve_latents(\n+    encoder_output: torch.Tensor, generator: Optional[torch.Generator] = None, sample_mode: str = \"sample\"\n+):\n+    if hasattr(encoder_output, \"latent_dist\") and sample_mode == \"sample\":\n+        return encoder_output.latent_dist.sample(generator)\n+    elif hasattr(encoder_output, \"latent_dist\") and sample_mode == \"argmax\":\n+        return encoder_output.latent_dist.mode()\n+    elif hasattr(encoder_output, \"latents\"):\n+        return encoder_output.latents\n+    else:\n+        raise AttributeError(\"Could not access latents of provided encoder_output\")\n+\n+\n+class LucyEditPipeline(DiffusionPipeline, WanLoraLoaderMixin):\n+    r\"\"\"\n+    Pipeline for video-to-video generation using Lucy Edit.\n+\n+    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods\n+    implemented for all pipelines (downloading, saving, running on a particular device, etc.).\n+\n+    Args:\n+        tokenizer ([`T5Tokenizer`]):\n+            Tokenizer from [T5](https://huggingface.co/docs/transformers/en/model_doc/t5#transformers.T5Tokenizer),\n+            specifically the [google/umt5-xxl](https://huggingface.co/google/umt5-xxl) variant.\n+        text_encoder ([`T5EncoderModel`]):\n+            [T5](https://huggingface.co/docs/transformers/en/model_doc/t5#transformers.T5EncoderModel), specifically\n+            the [google/umt5-xxl](https://huggingface.co/google/umt5-xxl) variant.\n+        transformer ([`WanTransformer3DModel`]):\n+            Conditional Transformer to denoise the input latents.\n+        scheduler ([`UniPCMultistepScheduler`]):\n+            A scheduler to be used in combination with `transformer` to denoise the encoded image latents.\n+        vae ([`AutoencoderKLWan`]):\n+            Variational Auto-Encoder (VAE) Model to encode and decode videos to and from latent representations.\n+        transformer_2 ([`WanTransformer3DModel`], *optional*):\n+            Conditional Transformer to denoise the input latents during the low-noise stage. If provided, enables\n+            two-stage denoising where `transformer` handles high-noise stages and `transformer_2` handles low-noise\n+            stages. If not provided, only `transformer` is used.\n+        boundary_ratio (`float`, *optional*, defaults to `None`):\n+            Ratio of total timesteps to use as the boundary for switching between transformers in two-stage denoising.\n+            The actual boundary timestep is calculated as `boundary_ratio * num_train_timesteps`. When provided,\n+            `transformer` handles timesteps >= boundary_timestep and `transformer_2` handles timesteps <\n+            boundary_timestep. If `None`, only `transformer` is used for the entire denoising process.\n+    \"\"\"\n+\n+    model_cpu_offload_seq = \"text_encoder->transformer->transformer_2->vae\"\n+    _callback_tensor_inputs = [\"latents\", \"prompt_embeds\", \"negative_prompt_embeds\"]\n+    _optional_components = [\"transformer\", \"transformer_2\"]\n+\n+    def __init__(\n+        self,\n+        tokenizer: AutoTokenizer,\n+        text_encoder: UMT5EncoderModel,\n+        vae: AutoencoderKLWan,\n+        scheduler: FlowMatchEulerDiscreteScheduler,\n+        transformer: Optional[WanTransformer3DModel] = None,\n+        transformer_2: Optional[WanTransformer3DModel] = None,\n+        boundary_ratio: Optional[float] = None,\n+        expand_timesteps: bool = False,  # Wan2.2 ti2v\n+    ):\n+        super().__init__()\n+\n+        self.register_modules(\n+            vae=vae,\n+            text_encoder=text_encoder,\n+            tokenizer=tokenizer,\n+            transformer=transformer,\n+            scheduler=scheduler,\n+            transformer_2=transformer_2,\n+        )\n+        self.register_to_config(boundary_ratio=boundary_ratio)\n+        self.register_to_config(expand_timesteps=expand_timesteps)\n+        self.vae_scale_factor_temporal = self.vae.config.scale_factor_temporal if getattr(self, \"vae\", None) else 4\n+        self.vae_scale_factor_spatial = self.vae.config.scale_factor_spatial if getattr(self, \"vae\", None) else 8\n+        self.video_processor = VideoProcessor(vae_scale_factor=self.vae_scale_factor_spatial)\n+\n+    # Copied from diffusers.pipelines.wan.pipeline_wan.WanPipeline._get_t5_prompt_embeds\n+    def _get_t5_prompt_embeds(\n+        self,\n+        prompt: Union[str, List[str]] = None,\n+        num_videos_per_prompt: int = 1,\n+        max_sequence_length: int = 226,\n+        device: Optional[torch.device] = None,\n+        dtype: Optional[torch.dtype] = None,\n+    ):\n+        device = device or self._execution_device\n+        dtype = dtype or self.text_encoder.dtype\n+\n+        prompt = [prompt] if isinstance(prompt, str) else prompt\n+        prompt = [prompt_clean(u) for u in prompt]\n+        batch_size = len(prompt)\n+\n+        text_inputs = self.tokenizer(\n+            prompt,\n+            padding=\"max_length\",\n+            max_length=max_sequence_length,\n+            truncation=True,\n+            add_special_tokens=True,\n+            return_attention_mask=True,\n+            return_tensors=\"pt\",\n+        )\n+        text_input_ids, mask = text_inputs.input_ids, text_inputs.attention_mask\n+        seq_lens = mask.gt(0).sum(dim=1).long()\n+\n+        prompt_embeds = self.text_encoder(text_input_ids.to(device), mask.to(device)).last_hidden_state\n+        prompt_embeds = prompt_embeds.to(dtype=dtype, device=device)\n+        prompt_embeds = [u[:v] for u, v in zip(prompt_embeds, seq_lens)]\n+        prompt_embeds = torch.stack(\n+            [torch.cat([u, u.new_zeros(max_sequence_length - u.size(0), u.size(1))]) for u in prompt_embeds], dim=0\n+        )\n+\n+        # duplicate text embeddings for each generation per prompt, using mps friendly method\n+        _, seq_len, _ = prompt_embeds.shape\n+        prompt_embeds = prompt_embeds.repeat(1, num_videos_per_prompt, 1)\n+        prompt_embeds = prompt_embeds.view(batch_size * num_videos_per_prompt, seq_len, -1)\n+\n+        return prompt_embeds\n+\n+    # Copied from diffusers.pipelines.wan.pipeline_wan.WanPipeline.encode_prompt\n+    def encode_prompt(\n+        self,\n+        prompt: Union[str, List[str]],\n+        negative_prompt: Optional[Union[str, List[str]]] = None,\n+        do_classifier_free_guidance: bool = True,\n+        num_videos_per_prompt: int = 1,\n+        prompt_embeds: Optional[torch.Tensor] = None,\n+        negative_prompt_embeds: Optional[torch.Tensor] = None,\n+        max_sequence_length: int = 226,\n+        device: Optional[torch.device] = None,\n+        dtype: Optional[torch.dtype] = None,\n+    ):\n+        r\"\"\"\n+        Encodes the prompt into text encoder hidden states.\n+\n+        Args:\n+            prompt (`str` or `List[str]`, *optional*):\n+                prompt to be encoded\n+            negative_prompt (`str` or `List[str]`, *optional*):\n+                The prompt or prompts not to guide the image generation. If not defined, one has to pass\n+                `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is\n+                less than `1`).\n+            do_classifier_free_guidance (`bool`, *optional*, defaults to `True`):\n+                Whether to use classifier free guidance or not.\n+            num_videos_per_prompt (`int`, *optional*, defaults to 1):\n+                Number of videos that should be generated per prompt. torch device to place the resulting embeddings on\n+            prompt_embeds (`torch.Tensor`, *optional*):\n+                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n+                provided, text embeddings will be generated from `prompt` input argument.\n+            negative_prompt_embeds (`torch.Tensor`, *optional*):\n+                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt\n+                weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt` input\n+                argument.\n+            device: (`torch.device`, *optional*):\n+                torch device\n+            dtype: (`torch.dtype`, *optional*):\n+                torch dtype\n+        \"\"\"\n+        device = device or self._execution_device\n+\n+        prompt = [prompt] if isinstance(prompt, str) else prompt\n+        if prompt is not None:\n+            batch_size = len(prompt)\n+        else:\n+            batch_size = prompt_embeds.shape[0]\n+\n+        if prompt_embeds is None:\n+            prompt_embeds = self._get_t5_prompt_embeds(\n+                prompt=prompt,\n+                num_videos_per_prompt=num_videos_per_prompt,\n+                max_sequence_length=max_sequence_length,\n+                device=device,\n+                dtype=dtype,\n+            )\n+\n+        if do_classifier_free_guidance and negative_prompt_embeds is None:\n+            negative_prompt = negative_prompt or \"\"\n+            negative_prompt = batch_size * [negative_prompt] if isinstance(negative_prompt, str) else negative_prompt\n+\n+            if prompt is not None and type(prompt) is not type(negative_prompt):\n+                raise TypeError(\n+                    f\"`negative_prompt` should be the same type to `prompt`, but got {type(negative_prompt)} !=\"\n+                    f\" {type(prompt)}.\"\n+                )\n+            elif batch_size != len(negative_prompt):\n+                raise ValueError(\n+                    f\"`negative_prompt`: {negative_prompt} has batch size {len(negative_prompt)}, but `prompt`:\"\n+                    f\" {prompt} has batch size {batch_size}. Please make sure that passed `negative_prompt` matches\"\n+                    \" the batch size of `prompt`.\"\n+                )\n+\n+            negative_prompt_embeds = self._get_t5_prompt_embeds(\n+                prompt=negative_prompt,\n+                num_videos_per_prompt=num_videos_per_prompt,\n+                max_sequence_length=max_sequence_length,\n+                device=device,\n+                dtype=dtype,\n+            )\n+\n+        return prompt_embeds, negative_prompt_embeds\n+\n+    def check_inputs(\n+        self,\n+        video,\n+        prompt,\n+        negative_prompt,\n+        height,\n+        width,\n+        prompt_embeds=None,\n+        negative_prompt_embeds=None,\n+        callback_on_step_end_tensor_inputs=None,\n+        guidance_scale_2=None,\n+    ):\n+        if height % 16 != 0 or width % 16 != 0:\n+            raise ValueError(f\"`height` and `width` have to be divisible by 16 but are {height} and {width}.\")\n+\n+        if callback_on_step_end_tensor_inputs is not None and not all(\n+            k in self._callback_tensor_inputs for k in callback_on_step_end_tensor_inputs\n+        ):\n+            raise ValueError(\n+                f\"`callback_on_step_end_tensor_inputs` has to be in {self._callback_tensor_inputs}, but found {[k for k in callback_on_step_end_tensor_inputs if k not in self._callback_tensor_inputs]}\"\n+            )\n+\n+        if prompt is not None and prompt_embeds is not None:\n+            raise ValueError(\n+                f\"Cannot forward both `prompt`: {prompt} and `prompt_embeds`: {prompt_embeds}. Please make sure to\"\n+                \" only forward one of the two.\"\n+            )\n+        elif negative_prompt is not None and negative_prompt_embeds is not None:\n+            raise ValueError(\n+                f\"Cannot forward both `negative_prompt`: {negative_prompt} and `negative_prompt_embeds`: {negative_prompt_embeds}. Please make sure to\"\n+                \" only forward one of the two.\"\n+            )\n+        elif prompt is None and prompt_embeds is None:\n+            raise ValueError(\n+                \"Provide either `prompt` or `prompt_embeds`. Cannot leave both `prompt` and `prompt_embeds` undefined.\"\n+            )\n+        elif prompt is not None and (not isinstance(prompt, str) and not isinstance(prompt, list)):\n+            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n+        elif negative_prompt is not None and (\n+            not isinstance(negative_prompt, str) and not isinstance(negative_prompt, list)\n+        ):\n+            raise ValueError(f\"`negative_prompt` has to be of type `str` or `list` but is {type(negative_prompt)}\")\n+\n+        if self.config.boundary_ratio is None and guidance_scale_2 is not None:\n+            raise ValueError(\"`guidance_scale_2` is only supported when the pipeline's `boundary_ratio` is not None.\")\n+\n+        if video is None:\n+            raise ValueError(\"`video` is required, received None.\")\n+\n+    def prepare_latents(\n+        self,\n+        video: Optional[torch.Tensor] = None,\n+        batch_size: int = 1,\n+        num_channels_latents: int = 16,\n+        height: int = 480,\n+        width: int = 832,\n+        dtype: Optional[torch.dtype] = None,\n+        device: Optional[torch.device] = None,\n+        generator: Optional[torch.Generator] = None,\n+        latents: Optional[torch.Tensor] = None,\n+    ) -> Tuple[torch.Tensor, torch.Tensor]:\n+        if isinstance(generator, list) and len(generator) != batch_size:\n+            raise ValueError(\n+                f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n+                f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n+            )\n+\n+        num_latent_frames = (\n+            (video.size(2) - 1) // self.vae_scale_factor_temporal + 1 if latents is None else latents.size(1)\n+        )\n+        shape = (\n+            batch_size,\n+            num_channels_latents,\n+            num_latent_frames,\n+            height // self.vae_scale_factor_spatial,\n+            width // self.vae_scale_factor_spatial,\n+        )\n+        # Prepare noise latents\n+        if latents is None:\n+            latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n+        else:\n+            latents = latents.to(device)\n+\n+        # Prepare condition latents\n+        condition_latents = [\n+            retrieve_latents(self.vae.encode(vid.unsqueeze(0)), sample_mode=\"argmax\") for vid in video\n+        ]\n+\n+        condition_latents = torch.cat(condition_latents, dim=0).to(dtype)\n+\n+        latents_mean = (\n+            torch.tensor(self.vae.config.latents_mean).view(1, self.vae.config.z_dim, 1, 1, 1).to(device, dtype)\n+        )\n+        latents_std = 1.0 / torch.tensor(self.vae.config.latents_std).view(1, self.vae.config.z_dim, 1, 1, 1).to(\n+            device, dtype\n+        )\n+\n+        condition_latents = (condition_latents - latents_mean) * latents_std\n+\n+        # Check shapes\n+        assert latents.shape == condition_latents.shape, (\n+            f\"Latents shape {latents.shape} does not match expected shape {condition_latents.shape}. Please check the input.\"\n+        )\n+\n+        return latents, condition_latents\n+\n+    @property\n+    def guidance_scale(self):\n+        return self._guidance_scale\n+\n+    @property\n+    def do_classifier_free_guidance(self):\n+        return self._guidance_scale > 1.0\n+\n+    @property\n+    def num_timesteps(self):\n+        return self._num_timesteps\n+\n+    @property\n+    def current_timestep(self):\n+        return self._current_timestep\n+\n+    @property\n+    def interrupt(self):\n+        return self._interrupt\n+\n+    @property\n+    def attention_kwargs(self):\n+        return self._attention_kwargs\n+\n+    @torch.no_grad()\n+    @replace_example_docstring(EXAMPLE_DOC_STRING)\n+    def __call__(\n+        self,\n+        video: List[Image.Image],\n+        prompt: Union[str, List[str]] = None,\n+        negative_prompt: Union[str, List[str]] = None,\n+        height: int = 480,\n+        width: int = 832,\n+        num_frames: int = 81,\n+        num_inference_steps: int = 50,\n+        guidance_scale: float = 5.0,\n+        guidance_scale_2: Optional[float] = None,\n+        num_videos_per_prompt: Optional[int] = 1,\n+        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n+        latents: Optional[torch.Tensor] = None,\n+        prompt_embeds: Optional[torch.Tensor] = None,\n+        negative_prompt_embeds: Optional[torch.Tensor] = None,\n+        output_type: Optional[str] = \"np\",\n+        return_dict: bool = True,\n+        attention_kwargs: Optional[Dict[str, Any]] = None,\n+        callback_on_step_end: Optional[\n+            Union[Callable[[int, int, Dict], None], PipelineCallback, MultiPipelineCallbacks]\n+        ] = None,\n+        callback_on_step_end_tensor_inputs: List[str] = [\"latents\"],\n+        max_sequence_length: int = 512,\n+    ):\n+        r\"\"\"\n+        The call function to the pipeline for generation.\n+\n+        Args:\n+            video (`List[Image.Image]`):\n+                The video to use as the condition for the video generation.\n+            prompt (`str` or `List[str]`, *optional*):\n+                The prompt or prompts to guide the image generation. If not defined, pass `prompt_embeds` instead.\n+            negative_prompt (`str` or `List[str]`, *optional*):\n+                The prompt or prompts to avoid during image generation. If not defined, pass `negative_prompt_embeds`\n+                instead. Ignored when not using guidance (`guidance_scale` < `1`).\n+            height (`int`, defaults to `480`):\n+                The height in pixels of the generated image.\n+            width (`int`, defaults to `832`):\n+                The width in pixels of the generated image.\n+            num_frames (`int`, defaults to `81`):\n+                The number of frames in the generated video.\n+            num_inference_steps (`int`, defaults to `50`):\n+                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n+                expense of slower inference.\n+            guidance_scale (`float`, defaults to `5.0`):\n+                Guidance scale as defined in [Classifier-Free Diffusion\n+                Guidance](https://huggingface.co/papers/2207.12598). `guidance_scale` is defined as `w` of equation 2.\n+                of [Imagen Paper](https://huggingface.co/papers/2205.11487). Guidance scale is enabled by setting\n+                `guidance_scale > 1`. Higher guidance scale encourages to generate images that are closely linked to\n+                the text `prompt`, usually at the expense of lower image quality.\n+            guidance_scale_2 (`float`, *optional*, defaults to `None`):\n+                Guidance scale for the low-noise stage transformer (`transformer_2`). If `None` and the pipeline's\n+                `boundary_ratio` is not None, uses the same value as `guidance_scale`. Only used when `transformer_2`\n+                and the pipeline's `boundary_ratio` are not None.\n+            num_videos_per_prompt (`int`, *optional*, defaults to 1):\n+                The number of images to generate per prompt.\n+            generator (`torch.Generator` or `List[torch.Generator]`, *optional*):\n+                A [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html) to make\n+                generation deterministic.\n+            latents (`torch.Tensor`, *optional*):\n+                Pre-generated noisy latents sampled from a Gaussian distribution, to be used as inputs for image\n+                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents\n+                tensor is generated by sampling using the supplied random `generator`.\n+            prompt_embeds (`torch.Tensor`, *optional*):\n+                Pre-generated text embeddings. Can be used to easily tweak text inputs (prompt weighting). If not\n+                provided, text embeddings are generated from the `prompt` input argument.\n+            output_type (`str`, *optional*, defaults to `\"np\"`):\n+                The output format of the generated image. Choose between `PIL.Image` or `np.array`.\n+            return_dict (`bool`, *optional*, defaults to `True`):\n+                Whether or not to return a [`LucyPipelineOutput`] instead of a plain tuple.\n+            attention_kwargs (`dict`, *optional*):\n+                A kwargs dictionary that if specified is passed along to the `AttentionProcessor` as defined under\n+                `self.processor` in\n+                [diffusers.models.attention_processor](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).\n+            callback_on_step_end (`Callable`, `PipelineCallback`, `MultiPipelineCallbacks`, *optional*):\n+                A function or a subclass of `PipelineCallback` or `MultiPipelineCallbacks` that is called at the end of\n+                each denoising step during the inference. with the following arguments: `callback_on_step_end(self:\n+                DiffusionPipeline, step: int, timestep: int, callback_kwargs: Dict)`. `callback_kwargs` will include a\n+                list of all tensors as specified by `callback_on_step_end_tensor_inputs`.\n+            callback_on_step_end_tensor_inputs (`List`, *optional*):\n+                The list of tensor inputs for the `callback_on_step_end` function. The tensors specified in the list\n+                will be passed as `callback_kwargs` argument. You will only be able to include variables listed in the\n+                `._callback_tensor_inputs` attribute of your pipeline class.\n+            max_sequence_length (`int`, defaults to `512`):\n+                The maximum sequence length of the text encoder. If the prompt is longer than this, it will be\n+                truncated. If the prompt is shorter, it will be padded to this length.\n+\n+        Examples:\n+\n+        Returns:\n+            [`~LucyPipelineOutput`] or `tuple`:\n+                If `return_dict` is `True`, [`LucyPipelineOutput`] is returned, otherwise a `tuple` is returned where\n+                the first element is a list with the generated images and the second element is a list of `bool`s\n+                indicating whether the corresponding generated image contains \"not-safe-for-work\" (nsfw) content.\n+        \"\"\"\n+\n+        if isinstance(callback_on_step_end, (PipelineCallback, MultiPipelineCallbacks)):\n+            callback_on_step_end_tensor_inputs = callback_on_step_end.tensor_inputs\n+\n+        # 1. Check inputs. Raise error if not correct\n+        self.check_inputs(\n+            video,\n+            prompt,\n+            negative_prompt,\n+            height,\n+            width,\n+            prompt_embeds,\n+            negative_prompt_embeds,\n+            callback_on_step_end_tensor_inputs,\n+            guidance_scale_2,\n+        )\n+\n+        if num_frames % self.vae_scale_factor_temporal != 1:\n+            logger.warning(\n+                f\"`num_frames - 1` has to be divisible by {self.vae_scale_factor_temporal}. Rounding to the nearest number.\"\n+            )\n+            num_frames = num_frames // self.vae_scale_factor_temporal * self.vae_scale_factor_temporal + 1\n+        num_frames = max(num_frames, 1)\n+\n+        if self.config.boundary_ratio is not None and guidance_scale_2 is None:\n+            guidance_scale_2 = guidance_scale\n+\n+        self._guidance_scale = guidance_scale\n+        self._guidance_scale_2 = guidance_scale_2\n+        self._attention_kwargs = attention_kwargs\n+        self._current_timestep = None\n+        self._interrupt = False\n+\n+        device = self._execution_device\n+\n+        # 2. Define call parameters\n+        if prompt is not None and isinstance(prompt, str):\n+            batch_size = 1\n+        elif prompt is not None and isinstance(prompt, list):\n+            batch_size = len(prompt)\n+        else:\n+            batch_size = prompt_embeds.shape[0]\n+\n+        # 3. Encode input prompt\n+        prompt_embeds, negative_prompt_embeds = self.encode_prompt(\n+            prompt=prompt,\n+            negative_prompt=negative_prompt,\n+            do_classifier_free_guidance=self.do_classifier_free_guidance,\n+            num_videos_per_prompt=num_videos_per_prompt,\n+            prompt_embeds=prompt_embeds,\n+            negative_prompt_embeds=negative_prompt_embeds,\n+            max_sequence_length=max_sequence_length,\n+            device=device,\n+        )\n+\n+        transformer_dtype = self.transformer.dtype if self.transformer is not None else self.transformer_2.dtype\n+        prompt_embeds = prompt_embeds.to(transformer_dtype)\n+        if negative_prompt_embeds is not None:\n+            negative_prompt_embeds = negative_prompt_embeds.to(transformer_dtype)\n+\n+        # 4. Prepare timesteps\n+        self.scheduler.set_timesteps(num_inference_steps, device=device)\n+        timesteps = self.scheduler.timesteps\n+\n+        # 5. Prepare latent variables\n+        num_channels_latents = (\n+            self.transformer.config.out_channels\n+            if self.transformer is not None\n+            else self.transformer_2.config.out_channels\n+        )\n+        video = self.video_processor.preprocess_video(video, height=height, width=width).to(\n+            device, dtype=torch.float32\n+        )\n+        latents, condition_latents = self.prepare_latents(\n+            video,\n+            batch_size * num_videos_per_prompt,\n+            num_channels_latents,\n+            height,\n+            width,\n+            torch.float32,\n+            device,\n+            generator,\n+            latents,\n+        )\n+\n+        mask = torch.ones(latents.shape, dtype=torch.float32, device=device)\n+\n+        # 6. Denoising loop\n+        num_warmup_steps = len(timesteps) - num_inference_steps * self.scheduler.order\n+        self._num_timesteps = len(timesteps)\n+\n+        if self.config.boundary_ratio is not None:\n+            boundary_timestep = self.config.boundary_ratio * self.scheduler.config.num_train_timesteps\n+        else:\n+            boundary_timestep = None\n+\n+        with self.progress_bar(total=num_inference_steps) as progress_bar:\n+            for i, t in enumerate(timesteps):\n+                if self.interrupt:\n+                    continue\n+\n+                self._current_timestep = t\n+\n+                if boundary_timestep is None or t >= boundary_timestep:\n+                    # wan2.1 or high-noise stage in wan2.2\n+                    current_model = self.transformer\n+                    current_guidance_scale = guidance_scale\n+                else:\n+                    # low-noise stage in wan2.2\n+                    current_model = self.transformer_2\n+                    current_guidance_scale = guidance_scale_2\n+\n+                # latent_model_input = latents.to(transformer_dtype)\n+                latent_model_input = torch.cat([latents, condition_latents], dim=1).to(transformer_dtype)\n+                # latent_model_input = torch.cat([latents, latents], dim=1).to(transformer_dtype)\n+                if self.config.expand_timesteps:\n+                    # seq_len: num_latent_frames * latent_height//2 * latent_width//2\n+                    temp_ts = (mask[0][0][:, ::2, ::2] * t).flatten()\n+                    # batch_size, seq_len\n+                    timestep = temp_ts.unsqueeze(0).expand(latents.shape[0], -1)\n+                else:\n+                    timestep = t.expand(latents.shape[0])\n+\n+                with current_model.cache_context(\"cond\"):\n+                    noise_pred = current_model(\n+                        hidden_states=latent_model_input,\n+                        timestep=timestep,\n+                        encoder_hidden_states=prompt_embeds,\n+                        attention_kwargs=attention_kwargs,\n+                        return_dict=False,\n+                    )[0]\n+\n+                if self.do_classifier_free_guidance:\n+                    with current_model.cache_context(\"uncond\"):\n+                        noise_uncond = current_model(\n+                            hidden_states=latent_model_input,\n+                            timestep=timestep,\n+                            encoder_hidden_states=negative_prompt_embeds,\n+                            attention_kwargs=attention_kwargs,\n+                            return_dict=False,\n+                        )[0]\n+                    noise_pred = noise_uncond + current_guidance_scale * (noise_pred - noise_uncond)\n+\n+                # compute the previous noisy sample x_t -> x_t-1\n+                latents = self.scheduler.step(noise_pred, t, latents, return_dict=False)[0]\n+\n+                if callback_on_step_end is not None:\n+                    callback_kwargs = {}\n+                    for k in callback_on_step_end_tensor_inputs:\n+                        callback_kwargs[k] = locals()[k]\n+                    callback_outputs = callback_on_step_end(self, i, t, callback_kwargs)\n+\n+                    latents = callback_outputs.pop(\"latents\", latents)\n+                    prompt_embeds = callback_outputs.pop(\"prompt_embeds\", prompt_embeds)\n+                    negative_prompt_embeds = callback_outputs.pop(\"negative_prompt_embeds\", negative_prompt_embeds)\n+\n+                # call the callback, if provided\n+                if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n+                    progress_bar.update()\n+\n+                if XLA_AVAILABLE:\n+                    xm.mark_step()\n+\n+        self._current_timestep = None\n+\n+        if not output_type == \"latent\":\n+            latents = latents.to(self.vae.dtype)\n+            latents_mean = (\n+                torch.tensor(self.vae.config.latents_mean)\n+                .view(1, self.vae.config.z_dim, 1, 1, 1)\n+                .to(latents.device, latents.dtype)\n+            )\n+            latents_std = 1.0 / torch.tensor(self.vae.config.latents_std).view(1, self.vae.config.z_dim, 1, 1, 1).to(\n+                latents.device, latents.dtype\n+            )\n+            latents = latents / latents_std + latents_mean\n+            video = self.vae.decode(latents, return_dict=False)[0]\n+            video = self.video_processor.postprocess_video(video, output_type=output_type)\n+        else:\n+            video = latents\n+\n+        # Offload all models\n+        self.maybe_free_model_hooks()\n+\n+        if not return_dict:\n+            return (video,)\n+\n+        return LucyPipelineOutput(frames=video)"
        },
        {
          "filename": "src/diffusers/pipelines/lucy/pipeline_output.py",
          "status": "added",
          "additions": 20,
          "deletions": 0,
          "changes": 20,
          "patch": "@@ -0,0 +1,20 @@\n+from dataclasses import dataclass\n+\n+import torch\n+\n+from diffusers.utils import BaseOutput\n+\n+\n+@dataclass\n+class LucyPipelineOutput(BaseOutput):\n+    r\"\"\"\n+    Output class for Lucy pipelines.\n+\n+    Args:\n+        frames (`torch.Tensor`, `np.ndarray`, or List[List[PIL.Image.Image]]):\n+            List of video outputs - It can be a nested list of length `batch_size,` with each sub-list containing\n+            denoised PIL image sequences of length `num_frames.` It can also be a NumPy array or Torch tensor of shape\n+            `(batch_size, num_frames, channels, height, width)`.\n+    \"\"\"\n+\n+    frames: torch.Tensor"
        },
        {
          "filename": "src/diffusers/utils/dummy_torch_and_transformers_objects.py",
          "status": "modified",
          "additions": 15,
          "deletions": 0,
          "changes": 15,
          "patch": "@@ -1592,6 +1592,21 @@ def from_pretrained(cls, *args, **kwargs):\n         requires_backends(cls, [\"torch\", \"transformers\"])\n \n \n+class LucyEditPipeline(metaclass=DummyObject):\n+    _backends = [\"torch\", \"transformers\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_config(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_pretrained(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+\n class Lumina2Pipeline(metaclass=DummyObject):\n     _backends = [\"torch\", \"transformers\"]\n "
        }
      ],
      "num_files": 6,
      "scraped_at": "2025-11-16T21:19:21.819414"
    },
    {
      "pr_number": 12328,
      "title": "Add RequestScopedPipeline for safe concurrent inference, tokenizer lock and non-mutating retrieve_timesteps",
      "body": "# What does this PR do?\r\n\r\nThis PR introduces a request-scoped pipeline abstraction and several safety/compatibility improvements that enable running many inference requests in parallel while keeping a single copy of the heavy weights (UNet, VAE, text encoder) in memory.\r\n\r\nMain changes:\r\n- Add `RequestScopedPipeline` (example implementation and utilities) which:\r\n  - Creates a lightweight per-request view of a pipeline via a shallow-copy (`copy.copy`).\r\n  - Clones only small, stateful components per-request (scheduler, RNG state, callbacks, small mutable attrs) while sharing large model weights.\r\n  - Detects and skips read-only pipeline properties (e.g., `components`) to avoid \"can't set attribute\" errors.\r\n  - Optionally enters a `model_cpu_offload_context()` to allow memory offload hooks during generation.\r\n- Add tokenizer concurrency safety:\r\n  - The request-scoped wrapper manages an internal tokenizer lock to avoid Rust tokenizer race conditions (`Already borrowed` errors).\r\n- Add `retrieve_timesteps(..., return_scheduler=True)` helper:\r\n  - Returns `(timesteps, num_inference_steps, scheduler)` **without mutating the shared scheduler**.\r\n  - Fully retro-compatible: if `return_scheduler=True` is not passed, behavior is identical to the previous API.\r\n- Add fallback heuristics:\r\n  - Prefer `scheduler.clone_for_request()` when available; otherwise attempt a safe `deepcopy()` and fall back to logging and safe defaults when cloning fails.\r\n- Documentation and examples:\r\n  - Add an example/demo server (under `examples/DiffusersServer/`) showing how to run a single model in memory and serve concurrent inference requests safely.\r\n  - Document recommended flags, environment, and an example POST request for `/api/diffusers/inference`.\r\n- Tests & CI:\r\n  - (See \"How to test\") unit tests and a simple concurrency test harness are included to validate the tokenizer lock and `retrieve_timesteps` behavior.\r\n\r\nMotivation and context\r\n- A naive concurrent server that calls `pipe.__call__` concurrently can hit race conditions (e.g., `scheduler.set_timesteps` mutating shared scheduler) or accidentally duplicate the full pipeline in memory (deepcopy), exploding GPU memory.\r\n- This PR provides a light-weight pattern to isolate per-request mutable state while keeping heavy model parameters shared, solving both correctness (race conditions) and memory usage problems.\r\n\r\nFiles changed / added (high level)\r\n- `src/diffusers/pipelines/pipeline_utils.py`  \u2014 `RequestScopedPipeline` implementation and utilities\r\n- `src/diffusers/pipelines/stable_diffusion_3/pipeline_stable_diffusion_3.py`, `src/diffusers/pipelines/flux/pipeline_flux.py`, `src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py`, `src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl.py`, `src/diffusers/pipelines/t2i_adapter/pipeline_stable_diffusion_adapter.py`, `src/diffusers/pipelines/t2i_adapter/pipeline_stable_diffusion_xl_adapter.py` \u2014 `retrieve_timesteps(..., return_scheduler=True)` helper (backwards compatible)\r\n- `src/diffusers/schedulers/*` - Implementation of the `clone_for_request(self, ...)` method to avoid race condition errors (It was adapted for each scheduler) \r\n- `examples/DiffusersServer/` \u2014 demo server and helper scripts:\r\n  - `serverasync.py` (FastAPI app factory / server example)\r\n  - `Pipelines.py` (pipeline loader classes)\r\n  - `uvicorn_diffu.py` (recommended uvicorn flags)\r\n  - `create_server.py`\r\n- Minor additions to project README describing the example server and the expected behavior\r\n\r\nBackward compatibility\r\n- `retrieve_timesteps` is fully retro-compatible: if users do not pass `return_scheduler=True`, the call behaves exactly as before (it will call `set_timesteps` on the shared scheduler).\r\n- Existing pipelines and public APIs are not modified in a breaking way. The new `RequestScopedPipeline` is additive and opt-in for server authors who want safe concurrency.\r\n- `RequestScopedPipeline` creates a lightweight, per-request view of a pipeline via a shallow copy and clones only small, mutable components (scheduler, RNG state, callbacks, small lists/dicts). Large model weights (UNet, VAE, text encoder) remain shared and are **not** duplicated.\r\n- Scheduler handling and `clone_for_request` semantics:\r\n  - When available, `scheduler.clone_for_request(num_inference_steps, ...)` is used as the preferred mechanism to obtain a scheduler configured for a single request. This ensures that any mutations performed by `set_timesteps(...)` are applied only to the local scheduler copy and never to the shared scheduler.\r\n  - If a scheduler does not implement `clone_for_request`, `retrieve_timesteps(..., return_scheduler=True)` attempts safe fallbacks in this order: (1) `deepcopy(scheduler)` and configure the copy, (2) `copy.copy(scheduler)` with a logged warning about potential shared-state risk. Only if all cloning strategies fail will the code fall back to mutating the original scheduler (and this is logged as a last-resort warning).\r\n  - This behavior is opt-in: callers who do not request a scheduler (or pass `return_scheduler=False`) preserve the original, pre-existing semantics.\r\n\r\nHow to test / reproduce\r\n1. Install the package in editable mode:\r\n ```bash\r\n   pip install -e .\r\n   pip install -r examples/DiffusersServer/requirements.txt\r\n```\r\n\r\n2. Start the example server:\r\n\r\n```bash\r\n   python examples/DiffusersServer/serverasync.py\r\n```\r\n\r\n   or\r\n\r\n```bash\r\n   python examples/DiffusersServer/uvicorn_diffu.py\r\n```\r\n3. Run multiple concurrent requests (example):\r\n\r\n```bash\r\n   python -c \"import requests, concurrent.futures, json\r\n   def r(): return requests.post('http://localhost:8500/api/diffusers/inference', json={'prompt':'A futuristic cityscape','num_inference_steps':30,'num_images_per_prompt':1}).json()\r\n   with concurrent.futures.ThreadPoolExecutor(max_workers=20) as ex: print([ex.submit(r).result() for _ in range(20)])\"\r\n```\r\n4. Verify that:\r\n\r\n   * No `Already borrowed` tokenizer errors happen under load.\r\n   * GPU memory usage does not grow linearly with requests (heavy weights remain shared).\r\n   * `retrieve_timesteps(..., return_scheduler=True)` returns the scheduler instance for per-request use and does not mutate the shared scheduler.\r\n\r\nPerformance notes\r\n\r\n* Small per-request overhead for shallow copy and cloning of small mutable state.\r\n* Large tensors/weights are shared; this keeps memory usage low while enabling tens of parallel inferences (recommended \\~10\u201350 inferences in parallel depending on hardware).\r\n\r\nSecurity & maintenance notes\r\n\r\n* The example server is a demo/harness and not hardened for production by itself; recommend placing it behind a proper auth/gateway and rate limits.\r\n* Add monitoring for memory and request queue lengths when deploying.\r\n\r\n## Before submitting\r\n\r\n* [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n* [ ] Did you read the [contributor guideline](https://github.com/huggingface/diffusers/blob/main/CONTRIBUTING.md)?\r\n* [ ] Did you read our [philosophy doc](https://github.com/huggingface/diffusers/blob/main/PHILOSOPHY.md)?\r\n* [ ] Was this discussed/approved via a GitHub issue or the \\[forum]? Please add a link to it if that's the case.\r\n* [ ] Did you make sure to update the documentation with your changes? (see `docs/usage/async_server.md`)\r\n* [ ] Did you write any new necessary tests? (see `tests/test_request_scoped.py`)\r\n\r\n## Who can review?\r\n\r\nCore library / Schedulers / Pipelines: @yiyixuxu, @asomoza, @sayakpaul\r\nGeneral / integrations: @DN6, @stevhliu\r\n",
      "html_url": "https://github.com/huggingface/diffusers/pull/12328",
      "created_at": "2025-09-14T04:42:04Z",
      "merged_at": "2025-09-18T06:03:44Z",
      "merge_commit_sha": "eda9ff8300eb3b8ceec15ef69d74e35abd3d39b3",
      "base_ref": "main",
      "head_sha": "7c4f88348a8d3536a4568398e8f1f81cabab1ddc",
      "user": "FredyRivera-dev",
      "files": [
        {
          "filename": "examples/server-async/Pipelines.py",
          "status": "added",
          "additions": 91,
          "deletions": 0,
          "changes": 91,
          "patch": "@@ -0,0 +1,91 @@\n+import logging\n+import os\n+from dataclasses import dataclass, field\n+from typing import List\n+\n+import torch\n+from pydantic import BaseModel\n+\n+from diffusers.pipelines.stable_diffusion_3.pipeline_stable_diffusion_3 import StableDiffusion3Pipeline\n+\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+class TextToImageInput(BaseModel):\n+    model: str\n+    prompt: str\n+    size: str | None = None\n+    n: int | None = None\n+\n+\n+@dataclass\n+class PresetModels:\n+    SD3: List[str] = field(default_factory=lambda: [\"stabilityai/stable-diffusion-3-medium\"])\n+    SD3_5: List[str] = field(\n+        default_factory=lambda: [\n+            \"stabilityai/stable-diffusion-3.5-large\",\n+            \"stabilityai/stable-diffusion-3.5-large-turbo\",\n+            \"stabilityai/stable-diffusion-3.5-medium\",\n+        ]\n+    )\n+\n+\n+class TextToImagePipelineSD3:\n+    def __init__(self, model_path: str | None = None):\n+        self.model_path = model_path or os.getenv(\"MODEL_PATH\")\n+        self.pipeline: StableDiffusion3Pipeline | None = None\n+        self.device: str | None = None\n+\n+    def start(self):\n+        if torch.cuda.is_available():\n+            model_path = self.model_path or \"stabilityai/stable-diffusion-3.5-large\"\n+            logger.info(\"Loading CUDA\")\n+            self.device = \"cuda\"\n+            self.pipeline = StableDiffusion3Pipeline.from_pretrained(\n+                model_path,\n+                torch_dtype=torch.float16,\n+            ).to(device=self.device)\n+        elif torch.backends.mps.is_available():\n+            model_path = self.model_path or \"stabilityai/stable-diffusion-3.5-medium\"\n+            logger.info(\"Loading MPS for Mac M Series\")\n+            self.device = \"mps\"\n+            self.pipeline = StableDiffusion3Pipeline.from_pretrained(\n+                model_path,\n+                torch_dtype=torch.bfloat16,\n+            ).to(device=self.device)\n+        else:\n+            raise Exception(\"No CUDA or MPS device available\")\n+\n+\n+class ModelPipelineInitializer:\n+    def __init__(self, model: str = \"\", type_models: str = \"t2im\"):\n+        self.model = model\n+        self.type_models = type_models\n+        self.pipeline = None\n+        self.device = \"cuda\" if torch.cuda.is_available() else \"mps\"\n+        self.model_type = None\n+\n+    def initialize_pipeline(self):\n+        if not self.model:\n+            raise ValueError(\"Model name not provided\")\n+\n+        # Check if model exists in PresetModels\n+        preset_models = PresetModels()\n+\n+        # Determine which model type we're dealing with\n+        if self.model in preset_models.SD3:\n+            self.model_type = \"SD3\"\n+        elif self.model in preset_models.SD3_5:\n+            self.model_type = \"SD3_5\"\n+\n+        # Create appropriate pipeline based on model type and type_models\n+        if self.type_models == \"t2im\":\n+            if self.model_type in [\"SD3\", \"SD3_5\"]:\n+                self.pipeline = TextToImagePipelineSD3(self.model)\n+            else:\n+                raise ValueError(f\"Model type {self.model_type} not supported for text-to-image\")\n+        elif self.type_models == \"t2v\":\n+            raise ValueError(f\"Unsupported type_models: {self.type_models}\")\n+\n+        return self.pipeline"
        },
        {
          "filename": "examples/server-async/README.md",
          "status": "added",
          "additions": 171,
          "deletions": 0,
          "changes": 171,
          "patch": "@@ -0,0 +1,171 @@\n+# Asynchronous server and parallel execution of models\n+\n+> Example/demo server that keeps a single model in memory while safely running parallel inference requests by creating per-request lightweight views and cloning only small, stateful components (schedulers, RNG state, small mutable attrs). Works with StableDiffusion3 pipelines.\n+> We recommend running 10 to 50 inferences in parallel for optimal performance, averaging between 25 and 30 seconds to 1 minute and 1 minute and 30 seconds. (This is only recommended if you have a GPU with 35GB of VRAM or more; otherwise, keep it to one or two inferences in parallel to avoid decoding or saving errors due to memory shortages.)\n+\n+## \u26a0\ufe0f IMPORTANT\n+\n+* The example demonstrates how to run pipelines like `StableDiffusion3-3.5` concurrently while keeping a single copy of the heavy model parameters on GPU.\n+\n+## Necessary components\n+\n+All the components needed to create the inference server are in the current directory:\n+\n+```\n+server-async/\n+\u251c\u2500\u2500 utils/\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500 __init__.py\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500 scheduler.py              # BaseAsyncScheduler wrapper and async_retrieve_timesteps for secure inferences\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500 requestscopedpipeline.py  # RequestScoped Pipeline for inference with a single in-memory model\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500 utils.py                  # Image/video saving utilities and service configuration\n+\u251c\u2500\u2500 Pipelines.py                   # pipeline loader classes (SD3)\n+\u251c\u2500\u2500 serverasync.py                 # FastAPI app with lifespan management and async inference endpoints\n+\u251c\u2500\u2500 test.py                        # Client test script for inference requests\n+\u251c\u2500\u2500 requirements.txt               # Dependencies\n+\u2514\u2500\u2500 README.md                      # This documentation\n+```\n+\n+## What `diffusers-async` adds / Why we needed it\n+\n+Core problem: a naive server that calls `pipe.__call__` concurrently can hit **race conditions** (e.g., `scheduler.set_timesteps` mutates shared state) or explode memory by deep-copying the whole pipeline per-request.\n+\n+`diffusers-async` / this example addresses that by:\n+\n+* **Request-scoped views**: `RequestScopedPipeline` creates a shallow copy of the pipeline per request so heavy weights (UNet, VAE, text encoder) remain shared and *are not duplicated*.\n+* **Per-request mutable state**: stateful small objects (scheduler, RNG state, small lists/dicts, callbacks) are cloned per request. The system uses `BaseAsyncScheduler.clone_for_request(...)` for scheduler cloning, with fallback to safe `deepcopy` or other heuristics.\n+* **Tokenizer concurrency safety**: `RequestScopedPipeline` now manages an internal tokenizer lock with automatic tokenizer detection and wrapping. This ensures that Rust tokenizers are safe to use under concurrency \u2014 race condition errors like `Already borrowed` no longer occur.\n+* **`async_retrieve_timesteps(..., return_scheduler=True)`**: fully retro-compatible helper that returns `(timesteps, num_inference_steps, scheduler)` without mutating the shared scheduler. For users not using `return_scheduler=True`, the behavior is identical to the original API.\n+* **Robust attribute handling**: wrapper avoids writing to read-only properties (e.g., `components`) and auto-detects small mutable attributes to clone while avoiding duplication of large tensors. Configurable tensor size threshold prevents cloning of large tensors.\n+* **Enhanced scheduler wrapping**: `BaseAsyncScheduler` automatically wraps schedulers with improved `__getattr__`, `__setattr__`, and debugging methods (`__repr__`, `__str__`).\n+\n+## How the server works (high-level flow)\n+\n+1. **Single model instance** is loaded into memory (GPU/MPS) when the server starts.\n+2. On each HTTP inference request:\n+\n+   * The server uses `RequestScopedPipeline.generate(...)` which:\n+\n+     * automatically wraps the base scheduler in `BaseAsyncScheduler` (if not already wrapped),\n+     * obtains a *local scheduler* (via `clone_for_request()` or `deepcopy`),\n+     * does `local_pipe = copy.copy(base_pipe)` (shallow copy),\n+     * sets `local_pipe.scheduler = local_scheduler` (if possible),\n+     * clones only small mutable attributes (callbacks, rng, small latents) with auto-detection,\n+     * wraps tokenizers with thread-safe locks to prevent race conditions,\n+     * optionally enters a `model_cpu_offload_context()` for memory offload hooks,\n+     * calls the pipeline on the local view (`local_pipe(...)`).\n+3. **Result**: inference completes, images are moved to CPU & saved (if requested), internal buffers freed (GC + `torch.cuda.empty_cache()`).\n+4. Multiple requests can run in parallel while sharing heavy weights and isolating mutable state.\n+\n+## How to set up and run the server\n+\n+### 1) Install dependencies\n+\n+Recommended: create a virtualenv / conda environment.\n+\n+```bash\n+pip install diffusers\n+pip install -r requirements.txt\n+```\n+\n+### 2) Start the server\n+\n+Using the `serverasync.py` file that already has everything you need:\n+\n+```bash\n+python serverasync.py\n+```\n+\n+The server will start on `http://localhost:8500` by default with the following features:\n+- FastAPI application with async lifespan management\n+- Automatic model loading and pipeline initialization\n+- Request counting and active inference tracking\n+- Memory cleanup after each inference\n+- CORS middleware for cross-origin requests\n+\n+### 3) Test the server\n+\n+Use the included test script:\n+\n+```bash\n+python test.py\n+```\n+\n+Or send a manual request:\n+\n+`POST /api/diffusers/inference` with JSON body:\n+\n+```json\n+{\n+  \"prompt\": \"A futuristic cityscape, vibrant colors\",\n+  \"num_inference_steps\": 30,\n+  \"num_images_per_prompt\": 1\n+}\n+```\n+\n+Response example:\n+\n+```json\n+{\n+  \"response\": [\"http://localhost:8500/images/img123.png\"]\n+}\n+```\n+\n+### 4) Server endpoints\n+\n+- `GET /` - Welcome message\n+- `POST /api/diffusers/inference` - Main inference endpoint\n+- `GET /images/{filename}` - Serve generated images\n+- `GET /api/status` - Server status and memory info\n+\n+## Advanced Configuration\n+\n+### RequestScopedPipeline Parameters\n+\n+```python\n+RequestScopedPipeline(\n+    pipeline,                        # Base pipeline to wrap\n+    mutable_attrs=None,             # Custom list of attributes to clone\n+    auto_detect_mutables=True,      # Enable automatic detection of mutable attributes\n+    tensor_numel_threshold=1_000_000, # Tensor size threshold for cloning\n+    tokenizer_lock=None,            # Custom threading lock for tokenizers\n+    wrap_scheduler=True             # Auto-wrap scheduler in BaseAsyncScheduler\n+)\n+```\n+\n+### BaseAsyncScheduler Features\n+\n+* Transparent proxy to the original scheduler with `__getattr__` and `__setattr__`\n+* `clone_for_request()` method for safe per-request scheduler cloning\n+* Enhanced debugging with `__repr__` and `__str__` methods\n+* Full compatibility with existing scheduler APIs\n+\n+### Server Configuration\n+\n+The server configuration can be modified in `serverasync.py` through the `ServerConfigModels` dataclass:\n+\n+```python\n+@dataclass\n+class ServerConfigModels:\n+    model: str = 'stabilityai/stable-diffusion-3.5-medium'  \n+    type_models: str = 't2im'  \n+    host: str = '0.0.0.0' \n+    port: int = 8500\n+```\n+\n+## Troubleshooting (quick)\n+\n+* `Already borrowed` \u2014 previously a Rust tokenizer concurrency error.\n+  \u2705 This is now fixed: `RequestScopedPipeline` automatically detects and wraps tokenizers with thread locks, so race conditions no longer happen.\n+\n+* `can't set attribute 'components'` \u2014 pipeline exposes read-only `components`.\n+  \u2705 The RequestScopedPipeline now detects read-only properties and skips setting them automatically.\n+\n+* Scheduler issues:\n+  * If the scheduler doesn't implement `clone_for_request` and `deepcopy` fails, we log and fallback \u2014 but prefer `async_retrieve_timesteps(..., return_scheduler=True)` to avoid mutating the shared scheduler.\n+  \u2705 Note: `async_retrieve_timesteps` is fully retro-compatible \u2014 if you don't pass `return_scheduler=True`, the behavior is unchanged.\n+\n+* Memory issues with large tensors:\n+  \u2705 The system now has configurable `tensor_numel_threshold` to prevent cloning of large tensors while still cloning small mutable ones.\n+\n+* Automatic tokenizer detection:\n+  \u2705 The system automatically identifies tokenizer components by checking for tokenizer methods, class names, and attributes, then applies thread-safe wrappers.\n\\ No newline at end of file"
        },
        {
          "filename": "examples/server-async/requirements.txt",
          "status": "added",
          "additions": 10,
          "deletions": 0,
          "changes": 10,
          "patch": "@@ -0,0 +1,10 @@\n+torch \n+torchvision \n+transformers \n+sentencepiece \n+fastapi \n+uvicorn \n+ftfy\n+accelerate\n+xformers\n+protobuf\n\\ No newline at end of file"
        },
        {
          "filename": "examples/server-async/serverasync.py",
          "status": "added",
          "additions": 230,
          "deletions": 0,
          "changes": 230,
          "patch": "@@ -0,0 +1,230 @@\n+import asyncio\n+import gc\n+import logging\n+import os\n+import random\n+import threading\n+from contextlib import asynccontextmanager\n+from dataclasses import dataclass\n+from typing import Any, Dict, Optional, Type\n+\n+import torch\n+from fastapi import FastAPI, HTTPException, Request\n+from fastapi.concurrency import run_in_threadpool\n+from fastapi.middleware.cors import CORSMiddleware\n+from fastapi.responses import FileResponse\n+from Pipelines import ModelPipelineInitializer\n+from pydantic import BaseModel\n+\n+from utils import RequestScopedPipeline, Utils\n+\n+\n+@dataclass\n+class ServerConfigModels:\n+    model: str = \"stabilityai/stable-diffusion-3.5-medium\"\n+    type_models: str = \"t2im\"\n+    constructor_pipeline: Optional[Type] = None\n+    custom_pipeline: Optional[Type] = None\n+    components: Optional[Dict[str, Any]] = None\n+    torch_dtype: Optional[torch.dtype] = None\n+    host: str = \"0.0.0.0\"\n+    port: int = 8500\n+\n+\n+server_config = ServerConfigModels()\n+\n+\n+@asynccontextmanager\n+async def lifespan(app: FastAPI):\n+    logging.basicConfig(level=logging.INFO)\n+    app.state.logger = logging.getLogger(\"diffusers-server\")\n+    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128,expandable_segments:True\"\n+    os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"0\"\n+\n+    app.state.total_requests = 0\n+    app.state.active_inferences = 0\n+    app.state.metrics_lock = asyncio.Lock()\n+    app.state.metrics_task = None\n+\n+    app.state.utils_app = Utils(\n+        host=server_config.host,\n+        port=server_config.port,\n+    )\n+\n+    async def metrics_loop():\n+        try:\n+            while True:\n+                async with app.state.metrics_lock:\n+                    total = app.state.total_requests\n+                    active = app.state.active_inferences\n+                app.state.logger.info(f\"[METRICS] total_requests={total} active_inferences={active}\")\n+                await asyncio.sleep(5)\n+        except asyncio.CancelledError:\n+            app.state.logger.info(\"Metrics loop cancelled\")\n+            raise\n+\n+    app.state.metrics_task = asyncio.create_task(metrics_loop())\n+\n+    try:\n+        yield\n+    finally:\n+        task = app.state.metrics_task\n+        if task:\n+            task.cancel()\n+            try:\n+                await task\n+            except asyncio.CancelledError:\n+                pass\n+\n+        try:\n+            stop_fn = getattr(model_pipeline, \"stop\", None) or getattr(model_pipeline, \"close\", None)\n+            if callable(stop_fn):\n+                await run_in_threadpool(stop_fn)\n+        except Exception as e:\n+            app.state.logger.warning(f\"Error during pipeline shutdown: {e}\")\n+\n+        app.state.logger.info(\"Lifespan shutdown complete\")\n+\n+\n+app = FastAPI(lifespan=lifespan)\n+\n+logger = logging.getLogger(\"DiffusersServer.Pipelines\")\n+\n+\n+initializer = ModelPipelineInitializer(\n+    model=server_config.model,\n+    type_models=server_config.type_models,\n+)\n+model_pipeline = initializer.initialize_pipeline()\n+model_pipeline.start()\n+\n+request_pipe = RequestScopedPipeline(model_pipeline.pipeline)\n+pipeline_lock = threading.Lock()\n+\n+logger.info(f\"Pipeline initialized and ready to receive requests (model ={server_config.model})\")\n+\n+app.state.MODEL_INITIALIZER = initializer\n+app.state.MODEL_PIPELINE = model_pipeline\n+app.state.REQUEST_PIPE = request_pipe\n+app.state.PIPELINE_LOCK = pipeline_lock\n+\n+\n+class JSONBodyQueryAPI(BaseModel):\n+    model: str | None = None\n+    prompt: str\n+    negative_prompt: str | None = None\n+    num_inference_steps: int = 28\n+    num_images_per_prompt: int = 1\n+\n+\n+@app.middleware(\"http\")\n+async def count_requests_middleware(request: Request, call_next):\n+    async with app.state.metrics_lock:\n+        app.state.total_requests += 1\n+    response = await call_next(request)\n+    return response\n+\n+\n+@app.get(\"/\")\n+async def root():\n+    return {\"message\": \"Welcome to the Diffusers Server\"}\n+\n+\n+@app.post(\"/api/diffusers/inference\")\n+async def api(json: JSONBodyQueryAPI):\n+    prompt = json.prompt\n+    negative_prompt = json.negative_prompt or \"\"\n+    num_steps = json.num_inference_steps\n+    num_images_per_prompt = json.num_images_per_prompt\n+\n+    wrapper = app.state.MODEL_PIPELINE\n+    initializer = app.state.MODEL_INITIALIZER\n+\n+    utils_app = app.state.utils_app\n+\n+    if not wrapper or not wrapper.pipeline:\n+        raise HTTPException(500, \"Model not initialized correctly\")\n+    if not prompt.strip():\n+        raise HTTPException(400, \"No prompt provided\")\n+\n+    def make_generator():\n+        g = torch.Generator(device=initializer.device)\n+        return g.manual_seed(random.randint(0, 10_000_000))\n+\n+    req_pipe = app.state.REQUEST_PIPE\n+\n+    def infer():\n+        gen = make_generator()\n+        return req_pipe.generate(\n+            prompt=prompt,\n+            negative_prompt=negative_prompt,\n+            generator=gen,\n+            num_inference_steps=num_steps,\n+            num_images_per_prompt=num_images_per_prompt,\n+            device=initializer.device,\n+            output_type=\"pil\",\n+        )\n+\n+    try:\n+        async with app.state.metrics_lock:\n+            app.state.active_inferences += 1\n+\n+        output = await run_in_threadpool(infer)\n+\n+        async with app.state.metrics_lock:\n+            app.state.active_inferences = max(0, app.state.active_inferences - 1)\n+\n+        urls = [utils_app.save_image(img) for img in output.images]\n+        return {\"response\": urls}\n+\n+    except Exception as e:\n+        async with app.state.metrics_lock:\n+            app.state.active_inferences = max(0, app.state.active_inferences - 1)\n+        logger.error(f\"Error during inference: {e}\")\n+        raise HTTPException(500, f\"Error in processing: {e}\")\n+\n+    finally:\n+        if torch.cuda.is_available():\n+            torch.cuda.synchronize()\n+            torch.cuda.empty_cache()\n+            torch.cuda.reset_peak_memory_stats()\n+            torch.cuda.ipc_collect()\n+        gc.collect()\n+\n+\n+@app.get(\"/images/{filename}\")\n+async def serve_image(filename: str):\n+    utils_app = app.state.utils_app\n+    file_path = os.path.join(utils_app.image_dir, filename)\n+    if not os.path.isfile(file_path):\n+        raise HTTPException(status_code=404, detail=\"Image not found\")\n+    return FileResponse(file_path, media_type=\"image/png\")\n+\n+\n+@app.get(\"/api/status\")\n+async def get_status():\n+    memory_info = {}\n+    if torch.cuda.is_available():\n+        memory_allocated = torch.cuda.memory_allocated() / 1024**3  # GB\n+        memory_reserved = torch.cuda.memory_reserved() / 1024**3  # GB\n+        memory_info = {\n+            \"memory_allocated_gb\": round(memory_allocated, 2),\n+            \"memory_reserved_gb\": round(memory_reserved, 2),\n+            \"device\": torch.cuda.get_device_name(0),\n+        }\n+\n+    return {\"current_model\": server_config.model, \"type_models\": server_config.type_models, \"memory\": memory_info}\n+\n+\n+app.add_middleware(\n+    CORSMiddleware,\n+    allow_origins=[\"*\"],\n+    allow_credentials=True,\n+    allow_methods=[\"*\"],\n+    allow_headers=[\"*\"],\n+)\n+\n+if __name__ == \"__main__\":\n+    import uvicorn\n+\n+    uvicorn.run(app, host=server_config.host, port=server_config.port)"
        },
        {
          "filename": "examples/server-async/test.py",
          "status": "added",
          "additions": 65,
          "deletions": 0,
          "changes": 65,
          "patch": "@@ -0,0 +1,65 @@\n+import os\n+import time\n+import urllib.parse\n+\n+import requests\n+\n+\n+SERVER_URL = \"http://localhost:8500/api/diffusers/inference\"\n+BASE_URL = \"http://localhost:8500\"\n+DOWNLOAD_FOLDER = \"generated_images\"\n+WAIT_BEFORE_DOWNLOAD = 2  # seconds\n+\n+os.makedirs(DOWNLOAD_FOLDER, exist_ok=True)\n+\n+\n+def save_from_url(url: str) -> str:\n+    \"\"\"Download the given URL (relative or absolute) and save it locally.\"\"\"\n+    if url.startswith(\"/\"):\n+        direct = BASE_URL.rstrip(\"/\") + url\n+    else:\n+        direct = url\n+    resp = requests.get(direct, timeout=60)\n+    resp.raise_for_status()\n+    filename = os.path.basename(urllib.parse.urlparse(direct).path) or f\"img_{int(time.time())}.png\"\n+    path = os.path.join(DOWNLOAD_FOLDER, filename)\n+    with open(path, \"wb\") as f:\n+        f.write(resp.content)\n+    return path\n+\n+\n+def main():\n+    payload = {\n+        \"prompt\": \"The T-800 Terminator Robot Returning From The Future, Anime Style\",\n+        \"num_inference_steps\": 30,\n+        \"num_images_per_prompt\": 1,\n+    }\n+\n+    print(\"Sending request...\")\n+    try:\n+        r = requests.post(SERVER_URL, json=payload, timeout=480)\n+        r.raise_for_status()\n+    except Exception as e:\n+        print(f\"Request failed: {e}\")\n+        return\n+\n+    body = r.json().get(\"response\", [])\n+    # Normalize to a list\n+    urls = body if isinstance(body, list) else [body] if body else []\n+    if not urls:\n+        print(\"No URLs found in the response. Check the server output.\")\n+        return\n+\n+    print(f\"Received {len(urls)} URL(s). Waiting {WAIT_BEFORE_DOWNLOAD}s before downloading...\")\n+    time.sleep(WAIT_BEFORE_DOWNLOAD)\n+\n+    for u in urls:\n+        try:\n+            path = save_from_url(u)\n+            print(f\"Image saved to: {path}\")\n+        except Exception as e:\n+            print(f\"Error downloading {u}: {e}\")\n+\n+\n+if __name__ == \"__main__\":\n+    main()"
        },
        {
          "filename": "examples/server-async/utils/__init__.py",
          "status": "added",
          "additions": 2,
          "deletions": 0,
          "changes": 2,
          "patch": "@@ -0,0 +1,2 @@\n+from .requestscopedpipeline import RequestScopedPipeline\n+from .utils import Utils"
        },
        {
          "filename": "examples/server-async/utils/requestscopedpipeline.py",
          "status": "added",
          "additions": 296,
          "deletions": 0,
          "changes": 296,
          "patch": "@@ -0,0 +1,296 @@\n+import copy\n+import threading\n+from typing import Any, Iterable, List, Optional\n+\n+import torch\n+\n+from diffusers.utils import logging\n+\n+from .scheduler import BaseAsyncScheduler, async_retrieve_timesteps\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+def safe_tokenize(tokenizer, *args, lock, **kwargs):\n+    with lock:\n+        return tokenizer(*args, **kwargs)\n+\n+\n+class RequestScopedPipeline:\n+    DEFAULT_MUTABLE_ATTRS = [\n+        \"_all_hooks\",\n+        \"_offload_device\",\n+        \"_progress_bar_config\",\n+        \"_progress_bar\",\n+        \"_rng_state\",\n+        \"_last_seed\",\n+        \"latents\",\n+    ]\n+\n+    def __init__(\n+        self,\n+        pipeline: Any,\n+        mutable_attrs: Optional[Iterable[str]] = None,\n+        auto_detect_mutables: bool = True,\n+        tensor_numel_threshold: int = 1_000_000,\n+        tokenizer_lock: Optional[threading.Lock] = None,\n+        wrap_scheduler: bool = True,\n+    ):\n+        self._base = pipeline\n+        self.unet = getattr(pipeline, \"unet\", None)\n+        self.vae = getattr(pipeline, \"vae\", None)\n+        self.text_encoder = getattr(pipeline, \"text_encoder\", None)\n+        self.components = getattr(pipeline, \"components\", None)\n+\n+        if wrap_scheduler and hasattr(pipeline, \"scheduler\") and pipeline.scheduler is not None:\n+            if not isinstance(pipeline.scheduler, BaseAsyncScheduler):\n+                pipeline.scheduler = BaseAsyncScheduler(pipeline.scheduler)\n+\n+        self._mutable_attrs = list(mutable_attrs) if mutable_attrs is not None else list(self.DEFAULT_MUTABLE_ATTRS)\n+        self._tokenizer_lock = tokenizer_lock if tokenizer_lock is not None else threading.Lock()\n+\n+        self._auto_detect_mutables = bool(auto_detect_mutables)\n+        self._tensor_numel_threshold = int(tensor_numel_threshold)\n+\n+        self._auto_detected_attrs: List[str] = []\n+\n+    def _make_local_scheduler(self, num_inference_steps: int, device: Optional[str] = None, **clone_kwargs):\n+        base_sched = getattr(self._base, \"scheduler\", None)\n+        if base_sched is None:\n+            return None\n+\n+        if not isinstance(base_sched, BaseAsyncScheduler):\n+            wrapped_scheduler = BaseAsyncScheduler(base_sched)\n+        else:\n+            wrapped_scheduler = base_sched\n+\n+        try:\n+            return wrapped_scheduler.clone_for_request(\n+                num_inference_steps=num_inference_steps, device=device, **clone_kwargs\n+            )\n+        except Exception as e:\n+            logger.debug(f\"clone_for_request failed: {e}; falling back to deepcopy()\")\n+            try:\n+                return copy.deepcopy(wrapped_scheduler)\n+            except Exception as e:\n+                logger.warning(f\"Deepcopy of scheduler failed: {e}. Returning original scheduler (*risky*).\")\n+                return wrapped_scheduler\n+\n+    def _autodetect_mutables(self, max_attrs: int = 40):\n+        if not self._auto_detect_mutables:\n+            return []\n+\n+        if self._auto_detected_attrs:\n+            return self._auto_detected_attrs\n+\n+        candidates: List[str] = []\n+        seen = set()\n+        for name in dir(self._base):\n+            if name.startswith(\"__\"):\n+                continue\n+            if name in self._mutable_attrs:\n+                continue\n+            if name in (\"to\", \"save_pretrained\", \"from_pretrained\"):\n+                continue\n+            try:\n+                val = getattr(self._base, name)\n+            except Exception:\n+                continue\n+\n+            import types\n+\n+            # skip callables and modules\n+            if callable(val) or isinstance(val, (types.ModuleType, types.FunctionType, types.MethodType)):\n+                continue\n+\n+            # containers -> candidate\n+            if isinstance(val, (dict, list, set, tuple, bytearray)):\n+                candidates.append(name)\n+                seen.add(name)\n+            else:\n+                # try Tensor detection\n+                try:\n+                    if isinstance(val, torch.Tensor):\n+                        if val.numel() <= self._tensor_numel_threshold:\n+                            candidates.append(name)\n+                            seen.add(name)\n+                        else:\n+                            logger.debug(f\"Ignoring large tensor attr '{name}', numel={val.numel()}\")\n+                except Exception:\n+                    continue\n+\n+            if len(candidates) >= max_attrs:\n+                break\n+\n+        self._auto_detected_attrs = candidates\n+        logger.debug(f\"Autodetected mutable attrs to clone: {self._auto_detected_attrs}\")\n+        return self._auto_detected_attrs\n+\n+    def _is_readonly_property(self, base_obj, attr_name: str) -> bool:\n+        try:\n+            cls = type(base_obj)\n+            descriptor = getattr(cls, attr_name, None)\n+            if isinstance(descriptor, property):\n+                return descriptor.fset is None\n+            if hasattr(descriptor, \"__set__\") is False and descriptor is not None:\n+                return False\n+        except Exception:\n+            pass\n+        return False\n+\n+    def _clone_mutable_attrs(self, base, local):\n+        attrs_to_clone = list(self._mutable_attrs)\n+        attrs_to_clone.extend(self._autodetect_mutables())\n+\n+        EXCLUDE_ATTRS = {\n+            \"components\",\n+        }\n+\n+        for attr in attrs_to_clone:\n+            if attr in EXCLUDE_ATTRS:\n+                logger.debug(f\"Skipping excluded attr '{attr}'\")\n+                continue\n+            if not hasattr(base, attr):\n+                continue\n+            if self._is_readonly_property(base, attr):\n+                logger.debug(f\"Skipping read-only property '{attr}'\")\n+                continue\n+\n+            try:\n+                val = getattr(base, attr)\n+            except Exception as e:\n+                logger.debug(f\"Could not getattr('{attr}') on base pipeline: {e}\")\n+                continue\n+\n+            try:\n+                if isinstance(val, dict):\n+                    setattr(local, attr, dict(val))\n+                elif isinstance(val, (list, tuple, set)):\n+                    setattr(local, attr, list(val))\n+                elif isinstance(val, bytearray):\n+                    setattr(local, attr, bytearray(val))\n+                else:\n+                    # small tensors or atomic values\n+                    if isinstance(val, torch.Tensor):\n+                        if val.numel() <= self._tensor_numel_threshold:\n+                            setattr(local, attr, val.clone())\n+                        else:\n+                            # don't clone big tensors, keep reference\n+                            setattr(local, attr, val)\n+                    else:\n+                        try:\n+                            setattr(local, attr, copy.copy(val))\n+                        except Exception:\n+                            setattr(local, attr, val)\n+            except (AttributeError, TypeError) as e:\n+                logger.debug(f\"Skipping cloning attribute '{attr}' because it is not settable: {e}\")\n+                continue\n+            except Exception as e:\n+                logger.debug(f\"Unexpected error cloning attribute '{attr}': {e}\")\n+                continue\n+\n+    def _is_tokenizer_component(self, component) -> bool:\n+        if component is None:\n+            return False\n+\n+        tokenizer_methods = [\"encode\", \"decode\", \"tokenize\", \"__call__\"]\n+        has_tokenizer_methods = any(hasattr(component, method) for method in tokenizer_methods)\n+\n+        class_name = component.__class__.__name__.lower()\n+        has_tokenizer_in_name = \"tokenizer\" in class_name\n+\n+        tokenizer_attrs = [\"vocab_size\", \"pad_token\", \"eos_token\", \"bos_token\"]\n+        has_tokenizer_attrs = any(hasattr(component, attr) for attr in tokenizer_attrs)\n+\n+        return has_tokenizer_methods and (has_tokenizer_in_name or has_tokenizer_attrs)\n+\n+    def generate(self, *args, num_inference_steps: int = 50, device: Optional[str] = None, **kwargs):\n+        local_scheduler = self._make_local_scheduler(num_inference_steps=num_inference_steps, device=device)\n+\n+        try:\n+            local_pipe = copy.copy(self._base)\n+        except Exception as e:\n+            logger.warning(f\"copy.copy(self._base) failed: {e}. Falling back to deepcopy (may increase memory).\")\n+            local_pipe = copy.deepcopy(self._base)\n+\n+        if local_scheduler is not None:\n+            try:\n+                timesteps, num_steps, configured_scheduler = async_retrieve_timesteps(\n+                    local_scheduler.scheduler,\n+                    num_inference_steps=num_inference_steps,\n+                    device=device,\n+                    return_scheduler=True,\n+                    **{k: v for k, v in kwargs.items() if k in [\"timesteps\", \"sigmas\"]},\n+                )\n+\n+                final_scheduler = BaseAsyncScheduler(configured_scheduler)\n+                setattr(local_pipe, \"scheduler\", final_scheduler)\n+            except Exception:\n+                logger.warning(\"Could not set scheduler on local pipe; proceeding without replacing scheduler.\")\n+\n+        self._clone_mutable_attrs(self._base, local_pipe)\n+\n+        # 4) wrap tokenizers on the local pipe with the lock wrapper\n+        tokenizer_wrappers = {}  # name -> original_tokenizer\n+        try:\n+            # a) wrap direct tokenizer attributes (tokenizer, tokenizer_2, ...)\n+            for name in dir(local_pipe):\n+                if \"tokenizer\" in name and not name.startswith(\"_\"):\n+                    tok = getattr(local_pipe, name, None)\n+                    if tok is not None and self._is_tokenizer_component(tok):\n+                        tokenizer_wrappers[name] = tok\n+                        setattr(\n+                            local_pipe,\n+                            name,\n+                            lambda *args, tok=tok, **kwargs: safe_tokenize(\n+                                tok, *args, lock=self._tokenizer_lock, **kwargs\n+                            ),\n+                        )\n+\n+            # b) wrap tokenizers in components dict\n+            if hasattr(local_pipe, \"components\") and isinstance(local_pipe.components, dict):\n+                for key, val in local_pipe.components.items():\n+                    if val is None:\n+                        continue\n+\n+                    if self._is_tokenizer_component(val):\n+                        tokenizer_wrappers[f\"components[{key}]\"] = val\n+                        local_pipe.components[key] = lambda *args, tokenizer=val, **kwargs: safe_tokenize(\n+                            tokenizer, *args, lock=self._tokenizer_lock, **kwargs\n+                        )\n+\n+        except Exception as e:\n+            logger.debug(f\"Tokenizer wrapping step encountered an error: {e}\")\n+\n+        result = None\n+        cm = getattr(local_pipe, \"model_cpu_offload_context\", None)\n+        try:\n+            if callable(cm):\n+                try:\n+                    with cm():\n+                        result = local_pipe(*args, num_inference_steps=num_inference_steps, **kwargs)\n+                except TypeError:\n+                    # cm might be a context manager instance rather than callable\n+                    try:\n+                        with cm:\n+                            result = local_pipe(*args, num_inference_steps=num_inference_steps, **kwargs)\n+                    except Exception as e:\n+                        logger.debug(f\"model_cpu_offload_context usage failed: {e}. Proceeding without it.\")\n+                        result = local_pipe(*args, num_inference_steps=num_inference_steps, **kwargs)\n+            else:\n+                # no offload context available \u2014 call directly\n+                result = local_pipe(*args, num_inference_steps=num_inference_steps, **kwargs)\n+\n+            return result\n+\n+        finally:\n+            try:\n+                for name, tok in tokenizer_wrappers.items():\n+                    if name.startswith(\"components[\"):\n+                        key = name[len(\"components[\") : -1]\n+                        local_pipe.components[key] = tok\n+                    else:\n+                        setattr(local_pipe, name, tok)\n+            except Exception as e:\n+                logger.debug(f\"Error restoring wrapped tokenizers: {e}\")"
        },
        {
          "filename": "examples/server-async/utils/scheduler.py",
          "status": "added",
          "additions": 141,
          "deletions": 0,
          "changes": 141,
          "patch": "@@ -0,0 +1,141 @@\n+import copy\n+import inspect\n+from typing import Any, List, Optional, Union\n+\n+import torch\n+\n+\n+class BaseAsyncScheduler:\n+    def __init__(self, scheduler: Any):\n+        self.scheduler = scheduler\n+\n+    def __getattr__(self, name: str):\n+        if hasattr(self.scheduler, name):\n+            return getattr(self.scheduler, name)\n+        raise AttributeError(f\"'{self.__class__.__name__}' object has no attribute '{name}'\")\n+\n+    def __setattr__(self, name: str, value):\n+        if name == \"scheduler\":\n+            super().__setattr__(name, value)\n+        else:\n+            if hasattr(self, \"scheduler\") and hasattr(self.scheduler, name):\n+                setattr(self.scheduler, name, value)\n+            else:\n+                super().__setattr__(name, value)\n+\n+    def clone_for_request(self, num_inference_steps: int, device: Union[str, torch.device, None] = None, **kwargs):\n+        local = copy.deepcopy(self.scheduler)\n+        local.set_timesteps(num_inference_steps=num_inference_steps, device=device, **kwargs)\n+        cloned = self.__class__(local)\n+        return cloned\n+\n+    def __repr__(self):\n+        return f\"BaseAsyncScheduler({repr(self.scheduler)})\"\n+\n+    def __str__(self):\n+        return f\"BaseAsyncScheduler wrapping: {str(self.scheduler)}\"\n+\n+\n+def async_retrieve_timesteps(\n+    scheduler,\n+    num_inference_steps: Optional[int] = None,\n+    device: Optional[Union[str, torch.device]] = None,\n+    timesteps: Optional[List[int]] = None,\n+    sigmas: Optional[List[float]] = None,\n+    **kwargs,\n+):\n+    r\"\"\"\n+    Calls the scheduler's `set_timesteps` method and retrieves timesteps from the scheduler after the call.\n+    Handles custom timesteps. Any kwargs will be supplied to `scheduler.set_timesteps`.\n+\n+    Backwards compatible: by default the function behaves exactly as before and returns\n+        (timesteps_tensor, num_inference_steps)\n+\n+    If the caller passes `return_scheduler=True` in kwargs, the function will **not** mutate the passed\n+    scheduler. Instead it will use a cloned scheduler if available (via `scheduler.clone_for_request`)\n+    or a deepcopy fallback, call `set_timesteps` on that cloned scheduler, and return:\n+        (timesteps_tensor, num_inference_steps, scheduler_in_use)\n+\n+    Args:\n+        scheduler (`SchedulerMixin`):\n+            The scheduler to get timesteps from.\n+        num_inference_steps (`int`):\n+            The number of diffusion steps used when generating samples with a pre-trained model. If used, `timesteps`\n+            must be `None`.\n+        device (`str` or `torch.device`, *optional*):\n+            The device to which the timesteps should be moved to. If `None`, the timesteps are not moved.\n+        timesteps (`List[int]`, *optional*):\n+            Custom timesteps used to override the timestep spacing strategy of the scheduler. If `timesteps` is passed,\n+            `num_inference_steps` and `sigmas` must be `None`.\n+        sigmas (`List[float]`, *optional*):\n+            Custom sigmas used to override the timestep spacing strategy of the scheduler. If `sigmas` is passed,\n+            `num_inference_steps` and `timesteps` must be `None`.\n+\n+    Optional kwargs:\n+        return_scheduler (bool, default False): if True, return (timesteps, num_inference_steps, scheduler_in_use)\n+            where `scheduler_in_use` is a scheduler instance that already has timesteps set.\n+            This mode will prefer `scheduler.clone_for_request(...)` if available, to avoid mutating the original scheduler.\n+\n+    Returns:\n+        `(timesteps_tensor, num_inference_steps)` by default (backwards compatible), or\n+        `(timesteps_tensor, num_inference_steps, scheduler_in_use)` if `return_scheduler=True`.\n+    \"\"\"\n+    # pop our optional control kwarg (keeps compatibility)\n+    return_scheduler = bool(kwargs.pop(\"return_scheduler\", False))\n+\n+    if timesteps is not None and sigmas is not None:\n+        raise ValueError(\"Only one of `timesteps` or `sigmas` can be passed. Please choose one to set custom values\")\n+\n+    # choose scheduler to call set_timesteps on\n+    scheduler_in_use = scheduler\n+    if return_scheduler:\n+        # Do not mutate the provided scheduler: prefer to clone if possible\n+        if hasattr(scheduler, \"clone_for_request\"):\n+            try:\n+                # clone_for_request may accept num_inference_steps or other kwargs; be permissive\n+                scheduler_in_use = scheduler.clone_for_request(\n+                    num_inference_steps=num_inference_steps or 0, device=device\n+                )\n+            except Exception:\n+                scheduler_in_use = copy.deepcopy(scheduler)\n+        else:\n+            # fallback deepcopy (scheduler tends to be smallish - acceptable)\n+            scheduler_in_use = copy.deepcopy(scheduler)\n+\n+    # helper to test if set_timesteps supports a particular kwarg\n+    def _accepts(param_name: str) -> bool:\n+        try:\n+            return param_name in set(inspect.signature(scheduler_in_use.set_timesteps).parameters.keys())\n+        except (ValueError, TypeError):\n+            # if signature introspection fails, be permissive and attempt the call later\n+            return False\n+\n+    # now call set_timesteps on the chosen scheduler_in_use (may be original or clone)\n+    if timesteps is not None:\n+        accepts_timesteps = _accepts(\"timesteps\")\n+        if not accepts_timesteps:\n+            raise ValueError(\n+                f\"The current scheduler class {scheduler_in_use.__class__}'s `set_timesteps` does not support custom\"\n+                f\" timestep schedules. Please check whether you are using the correct scheduler.\"\n+            )\n+        scheduler_in_use.set_timesteps(timesteps=timesteps, device=device, **kwargs)\n+        timesteps_out = scheduler_in_use.timesteps\n+        num_inference_steps = len(timesteps_out)\n+    elif sigmas is not None:\n+        accept_sigmas = _accepts(\"sigmas\")\n+        if not accept_sigmas:\n+            raise ValueError(\n+                f\"The current scheduler class {scheduler_in_use.__class__}'s `set_timesteps` does not support custom\"\n+                f\" sigmas schedules. Please check whether you are using the correct scheduler.\"\n+            )\n+        scheduler_in_use.set_timesteps(sigmas=sigmas, device=device, **kwargs)\n+        timesteps_out = scheduler_in_use.timesteps\n+        num_inference_steps = len(timesteps_out)\n+    else:\n+        # default path\n+        scheduler_in_use.set_timesteps(num_inference_steps, device=device, **kwargs)\n+        timesteps_out = scheduler_in_use.timesteps\n+\n+    if return_scheduler:\n+        return timesteps_out, num_inference_steps, scheduler_in_use\n+    return timesteps_out, num_inference_steps"
        },
        {
          "filename": "examples/server-async/utils/utils.py",
          "status": "added",
          "additions": 48,
          "deletions": 0,
          "changes": 48,
          "patch": "@@ -0,0 +1,48 @@\n+import gc\n+import logging\n+import os\n+import tempfile\n+import uuid\n+\n+import torch\n+\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+class Utils:\n+    def __init__(self, host: str = \"0.0.0.0\", port: int = 8500):\n+        self.service_url = f\"http://{host}:{port}\"\n+        self.image_dir = os.path.join(tempfile.gettempdir(), \"images\")\n+        if not os.path.exists(self.image_dir):\n+            os.makedirs(self.image_dir)\n+\n+        self.video_dir = os.path.join(tempfile.gettempdir(), \"videos\")\n+        if not os.path.exists(self.video_dir):\n+            os.makedirs(self.video_dir)\n+\n+    def save_image(self, image):\n+        if hasattr(image, \"to\"):\n+            try:\n+                image = image.to(\"cpu\")\n+            except Exception:\n+                pass\n+\n+        if isinstance(image, torch.Tensor):\n+            from torchvision import transforms\n+\n+            to_pil = transforms.ToPILImage()\n+            image = to_pil(image.squeeze(0).clamp(0, 1))\n+\n+        filename = \"img\" + str(uuid.uuid4()).split(\"-\")[0] + \".png\"\n+        image_path = os.path.join(self.image_dir, filename)\n+        logger.info(f\"Saving image to {image_path}\")\n+\n+        image.save(image_path, format=\"PNG\", optimize=True)\n+\n+        del image\n+        gc.collect()\n+        if torch.cuda.is_available():\n+            torch.cuda.empty_cache()\n+\n+        return os.path.join(self.service_url, \"images\", filename)"
        }
      ],
      "num_files": 9,
      "scraped_at": "2025-11-16T21:19:24.519782"
    },
    {
      "pr_number": 12315,
      "title": "[tests] Single scheduler in lora tests",
      "body": "As discussed in https://github.com/huggingface/diffusers/pull/12298",
      "html_url": "https://github.com/huggingface/diffusers/pull/12315",
      "created_at": "2025-09-11T05:22:36Z",
      "merged_at": "2025-09-24T03:06:50Z",
      "merge_commit_sha": "09e777a3e13cf811e35da57abfe6ce239d9b0f15",
      "base_ref": "main",
      "head_sha": "04dc75cb7dcc4d600fdc4abce1b73352a89664c5",
      "user": "sayakpaul",
      "files": [
        {
          "filename": "tests/lora/test_lora_layers_auraflow.py",
          "status": "modified",
          "additions": 0,
          "deletions": 1,
          "changes": 1,
          "patch": "@@ -43,7 +43,6 @@\n class AuraFlowLoRATests(unittest.TestCase, PeftLoraLoaderMixinTests):\n     pipeline_class = AuraFlowPipeline\n     scheduler_cls = FlowMatchEulerDiscreteScheduler\n-    scheduler_classes = [FlowMatchEulerDiscreteScheduler]\n     scheduler_kwargs = {}\n \n     transformer_kwargs = {"
        },
        {
          "filename": "tests/lora/test_lora_layers_cogvideox.py",
          "status": "modified",
          "additions": 0,
          "deletions": 2,
          "changes": 2,
          "patch": "@@ -21,7 +21,6 @@\n \n from diffusers import (\n     AutoencoderKLCogVideoX,\n-    CogVideoXDDIMScheduler,\n     CogVideoXDPMScheduler,\n     CogVideoXPipeline,\n     CogVideoXTransformer3DModel,\n@@ -44,7 +43,6 @@ class CogVideoXLoRATests(unittest.TestCase, PeftLoraLoaderMixinTests):\n     pipeline_class = CogVideoXPipeline\n     scheduler_cls = CogVideoXDPMScheduler\n     scheduler_kwargs = {\"timestep_spacing\": \"trailing\"}\n-    scheduler_classes = [CogVideoXDDIMScheduler, CogVideoXDPMScheduler]\n \n     transformer_kwargs = {\n         \"num_attention_heads\": 4,"
        },
        {
          "filename": "tests/lora/test_lora_layers_cogview4.py",
          "status": "modified",
          "additions": 17,
          "deletions": 19,
          "changes": 36,
          "patch": "@@ -50,7 +50,6 @@ def from_pretrained(*args, **kwargs):\n class CogView4LoRATests(unittest.TestCase, PeftLoraLoaderMixinTests):\n     pipeline_class = CogView4Pipeline\n     scheduler_cls = FlowMatchEulerDiscreteScheduler\n-    scheduler_classes = [FlowMatchEulerDiscreteScheduler]\n     scheduler_kwargs = {}\n \n     transformer_kwargs = {\n@@ -124,30 +123,29 @@ def test_simple_inference_save_pretrained(self):\n         \"\"\"\n         Tests a simple usecase where users could use saving utilities for LoRA through save_pretrained\n         \"\"\"\n-        for scheduler_cls in self.scheduler_classes:\n-            components, _, _ = self.get_dummy_components(scheduler_cls)\n-            pipe = self.pipeline_class(**components)\n-            pipe = pipe.to(torch_device)\n-            pipe.set_progress_bar_config(disable=None)\n-            _, _, inputs = self.get_dummy_inputs(with_generator=False)\n+        components, _, _ = self.get_dummy_components()\n+        pipe = self.pipeline_class(**components)\n+        pipe = pipe.to(torch_device)\n+        pipe.set_progress_bar_config(disable=None)\n+        _, _, inputs = self.get_dummy_inputs(with_generator=False)\n \n-            output_no_lora = pipe(**inputs, generator=torch.manual_seed(0))[0]\n-            self.assertTrue(output_no_lora.shape == self.output_shape)\n+        output_no_lora = pipe(**inputs, generator=torch.manual_seed(0))[0]\n+        self.assertTrue(output_no_lora.shape == self.output_shape)\n \n-            images_lora = pipe(**inputs, generator=torch.manual_seed(0))[0]\n+        images_lora = pipe(**inputs, generator=torch.manual_seed(0))[0]\n \n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                pipe.save_pretrained(tmpdirname)\n+        with tempfile.TemporaryDirectory() as tmpdirname:\n+            pipe.save_pretrained(tmpdirname)\n \n-                pipe_from_pretrained = self.pipeline_class.from_pretrained(tmpdirname)\n-                pipe_from_pretrained.to(torch_device)\n+            pipe_from_pretrained = self.pipeline_class.from_pretrained(tmpdirname)\n+            pipe_from_pretrained.to(torch_device)\n \n-            images_lora_save_pretrained = pipe_from_pretrained(**inputs, generator=torch.manual_seed(0))[0]\n+        images_lora_save_pretrained = pipe_from_pretrained(**inputs, generator=torch.manual_seed(0))[0]\n \n-            self.assertTrue(\n-                np.allclose(images_lora, images_lora_save_pretrained, atol=1e-3, rtol=1e-3),\n-                \"Loading from saved checkpoints should give same results.\",\n-            )\n+        self.assertTrue(\n+            np.allclose(images_lora, images_lora_save_pretrained, atol=1e-3, rtol=1e-3),\n+            \"Loading from saved checkpoints should give same results.\",\n+        )\n \n     @parameterized.expand([(\"block_level\", True), (\"leaf_level\", False)])\n     @require_torch_accelerator"
        },
        {
          "filename": "tests/lora/test_lora_layers_flux.py",
          "status": "modified",
          "additions": 2,
          "deletions": 4,
          "changes": 6,
          "patch": "@@ -55,9 +55,8 @@\n @require_peft_backend\n class FluxLoRATests(unittest.TestCase, PeftLoraLoaderMixinTests):\n     pipeline_class = FluxPipeline\n-    scheduler_cls = FlowMatchEulerDiscreteScheduler()\n+    scheduler_cls = FlowMatchEulerDiscreteScheduler\n     scheduler_kwargs = {}\n-    scheduler_classes = [FlowMatchEulerDiscreteScheduler]\n     transformer_kwargs = {\n         \"patch_size\": 1,\n         \"in_channels\": 4,\n@@ -282,9 +281,8 @@ def test_simple_inference_with_text_denoiser_multi_adapter_block_lora(self):\n \n class FluxControlLoRATests(unittest.TestCase, PeftLoraLoaderMixinTests):\n     pipeline_class = FluxControlPipeline\n-    scheduler_cls = FlowMatchEulerDiscreteScheduler()\n+    scheduler_cls = FlowMatchEulerDiscreteScheduler\n     scheduler_kwargs = {}\n-    scheduler_classes = [FlowMatchEulerDiscreteScheduler]\n     transformer_kwargs = {\n         \"patch_size\": 1,\n         \"in_channels\": 8,"
        },
        {
          "filename": "tests/lora/test_lora_layers_hunyuanvideo.py",
          "status": "modified",
          "additions": 0,
          "deletions": 1,
          "changes": 1,
          "patch": "@@ -51,7 +51,6 @@\n class HunyuanVideoLoRATests(unittest.TestCase, PeftLoraLoaderMixinTests):\n     pipeline_class = HunyuanVideoPipeline\n     scheduler_cls = FlowMatchEulerDiscreteScheduler\n-    scheduler_classes = [FlowMatchEulerDiscreteScheduler]\n     scheduler_kwargs = {}\n \n     transformer_kwargs = {"
        },
        {
          "filename": "tests/lora/test_lora_layers_ltx_video.py",
          "status": "modified",
          "additions": 0,
          "deletions": 1,
          "changes": 1,
          "patch": "@@ -37,7 +37,6 @@\n class LTXVideoLoRATests(unittest.TestCase, PeftLoraLoaderMixinTests):\n     pipeline_class = LTXPipeline\n     scheduler_cls = FlowMatchEulerDiscreteScheduler\n-    scheduler_classes = [FlowMatchEulerDiscreteScheduler]\n     scheduler_kwargs = {}\n \n     transformer_kwargs = {"
        },
        {
          "filename": "tests/lora/test_lora_layers_lumina2.py",
          "status": "modified",
          "additions": 27,
          "deletions": 31,
          "changes": 58,
          "patch": "@@ -39,7 +39,6 @@\n class Lumina2LoRATests(unittest.TestCase, PeftLoraLoaderMixinTests):\n     pipeline_class = Lumina2Pipeline\n     scheduler_cls = FlowMatchEulerDiscreteScheduler\n-    scheduler_classes = [FlowMatchEulerDiscreteScheduler]\n     scheduler_kwargs = {}\n \n     transformer_kwargs = {\n@@ -141,33 +140,30 @@ def test_simple_inference_with_text_lora_save_load(self):\n         strict=False,\n     )\n     def test_lora_fuse_nan(self):\n-        for scheduler_cls in self.scheduler_classes:\n-            components, text_lora_config, denoiser_lora_config = self.get_dummy_components(scheduler_cls)\n-            pipe = self.pipeline_class(**components)\n-            pipe = pipe.to(torch_device)\n-            pipe.set_progress_bar_config(disable=None)\n-            _, _, inputs = self.get_dummy_inputs(with_generator=False)\n-\n-            if \"text_encoder\" in self.pipeline_class._lora_loadable_modules:\n-                pipe.text_encoder.add_adapter(text_lora_config, \"adapter-1\")\n-                self.assertTrue(\n-                    check_if_lora_correctly_set(pipe.text_encoder), \"Lora not correctly set in text encoder\"\n-                )\n-\n-            denoiser = pipe.transformer if self.unet_kwargs is None else pipe.unet\n-            denoiser.add_adapter(denoiser_lora_config, \"adapter-1\")\n-            self.assertTrue(check_if_lora_correctly_set(denoiser), \"Lora not correctly set in denoiser.\")\n-\n-            # corrupt one LoRA weight with `inf` values\n-            with torch.no_grad():\n-                pipe.transformer.layers[0].attn.to_q.lora_A[\"adapter-1\"].weight += float(\"inf\")\n-\n-            # with `safe_fusing=True` we should see an Error\n-            with self.assertRaises(ValueError):\n-                pipe.fuse_lora(components=self.pipeline_class._lora_loadable_modules, safe_fusing=True)\n-\n-            # without we should not see an error, but every image will be black\n-            pipe.fuse_lora(components=self.pipeline_class._lora_loadable_modules, safe_fusing=False)\n-            out = pipe(**inputs)[0]\n-\n-            self.assertTrue(np.isnan(out).all())\n+        components, text_lora_config, denoiser_lora_config = self.get_dummy_components()\n+        pipe = self.pipeline_class(**components)\n+        pipe = pipe.to(torch_device)\n+        pipe.set_progress_bar_config(disable=None)\n+        _, _, inputs = self.get_dummy_inputs(with_generator=False)\n+\n+        if \"text_encoder\" in self.pipeline_class._lora_loadable_modules:\n+            pipe.text_encoder.add_adapter(text_lora_config, \"adapter-1\")\n+            self.assertTrue(check_if_lora_correctly_set(pipe.text_encoder), \"Lora not correctly set in text encoder\")\n+\n+        denoiser = pipe.transformer if self.unet_kwargs is None else pipe.unet\n+        denoiser.add_adapter(denoiser_lora_config, \"adapter-1\")\n+        self.assertTrue(check_if_lora_correctly_set(denoiser), \"Lora not correctly set in denoiser.\")\n+\n+        # corrupt one LoRA weight with `inf` values\n+        with torch.no_grad():\n+            pipe.transformer.layers[0].attn.to_q.lora_A[\"adapter-1\"].weight += float(\"inf\")\n+\n+        # with `safe_fusing=True` we should see an Error\n+        with self.assertRaises(ValueError):\n+            pipe.fuse_lora(components=self.pipeline_class._lora_loadable_modules, safe_fusing=True)\n+\n+        # without we should not see an error, but every image will be black\n+        pipe.fuse_lora(components=self.pipeline_class._lora_loadable_modules, safe_fusing=False)\n+        out = pipe(**inputs)[0]\n+\n+        self.assertTrue(np.isnan(out).all())"
        },
        {
          "filename": "tests/lora/test_lora_layers_mochi.py",
          "status": "modified",
          "additions": 0,
          "deletions": 1,
          "changes": 1,
          "patch": "@@ -37,7 +37,6 @@\n class MochiLoRATests(unittest.TestCase, PeftLoraLoaderMixinTests):\n     pipeline_class = MochiPipeline\n     scheduler_cls = FlowMatchEulerDiscreteScheduler\n-    scheduler_classes = [FlowMatchEulerDiscreteScheduler]\n     scheduler_kwargs = {}\n \n     transformer_kwargs = {"
        },
        {
          "filename": "tests/lora/test_lora_layers_qwenimage.py",
          "status": "modified",
          "additions": 0,
          "deletions": 1,
          "changes": 1,
          "patch": "@@ -37,7 +37,6 @@\n class QwenImageLoRATests(unittest.TestCase, PeftLoraLoaderMixinTests):\n     pipeline_class = QwenImagePipeline\n     scheduler_cls = FlowMatchEulerDiscreteScheduler\n-    scheduler_classes = [FlowMatchEulerDiscreteScheduler]\n     scheduler_kwargs = {}\n \n     transformer_kwargs = {"
        },
        {
          "filename": "tests/lora/test_lora_layers_sana.py",
          "status": "modified",
          "additions": 2,
          "deletions": 3,
          "changes": 5,
          "patch": "@@ -31,9 +31,8 @@\n @require_peft_backend\n class SanaLoRATests(unittest.TestCase, PeftLoraLoaderMixinTests):\n     pipeline_class = SanaPipeline\n-    scheduler_cls = FlowMatchEulerDiscreteScheduler(shift=7.0)\n-    scheduler_kwargs = {}\n-    scheduler_classes = [FlowMatchEulerDiscreteScheduler]\n+    scheduler_cls = FlowMatchEulerDiscreteScheduler\n+    scheduler_kwargs = {\"shift\": 7.0}\n     transformer_kwargs = {\n         \"patch_size\": 1,\n         \"in_channels\": 4,"
        },
        {
          "filename": "tests/lora/test_lora_layers_sd3.py",
          "status": "modified",
          "additions": 0,
          "deletions": 1,
          "changes": 1,
          "patch": "@@ -55,7 +55,6 @@ class SD3LoRATests(unittest.TestCase, PeftLoraLoaderMixinTests):\n     pipeline_class = StableDiffusion3Pipeline\n     scheduler_cls = FlowMatchEulerDiscreteScheduler\n     scheduler_kwargs = {}\n-    scheduler_classes = [FlowMatchEulerDiscreteScheduler]\n     transformer_kwargs = {\n         \"sample_size\": 32,\n         \"patch_size\": 1,"
        },
        {
          "filename": "tests/lora/test_lora_layers_wan.py",
          "status": "modified",
          "additions": 0,
          "deletions": 1,
          "changes": 1,
          "patch": "@@ -42,7 +42,6 @@\n class WanLoRATests(unittest.TestCase, PeftLoraLoaderMixinTests):\n     pipeline_class = WanPipeline\n     scheduler_cls = FlowMatchEulerDiscreteScheduler\n-    scheduler_classes = [FlowMatchEulerDiscreteScheduler]\n     scheduler_kwargs = {}\n \n     transformer_kwargs = {"
        },
        {
          "filename": "tests/lora/test_lora_layers_wanvace.py",
          "status": "modified",
          "additions": 1,
          "deletions": 3,
          "changes": 4,
          "patch": "@@ -50,7 +50,6 @@\n class WanVACELoRATests(unittest.TestCase, PeftLoraLoaderMixinTests):\n     pipeline_class = WanVACEPipeline\n     scheduler_cls = FlowMatchEulerDiscreteScheduler\n-    scheduler_classes = [FlowMatchEulerDiscreteScheduler]\n     scheduler_kwargs = {}\n \n     transformer_kwargs = {\n@@ -165,9 +164,8 @@ def test_layerwise_casting_inference_denoiser(self):\n \n     @require_peft_version_greater(\"0.13.2\")\n     def test_lora_exclude_modules_wanvace(self):\n-        scheduler_cls = self.scheduler_classes[0]\n         exclude_module_name = \"vace_blocks.0.proj_out\"\n-        components, text_lora_config, denoiser_lora_config = self.get_dummy_components(scheduler_cls)\n+        components, text_lora_config, denoiser_lora_config = self.get_dummy_components()\n         pipe = self.pipeline_class(**components).to(torch_device)\n         _, _, inputs = self.get_dummy_inputs(with_generator=False)\n "
        },
        {
          "filename": "tests/lora/utils.py",
          "status": "modified",
          "additions": 995,
          "deletions": 1074,
          "changes": 2069,
          "patch": ""
        }
      ],
      "num_files": 14,
      "scraped_at": "2025-11-16T21:19:25.973648"
    },
    {
      "pr_number": 12301,
      "title": "Support ControlNet-Inpainting for Qwen-Image",
      "body": "# What does this PR do?\r\n\r\nAdd ControlNet-Inpainting ([InstantX/Qwen-Image-ControlNet-Inpainting](https://huggingface.co/InstantX/Qwen-Image-ControlNet-Inpainting)) support for Qwen-Image. The checkpoint will be set public soon.\r\n\r\n## Inference\r\n```\r\nimport torch\r\nfrom diffusers.utils import load_image\r\n\r\n# pip install git+https://github.com/huggingface/diffusers\r\nfrom diffusers import QwenImageControlNetModel, QwenImageControlNetInpaintPipeline\r\n\r\nbase_model = \"Qwen/Qwen-Image\"\r\ncontrolnet_model = \"InstantX/Qwen-Image-ControlNet-Inpainting\"\r\n\r\ncontrolnet = QwenImageControlNetModel.from_pretrained(controlnet_model, torch_dtype=torch.bfloat16)\r\n\r\npipe = QwenImageControlNetInpaintPipeline.from_pretrained(\r\n    base_model, controlnet=controlnet, torch_dtype=torch.bfloat16\r\n)\r\npipe.to(\"cuda\")\r\n\r\nimage = load_image(\"https://huggingface.co/InstantX/Qwen-Image-ControlNet-Inpainting/resolve/main/assets/images/image1.png\")\r\nmask_image = load_image(\"https://huggingface.co/InstantX/Qwen-Image-ControlNet-Inpainting/resolve/main/assets/masks/mask1.png\")\r\nprompt = \"\u4e00\u8f86\u7eff\u8272\u7684\u51fa\u79df\u8f66\u884c\u9a76\u5728\u8def\u4e0a\"\r\n\r\nimage = pipe(\r\n    prompt=prompt,\r\n    negative_prompt=\" \",\r\n    control_image=image,\r\n    control_mask=mask_image,\r\n    controlnet_conditioning_scale=controlnet_conditioning_scale,\r\n    width=control_image.size[0],\r\n    height=control_image.size[1],\r\n    num_inference_steps=30,\r\n    true_cfg_scale=4.0,\r\n    generator=torch.Generator(device=\"cuda\").manual_seed(42),\r\n).images[0]\r\nimage.save(f\"qwenimage_cn_inpaint_result.png\")\r\n```\r\n",
      "html_url": "https://github.com/huggingface/diffusers/pull/12301",
      "created_at": "2025-09-08T13:00:37Z",
      "merged_at": "2025-09-09T00:59:27Z",
      "merge_commit_sha": "4e36bb0d23a0450079560ac12d2858e2eb3f7e24",
      "base_ref": "main",
      "head_sha": "37ed5b74a42f698de36e6e523622263535158580",
      "user": "haofanwang",
      "files": [
        {
          "filename": "src/diffusers/__init__.py",
          "status": "modified",
          "additions": 2,
          "deletions": 0,
          "changes": 2,
          "patch": "@@ -510,6 +510,7 @@\n             \"PixArtAlphaPipeline\",\n             \"PixArtSigmaPAGPipeline\",\n             \"PixArtSigmaPipeline\",\n+            \"QwenImageControlNetInpaintPipeline\",\n             \"QwenImageControlNetPipeline\",\n             \"QwenImageEditInpaintPipeline\",\n             \"QwenImageEditPipeline\",\n@@ -1163,6 +1164,7 @@\n             PixArtAlphaPipeline,\n             PixArtSigmaPAGPipeline,\n             PixArtSigmaPipeline,\n+            QwenImageControlNetInpaintPipeline,\n             QwenImageControlNetPipeline,\n             QwenImageEditInpaintPipeline,\n             QwenImageEditPipeline,"
        },
        {
          "filename": "src/diffusers/pipelines/__init__.py",
          "status": "modified",
          "additions": 2,
          "deletions": 0,
          "changes": 2,
          "patch": "@@ -394,6 +394,7 @@\n         \"QwenImageInpaintPipeline\",\n         \"QwenImageEditPipeline\",\n         \"QwenImageEditInpaintPipeline\",\n+        \"QwenImageControlNetInpaintPipeline\",\n         \"QwenImageControlNetPipeline\",\n     ]\n try:\n@@ -714,6 +715,7 @@\n         from .pia import PIAPipeline\n         from .pixart_alpha import PixArtAlphaPipeline, PixArtSigmaPipeline\n         from .qwenimage import (\n+            QwenImageControlNetInpaintPipeline,\n             QwenImageControlNetPipeline,\n             QwenImageEditInpaintPipeline,\n             QwenImageEditPipeline,"
        },
        {
          "filename": "src/diffusers/pipelines/qwenimage/__init__.py",
          "status": "modified",
          "additions": 2,
          "deletions": 0,
          "changes": 2,
          "patch": "@@ -25,6 +25,7 @@\n     _import_structure[\"modeling_qwenimage\"] = [\"ReduxImageEncoder\"]\n     _import_structure[\"pipeline_qwenimage\"] = [\"QwenImagePipeline\"]\n     _import_structure[\"pipeline_qwenimage_controlnet\"] = [\"QwenImageControlNetPipeline\"]\n+    _import_structure[\"pipeline_qwenimage_controlnet_inpaint\"] = [\"QwenImageControlNetInpaintPipeline\"]\n     _import_structure[\"pipeline_qwenimage_edit\"] = [\"QwenImageEditPipeline\"]\n     _import_structure[\"pipeline_qwenimage_edit_inpaint\"] = [\"QwenImageEditInpaintPipeline\"]\n     _import_structure[\"pipeline_qwenimage_img2img\"] = [\"QwenImageImg2ImgPipeline\"]\n@@ -39,6 +40,7 @@\n     else:\n         from .pipeline_qwenimage import QwenImagePipeline\n         from .pipeline_qwenimage_controlnet import QwenImageControlNetPipeline\n+        from .pipeline_qwenimage_controlnet_inpaint import QwenImageControlNetInpaintPipeline\n         from .pipeline_qwenimage_edit import QwenImageEditPipeline\n         from .pipeline_qwenimage_edit_inpaint import QwenImageEditInpaintPipeline\n         from .pipeline_qwenimage_img2img import QwenImageImg2ImgPipeline"
        },
        {
          "filename": "src/diffusers/pipelines/qwenimage/pipeline_qwenimage_controlnet_inpaint.py",
          "status": "added",
          "additions": 941,
          "deletions": 0,
          "changes": 941,
          "patch": "@@ -0,0 +1,941 @@\n+# Copyright 2025 Qwen-Image Team, The InstantX Team and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import inspect\n+from typing import Any, Callable, Dict, List, Optional, Union\n+\n+import numpy as np\n+import torch\n+from transformers import Qwen2_5_VLForConditionalGeneration, Qwen2Tokenizer\n+\n+from ...image_processor import PipelineImageInput, VaeImageProcessor\n+from ...loaders import QwenImageLoraLoaderMixin\n+from ...models import AutoencoderKLQwenImage, QwenImageTransformer2DModel\n+from ...models.controlnets.controlnet_qwenimage import QwenImageControlNetModel, QwenImageMultiControlNetModel\n+from ...schedulers import FlowMatchEulerDiscreteScheduler\n+from ...utils import is_torch_xla_available, logging, replace_example_docstring\n+from ...utils.torch_utils import randn_tensor\n+from ..pipeline_utils import DiffusionPipeline\n+from .pipeline_output import QwenImagePipelineOutput\n+\n+\n+if is_torch_xla_available():\n+    import torch_xla.core.xla_model as xm\n+\n+    XLA_AVAILABLE = True\n+else:\n+    XLA_AVAILABLE = False\n+\n+\n+logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n+\n+EXAMPLE_DOC_STRING = \"\"\"\n+    Examples:\n+        ```py\n+        >>> import torch\n+        >>> from diffusers.utils import load_image\n+        >>> from diffusers import QwenImageControlNetModel, QwenImageControlNetInpaintPipeline\n+\n+        >>> base_model_path = \"Qwen/Qwen-Image\"\n+        >>> controlnet_model_path = \"InstantX/Qwen-Image-ControlNet-Inpainting\"\n+        >>> controlnet = QwenImageControlNetModel.from_pretrained(controlnet_model_path, torch_dtype=torch.bfloat16)\n+        >>> pipe = QwenImageControlNetInpaintPipeline.from_pretrained(\n+        ...     base_model_path, controlnet=controlnet, torch_dtype=torch.bfloat16\n+        ... ).to(\"cuda\")\n+        >>> image = load_image(\n+        ...     \"https://huggingface.co/InstantX/Qwen-Image-ControlNet-Inpainting/resolve/main/assets/images/image1.png\"\n+        ... )\n+        >>> mask_image = load_image(\n+        ...     \"https://huggingface.co/InstantX/Qwen-Image-ControlNet-Inpainting/resolve/main/assets/masks/mask1.png\"\n+        ... )\n+        >>> prompt = \"\u4e00\u8f86\u7eff\u8272\u7684\u51fa\u79df\u8f66\u884c\u9a76\u5728\u8def\u4e0a\"\n+        >>> result = pipe(\n+        ...     prompt=prompt,\n+        ...     control_image=image,\n+        ...     control_mask=mask_image,\n+        ...     controlnet_conditioning_scale=1.0,\n+        ...     width=mask_image.size[0],\n+        ...     height=mask_image.size[1],\n+        ...     true_cfg_scale=4.0,\n+        ... ).images[0]\n+        >>> image.save(\"qwenimage_controlnet_inpaint.png\")\n+        ```\n+\"\"\"\n+\n+\n+# Coped from diffusers.pipelines.qwenimage.pipeline_qwenimage.calculate_shift\n+def calculate_shift(\n+    image_seq_len,\n+    base_seq_len: int = 256,\n+    max_seq_len: int = 4096,\n+    base_shift: float = 0.5,\n+    max_shift: float = 1.15,\n+):\n+    m = (max_shift - base_shift) / (max_seq_len - base_seq_len)\n+    b = base_shift - m * base_seq_len\n+    mu = image_seq_len * m + b\n+    return mu\n+\n+\n+# Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_img2img.retrieve_latents\n+def retrieve_latents(\n+    encoder_output: torch.Tensor, generator: Optional[torch.Generator] = None, sample_mode: str = \"sample\"\n+):\n+    if hasattr(encoder_output, \"latent_dist\") and sample_mode == \"sample\":\n+        return encoder_output.latent_dist.sample(generator)\n+    elif hasattr(encoder_output, \"latent_dist\") and sample_mode == \"argmax\":\n+        return encoder_output.latent_dist.mode()\n+    elif hasattr(encoder_output, \"latents\"):\n+        return encoder_output.latents\n+    else:\n+        raise AttributeError(\"Could not access latents of provided encoder_output\")\n+\n+\n+# Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.retrieve_timesteps\n+def retrieve_timesteps(\n+    scheduler,\n+    num_inference_steps: Optional[int] = None,\n+    device: Optional[Union[str, torch.device]] = None,\n+    timesteps: Optional[List[int]] = None,\n+    sigmas: Optional[List[float]] = None,\n+    **kwargs,\n+):\n+    r\"\"\"\n+    Calls the scheduler's `set_timesteps` method and retrieves timesteps from the scheduler after the call. Handles\n+    custom timesteps. Any kwargs will be supplied to `scheduler.set_timesteps`.\n+\n+    Args:\n+        scheduler (`SchedulerMixin`):\n+            The scheduler to get timesteps from.\n+        num_inference_steps (`int`):\n+            The number of diffusion steps used when generating samples with a pre-trained model. If used, `timesteps`\n+            must be `None`.\n+        device (`str` or `torch.device`, *optional*):\n+            The device to which the timesteps should be moved to. If `None`, the timesteps are not moved.\n+        timesteps (`List[int]`, *optional*):\n+            Custom timesteps used to override the timestep spacing strategy of the scheduler. If `timesteps` is passed,\n+            `num_inference_steps` and `sigmas` must be `None`.\n+        sigmas (`List[float]`, *optional*):\n+            Custom sigmas used to override the timestep spacing strategy of the scheduler. If `sigmas` is passed,\n+            `num_inference_steps` and `timesteps` must be `None`.\n+\n+    Returns:\n+        `Tuple[torch.Tensor, int]`: A tuple where the first element is the timestep schedule from the scheduler and the\n+        second element is the number of inference steps.\n+    \"\"\"\n+    if timesteps is not None and sigmas is not None:\n+        raise ValueError(\"Only one of `timesteps` or `sigmas` can be passed. Please choose one to set custom values\")\n+    if timesteps is not None:\n+        accepts_timesteps = \"timesteps\" in set(inspect.signature(scheduler.set_timesteps).parameters.keys())\n+        if not accepts_timesteps:\n+            raise ValueError(\n+                f\"The current scheduler class {scheduler.__class__}'s `set_timesteps` does not support custom\"\n+                f\" timestep schedules. Please check whether you are using the correct scheduler.\"\n+            )\n+        scheduler.set_timesteps(timesteps=timesteps, device=device, **kwargs)\n+        timesteps = scheduler.timesteps\n+        num_inference_steps = len(timesteps)\n+    elif sigmas is not None:\n+        accept_sigmas = \"sigmas\" in set(inspect.signature(scheduler.set_timesteps).parameters.keys())\n+        if not accept_sigmas:\n+            raise ValueError(\n+                f\"The current scheduler class {scheduler.__class__}'s `set_timesteps` does not support custom\"\n+                f\" sigmas schedules. Please check whether you are using the correct scheduler.\"\n+            )\n+        scheduler.set_timesteps(sigmas=sigmas, device=device, **kwargs)\n+        timesteps = scheduler.timesteps\n+        num_inference_steps = len(timesteps)\n+    else:\n+        scheduler.set_timesteps(num_inference_steps, device=device, **kwargs)\n+        timesteps = scheduler.timesteps\n+    return timesteps, num_inference_steps\n+\n+\n+class QwenImageControlNetInpaintPipeline(DiffusionPipeline, QwenImageLoraLoaderMixin):\n+    r\"\"\"\n+    The QwenImage pipeline for text-to-image generation.\n+\n+    Args:\n+        transformer ([`QwenImageTransformer2DModel`]):\n+            Conditional Transformer (MMDiT) architecture to denoise the encoded image latents.\n+        scheduler ([`FlowMatchEulerDiscreteScheduler`]):\n+            A scheduler to be used in combination with `transformer` to denoise the encoded image latents.\n+        vae ([`AutoencoderKL`]):\n+            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n+        text_encoder ([`Qwen2.5-VL-7B-Instruct`]):\n+            [Qwen2.5-VL-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct), specifically the\n+            [Qwen2.5-VL-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct) variant.\n+        tokenizer (`QwenTokenizer`):\n+            Tokenizer of class\n+            [CLIPTokenizer](https://huggingface.co/docs/transformers/en/model_doc/clip#transformers.CLIPTokenizer).\n+    \"\"\"\n+\n+    model_cpu_offload_seq = \"text_encoder->transformer->vae\"\n+    _callback_tensor_inputs = [\"latents\", \"prompt_embeds\"]\n+\n+    def __init__(\n+        self,\n+        scheduler: FlowMatchEulerDiscreteScheduler,\n+        vae: AutoencoderKLQwenImage,\n+        text_encoder: Qwen2_5_VLForConditionalGeneration,\n+        tokenizer: Qwen2Tokenizer,\n+        transformer: QwenImageTransformer2DModel,\n+        controlnet: QwenImageControlNetModel,\n+    ):\n+        super().__init__()\n+\n+        self.register_modules(\n+            vae=vae,\n+            text_encoder=text_encoder,\n+            tokenizer=tokenizer,\n+            transformer=transformer,\n+            scheduler=scheduler,\n+            controlnet=controlnet,\n+        )\n+        self.vae_scale_factor = 2 ** len(self.vae.temperal_downsample) if getattr(self, \"vae\", None) else 8\n+        # QwenImage latents are turned into 2x2 patches and packed. This means the latent width and height has to be divisible\n+        # by the patch size. So the vae scale factor is multiplied by the patch size to account for this\n+        self.image_processor = VaeImageProcessor(vae_scale_factor=self.vae_scale_factor * 2)\n+\n+        self.mask_processor = VaeImageProcessor(\n+            vae_scale_factor=self.vae_scale_factor * 2,\n+            do_resize=True,\n+            do_convert_grayscale=True,\n+            do_normalize=False,\n+            do_binarize=True,\n+        )\n+\n+        self.tokenizer_max_length = 1024\n+        self.prompt_template_encode = \"<|im_start|>system\\nDescribe the image by detailing the color, shape, size, texture, quantity, text, spatial relationships of the objects and background:<|im_end|>\\n<|im_start|>user\\n{}<|im_end|>\\n<|im_start|>assistant\\n\"\n+        self.prompt_template_encode_start_idx = 34\n+        self.default_sample_size = 128\n+\n+    # Coped from diffusers.pipelines.qwenimage.pipeline_qwenimage.extract_masked_hidden\n+    def _extract_masked_hidden(self, hidden_states: torch.Tensor, mask: torch.Tensor):\n+        bool_mask = mask.bool()\n+        valid_lengths = bool_mask.sum(dim=1)\n+        selected = hidden_states[bool_mask]\n+        split_result = torch.split(selected, valid_lengths.tolist(), dim=0)\n+\n+        return split_result\n+\n+    # Coped from diffusers.pipelines.qwenimage.pipeline_qwenimage.get_qwen_prompt_embeds\n+    def _get_qwen_prompt_embeds(\n+        self,\n+        prompt: Union[str, List[str]] = None,\n+        device: Optional[torch.device] = None,\n+        dtype: Optional[torch.dtype] = None,\n+    ):\n+        device = device or self._execution_device\n+        dtype = dtype or self.text_encoder.dtype\n+\n+        prompt = [prompt] if isinstance(prompt, str) else prompt\n+\n+        template = self.prompt_template_encode\n+        drop_idx = self.prompt_template_encode_start_idx\n+        txt = [template.format(e) for e in prompt]\n+        txt_tokens = self.tokenizer(\n+            txt, max_length=self.tokenizer_max_length + drop_idx, padding=True, truncation=True, return_tensors=\"pt\"\n+        ).to(self.device)\n+        encoder_hidden_states = self.text_encoder(\n+            input_ids=txt_tokens.input_ids,\n+            attention_mask=txt_tokens.attention_mask,\n+            output_hidden_states=True,\n+        )\n+        hidden_states = encoder_hidden_states.hidden_states[-1]\n+        split_hidden_states = self._extract_masked_hidden(hidden_states, txt_tokens.attention_mask)\n+        split_hidden_states = [e[drop_idx:] for e in split_hidden_states]\n+        attn_mask_list = [torch.ones(e.size(0), dtype=torch.long, device=e.device) for e in split_hidden_states]\n+        max_seq_len = max([e.size(0) for e in split_hidden_states])\n+        prompt_embeds = torch.stack(\n+            [torch.cat([u, u.new_zeros(max_seq_len - u.size(0), u.size(1))]) for u in split_hidden_states]\n+        )\n+        encoder_attention_mask = torch.stack(\n+            [torch.cat([u, u.new_zeros(max_seq_len - u.size(0))]) for u in attn_mask_list]\n+        )\n+\n+        prompt_embeds = prompt_embeds.to(dtype=dtype, device=device)\n+\n+        return prompt_embeds, encoder_attention_mask\n+\n+    # Coped from diffusers.pipelines.qwenimage.pipeline_qwenimage.encode_prompt\n+    def encode_prompt(\n+        self,\n+        prompt: Union[str, List[str]],\n+        device: Optional[torch.device] = None,\n+        num_images_per_prompt: int = 1,\n+        prompt_embeds: Optional[torch.Tensor] = None,\n+        prompt_embeds_mask: Optional[torch.Tensor] = None,\n+        max_sequence_length: int = 1024,\n+    ):\n+        r\"\"\"\n+        Args:\n+            prompt (`str` or `List[str]`, *optional*):\n+                prompt to be encoded\n+            device: (`torch.device`):\n+                torch device\n+            num_images_per_prompt (`int`):\n+                number of images that should be generated per prompt\n+            prompt_embeds (`torch.Tensor`, *optional*):\n+                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n+                provided, text embeddings will be generated from `prompt` input argument.\n+        \"\"\"\n+        device = device or self._execution_device\n+\n+        prompt = [prompt] if isinstance(prompt, str) else prompt\n+        batch_size = len(prompt) if prompt_embeds is None else prompt_embeds.shape[0]\n+\n+        if prompt_embeds is None:\n+            prompt_embeds, prompt_embeds_mask = self._get_qwen_prompt_embeds(prompt, device)\n+\n+        _, seq_len, _ = prompt_embeds.shape\n+        prompt_embeds = prompt_embeds.repeat(1, num_images_per_prompt, 1)\n+        prompt_embeds = prompt_embeds.view(batch_size * num_images_per_prompt, seq_len, -1)\n+        prompt_embeds_mask = prompt_embeds_mask.repeat(1, num_images_per_prompt, 1)\n+        prompt_embeds_mask = prompt_embeds_mask.view(batch_size * num_images_per_prompt, seq_len)\n+\n+        return prompt_embeds, prompt_embeds_mask\n+\n+    def check_inputs(\n+        self,\n+        prompt,\n+        height,\n+        width,\n+        negative_prompt=None,\n+        prompt_embeds=None,\n+        negative_prompt_embeds=None,\n+        prompt_embeds_mask=None,\n+        negative_prompt_embeds_mask=None,\n+        callback_on_step_end_tensor_inputs=None,\n+        max_sequence_length=None,\n+    ):\n+        if height % (self.vae_scale_factor * 2) != 0 or width % (self.vae_scale_factor * 2) != 0:\n+            logger.warning(\n+                f\"`height` and `width` have to be divisible by {self.vae_scale_factor * 2} but are {height} and {width}. Dimensions will be resized accordingly\"\n+            )\n+\n+        if callback_on_step_end_tensor_inputs is not None and not all(\n+            k in self._callback_tensor_inputs for k in callback_on_step_end_tensor_inputs\n+        ):\n+            raise ValueError(\n+                f\"`callback_on_step_end_tensor_inputs` has to be in {self._callback_tensor_inputs}, but found {[k for k in callback_on_step_end_tensor_inputs if k not in self._callback_tensor_inputs]}\"\n+            )\n+\n+        if prompt is not None and prompt_embeds is not None:\n+            raise ValueError(\n+                f\"Cannot forward both `prompt`: {prompt} and `prompt_embeds`: {prompt_embeds}. Please make sure to\"\n+                \" only forward one of the two.\"\n+            )\n+        elif prompt is None and prompt_embeds is None:\n+            raise ValueError(\n+                \"Provide either `prompt` or `prompt_embeds`. Cannot leave both `prompt` and `prompt_embeds` undefined.\"\n+            )\n+        elif prompt is not None and (not isinstance(prompt, str) and not isinstance(prompt, list)):\n+            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n+\n+        if negative_prompt is not None and negative_prompt_embeds is not None:\n+            raise ValueError(\n+                f\"Cannot forward both `negative_prompt`: {negative_prompt} and `negative_prompt_embeds`:\"\n+                f\" {negative_prompt_embeds}. Please make sure to only forward one of the two.\"\n+            )\n+\n+        if prompt_embeds is not None and prompt_embeds_mask is None:\n+            raise ValueError(\n+                \"If `prompt_embeds` are provided, `prompt_embeds_mask` also have to be passed. Make sure to generate `prompt_embeds_mask` from the same text encoder that was used to generate `prompt_embeds`.\"\n+            )\n+        if negative_prompt_embeds is not None and negative_prompt_embeds_mask is None:\n+            raise ValueError(\n+                \"If `negative_prompt_embeds` are provided, `negative_prompt_embeds_mask` also have to be passed. Make sure to generate `negative_prompt_embeds_mask` from the same text encoder that was used to generate `negative_prompt_embeds`.\"\n+            )\n+\n+        if max_sequence_length is not None and max_sequence_length > 1024:\n+            raise ValueError(f\"`max_sequence_length` cannot be greater than 1024 but is {max_sequence_length}\")\n+\n+    @staticmethod\n+    # Copied from diffusers.pipelines.qwenimage.pipeline_qwenimage.QwenImagePipeline._pack_latents\n+    def _pack_latents(latents, batch_size, num_channels_latents, height, width):\n+        latents = latents.view(batch_size, num_channels_latents, height // 2, 2, width // 2, 2)\n+        latents = latents.permute(0, 2, 4, 1, 3, 5)\n+        latents = latents.reshape(batch_size, (height // 2) * (width // 2), num_channels_latents * 4)\n+\n+        return latents\n+\n+    @staticmethod\n+    # Copied from diffusers.pipelines.qwenimage.pipeline_qwenimage.QwenImagePipeline._unpack_latents\n+    def _unpack_latents(latents, height, width, vae_scale_factor):\n+        batch_size, num_patches, channels = latents.shape\n+\n+        # VAE applies 8x compression on images but we must also account for packing which requires\n+        # latent height and width to be divisible by 2.\n+        height = 2 * (int(height) // (vae_scale_factor * 2))\n+        width = 2 * (int(width) // (vae_scale_factor * 2))\n+\n+        latents = latents.view(batch_size, height // 2, width // 2, channels // 4, 2, 2)\n+        latents = latents.permute(0, 3, 1, 4, 2, 5)\n+\n+        latents = latents.reshape(batch_size, channels // (2 * 2), 1, height, width)\n+\n+        return latents\n+\n+    def enable_vae_slicing(self):\n+        r\"\"\"\n+        Enable sliced VAE decoding. When this option is enabled, the VAE will split the input tensor in slices to\n+        compute decoding in several steps. This is useful to save some memory and allow larger batch sizes.\n+        \"\"\"\n+        self.vae.enable_slicing()\n+\n+    def disable_vae_slicing(self):\n+        r\"\"\"\n+        Disable sliced VAE decoding. If `enable_vae_slicing` was previously enabled, this method will go back to\n+        computing decoding in one step.\n+        \"\"\"\n+        self.vae.disable_slicing()\n+\n+    def enable_vae_tiling(self):\n+        r\"\"\"\n+        Enable tiled VAE decoding. When this option is enabled, the VAE will split the input tensor into tiles to\n+        compute decoding and encoding in several steps. This is useful for saving a large amount of memory and to allow\n+        processing larger images.\n+        \"\"\"\n+        self.vae.enable_tiling()\n+\n+    def disable_vae_tiling(self):\n+        r\"\"\"\n+        Disable tiled VAE decoding. If `enable_vae_tiling` was previously enabled, this method will go back to\n+        computing decoding in one step.\n+        \"\"\"\n+        self.vae.disable_tiling()\n+\n+    # Copied from diffusers.pipelines.qwenimage.pipeline_qwenimage.QwenImagePipeline.prepare_latents\n+    def prepare_latents(\n+        self,\n+        batch_size,\n+        num_channels_latents,\n+        height,\n+        width,\n+        dtype,\n+        device,\n+        generator,\n+        latents=None,\n+    ):\n+        # VAE applies 8x compression on images but we must also account for packing which requires\n+        # latent height and width to be divisible by 2.\n+        height = 2 * (int(height) // (self.vae_scale_factor * 2))\n+        width = 2 * (int(width) // (self.vae_scale_factor * 2))\n+\n+        shape = (batch_size, 1, num_channels_latents, height, width)\n+\n+        if latents is not None:\n+            return latents.to(device=device, dtype=dtype)\n+\n+        if isinstance(generator, list) and len(generator) != batch_size:\n+            raise ValueError(\n+                f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n+                f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n+            )\n+\n+        latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n+        latents = self._pack_latents(latents, batch_size, num_channels_latents, height, width)\n+\n+        return latents\n+\n+    # Copied from diffusers.pipelines.controlnet_sd3.pipeline_stable_diffusion_3_controlnet.StableDiffusion3ControlNetPipeline.prepare_image\n+    def prepare_image(\n+        self,\n+        image,\n+        width,\n+        height,\n+        batch_size,\n+        num_images_per_prompt,\n+        device,\n+        dtype,\n+        do_classifier_free_guidance=False,\n+        guess_mode=False,\n+    ):\n+        if isinstance(image, torch.Tensor):\n+            pass\n+        else:\n+            image = self.image_processor.preprocess(image, height=height, width=width)\n+\n+        image_batch_size = image.shape[0]\n+\n+        if image_batch_size == 1:\n+            repeat_by = batch_size\n+        else:\n+            # image batch size is the same as prompt batch size\n+            repeat_by = num_images_per_prompt\n+\n+        image = image.repeat_interleave(repeat_by, dim=0)\n+\n+        image = image.to(device=device, dtype=dtype)\n+\n+        if do_classifier_free_guidance and not guess_mode:\n+            image = torch.cat([image] * 2)\n+\n+        return image\n+\n+    def prepare_image_with_mask(\n+        self,\n+        image,\n+        mask,\n+        width,\n+        height,\n+        batch_size,\n+        num_images_per_prompt,\n+        device,\n+        dtype,\n+        do_classifier_free_guidance=False,\n+        guess_mode=False,\n+    ):\n+        if isinstance(image, torch.Tensor):\n+            pass\n+        else:\n+            image = self.image_processor.preprocess(image, height=height, width=width)\n+\n+        image_batch_size = image.shape[0]\n+\n+        if image_batch_size == 1:\n+            repeat_by = batch_size\n+        else:\n+            # image batch size is the same as prompt batch size\n+            repeat_by = num_images_per_prompt\n+\n+        image = image.repeat_interleave(repeat_by, dim=0)\n+        image = image.to(device=device, dtype=dtype)  # (bsz, 3, height_ori, width_ori)\n+\n+        # Prepare mask\n+        if isinstance(mask, torch.Tensor):\n+            pass\n+        else:\n+            mask = self.mask_processor.preprocess(mask, height=height, width=width)\n+        mask = mask.repeat_interleave(repeat_by, dim=0)\n+        mask = mask.to(device=device, dtype=dtype)  # (bsz, 1, height_ori, width_ori)\n+\n+        if image.ndim == 4:\n+            image = image.unsqueeze(2)\n+\n+        if mask.ndim == 4:\n+            mask = mask.unsqueeze(2)\n+\n+        # Get masked image\n+        masked_image = image.clone()\n+        masked_image[(mask > 0.5).repeat(1, 3, 1, 1, 1)] = -1  # (bsz, 3, 1, height_ori, width_ori)\n+\n+        self.vae_scale_factor = 2 ** len(self.vae.temperal_downsample)\n+        latents_mean = (torch.tensor(self.vae.config.latents_mean).view(1, self.vae.config.z_dim, 1, 1, 1)).to(device)\n+        latents_std = 1.0 / torch.tensor(self.vae.config.latents_std).view(1, self.vae.config.z_dim, 1, 1, 1).to(\n+            device\n+        )\n+\n+        # Encode to latents\n+        image_latents = self.vae.encode(masked_image.to(self.vae.dtype)).latent_dist.sample()\n+        image_latents = (image_latents - latents_mean) * latents_std\n+        image_latents = image_latents.to(dtype)  # torch.Size([1, 16, 1, height_ori//8, width_ori//8])\n+\n+        mask = torch.nn.functional.interpolate(\n+            mask, size=(image_latents.shape[-3], image_latents.shape[-2], image_latents.shape[-1])\n+        )\n+        mask = 1 - mask  # torch.Size([1, 1, 1, height_ori//8, width_ori//8])\n+\n+        control_image = torch.cat(\n+            [image_latents, mask], dim=1\n+        )  # torch.Size([1, 16+1, 1, height_ori//8, width_ori//8])\n+\n+        control_image = control_image.permute(0, 2, 1, 3, 4)  # torch.Size([1, 1, 16+1, height_ori//8, width_ori//8])\n+\n+        # pack\n+        control_image = self._pack_latents(\n+            control_image,\n+            batch_size=control_image.shape[0],\n+            num_channels_latents=control_image.shape[2],\n+            height=control_image.shape[3],\n+            width=control_image.shape[4],\n+        )\n+\n+        if do_classifier_free_guidance and not guess_mode:\n+            control_image = torch.cat([control_image] * 2)\n+\n+        return control_image\n+\n+    @property\n+    def guidance_scale(self):\n+        return self._guidance_scale\n+\n+    @property\n+    def attention_kwargs(self):\n+        return self._attention_kwargs\n+\n+    @property\n+    def num_timesteps(self):\n+        return self._num_timesteps\n+\n+    @property\n+    def current_timestep(self):\n+        return self._current_timestep\n+\n+    @property\n+    def interrupt(self):\n+        return self._interrupt\n+\n+    @torch.no_grad()\n+    @replace_example_docstring(EXAMPLE_DOC_STRING)\n+    def __call__(\n+        self,\n+        prompt: Union[str, List[str]] = None,\n+        negative_prompt: Union[str, List[str]] = None,\n+        true_cfg_scale: float = 4.0,\n+        height: Optional[int] = None,\n+        width: Optional[int] = None,\n+        num_inference_steps: int = 50,\n+        sigmas: Optional[List[float]] = None,\n+        guidance_scale: float = 1.0,\n+        control_guidance_start: Union[float, List[float]] = 0.0,\n+        control_guidance_end: Union[float, List[float]] = 1.0,\n+        control_image: PipelineImageInput = None,\n+        control_mask: PipelineImageInput = None,\n+        controlnet_conditioning_scale: Union[float, List[float]] = 1.0,\n+        num_images_per_prompt: int = 1,\n+        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n+        latents: Optional[torch.Tensor] = None,\n+        prompt_embeds: Optional[torch.Tensor] = None,\n+        prompt_embeds_mask: Optional[torch.Tensor] = None,\n+        negative_prompt_embeds: Optional[torch.Tensor] = None,\n+        negative_prompt_embeds_mask: Optional[torch.Tensor] = None,\n+        output_type: Optional[str] = \"pil\",\n+        return_dict: bool = True,\n+        attention_kwargs: Optional[Dict[str, Any]] = None,\n+        callback_on_step_end: Optional[Callable[[int, int, Dict], None]] = None,\n+        callback_on_step_end_tensor_inputs: List[str] = [\"latents\"],\n+        max_sequence_length: int = 512,\n+    ):\n+        r\"\"\"\n+        Function invoked when calling the pipeline for generation.\n+\n+        Args:\n+            prompt (`str` or `List[str]`, *optional*):\n+                The prompt or prompts to guide the image generation. If not defined, one has to pass `prompt_embeds`.\n+                instead.\n+            negative_prompt (`str` or `List[str]`, *optional*):\n+                The prompt or prompts not to guide the image generation. If not defined, one has to pass\n+                `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `true_cfg_scale` is\n+                not greater than `1`).\n+            true_cfg_scale (`float`, *optional*, defaults to 1.0):\n+                When > 1.0 and a provided `negative_prompt`, enables true classifier-free guidance.\n+            height (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n+                The height in pixels of the generated image. This is set to 1024 by default for the best results.\n+            width (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n+                The width in pixels of the generated image. This is set to 1024 by default for the best results.\n+            num_inference_steps (`int`, *optional*, defaults to 50):\n+                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n+                expense of slower inference.\n+            sigmas (`List[float]`, *optional*):\n+                Custom sigmas to use for the denoising process with schedulers which support a `sigmas` argument in\n+                their `set_timesteps` method. If not defined, the default behavior when `num_inference_steps` is passed\n+                will be used.\n+            guidance_scale (`float`, *optional*, defaults to 3.5):\n+                Guidance scale as defined in [Classifier-Free Diffusion\n+                Guidance](https://huggingface.co/papers/2207.12598). `guidance_scale` is defined as `w` of equation 2.\n+                of [Imagen Paper](https://huggingface.co/papers/2205.11487). Guidance scale is enabled by setting\n+                `guidance_scale > 1`. Higher guidance scale encourages to generate images that are closely linked to\n+                the text `prompt`, usually at the expense of lower image quality.\n+            num_images_per_prompt (`int`, *optional*, defaults to 1):\n+                The number of images to generate per prompt.\n+            generator (`torch.Generator` or `List[torch.Generator]`, *optional*):\n+                One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)\n+                to make generation deterministic.\n+            latents (`torch.Tensor`, *optional*):\n+                Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for image\n+                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents\n+                tensor will be generated by sampling using the supplied random `generator`.\n+            prompt_embeds (`torch.Tensor`, *optional*):\n+                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n+                provided, text embeddings will be generated from `prompt` input argument.\n+            negative_prompt_embeds (`torch.Tensor`, *optional*):\n+                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt\n+                weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt` input\n+                argument.\n+            output_type (`str`, *optional*, defaults to `\"pil\"`):\n+                The output format of the generate image. Choose between\n+                [PIL](https://pillow.readthedocs.io/en/stable/): `PIL.Image.Image` or `np.array`.\n+            return_dict (`bool`, *optional*, defaults to `True`):\n+                Whether or not to return a [`~pipelines.qwenimage.QwenImagePipelineOutput`] instead of a plain tuple.\n+            attention_kwargs (`dict`, *optional*):\n+                A kwargs dictionary that if specified is passed along to the `AttentionProcessor` as defined under\n+                `self.processor` in\n+                [diffusers.models.attention_processor](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).\n+            callback_on_step_end (`Callable`, *optional*):\n+                A function that calls at the end of each denoising steps during the inference. The function is called\n+                with the following arguments: `callback_on_step_end(self: DiffusionPipeline, step: int, timestep: int,\n+                callback_kwargs: Dict)`. `callback_kwargs` will include a list of all tensors as specified by\n+                `callback_on_step_end_tensor_inputs`.\n+            callback_on_step_end_tensor_inputs (`List`, *optional*):\n+                The list of tensor inputs for the `callback_on_step_end` function. The tensors specified in the list\n+                will be passed as `callback_kwargs` argument. You will only be able to include variables listed in the\n+                `._callback_tensor_inputs` attribute of your pipeline class.\n+            max_sequence_length (`int` defaults to 512): Maximum sequence length to use with the `prompt`.\n+\n+        Examples:\n+\n+        Returns:\n+            [`~pipelines.qwenimage.QwenImagePipelineOutput`] or `tuple`:\n+            [`~pipelines.qwenimage.QwenImagePipelineOutput`] if `return_dict` is True, otherwise a `tuple`. When\n+            returning a tuple, the first element is a list with the generated images.\n+        \"\"\"\n+\n+        height = height or self.default_sample_size * self.vae_scale_factor\n+        width = width or self.default_sample_size * self.vae_scale_factor\n+\n+        if not isinstance(control_guidance_start, list) and isinstance(control_guidance_end, list):\n+            control_guidance_start = len(control_guidance_end) * [control_guidance_start]\n+        elif not isinstance(control_guidance_end, list) and isinstance(control_guidance_start, list):\n+            control_guidance_end = len(control_guidance_start) * [control_guidance_end]\n+        elif not isinstance(control_guidance_start, list) and not isinstance(control_guidance_end, list):\n+            mult = len(control_image) if isinstance(self.controlnet, QwenImageMultiControlNetModel) else 1\n+            control_guidance_start, control_guidance_end = (\n+                mult * [control_guidance_start],\n+                mult * [control_guidance_end],\n+            )\n+\n+        # 1. Check inputs. Raise error if not correct\n+        self.check_inputs(\n+            prompt,\n+            height,\n+            width,\n+            negative_prompt=negative_prompt,\n+            prompt_embeds=prompt_embeds,\n+            negative_prompt_embeds=negative_prompt_embeds,\n+            prompt_embeds_mask=prompt_embeds_mask,\n+            negative_prompt_embeds_mask=negative_prompt_embeds_mask,\n+            callback_on_step_end_tensor_inputs=callback_on_step_end_tensor_inputs,\n+            max_sequence_length=max_sequence_length,\n+        )\n+\n+        self._guidance_scale = guidance_scale\n+        self._attention_kwargs = attention_kwargs\n+        self._current_timestep = None\n+        self._interrupt = False\n+\n+        # 2. Define call parameters\n+        if prompt is not None and isinstance(prompt, str):\n+            batch_size = 1\n+        elif prompt is not None and isinstance(prompt, list):\n+            batch_size = len(prompt)\n+        else:\n+            batch_size = prompt_embeds.shape[0]\n+\n+        device = self._execution_device\n+\n+        has_neg_prompt = negative_prompt is not None or (\n+            negative_prompt_embeds is not None and negative_prompt_embeds_mask is not None\n+        )\n+        do_true_cfg = true_cfg_scale > 1 and has_neg_prompt\n+        prompt_embeds, prompt_embeds_mask = self.encode_prompt(\n+            prompt=prompt,\n+            prompt_embeds=prompt_embeds,\n+            prompt_embeds_mask=prompt_embeds_mask,\n+            device=device,\n+            num_images_per_prompt=num_images_per_prompt,\n+            max_sequence_length=max_sequence_length,\n+        )\n+        if do_true_cfg:\n+            negative_prompt_embeds, negative_prompt_embeds_mask = self.encode_prompt(\n+                prompt=negative_prompt,\n+                prompt_embeds=negative_prompt_embeds,\n+                prompt_embeds_mask=negative_prompt_embeds_mask,\n+                device=device,\n+                num_images_per_prompt=num_images_per_prompt,\n+                max_sequence_length=max_sequence_length,\n+            )\n+\n+        # 3. Prepare control image\n+        num_channels_latents = self.transformer.config.in_channels // 4\n+        if isinstance(self.controlnet, QwenImageControlNetModel):\n+            control_image = self.prepare_image_with_mask(\n+                image=control_image,\n+                mask=control_mask,\n+                width=width,\n+                height=height,\n+                batch_size=batch_size * num_images_per_prompt,\n+                num_images_per_prompt=num_images_per_prompt,\n+                device=device,\n+                dtype=self.vae.dtype,\n+            )\n+\n+        # 4. Prepare latent variables\n+        num_channels_latents = self.transformer.config.in_channels // 4\n+        latents = self.prepare_latents(\n+            batch_size * num_images_per_prompt,\n+            num_channels_latents,\n+            height,\n+            width,\n+            prompt_embeds.dtype,\n+            device,\n+            generator,\n+            latents,\n+        )\n+        img_shapes = [(1, height // self.vae_scale_factor // 2, width // self.vae_scale_factor // 2)] * batch_size\n+\n+        # 5. Prepare timesteps\n+        sigmas = np.linspace(1.0, 1 / num_inference_steps, num_inference_steps) if sigmas is None else sigmas\n+        image_seq_len = latents.shape[1]\n+        mu = calculate_shift(\n+            image_seq_len,\n+            self.scheduler.config.get(\"base_image_seq_len\", 256),\n+            self.scheduler.config.get(\"max_image_seq_len\", 4096),\n+            self.scheduler.config.get(\"base_shift\", 0.5),\n+            self.scheduler.config.get(\"max_shift\", 1.15),\n+        )\n+        timesteps, num_inference_steps = retrieve_timesteps(\n+            self.scheduler,\n+            num_inference_steps,\n+            device,\n+            sigmas=sigmas,\n+            mu=mu,\n+        )\n+        num_warmup_steps = max(len(timesteps) - num_inference_steps * self.scheduler.order, 0)\n+        self._num_timesteps = len(timesteps)\n+\n+        controlnet_keep = []\n+        for i in range(len(timesteps)):\n+            keeps = [\n+                1.0 - float(i / len(timesteps) < s or (i + 1) / len(timesteps) > e)\n+                for s, e in zip(control_guidance_start, control_guidance_end)\n+            ]\n+            controlnet_keep.append(keeps[0] if isinstance(self.controlnet, QwenImageControlNetModel) else keeps)\n+\n+        # handle guidance\n+        if self.transformer.config.guidance_embeds:\n+            guidance = torch.full([1], guidance_scale, device=device, dtype=torch.float32)\n+            guidance = guidance.expand(latents.shape[0])\n+        else:\n+            guidance = None\n+\n+        if self.attention_kwargs is None:\n+            self._attention_kwargs = {}\n+\n+        # 6. Denoising loop\n+        self.scheduler.set_begin_index(0)\n+        with self.progress_bar(total=num_inference_steps) as progress_bar:\n+            for i, t in enumerate(timesteps):\n+                if self.interrupt:\n+                    continue\n+\n+                self._current_timestep = t\n+                # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n+                timestep = t.expand(latents.shape[0]).to(latents.dtype)\n+\n+                if isinstance(controlnet_keep[i], list):\n+                    cond_scale = [c * s for c, s in zip(controlnet_conditioning_scale, controlnet_keep[i])]\n+                else:\n+                    controlnet_cond_scale = controlnet_conditioning_scale\n+                    if isinstance(controlnet_cond_scale, list):\n+                        controlnet_cond_scale = controlnet_cond_scale[0]\n+                    cond_scale = controlnet_cond_scale * controlnet_keep[i]\n+\n+                # controlnet\n+                controlnet_block_samples = self.controlnet(\n+                    hidden_states=latents,\n+                    controlnet_cond=control_image.to(dtype=latents.dtype, device=device),\n+                    conditioning_scale=cond_scale,\n+                    timestep=timestep / 1000,\n+                    encoder_hidden_states=prompt_embeds,\n+                    encoder_hidden_states_mask=prompt_embeds_mask,\n+                    img_shapes=img_shapes,\n+                    txt_seq_lens=prompt_embeds_mask.sum(dim=1).tolist(),\n+                    return_dict=False,\n+                )\n+\n+                with self.transformer.cache_context(\"cond\"):\n+                    noise_pred = self.transformer(\n+                        hidden_states=latents,\n+                        timestep=timestep / 1000,\n+                        encoder_hidden_states=prompt_embeds,\n+                        encoder_hidden_states_mask=prompt_embeds_mask,\n+                        img_shapes=img_shapes,\n+                        txt_seq_lens=prompt_embeds_mask.sum(dim=1).tolist(),\n+                        controlnet_block_samples=controlnet_block_samples,\n+                        attention_kwargs=self.attention_kwargs,\n+                        return_dict=False,\n+                    )[0]\n+\n+                if do_true_cfg:\n+                    with self.transformer.cache_context(\"uncond\"):\n+                        neg_noise_pred = self.transformer(\n+                            hidden_states=latents,\n+                            timestep=timestep / 1000,\n+                            guidance=guidance,\n+                            encoder_hidden_states_mask=negative_prompt_embeds_mask,\n+                            encoder_hidden_states=negative_prompt_embeds,\n+                            img_shapes=img_shapes,\n+                            txt_seq_lens=negative_prompt_embeds_mask.sum(dim=1).tolist(),\n+                            controlnet_block_samples=controlnet_block_samples,\n+                            attention_kwargs=self.attention_kwargs,\n+                            return_dict=False,\n+                        )[0]\n+                    comb_pred = neg_noise_pred + true_cfg_scale * (noise_pred - neg_noise_pred)\n+\n+                    cond_norm = torch.norm(noise_pred, dim=-1, keepdim=True)\n+                    noise_norm = torch.norm(comb_pred, dim=-1, keepdim=True)\n+                    noise_pred = comb_pred * (cond_norm / noise_norm)\n+\n+                # compute the previous noisy sample x_t -> x_t-1\n+                latents_dtype = latents.dtype\n+                latents = self.scheduler.step(noise_pred, t, latents, return_dict=False)[0]\n+\n+                if latents.dtype != latents_dtype:\n+                    if torch.backends.mps.is_available():\n+                        # some platforms (eg. apple mps) misbehave due to a pytorch bug: https://github.com/pytorch/pytorch/pull/99272\n+                        latents = latents.to(latents_dtype)\n+\n+                if callback_on_step_end is not None:\n+                    callback_kwargs = {}\n+                    for k in callback_on_step_end_tensor_inputs:\n+                        callback_kwargs[k] = locals()[k]\n+                    callback_outputs = callback_on_step_end(self, i, t, callback_kwargs)\n+\n+                    latents = callback_outputs.pop(\"latents\", latents)\n+                    prompt_embeds = callback_outputs.pop(\"prompt_embeds\", prompt_embeds)\n+\n+                # call the callback, if provided\n+                if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n+                    progress_bar.update()\n+\n+                if XLA_AVAILABLE:\n+                    xm.mark_step()\n+\n+        self._current_timestep = None\n+        if output_type == \"latent\":\n+            image = latents\n+        else:\n+            latents = self._unpack_latents(latents, height, width, self.vae_scale_factor)\n+            latents = latents.to(self.vae.dtype)\n+            latents_mean = (\n+                torch.tensor(self.vae.config.latents_mean)\n+                .view(1, self.vae.config.z_dim, 1, 1, 1)\n+                .to(latents.device, latents.dtype)\n+            )\n+            latents_std = 1.0 / torch.tensor(self.vae.config.latents_std).view(1, self.vae.config.z_dim, 1, 1, 1).to(\n+                latents.device, latents.dtype\n+            )\n+            latents = latents / latents_std + latents_mean\n+            image = self.vae.decode(latents, return_dict=False)[0][:, :, 0]\n+            image = self.image_processor.postprocess(image, output_type=output_type)\n+\n+        # Offload all models\n+        self.maybe_free_model_hooks()\n+\n+        if not return_dict:\n+            return (image,)\n+\n+        return QwenImagePipelineOutput(images=image)"
        },
        {
          "filename": "src/diffusers/utils/dummy_torch_and_transformers_objects.py",
          "status": "modified",
          "additions": 15,
          "deletions": 0,
          "changes": 15,
          "patch": "@@ -1817,6 +1817,21 @@ def from_pretrained(cls, *args, **kwargs):\n         requires_backends(cls, [\"torch\", \"transformers\"])\n \n \n+class QwenImageControlNetInpaintPipeline(metaclass=DummyObject):\n+    _backends = [\"torch\", \"transformers\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_config(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_pretrained(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+\n class QwenImageControlNetPipeline(metaclass=DummyObject):\n     _backends = [\"torch\", \"transformers\"]\n "
        }
      ],
      "num_files": 5,
      "scraped_at": "2025-11-16T21:19:28.367351"
    },
    {
      "pr_number": 12289,
      "title": "Fix many type hint errors",
      "body": "I'm the maintainer of [cache-dit](https://github.com/vipshop/cache-dit). cache-dit relies on type annotations to match the forward pattern of blocks more precisely. However, I found that there are a large number of type annotation errors in diffusers. For instance, HiDream, HunyuanVideo, cogvideox, auraflow, cogview, lumina, etc. This PR has fixed all related errors.\r\n\r\nPTAL ~ @a-r-r-o-w @sayakpaul \r\n\r\nfor example, fix hidream type hint:\r\n\r\n```python\r\ntorch.Tensor -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]\r\n```",
      "html_url": "https://github.com/huggingface/diffusers/pull/12289",
      "created_at": "2025-09-05T05:39:13Z",
      "merged_at": "2025-09-17T04:52:15Z",
      "merge_commit_sha": "efb7a299af46d739dec6a57a5d2814165fba24b5",
      "base_ref": "main",
      "head_sha": "5a2f6f7880b5c54afbd89b080e7197e9ea8011ce",
      "user": "DefTruth",
      "files": [
        {
          "filename": "src/diffusers/models/attention.py",
          "status": "modified",
          "additions": 1,
          "deletions": 1,
          "changes": 2,
          "patch": "@@ -674,7 +674,7 @@ def forward(\n         encoder_hidden_states: torch.FloatTensor,\n         temb: torch.FloatTensor,\n         joint_attention_kwargs: Optional[Dict[str, Any]] = None,\n-    ):\n+    ) -> Tuple[torch.Tensor, torch.Tensor]:\n         joint_attention_kwargs = joint_attention_kwargs or {}\n         if self.use_dual_attention:\n             norm_hidden_states, gate_msa, shift_mlp, scale_mlp, gate_mlp, norm_hidden_states2, gate_msa2 = self.norm1("
        },
        {
          "filename": "src/diffusers/models/transformers/auraflow_transformer_2d.py",
          "status": "modified",
          "additions": 5,
          "deletions": 5,
          "changes": 10,
          "patch": "@@ -13,7 +13,7 @@\n # limitations under the License.\n \n \n-from typing import Any, Dict, Optional, Union\n+from typing import Any, Dict, Optional, Tuple, Union\n \n import torch\n import torch.nn as nn\n@@ -92,7 +92,7 @@ def pe_selection_index_based_on_dim(self, h, w):\n \n         return selected_indices\n \n-    def forward(self, latent):\n+    def forward(self, latent) -> torch.Tensor:\n         batch_size, num_channels, height, width = latent.size()\n         latent = latent.view(\n             batch_size,\n@@ -173,7 +173,7 @@ def forward(\n         hidden_states: torch.FloatTensor,\n         temb: torch.FloatTensor,\n         attention_kwargs: Optional[Dict[str, Any]] = None,\n-    ):\n+    ) -> torch.Tensor:\n         residual = hidden_states\n         attention_kwargs = attention_kwargs or {}\n \n@@ -242,7 +242,7 @@ def forward(\n         encoder_hidden_states: torch.FloatTensor,\n         temb: torch.FloatTensor,\n         attention_kwargs: Optional[Dict[str, Any]] = None,\n-    ):\n+    ) -> Tuple[torch.Tensor, torch.Tensor]:\n         residual = hidden_states\n         residual_context = encoder_hidden_states\n         attention_kwargs = attention_kwargs or {}\n@@ -472,7 +472,7 @@ def forward(\n         timestep: torch.LongTensor = None,\n         attention_kwargs: Optional[Dict[str, Any]] = None,\n         return_dict: bool = True,\n-    ) -> Union[torch.FloatTensor, Transformer2DModelOutput]:\n+    ) -> Union[Tuple[torch.Tensor], Transformer2DModelOutput]:\n         if attention_kwargs is not None:\n             attention_kwargs = attention_kwargs.copy()\n             lora_scale = attention_kwargs.pop(\"scale\", 1.0)"
        },
        {
          "filename": "src/diffusers/models/transformers/cogvideox_transformer_3d.py",
          "status": "modified",
          "additions": 2,
          "deletions": 2,
          "changes": 4,
          "patch": "@@ -122,7 +122,7 @@ def forward(\n         temb: torch.Tensor,\n         image_rotary_emb: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n         attention_kwargs: Optional[Dict[str, Any]] = None,\n-    ) -> torch.Tensor:\n+    ) -> Tuple[torch.Tensor, torch.Tensor]:\n         text_seq_length = encoder_hidden_states.size(1)\n         attention_kwargs = attention_kwargs or {}\n \n@@ -441,7 +441,7 @@ def forward(\n         image_rotary_emb: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n         attention_kwargs: Optional[Dict[str, Any]] = None,\n         return_dict: bool = True,\n-    ):\n+    ) -> Union[Tuple[torch.Tensor], Transformer2DModelOutput]:\n         if attention_kwargs is not None:\n             attention_kwargs = attention_kwargs.copy()\n             lora_scale = attention_kwargs.pop(\"scale\", 1.0)"
        },
        {
          "filename": "src/diffusers/models/transformers/consisid_transformer_3d.py",
          "status": "modified",
          "additions": 2,
          "deletions": 2,
          "changes": 4,
          "patch": "@@ -315,7 +315,7 @@ def forward(\n         encoder_hidden_states: torch.Tensor,\n         temb: torch.Tensor,\n         image_rotary_emb: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n-    ) -> torch.Tensor:\n+    ) -> Tuple[torch.Tensor, torch.Tensor]:\n         text_seq_length = encoder_hidden_states.size(1)\n \n         # norm & modulate\n@@ -691,7 +691,7 @@ def forward(\n         id_cond: Optional[torch.Tensor] = None,\n         id_vit_hidden: Optional[torch.Tensor] = None,\n         return_dict: bool = True,\n-    ):\n+    ) -> Union[Tuple[torch.Tensor], Transformer2DModelOutput]:\n         if attention_kwargs is not None:\n             attention_kwargs = attention_kwargs.copy()\n             lora_scale = attention_kwargs.pop(\"scale\", 1.0)"
        },
        {
          "filename": "src/diffusers/models/transformers/lumina_nextdit2d.py",
          "status": "modified",
          "additions": 3,
          "deletions": 3,
          "changes": 6,
          "patch": "@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from typing import Any, Dict, Optional\n+from typing import Any, Dict, Optional, Tuple, Union\n \n import torch\n import torch.nn as nn\n@@ -124,7 +124,7 @@ def forward(\n         encoder_mask: torch.Tensor,\n         temb: torch.Tensor,\n         cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n-    ):\n+    ) -> torch.Tensor:\n         \"\"\"\n         Perform a forward pass through the LuminaNextDiTBlock.\n \n@@ -297,7 +297,7 @@ def forward(\n         image_rotary_emb: torch.Tensor,\n         cross_attention_kwargs: Dict[str, Any] = None,\n         return_dict=True,\n-    ) -> torch.Tensor:\n+    ) -> Union[Tuple[torch.Tensor], Transformer2DModelOutput]:\n         \"\"\"\n         Forward pass of LuminaNextDiT.\n "
        },
        {
          "filename": "src/diffusers/models/transformers/transformer_bria.py",
          "status": "modified",
          "additions": 2,
          "deletions": 2,
          "changes": 4,
          "patch": "@@ -472,7 +472,7 @@ def forward(\n         temb: torch.Tensor,\n         image_rotary_emb: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n         attention_kwargs: Optional[Dict[str, Any]] = None,\n-    ) -> torch.Tensor:\n+    ) -> Tuple[torch.Tensor, torch.Tensor]:\n         text_seq_len = encoder_hidden_states.shape[1]\n         hidden_states = torch.cat([encoder_hidden_states, hidden_states], dim=1)\n \n@@ -588,7 +588,7 @@ def forward(\n         return_dict: bool = True,\n         controlnet_block_samples=None,\n         controlnet_single_block_samples=None,\n-    ) -> Union[torch.FloatTensor, Transformer2DModelOutput]:\n+    ) -> Union[Tuple[torch.Tensor], Transformer2DModelOutput]:\n         \"\"\"\n         The [`BriaTransformer2DModel`] forward method.\n "
        },
        {
          "filename": "src/diffusers/models/transformers/transformer_cogview3plus.py",
          "status": "modified",
          "additions": 3,
          "deletions": 3,
          "changes": 6,
          "patch": "@@ -13,7 +13,7 @@\n # limitations under the License.\n \n \n-from typing import Dict, Union\n+from typing import Dict, Tuple, Union\n \n import torch\n import torch.nn as nn\n@@ -79,7 +79,7 @@ def forward(\n         hidden_states: torch.Tensor,\n         encoder_hidden_states: torch.Tensor,\n         emb: torch.Tensor,\n-    ) -> torch.Tensor:\n+    ) -> Tuple[torch.Tensor, torch.Tensor]:\n         text_seq_length = encoder_hidden_states.size(1)\n \n         # norm & modulate\n@@ -293,7 +293,7 @@ def forward(\n         target_size: torch.Tensor,\n         crop_coords: torch.Tensor,\n         return_dict: bool = True,\n-    ) -> Union[torch.Tensor, Transformer2DModelOutput]:\n+    ) -> Union[Tuple[torch.Tensor], Transformer2DModelOutput]:\n         \"\"\"\n         The [`CogView3PlusTransformer2DModel`] forward method.\n "
        },
        {
          "filename": "src/diffusers/models/transformers/transformer_cogview4.py",
          "status": "modified",
          "additions": 2,
          "deletions": 2,
          "changes": 4,
          "patch": "@@ -494,7 +494,7 @@ def forward(\n         ] = None,\n         attention_mask: Optional[Dict[str, torch.Tensor]] = None,\n         attention_kwargs: Optional[Dict[str, Any]] = None,\n-    ) -> torch.Tensor:\n+    ) -> Tuple[torch.Tensor, torch.Tensor]:\n         # 1. Timestep conditioning\n         (\n             norm_hidden_states,\n@@ -717,7 +717,7 @@ def forward(\n         image_rotary_emb: Optional[\n             Union[Tuple[torch.Tensor, torch.Tensor], List[Tuple[torch.Tensor, torch.Tensor]]]\n         ] = None,\n-    ) -> Union[torch.Tensor, Transformer2DModelOutput]:\n+    ) -> Union[Tuple[torch.Tensor], Transformer2DModelOutput]:\n         if attention_kwargs is not None:\n             attention_kwargs = attention_kwargs.copy()\n             lora_scale = attention_kwargs.pop(\"scale\", 1.0)"
        },
        {
          "filename": "src/diffusers/models/transformers/transformer_hidream_image.py",
          "status": "modified",
          "additions": 5,
          "deletions": 5,
          "changes": 10,
          "patch": "@@ -55,7 +55,7 @@ def __init__(self, hidden_size, frequency_embedding_size=256):\n         self.time_proj = Timesteps(num_channels=frequency_embedding_size, flip_sin_to_cos=True, downscale_freq_shift=0)\n         self.timestep_embedder = TimestepEmbedding(in_channels=frequency_embedding_size, time_embed_dim=hidden_size)\n \n-    def forward(self, timesteps: torch.Tensor, wdtype: Optional[torch.dtype] = None):\n+    def forward(self, timesteps: torch.Tensor, wdtype: Optional[torch.dtype] = None) -> torch.Tensor:\n         t_emb = self.time_proj(timesteps).to(dtype=wdtype)\n         t_emb = self.timestep_embedder(t_emb)\n         return t_emb\n@@ -87,7 +87,7 @@ def __init__(\n         self.out_channels = out_channels\n         self.proj = nn.Linear(in_channels * patch_size * patch_size, out_channels, bias=True)\n \n-    def forward(self, latent):\n+    def forward(self, latent) -> torch.Tensor:\n         latent = self.proj(latent)\n         return latent\n \n@@ -534,7 +534,7 @@ def forward(\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         temb: Optional[torch.Tensor] = None,\n         image_rotary_emb: torch.Tensor = None,\n-    ) -> torch.Tensor:\n+    ) -> Tuple[torch.Tensor, torch.Tensor]:\n         wtype = hidden_states.dtype\n         (\n             shift_msa_i,\n@@ -592,7 +592,7 @@ def forward(\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         temb: Optional[torch.Tensor] = None,\n         image_rotary_emb: torch.Tensor = None,\n-    ) -> torch.Tensor:\n+    ) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n         return self.block(\n             hidden_states=hidden_states,\n             hidden_states_masks=hidden_states_masks,\n@@ -786,7 +786,7 @@ def forward(\n         attention_kwargs: Optional[Dict[str, Any]] = None,\n         return_dict: bool = True,\n         **kwargs,\n-    ):\n+    ) -> Union[Tuple[torch.Tensor], Transformer2DModelOutput]:\n         encoder_hidden_states = kwargs.get(\"encoder_hidden_states\", None)\n \n         if encoder_hidden_states is not None:"
        },
        {
          "filename": "src/diffusers/models/transformers/transformer_hunyuan_video.py",
          "status": "modified",
          "additions": 3,
          "deletions": 3,
          "changes": 6,
          "patch": "@@ -529,7 +529,7 @@ def forward(\n         image_rotary_emb: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n         *args,\n         **kwargs,\n-    ) -> torch.Tensor:\n+    ) -> Tuple[torch.Tensor, torch.Tensor]:\n         text_seq_length = encoder_hidden_states.shape[1]\n         hidden_states = torch.cat([hidden_states, encoder_hidden_states], dim=1)\n \n@@ -684,7 +684,7 @@ def forward(\n         image_rotary_emb: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n         token_replace_emb: torch.Tensor = None,\n         num_tokens: int = None,\n-    ) -> torch.Tensor:\n+    ) -> Tuple[torch.Tensor, torch.Tensor]:\n         text_seq_length = encoder_hidden_states.shape[1]\n         hidden_states = torch.cat([hidden_states, encoder_hidden_states], dim=1)\n \n@@ -1038,7 +1038,7 @@ def forward(\n         guidance: torch.Tensor = None,\n         attention_kwargs: Optional[Dict[str, Any]] = None,\n         return_dict: bool = True,\n-    ) -> Union[torch.Tensor, Dict[str, torch.Tensor]]:\n+    ) -> Union[Tuple[torch.Tensor], Transformer2DModelOutput]:\n         if attention_kwargs is not None:\n             attention_kwargs = attention_kwargs.copy()\n             lora_scale = attention_kwargs.pop(\"scale\", 1.0)"
        },
        {
          "filename": "src/diffusers/models/transformers/transformer_hunyuan_video_framepack.py",
          "status": "modified",
          "additions": 2,
          "deletions": 2,
          "changes": 4,
          "patch": "@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from typing import Any, Dict, List, Optional, Tuple\n+from typing import Any, Dict, List, Optional, Tuple, Union\n \n import torch\n import torch.nn as nn\n@@ -216,7 +216,7 @@ def forward(\n         indices_latents_history_4x: Optional[torch.Tensor] = None,\n         attention_kwargs: Optional[Dict[str, Any]] = None,\n         return_dict: bool = True,\n-    ):\n+    ) -> Union[Tuple[torch.Tensor], Transformer2DModelOutput]:\n         if attention_kwargs is not None:\n             attention_kwargs = attention_kwargs.copy()\n             lora_scale = attention_kwargs.pop(\"scale\", 1.0)"
        }
      ],
      "num_files": 11,
      "scraped_at": "2025-11-16T21:19:29.538075"
    },
    {
      "pr_number": 12275,
      "title": "[quantization] feat: support aobaseconfig classes in `TorchAOConfig`",
      "body": "# What does this PR do?\r\n\r\nThe `AOBaseConfig` classes introduced in `torchao` (since 0.9.0) are more flexible. Similar to [Transformers](https://huggingface.co/docs/transformers/main/en/quantization/torchao), this PR adds support for allowing them in Diffusers:\r\n\r\n```py\r\nfrom diffusers import DiffusionPipeline, TorchAoConfig, PipelineQuantizationConfig\r\nfrom torchao.quantization import Int8WeightOnlyConfig\r\nimport torch \r\n\r\nckpt_id = \"black-forest-labs/FLUX.1-dev\"\r\npipeline_quant_config = PipelineQuantizationConfig(\r\n    quant_mapping={\"transformer\": TorchAoConfig(Int8WeightOnlyConfig())}\r\n)\r\npipe = DiffusionPipeline.from_pretrained(\r\n    ckpt_id, quantization_config=pipeline_quant_config, torch_dtype=torch.bfloat16\r\n).to(\"cuda\")\r\n_ = pipe(\"dog\", num_inference_steps=2)\r\n```\r\n\r\n@stevhliu, would it be possible for you to propagate the relevant changes to [our TorchAO docs](https://huggingface.co/docs/diffusers/main/en/quantization/torchao) from [Transformers](https://huggingface.co/docs/transformers/main/en/quantization/torchao)? Can happen in a later PR.",
      "html_url": "https://github.com/huggingface/diffusers/pull/12275",
      "created_at": "2025-09-03T07:11:38Z",
      "merged_at": "2025-09-29T12:34:18Z",
      "merge_commit_sha": "64a5187d96f9376c7cf5123db810f2d2da79d7d0",
      "base_ref": "main",
      "head_sha": "715bd8cbddf42b2dd1059832af04f60d8c75dbab",
      "user": "sayakpaul",
      "files": [
        {
          "filename": "docs/source/en/quantization/torchao.md",
          "status": "modified",
          "additions": 66,
          "deletions": 39,
          "changes": 105,
          "patch": "@@ -11,69 +11,96 @@ specific language governing permissions and limitations under the License. -->\n \n # torchao\n \n-[TorchAO](https://github.com/pytorch/ao) is an architecture optimization library for PyTorch. It provides high-performance dtypes, optimization techniques, and kernels for inference and training, featuring composability with native PyTorch features like [torch.compile](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html), FullyShardedDataParallel (FSDP), and more.\n+[torchao](https://github.com/pytorch/ao) provides high-performance dtypes and optimizations based on quantization and sparsity for inference and training PyTorch models. It is supported for any model in any modality, as long as it supports loading with [Accelerate](https://hf.co/docs/accelerate/index) and contains `torch.nn.Linear` layers.\n \n-Before you begin, make sure you have Pytorch 2.5+ and TorchAO installed.\n+Make sure Pytorch 2.5+ and torchao are installed with the command below.\n \n ```bash\n-pip install -U torch torchao\n+uv pip install -U torch torchao\n ```\n \n+Each quantization dtype is available as a separate instance of a [AOBaseConfig](https://docs.pytorch.org/ao/main/api_ref_quantization.html#inference-apis-for-quantize) class. This provides more flexible configuration options by exposing more available arguments.\n \n-Quantize a model by passing [`TorchAoConfig`] to [`~ModelMixin.from_pretrained`] (you can also load pre-quantized models). This works for any model in any modality, as long as it supports loading with [Accelerate](https://hf.co/docs/accelerate/index) and contains `torch.nn.Linear` layers.\n+Pass the `AOBaseConfig` of a quantization dtype, like [Int4WeightOnlyConfig](https://docs.pytorch.org/ao/main/generated/torchao.quantization.Int4WeightOnlyConfig) to [`TorchAoConfig`] in [`~ModelMixin.from_pretrained`].\n \n-The example below only quantizes the weights to int8.\n-\n-```python\n+```py\n import torch\n-from diffusers import FluxPipeline, AutoModel, TorchAoConfig\n-\n-model_id = \"black-forest-labs/FLUX.1-dev\"\n-dtype = torch.bfloat16\n+from diffusers import DiffusionPipeline, PipelineQuantizationConfig, TorchAoConfig\n+from torchao.quantization import Int8WeightOnlyConfig\n \n-quantization_config = TorchAoConfig(\"int8wo\")\n-transformer = AutoModel.from_pretrained(\n-    model_id,\n-    subfolder=\"transformer\",\n-    quantization_config=quantization_config,\n-    torch_dtype=dtype,\n+pipeline_quant_config = PipelineQuantizationConfig(\n+    quant_mapping={\"transformer\": TorchAoConfig(Int8WeightOnlyConfig(group_size=128)))}\n )\n-pipe = FluxPipeline.from_pretrained(\n-    model_id,\n-    transformer=transformer,\n-    torch_dtype=dtype,\n+pipeline = DiffusionPipeline.from_pretrained(\n+    \"black-forest-labs/FLUX.1-dev\",\n+    quantzation_config=pipeline_quant_config,\n+    torch_dtype=torch.bfloat16,\n+    device_map=\"cuda\"\n )\n-pipe.to(\"cuda\")\n+```\n \n-# Without quantization: ~31.447 GB\n-# With quantization: ~20.40 GB\n-print(f\"Pipeline memory usage: {torch.cuda.max_memory_reserved() / 1024**3:.3f} GB\")\n+For simple use cases, you could also provide a string identifier in [`TorchAo`] as shown below.\n \n-prompt = \"A cat holding a sign that says hello world\"\n-image = pipe(\n-    prompt, num_inference_steps=50, guidance_scale=4.5, max_sequence_length=512\n-).images[0]\n-image.save(\"output.png\")\n+```py\n+import torch\n+from diffusers import DiffusionPipeline, PipelineQuantizationConfig, TorchAoConfig\n+\n+pipeline_quant_config = PipelineQuantizationConfig(\n+    quant_mapping={\"transformer\": TorchAoConfig(\"int8wo\")}\n+)\n+pipeline = DiffusionPipeline.from_pretrained(\n+    \"black-forest-labs/FLUX.1-dev\",\n+    quantzation_config=pipeline_quant_config,\n+    torch_dtype=torch.bfloat16,\n+    device_map=\"cuda\"\n+)\n ```\n \n-TorchAO is fully compatible with [torch.compile](../optimization/fp16#torchcompile), setting it apart from other quantization methods. This makes it easy to speed up inference with just one line of code.\n+## torch.compile\n+\n+torchao supports [torch.compile](../optimization/fp16#torchcompile) which can speed up inference with one line of code.\n \n ```python\n-# In the above code, add the following after initializing the transformer\n-transformer = torch.compile(transformer, mode=\"max-autotune\", fullgraph=True)\n+import torch\n+from diffusers import DiffusionPipeline, PipelineQuantizationConfig, TorchAoConfig\n+from torchao.quantization import Int4WeightOnlyConfig\n+\n+pipeline_quant_config = PipelineQuantizationConfig(\n+    quant_mapping={\"transformer\": TorchAoConfig(Int4WeightOnlyConfig(group_size=128)))}\n+)\n+pipeline = DiffusionPipeline.from_pretrained(\n+    \"black-forest-labs/FLUX.1-dev\",\n+    quantzation_config=pipeline_quant_config,\n+    torch_dtype=torch.bfloat16,\n+    device_map=\"cuda\"\n+)\n+\n+pipeline.transformer.compile(transformer, mode=\"max-autotune\", fullgraph=True)\n ```\n \n-For speed and memory benchmarks on Flux and CogVideoX, please refer to the table [here](https://github.com/huggingface/diffusers/pull/10009#issue-2688781450). You can also find some torchao [benchmarks](https://github.com/pytorch/ao/tree/main/torchao/quantization#benchmarks) numbers for various hardware.\n+Refer to this [table](https://github.com/huggingface/diffusers/pull/10009#issue-2688781450) for inference speed and memory usage benchmarks with Flux and CogVideoX. More benchmarks on various hardware are also available in the torchao [repository](https://github.com/pytorch/ao/tree/main/torchao/quantization#benchmarks).\n \n > [!TIP]\n > The FP8 post-training quantization schemes in torchao are effective for GPUs with compute capability of at least 8.9 (RTX-4090, Hopper, etc.). FP8 often provides the best speed, memory, and quality trade-off when generating images and videos. We recommend combining FP8 and torch.compile if your GPU is compatible.\n \n-torchao also supports an automatic quantization API through [autoquant](https://github.com/pytorch/ao/blob/main/torchao/quantization/README.md#autoquantization). Autoquantization determines the best quantization strategy applicable to a model by comparing the performance of each technique on chosen input types and shapes. Currently, this can be used directly on the underlying modeling components. Diffusers will also expose an autoquant configuration option in the future.\n+## autoquant\n+\n+torchao provides [autoquant](https://docs.pytorch.org/ao/stable/generated/torchao.quantization.autoquant.html#torchao.quantization.autoquant) an automatic quantization API. Autoquantization chooses the best quantization strategy by comparing the performance of each strategy on chosen input types and shapes. This is only supported in Diffusers for individual models at the moment.\n+\n+```py\n+import torch\n+from diffusers import DiffusionPipeline\n+from torchao.quantization import autoquant\n+\n+# Load the pipeline\n+pipeline = DiffusionPipeline.from_pretrained(\n+    \"black-forest-labs/FLUX.1-schnell\",\n+    torch_dtype=torch.bfloat16,\n+    device_map=\"cuda\"\n+)\n \n-The `TorchAoConfig` class accepts three parameters:\n-- `quant_type`: A string value mentioning one of the quantization types below.\n-- `modules_to_not_convert`: A list of module full/partial module names for which quantization should not be performed. For example, to not perform any quantization of the [`FluxTransformer2DModel`]'s first block, one would specify: `modules_to_not_convert=[\"single_transformer_blocks.0\"]`.\n-- `kwargs`: A dict of keyword arguments to pass to the underlying quantization method which will be invoked based on `quant_type`.\n+transformer = autoquant(pipeline.transformer)\n+```\n \n ## Supported quantization types\n "
        },
        {
          "filename": "src/diffusers/quantizers/quantization_config.py",
          "status": "modified",
          "additions": 136,
          "deletions": 29,
          "changes": 165,
          "patch": "@@ -21,19 +21,20 @@\n \"\"\"\n \n import copy\n+import dataclasses\n import importlib.metadata\n import inspect\n import json\n import os\n import warnings\n-from dataclasses import dataclass\n+from dataclasses import dataclass, is_dataclass\n from enum import Enum\n from functools import partial\n from typing import Any, Callable, Dict, List, Optional, Union\n \n from packaging import version\n \n-from ..utils import is_torch_available, is_torchao_available, logging\n+from ..utils import is_torch_available, is_torchao_available, is_torchao_version, logging\n \n \n if is_torch_available():\n@@ -443,7 +444,7 @@ class TorchAoConfig(QuantizationConfigMixin):\n     \"\"\"This is a config class for torchao quantization/sparsity techniques.\n \n     Args:\n-        quant_type (`str`):\n+        quant_type (Union[`str`, AOBaseConfig]):\n             The type of quantization we want to use, currently supporting:\n                 - **Integer quantization:**\n                     - Full function names: `int4_weight_only`, `int8_dynamic_activation_int4_weight`,\n@@ -465,6 +466,7 @@ class TorchAoConfig(QuantizationConfigMixin):\n                 - **Unsigned Integer quantization:**\n                     - Full function names: `uintx_weight_only`\n                     - Shorthands: `uint1wo`, `uint2wo`, `uint3wo`, `uint4wo`, `uint5wo`, `uint6wo`, `uint7wo`\n+                - An AOBaseConfig instance: for more advanced configuration options.\n         modules_to_not_convert (`List[str]`, *optional*, default to `None`):\n             The list of modules to not quantize, useful for quantizing models that explicitly require to have some\n             modules left in their original precision.\n@@ -478,6 +480,12 @@ class TorchAoConfig(QuantizationConfigMixin):\n         ```python\n         from diffusers import FluxTransformer2DModel, TorchAoConfig\n \n+        # AOBaseConfig-based configuration\n+        from torchao.quantization import Int8WeightOnlyConfig\n+\n+        quantization_config = TorchAoConfig(Int8WeightOnlyConfig())\n+\n+        # String-based config\n         quantization_config = TorchAoConfig(\"int8wo\")\n         transformer = FluxTransformer2DModel.from_pretrained(\n             \"black-forest-labs/Flux.1-Dev\",\n@@ -490,7 +498,7 @@ class TorchAoConfig(QuantizationConfigMixin):\n \n     def __init__(\n         self,\n-        quant_type: str,\n+        quant_type: Union[str, \"AOBaseConfig\"],  # noqa: F821\n         modules_to_not_convert: Optional[List[str]] = None,\n         **kwargs,\n     ) -> None:\n@@ -504,34 +512,103 @@ def __init__(\n         else:\n             self.quant_type_kwargs = kwargs\n \n-        TORCHAO_QUANT_TYPE_METHODS = self._get_torchao_quant_type_to_method()\n-        if self.quant_type not in TORCHAO_QUANT_TYPE_METHODS.keys():\n-            is_floating_quant_type = self.quant_type.startswith(\"float\") or self.quant_type.startswith(\"fp\")\n-            if is_floating_quant_type and not self._is_xpu_or_cuda_capability_atleast_8_9():\n+        self.post_init()\n+\n+    def post_init(self):\n+        if not isinstance(self.quant_type, str):\n+            if is_torchao_version(\"<=\", \"0.9.0\"):\n                 raise ValueError(\n-                    f\"Requested quantization type: {self.quant_type} is not supported on GPUs with CUDA capability <= 8.9. You \"\n-                    f\"can check the CUDA capability of your GPU using `torch.cuda.get_device_capability()`.\"\n+                    f\"torchao <= 0.9.0 only supports string quant_type, got {type(self.quant_type).__name__}. \"\n+                    f\"Upgrade to torchao > 0.9.0 to use AOBaseConfig.\"\n                 )\n \n-            raise ValueError(\n-                f\"Requested quantization type: {self.quant_type} is not supported or is an incorrect `quant_type` name. If you think the \"\n-                f\"provided quantization type should be supported, please open an issue at https://github.com/huggingface/diffusers/issues.\"\n-            )\n+            from torchao.quantization.quant_api import AOBaseConfig\n \n-        method = TORCHAO_QUANT_TYPE_METHODS[self.quant_type]\n-        signature = inspect.signature(method)\n-        all_kwargs = {\n-            param.name\n-            for param in signature.parameters.values()\n-            if param.kind in [inspect.Parameter.KEYWORD_ONLY, inspect.Parameter.POSITIONAL_OR_KEYWORD]\n-        }\n-        unsupported_kwargs = list(self.quant_type_kwargs.keys() - all_kwargs)\n+            if not isinstance(self.quant_type, AOBaseConfig):\n+                raise TypeError(f\"quant_type must be a AOBaseConfig instance, got {type(self.quant_type).__name__}\")\n \n-        if len(unsupported_kwargs) > 0:\n-            raise ValueError(\n-                f'The quantization method \"{quant_type}\" does not support the following keyword arguments: '\n-                f\"{unsupported_kwargs}. The following keywords arguments are supported: {all_kwargs}.\"\n-            )\n+        elif isinstance(self.quant_type, str):\n+            TORCHAO_QUANT_TYPE_METHODS = self._get_torchao_quant_type_to_method()\n+\n+            if self.quant_type not in TORCHAO_QUANT_TYPE_METHODS.keys():\n+                is_floating_quant_type = self.quant_type.startswith(\"float\") or self.quant_type.startswith(\"fp\")\n+                if is_floating_quant_type and not self._is_xpu_or_cuda_capability_atleast_8_9():\n+                    raise ValueError(\n+                        f\"Requested quantization type: {self.quant_type} is not supported on GPUs with CUDA capability <= 8.9. You \"\n+                        f\"can check the CUDA capability of your GPU using `torch.cuda.get_device_capability()`.\"\n+                    )\n+\n+                raise ValueError(\n+                    f\"Requested quantization type: {self.quant_type} is not supported or is an incorrect `quant_type` name. If you think the \"\n+                    f\"provided quantization type should be supported, please open an issue at https://github.com/huggingface/diffusers/issues.\"\n+                )\n+\n+            method = TORCHAO_QUANT_TYPE_METHODS[self.quant_type]\n+            signature = inspect.signature(method)\n+            all_kwargs = {\n+                param.name\n+                for param in signature.parameters.values()\n+                if param.kind in [inspect.Parameter.KEYWORD_ONLY, inspect.Parameter.POSITIONAL_OR_KEYWORD]\n+            }\n+            unsupported_kwargs = list(self.quant_type_kwargs.keys() - all_kwargs)\n+\n+            if len(unsupported_kwargs) > 0:\n+                raise ValueError(\n+                    f'The quantization method \"{self.quant_type}\" does not support the following keyword arguments: '\n+                    f\"{unsupported_kwargs}. The following keywords arguments are supported: {all_kwargs}.\"\n+                )\n+\n+    def to_dict(self):\n+        \"\"\"Convert configuration to a dictionary.\"\"\"\n+        d = super().to_dict()\n+\n+        if isinstance(self.quant_type, str):\n+            # Handle layout serialization if present\n+            if \"quant_type_kwargs\" in d and \"layout\" in d[\"quant_type_kwargs\"]:\n+                if is_dataclass(d[\"quant_type_kwargs\"][\"layout\"]):\n+                    d[\"quant_type_kwargs\"][\"layout\"] = [\n+                        d[\"quant_type_kwargs\"][\"layout\"].__class__.__name__,\n+                        dataclasses.asdict(d[\"quant_type_kwargs\"][\"layout\"]),\n+                    ]\n+                if isinstance(d[\"quant_type_kwargs\"][\"layout\"], list):\n+                    assert len(d[\"quant_type_kwargs\"][\"layout\"]) == 2, \"layout saves layout name and layout kwargs\"\n+                    assert isinstance(d[\"quant_type_kwargs\"][\"layout\"][0], str), \"layout name must be a string\"\n+                    assert isinstance(d[\"quant_type_kwargs\"][\"layout\"][1], dict), \"layout kwargs must be a dict\"\n+                else:\n+                    raise ValueError(\"layout must be a list\")\n+        else:\n+            # Handle AOBaseConfig serialization\n+            from torchao.core.config import config_to_dict\n+\n+            # For now we assume there is 1 config per Transformer, however in the future\n+            # We may want to support a config per fqn.\n+            d[\"quant_type\"] = {\"default\": config_to_dict(self.quant_type)}\n+\n+        return d\n+\n+    @classmethod\n+    def from_dict(cls, config_dict, return_unused_kwargs=False, **kwargs):\n+        \"\"\"Create configuration from a dictionary.\"\"\"\n+        if not is_torchao_version(\">\", \"0.9.0\"):\n+            raise NotImplementedError(\"TorchAoConfig requires torchao > 0.9.0 for construction from dict\")\n+        config_dict = config_dict.copy()\n+        quant_type = config_dict.pop(\"quant_type\")\n+\n+        if isinstance(quant_type, str):\n+            return cls(quant_type=quant_type, **config_dict)\n+        # Check if we only have one key which is \"default\"\n+        # In the future we may update this\n+        assert len(quant_type) == 1 and \"default\" in quant_type, (\n+            \"Expected only one key 'default' in quant_type dictionary\"\n+        )\n+        quant_type = quant_type[\"default\"]\n+\n+        # Deserialize quant_type if needed\n+        from torchao.core.config import config_from_dict\n+\n+        quant_type = config_from_dict(quant_type)\n+\n+        return cls(quant_type=quant_type, **config_dict)\n \n     @classmethod\n     def _get_torchao_quant_type_to_method(cls):\n@@ -681,8 +758,38 @@ def _is_xpu_or_cuda_capability_atleast_8_9() -> bool:\n             raise RuntimeError(\"TorchAO requires a CUDA compatible GPU or Intel XPU and installation of PyTorch.\")\n \n     def get_apply_tensor_subclass(self):\n-        TORCHAO_QUANT_TYPE_METHODS = self._get_torchao_quant_type_to_method()\n-        return TORCHAO_QUANT_TYPE_METHODS[self.quant_type](**self.quant_type_kwargs)\n+        \"\"\"Create the appropriate quantization method based on configuration.\"\"\"\n+        if not isinstance(self.quant_type, str):\n+            return self.quant_type\n+        else:\n+            methods = self._get_torchao_quant_type_to_method()\n+            quant_type_kwargs = self.quant_type_kwargs.copy()\n+            if (\n+                not torch.cuda.is_available()\n+                and is_torchao_available()\n+                and self.quant_type == \"int4_weight_only\"\n+                and version.parse(importlib.metadata.version(\"torchao\")) >= version.parse(\"0.8.0\")\n+                and quant_type_kwargs.get(\"layout\", None) is None\n+            ):\n+                if torch.xpu.is_available():\n+                    if version.parse(importlib.metadata.version(\"torchao\")) >= version.parse(\n+                        \"0.11.0\"\n+                    ) and version.parse(importlib.metadata.version(\"torch\")) > version.parse(\"2.7.9\"):\n+                        from torchao.dtypes import Int4XPULayout\n+                        from torchao.quantization.quant_primitives import ZeroPointDomain\n+\n+                        quant_type_kwargs[\"layout\"] = Int4XPULayout()\n+                        quant_type_kwargs[\"zero_point_domain\"] = ZeroPointDomain.INT\n+                    else:\n+                        raise ValueError(\n+                            \"TorchAoConfig requires torchao >= 0.11.0 and torch >= 2.8.0 for XPU support. Please upgrade the version or use run on CPU with the cpu version pytorch.\"\n+                        )\n+                else:\n+                    from torchao.dtypes import Int4CPULayout\n+\n+                    quant_type_kwargs[\"layout\"] = Int4CPULayout()\n+\n+            return methods[self.quant_type](**quant_type_kwargs)\n \n     def __repr__(self):\n         r\"\"\""
        },
        {
          "filename": "src/diffusers/quantizers/torchao/torchao_quantizer.py",
          "status": "modified",
          "additions": 71,
          "deletions": 21,
          "changes": 92,
          "patch": "@@ -18,9 +18,10 @@\n \"\"\"\n \n import importlib\n+import re\n import types\n from fnmatch import fnmatch\n-from typing import TYPE_CHECKING, Any, Dict, List, Union\n+from typing import TYPE_CHECKING, Any, Dict, List, Optional, Union\n \n from packaging import version\n \n@@ -107,6 +108,21 @@ def _update_torch_safe_globals():\n     _update_torch_safe_globals()\n \n \n+def fuzzy_match_size(config_name: str) -> Optional[str]:\n+    \"\"\"\n+    Extract the size digit from strings like \"4weight\", \"8weight\". Returns the digit as an integer if found, otherwise\n+    None.\n+    \"\"\"\n+    config_name = config_name.lower()\n+\n+    str_match = re.search(r\"(\\d)weight\", config_name)\n+\n+    if str_match:\n+        return str_match.group(1)\n+\n+    return None\n+\n+\n logger = logging.get_logger(__name__)\n \n \n@@ -176,8 +192,7 @@ def validate_environment(self, *args, **kwargs):\n \n     def update_torch_dtype(self, torch_dtype):\n         quant_type = self.quantization_config.quant_type\n-\n-        if quant_type.startswith(\"int\") or quant_type.startswith(\"uint\"):\n+        if isinstance(quant_type, str) and (quant_type.startswith(\"int\") or quant_type.startswith(\"uint\")):\n             if torch_dtype is not None and torch_dtype != torch.bfloat16:\n                 logger.warning(\n                     f\"You are trying to set torch_dtype to {torch_dtype} for int4/int8/uintx quantization, but \"\n@@ -197,24 +212,44 @@ def update_torch_dtype(self, torch_dtype):\n \n     def adjust_target_dtype(self, target_dtype: \"torch.dtype\") -> \"torch.dtype\":\n         quant_type = self.quantization_config.quant_type\n-\n-        if quant_type.startswith(\"int8\") or quant_type.startswith(\"int4\"):\n-            # Note that int4 weights are created by packing into torch.int8, but since there is no torch.int4, we use torch.int8\n-            return torch.int8\n-        elif quant_type == \"uintx_weight_only\":\n-            return self.quantization_config.quant_type_kwargs.get(\"dtype\", torch.uint8)\n-        elif quant_type.startswith(\"uint\"):\n-            return {\n-                1: torch.uint1,\n-                2: torch.uint2,\n-                3: torch.uint3,\n-                4: torch.uint4,\n-                5: torch.uint5,\n-                6: torch.uint6,\n-                7: torch.uint7,\n-            }[int(quant_type[4])]\n-        elif quant_type.startswith(\"float\") or quant_type.startswith(\"fp\"):\n-            return torch.bfloat16\n+        from accelerate.utils import CustomDtype\n+\n+        if isinstance(quant_type, str):\n+            if quant_type.startswith(\"int8\"):\n+                # Note that int4 weights are created by packing into torch.int8, but since there is no torch.int4, we use torch.int8\n+                return torch.int8\n+            elif quant_type.startswith(\"int4\"):\n+                return CustomDtype.INT4\n+            elif quant_type == \"uintx_weight_only\":\n+                return self.quantization_config.quant_type_kwargs.get(\"dtype\", torch.uint8)\n+            elif quant_type.startswith(\"uint\"):\n+                return {\n+                    1: torch.uint1,\n+                    2: torch.uint2,\n+                    3: torch.uint3,\n+                    4: torch.uint4,\n+                    5: torch.uint5,\n+                    6: torch.uint6,\n+                    7: torch.uint7,\n+                }[int(quant_type[4])]\n+            elif quant_type.startswith(\"float\") or quant_type.startswith(\"fp\"):\n+                return torch.bfloat16\n+\n+        elif is_torchao_version(\">\", \"0.9.0\"):\n+            from torchao.core.config import AOBaseConfig\n+\n+            quant_type = self.quantization_config.quant_type\n+            if isinstance(quant_type, AOBaseConfig):\n+                # Extract size digit using fuzzy match on the class name\n+                config_name = quant_type.__class__.__name__\n+                size_digit = fuzzy_match_size(config_name)\n+\n+                # Map the extracted digit to appropriate dtype\n+                if size_digit == \"4\":\n+                    return CustomDtype.INT4\n+                else:\n+                    # Default to int8\n+                    return torch.int8\n \n         if isinstance(target_dtype, SUPPORTED_TORCH_DTYPES_FOR_QUANTIZATION):\n             return target_dtype\n@@ -297,6 +332,21 @@ def get_cuda_warm_up_factor(self):\n         # Original mapping for non-AOBaseConfig types\n         # For the uint types, this is a best guess. Once these types become more used\n         # we can look into their nuances.\n+        if is_torchao_version(\">\", \"0.9.0\"):\n+            from torchao.core.config import AOBaseConfig\n+\n+            quant_type = self.quantization_config.quant_type\n+            # For autoquant case, it will be treated in the string implementation below in map_to_target_dtype\n+            if isinstance(quant_type, AOBaseConfig):\n+                # Extract size digit using fuzzy match on the class name\n+                config_name = quant_type.__class__.__name__\n+                size_digit = fuzzy_match_size(config_name)\n+\n+                if size_digit == \"4\":\n+                    return 8\n+                else:\n+                    return 4\n+\n         map_to_target_dtype = {\"int4_*\": 8, \"int8_*\": 4, \"uint*\": 8, \"float8*\": 4}\n         quant_type = self.quantization_config.quant_type\n         for pattern, target_dtype in map_to_target_dtype.items():"
        },
        {
          "filename": "tests/quantization/torchao/test_torchao.py",
          "status": "modified",
          "additions": 22,
          "deletions": 0,
          "changes": 22,
          "patch": "@@ -14,11 +14,13 @@\n # limitations under the License.\n \n import gc\n+import importlib.metadata\n import tempfile\n import unittest\n from typing import List\n \n import numpy as np\n+from packaging import version\n from parameterized import parameterized\n from transformers import AutoTokenizer, CLIPTextModel, CLIPTokenizer, T5EncoderModel\n \n@@ -65,6 +67,9 @@\n     from torchao.quantization.quant_primitives import MappingType\n     from torchao.utils import get_model_size_in_bytes\n \n+    if version.parse(importlib.metadata.version(\"torchao\")) >= version.Version(\"0.9.0\"):\n+        from torchao.quantization import Int8WeightOnlyConfig\n+\n \n @require_torch\n @require_torch_accelerator\n@@ -522,6 +527,15 @@ def test_sequential_cpu_offload(self):\n         inputs = self.get_dummy_inputs(torch_device)\n         _ = pipe(**inputs)\n \n+    @require_torchao_version_greater_or_equal(\"0.9.0\")\n+    def test_aobase_config(self):\n+        quantization_config = TorchAoConfig(Int8WeightOnlyConfig())\n+        components = self.get_dummy_components(quantization_config)\n+        pipe = FluxPipeline(**components).to(torch_device)\n+\n+        inputs = self.get_dummy_inputs(torch_device)\n+        _ = pipe(**inputs)\n+\n \n # Slices for these tests have been obtained on our aws-g6e-xlarge-plus runners\n @require_torch\n@@ -628,6 +642,14 @@ def test_int_a16w8_cpu(self):\n         self._test_original_model_expected_slice(quant_method, quant_method_kwargs, expected_slice)\n         self._check_serialization_expected_slice(quant_method, quant_method_kwargs, expected_slice, device)\n \n+    @require_torchao_version_greater_or_equal(\"0.9.0\")\n+    def test_aobase_config(self):\n+        quant_method, quant_method_kwargs = Int8WeightOnlyConfig(), {}\n+        expected_slice = np.array([0.3613, -0.127, -0.0223, -0.2539, -0.459, 0.4961, -0.1357, -0.6992, 0.4551])\n+        device = torch_device\n+        self._test_original_model_expected_slice(quant_method, quant_method_kwargs, expected_slice)\n+        self._check_serialization_expected_slice(quant_method, quant_method_kwargs, expected_slice, device)\n+\n \n @require_torchao_version_greater_or_equal(\"0.7.0\")\n class TorchAoCompileTest(QuantCompileTests, unittest.TestCase):"
        }
      ],
      "num_files": 4,
      "scraped_at": "2025-11-16T21:19:31.841053"
    },
    {
      "pr_number": 12234,
      "title": "[quant] allow `components_to_quantize` to be a non-list for single components",
      "body": "# What does this PR do?\r\n\r\nJust a small QoL improvement for pipeline-level quantization.\r\n\r\n```py\r\nPipelineQuantizationConfig(\r\n    quant_backend=\"bitsandbytes_4bit\",\r\n    quant_kwargs={\"load_in_4bit\": True, \"bnb_4bit_quant_type\": \"nf4\", \"bnb_4bit_compute_dtype\": torch.bfloat16},\r\n    components_to_quantize=\"transformer\", # instead of [\"transformer\"]\r\n)\r\n```",
      "html_url": "https://github.com/huggingface/diffusers/pull/12234",
      "created_at": "2025-08-25T12:56:58Z",
      "merged_at": "2025-09-10T19:47:08Z",
      "merge_commit_sha": "eb7ef26736055055df252d8f06d665fd407f6fe7",
      "base_ref": "main",
      "head_sha": "685cd638fe940626b567bdb62819fda2e73f98df",
      "user": "sayakpaul",
      "files": [
        {
          "filename": "docs/source/en/api/pipelines/cogvideox.md",
          "status": "modified",
          "additions": 1,
          "deletions": 1,
          "changes": 2,
          "patch": "@@ -50,7 +50,7 @@ from diffusers.utils import export_to_video\n pipeline_quant_config = PipelineQuantizationConfig(\n   quant_backend=\"torchao\",\n   quant_kwargs={\"quant_type\": \"int8wo\"},\n-  components_to_quantize=[\"transformer\"]\n+  components_to_quantize=\"transformer\"\n )\n \n # fp8 layerwise weight-casting"
        },
        {
          "filename": "docs/source/en/api/pipelines/hunyuan_video.md",
          "status": "modified",
          "additions": 3,
          "deletions": 3,
          "changes": 6,
          "patch": "@@ -54,7 +54,7 @@ pipeline_quant_config = PipelineQuantizationConfig(\n       \"bnb_4bit_quant_type\": \"nf4\",\n       \"bnb_4bit_compute_dtype\": torch.bfloat16\n       },\n-    components_to_quantize=[\"transformer\"]\n+    components_to_quantize=\"transformer\"\n )\n \n pipeline = HunyuanVideoPipeline.from_pretrained(\n@@ -91,7 +91,7 @@ pipeline_quant_config = PipelineQuantizationConfig(\n       \"bnb_4bit_quant_type\": \"nf4\",\n       \"bnb_4bit_compute_dtype\": torch.bfloat16\n       },\n-    components_to_quantize=[\"transformer\"]\n+    components_to_quantize=\"transformer\"\n )\n \n pipeline = HunyuanVideoPipeline.from_pretrained(\n@@ -139,7 +139,7 @@ export_to_video(video, \"output.mp4\", fps=15)\n         \"bnb_4bit_quant_type\": \"nf4\",\n         \"bnb_4bit_compute_dtype\": torch.bfloat16\n         },\n-      components_to_quantize=[\"transformer\"]\n+      components_to_quantize=\"transformer\"\n   )\n \n   pipeline = HunyuanVideoPipeline.from_pretrained("
        },
        {
          "filename": "docs/source/en/quantization/overview.md",
          "status": "modified",
          "additions": 4,
          "deletions": 1,
          "changes": 5,
          "patch": "@@ -34,7 +34,9 @@ Initialize [`~quantizers.PipelineQuantizationConfig`] with the following paramet\n > [!TIP]\n > These `quant_kwargs` arguments are different for each backend. Refer to the [Quantization API](../api/quantization) docs to view the arguments for each backend.\n \n-- `components_to_quantize` specifies which components of the pipeline to quantize. Typically, you should quantize the most compute intensive components like the transformer. The text encoder is another component to consider quantizing if a pipeline has more than one such as [`FluxPipeline`]. The example below quantizes the T5 text encoder in [`FluxPipeline`] while keeping the CLIP model intact.\n+- `components_to_quantize` specifies which component(s) of the pipeline to quantize. Typically, you should quantize the most compute intensive components like the transformer. The text encoder is another component to consider quantizing if a pipeline has more than one such as [`FluxPipeline`]. The example below quantizes the T5 text encoder in [`FluxPipeline`] while keeping the CLIP model intact.\n+\n+   `components_to_quantize` accepts either a list for multiple models or a string for a single model.\n \n The example below loads the bitsandbytes backend with the following arguments from [`~quantizers.quantization_config.BitsAndBytesConfig`], `load_in_4bit`, `bnb_4bit_quant_type`, and `bnb_4bit_compute_dtype`.\n \n@@ -62,6 +64,7 @@ pipe = DiffusionPipeline.from_pretrained(\n image = pipe(\"photo of a cute dog\").images[0]\n ```\n \n+\n ### Advanced quantization\n \n The `quant_mapping` argument provides more options for how to quantize each individual component in a pipeline, like combining different quantization backends."
        },
        {
          "filename": "docs/source/en/using-diffusers/text-img2vid.md",
          "status": "modified",
          "additions": 1,
          "deletions": 1,
          "changes": 2,
          "patch": "@@ -98,7 +98,7 @@ pipeline_quant_config = PipelineQuantizationConfig(\n     \"bnb_4bit_quant_type\": \"nf4\",\n     \"bnb_4bit_compute_dtype\": torch.bfloat16\n     },\n-  components_to_quantize=[\"transformer\"]\n+  components_to_quantize=\"transformer\"\n )\n \n pipeline = HunyuanVideoPipeline.from_pretrained("
        },
        {
          "filename": "src/diffusers/quantizers/pipe_quant_config.py",
          "status": "modified",
          "additions": 4,
          "deletions": 1,
          "changes": 5,
          "patch": "@@ -48,12 +48,15 @@ def __init__(\n         self,\n         quant_backend: str = None,\n         quant_kwargs: Dict[str, Union[str, float, int, dict]] = None,\n-        components_to_quantize: Optional[List[str]] = None,\n+        components_to_quantize: Optional[Union[List[str], str]] = None,\n         quant_mapping: Dict[str, Union[DiffQuantConfigMixin, \"TransformersQuantConfigMixin\"]] = None,\n     ):\n         self.quant_backend = quant_backend\n         # Initialize kwargs to be {} to set to the defaults.\n         self.quant_kwargs = quant_kwargs or {}\n+        if components_to_quantize:\n+            if isinstance(components_to_quantize, str):\n+                components_to_quantize = [components_to_quantize]\n         self.components_to_quantize = components_to_quantize\n         self.quant_mapping = quant_mapping\n         self.config_mapping = {}  # book-keeping Example: `{module_name: quant_config}`"
        },
        {
          "filename": "tests/quantization/test_pipeline_level_quantization.py",
          "status": "modified",
          "additions": 16,
          "deletions": 0,
          "changes": 16,
          "patch": "@@ -299,3 +299,19 @@ def _parse_config_string(self, config_string: str) -> tuple[str, dict]:\n         data = json.loads(json_part)\n \n         return data\n+\n+    def test_single_component_to_quantize(self):\n+        component_to_quantize = \"transformer\"\n+        quant_config = PipelineQuantizationConfig(\n+            quant_backend=\"bitsandbytes_8bit\",\n+            quant_kwargs={\"load_in_8bit\": True},\n+            components_to_quantize=component_to_quantize,\n+        )\n+        pipe = DiffusionPipeline.from_pretrained(\n+            self.model_name,\n+            quantization_config=quant_config,\n+            torch_dtype=torch.bfloat16,\n+        )\n+        for name, component in pipe.components.items():\n+            if name == component_to_quantize:\n+                self.assertTrue(hasattr(component.config, \"quantization_config\"))"
        }
      ],
      "num_files": 6,
      "scraped_at": "2025-11-16T21:19:42.009975"
    },
    {
      "pr_number": 12225,
      "title": "Add Qwen-Image-Edit Inpainting pipeline",
      "body": "# What does this PR do?\r\n\r\nThis PR introduces support for the **Qwen-Image-Edit** model in **Inpainting** tasks, expanding the model\u2019s creative capabilities and integration within the Diffusers library.\r\n\r\n## Example code\r\n```python\r\nimport torch\r\nfrom diffusers import QwenImageEditInpaintPipeline\r\nfrom diffusers.utils import load_image\r\nimport os\r\nos.environ[\"HF_ENABLE_PARALLEL_LOADING\"] = \"YES\"\r\n\r\npipe = QwenImageEditInpaintPipeline.from_pretrained(\"Qwen/Qwen-Image-Edit\", torch_dtype=torch.bfloat16)\r\npipe.to(\"cuda\")\r\nprompt = \"change the hat to red\"\r\nnegative_prompt = \" \"\r\nsource = load_image(\"https://github.com/Trgtuan10/Image_storage/blob/main/cute_cat.png?raw=true\")\r\nmask = load_image(\"https://github.com/Trgtuan10/Image_storage/blob/main/mask_cat.png?raw=true\")\r\n\r\nimage = pipe(\r\n    prompt=prompt,\r\n    negative_prompt=negative_prompt,\r\n    image=source,\r\n    mask_image=mask,\r\n    strength=1.0,\r\n    num_inference_steps=35,\r\n    true_cfg_scale=4.0,\r\n    generator=torch.Generator(device=\"cuda\").manual_seed(422)\r\n).images[0]\r\nimage.save(f\"qwen_inpainting.png\")\r\n```\r\n## Comparison about performance of QwenImage-Edit Inpaint\r\n\r\n<table>\r\n  <tr>\r\n    <td width=\"50%\"><b>Init image</b><br/>\r\n      <img src=\"https://github.com/Trgtuan10/Image_storage/blob/main/cute_cat.png?raw=true\" width=\"100%\"/>\r\n    </td>\r\n    <td width=\"50%\"><b>Mask</b><br/>\r\n      <img src=\"https://github.com/Trgtuan10/Image_storage/blob/main/mask_cat.png?raw=true\" width=\"100%\"/>\r\n    </td>\r\n  </tr>\r\n</table>\r\n\r\n<table>\r\n  <tr>\r\n    <td><b>QwenImage Inpaint</b><br/><img src=\"https://github.com/Trgtuan10/Image_storage/blob/main/qwen_inpainting_1.0.png?raw=true\" width=\"100%\"/></td>\r\n    <td><b>QwenImage-Edit</b><br/><img src=\"https://github.com/Trgtuan10/Image_storage/blob/main/output_image_edit_1.png?raw=true\" width=\"100%\"/></td>\r\n    <td><b>QwenImage-Edit Inpaint</b><br/><img src=\"https://github.com/Trgtuan10/Image_storage/blob/main/qwen_inpainting_1.png?raw=true\" width=\"100%\"/></td>\r\n  </tr>\r\n</table>\r\n\r\n\r\n## Who can review?\r\n\r\n- Pipelines and pipeline callbacks: @yiyixuxu and @asomoza\r\n\r\n\r\ncc @a-r-r-o-w @sayakpaul ",
      "html_url": "https://github.com/huggingface/diffusers/pull/12225",
      "created_at": "2025-08-23T15:34:05Z",
      "merged_at": "2025-08-31T05:49:15Z",
      "merge_commit_sha": "67ffa7031e5a4bf0991b692a424e36ca59e64ec9",
      "base_ref": "main",
      "head_sha": "2341ac243acdf4e0b015fc7a1e037e5e62664ecf",
      "user": "Trgtuan10",
      "files": [
        {
          "filename": "docs/source/en/api/pipelines/qwenimage.md",
          "status": "modified",
          "additions": 6,
          "deletions": 0,
          "changes": 6,
          "patch": "@@ -120,6 +120,12 @@ The `guidance_scale` parameter in the pipeline is there to support future guidan\n   - all\n   - __call__\n \n+## QwenImageEditInpaintPipeline\n+\n+[[autodoc]] QwenImageEditInpaintPipeline\n+  - all\n+  - __call__\n+\n ## QwenImaggeControlNetPipeline\n   - all\n   - __call__"
        },
        {
          "filename": "src/diffusers/__init__.py",
          "status": "modified",
          "additions": 2,
          "deletions": 0,
          "changes": 2,
          "patch": "@@ -494,6 +494,7 @@\n             \"PixArtSigmaPAGPipeline\",\n             \"PixArtSigmaPipeline\",\n             \"QwenImageControlNetPipeline\",\n+            \"QwenImageEditInpaintPipeline\",\n             \"QwenImageEditPipeline\",\n             \"QwenImageImg2ImgPipeline\",\n             \"QwenImageInpaintPipeline\",\n@@ -1134,6 +1135,7 @@\n             PixArtSigmaPAGPipeline,\n             PixArtSigmaPipeline,\n             QwenImageControlNetPipeline,\n+            QwenImageEditInpaintPipeline,\n             QwenImageEditPipeline,\n             QwenImageImg2ImgPipeline,\n             QwenImageInpaintPipeline,"
        },
        {
          "filename": "src/diffusers/pipelines/__init__.py",
          "status": "modified",
          "additions": 2,
          "deletions": 0,
          "changes": 2,
          "patch": "@@ -393,6 +393,7 @@\n         \"QwenImageImg2ImgPipeline\",\n         \"QwenImageInpaintPipeline\",\n         \"QwenImageEditPipeline\",\n+        \"QwenImageEditInpaintPipeline\",\n         \"QwenImageControlNetPipeline\",\n     ]\n try:\n@@ -714,6 +715,7 @@\n         from .pixart_alpha import PixArtAlphaPipeline, PixArtSigmaPipeline\n         from .qwenimage import (\n             QwenImageControlNetPipeline,\n+            QwenImageEditInpaintPipeline,\n             QwenImageEditPipeline,\n             QwenImageImg2ImgPipeline,\n             QwenImageInpaintPipeline,"
        },
        {
          "filename": "src/diffusers/pipelines/qwenimage/__init__.py",
          "status": "modified",
          "additions": 2,
          "deletions": 0,
          "changes": 2,
          "patch": "@@ -26,6 +26,7 @@\n     _import_structure[\"pipeline_qwenimage\"] = [\"QwenImagePipeline\"]\n     _import_structure[\"pipeline_qwenimage_controlnet\"] = [\"QwenImageControlNetPipeline\"]\n     _import_structure[\"pipeline_qwenimage_edit\"] = [\"QwenImageEditPipeline\"]\n+    _import_structure[\"pipeline_qwenimage_edit_inpaint\"] = [\"QwenImageEditInpaintPipeline\"]\n     _import_structure[\"pipeline_qwenimage_img2img\"] = [\"QwenImageImg2ImgPipeline\"]\n     _import_structure[\"pipeline_qwenimage_inpaint\"] = [\"QwenImageInpaintPipeline\"]\n \n@@ -39,6 +40,7 @@\n         from .pipeline_qwenimage import QwenImagePipeline\n         from .pipeline_qwenimage_controlnet import QwenImageControlNetPipeline\n         from .pipeline_qwenimage_edit import QwenImageEditPipeline\n+        from .pipeline_qwenimage_edit_inpaint import QwenImageEditInpaintPipeline\n         from .pipeline_qwenimage_img2img import QwenImageImg2ImgPipeline\n         from .pipeline_qwenimage_inpaint import QwenImageInpaintPipeline\n else:"
        },
        {
          "filename": "src/diffusers/pipelines/qwenimage/pipeline_qwenimage_edit_inpaint.py",
          "status": "added",
          "additions": 1106,
          "deletions": 0,
          "changes": 1106,
          "patch": "@@ -0,0 +1,1106 @@\n+# Copyright 2025 Qwen-Image Team and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import inspect\n+import math\n+from typing import Any, Callable, Dict, List, Optional, Union\n+\n+import numpy as np\n+import PIL.Image\n+import torch\n+from transformers import Qwen2_5_VLForConditionalGeneration, Qwen2Tokenizer, Qwen2VLProcessor\n+\n+from ...image_processor import PipelineImageInput, VaeImageProcessor\n+from ...loaders import QwenImageLoraLoaderMixin\n+from ...models import AutoencoderKLQwenImage, QwenImageTransformer2DModel\n+from ...schedulers import FlowMatchEulerDiscreteScheduler\n+from ...utils import is_torch_xla_available, logging, replace_example_docstring\n+from ...utils.torch_utils import randn_tensor\n+from ..pipeline_utils import DiffusionPipeline\n+from .pipeline_output import QwenImagePipelineOutput\n+\n+\n+if is_torch_xla_available():\n+    import torch_xla.core.xla_model as xm\n+\n+    XLA_AVAILABLE = True\n+else:\n+    XLA_AVAILABLE = False\n+\n+\n+logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n+\n+EXAMPLE_DOC_STRING = \"\"\"\n+    Examples:\n+        ```py\n+        >>> import torch\n+        >>> from PIL import Image\n+        >>> from diffusers import QwenImageEditInpaintPipeline\n+        >>> from diffusers.utils import load_image\n+\n+        >>> pipe = QwenImageEditInpaintPipeline.from_pretrained(\"Qwen/Qwen-Image-Edit\", torch_dtype=torch.bfloat16)\n+        >>> pipe.to(\"cuda\")\n+        >>> prompt = \"Face of a yellow cat, high resolution, sitting on a park bench\"\n+\n+        >>> img_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\"\n+        >>> mask_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png\"\n+        >>> source = load_image(img_url)\n+        >>> mask = load_image(mask_url)\n+        >>> image = pipe(\n+        ...     prompt=prompt, negative_prompt=\" \", image=source, mask_image=mask, strength=1.0, num_inference_steps=50\n+        ... ).images[0]\n+        >>> image.save(\"qwenimage_inpainting.png\")\n+        ```\n+\"\"\"\n+\n+\n+# Copied from diffusers.pipelines.qwenimage.pipeline_qwenimage.calculate_shift\n+def calculate_shift(\n+    image_seq_len,\n+    base_seq_len: int = 256,\n+    max_seq_len: int = 4096,\n+    base_shift: float = 0.5,\n+    max_shift: float = 1.15,\n+):\n+    m = (max_shift - base_shift) / (max_seq_len - base_seq_len)\n+    b = base_shift - m * base_seq_len\n+    mu = image_seq_len * m + b\n+    return mu\n+\n+\n+# Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.retrieve_timesteps\n+def retrieve_timesteps(\n+    scheduler,\n+    num_inference_steps: Optional[int] = None,\n+    device: Optional[Union[str, torch.device]] = None,\n+    timesteps: Optional[List[int]] = None,\n+    sigmas: Optional[List[float]] = None,\n+    **kwargs,\n+):\n+    r\"\"\"\n+    Calls the scheduler's `set_timesteps` method and retrieves timesteps from the scheduler after the call. Handles\n+    custom timesteps. Any kwargs will be supplied to `scheduler.set_timesteps`.\n+\n+    Args:\n+        scheduler (`SchedulerMixin`):\n+            The scheduler to get timesteps from.\n+        num_inference_steps (`int`):\n+            The number of diffusion steps used when generating samples with a pre-trained model. If used, `timesteps`\n+            must be `None`.\n+        device (`str` or `torch.device`, *optional*):\n+            The device to which the timesteps should be moved to. If `None`, the timesteps are not moved.\n+        timesteps (`List[int]`, *optional*):\n+            Custom timesteps used to override the timestep spacing strategy of the scheduler. If `timesteps` is passed,\n+            `num_inference_steps` and `sigmas` must be `None`.\n+        sigmas (`List[float]`, *optional*):\n+            Custom sigmas used to override the timestep spacing strategy of the scheduler. If `sigmas` is passed,\n+            `num_inference_steps` and `timesteps` must be `None`.\n+\n+    Returns:\n+        `Tuple[torch.Tensor, int]`: A tuple where the first element is the timestep schedule from the scheduler and the\n+        second element is the number of inference steps.\n+    \"\"\"\n+    if timesteps is not None and sigmas is not None:\n+        raise ValueError(\"Only one of `timesteps` or `sigmas` can be passed. Please choose one to set custom values\")\n+    if timesteps is not None:\n+        accepts_timesteps = \"timesteps\" in set(inspect.signature(scheduler.set_timesteps).parameters.keys())\n+        if not accepts_timesteps:\n+            raise ValueError(\n+                f\"The current scheduler class {scheduler.__class__}'s `set_timesteps` does not support custom\"\n+                f\" timestep schedules. Please check whether you are using the correct scheduler.\"\n+            )\n+        scheduler.set_timesteps(timesteps=timesteps, device=device, **kwargs)\n+        timesteps = scheduler.timesteps\n+        num_inference_steps = len(timesteps)\n+    elif sigmas is not None:\n+        accept_sigmas = \"sigmas\" in set(inspect.signature(scheduler.set_timesteps).parameters.keys())\n+        if not accept_sigmas:\n+            raise ValueError(\n+                f\"The current scheduler class {scheduler.__class__}'s `set_timesteps` does not support custom\"\n+                f\" sigmas schedules. Please check whether you are using the correct scheduler.\"\n+            )\n+        scheduler.set_timesteps(sigmas=sigmas, device=device, **kwargs)\n+        timesteps = scheduler.timesteps\n+        num_inference_steps = len(timesteps)\n+    else:\n+        scheduler.set_timesteps(num_inference_steps, device=device, **kwargs)\n+        timesteps = scheduler.timesteps\n+    return timesteps, num_inference_steps\n+\n+\n+# Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_img2img.retrieve_latents\n+def retrieve_latents(\n+    encoder_output: torch.Tensor, generator: Optional[torch.Generator] = None, sample_mode: str = \"sample\"\n+):\n+    if hasattr(encoder_output, \"latent_dist\") and sample_mode == \"sample\":\n+        return encoder_output.latent_dist.sample(generator)\n+    elif hasattr(encoder_output, \"latent_dist\") and sample_mode == \"argmax\":\n+        return encoder_output.latent_dist.mode()\n+    elif hasattr(encoder_output, \"latents\"):\n+        return encoder_output.latents\n+    else:\n+        raise AttributeError(\"Could not access latents of provided encoder_output\")\n+\n+\n+# Copied from diffusers.pipelines.qwenimage.pipeline_qwenimage_edit.calculate_dimensions\n+def calculate_dimensions(target_area, ratio):\n+    width = math.sqrt(target_area * ratio)\n+    height = width / ratio\n+\n+    width = round(width / 32) * 32\n+    height = round(height / 32) * 32\n+\n+    return width, height, None\n+\n+\n+class QwenImageEditInpaintPipeline(DiffusionPipeline, QwenImageLoraLoaderMixin):\n+    r\"\"\"\n+    The Qwen-Image-Edit pipeline for image editing.\n+\n+    Args:\n+        transformer ([`QwenImageTransformer2DModel`]):\n+            Conditional Transformer (MMDiT) architecture to denoise the encoded image latents.\n+        scheduler ([`FlowMatchEulerDiscreteScheduler`]):\n+            A scheduler to be used in combination with `transformer` to denoise the encoded image latents.\n+        vae ([`AutoencoderKL`]):\n+            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n+        text_encoder ([`Qwen2.5-VL-7B-Instruct`]):\n+            [Qwen2.5-VL-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct), specifically the\n+            [Qwen2.5-VL-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct) variant.\n+        tokenizer (`QwenTokenizer`):\n+            Tokenizer of class\n+            [CLIPTokenizer](https://huggingface.co/docs/transformers/en/model_doc/clip#transformers.CLIPTokenizer).\n+    \"\"\"\n+\n+    model_cpu_offload_seq = \"text_encoder->transformer->vae\"\n+    _callback_tensor_inputs = [\"latents\", \"prompt_embeds\"]\n+\n+    def __init__(\n+        self,\n+        scheduler: FlowMatchEulerDiscreteScheduler,\n+        vae: AutoencoderKLQwenImage,\n+        text_encoder: Qwen2_5_VLForConditionalGeneration,\n+        tokenizer: Qwen2Tokenizer,\n+        processor: Qwen2VLProcessor,\n+        transformer: QwenImageTransformer2DModel,\n+    ):\n+        super().__init__()\n+\n+        self.register_modules(\n+            vae=vae,\n+            text_encoder=text_encoder,\n+            tokenizer=tokenizer,\n+            processor=processor,\n+            transformer=transformer,\n+            scheduler=scheduler,\n+        )\n+        self.vae_scale_factor = 2 ** len(self.vae.temperal_downsample) if getattr(self, \"vae\", None) else 8\n+        self.latent_channels = self.vae.config.z_dim if getattr(self, \"vae\", None) else 16\n+        # QwenImage latents are turned into 2x2 patches and packed. This means the latent width and height has to be divisible\n+        # by the patch size. So the vae scale factor is multiplied by the patch size to account for this\n+        self.image_processor = VaeImageProcessor(vae_scale_factor=self.vae_scale_factor * 2)\n+        self.mask_processor = VaeImageProcessor(\n+            vae_scale_factor=self.vae_scale_factor * 2,\n+            vae_latent_channels=self.latent_channels,\n+            do_normalize=False,\n+            do_binarize=True,\n+            do_convert_grayscale=True,\n+        )\n+        self.vl_processor = processor\n+        self.tokenizer_max_length = 1024\n+\n+        self.prompt_template_encode = \"<|im_start|>system\\nDescribe the key features of the input image (color, shape, size, texture, objects, background), then explain how the user's text instruction should alter or modify the image. Generate a new image that meets the user's requirements while maintaining consistency with the original input where appropriate.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>{}<|im_end|>\\n<|im_start|>assistant\\n\"\n+        self.prompt_template_encode_start_idx = 64\n+        self.default_sample_size = 128\n+\n+    # Copied from diffusers.pipelines.qwenimage.pipeline_qwenimage.QwenImagePipeline._extract_masked_hidden\n+    def _extract_masked_hidden(self, hidden_states: torch.Tensor, mask: torch.Tensor):\n+        bool_mask = mask.bool()\n+        valid_lengths = bool_mask.sum(dim=1)\n+        selected = hidden_states[bool_mask]\n+        split_result = torch.split(selected, valid_lengths.tolist(), dim=0)\n+\n+        return split_result\n+\n+    # Copied from diffusers.pipelines.qwenimage.pipeline_qwenimage_edit.QwenImageEditPipeline._get_qwen_prompt_embeds\n+    def _get_qwen_prompt_embeds(\n+        self,\n+        prompt: Union[str, List[str]] = None,\n+        image: Optional[torch.Tensor] = None,\n+        device: Optional[torch.device] = None,\n+        dtype: Optional[torch.dtype] = None,\n+    ):\n+        device = device or self._execution_device\n+        dtype = dtype or self.text_encoder.dtype\n+\n+        prompt = [prompt] if isinstance(prompt, str) else prompt\n+\n+        template = self.prompt_template_encode\n+        drop_idx = self.prompt_template_encode_start_idx\n+        txt = [template.format(e) for e in prompt]\n+\n+        model_inputs = self.processor(\n+            text=txt,\n+            images=image,\n+            padding=True,\n+            return_tensors=\"pt\",\n+        ).to(device)\n+\n+        outputs = self.text_encoder(\n+            input_ids=model_inputs.input_ids,\n+            attention_mask=model_inputs.attention_mask,\n+            pixel_values=model_inputs.pixel_values,\n+            image_grid_thw=model_inputs.image_grid_thw,\n+            output_hidden_states=True,\n+        )\n+\n+        hidden_states = outputs.hidden_states[-1]\n+        split_hidden_states = self._extract_masked_hidden(hidden_states, model_inputs.attention_mask)\n+        split_hidden_states = [e[drop_idx:] for e in split_hidden_states]\n+        attn_mask_list = [torch.ones(e.size(0), dtype=torch.long, device=e.device) for e in split_hidden_states]\n+        max_seq_len = max([e.size(0) for e in split_hidden_states])\n+        prompt_embeds = torch.stack(\n+            [torch.cat([u, u.new_zeros(max_seq_len - u.size(0), u.size(1))]) for u in split_hidden_states]\n+        )\n+        encoder_attention_mask = torch.stack(\n+            [torch.cat([u, u.new_zeros(max_seq_len - u.size(0))]) for u in attn_mask_list]\n+        )\n+\n+        prompt_embeds = prompt_embeds.to(dtype=dtype, device=device)\n+\n+        return prompt_embeds, encoder_attention_mask\n+\n+    # Copied from diffusers.pipelines.qwenimage.pipeline_qwenimage_edit.QwenImageEditPipeline.encode_prompt\n+    def encode_prompt(\n+        self,\n+        prompt: Union[str, List[str]],\n+        image: Optional[torch.Tensor] = None,\n+        device: Optional[torch.device] = None,\n+        num_images_per_prompt: int = 1,\n+        prompt_embeds: Optional[torch.Tensor] = None,\n+        prompt_embeds_mask: Optional[torch.Tensor] = None,\n+        max_sequence_length: int = 1024,\n+    ):\n+        r\"\"\"\n+\n+        Args:\n+            prompt (`str` or `List[str]`, *optional*):\n+                prompt to be encoded\n+            image (`torch.Tensor`, *optional*):\n+                image to be encoded\n+            device: (`torch.device`):\n+                torch device\n+            num_images_per_prompt (`int`):\n+                number of images that should be generated per prompt\n+            prompt_embeds (`torch.Tensor`, *optional*):\n+                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n+                provided, text embeddings will be generated from `prompt` input argument.\n+        \"\"\"\n+        device = device or self._execution_device\n+\n+        prompt = [prompt] if isinstance(prompt, str) else prompt\n+        batch_size = len(prompt) if prompt_embeds is None else prompt_embeds.shape[0]\n+\n+        if prompt_embeds is None:\n+            prompt_embeds, prompt_embeds_mask = self._get_qwen_prompt_embeds(prompt, image, device)\n+\n+        _, seq_len, _ = prompt_embeds.shape\n+        prompt_embeds = prompt_embeds.repeat(1, num_images_per_prompt, 1)\n+        prompt_embeds = prompt_embeds.view(batch_size * num_images_per_prompt, seq_len, -1)\n+        prompt_embeds_mask = prompt_embeds_mask.repeat(1, num_images_per_prompt, 1)\n+        prompt_embeds_mask = prompt_embeds_mask.view(batch_size * num_images_per_prompt, seq_len)\n+\n+        return prompt_embeds, prompt_embeds_mask\n+\n+    # Copied from diffusers.pipelines.qwenimage.pipeline_qwenimage_inpaint.QwenImageInpaintPipeline.check_inputs\n+    def check_inputs(\n+        self,\n+        prompt,\n+        image,\n+        mask_image,\n+        strength,\n+        height,\n+        width,\n+        output_type,\n+        negative_prompt=None,\n+        prompt_embeds=None,\n+        negative_prompt_embeds=None,\n+        prompt_embeds_mask=None,\n+        negative_prompt_embeds_mask=None,\n+        callback_on_step_end_tensor_inputs=None,\n+        padding_mask_crop=None,\n+        max_sequence_length=None,\n+    ):\n+        if strength < 0 or strength > 1:\n+            raise ValueError(f\"The value of strength should in [0.0, 1.0] but is {strength}\")\n+\n+        if height % (self.vae_scale_factor * 2) != 0 or width % (self.vae_scale_factor * 2) != 0:\n+            logger.warning(\n+                f\"`height` and `width` have to be divisible by {self.vae_scale_factor * 2} but are {height} and {width}. Dimensions will be resized accordingly\"\n+            )\n+\n+        if callback_on_step_end_tensor_inputs is not None and not all(\n+            k in self._callback_tensor_inputs for k in callback_on_step_end_tensor_inputs\n+        ):\n+            raise ValueError(\n+                f\"`callback_on_step_end_tensor_inputs` has to be in {self._callback_tensor_inputs}, but found {[k for k in callback_on_step_end_tensor_inputs if k not in self._callback_tensor_inputs]}\"\n+            )\n+\n+        if prompt is not None and prompt_embeds is not None:\n+            raise ValueError(\n+                f\"Cannot forward both `prompt`: {prompt} and `prompt_embeds`: {prompt_embeds}. Please make sure to\"\n+                \" only forward one of the two.\"\n+            )\n+        elif prompt is None and prompt_embeds is None:\n+            raise ValueError(\n+                \"Provide either `prompt` or `prompt_embeds`. Cannot leave both `prompt` and `prompt_embeds` undefined.\"\n+            )\n+        elif prompt is not None and (not isinstance(prompt, str) and not isinstance(prompt, list)):\n+            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n+\n+        if negative_prompt is not None and negative_prompt_embeds is not None:\n+            raise ValueError(\n+                f\"Cannot forward both `negative_prompt`: {negative_prompt} and `negative_prompt_embeds`:\"\n+                f\" {negative_prompt_embeds}. Please make sure to only forward one of the two.\"\n+            )\n+\n+        if prompt_embeds is not None and prompt_embeds_mask is None:\n+            raise ValueError(\n+                \"If `prompt_embeds` are provided, `prompt_embeds_mask` also have to be passed. Make sure to generate `prompt_embeds_mask` from the same text encoder that was used to generate `prompt_embeds`.\"\n+            )\n+        if negative_prompt_embeds is not None and negative_prompt_embeds_mask is None:\n+            raise ValueError(\n+                \"If `negative_prompt_embeds` are provided, `negative_prompt_embeds_mask` also have to be passed. Make sure to generate `negative_prompt_embeds_mask` from the same text encoder that was used to generate `negative_prompt_embeds`.\"\n+            )\n+        if padding_mask_crop is not None:\n+            if not isinstance(image, PIL.Image.Image):\n+                raise ValueError(\n+                    f\"The image should be a PIL image when inpainting mask crop, but is of type {type(image)}.\"\n+                )\n+            if not isinstance(mask_image, PIL.Image.Image):\n+                raise ValueError(\n+                    f\"The mask image should be a PIL image when inpainting mask crop, but is of type\"\n+                    f\" {type(mask_image)}.\"\n+                )\n+            if output_type != \"pil\":\n+                raise ValueError(f\"The output type should be PIL when inpainting mask crop, but is {output_type}.\")\n+\n+        if max_sequence_length is not None and max_sequence_length > 1024:\n+            raise ValueError(f\"`max_sequence_length` cannot be greater than 1024 but is {max_sequence_length}\")\n+\n+    @staticmethod\n+    # Copied from diffusers.pipelines.qwenimage.pipeline_qwenimage.QwenImagePipeline._pack_latents\n+    def _pack_latents(latents, batch_size, num_channels_latents, height, width):\n+        latents = latents.view(batch_size, num_channels_latents, height // 2, 2, width // 2, 2)\n+        latents = latents.permute(0, 2, 4, 1, 3, 5)\n+        latents = latents.reshape(batch_size, (height // 2) * (width // 2), num_channels_latents * 4)\n+\n+        return latents\n+\n+    @staticmethod\n+    # Copied from diffusers.pipelines.qwenimage.pipeline_qwenimage.QwenImagePipeline._unpack_latents\n+    def _unpack_latents(latents, height, width, vae_scale_factor):\n+        batch_size, num_patches, channels = latents.shape\n+\n+        # VAE applies 8x compression on images but we must also account for packing which requires\n+        # latent height and width to be divisible by 2.\n+        height = 2 * (int(height) // (vae_scale_factor * 2))\n+        width = 2 * (int(width) // (vae_scale_factor * 2))\n+\n+        latents = latents.view(batch_size, height // 2, width // 2, channels // 4, 2, 2)\n+        latents = latents.permute(0, 3, 1, 4, 2, 5)\n+\n+        latents = latents.reshape(batch_size, channels // (2 * 2), 1, height, width)\n+\n+        return latents\n+\n+    # Copied from diffusers.pipelines.qwenimage.pipeline_qwenimage_img2img.QwenImageImg2ImgPipeline._encode_vae_image\n+    def _encode_vae_image(self, image: torch.Tensor, generator: torch.Generator):\n+        if isinstance(generator, list):\n+            image_latents = [\n+                retrieve_latents(self.vae.encode(image[i : i + 1]), generator=generator[i])\n+                for i in range(image.shape[0])\n+            ]\n+            image_latents = torch.cat(image_latents, dim=0)\n+        else:\n+            image_latents = retrieve_latents(self.vae.encode(image), generator=generator)\n+\n+        latents_mean = (\n+            torch.tensor(self.vae.config.latents_mean)\n+            .view(1, self.vae.config.z_dim, 1, 1, 1)\n+            .to(image_latents.device, image_latents.dtype)\n+        )\n+        latents_std = 1.0 / torch.tensor(self.vae.config.latents_std).view(1, self.vae.config.z_dim, 1, 1, 1).to(\n+            image_latents.device, image_latents.dtype\n+        )\n+\n+        image_latents = (image_latents - latents_mean) * latents_std\n+\n+        return image_latents\n+\n+    # Copied from diffusers.pipelines.stable_diffusion_3.pipeline_stable_diffusion_3_img2img.StableDiffusion3Img2ImgPipeline.get_timesteps\n+    def get_timesteps(self, num_inference_steps, strength, device):\n+        # get the original timestep using init_timestep\n+        init_timestep = min(num_inference_steps * strength, num_inference_steps)\n+\n+        t_start = int(max(num_inference_steps - init_timestep, 0))\n+        timesteps = self.scheduler.timesteps[t_start * self.scheduler.order :]\n+        if hasattr(self.scheduler, \"set_begin_index\"):\n+            self.scheduler.set_begin_index(t_start * self.scheduler.order)\n+\n+        return timesteps, num_inference_steps - t_start\n+\n+    def enable_vae_slicing(self):\n+        r\"\"\"\n+        Enable sliced VAE decoding. When this option is enabled, the VAE will split the input tensor in slices to\n+        compute decoding in several steps. This is useful to save some memory and allow larger batch sizes.\n+        \"\"\"\n+        self.vae.enable_slicing()\n+\n+    def disable_vae_slicing(self):\n+        r\"\"\"\n+        Disable sliced VAE decoding. If `enable_vae_slicing` was previously enabled, this method will go back to\n+        computing decoding in one step.\n+        \"\"\"\n+        self.vae.disable_slicing()\n+\n+    def enable_vae_tiling(self):\n+        r\"\"\"\n+        Enable tiled VAE decoding. When this option is enabled, the VAE will split the input tensor into tiles to\n+        compute decoding and encoding in several steps. This is useful for saving a large amount of memory and to allow\n+        processing larger images.\n+        \"\"\"\n+        self.vae.enable_tiling()\n+\n+    def disable_vae_tiling(self):\n+        r\"\"\"\n+        Disable tiled VAE decoding. If `enable_vae_tiling` was previously enabled, this method will go back to\n+        computing decoding in one step.\n+        \"\"\"\n+        self.vae.disable_tiling()\n+\n+    # Copied from diffusers.pipelines.qwenimage.pipeline_qwenimage_inpaint.QwenImageInpaintPipeline.prepare_latents\n+    def prepare_latents(\n+        self,\n+        image,\n+        timestep,\n+        batch_size,\n+        num_channels_latents,\n+        height,\n+        width,\n+        dtype,\n+        device,\n+        generator,\n+        latents=None,\n+    ):\n+        if isinstance(generator, list) and len(generator) != batch_size:\n+            raise ValueError(\n+                f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n+                f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n+            )\n+        # VAE applies 8x compression on images but we must also account for packing which requires\n+        # latent height and width to be divisible by 2.\n+        height = 2 * (int(height) // (self.vae_scale_factor * 2))\n+        width = 2 * (int(width) // (self.vae_scale_factor * 2))\n+\n+        shape = (batch_size, 1, num_channels_latents, height, width)\n+\n+        # If image is [B,C,H,W] -> add T=1. If it's already [B,C,T,H,W], leave it.\n+        if image.dim() == 4:\n+            image = image.unsqueeze(2)\n+        elif image.dim() != 5:\n+            raise ValueError(f\"Expected image dims 4 or 5, got {image.dim()}.\")\n+\n+        if latents is not None:\n+            return latents.to(device=device, dtype=dtype)\n+\n+        image = image.to(device=device, dtype=dtype)\n+        if image.shape[1] != self.latent_channels:\n+            image_latents = self._encode_vae_image(image=image, generator=generator)  # [B,z,1,H',W']\n+        else:\n+            image_latents = image\n+        if batch_size > image_latents.shape[0] and batch_size % image_latents.shape[0] == 0:\n+            # expand init_latents for batch_size\n+            additional_image_per_prompt = batch_size // image_latents.shape[0]\n+            image_latents = torch.cat([image_latents] * additional_image_per_prompt, dim=0)\n+        elif batch_size > image_latents.shape[0] and batch_size % image_latents.shape[0] != 0:\n+            raise ValueError(\n+                f\"Cannot duplicate `image` of batch size {image_latents.shape[0]} to {batch_size} text prompts.\"\n+            )\n+        else:\n+            image_latents = torch.cat([image_latents], dim=0)\n+\n+        image_latents = image_latents.transpose(1, 2)  # [B,1,z,H',W']\n+\n+        if latents is None:\n+            noise = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n+            latents = self.scheduler.scale_noise(image_latents, timestep, noise)\n+        else:\n+            noise = latents.to(device)\n+            latents = noise\n+\n+        noise = self._pack_latents(noise, batch_size, num_channels_latents, height, width)\n+        image_latents = self._pack_latents(image_latents, batch_size, num_channels_latents, height, width)\n+        latents = self._pack_latents(latents, batch_size, num_channels_latents, height, width)\n+\n+        return latents, noise, image_latents\n+\n+    # Copied from diffusers.pipelines.qwenimage.pipeline_qwenimage_inpaint.QwenImageInpaintPipeline.prepare_mask_latents\n+    def prepare_mask_latents(\n+        self,\n+        mask,\n+        masked_image,\n+        batch_size,\n+        num_channels_latents,\n+        num_images_per_prompt,\n+        height,\n+        width,\n+        dtype,\n+        device,\n+        generator,\n+    ):\n+        # VAE applies 8x compression on images but we must also account for packing which requires\n+        # latent height and width to be divisible by 2.\n+        height = 2 * (int(height) // (self.vae_scale_factor * 2))\n+        width = 2 * (int(width) // (self.vae_scale_factor * 2))\n+        # resize the mask to latents shape as we concatenate the mask to the latents\n+        # we do that before converting to dtype to avoid breaking in case we're using cpu_offload\n+        # and half precision\n+        mask = torch.nn.functional.interpolate(mask, size=(height, width))\n+        mask = mask.to(device=device, dtype=dtype)\n+\n+        batch_size = batch_size * num_images_per_prompt\n+\n+        if masked_image.dim() == 4:\n+            masked_image = masked_image.unsqueeze(2)\n+        elif masked_image.dim() != 5:\n+            raise ValueError(f\"Expected image dims 4 or 5, got {masked_image.dim()}.\")\n+\n+        masked_image = masked_image.to(device=device, dtype=dtype)\n+\n+        if masked_image.shape[1] == self.latent_channels:\n+            masked_image_latents = masked_image\n+        else:\n+            masked_image_latents = self._encode_vae_image(image=masked_image, generator=generator)\n+\n+            # duplicate mask and masked_image_latents for each generation per prompt, using mps friendly method\n+        if mask.shape[0] < batch_size:\n+            if not batch_size % mask.shape[0] == 0:\n+                raise ValueError(\n+                    \"The passed mask and the required batch size don't match. Masks are supposed to be duplicated to\"\n+                    f\" a total batch size of {batch_size}, but {mask.shape[0]} masks were passed. Make sure the number\"\n+                    \" of masks that you pass is divisible by the total requested batch size.\"\n+                )\n+            mask = mask.repeat(batch_size // mask.shape[0], 1, 1, 1)\n+        if masked_image_latents.shape[0] < batch_size:\n+            if not batch_size % masked_image_latents.shape[0] == 0:\n+                raise ValueError(\n+                    \"The passed images and the required batch size don't match. Images are supposed to be duplicated\"\n+                    f\" to a total batch size of {batch_size}, but {masked_image_latents.shape[0]} images were passed.\"\n+                    \" Make sure the number of images that you pass is divisible by the total requested batch size.\"\n+                )\n+            masked_image_latents = masked_image_latents.repeat(batch_size // masked_image_latents.shape[0], 1, 1, 1, 1)\n+\n+        # aligning device to prevent device errors when concating it with the latent model input\n+        masked_image_latents = masked_image_latents.to(device=device, dtype=dtype)\n+\n+        masked_image_latents = self._pack_latents(\n+            masked_image_latents,\n+            batch_size,\n+            num_channels_latents,\n+            height,\n+            width,\n+        )\n+        mask = self._pack_latents(\n+            mask.repeat(1, num_channels_latents, 1, 1),\n+            batch_size,\n+            num_channels_latents,\n+            height,\n+            width,\n+        )\n+\n+        return mask, masked_image_latents\n+\n+    @property\n+    def guidance_scale(self):\n+        return self._guidance_scale\n+\n+    @property\n+    def attention_kwargs(self):\n+        return self._attention_kwargs\n+\n+    @property\n+    def num_timesteps(self):\n+        return self._num_timesteps\n+\n+    @property\n+    def current_timestep(self):\n+        return self._current_timestep\n+\n+    @property\n+    def interrupt(self):\n+        return self._interrupt\n+\n+    @torch.no_grad()\n+    @replace_example_docstring(EXAMPLE_DOC_STRING)\n+    def __call__(\n+        self,\n+        image: Optional[PipelineImageInput] = None,\n+        prompt: Union[str, List[str]] = None,\n+        negative_prompt: Union[str, List[str]] = None,\n+        mask_image: PipelineImageInput = None,\n+        masked_image_latents: PipelineImageInput = None,\n+        true_cfg_scale: float = 4.0,\n+        height: Optional[int] = None,\n+        width: Optional[int] = None,\n+        padding_mask_crop: Optional[int] = None,\n+        strength: float = 0.6,\n+        num_inference_steps: int = 50,\n+        sigmas: Optional[List[float]] = None,\n+        guidance_scale: Optional[float] = None,\n+        num_images_per_prompt: int = 1,\n+        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n+        latents: Optional[torch.Tensor] = None,\n+        prompt_embeds: Optional[torch.Tensor] = None,\n+        prompt_embeds_mask: Optional[torch.Tensor] = None,\n+        negative_prompt_embeds: Optional[torch.Tensor] = None,\n+        negative_prompt_embeds_mask: Optional[torch.Tensor] = None,\n+        output_type: Optional[str] = \"pil\",\n+        return_dict: bool = True,\n+        attention_kwargs: Optional[Dict[str, Any]] = None,\n+        callback_on_step_end: Optional[Callable[[int, int, Dict], None]] = None,\n+        callback_on_step_end_tensor_inputs: List[str] = [\"latents\"],\n+        max_sequence_length: int = 512,\n+    ):\n+        r\"\"\"\n+        Function invoked when calling the pipeline for generation.\n+\n+        Args:\n+            image (`torch.Tensor`, `PIL.Image.Image`, `np.ndarray`, `List[torch.Tensor]`, `List[PIL.Image.Image]`, or `List[np.ndarray]`):\n+                `Image`, numpy array or tensor representing an image batch to be used as the starting point. For both\n+                numpy array and pytorch tensor, the expected value range is between `[0, 1]` If it's a tensor or a list\n+                or tensors, the expected shape should be `(B, C, H, W)` or `(C, H, W)`. If it is a numpy array or a\n+                list of arrays, the expected shape should be `(B, H, W, C)` or `(H, W, C)` It can also accept image\n+                latents as `image`, but if passing latents directly it is not encoded again.\n+            prompt (`str` or `List[str]`, *optional*):\n+                The prompt or prompts to guide the image generation. If not defined, one has to pass `prompt_embeds`.\n+                instead.\n+            negative_prompt (`str` or `List[str]`, *optional*):\n+                The prompt or prompts not to guide the image generation. If not defined, one has to pass\n+                `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `true_cfg_scale` is\n+                not greater than `1`).\n+            true_cfg_scale (`float`, *optional*, defaults to 1.0):\n+                true_cfg_scale (`float`, *optional*, defaults to 1.0): Guidance scale as defined in [Classifier-Free\n+                Diffusion Guidance](https://huggingface.co/papers/2207.12598). `true_cfg_scale` is defined as `w` of\n+                equation 2. of [Imagen Paper](https://huggingface.co/papers/2205.11487). Classifier-free guidance is\n+                enabled by setting `true_cfg_scale > 1` and a provided `negative_prompt`. Higher guidance scale\n+                encourages to generate images that are closely linked to the text `prompt`, usually at the expense of\n+                lower image quality.\n+            mask_image (`torch.Tensor`, `PIL.Image.Image`, `np.ndarray`, `List[torch.Tensor]`, `List[PIL.Image.Image]`, or `List[np.ndarray]`):\n+                `Image`, numpy array or tensor representing an image batch to mask `image`. White pixels in the mask\n+                are repainted while black pixels are preserved. If `mask_image` is a PIL image, it is converted to a\n+                single channel (luminance) before use. If it's a numpy array or pytorch tensor, it should contain one\n+                color channel (L) instead of 3, so the expected shape for pytorch tensor would be `(B, 1, H, W)`, `(B,\n+                H, W)`, `(1, H, W)`, `(H, W)`. And for numpy array would be for `(B, H, W, 1)`, `(B, H, W)`, `(H, W,\n+                1)`, or `(H, W)`.\n+            mask_image_latent (`torch.Tensor`, `List[torch.Tensor]`):\n+                `Tensor` representing an image batch to mask `image` generated by VAE. If not provided, the mask\n+                latents tensor will ge generated by `mask_image`.\n+            height (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n+                The height in pixels of the generated image. This is set to 1024 by default for the best results.\n+            width (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n+                The width in pixels of the generated image. This is set to 1024 by default for the best results.\n+            padding_mask_crop (`int`, *optional*, defaults to `None`):\n+                The size of margin in the crop to be applied to the image and masking. If `None`, no crop is applied to\n+                image and mask_image. If `padding_mask_crop` is not `None`, it will first find a rectangular region\n+                with the same aspect ration of the image and contains all masked area, and then expand that area based\n+                on `padding_mask_crop`. The image and mask_image will then be cropped based on the expanded area before\n+                resizing to the original image size for inpainting. This is useful when the masked area is small while\n+                the image is large and contain information irrelevant for inpainting, such as background.\n+            strength (`float`, *optional*, defaults to 1.0):\n+                Indicates extent to transform the reference `image`. Must be between 0 and 1. `image` is used as a\n+                starting point and more noise is added the higher the `strength`. The number of denoising steps depends\n+                on the amount of noise initially added. When `strength` is 1, added noise is maximum and the denoising\n+                process runs for the full number of iterations specified in `num_inference_steps`. A value of 1\n+                essentially ignores `image`.\n+            num_inference_steps (`int`, *optional*, defaults to 50):\n+                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n+                expense of slower inference.\n+            sigmas (`List[float]`, *optional*):\n+                Custom sigmas to use for the denoising process with schedulers which support a `sigmas` argument in\n+                their `set_timesteps` method. If not defined, the default behavior when `num_inference_steps` is passed\n+                will be used.\n+            guidance_scale (`float`, *optional*, defaults to None):\n+                A guidance scale value for guidance distilled models. Unlike the traditional classifier-free guidance\n+                where the guidance scale is applied during inference through noise prediction rescaling, guidance\n+                distilled models take the guidance scale directly as an input parameter during forward pass. Guidance\n+                scale is enabled by setting `guidance_scale > 1`. Higher guidance scale encourages to generate images\n+                that are closely linked to the text `prompt`, usually at the expense of lower image quality. This\n+                parameter in the pipeline is there to support future guidance-distilled models when they come up. It is\n+                ignored when not using guidance distilled models. To enable traditional classifier-free guidance,\n+                please pass `true_cfg_scale > 1.0` and `negative_prompt` (even an empty negative prompt like \" \" should\n+                enable classifier-free guidance computations).\n+            num_images_per_prompt (`int`, *optional*, defaults to 1):\n+                The number of images to generate per prompt.\n+            generator (`torch.Generator` or `List[torch.Generator]`, *optional*):\n+                One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)\n+                to make generation deterministic.\n+            latents (`torch.Tensor`, *optional*):\n+                Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for image\n+                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents\n+                tensor will be generated by sampling using the supplied random `generator`.\n+            prompt_embeds (`torch.Tensor`, *optional*):\n+                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n+                provided, text embeddings will be generated from `prompt` input argument.\n+            negative_prompt_embeds (`torch.Tensor`, *optional*):\n+                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt\n+                weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt` input\n+                argument.\n+            output_type (`str`, *optional*, defaults to `\"pil\"`):\n+                The output format of the generate image. Choose between\n+                [PIL](https://pillow.readthedocs.io/en/stable/): `PIL.Image.Image` or `np.array`.\n+            return_dict (`bool`, *optional*, defaults to `True`):\n+                Whether or not to return a [`~pipelines.qwenimage.QwenImagePipelineOutput`] instead of a plain tuple.\n+            attention_kwargs (`dict`, *optional*):\n+                A kwargs dictionary that if specified is passed along to the `AttentionProcessor` as defined under\n+                `self.processor` in\n+                [diffusers.models.attention_processor](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).\n+            callback_on_step_end (`Callable`, *optional*):\n+                A function that calls at the end of each denoising steps during the inference. The function is called\n+                with the following arguments: `callback_on_step_end(self: DiffusionPipeline, step: int, timestep: int,\n+                callback_kwargs: Dict)`. `callback_kwargs` will include a list of all tensors as specified by\n+                `callback_on_step_end_tensor_inputs`.\n+            callback_on_step_end_tensor_inputs (`List`, *optional*):\n+                The list of tensor inputs for the `callback_on_step_end` function. The tensors specified in the list\n+                will be passed as `callback_kwargs` argument. You will only be able to include variables listed in the\n+                `._callback_tensor_inputs` attribute of your pipeline class.\n+            max_sequence_length (`int` defaults to 512): Maximum sequence length to use with the `prompt`.\n+\n+        Examples:\n+\n+        Returns:\n+            [`~pipelines.qwenimage.QwenImagePipelineOutput`] or `tuple`:\n+            [`~pipelines.qwenimage.QwenImagePipelineOutput`] if `return_dict` is True, otherwise a `tuple`. When\n+            returning a tuple, the first element is a list with the generated images.\n+        \"\"\"\n+        image_size = image[0].size if isinstance(image, list) else image.size\n+        calculated_width, calculated_height, _ = calculate_dimensions(1024 * 1024, image_size[0] / image_size[1])\n+\n+        # height and width are the same as the calculated height and width\n+        height = calculated_height\n+        width = calculated_width\n+\n+        multiple_of = self.vae_scale_factor * 2\n+        width = width // multiple_of * multiple_of\n+        height = height // multiple_of * multiple_of\n+\n+        # 1. Check inputs. Raise error if not correct\n+        self.check_inputs(\n+            prompt,\n+            image,\n+            mask_image,\n+            strength,\n+            height,\n+            width,\n+            output_type=output_type,\n+            negative_prompt=negative_prompt,\n+            prompt_embeds=prompt_embeds,\n+            negative_prompt_embeds=negative_prompt_embeds,\n+            prompt_embeds_mask=prompt_embeds_mask,\n+            negative_prompt_embeds_mask=negative_prompt_embeds_mask,\n+            callback_on_step_end_tensor_inputs=callback_on_step_end_tensor_inputs,\n+            padding_mask_crop=padding_mask_crop,\n+            max_sequence_length=max_sequence_length,\n+        )\n+\n+        self._guidance_scale = guidance_scale\n+        self._attention_kwargs = attention_kwargs\n+        self._current_timestep = None\n+        self._interrupt = False\n+\n+        # 2. Define call parameters\n+        if prompt is not None and isinstance(prompt, str):\n+            batch_size = 1\n+        elif prompt is not None and isinstance(prompt, list):\n+            batch_size = len(prompt)\n+        else:\n+            batch_size = prompt_embeds.shape[0]\n+\n+        device = self._execution_device\n+        # 3. Preprocess image\n+        if padding_mask_crop is not None:\n+            crops_coords = self.mask_processor.get_crop_region(mask_image, width, height, pad=padding_mask_crop)\n+            resize_mode = \"fill\"\n+        else:\n+            crops_coords = None\n+            resize_mode = \"default\"\n+\n+        if image is not None and not (isinstance(image, torch.Tensor) and image.size(1) == self.latent_channels):\n+            image = self.image_processor.resize(image, calculated_height, calculated_width)\n+            original_image = image\n+            prompt_image = image\n+            image = self.image_processor.preprocess(\n+                image,\n+                height=calculated_height,\n+                width=calculated_width,\n+                crops_coords=crops_coords,\n+                resize_mode=resize_mode,\n+            )\n+            image = image.to(dtype=torch.float32)\n+\n+        has_neg_prompt = negative_prompt is not None or (\n+            negative_prompt_embeds is not None and negative_prompt_embeds_mask is not None\n+        )\n+\n+        if true_cfg_scale > 1 and not has_neg_prompt:\n+            logger.warning(\n+                f\"true_cfg_scale is passed as {true_cfg_scale}, but classifier-free guidance is not enabled since no negative_prompt is provided.\"\n+            )\n+        elif true_cfg_scale <= 1 and has_neg_prompt:\n+            logger.warning(\n+                \" negative_prompt is passed but classifier-free guidance is not enabled since true_cfg_scale <= 1\"\n+            )\n+\n+        do_true_cfg = true_cfg_scale > 1 and has_neg_prompt\n+        prompt_embeds, prompt_embeds_mask = self.encode_prompt(\n+            image=prompt_image,\n+            prompt=prompt,\n+            prompt_embeds=prompt_embeds,\n+            prompt_embeds_mask=prompt_embeds_mask,\n+            device=device,\n+            num_images_per_prompt=num_images_per_prompt,\n+            max_sequence_length=max_sequence_length,\n+        )\n+        if do_true_cfg:\n+            negative_prompt_embeds, negative_prompt_embeds_mask = self.encode_prompt(\n+                image=prompt_image,\n+                prompt=negative_prompt,\n+                prompt_embeds=negative_prompt_embeds,\n+                prompt_embeds_mask=negative_prompt_embeds_mask,\n+                device=device,\n+                num_images_per_prompt=num_images_per_prompt,\n+                max_sequence_length=max_sequence_length,\n+            )\n+\n+        # 4. Prepare timesteps\n+        sigmas = np.linspace(1.0, 1 / num_inference_steps, num_inference_steps) if sigmas is None else sigmas\n+        image_seq_len = (int(height) // self.vae_scale_factor // 2) * (int(width) // self.vae_scale_factor // 2)\n+        mu = calculate_shift(\n+            image_seq_len,\n+            self.scheduler.config.get(\"base_image_seq_len\", 256),\n+            self.scheduler.config.get(\"max_image_seq_len\", 4096),\n+            self.scheduler.config.get(\"base_shift\", 0.5),\n+            self.scheduler.config.get(\"max_shift\", 1.15),\n+        )\n+        timesteps, num_inference_steps = retrieve_timesteps(\n+            self.scheduler,\n+            num_inference_steps,\n+            device,\n+            sigmas=sigmas,\n+            mu=mu,\n+        )\n+\n+        timesteps, num_inference_steps = self.get_timesteps(num_inference_steps, strength, device)\n+\n+        if num_inference_steps < 1:\n+            raise ValueError(\n+                f\"After adjusting the num_inference_steps by strength parameter: {strength}, the number of pipeline\"\n+                f\"steps is {num_inference_steps} which is < 1 and not appropriate for this pipeline.\"\n+            )\n+        latent_timestep = timesteps[:1].repeat(batch_size * num_images_per_prompt)\n+\n+        # 5. Prepare latent variables\n+        num_channels_latents = self.transformer.config.in_channels // 4\n+        latents, noise, image_latents = self.prepare_latents(\n+            image,\n+            latent_timestep,\n+            batch_size * num_images_per_prompt,\n+            num_channels_latents,\n+            height,\n+            width,\n+            prompt_embeds.dtype,\n+            device,\n+            generator,\n+            latents,\n+        )\n+\n+        mask_condition = self.mask_processor.preprocess(\n+            mask_image, height=height, width=width, resize_mode=resize_mode, crops_coords=crops_coords\n+        )\n+\n+        if masked_image_latents is None:\n+            masked_image = image * (mask_condition < 0.5)\n+        else:\n+            masked_image = masked_image_latents\n+\n+        mask, masked_image_latents = self.prepare_mask_latents(\n+            mask_condition,\n+            masked_image,\n+            batch_size,\n+            num_channels_latents,\n+            num_images_per_prompt,\n+            height,\n+            width,\n+            prompt_embeds.dtype,\n+            device,\n+            generator,\n+        )\n+\n+        img_shapes = [\n+            [\n+                (1, height // self.vae_scale_factor // 2, width // self.vae_scale_factor // 2),\n+                (1, calculated_height // self.vae_scale_factor // 2, calculated_width // self.vae_scale_factor // 2),\n+            ]\n+        ] * batch_size\n+\n+        num_warmup_steps = max(len(timesteps) - num_inference_steps * self.scheduler.order, 0)\n+        self._num_timesteps = len(timesteps)\n+\n+        # handle guidance\n+        if self.transformer.config.guidance_embeds and guidance_scale is None:\n+            raise ValueError(\"guidance_scale is required for guidance-distilled model.\")\n+        elif self.transformer.config.guidance_embeds:\n+            guidance = torch.full([1], guidance_scale, device=device, dtype=torch.float32)\n+            guidance = guidance.expand(latents.shape[0])\n+        elif not self.transformer.config.guidance_embeds and guidance_scale is not None:\n+            logger.warning(\n+                f\"guidance_scale is passed as {guidance_scale}, but ignored since the model is not guidance-distilled.\"\n+            )\n+            guidance = None\n+        elif not self.transformer.config.guidance_embeds and guidance_scale is None:\n+            guidance = None\n+\n+        if self.attention_kwargs is None:\n+            self._attention_kwargs = {}\n+\n+        txt_seq_lens = prompt_embeds_mask.sum(dim=1).tolist() if prompt_embeds_mask is not None else None\n+        negative_txt_seq_lens = (\n+            negative_prompt_embeds_mask.sum(dim=1).tolist() if negative_prompt_embeds_mask is not None else None\n+        )\n+\n+        # 6. Denoising loop\n+        with self.progress_bar(total=num_inference_steps) as progress_bar:\n+            for i, t in enumerate(timesteps):\n+                if self.interrupt:\n+                    continue\n+\n+                self._current_timestep = t\n+\n+                latent_model_input = latents\n+                if image_latents is not None:\n+                    latent_model_input = torch.cat([latents, image_latents], dim=1)\n+\n+                # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n+                timestep = t.expand(latents.shape[0]).to(latents.dtype)\n+                with self.transformer.cache_context(\"cond\"):\n+                    noise_pred = self.transformer(\n+                        hidden_states=latent_model_input,\n+                        timestep=timestep / 1000,\n+                        guidance=guidance,\n+                        encoder_hidden_states_mask=prompt_embeds_mask,\n+                        encoder_hidden_states=prompt_embeds,\n+                        img_shapes=img_shapes,\n+                        txt_seq_lens=txt_seq_lens,\n+                        attention_kwargs=self.attention_kwargs,\n+                        return_dict=False,\n+                    )[0]\n+                    noise_pred = noise_pred[:, : latents.size(1)]\n+\n+                if do_true_cfg:\n+                    with self.transformer.cache_context(\"uncond\"):\n+                        neg_noise_pred = self.transformer(\n+                            hidden_states=latent_model_input,\n+                            timestep=timestep / 1000,\n+                            guidance=guidance,\n+                            encoder_hidden_states_mask=negative_prompt_embeds_mask,\n+                            encoder_hidden_states=negative_prompt_embeds,\n+                            img_shapes=img_shapes,\n+                            txt_seq_lens=negative_txt_seq_lens,\n+                            attention_kwargs=self.attention_kwargs,\n+                            return_dict=False,\n+                        )[0]\n+                    neg_noise_pred = neg_noise_pred[:, : latents.size(1)]\n+                    comb_pred = neg_noise_pred + true_cfg_scale * (noise_pred - neg_noise_pred)\n+\n+                    cond_norm = torch.norm(noise_pred, dim=-1, keepdim=True)\n+                    noise_norm = torch.norm(comb_pred, dim=-1, keepdim=True)\n+                    noise_pred = comb_pred * (cond_norm / noise_norm)\n+\n+                # compute the previous noisy sample x_t -> x_t-1\n+                latents_dtype = latents.dtype\n+                latents = self.scheduler.step(noise_pred, t, latents, return_dict=False)[0]\n+\n+                # for 64 channel transformer only.\n+                init_latents_proper = image_latents\n+                init_mask = mask\n+\n+                if i < len(timesteps) - 1:\n+                    noise_timestep = timesteps[i + 1]\n+                    init_latents_proper = self.scheduler.scale_noise(\n+                        init_latents_proper, torch.tensor([noise_timestep]), noise\n+                    )\n+\n+                latents = (1 - init_mask) * init_latents_proper + init_mask * latents\n+\n+                if latents.dtype != latents_dtype:\n+                    if torch.backends.mps.is_available():\n+                        # some platforms (eg. apple mps) misbehave due to a pytorch bug: https://github.com/pytorch/pytorch/pull/99272\n+                        latents = latents.to(latents_dtype)\n+\n+                if callback_on_step_end is not None:\n+                    callback_kwargs = {}\n+                    for k in callback_on_step_end_tensor_inputs:\n+                        callback_kwargs[k] = locals()[k]\n+                    callback_outputs = callback_on_step_end(self, i, t, callback_kwargs)\n+\n+                    latents = callback_outputs.pop(\"latents\", latents)\n+                    prompt_embeds = callback_outputs.pop(\"prompt_embeds\", prompt_embeds)\n+\n+                # call the callback, if provided\n+                if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n+                    progress_bar.update()\n+\n+                if XLA_AVAILABLE:\n+                    xm.mark_step()\n+\n+        self._current_timestep = None\n+        if output_type == \"latent\":\n+            image = latents\n+        else:\n+            latents = self._unpack_latents(latents, height, width, self.vae_scale_factor)\n+            latents = latents.to(self.vae.dtype)\n+            latents_mean = (\n+                torch.tensor(self.vae.config.latents_mean)\n+                .view(1, self.vae.config.z_dim, 1, 1, 1)\n+                .to(latents.device, latents.dtype)\n+            )\n+            latents_std = 1.0 / torch.tensor(self.vae.config.latents_std).view(1, self.vae.config.z_dim, 1, 1, 1).to(\n+                latents.device, latents.dtype\n+            )\n+            latents = latents / latents_std + latents_mean\n+            image = self.vae.decode(latents, return_dict=False)[0][:, :, 0]\n+            image = self.image_processor.postprocess(image, output_type=output_type)\n+\n+            if padding_mask_crop is not None:\n+                image = [\n+                    self.image_processor.apply_overlay(mask_image, original_image, i, crops_coords) for i in image\n+                ]\n+\n+        # Offload all models\n+        self.maybe_free_model_hooks()\n+\n+        if not return_dict:\n+            return (image,)\n+\n+        return QwenImagePipelineOutput(images=image)"
        },
        {
          "filename": "src/diffusers/utils/dummy_torch_and_transformers_objects.py",
          "status": "modified",
          "additions": 15,
          "deletions": 0,
          "changes": 15,
          "patch": "@@ -1772,6 +1772,21 @@ def from_pretrained(cls, *args, **kwargs):\n         requires_backends(cls, [\"torch\", \"transformers\"])\n \n \n+class QwenImageEditInpaintPipeline(metaclass=DummyObject):\n+    _backends = [\"torch\", \"transformers\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_config(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_pretrained(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+\n class QwenImageEditPipeline(metaclass=DummyObject):\n     _backends = [\"torch\", \"transformers\"]\n "
        }
      ],
      "num_files": 6,
      "scraped_at": "2025-11-16T21:19:44.096223"
    },
    {
      "pr_number": 12223,
      "title": "[Qwen-Image] adding validation for guidance_scale, true_cfg_scale and negative_prompt",
      "body": "This PR adds the parameter validation for guidance_scale/true_cfg_scale/negative_prompt and try to improve user experience for the QwenImagePipeline\r\n\r\n\r\n### Behavior for Non-guidance-distilled models:\r\n* guidance_scale defaults to `None`, it will be ignored with warning if provided\r\n* CFG enabled only when `negative_prompt` provided (by default not provided) AND `true_cfg_scale > 1` (defaults to be `4.0`)\r\n* Warns if `true_cfg_scale > 1` but no` negative_prompt`\r\n* Warns if `negative_prompt` provided but `true_cfg_scale <= 1`\r\n\r\ntest script \r\n\r\n```py\r\nimport os\r\nimport torch\r\n\r\nfrom diffusers import QwenImagePipeline\r\n\r\npipeline = QwenImagePipeline.from_pretrained(\"Qwen/Qwen-Image\")\r\npipeline.to(torch.bfloat16)\r\npipeline.enable_model_cpu_offload(device=\"cuda:0\")\r\n\r\nprompt = \"\u73b0\u5b9e\u4e3b\u4e49\u98ce\u683c\u7684\u4eba\u50cf\u6444\u5f71\u4f5c\u54c1\uff0c\u753b\u9762\u4e3b\u4f53\u662f\u4e00\u4f4d\u5bb9\u8c8c\u60ca\u8273\u7684\u5973\u6027\u9762\u90e8\u7279\u5199\u3002\u5979\u62e5\u6709\u4e00\u5934\u81ea\u7136\u5fae\u5377\u7684\u77ed\u53d1\uff0c\u53d1\u4e1d\u6839\u6839\u5206\u660e\uff0c\u84ec\u677e\u7684\u5218\u6d77\u4fee\u9970\u7740\u989d\u5934\uff0c\u589e\u6dfb\u4fcf\u76ae\u611f\u3002\u5934\u4e0a\u4f69\u6234\u4e00\u9876\u7eff\u8272\u683c\u5b50\u857e\u4e1d\u8fb9\u5934\u5dfe\uff0c\u589e\u6dfb\u590d\u53e4\u4e0e\u67d4\u7f8e\u6c14\u606f\u3002\u8eab\u7740\u4e00\u4ef6\u7b80\u7ea6\u7eff\u8272\u80cc\u5fc3\u88d9\uff0c\u5728\u7eaf\u767d\u8272\u80cc\u666f\u4e0b\u683c\u5916\u7a81\u51fa\u3002\u4e24\u53ea\u624b\u5206\u522b\u63e1\u7740\u534a\u4e2a\u7ea2\u8272\u6843\u5b50\uff0c\u53cc\u624b\u8f7b\u8f7b\u8d34\u5728\u8138\u988a\u4e24\u4fa7\uff0c\u8425\u9020\u51fa\u53ef\u7231\u53c8\u5bcc\u6709\u521b\u610f\u7684\u89c6\u89c9\u6548\u679c\u3002  \u4eba\u7269\u8868\u60c5\u751f\u52a8\uff0c\u4e00\u53ea\u773c\u775b\u7741\u5f00\uff0c\u53e6\u4e00\u53ea\u5fae\u5fae\u95ed\u5408\uff0c\u5c55\u73b0\u51fa\u8c03\u76ae\u4e0e\u81ea\u4fe1\u7684\u795e\u6001\u3002\u6574\u4f53\u6784\u56fe\u91c7\u7528\u4e2a\u6027\u89c6\u89d2\u3001\u975e\u5bf9\u79f0\u6784\u56fe\uff0c\u805a\u7126\u4eba\u7269\u4e3b\u4f53\uff0c\u589e\u5f3a\u73b0\u573a\u611f\u548c\u65e2\u89c6\u611f\u3002\u80cc\u666f\u865a\u5316\u5904\u7406\uff0c\u5c42\u6b21\u4e30\u5bcc\uff0c\u666f\u6df1\u6548\u679c\u5f3a\u70c8\uff0c\u8425\u9020\u51fa\u4f4e\u5149\u6c1b\u56f4\u4e0b\u6d53\u539a\u7684\u60c5\u7eea\u5f20\u529b\u3002  \u753b\u9762\u7ec6\u8282\u7cbe\u81f4\uff0c\u8272\u5f69\u751f\u52a8\u9971\u6ee1\u5374\u4e0d\u5931\u67d4\u548c\uff0c\u5448\u73b0\u51fa\u5bcc\u58eb\u80f6\u7247\u72ec\u6709\u7684\u6e29\u6da6\u8d28\u611f\u3002\u5149\u5f71\u8fd0\u7528\u5145\u6ee1\u7f8e\u5b66\u5f20\u529b\uff0c\u5e26\u6709\u8f7b\u5fae\u8d85\u73b0\u5b9e\u7684\u5149\u6548\u5904\u7406\uff0c\u63d0\u5347\u6574\u4f53\u753b\u9762\u9ad8\u7ea7\u611f\u3002\u6574\u4f53\u98ce\u683c\u4e3a\u73b0\u5b9e\u4e3b\u4e49\u4eba\u50cf\u6444\u5f71\uff0c\u5f3a\u8c03\u7ec6\u817b\u7684\u7eb9\u7406\u4e0e\u827a\u672f\u5316\u7684\u5149\u7ebf\u8868\u73b0\uff0c\u582a\u79f0\u4e00\u5e45\u7ec6\u8282\u4e30\u5bcc\u3001\u6c1b\u56f4\u62c9\u6ee1\u7684\u6770\u4f5c\u3002\u8d85\u6e05\uff0c4K\uff0c\u7535\u5f71\u7ea7\u6784\u56fe\"\r\ncommon_inputs = {\r\n    \"prompt\": prompt,\r\n    \"height\": 1328,\r\n    \"width\": 1328,\r\n    \"num_inference_steps\": 50,\r\n}\r\n\r\n# test1: this work without cfg, should raise a warning since by default, true_cfg_scale = 4.0, but no negative_prompt is provided\r\nprint(\"test1:\")\r\ngenerator = torch.Generator(device=\"cuda:0\").manual_seed(0)\r\noutput = pipeline(**common_inputs, generator=generator).images[0]\r\noutput.save(\"yiyi_test_16_output_1_no_cfg.png\")\r\n\r\n# test2: this should work as expected with cfg, default true_cfg_scale = 4.0, and negative_prompt is provided.\r\nprint(\"test2:\")\r\ngenerator = torch.Generator(device=\"cuda:0\").manual_seed(0)\r\noutput = pipeline(**common_inputs, negative_prompt=\" \", generator=generator).images[0]\r\noutput.save(\"yiyi_test_16_output_2_cfg.png\")\r\n\r\n\r\n# test3: this should work as expected without cfg, true_cfg_scale <1 and no negative_prompt is provided\r\nprint(\"test3:\")\r\ngenerator = torch.Generator(device=\"cuda:0\").manual_seed(0)\r\noutput = pipeline(**common_inputs, true_cfg_scale=1.0, generator=generator).images[0]\r\noutput.save(\"yiyi_test_16_output_3_no_cfg.png\")\r\n\r\n# test4: without cfg, but get a warning since negative_prompt is provided.\r\nprint(\"test4:\")\r\ngenerator = torch.Generator(device=\"cuda:0\").manual_seed(0)\r\noutput = pipeline(**common_inputs, true_cfg_scale=1.0, negative_prompt=\" \", generator=generator).images[0]\r\noutput.save(\"yiyi_test_16_output_4_no_cfg.png\")\r\n\r\n# test5: this should get a warning since guidance_scale is passed but it is not a guidance-distilled model.\r\nprint(\"test5:\")\r\ngenerator = torch.Generator(device=\"cuda:0\").manual_seed(0)\r\noutput = pipeline(**common_inputs, negative_prompt=\" \", guidance_scale=1.0, generator=generator).images[0]\r\noutput.save(\"yiyi_test_16_output_5_cfg.png\")\r\n```\r\n\r\noutputs\r\n\r\n```\r\ntest1:\r\ntrue_cfg_scale is passed as 4.0, but classifier-free guidance is not enabled since no negative_prompt is provided.\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50/50 [00:58<00:00,  1.16s/it]\r\ntest2:\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50/50 [01:43<00:00,  2.08s/it]\r\ntest3:\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50/50 [00:58<00:00,  1.16s/it]\r\ntest4:\r\n negative_prompt is passed but classifier-free guidance is not enabled since true_cfg_scale <= 1\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50/50 [00:58<00:00,  1.16s/it]\r\ntest5:\r\nguidance_scale is passed as 1.0, but ignored since the model is not guidance-distilled.\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50/50 [01:43<00:00,  2.08s/it\r\n```\r\n\r\n### Behavior for Guidance-distilled models \r\n\r\n(currently we don't have a guidance-distilled qwen-image checkpoint, but the team might release on in the future)\r\n\r\n* guidance_scale is required (raises ValueError if None)\r\n* Can use both guidance distillation and CFG simultaneously\r\n* Same CFG validation logic as non-distilled models\r\n",
      "html_url": "https://github.com/huggingface/diffusers/pull/12223",
      "created_at": "2025-08-23T05:52:52Z",
      "merged_at": "2025-08-27T11:04:33Z",
      "merge_commit_sha": "865ba102b397b6f761423705142cbf9078d7b6d7",
      "base_ref": "main",
      "head_sha": "1efd106c3b654f8d9eb7d5bbd570d927a1bef0b6",
      "user": "yiyixuxu",
      "files": [
        {
          "filename": "src/diffusers/pipelines/qwenimage/pipeline_qwenimage.py",
          "status": "modified",
          "additions": 36,
          "deletions": 15,
          "changes": 51,
          "patch": "@@ -435,7 +435,7 @@ def __call__(\n         width: Optional[int] = None,\n         num_inference_steps: int = 50,\n         sigmas: Optional[List[float]] = None,\n-        guidance_scale: float = 1.0,\n+        guidance_scale: Optional[float] = None,\n         num_images_per_prompt: int = 1,\n         generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n         latents: Optional[torch.Tensor] = None,\n@@ -462,7 +462,12 @@ def __call__(\n                 `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `true_cfg_scale` is\n                 not greater than `1`).\n             true_cfg_scale (`float`, *optional*, defaults to 1.0):\n-                When > 1.0 and a provided `negative_prompt`, enables true classifier-free guidance.\n+                Guidance scale as defined in [Classifier-Free Diffusion\n+                Guidance](https://huggingface.co/papers/2207.12598). `true_cfg_scale` is defined as `w` of equation 2.\n+                of [Imagen Paper](https://huggingface.co/papers/2205.11487). Classifier-free guidance is enabled by\n+                setting `true_cfg_scale > 1` and a provided `negative_prompt`. Higher guidance scale encourages to\n+                generate images that are closely linked to the text `prompt`, usually at the expense of lower image\n+                quality.\n             height (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n                 The height in pixels of the generated image. This is set to 1024 by default for the best results.\n             width (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n@@ -474,17 +479,16 @@ def __call__(\n                 Custom sigmas to use for the denoising process with schedulers which support a `sigmas` argument in\n                 their `set_timesteps` method. If not defined, the default behavior when `num_inference_steps` is passed\n                 will be used.\n-            guidance_scale (`float`, *optional*, defaults to 3.5):\n-                Guidance scale as defined in [Classifier-Free Diffusion\n-                Guidance](https://huggingface.co/papers/2207.12598). `guidance_scale` is defined as `w` of equation 2.\n-                of [Imagen Paper](https://huggingface.co/papers/2205.11487). Guidance scale is enabled by setting\n-                `guidance_scale > 1`. Higher guidance scale encourages to generate images that are closely linked to\n-                the text `prompt`, usually at the expense of lower image quality.\n-\n-                This parameter in the pipeline is there to support future guidance-distilled models when they come up.\n-                Note that passing `guidance_scale` to the pipeline is ineffective. To enable classifier-free guidance,\n-                please pass `true_cfg_scale` and `negative_prompt` (even an empty negative prompt like \" \") should\n-                enable classifier-free guidance computations.\n+            guidance_scale (`float`, *optional*, defaults to None):\n+                A guidance scale value for guidance distilled models. Unlike the traditional classifier-free guidance\n+                where the guidance scale is applied during inference through noise prediction rescaling, guidance\n+                distilled models take the guidance scale directly as an input parameter during forward pass. Guidance\n+                scale is enabled by setting `guidance_scale > 1`. Higher guidance scale encourages to generate images\n+                that are closely linked to the text `prompt`, usually at the expense of lower image quality. This\n+                parameter in the pipeline is there to support future guidance-distilled models when they come up. It is\n+                ignored when not using guidance distilled models. To enable traditional classifier-free guidance,\n+                please pass `true_cfg_scale > 1.0` and `negative_prompt` (even an empty negative prompt like \" \" should\n+                enable classifier-free guidance computations).\n             num_images_per_prompt (`int`, *optional*, defaults to 1):\n                 The number of images to generate per prompt.\n             generator (`torch.Generator` or `List[torch.Generator]`, *optional*):\n@@ -564,6 +568,16 @@ def __call__(\n         has_neg_prompt = negative_prompt is not None or (\n             negative_prompt_embeds is not None and negative_prompt_embeds_mask is not None\n         )\n+\n+        if true_cfg_scale > 1 and not has_neg_prompt:\n+            logger.warning(\n+                f\"true_cfg_scale is passed as {true_cfg_scale}, but classifier-free guidance is not enabled since no negative_prompt is provided.\"\n+            )\n+        elif true_cfg_scale <= 1 and has_neg_prompt:\n+            logger.warning(\n+                \" negative_prompt is passed but classifier-free guidance is not enabled since true_cfg_scale <= 1\"\n+            )\n+\n         do_true_cfg = true_cfg_scale > 1 and has_neg_prompt\n         prompt_embeds, prompt_embeds_mask = self.encode_prompt(\n             prompt=prompt,\n@@ -618,10 +632,17 @@ def __call__(\n         self._num_timesteps = len(timesteps)\n \n         # handle guidance\n-        if self.transformer.config.guidance_embeds:\n+        if self.transformer.config.guidance_embeds and guidance_scale is None:\n+            raise ValueError(\"guidance_scale is required for guidance-distilled model.\")\n+        elif self.transformer.config.guidance_embeds:\n             guidance = torch.full([1], guidance_scale, device=device, dtype=torch.float32)\n             guidance = guidance.expand(latents.shape[0])\n-        else:\n+        elif not self.transformer.config.guidance_embeds and guidance_scale is not None:\n+            logger.warning(\n+                f\"guidance_scale is passed as {guidance_scale}, but ignored since the model is not guidance-distilled.\"\n+            )\n+            guidance = None\n+        elif not self.transformer.config.guidance_embeds and guidance_scale is None:\n             guidance = None\n \n         if self.attention_kwargs is None:"
        },
        {
          "filename": "src/diffusers/pipelines/qwenimage/pipeline_qwenimage_controlnet.py",
          "status": "modified",
          "additions": 36,
          "deletions": 10,
          "changes": 46,
          "patch": "@@ -535,7 +535,7 @@ def __call__(\n         width: Optional[int] = None,\n         num_inference_steps: int = 50,\n         sigmas: Optional[List[float]] = None,\n-        guidance_scale: float = 1.0,\n+        guidance_scale: Optional[float] = None,\n         control_guidance_start: Union[float, List[float]] = 0.0,\n         control_guidance_end: Union[float, List[float]] = 1.0,\n         control_image: PipelineImageInput = None,\n@@ -566,7 +566,12 @@ def __call__(\n                 `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `true_cfg_scale` is\n                 not greater than `1`).\n             true_cfg_scale (`float`, *optional*, defaults to 1.0):\n-                When > 1.0 and a provided `negative_prompt`, enables true classifier-free guidance.\n+                Guidance scale as defined in [Classifier-Free Diffusion\n+                Guidance](https://huggingface.co/papers/2207.12598). `true_cfg_scale` is defined as `w` of equation 2.\n+                of [Imagen Paper](https://huggingface.co/papers/2205.11487). Classifier-free guidance is enabled by\n+                setting `true_cfg_scale > 1` and a provided `negative_prompt`. Higher guidance scale encourages to\n+                generate images that are closely linked to the text `prompt`, usually at the expense of lower image\n+                quality.\n             height (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n                 The height in pixels of the generated image. This is set to 1024 by default for the best results.\n             width (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n@@ -578,12 +583,16 @@ def __call__(\n                 Custom sigmas to use for the denoising process with schedulers which support a `sigmas` argument in\n                 their `set_timesteps` method. If not defined, the default behavior when `num_inference_steps` is passed\n                 will be used.\n-            guidance_scale (`float`, *optional*, defaults to 3.5):\n-                Guidance scale as defined in [Classifier-Free Diffusion\n-                Guidance](https://huggingface.co/papers/2207.12598). `guidance_scale` is defined as `w` of equation 2.\n-                of [Imagen Paper](https://huggingface.co/papers/2205.11487). Guidance scale is enabled by setting\n-                `guidance_scale > 1`. Higher guidance scale encourages to generate images that are closely linked to\n-                the text `prompt`, usually at the expense of lower image quality.\n+            guidance_scale (`float`, *optional*, defaults to None):\n+                A guidance scale value for guidance distilled models. Unlike the traditional classifier-free guidance\n+                where the guidance scale is applied during inference through noise prediction rescaling, guidance\n+                distilled models take the guidance scale directly as an input parameter during forward pass. Guidance\n+                scale is enabled by setting `guidance_scale > 1`. Higher guidance scale encourages to generate images\n+                that are closely linked to the text `prompt`, usually at the expense of lower image quality. This\n+                parameter in the pipeline is there to support future guidance-distilled models when they come up. It is\n+                ignored when not using guidance distilled models. To enable traditional classifier-free guidance,\n+                please pass `true_cfg_scale > 1.0` and `negative_prompt` (even an empty negative prompt like \" \" should\n+                enable classifier-free guidance computations).\n             num_images_per_prompt (`int`, *optional*, defaults to 1):\n                 The number of images to generate per prompt.\n             generator (`torch.Generator` or `List[torch.Generator]`, *optional*):\n@@ -674,6 +683,16 @@ def __call__(\n         has_neg_prompt = negative_prompt is not None or (\n             negative_prompt_embeds is not None and negative_prompt_embeds_mask is not None\n         )\n+\n+        if true_cfg_scale > 1 and not has_neg_prompt:\n+            logger.warning(\n+                f\"true_cfg_scale is passed as {true_cfg_scale}, but classifier-free guidance is not enabled since no negative_prompt is provided.\"\n+            )\n+        elif true_cfg_scale <= 1 and has_neg_prompt:\n+            logger.warning(\n+                \" negative_prompt is passed but classifier-free guidance is not enabled since true_cfg_scale <= 1\"\n+            )\n+\n         do_true_cfg = true_cfg_scale > 1 and has_neg_prompt\n         prompt_embeds, prompt_embeds_mask = self.encode_prompt(\n             prompt=prompt,\n@@ -822,10 +841,17 @@ def __call__(\n             controlnet_keep.append(keeps[0] if isinstance(self.controlnet, QwenImageControlNetModel) else keeps)\n \n         # handle guidance\n-        if self.transformer.config.guidance_embeds:\n+        if self.transformer.config.guidance_embeds and guidance_scale is None:\n+            raise ValueError(\"guidance_scale is required for guidance-distilled model.\")\n+        elif self.transformer.config.guidance_embeds:\n             guidance = torch.full([1], guidance_scale, device=device, dtype=torch.float32)\n             guidance = guidance.expand(latents.shape[0])\n-        else:\n+        elif not self.transformer.config.guidance_embeds and guidance_scale is not None:\n+            logger.warning(\n+                f\"guidance_scale is passed as {guidance_scale}, but ignored since the model is not guidance-distilled.\"\n+            )\n+            guidance = None\n+        elif not self.transformer.config.guidance_embeds and guidance_scale is None:\n             guidance = None\n \n         if self.attention_kwargs is None:"
        },
        {
          "filename": "src/diffusers/pipelines/qwenimage/pipeline_qwenimage_edit.py",
          "status": "modified",
          "additions": 36,
          "deletions": 15,
          "changes": 51,
          "patch": "@@ -532,7 +532,7 @@ def __call__(\n         width: Optional[int] = None,\n         num_inference_steps: int = 50,\n         sigmas: Optional[List[float]] = None,\n-        guidance_scale: float = 1.0,\n+        guidance_scale: Optional[float] = None,\n         num_images_per_prompt: int = 1,\n         generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n         latents: Optional[torch.Tensor] = None,\n@@ -559,7 +559,12 @@ def __call__(\n                 `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `true_cfg_scale` is\n                 not greater than `1`).\n             true_cfg_scale (`float`, *optional*, defaults to 1.0):\n-                When > 1.0 and a provided `negative_prompt`, enables true classifier-free guidance.\n+                true_cfg_scale (`float`, *optional*, defaults to 1.0): Guidance scale as defined in [Classifier-Free\n+                Diffusion Guidance](https://huggingface.co/papers/2207.12598). `true_cfg_scale` is defined as `w` of\n+                equation 2. of [Imagen Paper](https://huggingface.co/papers/2205.11487). Classifier-free guidance is\n+                enabled by setting `true_cfg_scale > 1` and a provided `negative_prompt`. Higher guidance scale\n+                encourages to generate images that are closely linked to the text `prompt`, usually at the expense of\n+                lower image quality.\n             height (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n                 The height in pixels of the generated image. This is set to 1024 by default for the best results.\n             width (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n@@ -571,17 +576,16 @@ def __call__(\n                 Custom sigmas to use for the denoising process with schedulers which support a `sigmas` argument in\n                 their `set_timesteps` method. If not defined, the default behavior when `num_inference_steps` is passed\n                 will be used.\n-            guidance_scale (`float`, *optional*, defaults to 3.5):\n-                Guidance scale as defined in [Classifier-Free Diffusion\n-                Guidance](https://huggingface.co/papers/2207.12598). `guidance_scale` is defined as `w` of equation 2.\n-                of [Imagen Paper](https://huggingface.co/papers/2205.11487). Guidance scale is enabled by setting\n-                `guidance_scale > 1`. Higher guidance scale encourages to generate images that are closely linked to\n-                the text `prompt`, usually at the expense of lower image quality.\n-\n-                This parameter in the pipeline is there to support future guidance-distilled models when they come up.\n-                Note that passing `guidance_scale` to the pipeline is ineffective. To enable classifier-free guidance,\n-                please pass `true_cfg_scale` and `negative_prompt` (even an empty negative prompt like \" \") should\n-                enable classifier-free guidance computations.\n+            guidance_scale (`float`, *optional*, defaults to None):\n+                A guidance scale value for guidance distilled models. Unlike the traditional classifier-free guidance\n+                where the guidance scale is applied during inference through noise prediction rescaling, guidance\n+                distilled models take the guidance scale directly as an input parameter during forward pass. Guidance\n+                scale is enabled by setting `guidance_scale > 1`. Higher guidance scale encourages to generate images\n+                that are closely linked to the text `prompt`, usually at the expense of lower image quality. This\n+                parameter in the pipeline is there to support future guidance-distilled models when they come up. It is\n+                ignored when not using guidance distilled models. To enable traditional classifier-free guidance,\n+                please pass `true_cfg_scale > 1.0` and `negative_prompt` (even an empty negative prompt like \" \" should\n+                enable classifier-free guidance computations).\n             num_images_per_prompt (`int`, *optional*, defaults to 1):\n                 The number of images to generate per prompt.\n             generator (`torch.Generator` or `List[torch.Generator]`, *optional*):\n@@ -672,6 +676,16 @@ def __call__(\n         has_neg_prompt = negative_prompt is not None or (\n             negative_prompt_embeds is not None and negative_prompt_embeds_mask is not None\n         )\n+\n+        if true_cfg_scale > 1 and not has_neg_prompt:\n+            logger.warning(\n+                f\"true_cfg_scale is passed as {true_cfg_scale}, but classifier-free guidance is not enabled since no negative_prompt is provided.\"\n+            )\n+        elif true_cfg_scale <= 1 and has_neg_prompt:\n+            logger.warning(\n+                \" negative_prompt is passed but classifier-free guidance is not enabled since true_cfg_scale <= 1\"\n+            )\n+\n         do_true_cfg = true_cfg_scale > 1 and has_neg_prompt\n         prompt_embeds, prompt_embeds_mask = self.encode_prompt(\n             image=prompt_image,\n@@ -734,10 +748,17 @@ def __call__(\n         self._num_timesteps = len(timesteps)\n \n         # handle guidance\n-        if self.transformer.config.guidance_embeds:\n+        if self.transformer.config.guidance_embeds and guidance_scale is None:\n+            raise ValueError(\"guidance_scale is required for guidance-distilled model.\")\n+        elif self.transformer.config.guidance_embeds:\n             guidance = torch.full([1], guidance_scale, device=device, dtype=torch.float32)\n             guidance = guidance.expand(latents.shape[0])\n-        else:\n+        elif not self.transformer.config.guidance_embeds and guidance_scale is not None:\n+            logger.warning(\n+                f\"guidance_scale is passed as {guidance_scale}, but ignored since the model is not guidance-distilled.\"\n+            )\n+            guidance = None\n+        elif not self.transformer.config.guidance_embeds and guidance_scale is None:\n             guidance = None\n \n         if self.attention_kwargs is None:"
        },
        {
          "filename": "src/diffusers/pipelines/qwenimage/pipeline_qwenimage_img2img.py",
          "status": "modified",
          "additions": 36,
          "deletions": 15,
          "changes": 51,
          "patch": "@@ -511,7 +511,7 @@ def __call__(\n         strength: float = 0.6,\n         num_inference_steps: int = 50,\n         sigmas: Optional[List[float]] = None,\n-        guidance_scale: float = 1.0,\n+        guidance_scale: Optional[float] = None,\n         num_images_per_prompt: int = 1,\n         generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n         latents: Optional[torch.Tensor] = None,\n@@ -544,7 +544,12 @@ def __call__(\n                 list of arrays, the expected shape should be `(B, H, W, C)` or `(H, W, C)` It can also accept image\n                 latents as `image`, but if passing latents directly it is not encoded again.\n             true_cfg_scale (`float`, *optional*, defaults to 1.0):\n-                When > 1.0 and a provided `negative_prompt`, enables true classifier-free guidance.\n+                Guidance scale as defined in [Classifier-Free Diffusion\n+                Guidance](https://huggingface.co/papers/2207.12598). `true_cfg_scale` is defined as `w` of equation 2.\n+                of [Imagen Paper](https://huggingface.co/papers/2205.11487). Classifier-free guidance is enabled by\n+                setting `true_cfg_scale > 1` and a provided `negative_prompt`. Higher guidance scale encourages to\n+                generate images that are closely linked to the text `prompt`, usually at the expense of lower image\n+                quality.\n             height (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n                 The height in pixels of the generated image. This is set to 1024 by default for the best results.\n             width (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n@@ -562,17 +567,16 @@ def __call__(\n                 Custom sigmas to use for the denoising process with schedulers which support a `sigmas` argument in\n                 their `set_timesteps` method. If not defined, the default behavior when `num_inference_steps` is passed\n                 will be used.\n-            guidance_scale (`float`, *optional*, defaults to 3.5):\n-                Guidance scale as defined in [Classifier-Free Diffusion\n-                Guidance](https://huggingface.co/papers/2207.12598). `guidance_scale` is defined as `w` of equation 2.\n-                of [Imagen Paper](https://huggingface.co/papers/2205.11487). Guidance scale is enabled by setting\n-                `guidance_scale > 1`. Higher guidance scale encourages to generate images that are closely linked to\n-                the text `prompt`, usually at the expense of lower image quality.\n-\n-                This parameter in the pipeline is there to support future guidance-distilled models when they come up.\n-                Note that passing `guidance_scale` to the pipeline is ineffective. To enable classifier-free guidance,\n-                please pass `true_cfg_scale` and `negative_prompt` (even an empty negative prompt like \" \") should\n-                enable classifier-free guidance computations.\n+            guidance_scale (`float`, *optional*, defaults to None):\n+                A guidance scale value for guidance distilled models. Unlike the traditional classifier-free guidance\n+                where the guidance scale is applied during inference through noise prediction rescaling, guidance\n+                distilled models take the guidance scale directly as an input parameter during forward pass. Guidance\n+                scale is enabled by setting `guidance_scale > 1`. Higher guidance scale encourages to generate images\n+                that are closely linked to the text `prompt`, usually at the expense of lower image quality. This\n+                parameter in the pipeline is there to support future guidance-distilled models when they come up. It is\n+                ignored when not using guidance distilled models. To enable traditional classifier-free guidance,\n+                please pass `true_cfg_scale > 1.0` and `negative_prompt` (even an empty negative prompt like \" \" should\n+                enable classifier-free guidance computations).\n             num_images_per_prompt (`int`, *optional*, defaults to 1):\n                 The number of images to generate per prompt.\n             generator (`torch.Generator` or `List[torch.Generator]`, *optional*):\n@@ -657,6 +661,16 @@ def __call__(\n         has_neg_prompt = negative_prompt is not None or (\n             negative_prompt_embeds is not None and negative_prompt_embeds_mask is not None\n         )\n+\n+        if true_cfg_scale > 1 and not has_neg_prompt:\n+            logger.warning(\n+                f\"true_cfg_scale is passed as {true_cfg_scale}, but classifier-free guidance is not enabled since no negative_prompt is provided.\"\n+            )\n+        elif true_cfg_scale <= 1 and has_neg_prompt:\n+            logger.warning(\n+                \" negative_prompt is passed but classifier-free guidance is not enabled since true_cfg_scale <= 1\"\n+            )\n+\n         do_true_cfg = true_cfg_scale > 1 and has_neg_prompt\n         prompt_embeds, prompt_embeds_mask = self.encode_prompt(\n             prompt=prompt,\n@@ -721,10 +735,17 @@ def __call__(\n         self._num_timesteps = len(timesteps)\n \n         # handle guidance\n-        if self.transformer.config.guidance_embeds:\n+        if self.transformer.config.guidance_embeds and guidance_scale is None:\n+            raise ValueError(\"guidance_scale is required for guidance-distilled model.\")\n+        elif self.transformer.config.guidance_embeds:\n             guidance = torch.full([1], guidance_scale, device=device, dtype=torch.float32)\n             guidance = guidance.expand(latents.shape[0])\n-        else:\n+        elif not self.transformer.config.guidance_embeds and guidance_scale is not None:\n+            logger.warning(\n+                f\"guidance_scale is passed as {guidance_scale}, but ignored since the model is not guidance-distilled.\"\n+            )\n+            guidance = None\n+        elif not self.transformer.config.guidance_embeds and guidance_scale is None:\n             guidance = None\n \n         if self.attention_kwargs is None:"
        },
        {
          "filename": "src/diffusers/pipelines/qwenimage/pipeline_qwenimage_inpaint.py",
          "status": "modified",
          "additions": 36,
          "deletions": 15,
          "changes": 51,
          "patch": "@@ -624,7 +624,7 @@ def __call__(\n         strength: float = 0.6,\n         num_inference_steps: int = 50,\n         sigmas: Optional[List[float]] = None,\n-        guidance_scale: float = 1.0,\n+        guidance_scale: Optional[float] = None,\n         num_images_per_prompt: int = 1,\n         generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n         latents: Optional[torch.Tensor] = None,\n@@ -657,7 +657,12 @@ def __call__(\n                 list of arrays, the expected shape should be `(B, H, W, C)` or `(H, W, C)` It can also accept image\n                 latents as `image`, but if passing latents directly it is not encoded again.\n             true_cfg_scale (`float`, *optional*, defaults to 1.0):\n-                When > 1.0 and a provided `negative_prompt`, enables true classifier-free guidance.\n+                Guidance scale as defined in [Classifier-Free Diffusion\n+                Guidance](https://huggingface.co/papers/2207.12598). `true_cfg_scale` is defined as `w` of equation 2.\n+                of [Imagen Paper](https://huggingface.co/papers/2205.11487). Classifier-free guidance is enabled by\n+                setting `true_cfg_scale > 1` and a provided `negative_prompt`. Higher guidance scale encourages to\n+                generate images that are closely linked to the text `prompt`, usually at the expense of lower image\n+                quality.\n             mask_image (`torch.Tensor`, `PIL.Image.Image`, `np.ndarray`, `List[torch.Tensor]`, `List[PIL.Image.Image]`, or `List[np.ndarray]`):\n                 `Image`, numpy array or tensor representing an image batch to mask `image`. White pixels in the mask\n                 are repainted while black pixels are preserved. If `mask_image` is a PIL image, it is converted to a\n@@ -692,17 +697,16 @@ def __call__(\n                 Custom sigmas to use for the denoising process with schedulers which support a `sigmas` argument in\n                 their `set_timesteps` method. If not defined, the default behavior when `num_inference_steps` is passed\n                 will be used.\n-            guidance_scale (`float`, *optional*, defaults to 3.5):\n-                Guidance scale as defined in [Classifier-Free Diffusion\n-                Guidance](https://huggingface.co/papers/2207.12598). `guidance_scale` is defined as `w` of equation 2.\n-                of [Imagen Paper](https://huggingface.co/papers/2205.11487). Guidance scale is enabled by setting\n-                `guidance_scale > 1`. Higher guidance scale encourages to generate images that are closely linked to\n-                the text `prompt`, usually at the expense of lower image quality.\n-\n-                This parameter in the pipeline is there to support future guidance-distilled models when they come up.\n-                Note that passing `guidance_scale` to the pipeline is ineffective. To enable classifier-free guidance,\n-                please pass `true_cfg_scale` and `negative_prompt` (even an empty negative prompt like \" \") should\n-                enable classifier-free guidance computations.\n+            guidance_scale (`float`, *optional*, defaults to None):\n+                A guidance scale value for guidance distilled models. Unlike the traditional classifier-free guidance\n+                where the guidance scale is applied during inference through noise prediction rescaling, guidance\n+                distilled models take the guidance scale directly as an input parameter during forward pass. Guidance\n+                scale is enabled by setting `guidance_scale > 1`. Higher guidance scale encourages to generate images\n+                that are closely linked to the text `prompt`, usually at the expense of lower image quality. This\n+                parameter in the pipeline is there to support future guidance-distilled models when they come up. It is\n+                ignored when not using guidance distilled models. To enable traditional classifier-free guidance,\n+                please pass `true_cfg_scale > 1.0` and `negative_prompt` (even an empty negative prompt like \" \" should\n+                enable classifier-free guidance computations).\n             num_images_per_prompt (`int`, *optional*, defaults to 1):\n                 The number of images to generate per prompt.\n             generator (`torch.Generator` or `List[torch.Generator]`, *optional*):\n@@ -801,6 +805,16 @@ def __call__(\n         has_neg_prompt = negative_prompt is not None or (\n             negative_prompt_embeds is not None and negative_prompt_embeds_mask is not None\n         )\n+\n+        if true_cfg_scale > 1 and not has_neg_prompt:\n+            logger.warning(\n+                f\"true_cfg_scale is passed as {true_cfg_scale}, but classifier-free guidance is not enabled since no negative_prompt is provided.\"\n+            )\n+        elif true_cfg_scale <= 1 and has_neg_prompt:\n+            logger.warning(\n+                \" negative_prompt is passed but classifier-free guidance is not enabled since true_cfg_scale <= 1\"\n+            )\n+\n         do_true_cfg = true_cfg_scale > 1 and has_neg_prompt\n         prompt_embeds, prompt_embeds_mask = self.encode_prompt(\n             prompt=prompt,\n@@ -890,10 +904,17 @@ def __call__(\n         self._num_timesteps = len(timesteps)\n \n         # handle guidance\n-        if self.transformer.config.guidance_embeds:\n+        if self.transformer.config.guidance_embeds and guidance_scale is None:\n+            raise ValueError(\"guidance_scale is required for guidance-distilled model.\")\n+        elif self.transformer.config.guidance_embeds:\n             guidance = torch.full([1], guidance_scale, device=device, dtype=torch.float32)\n             guidance = guidance.expand(latents.shape[0])\n-        else:\n+        elif not self.transformer.config.guidance_embeds and guidance_scale is not None:\n+            logger.warning(\n+                f\"guidance_scale is passed as {guidance_scale}, but ignored since the model is not guidance-distilled.\"\n+            )\n+            guidance = None\n+        elif not self.transformer.config.guidance_embeds and guidance_scale is None:\n             guidance = None\n \n         if self.attention_kwargs is None:"
        }
      ],
      "num_files": 5,
      "scraped_at": "2025-11-16T21:19:44.371350"
    },
    {
      "pr_number": 12220,
      "title": "[Modular] Qwen",
      "body": "# Qwen-Image\r\n\r\n- [x] Text2Image\r\n- [x] controlnet\r\n- [x] inpaint\r\n- [x] controlnet + inpaint \r\n- [x] img2img \r\n- [x] controlnet + img2img\r\n- [ ] diffdiff (next PR!)\r\n\r\n<details>\r\n<summary>Test Script for Qwen-Image Auto Pipeline</summary>\r\n\r\n```py\r\n# test modular auto (qwen image)\r\n# use standard repo\r\nimport os\r\nimport torch\r\n\r\nfrom diffusers import ModularPipeline, ComponentsManager\r\nfrom diffusers.modular_pipelines.qwenimage import ALL_BLOCKS\r\n\r\nfrom diffusers.utils import load_image\r\nfrom image_gen_aux import DepthPreprocessor\r\nimport numpy as np\r\nfrom PIL import Image\r\n\r\nimport logging\r\nlogging.getLogger().setLevel(logging.INFO)\r\nlogging.getLogger(\"diffusers\").setLevel(logging.INFO)\r\n\r\ndevice = \"cuda:2\"\r\noutput_name_prefix = \"test_modular_qwen_out\"\r\n\r\ncomponents = ComponentsManager()\r\ncomponents.enable_auto_cpu_offload(device=device)\r\n\r\npipeline = ModularPipeline.from_pretrained(\"Qwen/Qwen-Image\", components_manager=components)\r\nprint(pipeline)\r\n\r\npipeline.load_components(torch_dtype=torch.bfloat16)\r\n\r\nprint(\"pipeline loaded\")\r\nprint(pipeline)\r\nprint(f\" \")\r\nprint(f\"pipeline.blocks\")\r\nprint(pipeline.blocks)\r\nprint(f\" \")\r\n\r\nprint(f\"components:\")\r\nprint(components)\r\nprint(f\" \")\r\n\r\n\r\n# test1: text2image with custom height/width\r\n\r\nprompt = \"\u73b0\u5b9e\u4e3b\u4e49\u98ce\u683c\u7684\u4eba\u50cf\u6444\u5f71\u4f5c\u54c1\uff0c\u753b\u9762\u4e3b\u4f53\u662f\u4e00\u4f4d\u5bb9\u8c8c\u60ca\u8273\u7684\u5973\u6027\u9762\u90e8\u7279\u5199\u3002\u5979\u62e5\u6709\u4e00\u5934\u81ea\u7136\u5fae\u5377\u7684\u77ed\u53d1\uff0c\u53d1\u4e1d\u6839\u6839\u5206\u660e\uff0c\u84ec\u677e\u7684\u5218\u6d77\u4fee\u9970\u7740\u989d\u5934\uff0c\u589e\u6dfb\u4fcf\u76ae\u611f\u3002\u5934\u4e0a\u4f69\u6234\u4e00\u9876\u7eff\u8272\u683c\u5b50\u857e\u4e1d\u8fb9\u5934\u5dfe\uff0c\u589e\u6dfb\u590d\u53e4\u4e0e\u67d4\u7f8e\u6c14\u606f\u3002\u8eab\u7740\u4e00\u4ef6\u7b80\u7ea6\u7eff\u8272\u80cc\u5fc3\u88d9\uff0c\u5728\u7eaf\u767d\u8272\u80cc\u666f\u4e0b\u683c\u5916\u7a81\u51fa\u3002\u4e24\u53ea\u624b\u5206\u522b\u63e1\u7740\u534a\u4e2a\u7ea2\u8272\u6843\u5b50\uff0c\u53cc\u624b\u8f7b\u8f7b\u8d34\u5728\u8138\u988a\u4e24\u4fa7\uff0c\u8425\u9020\u51fa\u53ef\u7231\u53c8\u5bcc\u6709\u521b\u610f\u7684\u89c6\u89c9\u6548\u679c\u3002  \u4eba\u7269\u8868\u60c5\u751f\u52a8\uff0c\u4e00\u53ea\u773c\u775b\u7741\u5f00\uff0c\u53e6\u4e00\u53ea\u5fae\u5fae\u95ed\u5408\uff0c\u5c55\u73b0\u51fa\u8c03\u76ae\u4e0e\u81ea\u4fe1\u7684\u795e\u6001\u3002\u6574\u4f53\u6784\u56fe\u91c7\u7528\u4e2a\u6027\u89c6\u89d2\u3001\u975e\u5bf9\u79f0\u6784\u56fe\uff0c\u805a\u7126\u4eba\u7269\u4e3b\u4f53\uff0c\u589e\u5f3a\u73b0\u573a\u611f\u548c\u65e2\u89c6\u611f\u3002\u80cc\u666f\u865a\u5316\u5904\u7406\uff0c\u5c42\u6b21\u4e30\u5bcc\uff0c\u666f\u6df1\u6548\u679c\u5f3a\u70c8\uff0c\u8425\u9020\u51fa\u4f4e\u5149\u6c1b\u56f4\u4e0b\u6d53\u539a\u7684\u60c5\u7eea\u5f20\u529b\u3002  \u753b\u9762\u7ec6\u8282\u7cbe\u81f4\uff0c\u8272\u5f69\u751f\u52a8\u9971\u6ee1\u5374\u4e0d\u5931\u67d4\u548c\uff0c\u5448\u73b0\u51fa\u5bcc\u58eb\u80f6\u7247\u72ec\u6709\u7684\u6e29\u6da6\u8d28\u611f\u3002\u5149\u5f71\u8fd0\u7528\u5145\u6ee1\u7f8e\u5b66\u5f20\u529b\uff0c\u5e26\u6709\u8f7b\u5fae\u8d85\u73b0\u5b9e\u7684\u5149\u6548\u5904\u7406\uff0c\u63d0\u5347\u6574\u4f53\u753b\u9762\u9ad8\u7ea7\u611f\u3002\u6574\u4f53\u98ce\u683c\u4e3a\u73b0\u5b9e\u4e3b\u4e49\u4eba\u50cf\u6444\u5f71\uff0c\u5f3a\u8c03\u7ec6\u817b\u7684\u7eb9\u7406\u4e0e\u827a\u672f\u5316\u7684\u5149\u7ebf\u8868\u73b0\uff0c\u582a\u79f0\u4e00\u5e45\u7ec6\u8282\u4e30\u5bcc\u3001\u6c1b\u56f4\u62c9\u6ee1\u7684\u6770\u4f5c\u3002\u8d85\u6e05\uff0c4K\uff0c\u7535\u5f71\u7ea7\u6784\u56fe\"\r\ninputs = {\r\n    \"prompt\": prompt,\r\n    \"generator\": torch.manual_seed(0),\r\n    \"negative_prompt\": \" \",\r\n    \"height\": 1328,\r\n    \"width\": 1328,\r\n    \"num_inference_steps\": 50,\r\n    \"num_images_per_prompt\": 1,\r\n}\r\n\r\noutput_images = pipeline(**inputs, output=\"images\")\r\nfor i, image in enumerate(output_images):\r\n    assert image.size == (1328, 1328)\r\n    image.save(f\"{output_name_prefix}_1_text2image_1328_{i}.png\")\r\n    print(f\"image saved at {os.path.abspath(f'{output_name_prefix}_1_text2image_1328_{i}.png')}\")\r\n\r\n\r\n# test2: text2image with default height and width\r\nprompt = \"\u73b0\u5b9e\u4e3b\u4e49\u98ce\u683c\u7684\u4eba\u50cf\u6444\u5f71\u4f5c\u54c1\uff0c\u753b\u9762\u4e3b\u4f53\u662f\u4e00\u4f4d\u5bb9\u8c8c\u60ca\u8273\u7684\u5973\u6027\u9762\u90e8\u7279\u5199\u3002\u5979\u62e5\u6709\u4e00\u5934\u81ea\u7136\u5fae\u5377\u7684\u77ed\u53d1\uff0c\u53d1\u4e1d\u6839\u6839\u5206\u660e\uff0c\u84ec\u677e\u7684\u5218\u6d77\u4fee\u9970\u7740\u989d\u5934\uff0c\u589e\u6dfb\u4fcf\u76ae\u611f\u3002\u5934\u4e0a\u4f69\u6234\u4e00\u9876\u7eff\u8272\u683c\u5b50\u857e\u4e1d\u8fb9\u5934\u5dfe\uff0c\u589e\u6dfb\u590d\u53e4\u4e0e\u67d4\u7f8e\u6c14\u606f\u3002\u8eab\u7740\u4e00\u4ef6\u7b80\u7ea6\u7eff\u8272\u80cc\u5fc3\u88d9\uff0c\u5728\u7eaf\u767d\u8272\u80cc\u666f\u4e0b\u683c\u5916\u7a81\u51fa\u3002\u4e24\u53ea\u624b\u5206\u522b\u63e1\u7740\u534a\u4e2a\u7ea2\u8272\u6843\u5b50\uff0c\u53cc\u624b\u8f7b\u8f7b\u8d34\u5728\u8138\u988a\u4e24\u4fa7\uff0c\u8425\u9020\u51fa\u53ef\u7231\u53c8\u5bcc\u6709\u521b\u610f\u7684\u89c6\u89c9\u6548\u679c\u3002  \u4eba\u7269\u8868\u60c5\u751f\u52a8\uff0c\u4e00\u53ea\u773c\u775b\u7741\u5f00\uff0c\u53e6\u4e00\u53ea\u5fae\u5fae\u95ed\u5408\uff0c\u5c55\u73b0\u51fa\u8c03\u76ae\u4e0e\u81ea\u4fe1\u7684\u795e\u6001\u3002\u6574\u4f53\u6784\u56fe\u91c7\u7528\u4e2a\u6027\u89c6\u89d2\u3001\u975e\u5bf9\u79f0\u6784\u56fe\uff0c\u805a\u7126\u4eba\u7269\u4e3b\u4f53\uff0c\u589e\u5f3a\u73b0\u573a\u611f\u548c\u65e2\u89c6\u611f\u3002\u80cc\u666f\u865a\u5316\u5904\u7406\uff0c\u5c42\u6b21\u4e30\u5bcc\uff0c\u666f\u6df1\u6548\u679c\u5f3a\u70c8\uff0c\u8425\u9020\u51fa\u4f4e\u5149\u6c1b\u56f4\u4e0b\u6d53\u539a\u7684\u60c5\u7eea\u5f20\u529b\u3002  \u753b\u9762\u7ec6\u8282\u7cbe\u81f4\uff0c\u8272\u5f69\u751f\u52a8\u9971\u6ee1\u5374\u4e0d\u5931\u67d4\u548c\uff0c\u5448\u73b0\u51fa\u5bcc\u58eb\u80f6\u7247\u72ec\u6709\u7684\u6e29\u6da6\u8d28\u611f\u3002\u5149\u5f71\u8fd0\u7528\u5145\u6ee1\u7f8e\u5b66\u5f20\u529b\uff0c\u5e26\u6709\u8f7b\u5fae\u8d85\u73b0\u5b9e\u7684\u5149\u6548\u5904\u7406\uff0c\u63d0\u5347\u6574\u4f53\u753b\u9762\u9ad8\u7ea7\u611f\u3002\u6574\u4f53\u98ce\u683c\u4e3a\u73b0\u5b9e\u4e3b\u4e49\u4eba\u50cf\u6444\u5f71\uff0c\u5f3a\u8c03\u7ec6\u817b\u7684\u7eb9\u7406\u4e0e\u827a\u672f\u5316\u7684\u5149\u7ebf\u8868\u73b0\uff0c\u582a\u79f0\u4e00\u5e45\u7ec6\u8282\u4e30\u5bcc\u3001\u6c1b\u56f4\u62c9\u6ee1\u7684\u6770\u4f5c\u3002\u8d85\u6e05\uff0c4K\uff0c\u7535\u5f71\u7ea7\u6784\u56fe\"\r\ninputs = {\r\n    \"prompt\": prompt,\r\n    \"generator\": torch.manual_seed(0),\r\n    \"negative_prompt\": \" \",\r\n    \"num_inference_steps\": 50,\r\n    \"num_images_per_prompt\": 1,\r\n}\r\n\r\noutput_images = pipeline(**inputs, output=\"images\")\r\nfor i, image in enumerate(output_images):\r\n    assert image.size == (1024, 1024)\r\n    image.save(f\"{output_name_prefix}_1_text2image_1024_{i}.png\")\r\n    print(f\"image saved at {os.path.abspath(f'{output_name_prefix}_1_text2image_1024_{i}.png')}\")\r\n\r\n# test3: inpaint\r\n\r\nprompt = \"cat wizard with red hat, gandalf, lord of the rings, detailed, fantasy, cute, adorable, Pixar, Disney\"\r\nnegative_prompt = \" \"\r\nsource = load_image(\"https://github.com/Trgtuan10/Image_storage/blob/main/cute_cat.png?raw=true\")\r\nmask = load_image(\"https://github.com/Trgtuan10/Image_storage/blob/main/mask_cat.png?raw=true\")\r\n\r\nstrengths = [0.9]\r\n\r\nprint(f\"source.size: {source.size}\")\r\n\r\nfor strength in strengths:\r\n    image = pipeline(\r\n        prompt=prompt,\r\n        negative_prompt=negative_prompt,\r\n        height=source.size[1],\r\n        width=source.size[0],\r\n        image=source,\r\n        mask_image=mask,\r\n        strength=strength,\r\n        num_inference_steps=35,\r\n        generator=torch.Generator(device=\"cuda\").manual_seed(42),\r\n        output=\"images\"\r\n    )[0]\r\n    image.save(f\"{output_name_prefix}_2_inpaint_{strength}.png\")\r\n    assert image.size == source.size\r\n    print(f\"image saved at {os.path.abspath(f'{output_name_prefix}_2_inpaint_{strength}.png')}\")\r\n\r\n\r\n# test4: controlnet\r\n\r\nprint(\"test controlnet\")\r\n\r\n# canny\r\nfrom diffusers import QwenImageControlNetModel, QwenImageMultiControlNetModel\r\n\r\ncontrolnet_spec = pipeline.get_component_spec(\"controlnet\")\r\ncontrolnet_spec.repo = \"InstantX/Qwen-Image-ControlNet-Union\"\r\ncontrolnet = controlnet_spec.load(torch_dtype=torch.bfloat16)\r\n\r\npipeline.update_components(controlnet=controlnet)\r\n\r\nprint(\"pipeline (with controlnet)\")\r\nprint(pipeline)\r\nprint(f\" \")\r\nprint(\"components (with controlnet)\")\r\nprint(components)\r\nprint(f\" \")\r\n\r\n\r\ncontrol_image = load_image(\"https://huggingface.co/datasets/YiYiXu/testing-images/resolve/main/qwencond_input.png\")\r\nprompt = \"Aesthetics art, traditional asian pagoda, elaborate golden accents, sky blue and white color palette, swirling cloud pattern, digital illustration, east asian architecture, ornamental rooftop, intricate detailing on building, cultural representation.\"\r\ncontrolnet_conditioning_scale = 1.0\r\n\r\nprint(f\"control_image.size: {control_image.size}\")\r\n\r\nimages = pipeline(\r\n    prompt=prompt,\r\n    negative_prompt=\" \",\r\n    control_image=control_image,\r\n    controlnet_conditioning_scale=controlnet_conditioning_scale,\r\n    width=control_image.size[0],\r\n    height=control_image.size[1],\r\n    generator=torch.Generator(device=\"cuda\").manual_seed(42),\r\n    output=\"images\"\r\n)\r\nfor i, image in enumerate(images):\r\n    assert image.size == control_image.size\r\n    image.save(f\"{output_name_prefix}_3_controlnet_{i}.png\")\r\n    print(f\"image saved at {os.path.abspath(f'{output_name_prefix}_3_controlnet_{i}.png')}\")\r\n\r\nprint(f\" components:\")\r\nprint(components)\r\nprint(f\" \")\r\n\r\n# test5: multi-controlnet \r\nmulti_controlnet = QwenImageMultiControlNetModel([controlnet])\r\npipeline.update_components(controlnet=multi_controlnet)\r\n\r\nimages = pipeline(\r\n    prompt=prompt,\r\n    negative_prompt=\" \",\r\n    control_image=[control_image, control_image],\r\n    controlnet_conditioning_scale=[controlnet_conditioning_scale/2, controlnet_conditioning_scale/2],\r\n    width=control_image.size[0],\r\n    height=control_image.size[1],\r\n    generator=torch.Generator(device=\"cuda\").manual_seed(42),\r\n    output=\"images\"\r\n)\r\nfor i, image in enumerate(images):\r\n    assert image.size == control_image.size\r\n    image.save(f\"{output_name_prefix}_3_controlnet_multi_{i}.png\")\r\n    print(f\"image saved at {os.path.abspath(f'{output_name_prefix}_3_controlnet_multi_{i}.png')}\")\r\n\r\n\r\n\r\n# test6: multi-controlnet, default height/width and num_images_per_prompt = 2\r\nmulti_controlnet = QwenImageMultiControlNetModel([controlnet])\r\npipeline.update_components(controlnet=multi_controlnet)\r\n\r\nimages = pipeline(\r\n    prompt=prompt,\r\n    negative_prompt=\" \",\r\n    control_image=[control_image, control_image],\r\n    controlnet_conditioning_scale=[controlnet_conditioning_scale/2, controlnet_conditioning_scale/2],\r\n    num_images_per_prompt=2,\r\n    generator=torch.Generator(device=\"cuda\").manual_seed(42),\r\n    output=\"images\"\r\n)\r\nfor i, image in enumerate(images):\r\n    assert image.size == (1024, 1024)\r\n    image.save(f\"{output_name_prefix}_3_controlnet_multi_2_{i}.png\")\r\n    print(f\"image saved at {os.path.abspath(f'{output_name_prefix}_3_controlnet_multi_2_{i}.png')}\")\r\n\r\n# test7: controlnet + inpaint\r\n\r\npipeline.update_components(controlnet=controlnet)\r\n\r\nprompt = \"a blue robot singing opera with human-like expressions\"\r\nimage = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/robot.png\")\r\n\r\nhead_mask = np.zeros_like(image)\r\nhead_mask[65:580,300:642] = 255\r\nmask_image = Image.fromarray(head_mask)\r\n\r\nprocessor = DepthPreprocessor.from_pretrained(\"LiheYoung/depth-anything-large-hf\")\r\ncontrol_image = processor(image)[0].convert(\"RGB\")\r\n\r\nprint(f\"image.size: {image.size}\")\r\nprint(f\"control_image.size: {control_image.size}\")\r\nprint(f\"mask_image.size: {mask_image.size}\")\r\n\r\nimage_output = pipeline(\r\n    prompt=prompt,\r\n    image=image,\r\n    mask_image=mask_image,\r\n    control_image=control_image,\r\n    strength=0.9,\r\n    num_inference_steps=30,\r\n    output=\"images\",\r\n)\r\nfor i, image in enumerate(image_output):\r\n    assert image.size == (1024, 1024)\r\n    image.save(f\"{output_name_prefix}_4_controlnet_inpaint_{i}.png\")\r\n    print(f\"image saved at {os.path.abspath(f'{output_name_prefix}_4_controlnet_inpaint_{i}.png')}\")\r\n\r\n\r\n\r\n# test8: update guider (PAG)\r\n\r\nfrom diffusers import LayerSkipConfig, PerturbedAttentionGuidance\r\n\r\n# make a copy of the cfg guider to swith back later\r\ncfg_guider_spec = pipeline.get_component_spec(\"guider\")\r\n\r\npag_config = LayerSkipConfig(indices=[2, 9], skip_attention=False, skip_attention_scores=True, skip_ff=False)\r\npag_guider = PerturbedAttentionGuidance(\r\n    guidance_scale=5.0, perturbed_guidance_scale=2.5, perturbed_guidance_config=pag_config\r\n)\r\npipeline.update_components(guider=pag_guider)\r\n\r\nprint(\"pipeline.guider\")\r\nprint(pipeline.guider)\r\n\r\n\r\n# prompt = \"A painting of a squirrel eating a burger\"\r\nprompt = \"\u73b0\u5b9e\u4e3b\u4e49\u98ce\u683c\u7684\u4eba\u50cf\u6444\u5f71\u4f5c\u54c1\uff0c\u753b\u9762\u4e3b\u4f53\u662f\u4e00\u4f4d\u5bb9\u8c8c\u60ca\u8273\u7684\u5973\u6027\u9762\u90e8\u7279\u5199\u3002\u5979\u62e5\u6709\u4e00\u5934\u81ea\u7136\u5fae\u5377\u7684\u77ed\u53d1\uff0c\u53d1\u4e1d\u6839\u6839\u5206\u660e\uff0c\u84ec\u677e\u7684\u5218\u6d77\u4fee\u9970\u7740\u989d\u5934\uff0c\u589e\u6dfb\u4fcf\u76ae\u611f\u3002\u5934\u4e0a\u4f69\u6234\u4e00\u9876\u7eff\u8272\u683c\u5b50\u857e\u4e1d\u8fb9\u5934\u5dfe\uff0c\u589e\u6dfb\u590d\u53e4\u4e0e\u67d4\u7f8e\u6c14\u606f\u3002\u8eab\u7740\u4e00\u4ef6\u7b80\u7ea6\u7eff\u8272\u80cc\u5fc3\u88d9\uff0c\u5728\u7eaf\u767d\u8272\u80cc\u666f\u4e0b\u683c\u5916\u7a81\u51fa\u3002\u4e24\u53ea\u624b\u5206\u522b\u63e1\u7740\u534a\u4e2a\u7ea2\u8272\u6843\u5b50\uff0c\u53cc\u624b\u8f7b\u8f7b\u8d34\u5728\u8138\u988a\u4e24\u4fa7\uff0c\u8425\u9020\u51fa\u53ef\u7231\u53c8\u5bcc\u6709\u521b\u610f\u7684\u89c6\u89c9\u6548\u679c\u3002  \u4eba\u7269\u8868\u60c5\u751f\u52a8\uff0c\u4e00\u53ea\u773c\u775b\u7741\u5f00\uff0c\u53e6\u4e00\u53ea\u5fae\u5fae\u95ed\u5408\uff0c\u5c55\u73b0\u51fa\u8c03\u76ae\u4e0e\u81ea\u4fe1\u7684\u795e\u6001\u3002\u6574\u4f53\u6784\u56fe\u91c7\u7528\u4e2a\u6027\u89c6\u89d2\u3001\u975e\u5bf9\u79f0\u6784\u56fe\uff0c\u805a\u7126\u4eba\u7269\u4e3b\u4f53\uff0c\u589e\u5f3a\u73b0\u573a\u611f\u548c\u65e2\u89c6\u611f\u3002\u80cc\u666f\u865a\u5316\u5904\u7406\uff0c\u5c42\u6b21\u4e30\u5bcc\uff0c\u666f\u6df1\u6548\u679c\u5f3a\u70c8\uff0c\u8425\u9020\u51fa\u4f4e\u5149\u6c1b\u56f4\u4e0b\u6d53\u539a\u7684\u60c5\u7eea\u5f20\u529b\u3002  \u753b\u9762\u7ec6\u8282\u7cbe\u81f4\uff0c\u8272\u5f69\u751f\u52a8\u9971\u6ee1\u5374\u4e0d\u5931\u67d4\u548c\uff0c\u5448\u73b0\u51fa\u5bcc\u58eb\u80f6\u7247\u72ec\u6709\u7684\u6e29\u6da6\u8d28\u611f\u3002\u5149\u5f71\u8fd0\u7528\u5145\u6ee1\u7f8e\u5b66\u5f20\u529b\uff0c\u5e26\u6709\u8f7b\u5fae\u8d85\u73b0\u5b9e\u7684\u5149\u6548\u5904\u7406\uff0c\u63d0\u5347\u6574\u4f53\u753b\u9762\u9ad8\u7ea7\u611f\u3002\u6574\u4f53\u98ce\u683c\u4e3a\u73b0\u5b9e\u4e3b\u4e49\u4eba\u50cf\u6444\u5f71\uff0c\u5f3a\u8c03\u7ec6\u817b\u7684\u7eb9\u7406\u4e0e\u827a\u672f\u5316\u7684\u5149\u7ebf\u8868\u73b0\uff0c\u582a\u79f0\u4e00\u5e45\u7ec6\u8282\u4e30\u5bcc\u3001\u6c1b\u56f4\u62c9\u6ee1\u7684\u6770\u4f5c\u3002\u8d85\u6e05\uff0c4K\uff0c\u7535\u5f71\u7ea7\u6784\u56fe\"\r\ninputs = {\r\n    \"prompt\": prompt,\r\n    \"generator\": torch.manual_seed(0),\r\n    \"negative_prompt\": \" \",\r\n    \"height\": 1328,\r\n    \"width\": 1328,\r\n    \"num_inference_steps\": 50,\r\n    \"num_images_per_prompt\": 1,\r\n}\r\n\r\noutput_images = pipeline(**inputs, output=\"images\")\r\nfor i, image in enumerate(output_images):\r\n    assert image.size == (1328, 1328)\r\n    image.save(f\"{output_name_prefix}_5_guider_{i}.png\")\r\n    print(f\"image saved at {os.path.abspath(f'{output_name_prefix}_5_guider_{i}.png')}\")\r\n\r\n\r\n\r\n# test9: img2img\r\n\r\nprint(f\"pipeline.guider\")\r\nprint(pipeline.guider)\r\npipeline.update_components(guider=cfg_guider_spec)\r\nprint(f\"pipeline.guider\")\r\nprint(pipeline.guider)\r\n\r\n\r\ninit_image = load_image(\"https://github.com/Trgtuan10/Image_storage/blob/main/cute_cat.png?raw=true\")\r\nprompt = \"wizard dog, Gandalf-inspired, Lord of the Rings aesthetic, majestic yet cute, Studio Ghibli style\"\r\n\r\nstrengths = [0.6, 0.9, 1.0]\r\n\r\nfor s in strengths:\r\n    out = pipeline(\r\n        prompt=prompt,\r\n        image=init_image,\r\n        height=init_image.size[1],\r\n        width=init_image.size[0],\r\n        strength=s,\r\n        num_inference_steps=35,\r\n        generator=torch.Generator(device=\"cuda\").manual_seed(42),\r\n    )\r\n    out.images[0].save(f\"yiyi_test_5_output_6_img2img_{s}.png\")\r\n    print(f\"image saved at {os.path.abspath(f'yiyi_test_5_output_6_img2img_{s}.png')}\")\r\n\r\n\r\n# test10: img2img + controlnet\r\n\r\n# extract canny\r\nget_image_step = ModularPipeline.from_pretrained(\"YiYiXu/image_inputs\", trust_remote_code=True)\r\ncontrol_image = get_image_step(image=init_image, processor_id=\"canny\", output=\"image\")\r\ncontrolnet_conditioning_scale = 1.0\r\n\r\nstrengths = [0.6, 0.9, 1.0]\r\n\r\nfor s in strengths:\r\n    out = pipeline(\r\n        prompt=prompt,\r\n        image=init_image,\r\n        control_image=control_image,\r\n        controlnet_conditioning_scale=controlnet_conditioning_scale,\r\n        height=init_image.size[1],\r\n        width=init_image.size[0],\r\n        strength=s,\r\n        num_inference_steps=35,\r\n        generator=torch.Generator(device=\"cuda\").manual_seed(42),\r\n    )\r\n    out.images[0].save(f\"yiyi_test_5_output_6_img2img_controlnet_{s}.png\")\r\n    print(f\"image saved at {os.path.abspath(f'yiyi_test_5_output_6_img2img_controlnet_{s}.png')}\")\r\n\r\nprint(f\" components:\")\r\nprint(components)\r\nprint(f\" \")\r\n```\r\n\r\n</details>\r\n\r\n\r\n# QwenImage Edit\r\n\r\n- [x] Edit\r\n- [x] Edit + Inpaint\r\n- [ ] diffdiff (next PR!)\r\n\r\n<details>\r\n<summary>Test script for QwenImage-Edit in Modular</summary>\r\n\r\n```py\r\n# test modular auto (qwen image edit)\r\n# use standard repo\r\nimport os\r\nimport torch\r\n\r\nfrom diffusers import ModularPipeline, ComponentsManager\r\nfrom diffusers.modular_pipelines.qwenimage import ALL_BLOCKS\r\n\r\nfrom diffusers.utils import load_image\r\nfrom image_gen_aux import DepthPreprocessor\r\nimport numpy as np\r\nfrom PIL import Image\r\n\r\nimport logging\r\nlogging.getLogger().setLevel(logging.INFO)\r\nlogging.getLogger(\"diffusers\").setLevel(logging.INFO)\r\n\r\ndevice = \"cuda:2\"\r\noutput_name_prefix = \"test_modular_qwen_edit_output\"\r\n\r\ncomponents = ComponentsManager()\r\ncomponents.enable_auto_cpu_offload(device=device)\r\n\r\npipeline = ModularPipeline.from_pretrained(\"Qwen/Qwen-Image-Edit\", components_manager=components)\r\nprint(pipeline)\r\n\r\npipeline.load_components(torch_dtype=torch.bfloat16)\r\n\r\nprint(\"pipeline loaded\")\r\nprint(pipeline)\r\nprint(f\" \")\r\nprint(f\"pipeline.blocks\")\r\nprint(pipeline.blocks)\r\nprint(f\" \")\r\n\r\nprint(f\"components:\")\r\nprint(components)\r\nprint(f\" \")\r\n\r\n\r\n# edit\r\n\r\nprompt = \"change the hat to red\"\r\nnegative_prompt = \" \"\r\nsource = load_image(\"https://github.com/Trgtuan10/Image_storage/blob/main/cute_cat.png?raw=true\")\r\nmask = load_image(\"https://github.com/Trgtuan10/Image_storage/blob/main/mask_cat.png?raw=true\")\r\n\r\n# edit\r\nprint(f\"source size: {source.size}\")\r\nprint(f\"mask size: {mask.size}\")\r\noutput_images = pipeline(\r\n    prompt=prompt,\r\n    negative_prompt=negative_prompt,\r\n    image=source,\r\n    num_inference_steps=35,\r\n    generator=torch.Generator(device=\"cuda\").manual_seed(42),\r\n).images\r\nfor i, image in enumerate(output_images):\r\n    image.save(f\"{output_name_prefix}_1_edit_{source.size[1]}_{source.size[0]}_{i}.png\")\r\n    print(f\"image size: {image.size}\")\r\n    print(f\"image saved at {os.path.abspath(f'{output_name_prefix}_1_edit_{source.size[1]}_{source.size[0]}_{i}.png')}\")\r\n\r\n# edit + update guider (guidance_scale=4.5)\r\n\r\ncfg_guider_spec = pipeline.get_component_spec(\"guider\")\r\ncfg_guider_spec.config[\"guidance_scale\"] = 4.5\r\npipeline.update_components(guider=cfg_guider_spec)\r\n\r\nprint(f\" print pipeline.guider\")\r\nprint(pipeline.guider)\r\n\r\noutput_images = pipeline(\r\n    prompt=prompt,\r\n    negative_prompt=negative_prompt,\r\n    image=source,\r\n    num_inference_steps=35,\r\n    generator=torch.Generator(device=\"cuda\").manual_seed(42),\r\n).images\r\nfor i, image in enumerate(output_images):\r\n    image.save(f\"{output_name_prefix}_2_edit_guidance_scale_4.5_{i}.png\")\r\n    print(f\"image size: {image.size}\")\r\n    print(f\"image saved at {os.path.abspath(f'{output_name_prefix}_2_edit_guidance_scale_4.5_{i}.png')}\")\r\n\r\n# edit + num_images_per_prompt==2\r\noutput_images = pipeline(\r\n    prompt=prompt,\r\n    negative_prompt=negative_prompt,\r\n    image=source,\r\n    num_inference_steps=35,\r\n    generator=torch.Generator(device=\"cuda\").manual_seed(42),\r\n    num_images_per_prompt=2,\r\n).images\r\nfor i, image in enumerate(output_images):\r\n    image.save(f\"{output_name_prefix}_3_edit_num_images_per_prompt_2_{i}.png\")\r\n    print(f\"image size: {image.size}\")\r\n    print(f\"image saved at {os.path.abspath(f'{output_name_prefix}_3_edit_num_images_per_prompt_2_{i}.png')}\")\r\n\r\n# edit + pag \r\nfrom diffusers import LayerSkipConfig, PerturbedAttentionGuidance\r\n\r\npag_config = LayerSkipConfig(indices=[2, 9], skip_attention=False, skip_attention_scores=True, skip_ff=False)\r\npag_guider = PerturbedAttentionGuidance(\r\n    guidance_scale=5.0, perturbed_guidance_scale=2.5, perturbed_guidance_config=pag_config\r\n)\r\npipeline.update_components(guider=pag_guider)\r\n\r\nprint(f\" print pipeline.guider\")\r\nprint(pipeline.guider)\r\n\r\noutput_images = pipeline(\r\n    prompt=prompt,\r\n    negative_prompt=negative_prompt,\r\n    image=source,\r\n    num_inference_steps=35,\r\n    generator=torch.Generator(device=\"cuda\").manual_seed(42),\r\n).images\r\nfor i, image in enumerate(output_images):\r\n    image.save(f\"{output_name_prefix}_4_edit_pag_{i}.png\")\r\n    print(f\"image size: {image.size}\")\r\n    print(f\"image saved at {os.path.abspath(f'{output_name_prefix}_4_edit_pag_{i}.png')}\")\r\n\r\n\r\n# inpaint\r\n\r\nstrengths = [0.9, 1.0]\r\nprint(f\" pipeline.guider\")\r\nprint(pipeline.guider)\r\n\r\nfor strength in strengths:\r\n    image_output = pipeline(\r\n        prompt=prompt,\r\n        negative_prompt=negative_prompt,\r\n        image=source,\r\n        mask_image=mask,\r\n        strength=strength,\r\n        num_inference_steps=35,\r\n        generator=torch.Generator(device=\"cuda\").manual_seed(42),\r\n    ).images[0]\r\n    image_output.save(f\"{output_name_prefix}_5_inpaint_pag_{strength}.png\")\r\n    print(f\"image saved at {os.path.abspath(f'{output_name_prefix}_5_inpaint_pag_{strength}.png')}\")\r\n\r\n\r\n# edit + cfg guider\r\n\r\npipeline.update_components(guider=cfg_guider_spec)\r\n\r\nprint(f\" print pipeline.guider\")\r\nprint(pipeline.guider)\r\n\r\ninput_image = load_image(\"https://huggingface.co/datasets/YiYiXu/testing-images/resolve/main/qwenedit_input.png\")\r\nseed = 43\r\nprompt = \"The woman is displaying a plush toy product in her hand, while preserving her exact facial features, expression, clothing, and pose. Maintain the same background, natural lighting, and overall photographic composition and style.\"\r\ninputs = {\r\n    \"image\": input_image,\r\n    \"prompt\": prompt,\r\n    \"generator\": torch.Generator(device=device).manual_seed(seed),\r\n    \"num_inference_steps\": 50,\r\n    # \"height\": 1024,\r\n    # \"width\": 1024,\r\n}\r\n\r\noutput_images = pipeline(**inputs, output=\"images\")\r\nfor i, image in enumerate(output_images):\r\n    image.save(f\"{output_name_prefix}_6_inpaint_cfg_{i}.png\")\r\n    print(f\"image saved at {os.path.abspath(f'{output_name_prefix}_6_inpaint_cfg_{i}.png')}\")\r\n\r\n\r\n\r\n\r\n# edit + cfg guider + custom size\r\n\r\npipeline.update_components(guider=cfg_guider_spec)\r\n\r\nprint(f\" print pipeline.guider\")\r\nprint(pipeline.guider)\r\n\r\ninput_image = load_image(\"https://huggingface.co/datasets/YiYiXu/testing-images/resolve/main/qwenedit_input.png\")\r\nseed = 43\r\nprompt = \"The woman is displaying a plush toy product in her hand, while preserving her exact facial features, expression, clothing, and pose. Maintain the same background, natural lighting, and overall photographic composition and style.\"\r\ninputs = {\r\n    \"image\": input_image,\r\n    \"prompt\": prompt,\r\n    \"generator\": torch.Generator(device=device).manual_seed(seed),\r\n    \"num_inference_steps\": 50,\r\n    \"height\": 1024,\r\n    \"width\": 1024,\r\n}\r\n\r\noutput_images = pipeline(**inputs, output=\"images\")\r\nfor i, image in enumerate(output_images):\r\n    assert image.size == (1024, 1024)\r\n    image.save(f\"{output_name_prefix}_7_inpaint_cfg_custom_size_{i}.png\")\r\n    print(f\"image saved at {os.path.abspath(f'{output_name_prefix}_7_inpaint_cfg_custom_size_{i}.png')}\")\r\n```\r\n</details>\r\n\r\n\r\n# How to Use\r\n\r\nthe shorter version, check the test scripts above for complete, runnable examples\r\n\r\n\r\n#### to load from standard repo\r\n\r\n```py\r\nimport torch\r\nfrom diffusers import ModularPipeline, ComponentsManager\r\n\r\nrepo_id = \"Qwen/Qwen-Image\"\r\n# repo_id = \"Qwen/Qwen-Image-Edit\"\r\n\r\ncomponents = ComponentsManager()\r\ncomponents.enable_auto_cpu_offload(device=\"cuda\")\r\npipeline = ModularPipeline.from_pretrained(repo_id, components_manager=components)\r\npipeline.load_components(torch_dtype=torch.float16)\r\nprint(pipeline)\r\n```\r\n\r\n#### add controlnet (we currently only have controlnet for Qwen-Image)\r\n\r\n```py\r\nfrom diffusers import QwenImageControlNetModel, QwenImageMultiControlNetModel\r\n\r\ncontrolnet_spec = pipeline.get_component_spec(\"controlnet\")\r\ncontrolnet_spec.repo = \"InstantX/Qwen-Image-ControlNet-Union\"\r\ncontrolnet = controlnet_spec.load(torch_dtype=torch.bfloat16)\r\npipeline.update_components(controlnet=controlnet)\r\n```\r\n\r\n#### update guider\r\nchange `guidance_scale`\r\n\r\n```py\r\ncfg_guider_spec = pipeline.get_component_spec(\"guider\")\r\ncfg_guider_spec.config[\"guidance_scale\"] = 4.5\r\npipeline.update_components(guider=cfg_guider_spec)\r\n```\r\n\r\nuse a different guidance method\r\n```py\r\nfrom diffusers import LayerSkipConfig, PerturbedAttentionGuidance\r\n\r\npag_config = LayerSkipConfig(indices=[2, 9], skip_attention=False, skip_attention_scores=True, skip_ff=False)\r\npag_guider = PerturbedAttentionGuidance(\r\n    guidance_scale=5.0, perturbed_guidance_scale=2.5, perturbed_guidance_config=pag_config\r\n)\r\npipeline.update_components(guider=pag_guider)\r\n```\r\n\r\n#### to run inference\r\n\r\nYou can use same pipeline to run all tasks we support, the code is pretty much same as in regular pipelines\r\n\r\n```py\r\n# text2image\r\npipeline(prompt=prompt, ...).images[0]\r\n```\r\n\r\n```py\r\n# image2image\r\npipeline(prompt=prompt, image=...,  strength=..., ...).images[0]\r\n```\r\n\r\n```py\r\n# inpaint \r\npipeline(prompt=prompt, image=...,  mask_image=..., strength=...,).images[0]\r\n```\r\n\r\nadd controlnet to text2image, img2img, inpaint, just pass `control_image` along with any other controlnet related arguments\r\n\r\n```py\r\n# text2image + controlnet\r\npipeline(prompt=prompt, control_image=, ...).images[0]\r\n```\r\n\r\n```py\r\n# image2image + controlnet\r\npipeline(prompt=prompt, image=...,  strength=..., control_image=..., ...).images[0]\r\n```\r\n\r\n```py\r\n# inpaint + controlnet\r\npipeline(prompt=prompt, image=...,  mask_image=..., strength=..., control_image=..., ...).images[0]\r\n```\r\n\r\n\r\n",
      "html_url": "https://github.com/huggingface/diffusers/pull/12220",
      "created_at": "2025-08-22T18:43:03Z",
      "merged_at": "2025-09-08T10:27:02Z",
      "merge_commit_sha": "f50b18eec7d646bf98aef576dbb0f47ff512beaa",
      "base_ref": "main",
      "head_sha": "9c5830e8cfd890acc3e05965573fae4d81744546",
      "user": "yiyixuxu",
      "files": [
        {
          "filename": "docs/source/en/api/image_processor.md",
          "status": "modified",
          "additions": 6,
          "deletions": 0,
          "changes": 6,
          "patch": "@@ -20,6 +20,12 @@ All pipelines with [`VaeImageProcessor`] accept PIL Image, PyTorch tensor, or Nu\n \n [[autodoc]] image_processor.VaeImageProcessor\n \n+## InpaintProcessor\n+\n+The [`InpaintProcessor`] accepts `mask` and `image` inputs and process them together. Optionally, it can accept padding_mask_crop and apply mask overlay.\n+\n+[[autodoc]] image_processor.InpaintProcessor\n+\n ## VaeImageProcessorLDM3D\n \n The [`VaeImageProcessorLDM3D`] accepts RGB and depth inputs and returns RGB and depth outputs."
        },
        {
          "filename": "src/diffusers/__init__.py",
          "status": "modified",
          "additions": 8,
          "deletions": 0,
          "changes": 8,
          "patch": "@@ -372,6 +372,10 @@\n         [\n             \"FluxAutoBlocks\",\n             \"FluxModularPipeline\",\n+            \"QwenImageAutoBlocks\",\n+            \"QwenImageEditAutoBlocks\",\n+            \"QwenImageEditModularPipeline\",\n+            \"QwenImageModularPipeline\",\n             \"StableDiffusionXLAutoBlocks\",\n             \"StableDiffusionXLModularPipeline\",\n             \"WanAutoBlocks\",\n@@ -1017,6 +1021,10 @@\n         from .modular_pipelines import (\n             FluxAutoBlocks,\n             FluxModularPipeline,\n+            QwenImageAutoBlocks,\n+            QwenImageEditAutoBlocks,\n+            QwenImageEditModularPipeline,\n+            QwenImageModularPipeline,\n             StableDiffusionXLAutoBlocks,\n             StableDiffusionXLModularPipeline,\n             WanAutoBlocks,"
        },
        {
          "filename": "src/diffusers/hooks/_helpers.py",
          "status": "modified",
          "additions": 10,
          "deletions": 0,
          "changes": 10,
          "patch": "@@ -108,6 +108,7 @@ def _register_attention_processors_metadata():\n     from ..models.attention_processor import AttnProcessor2_0\n     from ..models.transformers.transformer_cogview4 import CogView4AttnProcessor\n     from ..models.transformers.transformer_flux import FluxAttnProcessor\n+    from ..models.transformers.transformer_qwenimage import QwenDoubleStreamAttnProcessor2_0\n     from ..models.transformers.transformer_wan import WanAttnProcessor2_0\n \n     # AttnProcessor2_0\n@@ -140,6 +141,14 @@ def _register_attention_processors_metadata():\n         metadata=AttentionProcessorMetadata(skip_processor_output_fn=_skip_proc_output_fn_Attention_FluxAttnProcessor),\n     )\n \n+    # QwenDoubleStreamAttnProcessor2\n+    AttentionProcessorRegistry.register(\n+        model_class=QwenDoubleStreamAttnProcessor2_0,\n+        metadata=AttentionProcessorMetadata(\n+            skip_processor_output_fn=_skip_proc_output_fn_Attention_QwenDoubleStreamAttnProcessor2_0\n+        ),\n+    )\n+\n \n def _register_transformer_blocks_metadata():\n     from ..models.attention import BasicTransformerBlock\n@@ -298,4 +307,5 @@ def _skip_attention___ret___hidden_states___encoder_hidden_states(self, *args, *\n _skip_proc_output_fn_Attention_WanAttnProcessor2_0 = _skip_attention___ret___hidden_states\n # not sure what this is yet.\n _skip_proc_output_fn_Attention_FluxAttnProcessor = _skip_attention___ret___hidden_states\n+_skip_proc_output_fn_Attention_QwenDoubleStreamAttnProcessor2_0 = _skip_attention___ret___hidden_states\n # fmt: on"
        },
        {
          "filename": "src/diffusers/image_processor.py",
          "status": "modified",
          "additions": 132,
          "deletions": 0,
          "changes": 132,
          "patch": "@@ -523,6 +523,7 @@ def resize(\n                 size=(height, width),\n             )\n             image = self.pt_to_numpy(image)\n+\n         return image\n \n     def binarize(self, image: PIL.Image.Image) -> PIL.Image.Image:\n@@ -838,6 +839,137 @@ def apply_overlay(\n         return image\n \n \n+class InpaintProcessor(ConfigMixin):\n+    \"\"\"\n+    Image processor for inpainting image and mask.\n+    \"\"\"\n+\n+    config_name = CONFIG_NAME\n+\n+    @register_to_config\n+    def __init__(\n+        self,\n+        do_resize: bool = True,\n+        vae_scale_factor: int = 8,\n+        vae_latent_channels: int = 4,\n+        resample: str = \"lanczos\",\n+        reducing_gap: int = None,\n+        do_normalize: bool = True,\n+        do_binarize: bool = False,\n+        do_convert_grayscale: bool = False,\n+        mask_do_normalize: bool = False,\n+        mask_do_binarize: bool = True,\n+        mask_do_convert_grayscale: bool = True,\n+    ):\n+        super().__init__()\n+\n+        self._image_processor = VaeImageProcessor(\n+            do_resize=do_resize,\n+            vae_scale_factor=vae_scale_factor,\n+            vae_latent_channels=vae_latent_channels,\n+            resample=resample,\n+            reducing_gap=reducing_gap,\n+            do_normalize=do_normalize,\n+            do_binarize=do_binarize,\n+            do_convert_grayscale=do_convert_grayscale,\n+        )\n+        self._mask_processor = VaeImageProcessor(\n+            do_resize=do_resize,\n+            vae_scale_factor=vae_scale_factor,\n+            vae_latent_channels=vae_latent_channels,\n+            resample=resample,\n+            reducing_gap=reducing_gap,\n+            do_normalize=mask_do_normalize,\n+            do_binarize=mask_do_binarize,\n+            do_convert_grayscale=mask_do_convert_grayscale,\n+        )\n+\n+    def preprocess(\n+        self,\n+        image: PIL.Image.Image,\n+        mask: PIL.Image.Image = None,\n+        height: int = None,\n+        width: int = None,\n+        padding_mask_crop: Optional[int] = None,\n+    ) -> Tuple[torch.Tensor, torch.Tensor]:\n+        \"\"\"\n+        Preprocess the image and mask.\n+        \"\"\"\n+        if mask is None and padding_mask_crop is not None:\n+            raise ValueError(\"mask must be provided if padding_mask_crop is provided\")\n+\n+        # if mask is None, same behavior as regular image processor\n+        if mask is None:\n+            return self._image_processor.preprocess(image, height=height, width=width)\n+\n+        if padding_mask_crop is not None:\n+            crops_coords = self._image_processor.get_crop_region(mask, width, height, pad=padding_mask_crop)\n+            resize_mode = \"fill\"\n+        else:\n+            crops_coords = None\n+            resize_mode = \"default\"\n+\n+        processed_image = self._image_processor.preprocess(\n+            image,\n+            height=height,\n+            width=width,\n+            crops_coords=crops_coords,\n+            resize_mode=resize_mode,\n+        )\n+\n+        processed_mask = self._mask_processor.preprocess(\n+            mask,\n+            height=height,\n+            width=width,\n+            resize_mode=resize_mode,\n+            crops_coords=crops_coords,\n+        )\n+\n+        if crops_coords is not None:\n+            postprocessing_kwargs = {\n+                \"crops_coords\": crops_coords,\n+                \"original_image\": image,\n+                \"original_mask\": mask,\n+            }\n+        else:\n+            postprocessing_kwargs = {\n+                \"crops_coords\": None,\n+                \"original_image\": None,\n+                \"original_mask\": None,\n+            }\n+\n+        return processed_image, processed_mask, postprocessing_kwargs\n+\n+    def postprocess(\n+        self,\n+        image: torch.Tensor,\n+        output_type: str = \"pil\",\n+        original_image: Optional[PIL.Image.Image] = None,\n+        original_mask: Optional[PIL.Image.Image] = None,\n+        crops_coords: Optional[Tuple[int, int, int, int]] = None,\n+    ) -> Tuple[PIL.Image.Image, PIL.Image.Image]:\n+        \"\"\"\n+        Postprocess the image, optionally apply mask overlay\n+        \"\"\"\n+        image = self._image_processor.postprocess(\n+            image,\n+            output_type=output_type,\n+        )\n+        # optionally apply the mask overlay\n+        if crops_coords is not None and (original_image is None or original_mask is None):\n+            raise ValueError(\"original_image and original_mask must be provided if crops_coords is provided\")\n+\n+        elif crops_coords is not None and output_type != \"pil\":\n+            raise ValueError(\"output_type must be 'pil' if crops_coords is provided\")\n+\n+        elif crops_coords is not None:\n+            image = [\n+                self._image_processor.apply_overlay(original_mask, original_image, i, crops_coords) for i in image\n+            ]\n+\n+        return image\n+\n+\n class VaeImageProcessorLDM3D(VaeImageProcessor):\n     \"\"\"\n     Image processor for VAE LDM3D."
        },
        {
          "filename": "src/diffusers/modular_pipelines/__init__.py",
          "status": "modified",
          "additions": 12,
          "deletions": 0,
          "changes": 12,
          "patch": "@@ -47,6 +47,12 @@\n     _import_structure[\"stable_diffusion_xl\"] = [\"StableDiffusionXLAutoBlocks\", \"StableDiffusionXLModularPipeline\"]\n     _import_structure[\"wan\"] = [\"WanAutoBlocks\", \"WanModularPipeline\"]\n     _import_structure[\"flux\"] = [\"FluxAutoBlocks\", \"FluxModularPipeline\"]\n+    _import_structure[\"qwenimage\"] = [\n+        \"QwenImageAutoBlocks\",\n+        \"QwenImageModularPipeline\",\n+        \"QwenImageEditModularPipeline\",\n+        \"QwenImageEditAutoBlocks\",\n+    ]\n     _import_structure[\"components_manager\"] = [\"ComponentsManager\"]\n \n if TYPE_CHECKING or DIFFUSERS_SLOW_IMPORT:\n@@ -68,6 +74,12 @@\n             SequentialPipelineBlocks,\n         )\n         from .modular_pipeline_utils import ComponentSpec, ConfigSpec, InputParam, InsertableDict, OutputParam\n+        from .qwenimage import (\n+            QwenImageAutoBlocks,\n+            QwenImageEditAutoBlocks,\n+            QwenImageEditModularPipeline,\n+            QwenImageModularPipeline,\n+        )\n         from .stable_diffusion_xl import StableDiffusionXLAutoBlocks, StableDiffusionXLModularPipeline\n         from .wan import WanAutoBlocks, WanModularPipeline\n else:"
        },
        {
          "filename": "src/diffusers/modular_pipelines/modular_pipeline.py",
          "status": "modified",
          "additions": 28,
          "deletions": 9,
          "changes": 37,
          "patch": "@@ -56,6 +56,8 @@\n         (\"stable-diffusion-xl\", \"StableDiffusionXLModularPipeline\"),\n         (\"wan\", \"WanModularPipeline\"),\n         (\"flux\", \"FluxModularPipeline\"),\n+        (\"qwenimage\", \"QwenImageModularPipeline\"),\n+        (\"qwenimage-edit\", \"QwenImageEditModularPipeline\"),\n     ]\n )\n \n@@ -64,6 +66,8 @@\n         (\"StableDiffusionXLModularPipeline\", \"StableDiffusionXLAutoBlocks\"),\n         (\"WanModularPipeline\", \"WanAutoBlocks\"),\n         (\"FluxModularPipeline\", \"FluxAutoBlocks\"),\n+        (\"QwenImageModularPipeline\", \"QwenImageAutoBlocks\"),\n+        (\"QwenImageEditModularPipeline\", \"QwenImageEditAutoBlocks\"),\n     ]\n )\n \n@@ -133,8 +137,8 @@ def __getattr__(self, name):\n         Allow attribute access to intermediate values. If an attribute is not found in the object, look for it in the\n         intermediates dict.\n         \"\"\"\n-        if name in self.intermediates:\n-            return self.intermediates[name]\n+        if name in self.values:\n+            return self.values[name]\n         raise AttributeError(f\"'{self.__class__.__name__}' object has no attribute '{name}'\")\n \n     def __repr__(self):\n@@ -548,8 +552,11 @@ class AutoPipelineBlocks(ModularPipelineBlocks):\n \n     def __init__(self):\n         sub_blocks = InsertableDict()\n-        for block_name, block_cls in zip(self.block_names, self.block_classes):\n-            sub_blocks[block_name] = block_cls()\n+        for block_name, block in zip(self.block_names, self.block_classes):\n+            if inspect.isclass(block):\n+                sub_blocks[block_name] = block()\n+            else:\n+                sub_blocks[block_name] = block\n         self.sub_blocks = sub_blocks\n         if not (len(self.block_classes) == len(self.block_names) == len(self.block_trigger_inputs)):\n             raise ValueError(\n@@ -830,7 +837,9 @@ def expected_configs(self):\n         return expected_configs\n \n     @classmethod\n-    def from_blocks_dict(cls, blocks_dict: Dict[str, Any]) -> \"SequentialPipelineBlocks\":\n+    def from_blocks_dict(\n+        cls, blocks_dict: Dict[str, Any], description: Optional[str] = None\n+    ) -> \"SequentialPipelineBlocks\":\n         \"\"\"Creates a SequentialPipelineBlocks instance from a dictionary of blocks.\n \n         Args:\n@@ -852,12 +861,19 @@ def from_blocks_dict(cls, blocks_dict: Dict[str, Any]) -> \"SequentialPipelineBlo\n         instance.block_classes = [block.__class__ for block in sub_blocks.values()]\n         instance.block_names = list(sub_blocks.keys())\n         instance.sub_blocks = sub_blocks\n+\n+        if description is not None:\n+            instance.description = description\n+\n         return instance\n \n     def __init__(self):\n         sub_blocks = InsertableDict()\n-        for block_name, block_cls in zip(self.block_names, self.block_classes):\n-            sub_blocks[block_name] = block_cls()\n+        for block_name, block in zip(self.block_names, self.block_classes):\n+            if inspect.isclass(block):\n+                sub_blocks[block_name] = block()\n+            else:\n+                sub_blocks[block_name] = block\n         self.sub_blocks = sub_blocks\n \n     def _get_inputs(self):\n@@ -1280,8 +1296,11 @@ def outputs(self) -> List[str]:\n \n     def __init__(self):\n         sub_blocks = InsertableDict()\n-        for block_name, block_cls in zip(self.block_names, self.block_classes):\n-            sub_blocks[block_name] = block_cls()\n+        for block_name, block in zip(self.block_names, self.block_classes):\n+            if inspect.isclass(block):\n+                sub_blocks[block_name] = block()\n+            else:\n+                sub_blocks[block_name] = block\n         self.sub_blocks = sub_blocks\n \n     @classmethod"
        },
        {
          "filename": "src/diffusers/modular_pipelines/qwenimage/__init__.py",
          "status": "added",
          "additions": 75,
          "deletions": 0,
          "changes": 75,
          "patch": "@@ -0,0 +1,75 @@\n+from typing import TYPE_CHECKING\n+\n+from ...utils import (\n+    DIFFUSERS_SLOW_IMPORT,\n+    OptionalDependencyNotAvailable,\n+    _LazyModule,\n+    get_objects_from_module,\n+    is_torch_available,\n+    is_transformers_available,\n+)\n+\n+\n+_dummy_objects = {}\n+_import_structure = {}\n+\n+try:\n+    if not (is_transformers_available() and is_torch_available()):\n+        raise OptionalDependencyNotAvailable()\n+except OptionalDependencyNotAvailable:\n+    from ...utils import dummy_torch_and_transformers_objects  # noqa F403\n+\n+    _dummy_objects.update(get_objects_from_module(dummy_torch_and_transformers_objects))\n+else:\n+    _import_structure[\"encoders\"] = [\"QwenImageTextEncoderStep\"]\n+    _import_structure[\"modular_blocks\"] = [\n+        \"ALL_BLOCKS\",\n+        \"AUTO_BLOCKS\",\n+        \"CONTROLNET_BLOCKS\",\n+        \"EDIT_AUTO_BLOCKS\",\n+        \"EDIT_BLOCKS\",\n+        \"EDIT_INPAINT_BLOCKS\",\n+        \"IMAGE2IMAGE_BLOCKS\",\n+        \"INPAINT_BLOCKS\",\n+        \"TEXT2IMAGE_BLOCKS\",\n+        \"QwenImageAutoBlocks\",\n+        \"QwenImageEditAutoBlocks\",\n+    ]\n+    _import_structure[\"modular_pipeline\"] = [\"QwenImageEditModularPipeline\", \"QwenImageModularPipeline\"]\n+\n+if TYPE_CHECKING or DIFFUSERS_SLOW_IMPORT:\n+    try:\n+        if not (is_transformers_available() and is_torch_available()):\n+            raise OptionalDependencyNotAvailable()\n+    except OptionalDependencyNotAvailable:\n+        from ...utils.dummy_torch_and_transformers_objects import *  # noqa F403\n+    else:\n+        from .encoders import (\n+            QwenImageTextEncoderStep,\n+        )\n+        from .modular_blocks import (\n+            ALL_BLOCKS,\n+            AUTO_BLOCKS,\n+            CONTROLNET_BLOCKS,\n+            EDIT_AUTO_BLOCKS,\n+            EDIT_BLOCKS,\n+            EDIT_INPAINT_BLOCKS,\n+            IMAGE2IMAGE_BLOCKS,\n+            INPAINT_BLOCKS,\n+            TEXT2IMAGE_BLOCKS,\n+            QwenImageAutoBlocks,\n+            QwenImageEditAutoBlocks,\n+        )\n+        from .modular_pipeline import QwenImageEditModularPipeline, QwenImageModularPipeline\n+else:\n+    import sys\n+\n+    sys.modules[__name__] = _LazyModule(\n+        __name__,\n+        globals()[\"__file__\"],\n+        _import_structure,\n+        module_spec=__spec__,\n+    )\n+\n+    for name, value in _dummy_objects.items():\n+        setattr(sys.modules[__name__], name, value)"
        },
        {
          "filename": "src/diffusers/modular_pipelines/qwenimage/before_denoise.py",
          "status": "added",
          "additions": 727,
          "deletions": 0,
          "changes": 727,
          "patch": "@@ -0,0 +1,727 @@\n+# Copyright 2025 Qwen-Image Team and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import inspect\n+from typing import List, Optional, Tuple, Union\n+\n+import numpy as np\n+import torch\n+\n+from ...models import QwenImageControlNetModel, QwenImageMultiControlNetModel\n+from ...schedulers import FlowMatchEulerDiscreteScheduler\n+from ...utils.torch_utils import randn_tensor, unwrap_module\n+from ..modular_pipeline import ModularPipelineBlocks, PipelineState\n+from ..modular_pipeline_utils import ComponentSpec, InputParam, OutputParam\n+from .modular_pipeline import QwenImageModularPipeline, QwenImagePachifier\n+\n+\n+# Copied from diffusers.pipelines.qwenimage.pipeline_qwenimage.calculate_shift\n+def calculate_shift(\n+    image_seq_len,\n+    base_seq_len: int = 256,\n+    max_seq_len: int = 4096,\n+    base_shift: float = 0.5,\n+    max_shift: float = 1.15,\n+):\n+    m = (max_shift - base_shift) / (max_seq_len - base_seq_len)\n+    b = base_shift - m * base_seq_len\n+    mu = image_seq_len * m + b\n+    return mu\n+\n+\n+# Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.retrieve_timesteps\n+def retrieve_timesteps(\n+    scheduler,\n+    num_inference_steps: Optional[int] = None,\n+    device: Optional[Union[str, torch.device]] = None,\n+    timesteps: Optional[List[int]] = None,\n+    sigmas: Optional[List[float]] = None,\n+    **kwargs,\n+):\n+    r\"\"\"\n+    Calls the scheduler's `set_timesteps` method and retrieves timesteps from the scheduler after the call. Handles\n+    custom timesteps. Any kwargs will be supplied to `scheduler.set_timesteps`.\n+\n+    Args:\n+        scheduler (`SchedulerMixin`):\n+            The scheduler to get timesteps from.\n+        num_inference_steps (`int`):\n+            The number of diffusion steps used when generating samples with a pre-trained model. If used, `timesteps`\n+            must be `None`.\n+        device (`str` or `torch.device`, *optional*):\n+            The device to which the timesteps should be moved to. If `None`, the timesteps are not moved.\n+        timesteps (`List[int]`, *optional*):\n+            Custom timesteps used to override the timestep spacing strategy of the scheduler. If `timesteps` is passed,\n+            `num_inference_steps` and `sigmas` must be `None`.\n+        sigmas (`List[float]`, *optional*):\n+            Custom sigmas used to override the timestep spacing strategy of the scheduler. If `sigmas` is passed,\n+            `num_inference_steps` and `timesteps` must be `None`.\n+\n+    Returns:\n+        `Tuple[torch.Tensor, int]`: A tuple where the first element is the timestep schedule from the scheduler and the\n+        second element is the number of inference steps.\n+    \"\"\"\n+    if timesteps is not None and sigmas is not None:\n+        raise ValueError(\"Only one of `timesteps` or `sigmas` can be passed. Please choose one to set custom values\")\n+    if timesteps is not None:\n+        accepts_timesteps = \"timesteps\" in set(inspect.signature(scheduler.set_timesteps).parameters.keys())\n+        if not accepts_timesteps:\n+            raise ValueError(\n+                f\"The current scheduler class {scheduler.__class__}'s `set_timesteps` does not support custom\"\n+                f\" timestep schedules. Please check whether you are using the correct scheduler.\"\n+            )\n+        scheduler.set_timesteps(timesteps=timesteps, device=device, **kwargs)\n+        timesteps = scheduler.timesteps\n+        num_inference_steps = len(timesteps)\n+    elif sigmas is not None:\n+        accept_sigmas = \"sigmas\" in set(inspect.signature(scheduler.set_timesteps).parameters.keys())\n+        if not accept_sigmas:\n+            raise ValueError(\n+                f\"The current scheduler class {scheduler.__class__}'s `set_timesteps` does not support custom\"\n+                f\" sigmas schedules. Please check whether you are using the correct scheduler.\"\n+            )\n+        scheduler.set_timesteps(sigmas=sigmas, device=device, **kwargs)\n+        timesteps = scheduler.timesteps\n+        num_inference_steps = len(timesteps)\n+    else:\n+        scheduler.set_timesteps(num_inference_steps, device=device, **kwargs)\n+        timesteps = scheduler.timesteps\n+    return timesteps, num_inference_steps\n+\n+\n+# modified from diffusers.pipelines.stable_diffusion_3.pipeline_stable_diffusion_3_img2img.StableDiffusion3Img2ImgPipeline.get_timesteps\n+def get_timesteps(scheduler, num_inference_steps, strength):\n+    # get the original timestep using init_timestep\n+    init_timestep = min(num_inference_steps * strength, num_inference_steps)\n+\n+    t_start = int(max(num_inference_steps - init_timestep, 0))\n+    timesteps = scheduler.timesteps[t_start * scheduler.order :]\n+    if hasattr(scheduler, \"set_begin_index\"):\n+        scheduler.set_begin_index(t_start * scheduler.order)\n+\n+    return timesteps, num_inference_steps - t_start\n+\n+\n+# Prepare Latents steps\n+\n+\n+class QwenImagePrepareLatentsStep(ModularPipelineBlocks):\n+    model_name = \"qwenimage\"\n+\n+    @property\n+    def description(self) -> str:\n+        return \"Prepare initial random noise for the generation process\"\n+\n+    @property\n+    def expected_components(self) -> List[ComponentSpec]:\n+        return [\n+            ComponentSpec(\"pachifier\", QwenImagePachifier, default_creation_method=\"from_config\"),\n+        ]\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        return [\n+            InputParam(name=\"height\"),\n+            InputParam(name=\"width\"),\n+            InputParam(name=\"num_images_per_prompt\", default=1),\n+            InputParam(name=\"generator\"),\n+            InputParam(\n+                name=\"batch_size\",\n+                required=True,\n+                type_hint=int,\n+                description=\"Number of prompts, the final batch size of model inputs should be batch_size * num_images_per_prompt. Can be generated in input step.\",\n+            ),\n+            InputParam(\n+                name=\"dtype\",\n+                required=True,\n+                type_hint=torch.dtype,\n+                description=\"The dtype of the model inputs, can be generated in input step.\",\n+            ),\n+        ]\n+\n+    @property\n+    def intermediate_outputs(self) -> List[OutputParam]:\n+        return [\n+            OutputParam(\n+                name=\"latents\",\n+                type_hint=torch.Tensor,\n+                description=\"The initial latents to use for the denoising process\",\n+            ),\n+        ]\n+\n+    @staticmethod\n+    def check_inputs(height, width, vae_scale_factor):\n+        if height is not None and height % (vae_scale_factor * 2) != 0:\n+            raise ValueError(f\"Height must be divisible by {vae_scale_factor * 2} but is {height}\")\n+\n+        if width is not None and width % (vae_scale_factor * 2) != 0:\n+            raise ValueError(f\"Width must be divisible by {vae_scale_factor * 2} but is {width}\")\n+\n+    @torch.no_grad()\n+    def __call__(self, components: QwenImageModularPipeline, state: PipelineState) -> PipelineState:\n+        block_state = self.get_block_state(state)\n+\n+        self.check_inputs(\n+            height=block_state.height,\n+            width=block_state.width,\n+            vae_scale_factor=components.vae_scale_factor,\n+        )\n+\n+        device = components._execution_device\n+        batch_size = block_state.batch_size * block_state.num_images_per_prompt\n+\n+        # we can update the height and width here since it's used to generate the initial\n+        block_state.height = block_state.height or components.default_height\n+        block_state.width = block_state.width or components.default_width\n+\n+        # VAE applies 8x compression on images but we must also account for packing which requires\n+        # latent height and width to be divisible by 2.\n+        latent_height = 2 * (int(block_state.height) // (components.vae_scale_factor * 2))\n+        latent_width = 2 * (int(block_state.width) // (components.vae_scale_factor * 2))\n+\n+        shape = (batch_size, components.num_channels_latents, 1, latent_height, latent_width)\n+        if isinstance(block_state.generator, list) and len(block_state.generator) != batch_size:\n+            raise ValueError(\n+                f\"You have passed a list of generators of length {len(block_state.generator)}, but requested an effective batch\"\n+                f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n+            )\n+\n+        block_state.latents = randn_tensor(\n+            shape, generator=block_state.generator, device=device, dtype=block_state.dtype\n+        )\n+        block_state.latents = components.pachifier.pack_latents(block_state.latents)\n+\n+        self.set_block_state(state, block_state)\n+\n+        return components, state\n+\n+\n+class QwenImagePrepareLatentsWithStrengthStep(ModularPipelineBlocks):\n+    model_name = \"qwenimage\"\n+\n+    @property\n+    def description(self) -> str:\n+        return \"Step that adds noise to image latents for image-to-image/inpainting. Should be run after set_timesteps, prepare_latents. Both noise and image latents should alreadybe patchified.\"\n+\n+    @property\n+    def expected_components(self) -> List[ComponentSpec]:\n+        return [\n+            ComponentSpec(\"scheduler\", FlowMatchEulerDiscreteScheduler),\n+        ]\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        return [\n+            InputParam(\n+                name=\"latents\",\n+                required=True,\n+                type_hint=torch.Tensor,\n+                description=\"The initial random noised, can be generated in prepare latent step.\",\n+            ),\n+            InputParam(\n+                name=\"image_latents\",\n+                required=True,\n+                type_hint=torch.Tensor,\n+                description=\"The image latents to use for the denoising process. Can be generated in vae encoder and packed in input step.\",\n+            ),\n+            InputParam(\n+                name=\"timesteps\",\n+                required=True,\n+                type_hint=torch.Tensor,\n+                description=\"The timesteps to use for the denoising process. Can be generated in set_timesteps step.\",\n+            ),\n+        ]\n+\n+    @property\n+    def intermediate_outputs(self) -> List[OutputParam]:\n+        return [\n+            OutputParam(\n+                name=\"initial_noise\",\n+                type_hint=torch.Tensor,\n+                description=\"The initial random noised used for inpainting denoising.\",\n+            ),\n+        ]\n+\n+    @staticmethod\n+    def check_inputs(image_latents, latents):\n+        if image_latents.shape[0] != latents.shape[0]:\n+            raise ValueError(\n+                f\"`image_latents` must have have same batch size as `latents`, but got {image_latents.shape[0]} and {latents.shape[0]}\"\n+            )\n+\n+        if image_latents.ndim != 3:\n+            raise ValueError(f\"`image_latents` must have 3 dimensions (patchified), but got {image_latents.ndim}\")\n+\n+    @torch.no_grad()\n+    def __call__(self, components: QwenImageModularPipeline, state: PipelineState) -> PipelineState:\n+        block_state = self.get_block_state(state)\n+\n+        self.check_inputs(\n+            image_latents=block_state.image_latents,\n+            latents=block_state.latents,\n+        )\n+\n+        # prepare latent timestep\n+        latent_timestep = block_state.timesteps[:1].repeat(block_state.latents.shape[0])\n+\n+        # make copy of initial_noise\n+        block_state.initial_noise = block_state.latents\n+\n+        # scale noise\n+        block_state.latents = components.scheduler.scale_noise(\n+            block_state.image_latents, latent_timestep, block_state.latents\n+        )\n+\n+        self.set_block_state(state, block_state)\n+\n+        return components, state\n+\n+\n+class QwenImageCreateMaskLatentsStep(ModularPipelineBlocks):\n+    model_name = \"qwenimage\"\n+\n+    @property\n+    def description(self) -> str:\n+        return \"Step that creates mask latents from preprocessed mask_image by interpolating to latent space.\"\n+\n+    @property\n+    def expected_components(self) -> List[ComponentSpec]:\n+        return [\n+            ComponentSpec(\"pachifier\", QwenImagePachifier, default_creation_method=\"from_config\"),\n+        ]\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        return [\n+            InputParam(\n+                name=\"processed_mask_image\",\n+                required=True,\n+                type_hint=torch.Tensor,\n+                description=\"The processed mask to use for the inpainting process.\",\n+            ),\n+            InputParam(name=\"height\", required=True),\n+            InputParam(name=\"width\", required=True),\n+            InputParam(name=\"dtype\", required=True),\n+        ]\n+\n+    @property\n+    def intermediate_outputs(self) -> List[OutputParam]:\n+        return [\n+            OutputParam(\n+                name=\"mask\", type_hint=torch.Tensor, description=\"The mask to use for the inpainting process.\"\n+            ),\n+        ]\n+\n+    @torch.no_grad()\n+    def __call__(self, components: QwenImageModularPipeline, state: PipelineState) -> PipelineState:\n+        block_state = self.get_block_state(state)\n+\n+        device = components._execution_device\n+\n+        # VAE applies 8x compression on images but we must also account for packing which requires\n+        # latent height and width to be divisible by 2.\n+\n+        height_latents = 2 * (int(block_state.height) // (components.vae_scale_factor * 2))\n+        width_latents = 2 * (int(block_state.width) // (components.vae_scale_factor * 2))\n+\n+        block_state.mask = torch.nn.functional.interpolate(\n+            block_state.processed_mask_image,\n+            size=(height_latents, width_latents),\n+        )\n+\n+        block_state.mask = block_state.mask.unsqueeze(2)\n+        block_state.mask = block_state.mask.repeat(1, components.num_channels_latents, 1, 1, 1)\n+        block_state.mask = block_state.mask.to(device=device, dtype=block_state.dtype)\n+\n+        block_state.mask = components.pachifier.pack_latents(block_state.mask)\n+\n+        self.set_block_state(state, block_state)\n+\n+        return components, state\n+\n+\n+# Set Timesteps steps\n+\n+\n+class QwenImageSetTimestepsStep(ModularPipelineBlocks):\n+    model_name = \"qwenimage\"\n+\n+    @property\n+    def description(self) -> str:\n+        return \"Step that sets the the scheduler's timesteps for text-to-image generation. Should be run after prepare latents step.\"\n+\n+    @property\n+    def expected_components(self) -> List[ComponentSpec]:\n+        return [\n+            ComponentSpec(\"scheduler\", FlowMatchEulerDiscreteScheduler),\n+        ]\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        return [\n+            InputParam(name=\"num_inference_steps\", default=50),\n+            InputParam(name=\"sigmas\"),\n+            InputParam(\n+                name=\"latents\",\n+                required=True,\n+                type_hint=torch.Tensor,\n+                description=\"The latents to use for the denoising process, used to calculate the image sequence length.\",\n+            ),\n+        ]\n+\n+    @property\n+    def intermediate_outputs(self) -> List[OutputParam]:\n+        return [\n+            OutputParam(\n+                name=\"timesteps\", type_hint=torch.Tensor, description=\"The timesteps to use for the denoising process\"\n+            ),\n+        ]\n+\n+    def __call__(self, components: QwenImageModularPipeline, state: PipelineState) -> PipelineState:\n+        block_state = self.get_block_state(state)\n+\n+        device = components._execution_device\n+        sigmas = (\n+            np.linspace(1.0, 1 / block_state.num_inference_steps, block_state.num_inference_steps)\n+            if block_state.sigmas is None\n+            else block_state.sigmas\n+        )\n+\n+        mu = calculate_shift(\n+            image_seq_len=block_state.latents.shape[1],\n+            base_seq_len=components.scheduler.config.get(\"base_image_seq_len\", 256),\n+            max_seq_len=components.scheduler.config.get(\"max_image_seq_len\", 4096),\n+            base_shift=components.scheduler.config.get(\"base_shift\", 0.5),\n+            max_shift=components.scheduler.config.get(\"max_shift\", 1.15),\n+        )\n+        block_state.timesteps, block_state.num_inference_steps = retrieve_timesteps(\n+            scheduler=components.scheduler,\n+            num_inference_steps=block_state.num_inference_steps,\n+            device=device,\n+            sigmas=sigmas,\n+            mu=mu,\n+        )\n+\n+        components.scheduler.set_begin_index(0)\n+\n+        self.set_block_state(state, block_state)\n+\n+        return components, state\n+\n+\n+class QwenImageSetTimestepsWithStrengthStep(ModularPipelineBlocks):\n+    model_name = \"qwenimage\"\n+\n+    @property\n+    def description(self) -> str:\n+        return \"Step that sets the the scheduler's timesteps for image-to-image generation, and inpainting. Should be run after prepare latents step.\"\n+\n+    @property\n+    def expected_components(self) -> List[ComponentSpec]:\n+        return [\n+            ComponentSpec(\"scheduler\", FlowMatchEulerDiscreteScheduler),\n+        ]\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        return [\n+            InputParam(name=\"num_inference_steps\", default=50),\n+            InputParam(name=\"sigmas\"),\n+            InputParam(\n+                name=\"latents\",\n+                required=True,\n+                type_hint=torch.Tensor,\n+                description=\"The latents to use for the denoising process, used to calculate the image sequence length.\",\n+            ),\n+            InputParam(name=\"strength\", default=0.9),\n+        ]\n+\n+    @property\n+    def intermediate_outputs(self) -> List[OutputParam]:\n+        return [\n+            OutputParam(\n+                name=\"timesteps\",\n+                type_hint=torch.Tensor,\n+                description=\"The timesteps to use for the denoising process. Can be generated in set_timesteps step.\",\n+            ),\n+        ]\n+\n+    def __call__(self, components: QwenImageModularPipeline, state: PipelineState) -> PipelineState:\n+        block_state = self.get_block_state(state)\n+\n+        device = components._execution_device\n+        sigmas = (\n+            np.linspace(1.0, 1 / block_state.num_inference_steps, block_state.num_inference_steps)\n+            if block_state.sigmas is None\n+            else block_state.sigmas\n+        )\n+\n+        mu = calculate_shift(\n+            image_seq_len=block_state.latents.shape[1],\n+            base_seq_len=components.scheduler.config.get(\"base_image_seq_len\", 256),\n+            max_seq_len=components.scheduler.config.get(\"max_image_seq_len\", 4096),\n+            base_shift=components.scheduler.config.get(\"base_shift\", 0.5),\n+            max_shift=components.scheduler.config.get(\"max_shift\", 1.15),\n+        )\n+        block_state.timesteps, block_state.num_inference_steps = retrieve_timesteps(\n+            scheduler=components.scheduler,\n+            num_inference_steps=block_state.num_inference_steps,\n+            device=device,\n+            sigmas=sigmas,\n+            mu=mu,\n+        )\n+\n+        block_state.timesteps, block_state.num_inference_steps = get_timesteps(\n+            scheduler=components.scheduler,\n+            num_inference_steps=block_state.num_inference_steps,\n+            strength=block_state.strength,\n+        )\n+\n+        self.set_block_state(state, block_state)\n+\n+        return components, state\n+\n+\n+# other inputs for denoiser\n+\n+## RoPE inputs for denoiser\n+\n+\n+class QwenImageRoPEInputsStep(ModularPipelineBlocks):\n+    model_name = \"qwenimage\"\n+\n+    @property\n+    def description(self) -> str:\n+        return (\n+            \"Step that prepares the RoPE inputs for the denoising process. Should be place after prepare_latents step\"\n+        )\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        return [\n+            InputParam(name=\"batch_size\", required=True),\n+            InputParam(name=\"height\", required=True),\n+            InputParam(name=\"width\", required=True),\n+            InputParam(name=\"prompt_embeds_mask\"),\n+            InputParam(name=\"negative_prompt_embeds_mask\"),\n+        ]\n+\n+    @property\n+    def intermediate_outputs(self) -> List[OutputParam]:\n+        return [\n+            OutputParam(\n+                name=\"img_shapes\",\n+                type_hint=List[List[Tuple[int, int, int]]],\n+                description=\"The shapes of the images latents, used for RoPE calculation\",\n+            ),\n+            OutputParam(\n+                name=\"txt_seq_lens\",\n+                kwargs_type=\"denoiser_input_fields\",\n+                type_hint=List[int],\n+                description=\"The sequence lengths of the prompt embeds, used for RoPE calculation\",\n+            ),\n+            OutputParam(\n+                name=\"negative_txt_seq_lens\",\n+                kwargs_type=\"denoiser_input_fields\",\n+                type_hint=List[int],\n+                description=\"The sequence lengths of the negative prompt embeds, used for RoPE calculation\",\n+            ),\n+        ]\n+\n+    def __call__(self, components: QwenImageModularPipeline, state: PipelineState) -> PipelineState:\n+        block_state = self.get_block_state(state)\n+\n+        block_state.img_shapes = [\n+            [\n+                (\n+                    1,\n+                    block_state.height // components.vae_scale_factor // 2,\n+                    block_state.width // components.vae_scale_factor // 2,\n+                )\n+            ]\n+            * block_state.batch_size\n+        ]\n+        block_state.txt_seq_lens = (\n+            block_state.prompt_embeds_mask.sum(dim=1).tolist() if block_state.prompt_embeds_mask is not None else None\n+        )\n+        block_state.negative_txt_seq_lens = (\n+            block_state.negative_prompt_embeds_mask.sum(dim=1).tolist()\n+            if block_state.negative_prompt_embeds_mask is not None\n+            else None\n+        )\n+\n+        self.set_block_state(state, block_state)\n+\n+        return components, state\n+\n+\n+class QwenImageEditRoPEInputsStep(ModularPipelineBlocks):\n+    model_name = \"qwenimage\"\n+\n+    @property\n+    def description(self) -> str:\n+        return \"Step that prepares the RoPE inputs for denoising process. This is used in QwenImage Edit. Should be place after prepare_latents step\"\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        return [\n+            InputParam(name=\"batch_size\", required=True),\n+            InputParam(\n+                name=\"resized_image\", required=True, type_hint=torch.Tensor, description=\"The resized image input\"\n+            ),\n+            InputParam(name=\"height\", required=True),\n+            InputParam(name=\"width\", required=True),\n+            InputParam(name=\"prompt_embeds_mask\"),\n+            InputParam(name=\"negative_prompt_embeds_mask\"),\n+        ]\n+\n+    @property\n+    def intermediate_outputs(self) -> List[OutputParam]:\n+        return [\n+            OutputParam(\n+                name=\"img_shapes\",\n+                type_hint=List[List[Tuple[int, int, int]]],\n+                description=\"The shapes of the images latents, used for RoPE calculation\",\n+            ),\n+            OutputParam(\n+                name=\"txt_seq_lens\",\n+                kwargs_type=\"denoiser_input_fields\",\n+                type_hint=List[int],\n+                description=\"The sequence lengths of the prompt embeds, used for RoPE calculation\",\n+            ),\n+            OutputParam(\n+                name=\"negative_txt_seq_lens\",\n+                kwargs_type=\"denoiser_input_fields\",\n+                type_hint=List[int],\n+                description=\"The sequence lengths of the negative prompt embeds, used for RoPE calculation\",\n+            ),\n+        ]\n+\n+    def __call__(self, components: QwenImageModularPipeline, state: PipelineState) -> PipelineState:\n+        block_state = self.get_block_state(state)\n+\n+        # for edit, image size can be different from the target size (height/width)\n+        image = (\n+            block_state.resized_image[0] if isinstance(block_state.resized_image, list) else block_state.resized_image\n+        )\n+        image_width, image_height = image.size\n+\n+        block_state.img_shapes = [\n+            [\n+                (\n+                    1,\n+                    block_state.height // components.vae_scale_factor // 2,\n+                    block_state.width // components.vae_scale_factor // 2,\n+                ),\n+                (1, image_height // components.vae_scale_factor // 2, image_width // components.vae_scale_factor // 2),\n+            ]\n+        ] * block_state.batch_size\n+\n+        block_state.txt_seq_lens = (\n+            block_state.prompt_embeds_mask.sum(dim=1).tolist() if block_state.prompt_embeds_mask is not None else None\n+        )\n+        block_state.negative_txt_seq_lens = (\n+            block_state.negative_prompt_embeds_mask.sum(dim=1).tolist()\n+            if block_state.negative_prompt_embeds_mask is not None\n+            else None\n+        )\n+\n+        self.set_block_state(state, block_state)\n+\n+        return components, state\n+\n+\n+## ControlNet inputs for denoiser\n+class QwenImageControlNetBeforeDenoiserStep(ModularPipelineBlocks):\n+    model_name = \"qwenimage\"\n+\n+    @property\n+    def expected_components(self) -> List[ComponentSpec]:\n+        return [\n+            ComponentSpec(\"controlnet\", QwenImageControlNetModel),\n+        ]\n+\n+    @property\n+    def description(self) -> str:\n+        return \"step that prepare inputs for controlnet. Insert before the Denoise Step, after set_timesteps step.\"\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        return [\n+            InputParam(\"control_guidance_start\", default=0.0),\n+            InputParam(\"control_guidance_end\", default=1.0),\n+            InputParam(\"controlnet_conditioning_scale\", default=1.0),\n+            InputParam(\"control_image_latents\", required=True),\n+            InputParam(\n+                \"timesteps\",\n+                required=True,\n+                type_hint=torch.Tensor,\n+                description=\"The timesteps to use for the denoising process. Can be generated in set_timesteps step.\",\n+            ),\n+        ]\n+\n+    @property\n+    def intermediate_outputs(self) -> List[OutputParam]:\n+        return [\n+            OutputParam(\"controlnet_keep\", type_hint=List[float], description=\"The controlnet keep values\"),\n+        ]\n+\n+    @torch.no_grad()\n+    def __call__(self, components: QwenImageModularPipeline, state: PipelineState) -> PipelineState:\n+        block_state = self.get_block_state(state)\n+\n+        controlnet = unwrap_module(components.controlnet)\n+\n+        # control_guidance_start/control_guidance_end (align format)\n+        if not isinstance(block_state.control_guidance_start, list) and isinstance(\n+            block_state.control_guidance_end, list\n+        ):\n+            block_state.control_guidance_start = len(block_state.control_guidance_end) * [\n+                block_state.control_guidance_start\n+            ]\n+        elif not isinstance(block_state.control_guidance_end, list) and isinstance(\n+            block_state.control_guidance_start, list\n+        ):\n+            block_state.control_guidance_end = len(block_state.control_guidance_start) * [\n+                block_state.control_guidance_end\n+            ]\n+        elif not isinstance(block_state.control_guidance_start, list) and not isinstance(\n+            block_state.control_guidance_end, list\n+        ):\n+            mult = (\n+                len(block_state.control_image_latents) if isinstance(controlnet, QwenImageMultiControlNetModel) else 1\n+            )\n+            block_state.control_guidance_start, block_state.control_guidance_end = (\n+                mult * [block_state.control_guidance_start],\n+                mult * [block_state.control_guidance_end],\n+            )\n+\n+        # controlnet_conditioning_scale (align format)\n+        if isinstance(controlnet, QwenImageMultiControlNetModel) and isinstance(\n+            block_state.controlnet_conditioning_scale, float\n+        ):\n+            block_state.controlnet_conditioning_scale = [block_state.controlnet_conditioning_scale] * mult\n+\n+        # controlnet_keep\n+        block_state.controlnet_keep = []\n+        for i in range(len(block_state.timesteps)):\n+            keeps = [\n+                1.0 - float(i / len(block_state.timesteps) < s or (i + 1) / len(block_state.timesteps) > e)\n+                for s, e in zip(block_state.control_guidance_start, block_state.control_guidance_end)\n+            ]\n+            block_state.controlnet_keep.append(keeps[0] if isinstance(controlnet, QwenImageControlNetModel) else keeps)\n+\n+        self.set_block_state(state, block_state)\n+\n+        return components, state"
        },
        {
          "filename": "src/diffusers/modular_pipelines/qwenimage/decoders.py",
          "status": "added",
          "additions": 203,
          "deletions": 0,
          "changes": 203,
          "patch": "@@ -0,0 +1,203 @@\n+# Copyright 2025 Qwen-Image Team and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import List, Union\n+\n+import numpy as np\n+import PIL\n+import torch\n+\n+from ...configuration_utils import FrozenDict\n+from ...image_processor import InpaintProcessor, VaeImageProcessor\n+from ...models import AutoencoderKLQwenImage\n+from ...utils import logging\n+from ..modular_pipeline import ModularPipelineBlocks, PipelineState\n+from ..modular_pipeline_utils import ComponentSpec, InputParam, OutputParam\n+from .modular_pipeline import QwenImageModularPipeline, QwenImagePachifier\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class QwenImageDecoderStep(ModularPipelineBlocks):\n+    model_name = \"qwenimage\"\n+\n+    @property\n+    def description(self) -> str:\n+        return \"Step that decodes the latents to images\"\n+\n+    @property\n+    def expected_components(self) -> List[ComponentSpec]:\n+        components = [\n+            ComponentSpec(\"vae\", AutoencoderKLQwenImage),\n+            ComponentSpec(\"pachifier\", QwenImagePachifier, default_creation_method=\"from_config\"),\n+        ]\n+\n+        return components\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        return [\n+            InputParam(name=\"height\", required=True),\n+            InputParam(name=\"width\", required=True),\n+            InputParam(\n+                name=\"latents\",\n+                required=True,\n+                type_hint=torch.Tensor,\n+                description=\"The latents to decode, can be generated in the denoise step\",\n+            ),\n+        ]\n+\n+    @property\n+    def intermediate_outputs(self) -> List[str]:\n+        return [\n+            OutputParam(\n+                \"images\",\n+                type_hint=Union[List[PIL.Image.Image], List[torch.Tensor], List[np.array]],\n+                description=\"The generated images, can be a PIL.Image.Image, torch.Tensor or a numpy array\",\n+            )\n+        ]\n+\n+    @torch.no_grad()\n+    def __call__(self, components: QwenImageModularPipeline, state: PipelineState) -> PipelineState:\n+        block_state = self.get_block_state(state)\n+\n+        # YiYi Notes: remove support for output_type = \"latents', we can just skip decode/encode step in modular\n+        block_state.latents = components.pachifier.unpack_latents(\n+            block_state.latents, block_state.height, block_state.width\n+        )\n+        block_state.latents = block_state.latents.to(components.vae.dtype)\n+\n+        latents_mean = (\n+            torch.tensor(components.vae.config.latents_mean)\n+            .view(1, components.vae.config.z_dim, 1, 1, 1)\n+            .to(block_state.latents.device, block_state.latents.dtype)\n+        )\n+        latents_std = 1.0 / torch.tensor(components.vae.config.latents_std).view(\n+            1, components.vae.config.z_dim, 1, 1, 1\n+        ).to(block_state.latents.device, block_state.latents.dtype)\n+        block_state.latents = block_state.latents / latents_std + latents_mean\n+        block_state.images = components.vae.decode(block_state.latents, return_dict=False)[0][:, :, 0]\n+\n+        self.set_block_state(state, block_state)\n+        return components, state\n+\n+\n+class QwenImageProcessImagesOutputStep(ModularPipelineBlocks):\n+    model_name = \"qwenimage\"\n+\n+    @property\n+    def description(self) -> str:\n+        return \"postprocess the generated image\"\n+\n+    @property\n+    def expected_components(self) -> List[ComponentSpec]:\n+        return [\n+            ComponentSpec(\n+                \"image_processor\",\n+                VaeImageProcessor,\n+                config=FrozenDict({\"vae_scale_factor\": 16}),\n+                default_creation_method=\"from_config\",\n+            ),\n+        ]\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        return [\n+            InputParam(\"images\", required=True, description=\"the generated image from decoders step\"),\n+            InputParam(\n+                name=\"output_type\",\n+                default=\"pil\",\n+                type_hint=str,\n+                description=\"The type of the output images, can be 'pil', 'np', 'pt'\",\n+            ),\n+        ]\n+\n+    @staticmethod\n+    def check_inputs(output_type):\n+        if output_type not in [\"pil\", \"np\", \"pt\"]:\n+            raise ValueError(f\"Invalid output_type: {output_type}\")\n+\n+    @torch.no_grad()\n+    def __call__(self, components: QwenImageModularPipeline, state: PipelineState):\n+        block_state = self.get_block_state(state)\n+\n+        self.check_inputs(block_state.output_type)\n+\n+        block_state.images = components.image_processor.postprocess(\n+            image=block_state.images,\n+            output_type=block_state.output_type,\n+        )\n+\n+        self.set_block_state(state, block_state)\n+        return components, state\n+\n+\n+class QwenImageInpaintProcessImagesOutputStep(ModularPipelineBlocks):\n+    model_name = \"qwenimage\"\n+\n+    @property\n+    def description(self) -> str:\n+        return \"postprocess the generated image, optional apply the mask overally to the original image..\"\n+\n+    @property\n+    def expected_components(self) -> List[ComponentSpec]:\n+        return [\n+            ComponentSpec(\n+                \"image_mask_processor\",\n+                InpaintProcessor,\n+                config=FrozenDict({\"vae_scale_factor\": 16}),\n+                default_creation_method=\"from_config\",\n+            ),\n+        ]\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        return [\n+            InputParam(\"images\", required=True, description=\"the generated image from decoders step\"),\n+            InputParam(\n+                name=\"output_type\",\n+                default=\"pil\",\n+                type_hint=str,\n+                description=\"The type of the output images, can be 'pil', 'np', 'pt'\",\n+            ),\n+            InputParam(\"mask_overlay_kwargs\"),\n+        ]\n+\n+    @staticmethod\n+    def check_inputs(output_type, mask_overlay_kwargs):\n+        if output_type not in [\"pil\", \"np\", \"pt\"]:\n+            raise ValueError(f\"Invalid output_type: {output_type}\")\n+\n+        if mask_overlay_kwargs and output_type != \"pil\":\n+            raise ValueError(\"only support output_type 'pil' for mask overlay\")\n+\n+    @torch.no_grad()\n+    def __call__(self, components: QwenImageModularPipeline, state: PipelineState):\n+        block_state = self.get_block_state(state)\n+\n+        self.check_inputs(block_state.output_type, block_state.mask_overlay_kwargs)\n+\n+        if block_state.mask_overlay_kwargs is None:\n+            mask_overlay_kwargs = {}\n+        else:\n+            mask_overlay_kwargs = block_state.mask_overlay_kwargs\n+\n+        block_state.images = components.image_mask_processor.postprocess(\n+            image=block_state.images,\n+            **mask_overlay_kwargs,\n+        )\n+\n+        self.set_block_state(state, block_state)\n+        return components, state"
        },
        {
          "filename": "src/diffusers/modular_pipelines/qwenimage/denoise.py",
          "status": "added",
          "additions": 668,
          "deletions": 0,
          "changes": 668,
          "patch": "@@ -0,0 +1,668 @@\n+# Copyright 2025 Qwen-Image Team and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import List, Tuple\n+\n+import torch\n+\n+from ...configuration_utils import FrozenDict\n+from ...guiders import ClassifierFreeGuidance\n+from ...models import QwenImageControlNetModel, QwenImageTransformer2DModel\n+from ...schedulers import FlowMatchEulerDiscreteScheduler\n+from ...utils import logging\n+from ..modular_pipeline import BlockState, LoopSequentialPipelineBlocks, ModularPipelineBlocks, PipelineState\n+from ..modular_pipeline_utils import ComponentSpec, InputParam, OutputParam\n+from .modular_pipeline import QwenImageModularPipeline\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class QwenImageLoopBeforeDenoiser(ModularPipelineBlocks):\n+    model_name = \"qwenimage\"\n+\n+    @property\n+    def description(self) -> str:\n+        return (\n+            \"step within the denoising loop that prepares the latent input for the denoiser. \"\n+            \"This block should be used to compose the `sub_blocks` attribute of a `LoopSequentialPipelineBlocks` \"\n+            \"object (e.g. `QwenImageDenoiseLoopWrapper`)\"\n+        )\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        return [\n+            InputParam(\n+                \"latents\",\n+                required=True,\n+                type_hint=torch.Tensor,\n+                description=\"The initial latents to use for the denoising process. Can be generated in prepare_latent step.\",\n+            ),\n+        ]\n+\n+    @torch.no_grad()\n+    def __call__(self, components: QwenImageModularPipeline, block_state: BlockState, i: int, t: torch.Tensor):\n+        # one timestep\n+        block_state.timestep = t.expand(block_state.latents.shape[0]).to(block_state.latents.dtype)\n+        block_state.latent_model_input = block_state.latents\n+        return components, block_state\n+\n+\n+class QwenImageEditLoopBeforeDenoiser(ModularPipelineBlocks):\n+    model_name = \"qwenimage\"\n+\n+    @property\n+    def description(self) -> str:\n+        return (\n+            \"step within the denoising loop that prepares the latent input for the denoiser. \"\n+            \"This block should be used to compose the `sub_blocks` attribute of a `LoopSequentialPipelineBlocks` \"\n+            \"object (e.g. `QwenImageDenoiseLoopWrapper`)\"\n+        )\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        return [\n+            InputParam(\n+                \"latents\",\n+                required=True,\n+                type_hint=torch.Tensor,\n+                description=\"The initial latents to use for the denoising process. Can be generated in prepare_latent step.\",\n+            ),\n+            InputParam(\n+                \"image_latents\",\n+                required=True,\n+                type_hint=torch.Tensor,\n+                description=\"The initial image latents to use for the denoising process. Can be encoded in vae_encoder step and packed in prepare_image_latents step.\",\n+            ),\n+        ]\n+\n+    @torch.no_grad()\n+    def __call__(self, components: QwenImageModularPipeline, block_state: BlockState, i: int, t: torch.Tensor):\n+        # one timestep\n+\n+        block_state.latent_model_input = torch.cat([block_state.latents, block_state.image_latents], dim=1)\n+        block_state.timestep = t.expand(block_state.latents.shape[0]).to(block_state.latents.dtype)\n+        return components, block_state\n+\n+\n+class QwenImageLoopBeforeDenoiserControlNet(ModularPipelineBlocks):\n+    model_name = \"qwenimage\"\n+\n+    @property\n+    def expected_components(self) -> List[ComponentSpec]:\n+        return [\n+            ComponentSpec(\n+                \"guider\",\n+                ClassifierFreeGuidance,\n+                config=FrozenDict({\"guidance_scale\": 4.0}),\n+                default_creation_method=\"from_config\",\n+            ),\n+            ComponentSpec(\"controlnet\", QwenImageControlNetModel),\n+        ]\n+\n+    @property\n+    def description(self) -> str:\n+        return (\n+            \"step within the denoising loop that runs the controlnet before the denoiser. \"\n+            \"This block should be used to compose the `sub_blocks` attribute of a `LoopSequentialPipelineBlocks` \"\n+            \"object (e.g. `QwenImageDenoiseLoopWrapper`)\"\n+        )\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        return [\n+            InputParam(\n+                \"control_image_latents\",\n+                required=True,\n+                type_hint=torch.Tensor,\n+                description=\"The control image to use for the denoising process. Can be generated in prepare_controlnet_inputs step.\",\n+            ),\n+            InputParam(\n+                \"controlnet_conditioning_scale\",\n+                type_hint=float,\n+                description=\"The controlnet conditioning scale value to use for the denoising process. Can be generated in prepare_controlnet_inputs step.\",\n+            ),\n+            InputParam(\n+                \"controlnet_keep\",\n+                required=True,\n+                type_hint=List[float],\n+                description=\"The controlnet keep values to use for the denoising process. Can be generated in prepare_controlnet_inputs step.\",\n+            ),\n+            InputParam(\n+                \"num_inference_steps\",\n+                required=True,\n+                type_hint=int,\n+                description=\"The number of inference steps to use for the denoising process. Can be generated in set_timesteps step.\",\n+            ),\n+            InputParam(\n+                kwargs_type=\"denoiser_input_fields\",\n+                description=(\n+                    \"All conditional model inputs for the denoiser. \"\n+                    \"It should contain prompt_embeds/negative_prompt_embeds, txt_seq_lens/negative_txt_seq_lens.\"\n+                ),\n+            ),\n+        ]\n+\n+    @torch.no_grad()\n+    def __call__(self, components: QwenImageModularPipeline, block_state: BlockState, i: int, t: int):\n+        # cond_scale for the timestep (controlnet input)\n+        if isinstance(block_state.controlnet_keep[i], list):\n+            block_state.cond_scale = [\n+                c * s for c, s in zip(block_state.controlnet_conditioning_scale, block_state.controlnet_keep[i])\n+            ]\n+        else:\n+            controlnet_cond_scale = block_state.controlnet_conditioning_scale\n+            if isinstance(controlnet_cond_scale, list):\n+                controlnet_cond_scale = controlnet_cond_scale[0]\n+            block_state.cond_scale = controlnet_cond_scale * block_state.controlnet_keep[i]\n+\n+        # run controlnet for the guidance batch\n+        controlnet_block_samples = components.controlnet(\n+            hidden_states=block_state.latent_model_input,\n+            controlnet_cond=block_state.control_image_latents,\n+            conditioning_scale=block_state.cond_scale,\n+            timestep=block_state.timestep / 1000,\n+            img_shapes=block_state.img_shapes,\n+            encoder_hidden_states=block_state.prompt_embeds,\n+            encoder_hidden_states_mask=block_state.prompt_embeds_mask,\n+            txt_seq_lens=block_state.txt_seq_lens,\n+            return_dict=False,\n+        )\n+\n+        block_state.additional_cond_kwargs[\"controlnet_block_samples\"] = controlnet_block_samples\n+\n+        return components, block_state\n+\n+\n+class QwenImageLoopDenoiser(ModularPipelineBlocks):\n+    model_name = \"qwenimage\"\n+\n+    @property\n+    def description(self) -> str:\n+        return (\n+            \"step within the denoising loop that denoise the latent input for the denoiser. \"\n+            \"This block should be used to compose the `sub_blocks` attribute of a `LoopSequentialPipelineBlocks` \"\n+            \"object (e.g. `QwenImageDenoiseLoopWrapper`)\"\n+        )\n+\n+    @property\n+    def expected_components(self) -> List[ComponentSpec]:\n+        return [\n+            ComponentSpec(\n+                \"guider\",\n+                ClassifierFreeGuidance,\n+                config=FrozenDict({\"guidance_scale\": 4.0}),\n+                default_creation_method=\"from_config\",\n+            ),\n+            ComponentSpec(\"transformer\", QwenImageTransformer2DModel),\n+        ]\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        return [\n+            InputParam(\"attention_kwargs\"),\n+            InputParam(\n+                \"latents\",\n+                required=True,\n+                type_hint=torch.Tensor,\n+                description=\"The latents to use for the denoising process. Can be generated in prepare_latents step.\",\n+            ),\n+            InputParam(\n+                \"num_inference_steps\",\n+                required=True,\n+                type_hint=int,\n+                description=\"The number of inference steps to use for the denoising process. Can be generated in set_timesteps step.\",\n+            ),\n+            InputParam(\n+                kwargs_type=\"denoiser_input_fields\",\n+                description=\"conditional model inputs for the denoiser: e.g. prompt_embeds, negative_prompt_embeds, etc.\",\n+            ),\n+            InputParam(\n+                \"img_shapes\",\n+                required=True,\n+                type_hint=List[Tuple[int, int]],\n+                description=\"The shape of the image latents for RoPE calculation. Can be generated in prepare_additional_inputs step.\",\n+            ),\n+        ]\n+\n+    @torch.no_grad()\n+    def __call__(self, components: QwenImageModularPipeline, block_state: BlockState, i: int, t: torch.Tensor):\n+        guider_input_fields = {\n+            \"encoder_hidden_states\": (\"prompt_embeds\", \"negative_prompt_embeds\"),\n+            \"encoder_hidden_states_mask\": (\"prompt_embeds_mask\", \"negative_prompt_embeds_mask\"),\n+            \"txt_seq_lens\": (\"txt_seq_lens\", \"negative_txt_seq_lens\"),\n+        }\n+\n+        components.guider.set_state(step=i, num_inference_steps=block_state.num_inference_steps, timestep=t)\n+        guider_state = components.guider.prepare_inputs(block_state, guider_input_fields)\n+\n+        for guider_state_batch in guider_state:\n+            components.guider.prepare_models(components.transformer)\n+            cond_kwargs = guider_state_batch.as_dict()\n+            cond_kwargs = {k: v for k, v in cond_kwargs.items() if k in guider_input_fields}\n+\n+            # YiYi TODO: add cache context\n+            guider_state_batch.noise_pred = components.transformer(\n+                hidden_states=block_state.latent_model_input,\n+                timestep=block_state.timestep / 1000,\n+                img_shapes=block_state.img_shapes,\n+                attention_kwargs=block_state.attention_kwargs,\n+                return_dict=False,\n+                **cond_kwargs,\n+                **block_state.additional_cond_kwargs,\n+            )[0]\n+\n+            components.guider.cleanup_models(components.transformer)\n+\n+        guider_output = components.guider(guider_state)\n+\n+        # apply guidance rescale\n+        pred_cond_norm = torch.norm(guider_output.pred_cond, dim=-1, keepdim=True)\n+        pred_norm = torch.norm(guider_output.pred, dim=-1, keepdim=True)\n+        block_state.noise_pred = guider_output.pred * (pred_cond_norm / pred_norm)\n+\n+        return components, block_state\n+\n+\n+class QwenImageEditLoopDenoiser(ModularPipelineBlocks):\n+    model_name = \"qwenimage\"\n+\n+    @property\n+    def description(self) -> str:\n+        return (\n+            \"step within the denoising loop that denoise the latent input for the denoiser. \"\n+            \"This block should be used to compose the `sub_blocks` attribute of a `LoopSequentialPipelineBlocks` \"\n+            \"object (e.g. `QwenImageDenoiseLoopWrapper`)\"\n+        )\n+\n+    @property\n+    def expected_components(self) -> List[ComponentSpec]:\n+        return [\n+            ComponentSpec(\n+                \"guider\",\n+                ClassifierFreeGuidance,\n+                config=FrozenDict({\"guidance_scale\": 4.0}),\n+                default_creation_method=\"from_config\",\n+            ),\n+            ComponentSpec(\"transformer\", QwenImageTransformer2DModel),\n+        ]\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        return [\n+            InputParam(\"attention_kwargs\"),\n+            InputParam(\n+                \"latents\",\n+                required=True,\n+                type_hint=torch.Tensor,\n+                description=\"The latents to use for the denoising process. Can be generated in prepare_latents step.\",\n+            ),\n+            InputParam(\n+                \"num_inference_steps\",\n+                required=True,\n+                type_hint=int,\n+                description=\"The number of inference steps to use for the denoising process. Can be generated in set_timesteps step.\",\n+            ),\n+            InputParam(\n+                kwargs_type=\"denoiser_input_fields\",\n+                description=\"conditional model inputs for the denoiser: e.g. prompt_embeds, negative_prompt_embeds, etc.\",\n+            ),\n+            InputParam(\n+                \"img_shapes\",\n+                required=True,\n+                type_hint=List[Tuple[int, int]],\n+                description=\"The shape of the image latents for RoPE calculation. Can be generated in prepare_additional_inputs step.\",\n+            ),\n+        ]\n+\n+    @torch.no_grad()\n+    def __call__(self, components: QwenImageModularPipeline, block_state: BlockState, i: int, t: torch.Tensor):\n+        guider_input_fields = {\n+            \"encoder_hidden_states\": (\"prompt_embeds\", \"negative_prompt_embeds\"),\n+            \"encoder_hidden_states_mask\": (\"prompt_embeds_mask\", \"negative_prompt_embeds_mask\"),\n+            \"txt_seq_lens\": (\"txt_seq_lens\", \"negative_txt_seq_lens\"),\n+        }\n+\n+        components.guider.set_state(step=i, num_inference_steps=block_state.num_inference_steps, timestep=t)\n+        guider_state = components.guider.prepare_inputs(block_state, guider_input_fields)\n+\n+        for guider_state_batch in guider_state:\n+            components.guider.prepare_models(components.transformer)\n+            cond_kwargs = guider_state_batch.as_dict()\n+            cond_kwargs = {k: v for k, v in cond_kwargs.items() if k in guider_input_fields}\n+\n+            # YiYi TODO: add cache context\n+            guider_state_batch.noise_pred = components.transformer(\n+                hidden_states=block_state.latent_model_input,\n+                timestep=block_state.timestep / 1000,\n+                img_shapes=block_state.img_shapes,\n+                attention_kwargs=block_state.attention_kwargs,\n+                return_dict=False,\n+                **cond_kwargs,\n+                **block_state.additional_cond_kwargs,\n+            )[0]\n+\n+            components.guider.cleanup_models(components.transformer)\n+\n+        guider_output = components.guider(guider_state)\n+\n+        pred = guider_output.pred[:, : block_state.latents.size(1)]\n+        pred_cond = guider_output.pred_cond[:, : block_state.latents.size(1)]\n+\n+        # apply guidance rescale\n+        pred_cond_norm = torch.norm(pred_cond, dim=-1, keepdim=True)\n+        pred_norm = torch.norm(pred, dim=-1, keepdim=True)\n+        block_state.noise_pred = pred * (pred_cond_norm / pred_norm)\n+\n+        return components, block_state\n+\n+\n+class QwenImageLoopAfterDenoiser(ModularPipelineBlocks):\n+    model_name = \"qwenimage\"\n+\n+    @property\n+    def description(self) -> str:\n+        return (\n+            \"step within the denoising loop that updates the latents. \"\n+            \"This block should be used to compose the `sub_blocks` attribute of a `LoopSequentialPipelineBlocks` \"\n+            \"object (e.g. `QwenImageDenoiseLoopWrapper`)\"\n+        )\n+\n+    @property\n+    def expected_components(self) -> List[ComponentSpec]:\n+        return [\n+            ComponentSpec(\"scheduler\", FlowMatchEulerDiscreteScheduler),\n+        ]\n+\n+    @property\n+    def intermediate_outputs(self) -> List[OutputParam]:\n+        return [\n+            OutputParam(\"latents\", type_hint=torch.Tensor, description=\"The denoised latents.\"),\n+        ]\n+\n+    @torch.no_grad()\n+    def __call__(self, components: QwenImageModularPipeline, block_state: BlockState, i: int, t: torch.Tensor):\n+        latents_dtype = block_state.latents.dtype\n+        block_state.latents = components.scheduler.step(\n+            block_state.noise_pred,\n+            t,\n+            block_state.latents,\n+            return_dict=False,\n+        )[0]\n+\n+        if block_state.latents.dtype != latents_dtype:\n+            if torch.backends.mps.is_available():\n+                # some platforms (eg. apple mps) misbehave due to a pytorch bug: https://github.com/pytorch/pytorch/pull/99272\n+                block_state.latents = block_state.latents.to(latents_dtype)\n+\n+        return components, block_state\n+\n+\n+class QwenImageLoopAfterDenoiserInpaint(ModularPipelineBlocks):\n+    model_name = \"qwenimage\"\n+\n+    @property\n+    def description(self) -> str:\n+        return (\n+            \"step within the denoising loop that updates the latents using mask and image_latents for inpainting. \"\n+            \"This block should be used to compose the `sub_blocks` attribute of a `LoopSequentialPipelineBlocks` \"\n+            \"object (e.g. `QwenImageDenoiseLoopWrapper`)\"\n+        )\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        return [\n+            InputParam(\n+                \"mask\",\n+                required=True,\n+                type_hint=torch.Tensor,\n+                description=\"The mask to use for the inpainting process. Can be generated in inpaint prepare latents step.\",\n+            ),\n+            InputParam(\n+                \"image_latents\",\n+                required=True,\n+                type_hint=torch.Tensor,\n+                description=\"The image latents to use for the inpainting process. Can be generated in inpaint prepare latents step.\",\n+            ),\n+            InputParam(\n+                \"initial_noise\",\n+                required=True,\n+                type_hint=torch.Tensor,\n+                description=\"The initial noise to use for the inpainting process. Can be generated in inpaint prepare latents step.\",\n+            ),\n+            InputParam(\n+                \"timesteps\",\n+                required=True,\n+                type_hint=torch.Tensor,\n+                description=\"The timesteps to use for the denoising process. Can be generated in set_timesteps step.\",\n+            ),\n+        ]\n+\n+    @torch.no_grad()\n+    def __call__(self, components: QwenImageModularPipeline, block_state: BlockState, i: int, t: torch.Tensor):\n+        block_state.init_latents_proper = block_state.image_latents\n+        if i < len(block_state.timesteps) - 1:\n+            block_state.noise_timestep = block_state.timesteps[i + 1]\n+            block_state.init_latents_proper = components.scheduler.scale_noise(\n+                block_state.init_latents_proper, torch.tensor([block_state.noise_timestep]), block_state.initial_noise\n+            )\n+\n+        block_state.latents = (\n+            1 - block_state.mask\n+        ) * block_state.init_latents_proper + block_state.mask * block_state.latents\n+\n+        return components, block_state\n+\n+\n+class QwenImageDenoiseLoopWrapper(LoopSequentialPipelineBlocks):\n+    model_name = \"qwenimage\"\n+\n+    @property\n+    def description(self) -> str:\n+        return (\n+            \"Pipeline block that iteratively denoise the latents over `timesteps`. \"\n+            \"The specific steps with each iteration can be customized with `sub_blocks` attributes\"\n+        )\n+\n+    @property\n+    def loop_expected_components(self) -> List[ComponentSpec]:\n+        return [\n+            ComponentSpec(\"scheduler\", FlowMatchEulerDiscreteScheduler),\n+        ]\n+\n+    @property\n+    def loop_inputs(self) -> List[InputParam]:\n+        return [\n+            InputParam(\n+                \"timesteps\",\n+                required=True,\n+                type_hint=torch.Tensor,\n+                description=\"The timesteps to use for the denoising process. Can be generated in set_timesteps step.\",\n+            ),\n+            InputParam(\n+                \"num_inference_steps\",\n+                required=True,\n+                type_hint=int,\n+                description=\"The number of inference steps to use for the denoising process. Can be generated in set_timesteps step.\",\n+            ),\n+        ]\n+\n+    @torch.no_grad()\n+    def __call__(self, components: QwenImageModularPipeline, state: PipelineState) -> PipelineState:\n+        block_state = self.get_block_state(state)\n+\n+        block_state.num_warmup_steps = max(\n+            len(block_state.timesteps) - block_state.num_inference_steps * components.scheduler.order, 0\n+        )\n+\n+        block_state.additional_cond_kwargs = {}\n+\n+        with self.progress_bar(total=block_state.num_inference_steps) as progress_bar:\n+            for i, t in enumerate(block_state.timesteps):\n+                components, block_state = self.loop_step(components, block_state, i=i, t=t)\n+                if i == len(block_state.timesteps) - 1 or (\n+                    (i + 1) > block_state.num_warmup_steps and (i + 1) % components.scheduler.order == 0\n+                ):\n+                    progress_bar.update()\n+\n+        self.set_block_state(state, block_state)\n+\n+        return components, state\n+\n+\n+# composing the denoising loops\n+class QwenImageDenoiseStep(QwenImageDenoiseLoopWrapper):\n+    block_classes = [\n+        QwenImageLoopBeforeDenoiser,\n+        QwenImageLoopDenoiser,\n+        QwenImageLoopAfterDenoiser,\n+    ]\n+    block_names = [\"before_denoiser\", \"denoiser\", \"after_denoiser\"]\n+\n+    @property\n+    def description(self) -> str:\n+        return (\n+            \"Denoise step that iteratively denoise the latents. \\n\"\n+            \"Its loop logic is defined in `QwenImageDenoiseLoopWrapper.__call__` method \\n\"\n+            \"At each iteration, it runs blocks defined in `sub_blocks` sequencially:\\n\"\n+            \" - `QwenImageLoopBeforeDenoiser`\\n\"\n+            \" - `QwenImageLoopDenoiser`\\n\"\n+            \" - `QwenImageLoopAfterDenoiser`\\n\"\n+            \"This block supports text2image and image2image tasks for QwenImage.\"\n+        )\n+\n+\n+# composing the inpainting denoising loops\n+class QwenImageInpaintDenoiseStep(QwenImageDenoiseLoopWrapper):\n+    block_classes = [\n+        QwenImageLoopBeforeDenoiser,\n+        QwenImageLoopDenoiser,\n+        QwenImageLoopAfterDenoiser,\n+        QwenImageLoopAfterDenoiserInpaint,\n+    ]\n+    block_names = [\"before_denoiser\", \"denoiser\", \"after_denoiser\", \"after_denoiser_inpaint\"]\n+\n+    @property\n+    def description(self) -> str:\n+        return (\n+            \"Denoise step that iteratively denoise the latents. \\n\"\n+            \"Its loop logic is defined in `QwenImageDenoiseLoopWrapper.__call__` method \\n\"\n+            \"At each iteration, it runs blocks defined in `sub_blocks` sequencially:\\n\"\n+            \" - `QwenImageLoopBeforeDenoiser`\\n\"\n+            \" - `QwenImageLoopDenoiser`\\n\"\n+            \" - `QwenImageLoopAfterDenoiser`\\n\"\n+            \" - `QwenImageLoopAfterDenoiserInpaint`\\n\"\n+            \"This block supports inpainting tasks for QwenImage.\"\n+        )\n+\n+\n+# composing the controlnet denoising loops\n+class QwenImageControlNetDenoiseStep(QwenImageDenoiseLoopWrapper):\n+    block_classes = [\n+        QwenImageLoopBeforeDenoiser,\n+        QwenImageLoopBeforeDenoiserControlNet,\n+        QwenImageLoopDenoiser,\n+        QwenImageLoopAfterDenoiser,\n+    ]\n+    block_names = [\"before_denoiser\", \"before_denoiser_controlnet\", \"denoiser\", \"after_denoiser\"]\n+\n+    @property\n+    def description(self) -> str:\n+        return (\n+            \"Denoise step that iteratively denoise the latents. \\n\"\n+            \"Its loop logic is defined in `QwenImageDenoiseLoopWrapper.__call__` method \\n\"\n+            \"At each iteration, it runs blocks defined in `sub_blocks` sequencially:\\n\"\n+            \" - `QwenImageLoopBeforeDenoiser`\\n\"\n+            \" - `QwenImageLoopBeforeDenoiserControlNet`\\n\"\n+            \" - `QwenImageLoopDenoiser`\\n\"\n+            \" - `QwenImageLoopAfterDenoiser`\\n\"\n+            \"This block supports text2img/img2img tasks with controlnet for QwenImage.\"\n+        )\n+\n+\n+# composing the controlnet denoising loops\n+class QwenImageInpaintControlNetDenoiseStep(QwenImageDenoiseLoopWrapper):\n+    block_classes = [\n+        QwenImageLoopBeforeDenoiser,\n+        QwenImageLoopBeforeDenoiserControlNet,\n+        QwenImageLoopDenoiser,\n+        QwenImageLoopAfterDenoiser,\n+        QwenImageLoopAfterDenoiserInpaint,\n+    ]\n+    block_names = [\n+        \"before_denoiser\",\n+        \"before_denoiser_controlnet\",\n+        \"denoiser\",\n+        \"after_denoiser\",\n+        \"after_denoiser_inpaint\",\n+    ]\n+\n+    @property\n+    def description(self) -> str:\n+        return (\n+            \"Denoise step that iteratively denoise the latents. \\n\"\n+            \"Its loop logic is defined in `QwenImageDenoiseLoopWrapper.__call__` method \\n\"\n+            \"At each iteration, it runs blocks defined in `sub_blocks` sequencially:\\n\"\n+            \" - `QwenImageLoopBeforeDenoiser`\\n\"\n+            \" - `QwenImageLoopBeforeDenoiserControlNet`\\n\"\n+            \" - `QwenImageLoopDenoiser`\\n\"\n+            \" - `QwenImageLoopAfterDenoiser`\\n\"\n+            \" - `QwenImageLoopAfterDenoiserInpaint`\\n\"\n+            \"This block supports inpainting tasks with controlnet for QwenImage.\"\n+        )\n+\n+\n+# composing the denoising loops\n+class QwenImageEditDenoiseStep(QwenImageDenoiseLoopWrapper):\n+    block_classes = [\n+        QwenImageEditLoopBeforeDenoiser,\n+        QwenImageEditLoopDenoiser,\n+        QwenImageLoopAfterDenoiser,\n+    ]\n+    block_names = [\"before_denoiser\", \"denoiser\", \"after_denoiser\"]\n+\n+    @property\n+    def description(self) -> str:\n+        return (\n+            \"Denoise step that iteratively denoise the latents. \\n\"\n+            \"Its loop logic is defined in `QwenImageDenoiseLoopWrapper.__call__` method \\n\"\n+            \"At each iteration, it runs blocks defined in `sub_blocks` sequencially:\\n\"\n+            \" - `QwenImageEditLoopBeforeDenoiser`\\n\"\n+            \" - `QwenImageEditLoopDenoiser`\\n\"\n+            \" - `QwenImageLoopAfterDenoiser`\\n\"\n+            \"This block supports QwenImage Edit.\"\n+        )\n+\n+\n+class QwenImageEditInpaintDenoiseStep(QwenImageDenoiseLoopWrapper):\n+    block_classes = [\n+        QwenImageEditLoopBeforeDenoiser,\n+        QwenImageEditLoopDenoiser,\n+        QwenImageLoopAfterDenoiser,\n+        QwenImageLoopAfterDenoiserInpaint,\n+    ]\n+    block_names = [\"before_denoiser\", \"denoiser\", \"after_denoiser\", \"after_denoiser_inpaint\"]\n+\n+    @property\n+    def description(self) -> str:\n+        return (\n+            \"Denoise step that iteratively denoise the latents. \\n\"\n+            \"Its loop logic is defined in `QwenImageDenoiseLoopWrapper.__call__` method \\n\"\n+            \"At each iteration, it runs blocks defined in `sub_blocks` sequencially:\\n\"\n+            \" - `QwenImageEditLoopBeforeDenoiser`\\n\"\n+            \" - `QwenImageEditLoopDenoiser`\\n\"\n+            \" - `QwenImageLoopAfterDenoiser`\\n\"\n+            \" - `QwenImageLoopAfterDenoiserInpaint`\\n\"\n+            \"This block supports inpainting tasks for QwenImage Edit.\"\n+        )"
        },
        {
          "filename": "src/diffusers/modular_pipelines/qwenimage/encoders.py",
          "status": "added",
          "additions": 857,
          "deletions": 0,
          "changes": 857,
          "patch": "@@ -0,0 +1,857 @@\n+# Copyright 2025 Qwen-Image Team and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import Dict, List, Optional, Union\n+\n+import PIL\n+import torch\n+from transformers import Qwen2_5_VLForConditionalGeneration, Qwen2Tokenizer, Qwen2VLProcessor\n+\n+from ...configuration_utils import FrozenDict\n+from ...guiders import ClassifierFreeGuidance\n+from ...image_processor import InpaintProcessor, VaeImageProcessor, is_valid_image, is_valid_image_imagelist\n+from ...models import AutoencoderKLQwenImage, QwenImageControlNetModel, QwenImageMultiControlNetModel\n+from ...pipelines.qwenimage.pipeline_qwenimage_edit import calculate_dimensions\n+from ...utils import logging\n+from ...utils.torch_utils import unwrap_module\n+from ..modular_pipeline import ModularPipelineBlocks, PipelineState\n+from ..modular_pipeline_utils import ComponentSpec, ConfigSpec, InputParam, OutputParam\n+from .modular_pipeline import QwenImageModularPipeline\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+def _extract_masked_hidden(hidden_states: torch.Tensor, mask: torch.Tensor):\n+    bool_mask = mask.bool()\n+    valid_lengths = bool_mask.sum(dim=1)\n+    selected = hidden_states[bool_mask]\n+    split_result = torch.split(selected, valid_lengths.tolist(), dim=0)\n+    return split_result\n+\n+\n+def get_qwen_prompt_embeds(\n+    text_encoder,\n+    tokenizer,\n+    prompt: Union[str, List[str]] = None,\n+    prompt_template_encode: str = \"<|im_start|>system\\nDescribe the image by detailing the color, shape, size, texture, quantity, text, spatial relationships of the objects and background:<|im_end|>\\n<|im_start|>user\\n{}<|im_end|>\\n<|im_start|>assistant\\n\",\n+    prompt_template_encode_start_idx: int = 34,\n+    tokenizer_max_length: int = 1024,\n+    device: Optional[torch.device] = None,\n+):\n+    prompt = [prompt] if isinstance(prompt, str) else prompt\n+\n+    template = prompt_template_encode\n+    drop_idx = prompt_template_encode_start_idx\n+    txt = [template.format(e) for e in prompt]\n+    txt_tokens = tokenizer(\n+        txt, max_length=tokenizer_max_length + drop_idx, padding=True, truncation=True, return_tensors=\"pt\"\n+    ).to(device)\n+    encoder_hidden_states = text_encoder(\n+        input_ids=txt_tokens.input_ids,\n+        attention_mask=txt_tokens.attention_mask,\n+        output_hidden_states=True,\n+    )\n+    hidden_states = encoder_hidden_states.hidden_states[-1]\n+\n+    split_hidden_states = _extract_masked_hidden(hidden_states, txt_tokens.attention_mask)\n+    split_hidden_states = [e[drop_idx:] for e in split_hidden_states]\n+    attn_mask_list = [torch.ones(e.size(0), dtype=torch.long, device=e.device) for e in split_hidden_states]\n+    max_seq_len = max([e.size(0) for e in split_hidden_states])\n+    prompt_embeds = torch.stack(\n+        [torch.cat([u, u.new_zeros(max_seq_len - u.size(0), u.size(1))]) for u in split_hidden_states]\n+    )\n+    encoder_attention_mask = torch.stack(\n+        [torch.cat([u, u.new_zeros(max_seq_len - u.size(0))]) for u in attn_mask_list]\n+    )\n+\n+    prompt_embeds = prompt_embeds.to(device=device)\n+\n+    return prompt_embeds, encoder_attention_mask\n+\n+\n+def get_qwen_prompt_embeds_edit(\n+    text_encoder,\n+    processor,\n+    prompt: Union[str, List[str]] = None,\n+    image: Optional[torch.Tensor] = None,\n+    prompt_template_encode: str = \"<|im_start|>system\\nDescribe the key features of the input image (color, shape, size, texture, objects, background), then explain how the user's text instruction should alter or modify the image. Generate a new image that meets the user's requirements while maintaining consistency with the original input where appropriate.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>{}<|im_end|>\\n<|im_start|>assistant\\n\",\n+    prompt_template_encode_start_idx: int = 64,\n+    device: Optional[torch.device] = None,\n+):\n+    prompt = [prompt] if isinstance(prompt, str) else prompt\n+\n+    template = prompt_template_encode\n+    drop_idx = prompt_template_encode_start_idx\n+    txt = [template.format(e) for e in prompt]\n+\n+    model_inputs = processor(\n+        text=txt,\n+        images=image,\n+        padding=True,\n+        return_tensors=\"pt\",\n+    ).to(device)\n+\n+    outputs = text_encoder(\n+        input_ids=model_inputs.input_ids,\n+        attention_mask=model_inputs.attention_mask,\n+        pixel_values=model_inputs.pixel_values,\n+        image_grid_thw=model_inputs.image_grid_thw,\n+        output_hidden_states=True,\n+    )\n+\n+    hidden_states = outputs.hidden_states[-1]\n+    split_hidden_states = _extract_masked_hidden(hidden_states, model_inputs.attention_mask)\n+    split_hidden_states = [e[drop_idx:] for e in split_hidden_states]\n+    attn_mask_list = [torch.ones(e.size(0), dtype=torch.long, device=e.device) for e in split_hidden_states]\n+    max_seq_len = max([e.size(0) for e in split_hidden_states])\n+    prompt_embeds = torch.stack(\n+        [torch.cat([u, u.new_zeros(max_seq_len - u.size(0), u.size(1))]) for u in split_hidden_states]\n+    )\n+    encoder_attention_mask = torch.stack(\n+        [torch.cat([u, u.new_zeros(max_seq_len - u.size(0))]) for u in attn_mask_list]\n+    )\n+\n+    prompt_embeds = prompt_embeds.to(device=device)\n+\n+    return prompt_embeds, encoder_attention_mask\n+\n+\n+# Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_img2img.retrieve_latents\n+def retrieve_latents(\n+    encoder_output: torch.Tensor, generator: Optional[torch.Generator] = None, sample_mode: str = \"sample\"\n+):\n+    if hasattr(encoder_output, \"latent_dist\") and sample_mode == \"sample\":\n+        return encoder_output.latent_dist.sample(generator)\n+    elif hasattr(encoder_output, \"latent_dist\") and sample_mode == \"argmax\":\n+        return encoder_output.latent_dist.mode()\n+    elif hasattr(encoder_output, \"latents\"):\n+        return encoder_output.latents\n+    else:\n+        raise AttributeError(\"Could not access latents of provided encoder_output\")\n+\n+\n+# Modified from diffusers.pipelines.qwenimage.pipeline_qwenimage.QwenImagePipeline._encode_vae_image\n+def encode_vae_image(\n+    image: torch.Tensor,\n+    vae: AutoencoderKLQwenImage,\n+    generator: torch.Generator,\n+    device: torch.device,\n+    dtype: torch.dtype,\n+    latent_channels: int = 16,\n+    sample_mode: str = \"argmax\",\n+):\n+    if not isinstance(image, torch.Tensor):\n+        raise ValueError(f\"Expected image to be a tensor, got {type(image)}.\")\n+\n+    # preprocessed image should be a 4D tensor: batch_size, num_channels, height, width\n+    if image.dim() == 4:\n+        image = image.unsqueeze(2)\n+    elif image.dim() != 5:\n+        raise ValueError(f\"Expected image dims 4 or 5, got {image.dim()}.\")\n+\n+    image = image.to(device=device, dtype=dtype)\n+\n+    if isinstance(generator, list):\n+        image_latents = [\n+            retrieve_latents(vae.encode(image[i : i + 1]), generator=generator[i], sample_mode=sample_mode)\n+            for i in range(image.shape[0])\n+        ]\n+        image_latents = torch.cat(image_latents, dim=0)\n+    else:\n+        image_latents = retrieve_latents(vae.encode(image), generator=generator, sample_mode=sample_mode)\n+    latents_mean = (\n+        torch.tensor(vae.config.latents_mean)\n+        .view(1, latent_channels, 1, 1, 1)\n+        .to(image_latents.device, image_latents.dtype)\n+    )\n+    latents_std = (\n+        torch.tensor(vae.config.latents_std)\n+        .view(1, latent_channels, 1, 1, 1)\n+        .to(image_latents.device, image_latents.dtype)\n+    )\n+    image_latents = (image_latents - latents_mean) / latents_std\n+\n+    return image_latents\n+\n+\n+class QwenImageEditResizeDynamicStep(ModularPipelineBlocks):\n+    model_name = \"qwenimage\"\n+\n+    def __init__(self, input_name: str = \"image\", output_name: str = \"resized_image\"):\n+        \"\"\"Create a configurable step for resizing images to the target area (1024 * 1024) while maintaining the aspect ratio.\n+\n+        This block resizes an input image tensor and exposes the resized result under configurable input and output\n+        names. Use this when you need to wire the resize step to different image fields (e.g., \"image\",\n+        \"control_image\")\n+\n+        Args:\n+            input_name (str, optional): Name of the image field to read from the\n+                pipeline state. Defaults to \"image\".\n+            output_name (str, optional): Name of the resized image field to write\n+                back to the pipeline state. Defaults to \"resized_image\".\n+        \"\"\"\n+        if not isinstance(input_name, str) or not isinstance(output_name, str):\n+            raise ValueError(\n+                f\"input_name and output_name must be strings but are {type(input_name)} and {type(output_name)}\"\n+            )\n+        self._image_input_name = input_name\n+        self._resized_image_output_name = output_name\n+        super().__init__()\n+\n+    @property\n+    def description(self) -> str:\n+        return f\"Image Resize step that resize the {self._image_input_name} to the target area (1024 * 1024) while maintaining the aspect ratio.\"\n+\n+    @property\n+    def expected_components(self) -> List[ComponentSpec]:\n+        return [\n+            ComponentSpec(\n+                \"image_resize_processor\",\n+                VaeImageProcessor,\n+                config=FrozenDict({\"vae_scale_factor\": 16}),\n+                default_creation_method=\"from_config\",\n+            ),\n+        ]\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        return [\n+            InputParam(\n+                name=self._image_input_name, required=True, type_hint=torch.Tensor, description=\"The image to resize\"\n+            ),\n+        ]\n+\n+    @property\n+    def intermediate_outputs(self) -> List[OutputParam]:\n+        return [\n+            OutputParam(\n+                name=self._resized_image_output_name, type_hint=List[PIL.Image.Image], description=\"The resized images\"\n+            ),\n+        ]\n+\n+    @torch.no_grad()\n+    def __call__(self, components: QwenImageModularPipeline, state: PipelineState):\n+        block_state = self.get_block_state(state)\n+\n+        images = getattr(block_state, self._image_input_name)\n+\n+        if not is_valid_image_imagelist(images):\n+            raise ValueError(f\"Images must be image or list of images but are {type(images)}\")\n+\n+        if is_valid_image(images):\n+            images = [images]\n+\n+        image_width, image_height = images[0].size\n+        calculated_width, calculated_height, _ = calculate_dimensions(1024 * 1024, image_width / image_height)\n+\n+        resized_images = [\n+            components.image_resize_processor.resize(image, height=calculated_height, width=calculated_width)\n+            for image in images\n+        ]\n+\n+        setattr(block_state, self._resized_image_output_name, resized_images)\n+        self.set_block_state(state, block_state)\n+        return components, state\n+\n+\n+class QwenImageTextEncoderStep(ModularPipelineBlocks):\n+    model_name = \"qwenimage\"\n+\n+    @property\n+    def description(self) -> str:\n+        return \"Text Encoder step that generate text_embeddings to guide the image generation\"\n+\n+    @property\n+    def expected_components(self) -> List[ComponentSpec]:\n+        return [\n+            ComponentSpec(\"text_encoder\", Qwen2_5_VLForConditionalGeneration, description=\"The text encoder to use\"),\n+            ComponentSpec(\"tokenizer\", Qwen2Tokenizer, description=\"The tokenizer to use\"),\n+            ComponentSpec(\n+                \"guider\",\n+                ClassifierFreeGuidance,\n+                config=FrozenDict({\"guidance_scale\": 4.0}),\n+                default_creation_method=\"from_config\",\n+            ),\n+        ]\n+\n+    @property\n+    def expected_configs(self) -> List[ConfigSpec]:\n+        return [\n+            ConfigSpec(\n+                name=\"prompt_template_encode\",\n+                default=\"<|im_start|>system\\nDescribe the image by detailing the color, shape, size, texture, quantity, text, spatial relationships of the objects and background:<|im_end|>\\n<|im_start|>user\\n{}<|im_end|>\\n<|im_start|>assistant\\n\",\n+            ),\n+            ConfigSpec(name=\"prompt_template_encode_start_idx\", default=34),\n+            ConfigSpec(name=\"tokenizer_max_length\", default=1024),\n+        ]\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        return [\n+            InputParam(name=\"prompt\", required=True, type_hint=str, description=\"The prompt to encode\"),\n+            InputParam(name=\"negative_prompt\", type_hint=str, description=\"The negative prompt to encode\"),\n+            InputParam(\n+                name=\"max_sequence_length\", type_hint=int, description=\"The max sequence length to use\", default=1024\n+            ),\n+        ]\n+\n+    @property\n+    def intermediate_outputs(self) -> List[OutputParam]:\n+        return [\n+            OutputParam(\n+                name=\"prompt_embeds\",\n+                kwargs_type=\"denoiser_input_fields\",\n+                type_hint=torch.Tensor,\n+                description=\"The prompt embeddings\",\n+            ),\n+            OutputParam(\n+                name=\"prompt_embeds_mask\",\n+                kwargs_type=\"denoiser_input_fields\",\n+                type_hint=torch.Tensor,\n+                description=\"The encoder attention mask\",\n+            ),\n+            OutputParam(\n+                name=\"negative_prompt_embeds\",\n+                kwargs_type=\"denoiser_input_fields\",\n+                type_hint=torch.Tensor,\n+                description=\"The negative prompt embeddings\",\n+            ),\n+            OutputParam(\n+                name=\"negative_prompt_embeds_mask\",\n+                kwargs_type=\"denoiser_input_fields\",\n+                type_hint=torch.Tensor,\n+                description=\"The negative prompt embeddings mask\",\n+            ),\n+        ]\n+\n+    @staticmethod\n+    def check_inputs(prompt, negative_prompt, max_sequence_length):\n+        if not isinstance(prompt, str) and not isinstance(prompt, list):\n+            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n+\n+        if (\n+            negative_prompt is not None\n+            and not isinstance(negative_prompt, str)\n+            and not isinstance(negative_prompt, list)\n+        ):\n+            raise ValueError(f\"`negative_prompt` has to be of type `str` or `list` but is {type(negative_prompt)}\")\n+\n+        if max_sequence_length is not None and max_sequence_length > 1024:\n+            raise ValueError(f\"`max_sequence_length` cannot be greater than 1024 but is {max_sequence_length}\")\n+\n+    @torch.no_grad()\n+    def __call__(self, components: QwenImageModularPipeline, state: PipelineState):\n+        block_state = self.get_block_state(state)\n+\n+        device = components._execution_device\n+        self.check_inputs(block_state.prompt, block_state.negative_prompt, block_state.max_sequence_length)\n+\n+        block_state.prompt_embeds, block_state.prompt_embeds_mask = get_qwen_prompt_embeds(\n+            components.text_encoder,\n+            components.tokenizer,\n+            prompt=block_state.prompt,\n+            prompt_template_encode=components.config.prompt_template_encode,\n+            prompt_template_encode_start_idx=components.config.prompt_template_encode_start_idx,\n+            tokenizer_max_length=components.config.tokenizer_max_length,\n+            device=device,\n+        )\n+\n+        block_state.prompt_embeds = block_state.prompt_embeds[:, : block_state.max_sequence_length]\n+        block_state.prompt_embeds_mask = block_state.prompt_embeds_mask[:, : block_state.max_sequence_length]\n+\n+        if components.requires_unconditional_embeds:\n+            negative_prompt = block_state.negative_prompt or \"\"\n+            block_state.negative_prompt_embeds, block_state.negative_prompt_embeds_mask = get_qwen_prompt_embeds(\n+                components.text_encoder,\n+                components.tokenizer,\n+                prompt=negative_prompt,\n+                prompt_template_encode=components.config.prompt_template_encode,\n+                prompt_template_encode_start_idx=components.config.prompt_template_encode_start_idx,\n+                tokenizer_max_length=components.config.tokenizer_max_length,\n+                device=device,\n+            )\n+            block_state.negative_prompt_embeds = block_state.negative_prompt_embeds[\n+                :, : block_state.max_sequence_length\n+            ]\n+            block_state.negative_prompt_embeds_mask = block_state.negative_prompt_embeds_mask[\n+                :, : block_state.max_sequence_length\n+            ]\n+\n+        self.set_block_state(state, block_state)\n+        return components, state\n+\n+\n+class QwenImageEditTextEncoderStep(ModularPipelineBlocks):\n+    model_name = \"qwenimage\"\n+\n+    @property\n+    def description(self) -> str:\n+        return \"Text Encoder step that processes both prompt and image together to generate text embeddings for guiding image generation\"\n+\n+    @property\n+    def expected_components(self) -> List[ComponentSpec]:\n+        return [\n+            ComponentSpec(\"text_encoder\", Qwen2_5_VLForConditionalGeneration),\n+            ComponentSpec(\"processor\", Qwen2VLProcessor),\n+            ComponentSpec(\n+                \"guider\",\n+                ClassifierFreeGuidance,\n+                config=FrozenDict({\"guidance_scale\": 4.0}),\n+                default_creation_method=\"from_config\",\n+            ),\n+        ]\n+\n+    @property\n+    def expected_configs(self) -> List[ConfigSpec]:\n+        return [\n+            ConfigSpec(\n+                name=\"prompt_template_encode\",\n+                default=\"<|im_start|>system\\nDescribe the key features of the input image (color, shape, size, texture, objects, background), then explain how the user's text instruction should alter or modify the image. Generate a new image that meets the user's requirements while maintaining consistency with the original input where appropriate.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>{}<|im_end|>\\n<|im_start|>assistant\\n\",\n+            ),\n+            ConfigSpec(name=\"prompt_template_encode_start_idx\", default=64),\n+        ]\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        return [\n+            InputParam(name=\"prompt\", required=True, type_hint=str, description=\"The prompt to encode\"),\n+            InputParam(name=\"negative_prompt\", type_hint=str, description=\"The negative prompt to encode\"),\n+            InputParam(\n+                name=\"resized_image\",\n+                required=True,\n+                type_hint=torch.Tensor,\n+                description=\"The image prompt to encode, should be resized using resize step\",\n+            ),\n+        ]\n+\n+    @property\n+    def intermediate_outputs(self) -> List[OutputParam]:\n+        return [\n+            OutputParam(\n+                name=\"prompt_embeds\",\n+                kwargs_type=\"denoiser_input_fields\",\n+                type_hint=torch.Tensor,\n+                description=\"The prompt embeddings\",\n+            ),\n+            OutputParam(\n+                name=\"prompt_embeds_mask\",\n+                kwargs_type=\"denoiser_input_fields\",\n+                type_hint=torch.Tensor,\n+                description=\"The encoder attention mask\",\n+            ),\n+            OutputParam(\n+                name=\"negative_prompt_embeds\",\n+                kwargs_type=\"denoiser_input_fields\",\n+                type_hint=torch.Tensor,\n+                description=\"The negative prompt embeddings\",\n+            ),\n+            OutputParam(\n+                name=\"negative_prompt_embeds_mask\",\n+                kwargs_type=\"denoiser_input_fields\",\n+                type_hint=torch.Tensor,\n+                description=\"The negative prompt embeddings mask\",\n+            ),\n+        ]\n+\n+    @staticmethod\n+    def check_inputs(prompt, negative_prompt):\n+        if not isinstance(prompt, str) and not isinstance(prompt, list):\n+            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n+\n+        if (\n+            negative_prompt is not None\n+            and not isinstance(negative_prompt, str)\n+            and not isinstance(negative_prompt, list)\n+        ):\n+            raise ValueError(f\"`negative_prompt` has to be of type `str` or `list` but is {type(negative_prompt)}\")\n+\n+    @torch.no_grad()\n+    def __call__(self, components: QwenImageModularPipeline, state: PipelineState):\n+        block_state = self.get_block_state(state)\n+\n+        self.check_inputs(block_state.prompt, block_state.negative_prompt)\n+\n+        device = components._execution_device\n+\n+        block_state.prompt_embeds, block_state.prompt_embeds_mask = get_qwen_prompt_embeds_edit(\n+            components.text_encoder,\n+            components.processor,\n+            prompt=block_state.prompt,\n+            image=block_state.resized_image,\n+            prompt_template_encode=components.config.prompt_template_encode,\n+            prompt_template_encode_start_idx=components.config.prompt_template_encode_start_idx,\n+            device=device,\n+        )\n+\n+        if components.requires_unconditional_embeds:\n+            negative_prompt = block_state.negative_prompt or \"\"\n+            block_state.negative_prompt_embeds, block_state.negative_prompt_embeds_mask = get_qwen_prompt_embeds_edit(\n+                components.text_encoder,\n+                components.processor,\n+                prompt=negative_prompt,\n+                image=block_state.resized_image,\n+                prompt_template_encode=components.config.prompt_template_encode,\n+                prompt_template_encode_start_idx=components.config.prompt_template_encode_start_idx,\n+                device=device,\n+            )\n+\n+        self.set_block_state(state, block_state)\n+        return components, state\n+\n+\n+class QwenImageInpaintProcessImagesInputStep(ModularPipelineBlocks):\n+    model_name = \"qwenimage\"\n+\n+    @property\n+    def description(self) -> str:\n+        return \"Image Preprocess step for inpainting task. This processes the image and mask inputs together. Images can be resized first using QwenImageEditResizeDynamicStep.\"\n+\n+    @property\n+    def expected_components(self) -> List[ComponentSpec]:\n+        return [\n+            ComponentSpec(\n+                \"image_mask_processor\",\n+                InpaintProcessor,\n+                config=FrozenDict({\"vae_scale_factor\": 16}),\n+                default_creation_method=\"from_config\",\n+            ),\n+        ]\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        return [\n+            InputParam(\"mask_image\", required=True),\n+            InputParam(\"resized_image\"),\n+            InputParam(\"image\"),\n+            InputParam(\"height\"),\n+            InputParam(\"width\"),\n+            InputParam(\"padding_mask_crop\"),\n+        ]\n+\n+    @property\n+    def intermediate_outputs(self) -> List[OutputParam]:\n+        return [\n+            OutputParam(name=\"processed_image\"),\n+            OutputParam(name=\"processed_mask_image\"),\n+            OutputParam(\n+                name=\"mask_overlay_kwargs\",\n+                type_hint=Dict,\n+                description=\"The kwargs for the postprocess step to apply the mask overlay\",\n+            ),\n+        ]\n+\n+    @staticmethod\n+    def check_inputs(height, width, vae_scale_factor):\n+        if height is not None and height % (vae_scale_factor * 2) != 0:\n+            raise ValueError(f\"Height must be divisible by {vae_scale_factor * 2} but is {height}\")\n+\n+        if width is not None and width % (vae_scale_factor * 2) != 0:\n+            raise ValueError(f\"Width must be divisible by {vae_scale_factor * 2} but is {width}\")\n+\n+    @torch.no_grad()\n+    def __call__(self, components: QwenImageModularPipeline, state: PipelineState):\n+        block_state = self.get_block_state(state)\n+\n+        if block_state.resized_image is None and block_state.image is None:\n+            raise ValueError(\"resized_image and image cannot be None at the same time\")\n+\n+        if block_state.resized_image is None:\n+            image = block_state.image\n+            self.check_inputs(\n+                height=block_state.height, width=block_state.width, vae_scale_factor=components.vae_scale_factor\n+            )\n+            height = block_state.height or components.default_height\n+            width = block_state.width or components.default_width\n+        else:\n+            width, height = block_state.resized_image[0].size\n+            image = block_state.resized_image\n+\n+        block_state.processed_image, block_state.processed_mask_image, block_state.mask_overlay_kwargs = (\n+            components.image_mask_processor.preprocess(\n+                image=image,\n+                mask=block_state.mask_image,\n+                height=height,\n+                width=width,\n+                padding_mask_crop=block_state.padding_mask_crop,\n+            )\n+        )\n+\n+        self.set_block_state(state, block_state)\n+        return components, state\n+\n+\n+class QwenImageProcessImagesInputStep(ModularPipelineBlocks):\n+    model_name = \"qwenimage\"\n+\n+    @property\n+    def description(self) -> str:\n+        return \"Image Preprocess step. Images can be resized first using QwenImageEditResizeDynamicStep.\"\n+\n+    @property\n+    def expected_components(self) -> List[ComponentSpec]:\n+        return [\n+            ComponentSpec(\n+                \"image_processor\",\n+                VaeImageProcessor,\n+                config=FrozenDict({\"vae_scale_factor\": 16}),\n+                default_creation_method=\"from_config\",\n+            ),\n+        ]\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        return [\n+            InputParam(\"resized_image\"),\n+            InputParam(\"image\"),\n+            InputParam(\"height\"),\n+            InputParam(\"width\"),\n+        ]\n+\n+    @property\n+    def intermediate_outputs(self) -> List[OutputParam]:\n+        return [\n+            OutputParam(name=\"processed_image\"),\n+        ]\n+\n+    @staticmethod\n+    def check_inputs(height, width, vae_scale_factor):\n+        if height is not None and height % (vae_scale_factor * 2) != 0:\n+            raise ValueError(f\"Height must be divisible by {vae_scale_factor * 2} but is {height}\")\n+\n+        if width is not None and width % (vae_scale_factor * 2) != 0:\n+            raise ValueError(f\"Width must be divisible by {vae_scale_factor * 2} but is {width}\")\n+\n+    @torch.no_grad()\n+    def __call__(self, components: QwenImageModularPipeline, state: PipelineState):\n+        block_state = self.get_block_state(state)\n+\n+        if block_state.resized_image is None and block_state.image is None:\n+            raise ValueError(\"resized_image and image cannot be None at the same time\")\n+\n+        if block_state.resized_image is None:\n+            image = block_state.image\n+            self.check_inputs(\n+                height=block_state.height, width=block_state.width, vae_scale_factor=components.vae_scale_factor\n+            )\n+            height = block_state.height or components.default_height\n+            width = block_state.width or components.default_width\n+        else:\n+            width, height = block_state.resized_image[0].size\n+            image = block_state.resized_image\n+\n+        block_state.processed_image = components.image_processor.preprocess(\n+            image=image,\n+            height=height,\n+            width=width,\n+        )\n+\n+        self.set_block_state(state, block_state)\n+        return components, state\n+\n+\n+class QwenImageVaeEncoderDynamicStep(ModularPipelineBlocks):\n+    model_name = \"qwenimage\"\n+\n+    def __init__(\n+        self,\n+        input_name: str = \"processed_image\",\n+        output_name: str = \"image_latents\",\n+    ):\n+        \"\"\"Initialize a VAE encoder step for converting images to latent representations.\n+\n+        Both the input and output names are configurable so this block can be configured to process to different image\n+        inputs (e.g., \"processed_image\" -> \"image_latents\", \"processed_control_image\" -> \"control_image_latents\").\n+\n+        Args:\n+            input_name (str, optional): Name of the input image tensor. Defaults to \"processed_image\".\n+                Examples: \"processed_image\" or \"processed_control_image\"\n+            output_name (str, optional): Name of the output latent tensor. Defaults to \"image_latents\".\n+                Examples: \"image_latents\" or \"control_image_latents\"\n+\n+        Examples:\n+            # Basic usage with default settings (includes image processor) QwenImageVaeEncoderDynamicStep()\n+\n+            # Custom input/output names for control image QwenImageVaeEncoderDynamicStep(\n+                input_name=\"processed_control_image\", output_name=\"control_image_latents\"\n+            )\n+        \"\"\"\n+        self._image_input_name = input_name\n+        self._image_latents_output_name = output_name\n+        super().__init__()\n+\n+    @property\n+    def description(self) -> str:\n+        return f\"Dynamic VAE Encoder step that converts {self._image_input_name} into latent representations {self._image_latents_output_name}.\\n\"\n+\n+    @property\n+    def expected_components(self) -> List[ComponentSpec]:\n+        components = [\n+            ComponentSpec(\"vae\", AutoencoderKLQwenImage),\n+        ]\n+        return components\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        inputs = [\n+            InputParam(self._image_input_name, required=True),\n+            InputParam(\"generator\"),\n+        ]\n+        return inputs\n+\n+    @property\n+    def intermediate_outputs(self) -> List[OutputParam]:\n+        return [\n+            OutputParam(\n+                self._image_latents_output_name,\n+                type_hint=torch.Tensor,\n+                description=\"The latents representing the reference image\",\n+            )\n+        ]\n+\n+    @torch.no_grad()\n+    def __call__(self, components: QwenImageModularPipeline, state: PipelineState) -> PipelineState:\n+        block_state = self.get_block_state(state)\n+\n+        device = components._execution_device\n+        dtype = components.vae.dtype\n+\n+        image = getattr(block_state, self._image_input_name)\n+\n+        # Encode image into latents\n+        image_latents = encode_vae_image(\n+            image=image,\n+            vae=components.vae,\n+            generator=block_state.generator,\n+            device=device,\n+            dtype=dtype,\n+            latent_channels=components.num_channels_latents,\n+        )\n+\n+        setattr(block_state, self._image_latents_output_name, image_latents)\n+\n+        self.set_block_state(state, block_state)\n+\n+        return components, state\n+\n+\n+class QwenImageControlNetVaeEncoderStep(ModularPipelineBlocks):\n+    model_name = \"qwenimage\"\n+\n+    @property\n+    def description(self) -> str:\n+        return \"VAE Encoder step that converts `control_image` into latent representations control_image_latents.\\n\"\n+\n+    @property\n+    def expected_components(self) -> List[ComponentSpec]:\n+        components = [\n+            ComponentSpec(\"vae\", AutoencoderKLQwenImage),\n+            ComponentSpec(\"controlnet\", QwenImageControlNetModel),\n+            ComponentSpec(\n+                \"control_image_processor\",\n+                VaeImageProcessor,\n+                config=FrozenDict({\"vae_scale_factor\": 16}),\n+                default_creation_method=\"from_config\",\n+            ),\n+        ]\n+        return components\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        inputs = [\n+            InputParam(\"control_image\", required=True),\n+            InputParam(\"height\"),\n+            InputParam(\"width\"),\n+            InputParam(\"generator\"),\n+        ]\n+        return inputs\n+\n+    @property\n+    def intermediate_outputs(self) -> List[OutputParam]:\n+        return [\n+            OutputParam(\n+                \"control_image_latents\",\n+                type_hint=torch.Tensor,\n+                description=\"The latents representing the control image\",\n+            )\n+        ]\n+\n+    @staticmethod\n+    def check_inputs(height, width, vae_scale_factor):\n+        if height is not None and height % (vae_scale_factor * 2) != 0:\n+            raise ValueError(f\"Height must be divisible by {vae_scale_factor * 2} but is {height}\")\n+\n+        if width is not None and width % (vae_scale_factor * 2) != 0:\n+            raise ValueError(f\"Width must be divisible by {vae_scale_factor * 2} but is {width}\")\n+\n+    @torch.no_grad()\n+    def __call__(self, components: QwenImageModularPipeline, state: PipelineState) -> PipelineState:\n+        block_state = self.get_block_state(state)\n+\n+        self.check_inputs(block_state.height, block_state.width, components.vae_scale_factor)\n+\n+        device = components._execution_device\n+        dtype = components.vae.dtype\n+\n+        height = block_state.height or components.default_height\n+        width = block_state.width or components.default_width\n+\n+        controlnet = unwrap_module(components.controlnet)\n+        if isinstance(controlnet, QwenImageMultiControlNetModel) and not isinstance(block_state.control_image, list):\n+            block_state.control_image = [block_state.control_image]\n+\n+        if isinstance(controlnet, QwenImageMultiControlNetModel):\n+            block_state.control_image_latents = []\n+            for control_image_ in block_state.control_image:\n+                control_image_ = components.control_image_processor.preprocess(\n+                    image=control_image_,\n+                    height=height,\n+                    width=width,\n+                )\n+\n+                control_image_latents_ = encode_vae_image(\n+                    image=control_image_,\n+                    vae=components.vae,\n+                    generator=block_state.generator,\n+                    device=device,\n+                    dtype=dtype,\n+                    latent_channels=components.num_channels_latents,\n+                    sample_mode=\"sample\",\n+                )\n+                block_state.control_image_latents.append(control_image_latents_)\n+\n+        elif isinstance(controlnet, QwenImageControlNetModel):\n+            control_image = components.control_image_processor.preprocess(\n+                image=block_state.control_image,\n+                height=height,\n+                width=width,\n+            )\n+            block_state.control_image_latents = encode_vae_image(\n+                image=control_image,\n+                vae=components.vae,\n+                generator=block_state.generator,\n+                device=device,\n+                dtype=dtype,\n+                latent_channels=components.num_channels_latents,\n+                sample_mode=\"sample\",\n+            )\n+\n+        else:\n+            raise ValueError(\n+                f\"Expected controlnet to be a QwenImageControlNetModel or QwenImageMultiControlNetModel, got {type(controlnet)}\"\n+            )\n+\n+        self.set_block_state(state, block_state)\n+\n+        return components, state"
        },
        {
          "filename": "src/diffusers/modular_pipelines/qwenimage/inputs.py",
          "status": "added",
          "additions": 431,
          "deletions": 0,
          "changes": 431,
          "patch": "@@ -0,0 +1,431 @@\n+# Copyright 2025 Qwen-Image Team and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import List, Tuple\n+\n+import torch\n+\n+from ...models import QwenImageMultiControlNetModel\n+from ..modular_pipeline import ModularPipelineBlocks, PipelineState\n+from ..modular_pipeline_utils import ComponentSpec, InputParam, OutputParam\n+from .modular_pipeline import QwenImageModularPipeline, QwenImagePachifier\n+\n+\n+def repeat_tensor_to_batch_size(\n+    input_name: str,\n+    input_tensor: torch.Tensor,\n+    batch_size: int,\n+    num_images_per_prompt: int = 1,\n+) -> torch.Tensor:\n+    \"\"\"Repeat tensor elements to match the final batch size.\n+\n+    This function expands a tensor's batch dimension to match the final batch size (batch_size * num_images_per_prompt)\n+    by repeating each element along dimension 0.\n+\n+    The input tensor must have batch size 1 or batch_size. The function will:\n+    - If batch size is 1: repeat each element (batch_size * num_images_per_prompt) times\n+    - If batch size equals batch_size: repeat each element num_images_per_prompt times\n+\n+    Args:\n+        input_name (str): Name of the input tensor (used for error messages)\n+        input_tensor (torch.Tensor): The tensor to repeat. Must have batch size 1 or batch_size.\n+        batch_size (int): The base batch size (number of prompts)\n+        num_images_per_prompt (int, optional): Number of images to generate per prompt. Defaults to 1.\n+\n+    Returns:\n+        torch.Tensor: The repeated tensor with final batch size (batch_size * num_images_per_prompt)\n+\n+    Raises:\n+        ValueError: If input_tensor is not a torch.Tensor or has invalid batch size\n+\n+    Examples:\n+        tensor = torch.tensor([[1, 2, 3]]) # shape: [1, 3] repeated = repeat_tensor_to_batch_size(\"image\", tensor,\n+        batch_size=2, num_images_per_prompt=2) repeated # tensor([[1, 2, 3], [1, 2, 3], [1, 2, 3], [1, 2, 3]]) - shape:\n+        [4, 3]\n+\n+        tensor = torch.tensor([[1, 2, 3], [4, 5, 6]]) # shape: [2, 3] repeated = repeat_tensor_to_batch_size(\"image\",\n+        tensor, batch_size=2, num_images_per_prompt=2) repeated # tensor([[1, 2, 3], [1, 2, 3], [4, 5, 6], [4, 5, 6]])\n+        - shape: [4, 3]\n+    \"\"\"\n+    # make sure input is a tensor\n+    if not isinstance(input_tensor, torch.Tensor):\n+        raise ValueError(f\"`{input_name}` must be a tensor\")\n+\n+    # make sure input tensor e.g. image_latents has batch size 1 or batch_size same as prompts\n+    if input_tensor.shape[0] == 1:\n+        repeat_by = batch_size * num_images_per_prompt\n+    elif input_tensor.shape[0] == batch_size:\n+        repeat_by = num_images_per_prompt\n+    else:\n+        raise ValueError(\n+            f\"`{input_name}` must have have batch size 1 or {batch_size}, but got {input_tensor.shape[0]}\"\n+        )\n+\n+    # expand the tensor to match the batch_size * num_images_per_prompt\n+    input_tensor = input_tensor.repeat_interleave(repeat_by, dim=0)\n+\n+    return input_tensor\n+\n+\n+def calculate_dimension_from_latents(latents: torch.Tensor, vae_scale_factor: int) -> Tuple[int, int]:\n+    \"\"\"Calculate image dimensions from latent tensor dimensions.\n+\n+    This function converts latent space dimensions to image space dimensions by multiplying the latent height and width\n+    by the VAE scale factor.\n+\n+    Args:\n+        latents (torch.Tensor): The latent tensor. Must have 4 or 5 dimensions.\n+            Expected shapes: [batch, channels, height, width] or [batch, channels, frames, height, width]\n+        vae_scale_factor (int): The scale factor used by the VAE to compress images.\n+            Typically 8 for most VAEs (image is 8x larger than latents in each dimension)\n+\n+    Returns:\n+        Tuple[int, int]: The calculated image dimensions as (height, width)\n+\n+    Raises:\n+        ValueError: If latents tensor doesn't have 4 or 5 dimensions\n+\n+    \"\"\"\n+    # make sure the latents are not packed\n+    if latents.ndim != 4 and latents.ndim != 5:\n+        raise ValueError(f\"unpacked latents must have 4 or 5 dimensions, but got {latents.ndim}\")\n+\n+    latent_height, latent_width = latents.shape[-2:]\n+\n+    height = latent_height * vae_scale_factor\n+    width = latent_width * vae_scale_factor\n+\n+    return height, width\n+\n+\n+class QwenImageTextInputsStep(ModularPipelineBlocks):\n+    model_name = \"qwenimage\"\n+\n+    @property\n+    def description(self) -> str:\n+        summary_section = (\n+            \"Text input processing step that standardizes text embeddings for the pipeline.\\n\"\n+            \"This step:\\n\"\n+            \"  1. Determines `batch_size` and `dtype` based on `prompt_embeds`\\n\"\n+            \"  2. Ensures all text embeddings have consistent batch sizes (batch_size * num_images_per_prompt)\"\n+        )\n+\n+        # Placement guidance\n+        placement_section = \"\\n\\nThis block should be placed after all encoder steps to process the text embeddings before they are used in subsequent pipeline steps.\"\n+\n+        return summary_section + placement_section\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        return [\n+            InputParam(name=\"num_images_per_prompt\", default=1),\n+            InputParam(name=\"prompt_embeds\", required=True, kwargs_type=\"denoiser_input_fields\"),\n+            InputParam(name=\"prompt_embeds_mask\", required=True, kwargs_type=\"denoiser_input_fields\"),\n+            InputParam(name=\"negative_prompt_embeds\", kwargs_type=\"denoiser_input_fields\"),\n+            InputParam(name=\"negative_prompt_embeds_mask\", kwargs_type=\"denoiser_input_fields\"),\n+        ]\n+\n+    @property\n+    def intermediate_outputs(self) -> List[str]:\n+        return [\n+            OutputParam(\n+                \"batch_size\",\n+                type_hint=int,\n+                description=\"Number of prompts, the final batch size of model inputs should be batch_size * num_images_per_prompt\",\n+            ),\n+            OutputParam(\n+                \"dtype\",\n+                type_hint=torch.dtype,\n+                description=\"Data type of model tensor inputs (determined by `prompt_embeds`)\",\n+            ),\n+        ]\n+\n+    @staticmethod\n+    def check_inputs(\n+        prompt_embeds,\n+        prompt_embeds_mask,\n+        negative_prompt_embeds,\n+        negative_prompt_embeds_mask,\n+    ):\n+        if negative_prompt_embeds is not None and negative_prompt_embeds_mask is None:\n+            raise ValueError(\"`negative_prompt_embeds_mask` is required when `negative_prompt_embeds` is not None\")\n+\n+        if negative_prompt_embeds is None and negative_prompt_embeds_mask is not None:\n+            raise ValueError(\"cannot pass `negative_prompt_embeds_mask` without `negative_prompt_embeds`\")\n+\n+        if prompt_embeds_mask.shape[0] != prompt_embeds.shape[0]:\n+            raise ValueError(\"`prompt_embeds_mask` must have the same batch size as `prompt_embeds`\")\n+\n+        elif negative_prompt_embeds is not None and negative_prompt_embeds.shape[0] != prompt_embeds.shape[0]:\n+            raise ValueError(\"`negative_prompt_embeds` must have the same batch size as `prompt_embeds`\")\n+\n+        elif (\n+            negative_prompt_embeds_mask is not None and negative_prompt_embeds_mask.shape[0] != prompt_embeds.shape[0]\n+        ):\n+            raise ValueError(\"`negative_prompt_embeds_mask` must have the same batch size as `prompt_embeds`\")\n+\n+    def __call__(self, components: QwenImageModularPipeline, state: PipelineState) -> PipelineState:\n+        block_state = self.get_block_state(state)\n+\n+        self.check_inputs(\n+            prompt_embeds=block_state.prompt_embeds,\n+            prompt_embeds_mask=block_state.prompt_embeds_mask,\n+            negative_prompt_embeds=block_state.negative_prompt_embeds,\n+            negative_prompt_embeds_mask=block_state.negative_prompt_embeds_mask,\n+        )\n+\n+        block_state.batch_size = block_state.prompt_embeds.shape[0]\n+        block_state.dtype = block_state.prompt_embeds.dtype\n+\n+        _, seq_len, _ = block_state.prompt_embeds.shape\n+\n+        block_state.prompt_embeds = block_state.prompt_embeds.repeat(1, block_state.num_images_per_prompt, 1)\n+        block_state.prompt_embeds = block_state.prompt_embeds.view(\n+            block_state.batch_size * block_state.num_images_per_prompt, seq_len, -1\n+        )\n+\n+        block_state.prompt_embeds_mask = block_state.prompt_embeds_mask.repeat(1, block_state.num_images_per_prompt, 1)\n+        block_state.prompt_embeds_mask = block_state.prompt_embeds_mask.view(\n+            block_state.batch_size * block_state.num_images_per_prompt, seq_len\n+        )\n+\n+        if block_state.negative_prompt_embeds is not None:\n+            _, seq_len, _ = block_state.negative_prompt_embeds.shape\n+            block_state.negative_prompt_embeds = block_state.negative_prompt_embeds.repeat(\n+                1, block_state.num_images_per_prompt, 1\n+            )\n+            block_state.negative_prompt_embeds = block_state.negative_prompt_embeds.view(\n+                block_state.batch_size * block_state.num_images_per_prompt, seq_len, -1\n+            )\n+\n+            block_state.negative_prompt_embeds_mask = block_state.negative_prompt_embeds_mask.repeat(\n+                1, block_state.num_images_per_prompt, 1\n+            )\n+            block_state.negative_prompt_embeds_mask = block_state.negative_prompt_embeds_mask.view(\n+                block_state.batch_size * block_state.num_images_per_prompt, seq_len\n+            )\n+\n+        self.set_block_state(state, block_state)\n+\n+        return components, state\n+\n+\n+class QwenImageInputsDynamicStep(ModularPipelineBlocks):\n+    model_name = \"qwenimage\"\n+\n+    def __init__(\n+        self,\n+        image_latent_inputs: List[str] = [\"image_latents\"],\n+        additional_batch_inputs: List[str] = [],\n+    ):\n+        \"\"\"Initialize a configurable step that standardizes the inputs for the denoising step. It:\\n\"\n+\n+        This step handles multiple common tasks to prepare inputs for the denoising step:\n+        1. For encoded image latents, use it update height/width if None, patchifies, and expands batch size\n+        2. For additional_batch_inputs: Only expands batch dimensions to match final batch size\n+\n+        This is a dynamic block that allows you to configure which inputs to process.\n+\n+        Args:\n+            image_latent_inputs (List[str], optional): Names of image latent tensors to process.\n+                These will be used to determine height/width, patchified, and batch-expanded. Can be a single string or\n+                list of strings. Defaults to [\"image_latents\"]. Examples: [\"image_latents\"], [\"control_image_latents\"]\n+            additional_batch_inputs (List[str], optional):\n+                Names of additional conditional input tensors to expand batch size. These tensors will only have their\n+                batch dimensions adjusted to match the final batch size. Can be a single string or list of strings.\n+                Defaults to []. Examples: [\"processed_mask_image\"]\n+\n+        Examples:\n+            # Configure to process image_latents (default behavior) QwenImageInputsDynamicStep()\n+\n+            # Configure to process multiple image latent inputs\n+            QwenImageInputsDynamicStep(image_latent_inputs=[\"image_latents\", \"control_image_latents\"])\n+\n+            # Configure to process image latents and additional batch inputs QwenImageInputsDynamicStep(\n+                image_latent_inputs=[\"image_latents\"], additional_batch_inputs=[\"processed_mask_image\"]\n+            )\n+        \"\"\"\n+        if not isinstance(image_latent_inputs, list):\n+            image_latent_inputs = [image_latent_inputs]\n+        if not isinstance(additional_batch_inputs, list):\n+            additional_batch_inputs = [additional_batch_inputs]\n+\n+        self._image_latent_inputs = image_latent_inputs\n+        self._additional_batch_inputs = additional_batch_inputs\n+        super().__init__()\n+\n+    @property\n+    def description(self) -> str:\n+        # Functionality section\n+        summary_section = (\n+            \"Input processing step that:\\n\"\n+            \"  1. For image latent inputs: Updates height/width if None, patchifies latents, and expands batch size\\n\"\n+            \"  2. For additional batch inputs: Expands batch dimensions to match final batch size\"\n+        )\n+\n+        # Inputs info\n+        inputs_info = \"\"\n+        if self._image_latent_inputs or self._additional_batch_inputs:\n+            inputs_info = \"\\n\\nConfigured inputs:\"\n+            if self._image_latent_inputs:\n+                inputs_info += f\"\\n  - Image latent inputs: {self._image_latent_inputs}\"\n+            if self._additional_batch_inputs:\n+                inputs_info += f\"\\n  - Additional batch inputs: {self._additional_batch_inputs}\"\n+\n+        # Placement guidance\n+        placement_section = \"\\n\\nThis block should be placed after the encoder steps and the text input step.\"\n+\n+        return summary_section + inputs_info + placement_section\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        inputs = [\n+            InputParam(name=\"num_images_per_prompt\", default=1),\n+            InputParam(name=\"batch_size\", required=True),\n+            InputParam(name=\"height\"),\n+            InputParam(name=\"width\"),\n+        ]\n+\n+        # Add image latent inputs\n+        for image_latent_input_name in self._image_latent_inputs:\n+            inputs.append(InputParam(name=image_latent_input_name))\n+\n+        # Add additional batch inputs\n+        for input_name in self._additional_batch_inputs:\n+            inputs.append(InputParam(name=input_name))\n+\n+        return inputs\n+\n+    @property\n+    def expected_components(self) -> List[ComponentSpec]:\n+        return [\n+            ComponentSpec(\"pachifier\", QwenImagePachifier, default_creation_method=\"from_config\"),\n+        ]\n+\n+    def __call__(self, components: QwenImageModularPipeline, state: PipelineState) -> PipelineState:\n+        block_state = self.get_block_state(state)\n+\n+        # Process image latent inputs (height/width calculation, patchify, and batch expansion)\n+        for image_latent_input_name in self._image_latent_inputs:\n+            image_latent_tensor = getattr(block_state, image_latent_input_name)\n+            if image_latent_tensor is None:\n+                continue\n+\n+            # 1. Calculate height/width from latents\n+            height, width = calculate_dimension_from_latents(image_latent_tensor, components.vae_scale_factor)\n+            block_state.height = block_state.height or height\n+            block_state.width = block_state.width or width\n+\n+            # 2. Patchify the image latent tensor\n+            image_latent_tensor = components.pachifier.pack_latents(image_latent_tensor)\n+\n+            # 3. Expand batch size\n+            image_latent_tensor = repeat_tensor_to_batch_size(\n+                input_name=image_latent_input_name,\n+                input_tensor=image_latent_tensor,\n+                num_images_per_prompt=block_state.num_images_per_prompt,\n+                batch_size=block_state.batch_size,\n+            )\n+\n+            setattr(block_state, image_latent_input_name, image_latent_tensor)\n+\n+        # Process additional batch inputs (only batch expansion)\n+        for input_name in self._additional_batch_inputs:\n+            input_tensor = getattr(block_state, input_name)\n+            if input_tensor is None:\n+                continue\n+\n+            # Only expand batch size\n+            input_tensor = repeat_tensor_to_batch_size(\n+                input_name=input_name,\n+                input_tensor=input_tensor,\n+                num_images_per_prompt=block_state.num_images_per_prompt,\n+                batch_size=block_state.batch_size,\n+            )\n+\n+            setattr(block_state, input_name, input_tensor)\n+\n+        self.set_block_state(state, block_state)\n+        return components, state\n+\n+\n+class QwenImageControlNetInputsStep(ModularPipelineBlocks):\n+    model_name = \"qwenimage\"\n+\n+    @property\n+    def description(self) -> str:\n+        return \"prepare the `control_image_latents` for controlnet. Insert after all the other inputs steps.\"\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        return [\n+            InputParam(name=\"control_image_latents\", required=True),\n+            InputParam(name=\"batch_size\", required=True),\n+            InputParam(name=\"num_images_per_prompt\", default=1),\n+            InputParam(name=\"height\"),\n+            InputParam(name=\"width\"),\n+        ]\n+\n+    @torch.no_grad()\n+    def __call__(self, components: QwenImageModularPipeline, state: PipelineState) -> PipelineState:\n+        block_state = self.get_block_state(state)\n+\n+        if isinstance(components.controlnet, QwenImageMultiControlNetModel):\n+            control_image_latents = []\n+            # loop through each control_image_latents\n+            for i, control_image_latents_ in enumerate(block_state.control_image_latents):\n+                # 1. update height/width if not provided\n+                height, width = calculate_dimension_from_latents(control_image_latents_, components.vae_scale_factor)\n+                block_state.height = block_state.height or height\n+                block_state.width = block_state.width or width\n+\n+                # 2. pack\n+                control_image_latents_ = components.pachifier.pack_latents(control_image_latents_)\n+\n+                # 3. repeat to match the batch size\n+                control_image_latents_ = repeat_tensor_to_batch_size(\n+                    input_name=f\"control_image_latents[{i}]\",\n+                    input_tensor=control_image_latents_,\n+                    num_images_per_prompt=block_state.num_images_per_prompt,\n+                    batch_size=block_state.batch_size,\n+                )\n+\n+                control_image_latents.append(control_image_latents_)\n+\n+            block_state.control_image_latents = control_image_latents\n+\n+        else:\n+            # 1. update height/width if not provided\n+            height, width = calculate_dimension_from_latents(\n+                block_state.control_image_latents, components.vae_scale_factor\n+            )\n+            block_state.height = block_state.height or height\n+            block_state.width = block_state.width or width\n+\n+            # 2. pack\n+            block_state.control_image_latents = components.pachifier.pack_latents(block_state.control_image_latents)\n+\n+            # 3. repeat to match the batch size\n+            block_state.control_image_latents = repeat_tensor_to_batch_size(\n+                input_name=\"control_image_latents\",\n+                input_tensor=block_state.control_image_latents,\n+                num_images_per_prompt=block_state.num_images_per_prompt,\n+                batch_size=block_state.batch_size,\n+            )\n+\n+            block_state.control_image_latents = block_state.control_image_latents\n+\n+        self.set_block_state(state, block_state)\n+\n+        return components, state"
        },
        {
          "filename": "src/diffusers/modular_pipelines/qwenimage/modular_blocks.py",
          "status": "added",
          "additions": 841,
          "deletions": 0,
          "changes": 841,
          "patch": "@@ -0,0 +1,841 @@\n+# Copyright 2025 Qwen-Image Team and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from ...utils import logging\n+from ..modular_pipeline import AutoPipelineBlocks, SequentialPipelineBlocks\n+from ..modular_pipeline_utils import InsertableDict\n+from .before_denoise import (\n+    QwenImageControlNetBeforeDenoiserStep,\n+    QwenImageCreateMaskLatentsStep,\n+    QwenImageEditRoPEInputsStep,\n+    QwenImagePrepareLatentsStep,\n+    QwenImagePrepareLatentsWithStrengthStep,\n+    QwenImageRoPEInputsStep,\n+    QwenImageSetTimestepsStep,\n+    QwenImageSetTimestepsWithStrengthStep,\n+)\n+from .decoders import QwenImageDecoderStep, QwenImageInpaintProcessImagesOutputStep, QwenImageProcessImagesOutputStep\n+from .denoise import (\n+    QwenImageControlNetDenoiseStep,\n+    QwenImageDenoiseStep,\n+    QwenImageEditDenoiseStep,\n+    QwenImageEditInpaintDenoiseStep,\n+    QwenImageInpaintControlNetDenoiseStep,\n+    QwenImageInpaintDenoiseStep,\n+    QwenImageLoopBeforeDenoiserControlNet,\n+)\n+from .encoders import (\n+    QwenImageControlNetVaeEncoderStep,\n+    QwenImageEditResizeDynamicStep,\n+    QwenImageEditTextEncoderStep,\n+    QwenImageInpaintProcessImagesInputStep,\n+    QwenImageProcessImagesInputStep,\n+    QwenImageTextEncoderStep,\n+    QwenImageVaeEncoderDynamicStep,\n+)\n+from .inputs import QwenImageControlNetInputsStep, QwenImageInputsDynamicStep, QwenImageTextInputsStep\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+# 1. QwenImage\n+\n+## 1.1 QwenImage/text2image\n+\n+#### QwenImage/decode\n+#### (standard decode step works for most tasks except for inpaint)\n+QwenImageDecodeBlocks = InsertableDict(\n+    [\n+        (\"decode\", QwenImageDecoderStep()),\n+        (\"postprocess\", QwenImageProcessImagesOutputStep()),\n+    ]\n+)\n+\n+\n+class QwenImageDecodeStep(SequentialPipelineBlocks):\n+    model_name = \"qwenimage\"\n+    block_classes = QwenImageDecodeBlocks.values()\n+    block_names = QwenImageDecodeBlocks.keys()\n+\n+    @property\n+    def description(self):\n+        return \"Decode step that decodes the latents to images and postprocess the generated image.\"\n+\n+\n+#### QwenImage/text2image presets\n+TEXT2IMAGE_BLOCKS = InsertableDict(\n+    [\n+        (\"text_encoder\", QwenImageTextEncoderStep()),\n+        (\"input\", QwenImageTextInputsStep()),\n+        (\"prepare_latents\", QwenImagePrepareLatentsStep()),\n+        (\"set_timesteps\", QwenImageSetTimestepsStep()),\n+        (\"prepare_rope_inputs\", QwenImageRoPEInputsStep()),\n+        (\"denoise\", QwenImageDenoiseStep()),\n+        (\"decode\", QwenImageDecodeStep()),\n+    ]\n+)\n+\n+\n+## 1.2 QwenImage/inpaint\n+\n+#### QwenImage/inpaint vae encoder\n+QwenImageInpaintVaeEncoderBlocks = InsertableDict(\n+    [\n+        (\n+            \"preprocess\",\n+            QwenImageInpaintProcessImagesInputStep,\n+        ),  # image, mask_image -> processed_image, processed_mask_image, mask_overlay_kwargs\n+        (\"encode\", QwenImageVaeEncoderDynamicStep()),  # processed_image -> image_latents\n+    ]\n+)\n+\n+\n+class QwenImageInpaintVaeEncoderStep(SequentialPipelineBlocks):\n+    model_name = \"qwenimage\"\n+    block_classes = QwenImageInpaintVaeEncoderBlocks.values()\n+    block_names = QwenImageInpaintVaeEncoderBlocks.keys()\n+\n+    @property\n+    def description(self) -> str:\n+        return (\n+            \"This step is used for processing image and mask inputs for inpainting tasks. It:\\n\"\n+            \" - Resizes the image to the target size, based on `height` and `width`.\\n\"\n+            \" - Processes and updates `image` and `mask_image`.\\n\"\n+            \" - Creates `image_latents`.\"\n+        )\n+\n+\n+#### QwenImage/inpaint inputs\n+QwenImageInpaintInputBlocks = InsertableDict(\n+    [\n+        (\"text_inputs\", QwenImageTextInputsStep()),  # default step to process text embeddings\n+        (\n+            \"additional_inputs\",\n+            QwenImageInputsDynamicStep(\n+                image_latent_inputs=[\"image_latents\"], additional_batch_inputs=[\"processed_mask_image\"]\n+            ),\n+        ),\n+    ]\n+)\n+\n+\n+class QwenImageInpaintInputStep(SequentialPipelineBlocks):\n+    model_name = \"qwenimage\"\n+    block_classes = QwenImageInpaintInputBlocks.values()\n+    block_names = QwenImageInpaintInputBlocks.keys()\n+\n+    @property\n+    def description(self):\n+        return \"Input step that prepares the inputs for the inpainting denoising step. It:\\n\"\n+        \" - make sure the text embeddings have consistent batch size as well as the additional inputs (`image_latents` and `processed_mask_image`).\\n\"\n+        \" - update height/width based `image_latents`, patchify `image_latents`.\"\n+\n+\n+# QwenImage/inpaint prepare latents\n+QwenImageInpaintPrepareLatentsBlocks = InsertableDict(\n+    [\n+        (\"add_noise_to_latents\", QwenImagePrepareLatentsWithStrengthStep()),\n+        (\"create_mask_latents\", QwenImageCreateMaskLatentsStep()),\n+    ]\n+)\n+\n+\n+class QwenImageInpaintPrepareLatentsStep(SequentialPipelineBlocks):\n+    model_name = \"qwenimage\"\n+    block_classes = QwenImageInpaintPrepareLatentsBlocks.values()\n+    block_names = QwenImageInpaintPrepareLatentsBlocks.keys()\n+\n+    @property\n+    def description(self) -> str:\n+        return (\n+            \"This step prepares the latents/image_latents and mask inputs for the inpainting denoising step. It:\\n\"\n+            \" - Add noise to the image latents to create the latents input for the denoiser.\\n\"\n+            \" - Create the pachified latents `mask` based on the processedmask image.\\n\"\n+        )\n+\n+\n+#### QwenImage/inpaint decode\n+QwenImageInpaintDecodeBlocks = InsertableDict(\n+    [\n+        (\"decode\", QwenImageDecoderStep()),\n+        (\"postprocess\", QwenImageInpaintProcessImagesOutputStep()),\n+    ]\n+)\n+\n+\n+class QwenImageInpaintDecodeStep(SequentialPipelineBlocks):\n+    model_name = \"qwenimage\"\n+    block_classes = QwenImageInpaintDecodeBlocks.values()\n+    block_names = QwenImageInpaintDecodeBlocks.keys()\n+\n+    @property\n+    def description(self):\n+        return \"Decode step that decodes the latents to images and postprocess the generated image, optional apply the mask overally to the original image.\"\n+\n+\n+#### QwenImage/inpaint presets\n+INPAINT_BLOCKS = InsertableDict(\n+    [\n+        (\"text_encoder\", QwenImageTextEncoderStep()),\n+        (\"vae_encoder\", QwenImageInpaintVaeEncoderStep()),\n+        (\"input\", QwenImageInpaintInputStep()),\n+        (\"prepare_latents\", QwenImagePrepareLatentsStep()),\n+        (\"set_timesteps\", QwenImageSetTimestepsWithStrengthStep()),\n+        (\"prepare_inpaint_latents\", QwenImageInpaintPrepareLatentsStep()),\n+        (\"prepare_rope_inputs\", QwenImageRoPEInputsStep()),\n+        (\"denoise\", QwenImageInpaintDenoiseStep()),\n+        (\"decode\", QwenImageInpaintDecodeStep()),\n+    ]\n+)\n+\n+\n+## 1.3 QwenImage/img2img\n+\n+#### QwenImage/img2img vae encoder\n+QwenImageImg2ImgVaeEncoderBlocks = InsertableDict(\n+    [\n+        (\"preprocess\", QwenImageProcessImagesInputStep()),\n+        (\"encode\", QwenImageVaeEncoderDynamicStep()),\n+    ]\n+)\n+\n+\n+class QwenImageImg2ImgVaeEncoderStep(SequentialPipelineBlocks):\n+    model_name = \"qwenimage\"\n+\n+    block_classes = QwenImageImg2ImgVaeEncoderBlocks.values()\n+    block_names = QwenImageImg2ImgVaeEncoderBlocks.keys()\n+\n+    @property\n+    def description(self) -> str:\n+        return \"Vae encoder step that preprocess andencode the image inputs into their latent representations.\"\n+\n+\n+#### QwenImage/img2img inputs\n+QwenImageImg2ImgInputBlocks = InsertableDict(\n+    [\n+        (\"text_inputs\", QwenImageTextInputsStep()),  # default step to process text embeddings\n+        (\"additional_inputs\", QwenImageInputsDynamicStep(image_latent_inputs=[\"image_latents\"])),\n+    ]\n+)\n+\n+\n+class QwenImageImg2ImgInputStep(SequentialPipelineBlocks):\n+    model_name = \"qwenimage\"\n+    block_classes = QwenImageImg2ImgInputBlocks.values()\n+    block_names = QwenImageImg2ImgInputBlocks.keys()\n+\n+    @property\n+    def description(self):\n+        return \"Input step that prepares the inputs for the img2img denoising step. It:\\n\"\n+        \" - make sure the text embeddings have consistent batch size as well as the additional inputs (`image_latents`).\\n\"\n+        \" - update height/width based `image_latents`, patchify `image_latents`.\"\n+\n+\n+#### QwenImage/img2img presets\n+IMAGE2IMAGE_BLOCKS = InsertableDict(\n+    [\n+        (\"text_encoder\", QwenImageTextEncoderStep()),\n+        (\"vae_encoder\", QwenImageImg2ImgVaeEncoderStep()),\n+        (\"input\", QwenImageImg2ImgInputStep()),\n+        (\"prepare_latents\", QwenImagePrepareLatentsStep()),\n+        (\"set_timesteps\", QwenImageSetTimestepsWithStrengthStep()),\n+        (\"prepare_img2img_latents\", QwenImagePrepareLatentsWithStrengthStep()),\n+        (\"prepare_rope_inputs\", QwenImageRoPEInputsStep()),\n+        (\"denoise\", QwenImageDenoiseStep()),\n+        (\"decode\", QwenImageDecodeStep()),\n+    ]\n+)\n+\n+\n+## 1.4 QwenImage/controlnet\n+\n+#### QwenImage/controlnet presets\n+CONTROLNET_BLOCKS = InsertableDict(\n+    [\n+        (\"controlnet_vae_encoder\", QwenImageControlNetVaeEncoderStep()),  # vae encoder step for control_image\n+        (\"controlnet_inputs\", QwenImageControlNetInputsStep()),  # additional input step for controlnet\n+        (\n+            \"controlnet_before_denoise\",\n+            QwenImageControlNetBeforeDenoiserStep(),\n+        ),  # before denoise step (after set_timesteps step)\n+        (\n+            \"controlnet_denoise_loop_before\",\n+            QwenImageLoopBeforeDenoiserControlNet(),\n+        ),  # controlnet loop step (insert before the denoiseloop_denoiser)\n+    ]\n+)\n+\n+\n+## 1.5 QwenImage/auto encoders\n+\n+\n+#### for inpaint and img2img tasks\n+class QwenImageAutoVaeEncoderStep(AutoPipelineBlocks):\n+    block_classes = [QwenImageInpaintVaeEncoderStep, QwenImageImg2ImgVaeEncoderStep]\n+    block_names = [\"inpaint\", \"img2img\"]\n+    block_trigger_inputs = [\"mask_image\", \"image\"]\n+\n+    @property\n+    def description(self):\n+        return (\n+            \"Vae encoder step that encode the image inputs into their latent representations.\\n\"\n+            + \"This is an auto pipeline block.\\n\"\n+            + \" - `QwenImageInpaintVaeEncoderStep` (inpaint) is used when `mask_image` is provided.\\n\"\n+            + \" - `QwenImageImg2ImgVaeEncoderStep` (img2img) is used when `image` is provided.\\n\"\n+            + \" - if `mask_image` or `image` is not provided, step will be skipped.\"\n+        )\n+\n+\n+# for controlnet tasks\n+class QwenImageOptionalControlNetVaeEncoderStep(AutoPipelineBlocks):\n+    block_classes = [QwenImageControlNetVaeEncoderStep]\n+    block_names = [\"controlnet\"]\n+    block_trigger_inputs = [\"control_image\"]\n+\n+    @property\n+    def description(self):\n+        return (\n+            \"Vae encoder step that encode the image inputs into their latent representations.\\n\"\n+            + \"This is an auto pipeline block.\\n\"\n+            + \" - `QwenImageControlNetVaeEncoderStep` (controlnet) is used when `control_image` is provided.\\n\"\n+            + \" - if `control_image` is not provided, step will be skipped.\"\n+        )\n+\n+\n+## 1.6 QwenImage/auto inputs\n+\n+\n+# text2image/inpaint/img2img\n+class QwenImageAutoInputStep(AutoPipelineBlocks):\n+    block_classes = [QwenImageInpaintInputStep, QwenImageImg2ImgInputStep, QwenImageTextInputsStep]\n+    block_names = [\"inpaint\", \"img2img\", \"text2image\"]\n+    block_trigger_inputs = [\"processed_mask_image\", \"image_latents\", None]\n+\n+    @property\n+    def description(self):\n+        return (\n+            \"Input step that standardize the inputs for the denoising step, e.g. make sure inputs have consistent batch size, and patchified. \\n\"\n+            \" This is an auto pipeline block that works for text2image/inpaint/img2img tasks.\\n\"\n+            + \" - `QwenImageInpaintInputStep` (inpaint) is used when `processed_mask_image` is provided.\\n\"\n+            + \" - `QwenImageImg2ImgInputStep` (img2img) is used when `image_latents` is provided.\\n\"\n+            + \" - `QwenImageTextInputsStep` (text2image) is used when both `processed_mask_image` and `image_latents` are not provided.\\n\"\n+        )\n+\n+\n+# controlnet\n+class QwenImageOptionalControlNetInputStep(AutoPipelineBlocks):\n+    block_classes = [QwenImageControlNetInputsStep]\n+    block_names = [\"controlnet\"]\n+    block_trigger_inputs = [\"control_image_latents\"]\n+\n+    @property\n+    def description(self):\n+        return (\n+            \"Controlnet input step that prepare the control_image_latents input.\\n\"\n+            + \"This is an auto pipeline block.\\n\"\n+            + \" - `QwenImageControlNetInputsStep` (controlnet) is used when `control_image_latents` is provided.\\n\"\n+            + \" - if `control_image_latents` is not provided, step will be skipped.\"\n+        )\n+\n+\n+## 1.7 QwenImage/auto before denoise step\n+# compose the steps into a BeforeDenoiseStep for text2image/img2img/inpaint tasks before combine into an auto step\n+\n+#  QwenImage/text2image before denoise\n+QwenImageText2ImageBeforeDenoiseBlocks = InsertableDict(\n+    [\n+        (\"prepare_latents\", QwenImagePrepareLatentsStep()),\n+        (\"set_timesteps\", QwenImageSetTimestepsStep()),\n+        (\"prepare_rope_inputs\", QwenImageRoPEInputsStep()),\n+    ]\n+)\n+\n+\n+class QwenImageText2ImageBeforeDenoiseStep(SequentialPipelineBlocks):\n+    model_name = \"qwenimage\"\n+    block_classes = QwenImageText2ImageBeforeDenoiseBlocks.values()\n+    block_names = QwenImageText2ImageBeforeDenoiseBlocks.keys()\n+\n+    @property\n+    def description(self):\n+        return \"Before denoise step that prepare the inputs (timesteps, latents, rope inputs etc.) for the denoise step for text2image task.\"\n+\n+\n+# QwenImage/inpaint before denoise\n+QwenImageInpaintBeforeDenoiseBlocks = InsertableDict(\n+    [\n+        (\"prepare_latents\", QwenImagePrepareLatentsStep()),\n+        (\"set_timesteps\", QwenImageSetTimestepsWithStrengthStep()),\n+        (\"prepare_inpaint_latents\", QwenImageInpaintPrepareLatentsStep()),\n+        (\"prepare_rope_inputs\", QwenImageRoPEInputsStep()),\n+    ]\n+)\n+\n+\n+class QwenImageInpaintBeforeDenoiseStep(SequentialPipelineBlocks):\n+    model_name = \"qwenimage\"\n+    block_classes = QwenImageInpaintBeforeDenoiseBlocks.values()\n+    block_names = QwenImageInpaintBeforeDenoiseBlocks.keys()\n+\n+    @property\n+    def description(self):\n+        return \"Before denoise step that prepare the inputs (timesteps, latents, rope inputs etc.) for the denoise step for inpaint task.\"\n+\n+\n+# QwenImage/img2img before denoise\n+QwenImageImg2ImgBeforeDenoiseBlocks = InsertableDict(\n+    [\n+        (\"prepare_latents\", QwenImagePrepareLatentsStep()),\n+        (\"set_timesteps\", QwenImageSetTimestepsWithStrengthStep()),\n+        (\"prepare_img2img_latents\", QwenImagePrepareLatentsWithStrengthStep()),\n+        (\"prepare_rope_inputs\", QwenImageRoPEInputsStep()),\n+    ]\n+)\n+\n+\n+class QwenImageImg2ImgBeforeDenoiseStep(SequentialPipelineBlocks):\n+    model_name = \"qwenimage\"\n+    block_classes = QwenImageImg2ImgBeforeDenoiseBlocks.values()\n+    block_names = QwenImageImg2ImgBeforeDenoiseBlocks.keys()\n+\n+    @property\n+    def description(self):\n+        return \"Before denoise step that prepare the inputs (timesteps, latents, rope inputs etc.) for the denoise step for img2img task.\"\n+\n+\n+# auto before_denoise step for text2image, inpaint, img2img tasks\n+class QwenImageAutoBeforeDenoiseStep(AutoPipelineBlocks):\n+    block_classes = [\n+        QwenImageInpaintBeforeDenoiseStep,\n+        QwenImageImg2ImgBeforeDenoiseStep,\n+        QwenImageText2ImageBeforeDenoiseStep,\n+    ]\n+    block_names = [\"inpaint\", \"img2img\", \"text2image\"]\n+    block_trigger_inputs = [\"processed_mask_image\", \"image_latents\", None]\n+\n+    @property\n+    def description(self):\n+        return (\n+            \"Before denoise step that prepare the inputs (timesteps, latents, rope inputs etc.) for the denoise step.\\n\"\n+            + \"This is an auto pipeline block that works for text2img, inpainting, img2img tasks.\\n\"\n+            + \" - `QwenImageInpaintBeforeDenoiseStep` (inpaint) is used when `processed_mask_image` is provided.\\n\"\n+            + \" - `QwenImageImg2ImgBeforeDenoiseStep` (img2img) is used when `image_latents` is provided.\\n\"\n+            + \" - `QwenImageText2ImageBeforeDenoiseStep` (text2image) is used when both `processed_mask_image` and `image_latents` are not provided.\\n\"\n+        )\n+\n+\n+# auto before_denoise step for controlnet tasks\n+class QwenImageOptionalControlNetBeforeDenoiseStep(AutoPipelineBlocks):\n+    block_classes = [QwenImageControlNetBeforeDenoiserStep]\n+    block_names = [\"controlnet\"]\n+    block_trigger_inputs = [\"control_image_latents\"]\n+\n+    @property\n+    def description(self):\n+        return (\n+            \"Controlnet before denoise step that prepare the controlnet input.\\n\"\n+            + \"This is an auto pipeline block.\\n\"\n+            + \" - `QwenImageControlNetBeforeDenoiserStep` (controlnet) is used when `control_image_latents` is provided.\\n\"\n+            + \" - if `control_image_latents` is not provided, step will be skipped.\"\n+        )\n+\n+\n+## 1.8 QwenImage/auto denoise\n+\n+\n+# auto denoise step for controlnet tasks: works for all tasks with controlnet\n+class QwenImageControlNetAutoDenoiseStep(AutoPipelineBlocks):\n+    block_classes = [QwenImageInpaintControlNetDenoiseStep, QwenImageControlNetDenoiseStep]\n+    block_names = [\"inpaint_denoise\", \"denoise\"]\n+    block_trigger_inputs = [\"mask\", None]\n+\n+    @property\n+    def description(self):\n+        return (\n+            \"Controlnet step during the denoising process. \\n\"\n+            \" This is an auto pipeline block that works for inpaint and text2image/img2img tasks with controlnet.\\n\"\n+            + \" - `QwenImageInpaintControlNetDenoiseStep` (inpaint) is used when `mask` is provided.\\n\"\n+            + \" - `QwenImageControlNetDenoiseStep` (text2image/img2img) is used when `mask` is not provided.\\n\"\n+        )\n+\n+\n+# auto denoise step for everything: works for all tasks with or without controlnet\n+class QwenImageAutoDenoiseStep(AutoPipelineBlocks):\n+    block_classes = [\n+        QwenImageControlNetAutoDenoiseStep,\n+        QwenImageInpaintDenoiseStep,\n+        QwenImageDenoiseStep,\n+    ]\n+    block_names = [\"controlnet_denoise\", \"inpaint_denoise\", \"denoise\"]\n+    block_trigger_inputs = [\"control_image_latents\", \"mask\", None]\n+\n+    @property\n+    def description(self):\n+        return (\n+            \"Denoise step that iteratively denoise the latents. \\n\"\n+            \" This is an auto pipeline block that works for inpaint/text2image/img2img tasks. It also works with controlnet\\n\"\n+            + \" - `QwenImageControlNetAutoDenoiseStep` (controlnet) is used when `control_image_latents` is provided.\\n\"\n+            + \" - `QwenImageInpaintDenoiseStep` (inpaint) is used when `mask` is provided and `control_image_latents` is not provided.\\n\"\n+            + \" - `QwenImageDenoiseStep` (text2image/img2img) is used when `mask` is not provided and `control_image_latents` is not provided.\\n\"\n+        )\n+\n+\n+## 1.9 QwenImage/auto decode\n+# auto decode step for inpaint and text2image tasks\n+\n+\n+class QwenImageAutoDecodeStep(AutoPipelineBlocks):\n+    block_classes = [QwenImageInpaintDecodeStep, QwenImageDecodeStep]\n+    block_names = [\"inpaint_decode\", \"decode\"]\n+    block_trigger_inputs = [\"mask\", None]\n+\n+    @property\n+    def description(self):\n+        return (\n+            \"Decode step that decode the latents into images. \\n\"\n+            \" This is an auto pipeline block that works for inpaint/text2image/img2img tasks, for both QwenImage and QwenImage-Edit.\\n\"\n+            + \" - `QwenImageInpaintDecodeStep` (inpaint) is used when `mask` is provided.\\n\"\n+            + \" - `QwenImageDecodeStep` (text2image/img2img) is used when `mask` is not provided.\\n\"\n+        )\n+\n+\n+## 1.10 QwenImage/auto block & presets\n+AUTO_BLOCKS = InsertableDict(\n+    [\n+        (\"text_encoder\", QwenImageTextEncoderStep()),\n+        (\"vae_encoder\", QwenImageAutoVaeEncoderStep()),\n+        (\"controlnet_vae_encoder\", QwenImageOptionalControlNetVaeEncoderStep()),\n+        (\"input\", QwenImageAutoInputStep()),\n+        (\"controlnet_input\", QwenImageOptionalControlNetInputStep()),\n+        (\"before_denoise\", QwenImageAutoBeforeDenoiseStep()),\n+        (\"controlnet_before_denoise\", QwenImageOptionalControlNetBeforeDenoiseStep()),\n+        (\"denoise\", QwenImageAutoDenoiseStep()),\n+        (\"decode\", QwenImageAutoDecodeStep()),\n+    ]\n+)\n+\n+\n+class QwenImageAutoBlocks(SequentialPipelineBlocks):\n+    model_name = \"qwenimage\"\n+\n+    block_classes = AUTO_BLOCKS.values()\n+    block_names = AUTO_BLOCKS.keys()\n+\n+    @property\n+    def description(self):\n+        return (\n+            \"Auto Modular pipeline for text-to-image, image-to-image, inpainting, and controlnet tasks using QwenImage.\\n\"\n+            + \"- for image-to-image generation, you need to provide `image`\\n\"\n+            + \"- for inpainting, you need to provide `mask_image` and `image`, optionally you can provide `padding_mask_crop` \\n\"\n+            + \"- to run the controlnet workflow, you need to provide `control_image`\\n\"\n+            + \"- for text-to-image generation, all you need to provide is `prompt`\"\n+        )\n+\n+\n+# 2. QwenImage-Edit\n+\n+## 2.1 QwenImage-Edit/edit\n+\n+#### QwenImage-Edit/edit vl encoder: take both image and text prompts\n+QwenImageEditVLEncoderBlocks = InsertableDict(\n+    [\n+        (\"resize\", QwenImageEditResizeDynamicStep()),\n+        (\"encode\", QwenImageEditTextEncoderStep()),\n+    ]\n+)\n+\n+\n+class QwenImageEditVLEncoderStep(SequentialPipelineBlocks):\n+    model_name = \"qwenimage\"\n+    block_classes = QwenImageEditVLEncoderBlocks.values()\n+    block_names = QwenImageEditVLEncoderBlocks.keys()\n+\n+    @property\n+    def description(self) -> str:\n+        return \"QwenImage-Edit VL encoder step that encode the image an text prompts together.\"\n+\n+\n+#### QwenImage-Edit/edit vae encoder\n+QwenImageEditVaeEncoderBlocks = InsertableDict(\n+    [\n+        (\"resize\", QwenImageEditResizeDynamicStep()),  # edit has a different resize step\n+        (\"preprocess\", QwenImageProcessImagesInputStep()),  # resized_image -> processed_image\n+        (\"encode\", QwenImageVaeEncoderDynamicStep()),  # processed_image -> image_latents\n+    ]\n+)\n+\n+\n+class QwenImageEditVaeEncoderStep(SequentialPipelineBlocks):\n+    model_name = \"qwenimage\"\n+    block_classes = QwenImageEditVaeEncoderBlocks.values()\n+    block_names = QwenImageEditVaeEncoderBlocks.keys()\n+\n+    @property\n+    def description(self) -> str:\n+        return \"Vae encoder step that encode the image inputs into their latent representations.\"\n+\n+\n+#### QwenImage-Edit/edit input\n+QwenImageEditInputBlocks = InsertableDict(\n+    [\n+        (\"text_inputs\", QwenImageTextInputsStep()),  # default step to process text embeddings\n+        (\"additional_inputs\", QwenImageInputsDynamicStep(image_latent_inputs=[\"image_latents\"])),\n+    ]\n+)\n+\n+\n+class QwenImageEditInputStep(SequentialPipelineBlocks):\n+    model_name = \"qwenimage\"\n+    block_classes = QwenImageEditInputBlocks.values()\n+    block_names = QwenImageEditInputBlocks.keys()\n+\n+    @property\n+    def description(self):\n+        return \"Input step that prepares the inputs for the edit denoising step. It:\\n\"\n+        \" - make sure the text embeddings have consistent batch size as well as the additional inputs: \\n\"\n+        \" - `image_latents`.\\n\"\n+        \" - update height/width based `image_latents`, patchify `image_latents`.\"\n+\n+\n+#### QwenImage/edit presets\n+EDIT_BLOCKS = InsertableDict(\n+    [\n+        (\"text_encoder\", QwenImageEditVLEncoderStep()),\n+        (\"vae_encoder\", QwenImageEditVaeEncoderStep()),\n+        (\"input\", QwenImageEditInputStep()),\n+        (\"prepare_latents\", QwenImagePrepareLatentsStep()),\n+        (\"set_timesteps\", QwenImageSetTimestepsStep()),\n+        (\"prepare_rope_inputs\", QwenImageEditRoPEInputsStep()),\n+        (\"denoise\", QwenImageEditDenoiseStep()),\n+        (\"decode\", QwenImageDecodeStep()),\n+    ]\n+)\n+\n+\n+## 2.2 QwenImage-Edit/edit inpaint\n+\n+#### QwenImage-Edit/edit inpaint vae encoder: the difference from regular inpaint is the resize step\n+QwenImageEditInpaintVaeEncoderBlocks = InsertableDict(\n+    [\n+        (\"resize\", QwenImageEditResizeDynamicStep()),  # image -> resized_image\n+        (\n+            \"preprocess\",\n+            QwenImageInpaintProcessImagesInputStep,\n+        ),  # resized_image, mask_image -> processed_image, processed_mask_image, mask_overlay_kwargs\n+        (\n+            \"encode\",\n+            QwenImageVaeEncoderDynamicStep(input_name=\"processed_image\", output_name=\"image_latents\"),\n+        ),  # processed_image -> image_latents\n+    ]\n+)\n+\n+\n+class QwenImageEditInpaintVaeEncoderStep(SequentialPipelineBlocks):\n+    model_name = \"qwenimage\"\n+    block_classes = QwenImageEditInpaintVaeEncoderBlocks.values()\n+    block_names = QwenImageEditInpaintVaeEncoderBlocks.keys()\n+\n+    @property\n+    def description(self) -> str:\n+        return (\n+            \"This step is used for processing image and mask inputs for QwenImage-Edit inpaint tasks. It:\\n\"\n+            \" - resize the image for target area (1024 * 1024) while maintaining the aspect ratio.\\n\"\n+            \" - process the resized image and mask image.\\n\"\n+            \" - create image latents.\"\n+        )\n+\n+\n+#### QwenImage-Edit/edit inpaint presets\n+EDIT_INPAINT_BLOCKS = InsertableDict(\n+    [\n+        (\"text_encoder\", QwenImageEditVLEncoderStep()),\n+        (\"vae_encoder\", QwenImageEditInpaintVaeEncoderStep()),\n+        (\"input\", QwenImageInpaintInputStep()),\n+        (\"prepare_latents\", QwenImagePrepareLatentsStep()),\n+        (\"set_timesteps\", QwenImageSetTimestepsWithStrengthStep()),\n+        (\"prepare_inpaint_latents\", QwenImageInpaintPrepareLatentsStep()),\n+        (\"prepare_rope_inputs\", QwenImageEditRoPEInputsStep()),\n+        (\"denoise\", QwenImageEditInpaintDenoiseStep()),\n+        (\"decode\", QwenImageInpaintDecodeStep()),\n+    ]\n+)\n+\n+\n+## 2.3 QwenImage-Edit/auto encoders\n+\n+\n+class QwenImageEditAutoVaeEncoderStep(AutoPipelineBlocks):\n+    block_classes = [\n+        QwenImageEditInpaintVaeEncoderStep,\n+        QwenImageEditVaeEncoderStep,\n+    ]\n+    block_names = [\"edit_inpaint\", \"edit\"]\n+    block_trigger_inputs = [\"mask_image\", \"image\"]\n+\n+    @property\n+    def description(self):\n+        return (\n+            \"Vae encoder step that encode the image inputs into their latent representations. \\n\"\n+            \" This is an auto pipeline block that works for edit and edit_inpaint tasks.\\n\"\n+            + \" - `QwenImageEditInpaintVaeEncoderStep` (edit_inpaint) is used when `mask_image` is provided.\\n\"\n+            + \" - `QwenImageEditVaeEncoderStep` (edit) is used when `image` is provided.\\n\"\n+            + \" - if `mask_image` or `image` is not provided, step will be skipped.\"\n+        )\n+\n+\n+## 2.4 QwenImage-Edit/auto inputs\n+class QwenImageEditAutoInputStep(AutoPipelineBlocks):\n+    block_classes = [QwenImageInpaintInputStep, QwenImageEditInputStep]\n+    block_names = [\"edit_inpaint\", \"edit\"]\n+    block_trigger_inputs = [\"processed_mask_image\", \"image\"]\n+\n+    @property\n+    def description(self):\n+        return (\n+            \"Input step that prepares the inputs for the edit denoising step.\\n\"\n+            + \" It is an auto pipeline block that works for edit and edit_inpaint tasks.\\n\"\n+            + \" - `QwenImageInpaintInputStep` (edit_inpaint) is used when `processed_mask_image` is provided.\\n\"\n+            + \" - `QwenImageEditInputStep` (edit) is used when `image_latents` is provided.\\n\"\n+            + \" - if `processed_mask_image` or `image_latents` is not provided, step will be skipped.\"\n+        )\n+\n+\n+## 2.5 QwenImage-Edit/auto before denoise\n+# compose the steps into a BeforeDenoiseStep for edit and edit_inpaint tasks before combine into an auto step\n+\n+#### QwenImage-Edit/edit before denoise\n+QwenImageEditBeforeDenoiseBlocks = InsertableDict(\n+    [\n+        (\"prepare_latents\", QwenImagePrepareLatentsStep()),\n+        (\"set_timesteps\", QwenImageSetTimestepsStep()),\n+        (\"prepare_rope_inputs\", QwenImageEditRoPEInputsStep()),\n+    ]\n+)\n+\n+\n+class QwenImageEditBeforeDenoiseStep(SequentialPipelineBlocks):\n+    model_name = \"qwenimage\"\n+    block_classes = QwenImageEditBeforeDenoiseBlocks.values()\n+    block_names = QwenImageEditBeforeDenoiseBlocks.keys()\n+\n+    @property\n+    def description(self):\n+        return \"Before denoise step that prepare the inputs (timesteps, latents, rope inputs etc.) for the denoise step for edit task.\"\n+\n+\n+#### QwenImage-Edit/edit inpaint before denoise\n+QwenImageEditInpaintBeforeDenoiseBlocks = InsertableDict(\n+    [\n+        (\"prepare_latents\", QwenImagePrepareLatentsStep()),\n+        (\"set_timesteps\", QwenImageSetTimestepsWithStrengthStep()),\n+        (\"prepare_inpaint_latents\", QwenImageInpaintPrepareLatentsStep()),\n+        (\"prepare_rope_inputs\", QwenImageEditRoPEInputsStep()),\n+    ]\n+)\n+\n+\n+class QwenImageEditInpaintBeforeDenoiseStep(SequentialPipelineBlocks):\n+    model_name = \"qwenimage\"\n+    block_classes = QwenImageEditInpaintBeforeDenoiseBlocks.values()\n+    block_names = QwenImageEditInpaintBeforeDenoiseBlocks.keys()\n+\n+    @property\n+    def description(self):\n+        return \"Before denoise step that prepare the inputs (timesteps, latents, rope inputs etc.) for the denoise step for edit inpaint task.\"\n+\n+\n+# auto before_denoise step for edit and edit_inpaint tasks\n+class QwenImageEditAutoBeforeDenoiseStep(AutoPipelineBlocks):\n+    model_name = \"qwenimage-edit\"\n+    block_classes = [\n+        QwenImageEditInpaintBeforeDenoiseStep,\n+        QwenImageEditBeforeDenoiseStep,\n+    ]\n+    block_names = [\"edit_inpaint\", \"edit\"]\n+    block_trigger_inputs = [\"processed_mask_image\", \"image_latents\"]\n+\n+    @property\n+    def description(self):\n+        return (\n+            \"Before denoise step that prepare the inputs (timesteps, latents, rope inputs etc.) for the denoise step.\\n\"\n+            + \"This is an auto pipeline block that works for edit (img2img) and edit inpaint tasks.\\n\"\n+            + \" - `QwenImageEditInpaintBeforeDenoiseStep` (edit_inpaint) is used when `processed_mask_image` is provided.\\n\"\n+            + \" - `QwenImageEditBeforeDenoiseStep` (edit) is used when `image_latents` is provided and `processed_mask_image` is not provided.\\n\"\n+            + \" - if `image_latents` or `processed_mask_image` is not provided, step will be skipped.\"\n+        )\n+\n+\n+## 2.6 QwenImage-Edit/auto denoise\n+\n+\n+class QwenImageEditAutoDenoiseStep(AutoPipelineBlocks):\n+    model_name = \"qwenimage-edit\"\n+\n+    block_classes = [QwenImageEditInpaintDenoiseStep, QwenImageEditDenoiseStep]\n+    block_names = [\"inpaint_denoise\", \"denoise\"]\n+    block_trigger_inputs = [\"processed_mask_image\", \"image_latents\"]\n+\n+    @property\n+    def description(self):\n+        return (\n+            \"Denoise step that iteratively denoise the latents. \\n\"\n+            + \"This block supports edit (img2img) and edit inpaint tasks for QwenImage Edit. \\n\"\n+            + \" - `QwenImageEditInpaintDenoiseStep` (inpaint) is used when `processed_mask_image` is provided.\\n\"\n+            + \" - `QwenImageEditDenoiseStep` (img2img) is used when `image_latents` is provided.\\n\"\n+            + \" - if `processed_mask_image` or `image_latents` is not provided, step will be skipped.\"\n+        )\n+\n+\n+## 2.7 QwenImage-Edit/auto blocks & presets\n+\n+EDIT_AUTO_BLOCKS = InsertableDict(\n+    [\n+        (\"text_encoder\", QwenImageEditVLEncoderStep()),\n+        (\"vae_encoder\", QwenImageEditAutoVaeEncoderStep()),\n+        (\"input\", QwenImageEditAutoInputStep()),\n+        (\"before_denoise\", QwenImageEditAutoBeforeDenoiseStep()),\n+        (\"denoise\", QwenImageEditAutoDenoiseStep()),\n+        (\"decode\", QwenImageAutoDecodeStep()),\n+    ]\n+)\n+\n+\n+class QwenImageEditAutoBlocks(SequentialPipelineBlocks):\n+    model_name = \"qwenimage-edit\"\n+    block_classes = EDIT_AUTO_BLOCKS.values()\n+    block_names = EDIT_AUTO_BLOCKS.keys()\n+\n+    @property\n+    def description(self):\n+        return (\n+            \"Auto Modular pipeline for edit (img2img) and edit inpaint tasks using QwenImage-Edit.\\n\"\n+            + \"- for edit (img2img) generation, you need to provide `image`\\n\"\n+            + \"- for edit inpainting, you need to provide `mask_image` and `image`, optionally you can provide `padding_mask_crop` \\n\"\n+        )\n+\n+\n+# 3. all block presets supported in QwenImage & QwenImage-Edit\n+\n+\n+ALL_BLOCKS = {\n+    \"text2image\": TEXT2IMAGE_BLOCKS,\n+    \"img2img\": IMAGE2IMAGE_BLOCKS,\n+    \"edit\": EDIT_BLOCKS,\n+    \"edit_inpaint\": EDIT_INPAINT_BLOCKS,\n+    \"inpaint\": INPAINT_BLOCKS,\n+    \"controlnet\": CONTROLNET_BLOCKS,\n+    \"auto\": AUTO_BLOCKS,\n+    \"edit_auto\": EDIT_AUTO_BLOCKS,\n+}"
        },
        {
          "filename": "src/diffusers/modular_pipelines/qwenimage/modular_pipeline.py",
          "status": "added",
          "additions": 202,
          "deletions": 0,
          "changes": 202,
          "patch": "@@ -0,0 +1,202 @@\n+# Copyright 2025 Qwen-Image Team and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\n+from ...configuration_utils import ConfigMixin, register_to_config\n+from ...loaders import QwenImageLoraLoaderMixin\n+from ..modular_pipeline import ModularPipeline\n+\n+\n+class QwenImagePachifier(ConfigMixin):\n+    \"\"\"\n+    A class to pack and unpack latents for QwenImage.\n+    \"\"\"\n+\n+    config_name = \"config.json\"\n+\n+    @register_to_config\n+    def __init__(\n+        self,\n+        patch_size: int = 2,\n+    ):\n+        super().__init__()\n+\n+    def pack_latents(self, latents):\n+        if latents.ndim != 4 and latents.ndim != 5:\n+            raise ValueError(f\"Latents must have 4 or 5 dimensions, but got {latents.ndim}\")\n+\n+        if latents.ndim == 4:\n+            latents = latents.unsqueeze(2)\n+\n+        batch_size, num_channels_latents, num_latent_frames, latent_height, latent_width = latents.shape\n+        patch_size = self.config.patch_size\n+\n+        if latent_height % patch_size != 0 or latent_width % patch_size != 0:\n+            raise ValueError(\n+                f\"Latent height and width must be divisible by {patch_size}, but got {latent_height} and {latent_width}\"\n+            )\n+\n+        latents = latents.view(\n+            batch_size,\n+            num_channels_latents,\n+            latent_height // patch_size,\n+            patch_size,\n+            latent_width // patch_size,\n+            patch_size,\n+        )\n+        latents = latents.permute(\n+            0, 2, 4, 1, 3, 5\n+        )  # Batch_size, num_patches_height, num_patches_width, num_channels_latents, patch_size, patch_size\n+        latents = latents.reshape(\n+            batch_size,\n+            (latent_height // patch_size) * (latent_width // patch_size),\n+            num_channels_latents * patch_size * patch_size,\n+        )\n+\n+        return latents\n+\n+    def unpack_latents(self, latents, height, width, vae_scale_factor=8):\n+        if latents.ndim != 3:\n+            raise ValueError(f\"Latents must have 3 dimensions, but got {latents.ndim}\")\n+\n+        batch_size, num_patches, channels = latents.shape\n+        patch_size = self.config.patch_size\n+\n+        # VAE applies 8x compression on images but we must also account for packing which requires\n+        # latent height and width to be divisible by 2.\n+        height = patch_size * (int(height) // (vae_scale_factor * patch_size))\n+        width = patch_size * (int(width) // (vae_scale_factor * patch_size))\n+\n+        latents = latents.view(\n+            batch_size,\n+            height // patch_size,\n+            width // patch_size,\n+            channels // (patch_size * patch_size),\n+            patch_size,\n+            patch_size,\n+        )\n+        latents = latents.permute(0, 3, 1, 4, 2, 5)\n+\n+        latents = latents.reshape(batch_size, channels // (patch_size * patch_size), 1, height, width)\n+\n+        return latents\n+\n+\n+class QwenImageModularPipeline(ModularPipeline, QwenImageLoraLoaderMixin):\n+    \"\"\"\n+    A ModularPipeline for QwenImage.\n+\n+    <Tip warning={true}>\n+\n+        This is an experimental feature and is likely to change in the future.\n+\n+    </Tip>\n+    \"\"\"\n+\n+    @property\n+    def default_height(self):\n+        return self.default_sample_size * self.vae_scale_factor\n+\n+    @property\n+    def default_width(self):\n+        return self.default_sample_size * self.vae_scale_factor\n+\n+    @property\n+    def default_sample_size(self):\n+        return 128\n+\n+    @property\n+    def vae_scale_factor(self):\n+        vae_scale_factor = 8\n+        if hasattr(self, \"vae\") and self.vae is not None:\n+            vae_scale_factor = 2 ** len(self.vae.temperal_downsample)\n+        return vae_scale_factor\n+\n+    @property\n+    def num_channels_latents(self):\n+        num_channels_latents = 16\n+        if hasattr(self, \"transformer\") and self.transformer is not None:\n+            num_channels_latents = self.transformer.config.in_channels // 4\n+        return num_channels_latents\n+\n+    @property\n+    def is_guidance_distilled(self):\n+        is_guidance_distilled = False\n+        if hasattr(self, \"transformer\") and self.transformer is not None:\n+            is_guidance_distilled = self.transformer.config.guidance_embeds\n+        return is_guidance_distilled\n+\n+    @property\n+    def requires_unconditional_embeds(self):\n+        requires_unconditional_embeds = False\n+\n+        if hasattr(self, \"guider\") and self.guider is not None:\n+            requires_unconditional_embeds = self.guider._enabled and self.guider.num_conditions > 1\n+\n+        return requires_unconditional_embeds\n+\n+\n+class QwenImageEditModularPipeline(ModularPipeline, QwenImageLoraLoaderMixin):\n+    \"\"\"\n+    A ModularPipeline for QwenImage-Edit.\n+\n+    <Tip warning={true}>\n+\n+        This is an experimental feature and is likely to change in the future.\n+\n+    </Tip>\n+    \"\"\"\n+\n+    # YiYi TODO: qwen edit should not provide default height/width, should be derived from the resized input image (after adjustment) produced by the resize step.\n+    @property\n+    def default_height(self):\n+        return self.default_sample_size * self.vae_scale_factor\n+\n+    @property\n+    def default_width(self):\n+        return self.default_sample_size * self.vae_scale_factor\n+\n+    @property\n+    def default_sample_size(self):\n+        return 128\n+\n+    @property\n+    def vae_scale_factor(self):\n+        vae_scale_factor = 8\n+        if hasattr(self, \"vae\") and self.vae is not None:\n+            vae_scale_factor = 2 ** len(self.vae.temperal_downsample)\n+        return vae_scale_factor\n+\n+    @property\n+    def num_channels_latents(self):\n+        num_channels_latents = 16\n+        if hasattr(self, \"transformer\") and self.transformer is not None:\n+            num_channels_latents = self.transformer.config.in_channels // 4\n+        return num_channels_latents\n+\n+    @property\n+    def is_guidance_distilled(self):\n+        is_guidance_distilled = False\n+        if hasattr(self, \"transformer\") and self.transformer is not None:\n+            is_guidance_distilled = self.transformer.config.guidance_embeds\n+        return is_guidance_distilled\n+\n+    @property\n+    def requires_unconditional_embeds(self):\n+        requires_unconditional_embeds = False\n+\n+        if hasattr(self, \"guider\") and self.guider is not None:\n+            requires_unconditional_embeds = self.guider._enabled and self.guider.num_conditions > 1\n+\n+        return requires_unconditional_embeds"
        },
        {
          "filename": "src/diffusers/modular_pipelines/stable_diffusion_xl/modular_pipeline.py",
          "status": "modified",
          "additions": 1,
          "deletions": 0,
          "changes": 1,
          "patch": "@@ -76,6 +76,7 @@ def vae_scale_factor(self):\n             vae_scale_factor = 2 ** (len(self.vae.config.block_out_channels) - 1)\n         return vae_scale_factor\n \n+    # YiYi TODO: change to num_channels_latents\n     @property\n     def num_channels_unet(self):\n         num_channels_unet = 4"
        },
        {
          "filename": "src/diffusers/pipelines/auto_pipeline.py",
          "status": "modified",
          "additions": 14,
          "deletions": 0,
          "changes": 14,
          "patch": "@@ -91,6 +91,14 @@\n     StableDiffusionXLPAGPipeline,\n )\n from .pixart_alpha import PixArtAlphaPipeline, PixArtSigmaPipeline\n+from .qwenimage import (\n+    QwenImageControlNetPipeline,\n+    QwenImageEditInpaintPipeline,\n+    QwenImageEditPipeline,\n+    QwenImageImg2ImgPipeline,\n+    QwenImageInpaintPipeline,\n+    QwenImagePipeline,\n+)\n from .sana import SanaPipeline\n from .stable_cascade import StableCascadeCombinedPipeline, StableCascadeDecoderPipeline\n from .stable_diffusion import (\n@@ -150,6 +158,8 @@\n         (\"cogview3\", CogView3PlusPipeline),\n         (\"cogview4\", CogView4Pipeline),\n         (\"cogview4-control\", CogView4ControlPipeline),\n+        (\"qwenimage\", QwenImagePipeline),\n+        (\"qwenimage-controlnet\", QwenImageControlNetPipeline),\n     ]\n )\n \n@@ -174,6 +184,8 @@\n         (\"flux-controlnet\", FluxControlNetImg2ImgPipeline),\n         (\"flux-control\", FluxControlImg2ImgPipeline),\n         (\"flux-kontext\", FluxKontextPipeline),\n+        (\"qwenimage\", QwenImageImg2ImgPipeline),\n+        (\"qwenimage-edit\", QwenImageEditPipeline),\n     ]\n )\n \n@@ -195,6 +207,8 @@\n         (\"flux-controlnet\", FluxControlNetInpaintPipeline),\n         (\"flux-control\", FluxControlInpaintPipeline),\n         (\"stable-diffusion-pag\", StableDiffusionPAGInpaintPipeline),\n+        (\"qwenimage\", QwenImageInpaintPipeline),\n+        (\"qwenimage-edit\", QwenImageEditInpaintPipeline),\n     ]\n )\n "
        },
        {
          "filename": "src/diffusers/utils/dummy_torch_and_transformers_objects.py",
          "status": "modified",
          "additions": 60,
          "deletions": 0,
          "changes": 60,
          "patch": "@@ -32,6 +32,66 @@ def from_pretrained(cls, *args, **kwargs):\n         requires_backends(cls, [\"torch\", \"transformers\"])\n \n \n+class QwenImageAutoBlocks(metaclass=DummyObject):\n+    _backends = [\"torch\", \"transformers\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_config(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_pretrained(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+\n+class QwenImageEditAutoBlocks(metaclass=DummyObject):\n+    _backends = [\"torch\", \"transformers\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_config(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_pretrained(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+\n+class QwenImageEditModularPipeline(metaclass=DummyObject):\n+    _backends = [\"torch\", \"transformers\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_config(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_pretrained(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+\n+class QwenImageModularPipeline(metaclass=DummyObject):\n+    _backends = [\"torch\", \"transformers\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_config(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_pretrained(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+\n class StableDiffusionXLAutoBlocks(metaclass=DummyObject):\n     _backends = [\"torch\", \"transformers\"]\n "
        }
      ],
      "num_files": 17,
      "scraped_at": "2025-11-16T21:19:44.723263"
    },
    {
      "pr_number": 12217,
      "title": "[Modular] Consolidate `load_default_components` into `load_components`",
      "body": "# What does this PR do?\r\n`load_default_components` is a thin wrapper around `load_components` and just deals with the case when `name=None`. IMO better to just consolidate it into `load_components`\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes # (issue)\r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [ ] Did you read the [contributor guideline](https://github.com/huggingface/diffusers/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you read our [philosophy doc](https://github.com/huggingface/diffusers/blob/main/PHILOSOPHY.md) (important for complex PRs)?\r\n- [ ] Was this discussed/approved via a GitHub issue or the [forum](https://discuss.huggingface.co/c/discussion-related-to-httpsgithubcomhuggingfacediffusers/63)? Please add a link to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/diffusers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/diffusers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @.\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nCore library:\r\n\r\n- Schedulers: @yiyixuxu\r\n- Pipelines and pipeline callbacks: @yiyixuxu and @asomoza\r\n- Training examples: @sayakpaul\r\n- Docs: @stevhliu and @sayakpaul\r\n- JAX and MPS: @pcuenca\r\n- Audio: @sanchit-gandhi\r\n- General functionalities: @sayakpaul @yiyixuxu @DN6\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @SunMarc\r\n- PEFT: @sayakpaul @BenjaminBossan\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- transformers: [different repo](https://github.com/huggingface/transformers)\r\n- safetensors: [different repo](https://github.com/huggingface/safetensors)\r\n\r\n-->\r\n",
      "html_url": "https://github.com/huggingface/diffusers/pull/12217",
      "created_at": "2025-08-22T13:08:42Z",
      "merged_at": "2025-08-28T14:25:02Z",
      "merge_commit_sha": "ba0e732eb059b9eb3afe4b643be471d93154d1cc",
      "base_ref": "main",
      "head_sha": "e30dc5176c17330455e5e6e80ca39fc6c7b96e45",
      "user": "DN6",
      "files": [
        {
          "filename": "docs/source/en/modular_diffusers/components_manager.md",
          "status": "modified",
          "additions": 3,
          "deletions": 3,
          "changes": 6,
          "patch": "@@ -51,10 +51,10 @@ t2i_pipeline = t2i_blocks.init_pipeline(modular_repo_id, components_manager=comp\n </hfoption>\n </hfoptions>\n \n-Components are only loaded and registered when using [`~ModularPipeline.load_components`] or [`~ModularPipeline.load_default_components`]. The example below uses [`~ModularPipeline.load_default_components`] to create a second pipeline that reuses all the components from the first one, and assigns it to a different collection\n+Components are only loaded and registered when using [`~ModularPipeline.load_components`] or [`~ModularPipeline.load_components`]. The example below uses [`~ModularPipeline.load_components`] to create a second pipeline that reuses all the components from the first one, and assigns it to a different collection\n \n ```py\n-pipe.load_default_components()\n+pipe.load_components()\n pipe2 = ModularPipeline.from_pretrained(\"YiYiXu/modular-demo-auto\", components_manager=comp, collection=\"test2\")\n ```\n \n@@ -187,4 +187,4 @@ comp.enable_auto_cpu_offload(device=\"cuda\")\n \n All models begin on the CPU and [`ComponentsManager`] moves them to the appropriate device right before they're needed, and moves other models back to the CPU when GPU memory is low.\n \n-You can set your own rules for which models to offload first.\n\\ No newline at end of file\n+You can set your own rules for which models to offload first."
        },
        {
          "filename": "docs/source/en/modular_diffusers/guiders.md",
          "status": "modified",
          "additions": 3,
          "deletions": 3,
          "changes": 6,
          "patch": "@@ -75,13 +75,13 @@ Guiders that are already saved on the Hub with a `modular_model_index.json` file\n }\n ```\n \n-The guider is only created after calling [`~ModularPipeline.load_default_components`] based on the loading specification in `modular_model_index.json`.\n+The guider is only created after calling [`~ModularPipeline.load_components`] based on the loading specification in `modular_model_index.json`.\n \n ```py\n t2i_pipeline = t2i_blocks.init_pipeline(\"YiYiXu/modular-doc-guider\")\n # not created during init\n assert t2i_pipeline.guider is None\n-t2i_pipeline.load_default_components()\n+t2i_pipeline.load_components()\n # loaded as PAG guider\n t2i_pipeline.guider\n ```\n@@ -172,4 +172,4 @@ t2i_pipeline.push_to_hub(\"YiYiXu/modular-doc-guider\")\n ```\n \n </hfoption>\n-</hfoptions>\n\\ No newline at end of file\n+</hfoptions>"
        },
        {
          "filename": "docs/source/en/modular_diffusers/modular_pipeline.md",
          "status": "modified",
          "additions": 7,
          "deletions": 7,
          "changes": 14,
          "patch": "@@ -29,7 +29,7 @@ blocks = SequentialPipelineBlocks.from_blocks_dict(TEXT2IMAGE_BLOCKS)\n modular_repo_id = \"YiYiXu/modular-loader-t2i-0704\"\n pipeline = blocks.init_pipeline(modular_repo_id)\n \n-pipeline.load_default_components(torch_dtype=torch.float16)\n+pipeline.load_components(torch_dtype=torch.float16)\n pipeline.to(\"cuda\")\n \n image = pipeline(prompt=\"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\", output=\"images\")[0]\n@@ -49,7 +49,7 @@ blocks = SequentialPipelineBlocks.from_blocks_dict(IMAGE2IMAGE_BLOCKS)\n modular_repo_id = \"YiYiXu/modular-loader-t2i-0704\"\n pipeline = blocks.init_pipeline(modular_repo_id)\n \n-pipeline.load_default_components(torch_dtype=torch.float16)\n+pipeline.load_components(torch_dtype=torch.float16)\n pipeline.to(\"cuda\")\n \n url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/sdxl-text2img.png\"\n@@ -73,7 +73,7 @@ blocks = SequentialPipelineBlocks.from_blocks_dict(INPAINT_BLOCKS)\n modular_repo_id = \"YiYiXu/modular-loader-t2i-0704\"\n pipeline = blocks.init_pipeline(modular_repo_id)\n \n-pipeline.load_default_components(torch_dtype=torch.float16)\n+pipeline.load_components(torch_dtype=torch.float16)\n pipeline.to(\"cuda\")\n \n img_url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/sdxl-text2img.png\"\n@@ -176,15 +176,15 @@ diffdiff_pipeline = ModularPipeline.from_pretrained(modular_repo_id, trust_remot\n \n ## Loading components\n \n-A [`ModularPipeline`] doesn't automatically instantiate with components. It only loads the configuration and component specifications. You can load all components with [`~ModularPipeline.load_default_components`] or only load specific components with [`~ModularPipeline.load_components`].\n+A [`ModularPipeline`] doesn't automatically instantiate with components. It only loads the configuration and component specifications. You can load all components with [`~ModularPipeline.load_components`] or only load specific components with [`~ModularPipeline.load_components`].\n \n <hfoptions id=\"load\">\n-<hfoption id=\"load_default_components\">\n+<hfoption id=\"load_components\">\n \n ```py\n import torch\n \n-t2i_pipeline.load_default_components(torch_dtype=torch.float16)\n+t2i_pipeline.load_components(torch_dtype=torch.float16)\n t2i_pipeline.to(\"cuda\")\n ```\n \n@@ -355,4 +355,4 @@ The [config.json](https://huggingface.co/YiYiXu/modular-diffdiff-0704/blob/main/\n     \"ModularPipelineBlocks\": \"block.DiffDiffBlocks\"\n   }\n }\n-```\n\\ No newline at end of file\n+```"
        },
        {
          "filename": "docs/source/en/modular_diffusers/quickstart.md",
          "status": "modified",
          "additions": 9,
          "deletions": 9,
          "changes": 18,
          "patch": "@@ -173,9 +173,9 @@ print(dd_blocks)\n \n ## ModularPipeline\n \n-Convert the [`SequentialPipelineBlocks`] into a [`ModularPipeline`] with the [`ModularPipeline.init_pipeline`] method. This initializes the expected components to load from a `modular_model_index.json` file. Explicitly load the components by calling [`ModularPipeline.load_default_components`].\n+Convert the [`SequentialPipelineBlocks`] into a [`ModularPipeline`] with the [`ModularPipeline.init_pipeline`] method. This initializes the expected components to load from a `modular_model_index.json` file. Explicitly load the components by calling [`ModularPipeline.load_components`].\n \n-It is a good idea to initialize the [`ComponentManager`] with the pipeline to help manage the different components. Once you call [`~ModularPipeline.load_default_components`], the components are registered to the [`ComponentManager`] and can be shared between workflows. The example below uses the `collection` argument to assign the components a `\"diffdiff\"` label for better organization.\n+It is a good idea to initialize the [`ComponentManager`] with the pipeline to help manage the different components. Once you call [`~ModularPipeline.load_components`], the components are registered to the [`ComponentManager`] and can be shared between workflows. The example below uses the `collection` argument to assign the components a `\"diffdiff\"` label for better organization.\n \n ```py\n from diffusers.modular_pipelines import ComponentsManager\n@@ -209,11 +209,11 @@ Use the [`sub_blocks.insert`] method to insert it into the [`ModularPipeline`].\n dd_blocks.sub_blocks.insert(\"ip_adapter\", ip_adapter_block, 0)\n ```\n \n-Call [`~ModularPipeline.init_pipeline`] to initialize a [`ModularPipeline`] and use [`~ModularPipeline.load_default_components`] to load the model components. Load and set the IP-Adapter to run the pipeline.\n+Call [`~ModularPipeline.init_pipeline`] to initialize a [`ModularPipeline`] and use [`~ModularPipeline.load_components`] to load the model components. Load and set the IP-Adapter to run the pipeline.\n \n ```py\n dd_pipeline = dd_blocks.init_pipeline(\"YiYiXu/modular-demo-auto\", collection=\"diffdiff\")\n-dd_pipeline.load_default_components(torch_dtype=torch.float16)\n+dd_pipeline.load_components(torch_dtype=torch.float16)\n dd_pipeline.loader.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"sdxl_models\", weight_name=\"ip-adapter_sdxl.bin\")\n dd_pipeline.loader.set_ip_adapter_scale(0.6)\n dd_pipeline = dd_pipeline.to(device)\n@@ -260,14 +260,14 @@ class SDXLDiffDiffControlNetDenoiseStep(StableDiffusionXLDenoiseLoopWrapper):\n controlnet_denoise_block = SDXLDiffDiffControlNetDenoiseStep()\n ```\n \n-Insert the `controlnet_input` block and replace the `denoise` block with the new `controlnet_denoise_block`. Initialize a [`ModularPipeline`] and [`~ModularPipeline.load_default_components`] into it.\n+Insert the `controlnet_input` block and replace the `denoise` block with the new `controlnet_denoise_block`. Initialize a [`ModularPipeline`] and [`~ModularPipeline.load_components`] into it.\n \n ```py\n dd_blocks.sub_blocks.insert(\"controlnet_input\", control_input_block, 7)\n dd_blocks.sub_blocks[\"denoise\"] = controlnet_denoise_block\n \n dd_pipeline = dd_blocks.init_pipeline(\"YiYiXu/modular-demo-auto\", collection=\"diffdiff\")\n-dd_pipeline.load_default_components(torch_dtype=torch.float16)\n+dd_pipeline.load_components(torch_dtype=torch.float16)\n dd_pipeline = dd_pipeline.to(device)\n \n control_image = load_image(\"https://huggingface.co/datasets/YiYiXu/testing-images/resolve/main/diffdiff_tomato_canny.jpeg\")\n@@ -320,7 +320,7 @@ Call [`SequentialPipelineBlocks.from_blocks_dict`] to create a [`SequentialPipel\n ```py\n dd_auto_blocks = SequentialPipelineBlocks.from_blocks_dict(DIFFDIFF_AUTO_BLOCKS)\n dd_pipeline = dd_auto_blocks.init_pipeline(\"YiYiXu/modular-demo-auto\", collection=\"diffdiff\")\n-dd_pipeline.load_default_components(torch_dtype=torch.float16)\n+dd_pipeline.load_components(torch_dtype=torch.float16)\n ```\n \n ## Share\n@@ -340,5 +340,5 @@ from diffusers.modular_pipelines import ModularPipeline, ComponentsManager\n components = ComponentsManager()\n \n diffdiff_pipeline = ModularPipeline.from_pretrained(\"YiYiXu/modular-diffdiff-0704\", trust_remote_code=True, components_manager=components, collection=\"diffdiff\")\n-diffdiff_pipeline.load_default_components(torch_dtype=torch.float16)\n-```\n\\ No newline at end of file\n+diffdiff_pipeline.load_components(torch_dtype=torch.float16)\n+```"
        },
        {
          "filename": "docs/source/zh/modular_diffusers/components_manager.md",
          "status": "modified",
          "additions": 3,
          "deletions": 3,
          "changes": 6,
          "patch": "@@ -48,10 +48,10 @@ t2i_pipeline = t2i_blocks.init_pipeline(modular_repo_id, components_manager=comp\n </hfoption>\n </hfoptions>\n \n-\u7ec4\u4ef6\u4ec5\u5728\u8c03\u7528 [`~ModularPipeline.load_components`] \u6216 [`~ModularPipeline.load_default_components`] \u65f6\u52a0\u8f7d\u548c\u6ce8\u518c\u3002\u4ee5\u4e0b\u793a\u4f8b\u4f7f\u7528 [`~ModularPipeline.load_default_components`] \u521b\u5efa\u7b2c\u4e8c\u4e2a\u7ba1\u9053\uff0c\u91cd\u7528\u7b2c\u4e00\u4e2a\u7ba1\u9053\u7684\u6240\u6709\u7ec4\u4ef6\uff0c\u5e76\u5c06\u5176\u5206\u914d\u5230\u4e0d\u540c\u7684\u96c6\u5408\u3002\n+\u7ec4\u4ef6\u4ec5\u5728\u8c03\u7528 [`~ModularPipeline.load_components`] \u6216 [`~ModularPipeline.load_components`] \u65f6\u52a0\u8f7d\u548c\u6ce8\u518c\u3002\u4ee5\u4e0b\u793a\u4f8b\u4f7f\u7528 [`~ModularPipeline.load_components`] \u521b\u5efa\u7b2c\u4e8c\u4e2a\u7ba1\u9053\uff0c\u91cd\u7528\u7b2c\u4e00\u4e2a\u7ba1\u9053\u7684\u6240\u6709\u7ec4\u4ef6\uff0c\u5e76\u5c06\u5176\u5206\u914d\u5230\u4e0d\u540c\u7684\u96c6\u5408\u3002\n \n ```py\n-pipe.load_default_components()\n+pipe.load_components()\n pipe2 = ModularPipeline.from_pretrained(\"YiYiXu/modular-demo-auto\", components_manager=comp, collection=\"test2\")\n ```\n \n@@ -185,4 +185,4 @@ comp.enable_auto_cpu_offload(device=\"cuda\")\n \n \u6240\u6709\u6a21\u578b\u5f00\u59cb\u65f6\u90fd\u5728 CPU \u4e0a\uff0c[`ComponentsManager`] \u5728\u9700\u8981\u5b83\u4eec\u4e4b\u524d\u5c06\u5b83\u4eec\u79fb\u52a8\u5230\u9002\u5f53\u7684\u8bbe\u5907\uff0c\u5e76\u5728 GPU \u5185\u5b58\u4e0d\u8db3\u65f6\u5c06\u5176\u4ed6\u6a21\u578b\u79fb\u56de CPU\u3002\n \n-\u60a8\u53ef\u4ee5\u8bbe\u7f6e\u81ea\u5df1\u7684\u89c4\u5219\u6765\u51b3\u5b9a\u54ea\u4e9b\u6a21\u578b\u8981\u5378\u8f7d\u3002\n\\ No newline at end of file\n+\u60a8\u53ef\u4ee5\u8bbe\u7f6e\u81ea\u5df1\u7684\u89c4\u5219\u6765\u51b3\u5b9a\u54ea\u4e9b\u6a21\u578b\u8981\u5378\u8f7d\u3002"
        },
        {
          "filename": "docs/source/zh/modular_diffusers/guiders.md",
          "status": "modified",
          "additions": 3,
          "deletions": 3,
          "changes": 6,
          "patch": "@@ -73,13 +73,13 @@ ComponentSpec(name='guider', type_hint=<class 'diffusers.guiders.perturbed_atten\n }\n ```\n \n-\u5f15\u5bfc\u5668\u53ea\u6709\u5728\u8c03\u7528 [`~ModularPipeline.load_default_components`] \u4e4b\u540e\u624d\u4f1a\u521b\u5efa\uff0c\u57fa\u4e8e `modular_model_index.json` \u4e2d\u7684\u52a0\u8f7d\u89c4\u8303\u3002\n+\u5f15\u5bfc\u5668\u53ea\u6709\u5728\u8c03\u7528 [`~ModularPipeline.load_components`] \u4e4b\u540e\u624d\u4f1a\u521b\u5efa\uff0c\u57fa\u4e8e `modular_model_index.json` \u4e2d\u7684\u52a0\u8f7d\u89c4\u8303\u3002\n \n ```py\n t2i_pipeline = t2i_blocks.init_pipeline(\"YiYiXu/modular-doc-guider\")\n # \u5728\u521d\u59cb\u5316\u65f6\u672a\u521b\u5efa\n assert t2i_pipeline.guider is None\n-t2i_pipeline.load_default_components()\n+t2i_pipeline.load_components()\n # \u52a0\u8f7d\u4e3a PAG \u5f15\u5bfc\u5668\n t2i_pipeline.guider\n ```\n@@ -170,4 +170,4 @@ t2i_pipeline.push_to_hub(\"YiYiXu/modular-doc-guider\")\n ```\n \n </hfoption>\n-</hfoptions>\n\\ No newline at end of file\n+</hfoptions>"
        },
        {
          "filename": "docs/source/zh/modular_diffusers/modular_pipeline.md",
          "status": "modified",
          "additions": 6,
          "deletions": 6,
          "changes": 12,
          "patch": "@@ -28,7 +28,7 @@ blocks = SequentialPipelineBlocks.from_blocks_dict(TEXT2IMAGE_BLOCKS)\n modular_repo_id = \"YiYiXu/modular-loader-t2i-0704\"\n pipeline = blocks.init_pipeline(modular_repo_id)\n \n-pipeline.load_default_components(torch_dtype=torch.float16)\n+pipeline.load_components(torch_dtype=torch.float16)\n pipeline.to(\"cuda\")\n \n image = pipeline(prompt=\"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\", output=\"images\")[0]\n@@ -48,7 +48,7 @@ blocks = SequentialPipelineBlocks.from_blocks_dict(IMAGE2IMAGE_BLOCKS)\n modular_repo_id = \"YiYiXu/modular-loader-t2i-0704\"\n pipeline = blocks.init_pipeline(modular_repo_id)\n \n-pipeline.load_default_components(torch_dtype=torch.float16)\n+pipeline.load_components(torch_dtype=torch.float16)\n pipeline.to(\"cuda\")\n \n url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/sdxl-text2img.png\"\n@@ -72,7 +72,7 @@ blocks = SequentialPipelineBlocks.from_blocks_dict(INPAINT_BLOCKS)\n modular_repo_id = \"YiYiXu/modular-loader-t2i-0704\"\n pipeline = blocks.init_pipeline(modular_repo_id)\n \n-pipeline.load_default_components(torch_dtype=torch.float16)\n+pipeline.load_components(torch_dtype=torch.float16)\n pipeline.to(\"cuda\")\n \n img_url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/sdxl-text2img.png\"\n@@ -176,15 +176,15 @@ diffdiff_pipeline = ModularPipeline.from_pretrained(modular_repo_id, trust_remot\n \n ## \u52a0\u8f7d\u7ec4\u4ef6\n \n-\u4e00\u4e2a[`ModularPipeline`]\u4e0d\u4f1a\u81ea\u52a8\u5b9e\u4f8b\u5316\u7ec4\u4ef6\u3002\u5b83\u53ea\u52a0\u8f7d\u914d\u7f6e\u548c\u7ec4\u4ef6\u89c4\u8303\u3002\u60a8\u53ef\u4ee5\u4f7f\u7528[`~ModularPipeline.load_default_components`]\u52a0\u8f7d\u6240\u6709\u7ec4\u4ef6\uff0c\u6216\u4ec5\u4f7f\u7528[`~ModularPipeline.load_components`]\u52a0\u8f7d\u7279\u5b9a\u7ec4\u4ef6\u3002\n+\u4e00\u4e2a[`ModularPipeline`]\u4e0d\u4f1a\u81ea\u52a8\u5b9e\u4f8b\u5316\u7ec4\u4ef6\u3002\u5b83\u53ea\u52a0\u8f7d\u914d\u7f6e\u548c\u7ec4\u4ef6\u89c4\u8303\u3002\u60a8\u53ef\u4ee5\u4f7f\u7528[`~ModularPipeline.load_components`]\u52a0\u8f7d\u6240\u6709\u7ec4\u4ef6\uff0c\u6216\u4ec5\u4f7f\u7528[`~ModularPipeline.load_components`]\u52a0\u8f7d\u7279\u5b9a\u7ec4\u4ef6\u3002\n \n <hfoptions id=\"load\">\n-<hfoption id=\"load_default_components\">\n+<hfoption id=\"load_components\">\n \n ```py\n import torch\n \n-t2i_pipeline.load_default_components(torch_dtype=torch.float16)\n+t2i_pipeline.load_components(torch_dtype=torch.float16)\n t2i_pipeline.to(\"cuda\")\n ```\n "
        },
        {
          "filename": "docs/source/zh/modular_diffusers/quickstart.md",
          "status": "modified",
          "additions": 7,
          "deletions": 7,
          "changes": 14,
          "patch": "@@ -175,7 +175,7 @@ print(dd_blocks)\n \u5c06 [`SequentialPipelineBlocks`] \u8f6c\u6362\u4e3a [`ModularPipeline`]\uff0c\u4f7f\u7528 [`ModularPipeline.init_pipeline`] \u65b9\u6cd5\u3002\u8fd9\u4f1a\u521d\u59cb\u5316\u4ece `modular_model_index.json` \u6587\u4ef6\u52a0\u8f7d\u7684\u9884\u671f\u7ec4\u4ef6\u3002\u901a\u8fc7\u8c03\u7528 [`ModularPipeline.load_defau\n lt_components`]\u3002\n \n-\u521d\u59cb\u5316[`ComponentManager`]\u65f6\u4f20\u5165pipeline\u662f\u4e00\u4e2a\u597d\u4e3b\u610f\uff0c\u4ee5\u5e2e\u52a9\u7ba1\u7406\u4e0d\u540c\u7684\u7ec4\u4ef6\u3002\u4e00\u65e6\u8c03\u7528[`~ModularPipeline.load_default_components`]\uff0c\u7ec4\u4ef6\u5c31\u4f1a\u88ab\u6ce8\u518c\u5230[`ComponentManager`]\u4e2d\uff0c\u5e76\u4e14\u53ef\u4ee5\u5728\u5de5\u4f5c\u6d41\u4e4b\u95f4\u5171\u4eab\u3002\u4e0b\u9762\u7684\u4f8b\u5b50\u4f7f\u7528`collection`\u53c2\u6570\u4e3a\u7ec4\u4ef6\u5206\u914d\u4e86\u4e00\u4e2a`\"diffdiff\"`\u6807\u7b7e\uff0c\u4ee5\u4fbf\u66f4\u597d\u5730\u7ec4\u7ec7\u3002\n+\u521d\u59cb\u5316[`ComponentManager`]\u65f6\u4f20\u5165pipeline\u662f\u4e00\u4e2a\u597d\u4e3b\u610f\uff0c\u4ee5\u5e2e\u52a9\u7ba1\u7406\u4e0d\u540c\u7684\u7ec4\u4ef6\u3002\u4e00\u65e6\u8c03\u7528[`~ModularPipeline.load_components`]\uff0c\u7ec4\u4ef6\u5c31\u4f1a\u88ab\u6ce8\u518c\u5230[`ComponentManager`]\u4e2d\uff0c\u5e76\u4e14\u53ef\u4ee5\u5728\u5de5\u4f5c\u6d41\u4e4b\u95f4\u5171\u4eab\u3002\u4e0b\u9762\u7684\u4f8b\u5b50\u4f7f\u7528`collection`\u53c2\u6570\u4e3a\u7ec4\u4ef6\u5206\u914d\u4e86\u4e00\u4e2a`\"diffdiff\"`\u6807\u7b7e\uff0c\u4ee5\u4fbf\u66f4\u597d\u5730\u7ec4\u7ec7\u3002\n \n ```py\n from diffusers.modular_pipelines import ComponentsManager\n@@ -209,11 +209,11 @@ ip_adapter_block = StableDiffusionXLAutoIPAdapterStep()\n dd_blocks.sub_blocks.insert(\"ip_adapter\", ip_adapter_block, 0)\n ```\n \n-\u8c03\u7528[`~ModularPipeline.init_pipeline`]\u6765\u521d\u59cb\u5316\u4e00\u4e2a[`ModularPipeline`]\uff0c\u5e76\u4f7f\u7528[`~ModularPipeline.load_default_components`]\u52a0\u8f7d\u6a21\u578b\u7ec4\u4ef6\u3002\u52a0\u8f7d\u5e76\u8bbe\u7f6eIP-Adapter\u4ee5\u8fd0\u884cpipeline\u3002\n+\u8c03\u7528[`~ModularPipeline.init_pipeline`]\u6765\u521d\u59cb\u5316\u4e00\u4e2a[`ModularPipeline`]\uff0c\u5e76\u4f7f\u7528[`~ModularPipeline.load_components`]\u52a0\u8f7d\u6a21\u578b\u7ec4\u4ef6\u3002\u52a0\u8f7d\u5e76\u8bbe\u7f6eIP-Adapter\u4ee5\u8fd0\u884cpipeline\u3002\n \n ```py\n dd_pipeline = dd_blocks.init_pipeline(\"YiYiXu/modular-demo-auto\", collection=\"diffdiff\")\n-dd_pipeline.load_default_components(torch_dtype=torch.float16)\n+dd_pipeline.load_components(torch_dtype=torch.float16)\n dd_pipeline.loader.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"sdxl_models\", weight_name=\"ip-adapter_sdxl.bin\")\n dd_pipeline.loader.set_ip_adapter_scale(0.6)\n dd_pipeline = dd_pipeline.to(device)\n@@ -261,14 +261,14 @@ class SDXLDiffDiffControlNetDenoiseStep(StableDiffusionXLDenoiseLoopWrapper):\n controlnet_denoise_block = SDXLDiffDiffControlNetDenoiseStep()\n ```\n \n-\u63d2\u5165 `controlnet_input` \u5757\u5e76\u7528\u65b0\u7684 `controlnet_denoise_block` \u66ff\u6362 `denoise` \u5757\u3002\u521d\u59cb\u5316\u4e00\u4e2a [`ModularPipeline`] \u5e76\u5c06 [`~ModularPipeline.load_default_components`] \u52a0\u8f7d\u5230\u5176\u4e2d\u3002\n+\u63d2\u5165 `controlnet_input` \u5757\u5e76\u7528\u65b0\u7684 `controlnet_denoise_block` \u66ff\u6362 `denoise` \u5757\u3002\u521d\u59cb\u5316\u4e00\u4e2a [`ModularPipeline`] \u5e76\u5c06 [`~ModularPipeline.load_components`] \u52a0\u8f7d\u5230\u5176\u4e2d\u3002\n \n ```py\n dd_blocks.sub_blocks.insert(\"controlnet_input\", control_input_block, 7)\n dd_blocks.sub_blocks[\"denoise\"] = controlnet_denoise_block\n \n dd_pipeline = dd_blocks.init_pipeline(\"YiYiXu/modular-demo-auto\", collection=\"diffdiff\")\n-dd_pipeline.load_default_components(torch_dtype=torch.float16)\n+dd_pipeline.load_components(torch_dtype=torch.float16)\n dd_pipeline = dd_pipeline.to(device)\n \n control_image = load_image(\"https://huggingface.co/datasets/YiYiXu/testing-images/resolve/main/diffdiff_tomato_canny.jpeg\")\n@@ -322,7 +322,7 @@ DIFFDIFF_AUTO_BLOCKS.insert(\"controlnet_input\",StableDiffusionXLControlNetAutoIn\n ```py\n dd_auto_blocks = SequentialPipelineBlocks.from_blocks_dict(DIFFDIFF_AUTO_BLOCKS)\n dd_pipeline = dd_auto_blocks.init_pipeline(\"YiYiXu/modular-demo-auto\", collection=\"diffdiff\")\n-dd_pipeline.load_default_components(torch_dtype=torch.float16)\n+dd_pipeline.load_components(torch_dtype=torch.float16)\n ```\n \n ## \u5206\u4eab\n@@ -342,5 +342,5 @@ from diffusers.modular_pipelines import ModularPipeline, ComponentsManager\n components = ComponentsManager()\n \n diffdiff_pipeline = ModularPipeline.from_pretrained(\"YiYiXu/modular-diffdiff-0704\", trust_remote_code=True, components_manager=components, collection=\"diffdiff\")\n-diffdiff_pipeline.load_default_components(torch_dtype=torch.float16)\n+diffdiff_pipeline.load_components(torch_dtype=torch.float16)\n ```"
        },
        {
          "filename": "src/diffusers/modular_pipelines/modular_pipeline.py",
          "status": "modified",
          "additions": 15,
          "deletions": 22,
          "changes": 37,
          "patch": "@@ -1409,7 +1409,7 @@ def set_progress_bar_config(self, **kwargs):\n # YiYi TODO:\n # 1. look into the serialization of modular_model_index.json, make sure the items are properly ordered like model_index.json (currently a mess)\n # 2. do we need ConfigSpec? the are basically just key/val kwargs\n-# 3. imnprove docstring and potentially add validator for methods where we accpet kwargs to be passed to from_pretrained/save_pretrained/load_default_components(), load_components()\n+# 3. imnprove docstring and potentially add validator for methods where we accpet kwargs to be passed to from_pretrained/save_pretrained/load_components()\n class ModularPipeline(ConfigMixin, PushToHubMixin):\n     \"\"\"\n     Base class for all Modular pipelines.\n@@ -1478,7 +1478,7 @@ def __init__(\n             - Components with default_creation_method=\"from_config\" are created immediately, its specs are not included\n               in config dict and will not be saved in `modular_model_index.json`\n             - Components with default_creation_method=\"from_pretrained\" are set to None and can be loaded later with\n-              `load_default_components()`/`load_components()`\n+              `load_components()` (with or without specific component names)\n             - The pipeline's config dict is populated with component specs (only for from_pretrained components) and\n               config values, which will be saved as `modular_model_index.json` during `save_pretrained`\n             - The pipeline's config dict is also used to store the pipeline blocks's class name, which will be saved as\n@@ -1541,20 +1541,6 @@ def default_call_parameters(self) -> Dict[str, Any]:\n             params[input_param.name] = input_param.default\n         return params\n \n-    def load_default_components(self, **kwargs):\n-        \"\"\"\n-        Load from_pretrained components using the loading specs in the config dict.\n-\n-        Args:\n-            **kwargs: Additional arguments passed to `from_pretrained` method, e.g. torch_dtype, cache_dir, etc.\n-        \"\"\"\n-        names = [\n-            name\n-            for name in self._component_specs.keys()\n-            if self._component_specs[name].default_creation_method == \"from_pretrained\"\n-        ]\n-        self.load_components(names=names, **kwargs)\n-\n     @classmethod\n     @validate_hf_hub_args\n     def from_pretrained(\n@@ -1682,8 +1668,8 @@ def register_components(self, **kwargs):\n            - non from_pretrained components are created during __init__ and registered as the object itself\n         - Components are updated with the `update_components()` method: e.g. loader.update_components(unet=unet) or\n           loader.update_components(guider=guider_spec)\n-        - (from_pretrained) Components are loaded with the `load_default_components()` method: e.g.\n-          loader.load_default_components(names=[\"unet\"])\n+        - (from_pretrained) Components are loaded with the `load_components()` method: e.g.\n+          loader.load_components(names=[\"unet\"]) or loader.load_components() to load all default components\n \n         Args:\n             **kwargs: Keyword arguments where keys are component names and values are component objects.\n@@ -1995,21 +1981,28 @@ def update_components(self, **kwargs):\n         self.register_to_config(**config_to_register)\n \n     # YiYi TODO: support map for additional from_pretrained kwargs\n-    # YiYi/Dhruv TODO: consolidate load_components and load_default_components?\n-    def load_components(self, names: Union[List[str], str], **kwargs):\n+    def load_components(self, names: Optional[Union[List[str], str]] = None, **kwargs):\n         \"\"\"\n         Load selected components from specs.\n \n         Args:\n-            names: List of component names to load; by default will not load any components\n+            names: List of component names to load. If None, will load all components with\n+                   default_creation_method == \"from_pretrained\". If provided as a list or string, will load only the\n+                   specified components.\n             **kwargs: additional kwargs to be passed to `from_pretrained()`.Can be:\n              - a single value to be applied to all components to be loaded, e.g. torch_dtype=torch.bfloat16\n              - a dict, e.g. torch_dtype={\"unet\": torch.bfloat16, \"default\": torch.float32}\n              - if potentially override ComponentSpec if passed a different loading field in kwargs, e.g. `repo`,\n                `variant`, `revision`, etc.\n         \"\"\"\n \n-        if isinstance(names, str):\n+        if names is None:\n+            names = [\n+                name\n+                for name in self._component_specs.keys()\n+                if self._component_specs[name].default_creation_method == \"from_pretrained\"\n+            ]\n+        elif isinstance(names, str):\n             names = [names]\n         elif not isinstance(names, list):\n             raise ValueError(f\"Invalid type for names: {type(names)}\")"
        },
        {
          "filename": "tests/modular_pipelines/stable_diffusion_xl/test_modular_pipeline_stable_diffusion_xl.py",
          "status": "modified",
          "additions": 2,
          "deletions": 2,
          "changes": 4,
          "patch": "@@ -67,7 +67,7 @@ class SDXLModularTests:\n \n     def get_pipeline(self, components_manager=None, torch_dtype=torch.float32):\n         pipeline = self.pipeline_blocks_class().init_pipeline(self.repo, components_manager=components_manager)\n-        pipeline.load_default_components(torch_dtype=torch_dtype)\n+        pipeline.load_components(torch_dtype=torch_dtype)\n         return pipeline\n \n     def get_dummy_inputs(self, device, seed=0):\n@@ -158,7 +158,7 @@ def test_ip_adapter(self, expected_max_diff: float = 1e-4, expected_pipe_slice=N\n         blocks = self.pipeline_blocks_class()\n         _ = blocks.sub_blocks.pop(\"ip_adapter\")\n         pipe = blocks.init_pipeline(self.repo)\n-        pipe.load_default_components(torch_dtype=torch.float32)\n+        pipe.load_components(torch_dtype=torch.float32)\n         pipe = pipe.to(torch_device)\n         pipe.set_progress_bar_config(disable=None)\n         cross_attention_dim = pipe.unet.config.get(\"cross_attention_dim\")"
        },
        {
          "filename": "tests/modular_pipelines/test_modular_pipelines_common.py",
          "status": "modified",
          "additions": 1,
          "deletions": 1,
          "changes": 2,
          "patch": "@@ -343,7 +343,7 @@ def test_save_from_pretrained(self):\n         with tempfile.TemporaryDirectory() as tmpdirname:\n             base_pipe.save_pretrained(tmpdirname)\n             pipe = ModularPipeline.from_pretrained(tmpdirname).to(torch_device)\n-            pipe.load_default_components(torch_dtype=torch.float32)\n+            pipe.load_components(torch_dtype=torch.float32)\n             pipe.to(torch_device)\n \n         pipes.append(pipe)"
        }
      ],
      "num_files": 11,
      "scraped_at": "2025-11-16T21:19:45.322147"
    },
    {
      "pr_number": 12215,
      "title": "Support ControlNet for Qwen-Image",
      "body": "### What does this PR do?\r\n\r\nAdd ControlNet-Union ([InstantX/Qwen-Image-ControlNet-Union](https://huggingface.co/InstantX/Qwen-Image-ControlNet-Union)) support for Qwen-Image.\r\n\r\n### Inference\r\n```\r\nimport torch\r\nfrom diffusers.utils import load_image\r\nfrom diffusers import QwenImageControlNetPipeline, QwenImageControlNetModel\r\n\r\nbase_model = \"Qwen/Qwen-Image\"\r\ncontrolnet_model = \"InstantX/Qwen-Image-ControlNet-Union\"\r\n\r\ncontrolnet = QwenImageControlNetModel.from_pretrained(controlnet_model, torch_dtype=torch.bfloat16)\r\n\r\npipe = QwenImageControlNetPipeline.from_pretrained(\r\n    base_model, controlnet=controlnet, torch_dtype=torch.bfloat16\r\n)\r\npipe.to(\"cuda\")\r\n\r\n# canny\r\ncontrol_image = load_image(\"https://huggingface.co/InstantX/Qwen-Image-ControlNet-Union/resolve/main/conds/canny.png\")\r\nprompt = \"Aesthetics art, traditional asian pagoda, elaborate golden accents, sky blue and white color palette, swirling cloud pattern, digital illustration, east asian architecture, ornamental rooftop, intricate detailing on building, cultural representation.\"\r\ncontrolnet_conditioning_scale = 1.0\r\n\r\nimage = pipe(\r\n    prompt=prompt,\r\n    negative_prompt=\" \",\r\n    control_image=control_image,\r\n    controlnet_conditioning_scale=controlnet_conditioning_scale,\r\n    width=control_image.size[0],\r\n    height=control_image.size[1],\r\n    num_inference_steps=30,\r\n    true_cfg_scale=4.0,\r\n    generator=torch.Generator(device=\"cuda\").manual_seed(42),\r\n).images[0]\r\nimage.save(f\"qwenimage_cn_union_result.png\")\r\n```\r\n\r\n### Multi-Conditions\r\n```\r\nimport torch\r\nfrom diffusers.utils import load_image\r\nfrom diffusers import QwenImageControlNetPipeline, QwenImageControlNetModel, QwenImageMultiControlNetModel\r\n\r\nbase_model = \"Qwen/Qwen-Image\"\r\ncontrolnet_model = \"InstantX/Qwen-Image-ControlNet-Union\"\r\n\r\ncontrolnet = QwenImageControlNetModel.from_pretrained(controlnet_model, torch_dtype=torch.bfloat16)\r\ncontrolnet = QwenImageMultiControlNetModel([controlnet])\r\n\r\npipe = QwenImageControlNetPipeline.from_pretrained(\r\n    base_model, controlnet=controlnet, torch_dtype=torch.bfloat16\r\n)\r\npipe.to(\"cuda\")\r\n\r\n# canny\r\ncontrol_image = load_image(\"https://huggingface.co/InstantX/Qwen-Image-ControlNet-Union/resolve/main/conds/canny.png\")\r\nprompt = \"Aesthetics art, traditional asian pagoda, elaborate golden accents, sky blue and white color palette, swirling cloud pattern, digital illustration, east asian architecture, ornamental rooftop, intricate detailing on building, cultural representation.\"\r\ncontrolnet_conditioning_scale = 1.0\r\n\r\n# Please note that the results will not be identical, because the generator is called in different order.\r\nimage = pipe(\r\n    prompt=prompt,\r\n    negative_prompt=\" \",\r\n    control_image=[control_image, control_image],\r\n    controlnet_conditioning_scale=[controlnet_conditioning_scale/2, controlnet_conditioning_scale/2],\r\n    width=control_image.size[0],\r\n    height=control_image.size[1],\r\n    num_inference_steps=30,\r\n    true_cfg_scale=4.0,\r\n    generator=torch.Generator(device=\"cuda\").manual_seed(42),\r\n).images[0]\r\nimage.save(f\"qwenimage_cn_union_multi_result_.png\")\r\n```\r\n\r\n### Sanity Check\r\n<img src=\"https://github.com/user-attachments/assets/d4560abe-1ab4-4b0f-8395-69073baf0547\" width=\"400\"/> <img src=\"https://github.com/user-attachments/assets/dc24d8d1-bf26-43bc-9739-9c3222e3a9c6\" width=\"400\"/>\r\n\r\n\r\n",
      "html_url": "https://github.com/huggingface/diffusers/pull/12215",
      "created_at": "2025-08-22T06:28:39Z",
      "merged_at": "2025-08-22T21:00:01Z",
      "merge_commit_sha": "561ab54de3d3aaa9007e76aeb3b15e8be3ed353f",
      "base_ref": "main",
      "head_sha": "e2408b8757e6e53e4450f5573de144dcf43e7747",
      "user": "haofanwang",
      "files": [
        {
          "filename": "docs/source/en/api/pipelines/qwenimage.md",
          "status": "modified",
          "additions": 4,
          "deletions": 0,
          "changes": 4,
          "patch": "@@ -120,6 +120,10 @@ The `guidance_scale` parameter in the pipeline is there to support future guidan\n   - all\n   - __call__\n \n+## QwenImaggeControlNetPipeline\n+  - all\n+  - __call__\n+\n ## QwenImagePipelineOutput\n \n [[autodoc]] pipelines.qwenimage.pipeline_output.QwenImagePipelineOutput\n\\ No newline at end of file"
        },
        {
          "filename": "src/diffusers/__init__.py",
          "status": "modified",
          "additions": 6,
          "deletions": 0,
          "changes": 6,
          "patch": "@@ -218,6 +218,8 @@\n             \"OmniGenTransformer2DModel\",\n             \"PixArtTransformer2DModel\",\n             \"PriorTransformer\",\n+            \"QwenImageControlNetModel\",\n+            \"QwenImageMultiControlNetModel\",\n             \"QwenImageTransformer2DModel\",\n             \"SanaControlNetModel\",\n             \"SanaTransformer2DModel\",\n@@ -491,6 +493,7 @@\n             \"PixArtAlphaPipeline\",\n             \"PixArtSigmaPAGPipeline\",\n             \"PixArtSigmaPipeline\",\n+            \"QwenImageControlNetPipeline\",\n             \"QwenImageEditPipeline\",\n             \"QwenImageImg2ImgPipeline\",\n             \"QwenImageInpaintPipeline\",\n@@ -885,6 +888,8 @@\n             OmniGenTransformer2DModel,\n             PixArtTransformer2DModel,\n             PriorTransformer,\n+            QwenImageControlNetModel,\n+            QwenImageMultiControlNetModel,\n             QwenImageTransformer2DModel,\n             SanaControlNetModel,\n             SanaTransformer2DModel,\n@@ -1128,6 +1133,7 @@\n             PixArtAlphaPipeline,\n             PixArtSigmaPAGPipeline,\n             PixArtSigmaPipeline,\n+            QwenImageControlNetPipeline,\n             QwenImageEditPipeline,\n             QwenImageImg2ImgPipeline,\n             QwenImageInpaintPipeline,"
        },
        {
          "filename": "src/diffusers/models/__init__.py",
          "status": "modified",
          "additions": 6,
          "deletions": 0,
          "changes": 6,
          "patch": "@@ -52,6 +52,10 @@\n         \"HunyuanDiT2DControlNetModel\",\n         \"HunyuanDiT2DMultiControlNetModel\",\n     ]\n+    _import_structure[\"controlnets.controlnet_qwenimage\"] = [\n+        \"QwenImageControlNetModel\",\n+        \"QwenImageMultiControlNetModel\",\n+    ]\n     _import_structure[\"controlnets.controlnet_sana\"] = [\"SanaControlNetModel\"]\n     _import_structure[\"controlnets.controlnet_sd3\"] = [\"SD3ControlNetModel\", \"SD3MultiControlNetModel\"]\n     _import_structure[\"controlnets.controlnet_sparsectrl\"] = [\"SparseControlNetModel\"]\n@@ -148,6 +152,8 @@\n             HunyuanDiT2DMultiControlNetModel,\n             MultiControlNetModel,\n             MultiControlNetUnionModel,\n+            QwenImageControlNetModel,\n+            QwenImageMultiControlNetModel,\n             SanaControlNetModel,\n             SD3ControlNetModel,\n             SD3MultiControlNetModel,"
        },
        {
          "filename": "src/diffusers/models/controlnets/__init__.py",
          "status": "modified",
          "additions": 1,
          "deletions": 0,
          "changes": 1,
          "patch": "@@ -9,6 +9,7 @@\n         HunyuanDiT2DControlNetModel,\n         HunyuanDiT2DMultiControlNetModel,\n     )\n+    from .controlnet_qwenimage import QwenImageControlNetModel, QwenImageMultiControlNetModel\n     from .controlnet_sana import SanaControlNetModel\n     from .controlnet_sd3 import SD3ControlNetModel, SD3ControlNetOutput, SD3MultiControlNetModel\n     from .controlnet_sparsectrl import ("
        },
        {
          "filename": "src/diffusers/models/controlnets/controlnet_qwenimage.py",
          "status": "added",
          "additions": 359,
          "deletions": 0,
          "changes": 359,
          "patch": "@@ -0,0 +1,359 @@\n+# Copyright 2025 Black Forest Labs, The HuggingFace Team and The InstantX Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from dataclasses import dataclass\n+from typing import Any, Dict, List, Optional, Tuple, Union\n+\n+import torch\n+import torch.nn as nn\n+\n+from ...configuration_utils import ConfigMixin, register_to_config\n+from ...loaders import FromOriginalModelMixin, PeftAdapterMixin\n+from ...utils import USE_PEFT_BACKEND, BaseOutput, logging, scale_lora_layers, unscale_lora_layers\n+from ..attention_processor import AttentionProcessor\n+from ..cache_utils import CacheMixin\n+from ..controlnets.controlnet import zero_module\n+from ..modeling_outputs import Transformer2DModelOutput\n+from ..modeling_utils import ModelMixin\n+from ..transformers.transformer_qwenimage import (\n+    QwenEmbedRope,\n+    QwenImageTransformerBlock,\n+    QwenTimestepProjEmbeddings,\n+    RMSNorm,\n+)\n+\n+\n+logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n+\n+\n+@dataclass\n+class QwenImageControlNetOutput(BaseOutput):\n+    controlnet_block_samples: Tuple[torch.Tensor]\n+\n+\n+class QwenImageControlNetModel(ModelMixin, ConfigMixin, PeftAdapterMixin, FromOriginalModelMixin, CacheMixin):\n+    _supports_gradient_checkpointing = True\n+\n+    @register_to_config\n+    def __init__(\n+        self,\n+        patch_size: int = 2,\n+        in_channels: int = 64,\n+        out_channels: Optional[int] = 16,\n+        num_layers: int = 60,\n+        attention_head_dim: int = 128,\n+        num_attention_heads: int = 24,\n+        joint_attention_dim: int = 3584,\n+        axes_dims_rope: Tuple[int, int, int] = (16, 56, 56),\n+        extra_condition_channels: int = 0,  # for controlnet-inpainting\n+    ):\n+        super().__init__()\n+        self.out_channels = out_channels or in_channels\n+        self.inner_dim = num_attention_heads * attention_head_dim\n+\n+        self.pos_embed = QwenEmbedRope(theta=10000, axes_dim=list(axes_dims_rope), scale_rope=True)\n+\n+        self.time_text_embed = QwenTimestepProjEmbeddings(embedding_dim=self.inner_dim)\n+\n+        self.txt_norm = RMSNorm(joint_attention_dim, eps=1e-6)\n+\n+        self.img_in = nn.Linear(in_channels, self.inner_dim)\n+        self.txt_in = nn.Linear(joint_attention_dim, self.inner_dim)\n+\n+        self.transformer_blocks = nn.ModuleList(\n+            [\n+                QwenImageTransformerBlock(\n+                    dim=self.inner_dim,\n+                    num_attention_heads=num_attention_heads,\n+                    attention_head_dim=attention_head_dim,\n+                )\n+                for _ in range(num_layers)\n+            ]\n+        )\n+\n+        # controlnet_blocks\n+        self.controlnet_blocks = nn.ModuleList([])\n+        for _ in range(len(self.transformer_blocks)):\n+            self.controlnet_blocks.append(zero_module(nn.Linear(self.inner_dim, self.inner_dim)))\n+        self.controlnet_x_embedder = zero_module(\n+            torch.nn.Linear(in_channels + extra_condition_channels, self.inner_dim)\n+        )\n+\n+        self.gradient_checkpointing = False\n+\n+    @property\n+    # Copied from diffusers.models.unets.unet_2d_condition.UNet2DConditionModel.attn_processors\n+    def attn_processors(self):\n+        r\"\"\"\n+        Returns:\n+            `dict` of attention processors: A dictionary containing all attention processors used in the model with\n+            indexed by its weight name.\n+        \"\"\"\n+        # set recursively\n+        processors = {}\n+\n+        def fn_recursive_add_processors(name: str, module: torch.nn.Module, processors: Dict[str, AttentionProcessor]):\n+            if hasattr(module, \"get_processor\"):\n+                processors[f\"{name}.processor\"] = module.get_processor()\n+\n+            for sub_name, child in module.named_children():\n+                fn_recursive_add_processors(f\"{name}.{sub_name}\", child, processors)\n+\n+            return processors\n+\n+        for name, module in self.named_children():\n+            fn_recursive_add_processors(name, module, processors)\n+\n+        return processors\n+\n+    # Copied from diffusers.models.unets.unet_2d_condition.UNet2DConditionModel.set_attn_processor\n+    def set_attn_processor(self, processor):\n+        r\"\"\"\n+        Sets the attention processor to use to compute attention.\n+\n+        Parameters:\n+            processor (`dict` of `AttentionProcessor` or only `AttentionProcessor`):\n+                The instantiated processor class or a dictionary of processor classes that will be set as the processor\n+                for **all** `Attention` layers.\n+\n+                If `processor` is a dict, the key needs to define the path to the corresponding cross attention\n+                processor. This is strongly recommended when setting trainable attention processors.\n+\n+        \"\"\"\n+        count = len(self.attn_processors.keys())\n+\n+        if isinstance(processor, dict) and len(processor) != count:\n+            raise ValueError(\n+                f\"A dict of processors was passed, but the number of processors {len(processor)} does not match the\"\n+                f\" number of attention layers: {count}. Please make sure to pass {count} processor classes.\"\n+            )\n+\n+        def fn_recursive_attn_processor(name: str, module: torch.nn.Module, processor):\n+            if hasattr(module, \"set_processor\"):\n+                if not isinstance(processor, dict):\n+                    module.set_processor(processor)\n+                else:\n+                    module.set_processor(processor.pop(f\"{name}.processor\"))\n+\n+            for sub_name, child in module.named_children():\n+                fn_recursive_attn_processor(f\"{name}.{sub_name}\", child, processor)\n+\n+        for name, module in self.named_children():\n+            fn_recursive_attn_processor(name, module, processor)\n+\n+    @classmethod\n+    def from_transformer(\n+        cls,\n+        transformer,\n+        num_layers: int = 5,\n+        attention_head_dim: int = 128,\n+        num_attention_heads: int = 24,\n+        load_weights_from_transformer=True,\n+        extra_condition_channels: int = 0,\n+    ):\n+        config = dict(transformer.config)\n+        config[\"num_layers\"] = num_layers\n+        config[\"attention_head_dim\"] = attention_head_dim\n+        config[\"num_attention_heads\"] = num_attention_heads\n+        config[\"extra_condition_channels\"] = extra_condition_channels\n+\n+        controlnet = cls.from_config(config)\n+\n+        if load_weights_from_transformer:\n+            controlnet.pos_embed.load_state_dict(transformer.pos_embed.state_dict())\n+            controlnet.time_text_embed.load_state_dict(transformer.time_text_embed.state_dict())\n+            controlnet.img_in.load_state_dict(transformer.img_in.state_dict())\n+            controlnet.txt_in.load_state_dict(transformer.txt_in.state_dict())\n+            controlnet.transformer_blocks.load_state_dict(transformer.transformer_blocks.state_dict(), strict=False)\n+            controlnet.controlnet_x_embedder = zero_module(controlnet.controlnet_x_embedder)\n+\n+        return controlnet\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        controlnet_cond: torch.Tensor,\n+        conditioning_scale: float = 1.0,\n+        encoder_hidden_states: torch.Tensor = None,\n+        encoder_hidden_states_mask: torch.Tensor = None,\n+        timestep: torch.LongTensor = None,\n+        img_shapes: Optional[List[Tuple[int, int, int]]] = None,\n+        txt_seq_lens: Optional[List[int]] = None,\n+        joint_attention_kwargs: Optional[Dict[str, Any]] = None,\n+        return_dict: bool = True,\n+    ) -> Union[torch.FloatTensor, Transformer2DModelOutput]:\n+        \"\"\"\n+        The [`FluxTransformer2DModel`] forward method.\n+\n+        Args:\n+            hidden_states (`torch.FloatTensor` of shape `(batch size, channel, height, width)`):\n+                Input `hidden_states`.\n+            controlnet_cond (`torch.Tensor`):\n+                The conditional input tensor of shape `(batch_size, sequence_length, hidden_size)`.\n+            conditioning_scale (`float`, defaults to `1.0`):\n+                The scale factor for ControlNet outputs.\n+            encoder_hidden_states (`torch.FloatTensor` of shape `(batch size, sequence_len, embed_dims)`):\n+                Conditional embeddings (embeddings computed from the input conditions such as prompts) to use.\n+            pooled_projections (`torch.FloatTensor` of shape `(batch_size, projection_dim)`): Embeddings projected\n+                from the embeddings of input conditions.\n+            timestep ( `torch.LongTensor`):\n+                Used to indicate denoising step.\n+            block_controlnet_hidden_states: (`list` of `torch.Tensor`):\n+                A list of tensors that if specified are added to the residuals of transformer blocks.\n+            joint_attention_kwargs (`dict`, *optional*):\n+                A kwargs dictionary that if specified is passed along to the `AttentionProcessor` as defined under\n+                `self.processor` in\n+                [diffusers.models.attention_processor](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).\n+            return_dict (`bool`, *optional*, defaults to `True`):\n+                Whether or not to return a [`~models.transformer_2d.Transformer2DModelOutput`] instead of a plain\n+                tuple.\n+\n+        Returns:\n+            If `return_dict` is True, an [`~models.transformer_2d.Transformer2DModelOutput`] is returned, otherwise a\n+            `tuple` where the first element is the sample tensor.\n+        \"\"\"\n+        if joint_attention_kwargs is not None:\n+            joint_attention_kwargs = joint_attention_kwargs.copy()\n+            lora_scale = joint_attention_kwargs.pop(\"scale\", 1.0)\n+        else:\n+            lora_scale = 1.0\n+\n+        if USE_PEFT_BACKEND:\n+            # weight the lora layers by setting `lora_scale` for each PEFT layer\n+            scale_lora_layers(self, lora_scale)\n+        else:\n+            if joint_attention_kwargs is not None and joint_attention_kwargs.get(\"scale\", None) is not None:\n+                logger.warning(\n+                    \"Passing `scale` via `joint_attention_kwargs` when not using the PEFT backend is ineffective.\"\n+                )\n+        hidden_states = self.img_in(hidden_states)\n+\n+        # add\n+        hidden_states = hidden_states + self.controlnet_x_embedder(controlnet_cond)\n+\n+        temb = self.time_text_embed(timestep, hidden_states)\n+\n+        image_rotary_emb = self.pos_embed(img_shapes, txt_seq_lens, device=hidden_states.device)\n+\n+        timestep = timestep.to(hidden_states.dtype)\n+        encoder_hidden_states = self.txt_norm(encoder_hidden_states)\n+        encoder_hidden_states = self.txt_in(encoder_hidden_states)\n+\n+        block_samples = ()\n+        for index_block, block in enumerate(self.transformer_blocks):\n+            if torch.is_grad_enabled() and self.gradient_checkpointing:\n+                encoder_hidden_states, hidden_states = self._gradient_checkpointing_func(\n+                    block,\n+                    hidden_states,\n+                    encoder_hidden_states,\n+                    encoder_hidden_states_mask,\n+                    temb,\n+                    image_rotary_emb,\n+                )\n+\n+            else:\n+                encoder_hidden_states, hidden_states = block(\n+                    hidden_states=hidden_states,\n+                    encoder_hidden_states=encoder_hidden_states,\n+                    encoder_hidden_states_mask=encoder_hidden_states_mask,\n+                    temb=temb,\n+                    image_rotary_emb=image_rotary_emb,\n+                    joint_attention_kwargs=joint_attention_kwargs,\n+                )\n+            block_samples = block_samples + (hidden_states,)\n+\n+        # controlnet block\n+        controlnet_block_samples = ()\n+        for block_sample, controlnet_block in zip(block_samples, self.controlnet_blocks):\n+            block_sample = controlnet_block(block_sample)\n+            controlnet_block_samples = controlnet_block_samples + (block_sample,)\n+\n+        # scaling\n+        controlnet_block_samples = [sample * conditioning_scale for sample in controlnet_block_samples]\n+        controlnet_block_samples = None if len(controlnet_block_samples) == 0 else controlnet_block_samples\n+\n+        if USE_PEFT_BACKEND:\n+            # remove `lora_scale` from each PEFT layer\n+            unscale_lora_layers(self, lora_scale)\n+\n+        if not return_dict:\n+            return controlnet_block_samples\n+\n+        return QwenImageControlNetOutput(\n+            controlnet_block_samples=controlnet_block_samples,\n+        )\n+\n+\n+class QwenImageMultiControlNetModel(ModelMixin, ConfigMixin, PeftAdapterMixin, FromOriginalModelMixin, CacheMixin):\n+    r\"\"\"\n+    `QwenImageMultiControlNetModel` wrapper class for Multi-QwenImageControlNetModel\n+\n+    This module is a wrapper for multiple instances of the `QwenImageControlNetModel`. The `forward()` API is designed\n+    to be compatible with `QwenImageControlNetModel`.\n+\n+    Args:\n+        controlnets (`List[QwenImageControlNetModel]`):\n+            Provides additional conditioning to the unet during the denoising process. You must set multiple\n+            `QwenImageControlNetModel` as a list.\n+    \"\"\"\n+\n+    def __init__(self, controlnets):\n+        super().__init__()\n+        self.nets = nn.ModuleList(controlnets)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.FloatTensor,\n+        controlnet_cond: List[torch.tensor],\n+        conditioning_scale: List[float],\n+        encoder_hidden_states: torch.Tensor = None,\n+        encoder_hidden_states_mask: torch.Tensor = None,\n+        timestep: torch.LongTensor = None,\n+        img_shapes: Optional[List[Tuple[int, int, int]]] = None,\n+        txt_seq_lens: Optional[List[int]] = None,\n+        joint_attention_kwargs: Optional[Dict[str, Any]] = None,\n+        return_dict: bool = True,\n+    ) -> Union[QwenImageControlNetOutput, Tuple]:\n+        # ControlNet-Union with multiple conditions\n+        # only load one ControlNet for saving memories\n+        if len(self.nets) == 1:\n+            controlnet = self.nets[0]\n+\n+            for i, (image, scale) in enumerate(zip(controlnet_cond, conditioning_scale)):\n+                block_samples = controlnet(\n+                    hidden_states=hidden_states,\n+                    controlnet_cond=image,\n+                    conditioning_scale=scale,\n+                    encoder_hidden_states=encoder_hidden_states,\n+                    encoder_hidden_states_mask=encoder_hidden_states_mask,\n+                    timestep=timestep,\n+                    img_shapes=img_shapes,\n+                    txt_seq_lens=txt_seq_lens,\n+                    joint_attention_kwargs=joint_attention_kwargs,\n+                    return_dict=return_dict,\n+                )\n+\n+                # merge samples\n+                if i == 0:\n+                    control_block_samples = block_samples\n+                else:\n+                    if block_samples is not None and control_block_samples is not None:\n+                        control_block_samples = [\n+                            control_block_sample + block_sample\n+                            for control_block_sample, block_sample in zip(control_block_samples, block_samples)\n+                        ]\n+        else:\n+            raise ValueError(\"QwenImageMultiControlNetModel only supports a single controlnet-union now.\")\n+\n+        return control_block_samples"
        },
        {
          "filename": "src/diffusers/models/transformers/transformer_qwenimage.py",
          "status": "modified",
          "additions": 8,
          "deletions": 0,
          "changes": 8,
          "patch": "@@ -16,6 +16,7 @@\n import math\n from typing import Any, Dict, List, Optional, Tuple, Union\n \n+import numpy as np\n import torch\n import torch.nn as nn\n import torch.nn.functional as F\n@@ -552,6 +553,7 @@ def forward(\n         txt_seq_lens: Optional[List[int]] = None,\n         guidance: torch.Tensor = None,  # TODO: this should probably be removed\n         attention_kwargs: Optional[Dict[str, Any]] = None,\n+        controlnet_block_samples=None,\n         return_dict: bool = True,\n     ) -> Union[torch.Tensor, Transformer2DModelOutput]:\n         \"\"\"\n@@ -631,6 +633,12 @@ def forward(\n                     joint_attention_kwargs=attention_kwargs,\n                 )\n \n+            # controlnet residual\n+            if controlnet_block_samples is not None:\n+                interval_control = len(self.transformer_blocks) / len(controlnet_block_samples)\n+                interval_control = int(np.ceil(interval_control))\n+                hidden_states = hidden_states + controlnet_block_samples[index_block // interval_control]\n+\n         # Use only the image part (hidden_states) from the dual-stream blocks\n         hidden_states = self.norm_out(hidden_states, temb)\n         output = self.proj_out(hidden_states)"
        },
        {
          "filename": "src/diffusers/modular_pipelines/modular_pipeline_utils.py",
          "status": "modified",
          "additions": 1,
          "deletions": 1,
          "changes": 2,
          "patch": "@@ -209,7 +209,7 @@ def decode_load_id(cls, load_id: str) -> Dict[str, Optional[str]]:\n \n         # Get all loading fields in order\n         loading_fields = cls.loading_fields()\n-        result = {f: None for f in loading_fields}\n+        result = dict.fromkeys(loading_fields)\n \n         if load_id == \"null\":\n             return result"
        },
        {
          "filename": "src/diffusers/pipelines/__init__.py",
          "status": "modified",
          "additions": 2,
          "deletions": 0,
          "changes": 2,
          "patch": "@@ -393,6 +393,7 @@\n         \"QwenImageImg2ImgPipeline\",\n         \"QwenImageInpaintPipeline\",\n         \"QwenImageEditPipeline\",\n+        \"QwenImageControlNetPipeline\",\n     ]\n try:\n     if not is_onnx_available():\n@@ -712,6 +713,7 @@\n         from .pia import PIAPipeline\n         from .pixart_alpha import PixArtAlphaPipeline, PixArtSigmaPipeline\n         from .qwenimage import (\n+            QwenImageControlNetPipeline,\n             QwenImageEditPipeline,\n             QwenImageImg2ImgPipeline,\n             QwenImageInpaintPipeline,"
        },
        {
          "filename": "src/diffusers/pipelines/qwenimage/__init__.py",
          "status": "modified",
          "additions": 2,
          "deletions": 0,
          "changes": 2,
          "patch": "@@ -24,6 +24,7 @@\n else:\n     _import_structure[\"modeling_qwenimage\"] = [\"ReduxImageEncoder\"]\n     _import_structure[\"pipeline_qwenimage\"] = [\"QwenImagePipeline\"]\n+    _import_structure[\"pipeline_qwenimage_controlnet\"] = [\"QwenImageControlNetPipeline\"]\n     _import_structure[\"pipeline_qwenimage_edit\"] = [\"QwenImageEditPipeline\"]\n     _import_structure[\"pipeline_qwenimage_img2img\"] = [\"QwenImageImg2ImgPipeline\"]\n     _import_structure[\"pipeline_qwenimage_inpaint\"] = [\"QwenImageInpaintPipeline\"]\n@@ -36,6 +37,7 @@\n         from ...utils.dummy_torch_and_transformers_objects import *  # noqa F403\n     else:\n         from .pipeline_qwenimage import QwenImagePipeline\n+        from .pipeline_qwenimage_controlnet import QwenImageControlNetPipeline\n         from .pipeline_qwenimage_edit import QwenImageEditPipeline\n         from .pipeline_qwenimage_img2img import QwenImageImg2ImgPipeline\n         from .pipeline_qwenimage_inpaint import QwenImageInpaintPipeline"
        },
        {
          "filename": "src/diffusers/pipelines/qwenimage/pipeline_qwenimage_controlnet.py",
          "status": "added",
          "additions": 948,
          "deletions": 0,
          "changes": 948,
          "patch": "@@ -0,0 +1,948 @@\n+# Copyright 2025 Qwen-Image Team, InstantX Team and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import inspect\n+from typing import Any, Callable, Dict, List, Optional, Union\n+\n+import numpy as np\n+import torch\n+from transformers import Qwen2_5_VLForConditionalGeneration, Qwen2Tokenizer\n+\n+from ...image_processor import PipelineImageInput, VaeImageProcessor\n+from ...loaders import QwenImageLoraLoaderMixin\n+from ...models import AutoencoderKLQwenImage, QwenImageTransformer2DModel\n+from ...models.controlnets.controlnet_qwenimage import QwenImageControlNetModel, QwenImageMultiControlNetModel\n+from ...schedulers import FlowMatchEulerDiscreteScheduler\n+from ...utils import is_torch_xla_available, logging, replace_example_docstring\n+from ...utils.torch_utils import randn_tensor\n+from ..pipeline_utils import DiffusionPipeline\n+from .pipeline_output import QwenImagePipelineOutput\n+\n+\n+if is_torch_xla_available():\n+    import torch_xla.core.xla_model as xm\n+\n+    XLA_AVAILABLE = True\n+else:\n+    XLA_AVAILABLE = False\n+\n+\n+logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n+\n+EXAMPLE_DOC_STRING = \"\"\"\n+    Examples:\n+        ```py\n+        >>> import torch\n+        >>> from diffusers.utils import load_image\n+        >>> from diffusers import QwenImageControlNetModel, QwenImageMultiControlNetModel, QwenImageControlNetPipeline\n+\n+        >>> # QwenImageControlNetModel\n+        >>> controlnet = QwenImageControlNetModel.from_pretrained(\n+        ...     \"InstantX/Qwen-Image-ControlNet-Union\", torch_dtype=torch.bfloat16\n+        ... )\n+        >>> pipe = QwenImageControlNetPipeline.from_pretrained(\n+        ...     \"Qwen/Qwen-Image\", controlnet=controlnet, torch_dtype=torch.bfloat16\n+        ... )\n+        >>> pipe.to(\"cuda\")\n+        >>> prompt = \"Aesthetics art, traditional asian pagoda, elaborate golden accents, sky blue and white color palette, swirling cloud pattern, digital illustration, east asian architecture, ornamental rooftop, intricate detailing on building, cultural representation.\"\n+        >>> negative_prompt = \" \"\n+        >>> control_image = load_image(\n+        ...     \"https://huggingface.co/InstantX/Qwen-Image-ControlNet-Union/resolve/main/conds/canny.png\"\n+        ... )\n+        >>> # Depending on the variant being used, the pipeline call will slightly vary.\n+        >>> # Refer to the pipeline documentation for more details.\n+        >>> image = pipe(\n+        ...     prompt,\n+        ...     negative_prompt=negative_prompt,\n+        ...     control_image=control_image,\n+        ...     controlnet_conditioning_scale=1.0,\n+        ...     num_inference_steps=30,\n+        ...     true_cfg_scale=4.0,\n+        ... ).images[0]\n+        >>> image.save(\"qwenimage_cn_union.png\")\n+\n+        >>> # QwenImageMultiControlNetModel\n+        >>> controlnet = QwenImageControlNetModel.from_pretrained(\n+        ...     \"InstantX/Qwen-Image-ControlNet-Union\", torch_dtype=torch.bfloat16\n+        ... )\n+        >>> controlnet = QwenImageMultiControlNetModel([controlnet])\n+        >>> pipe = QwenImageControlNetPipeline.from_pretrained(\n+        ...     \"Qwen/Qwen-Image\", controlnet=controlnet, torch_dtype=torch.bfloat16\n+        ... )\n+        >>> pipe.to(\"cuda\")\n+        >>> prompt = \"Aesthetics art, traditional asian pagoda, elaborate golden accents, sky blue and white color palette, swirling cloud pattern, digital illustration, east asian architecture, ornamental rooftop, intricate detailing on building, cultural representation.\"\n+        >>> negative_prompt = \" \"\n+        >>> control_image = load_image(\n+        ...     \"https://huggingface.co/InstantX/Qwen-Image-ControlNet-Union/resolve/main/conds/canny.png\"\n+        ... )\n+        >>> # Depending on the variant being used, the pipeline call will slightly vary.\n+        >>> # Refer to the pipeline documentation for more details.\n+        >>> image = pipe(\n+        ...     prompt,\n+        ...     negative_prompt=negative_prompt,\n+        ...     control_image=[control_image, control_image],\n+        ...     controlnet_conditioning_scale=[0.5, 0.5],\n+        ...     num_inference_steps=30,\n+        ...     true_cfg_scale=4.0,\n+        ... ).images[0]\n+        >>> image.save(\"qwenimage_cn_union_multi.png\")\n+        ```\n+\"\"\"\n+\n+\n+# Coped from diffusers.pipelines.qwenimage.pipeline_qwenimage.calculate_shift\n+def calculate_shift(\n+    image_seq_len,\n+    base_seq_len: int = 256,\n+    max_seq_len: int = 4096,\n+    base_shift: float = 0.5,\n+    max_shift: float = 1.15,\n+):\n+    m = (max_shift - base_shift) / (max_seq_len - base_seq_len)\n+    b = base_shift - m * base_seq_len\n+    mu = image_seq_len * m + b\n+    return mu\n+\n+\n+# Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_img2img.retrieve_latents\n+def retrieve_latents(\n+    encoder_output: torch.Tensor, generator: Optional[torch.Generator] = None, sample_mode: str = \"sample\"\n+):\n+    if hasattr(encoder_output, \"latent_dist\") and sample_mode == \"sample\":\n+        return encoder_output.latent_dist.sample(generator)\n+    elif hasattr(encoder_output, \"latent_dist\") and sample_mode == \"argmax\":\n+        return encoder_output.latent_dist.mode()\n+    elif hasattr(encoder_output, \"latents\"):\n+        return encoder_output.latents\n+    else:\n+        raise AttributeError(\"Could not access latents of provided encoder_output\")\n+\n+\n+# Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.retrieve_timesteps\n+def retrieve_timesteps(\n+    scheduler,\n+    num_inference_steps: Optional[int] = None,\n+    device: Optional[Union[str, torch.device]] = None,\n+    timesteps: Optional[List[int]] = None,\n+    sigmas: Optional[List[float]] = None,\n+    **kwargs,\n+):\n+    r\"\"\"\n+    Calls the scheduler's `set_timesteps` method and retrieves timesteps from the scheduler after the call. Handles\n+    custom timesteps. Any kwargs will be supplied to `scheduler.set_timesteps`.\n+\n+    Args:\n+        scheduler (`SchedulerMixin`):\n+            The scheduler to get timesteps from.\n+        num_inference_steps (`int`):\n+            The number of diffusion steps used when generating samples with a pre-trained model. If used, `timesteps`\n+            must be `None`.\n+        device (`str` or `torch.device`, *optional*):\n+            The device to which the timesteps should be moved to. If `None`, the timesteps are not moved.\n+        timesteps (`List[int]`, *optional*):\n+            Custom timesteps used to override the timestep spacing strategy of the scheduler. If `timesteps` is passed,\n+            `num_inference_steps` and `sigmas` must be `None`.\n+        sigmas (`List[float]`, *optional*):\n+            Custom sigmas used to override the timestep spacing strategy of the scheduler. If `sigmas` is passed,\n+            `num_inference_steps` and `timesteps` must be `None`.\n+\n+    Returns:\n+        `Tuple[torch.Tensor, int]`: A tuple where the first element is the timestep schedule from the scheduler and the\n+        second element is the number of inference steps.\n+    \"\"\"\n+    if timesteps is not None and sigmas is not None:\n+        raise ValueError(\"Only one of `timesteps` or `sigmas` can be passed. Please choose one to set custom values\")\n+    if timesteps is not None:\n+        accepts_timesteps = \"timesteps\" in set(inspect.signature(scheduler.set_timesteps).parameters.keys())\n+        if not accepts_timesteps:\n+            raise ValueError(\n+                f\"The current scheduler class {scheduler.__class__}'s `set_timesteps` does not support custom\"\n+                f\" timestep schedules. Please check whether you are using the correct scheduler.\"\n+            )\n+        scheduler.set_timesteps(timesteps=timesteps, device=device, **kwargs)\n+        timesteps = scheduler.timesteps\n+        num_inference_steps = len(timesteps)\n+    elif sigmas is not None:\n+        accept_sigmas = \"sigmas\" in set(inspect.signature(scheduler.set_timesteps).parameters.keys())\n+        if not accept_sigmas:\n+            raise ValueError(\n+                f\"The current scheduler class {scheduler.__class__}'s `set_timesteps` does not support custom\"\n+                f\" sigmas schedules. Please check whether you are using the correct scheduler.\"\n+            )\n+        scheduler.set_timesteps(sigmas=sigmas, device=device, **kwargs)\n+        timesteps = scheduler.timesteps\n+        num_inference_steps = len(timesteps)\n+    else:\n+        scheduler.set_timesteps(num_inference_steps, device=device, **kwargs)\n+        timesteps = scheduler.timesteps\n+    return timesteps, num_inference_steps\n+\n+\n+class QwenImageControlNetPipeline(DiffusionPipeline, QwenImageLoraLoaderMixin):\n+    r\"\"\"\n+    The QwenImage pipeline for text-to-image generation.\n+\n+    Args:\n+        transformer ([`QwenImageTransformer2DModel`]):\n+            Conditional Transformer (MMDiT) architecture to denoise the encoded image latents.\n+        scheduler ([`FlowMatchEulerDiscreteScheduler`]):\n+            A scheduler to be used in combination with `transformer` to denoise the encoded image latents.\n+        vae ([`AutoencoderKL`]):\n+            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n+        text_encoder ([`Qwen2.5-VL-7B-Instruct`]):\n+            [Qwen2.5-VL-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct), specifically the\n+            [Qwen2.5-VL-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct) variant.\n+        tokenizer (`QwenTokenizer`):\n+            Tokenizer of class\n+            [CLIPTokenizer](https://huggingface.co/docs/transformers/en/model_doc/clip#transformers.CLIPTokenizer).\n+    \"\"\"\n+\n+    model_cpu_offload_seq = \"text_encoder->transformer->vae\"\n+    _callback_tensor_inputs = [\"latents\", \"prompt_embeds\"]\n+\n+    def __init__(\n+        self,\n+        scheduler: FlowMatchEulerDiscreteScheduler,\n+        vae: AutoencoderKLQwenImage,\n+        text_encoder: Qwen2_5_VLForConditionalGeneration,\n+        tokenizer: Qwen2Tokenizer,\n+        transformer: QwenImageTransformer2DModel,\n+        controlnet: Union[QwenImageControlNetModel, QwenImageMultiControlNetModel],\n+    ):\n+        super().__init__()\n+\n+        self.register_modules(\n+            vae=vae,\n+            text_encoder=text_encoder,\n+            tokenizer=tokenizer,\n+            transformer=transformer,\n+            scheduler=scheduler,\n+            controlnet=controlnet,\n+        )\n+        self.vae_scale_factor = 2 ** len(self.vae.temperal_downsample) if getattr(self, \"vae\", None) else 8\n+        # QwenImage latents are turned into 2x2 patches and packed. This means the latent width and height has to be divisible\n+        # by the patch size. So the vae scale factor is multiplied by the patch size to account for this\n+        self.image_processor = VaeImageProcessor(vae_scale_factor=self.vae_scale_factor * 2)\n+        self.tokenizer_max_length = 1024\n+        self.prompt_template_encode = \"<|im_start|>system\\nDescribe the image by detailing the color, shape, size, texture, quantity, text, spatial relationships of the objects and background:<|im_end|>\\n<|im_start|>user\\n{}<|im_end|>\\n<|im_start|>assistant\\n\"\n+        self.prompt_template_encode_start_idx = 34\n+        self.default_sample_size = 128\n+\n+    # Coped from diffusers.pipelines.qwenimage.pipeline_qwenimage.extract_masked_hidden\n+    def _extract_masked_hidden(self, hidden_states: torch.Tensor, mask: torch.Tensor):\n+        bool_mask = mask.bool()\n+        valid_lengths = bool_mask.sum(dim=1)\n+        selected = hidden_states[bool_mask]\n+        split_result = torch.split(selected, valid_lengths.tolist(), dim=0)\n+\n+        return split_result\n+\n+    # Coped from diffusers.pipelines.qwenimage.pipeline_qwenimage.get_qwen_prompt_embeds\n+    def _get_qwen_prompt_embeds(\n+        self,\n+        prompt: Union[str, List[str]] = None,\n+        device: Optional[torch.device] = None,\n+        dtype: Optional[torch.dtype] = None,\n+    ):\n+        device = device or self._execution_device\n+        dtype = dtype or self.text_encoder.dtype\n+\n+        prompt = [prompt] if isinstance(prompt, str) else prompt\n+\n+        template = self.prompt_template_encode\n+        drop_idx = self.prompt_template_encode_start_idx\n+        txt = [template.format(e) for e in prompt]\n+        txt_tokens = self.tokenizer(\n+            txt, max_length=self.tokenizer_max_length + drop_idx, padding=True, truncation=True, return_tensors=\"pt\"\n+        ).to(self.device)\n+        encoder_hidden_states = self.text_encoder(\n+            input_ids=txt_tokens.input_ids,\n+            attention_mask=txt_tokens.attention_mask,\n+            output_hidden_states=True,\n+        )\n+        hidden_states = encoder_hidden_states.hidden_states[-1]\n+        split_hidden_states = self._extract_masked_hidden(hidden_states, txt_tokens.attention_mask)\n+        split_hidden_states = [e[drop_idx:] for e in split_hidden_states]\n+        attn_mask_list = [torch.ones(e.size(0), dtype=torch.long, device=e.device) for e in split_hidden_states]\n+        max_seq_len = max([e.size(0) for e in split_hidden_states])\n+        prompt_embeds = torch.stack(\n+            [torch.cat([u, u.new_zeros(max_seq_len - u.size(0), u.size(1))]) for u in split_hidden_states]\n+        )\n+        encoder_attention_mask = torch.stack(\n+            [torch.cat([u, u.new_zeros(max_seq_len - u.size(0))]) for u in attn_mask_list]\n+        )\n+\n+        prompt_embeds = prompt_embeds.to(dtype=dtype, device=device)\n+\n+        return prompt_embeds, encoder_attention_mask\n+\n+    # Coped from diffusers.pipelines.qwenimage.pipeline_qwenimage.encode_prompt\n+    def encode_prompt(\n+        self,\n+        prompt: Union[str, List[str]],\n+        device: Optional[torch.device] = None,\n+        num_images_per_prompt: int = 1,\n+        prompt_embeds: Optional[torch.Tensor] = None,\n+        prompt_embeds_mask: Optional[torch.Tensor] = None,\n+        max_sequence_length: int = 1024,\n+    ):\n+        r\"\"\"\n+\n+        Args:\n+            prompt (`str` or `List[str]`, *optional*):\n+                prompt to be encoded\n+            device: (`torch.device`):\n+                torch device\n+            num_images_per_prompt (`int`):\n+                number of images that should be generated per prompt\n+            prompt_embeds (`torch.Tensor`, *optional*):\n+                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n+                provided, text embeddings will be generated from `prompt` input argument.\n+        \"\"\"\n+        device = device or self._execution_device\n+\n+        prompt = [prompt] if isinstance(prompt, str) else prompt\n+        batch_size = len(prompt) if prompt_embeds is None else prompt_embeds.shape[0]\n+\n+        if prompt_embeds is None:\n+            prompt_embeds, prompt_embeds_mask = self._get_qwen_prompt_embeds(prompt, device)\n+\n+        _, seq_len, _ = prompt_embeds.shape\n+        prompt_embeds = prompt_embeds.repeat(1, num_images_per_prompt, 1)\n+        prompt_embeds = prompt_embeds.view(batch_size * num_images_per_prompt, seq_len, -1)\n+        prompt_embeds_mask = prompt_embeds_mask.repeat(1, num_images_per_prompt, 1)\n+        prompt_embeds_mask = prompt_embeds_mask.view(batch_size * num_images_per_prompt, seq_len)\n+\n+        return prompt_embeds, prompt_embeds_mask\n+\n+    def check_inputs(\n+        self,\n+        prompt,\n+        height,\n+        width,\n+        negative_prompt=None,\n+        prompt_embeds=None,\n+        negative_prompt_embeds=None,\n+        prompt_embeds_mask=None,\n+        negative_prompt_embeds_mask=None,\n+        callback_on_step_end_tensor_inputs=None,\n+        max_sequence_length=None,\n+    ):\n+        if height % (self.vae_scale_factor * 2) != 0 or width % (self.vae_scale_factor * 2) != 0:\n+            logger.warning(\n+                f\"`height` and `width` have to be divisible by {self.vae_scale_factor * 2} but are {height} and {width}. Dimensions will be resized accordingly\"\n+            )\n+\n+        if callback_on_step_end_tensor_inputs is not None and not all(\n+            k in self._callback_tensor_inputs for k in callback_on_step_end_tensor_inputs\n+        ):\n+            raise ValueError(\n+                f\"`callback_on_step_end_tensor_inputs` has to be in {self._callback_tensor_inputs}, but found {[k for k in callback_on_step_end_tensor_inputs if k not in self._callback_tensor_inputs]}\"\n+            )\n+\n+        if prompt is not None and prompt_embeds is not None:\n+            raise ValueError(\n+                f\"Cannot forward both `prompt`: {prompt} and `prompt_embeds`: {prompt_embeds}. Please make sure to\"\n+                \" only forward one of the two.\"\n+            )\n+        elif prompt is None and prompt_embeds is None:\n+            raise ValueError(\n+                \"Provide either `prompt` or `prompt_embeds`. Cannot leave both `prompt` and `prompt_embeds` undefined.\"\n+            )\n+        elif prompt is not None and (not isinstance(prompt, str) and not isinstance(prompt, list)):\n+            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n+\n+        if negative_prompt is not None and negative_prompt_embeds is not None:\n+            raise ValueError(\n+                f\"Cannot forward both `negative_prompt`: {negative_prompt} and `negative_prompt_embeds`:\"\n+                f\" {negative_prompt_embeds}. Please make sure to only forward one of the two.\"\n+            )\n+\n+        if prompt_embeds is not None and prompt_embeds_mask is None:\n+            raise ValueError(\n+                \"If `prompt_embeds` are provided, `prompt_embeds_mask` also have to be passed. Make sure to generate `prompt_embeds_mask` from the same text encoder that was used to generate `prompt_embeds`.\"\n+            )\n+        if negative_prompt_embeds is not None and negative_prompt_embeds_mask is None:\n+            raise ValueError(\n+                \"If `negative_prompt_embeds` are provided, `negative_prompt_embeds_mask` also have to be passed. Make sure to generate `negative_prompt_embeds_mask` from the same text encoder that was used to generate `negative_prompt_embeds`.\"\n+            )\n+\n+        if max_sequence_length is not None and max_sequence_length > 1024:\n+            raise ValueError(f\"`max_sequence_length` cannot be greater than 1024 but is {max_sequence_length}\")\n+\n+    @staticmethod\n+    # Copied from diffusers.pipelines.qwenimage.pipeline_qwenimage.QwenImagePipeline._pack_latents\n+    def _pack_latents(latents, batch_size, num_channels_latents, height, width):\n+        latents = latents.view(batch_size, num_channels_latents, height // 2, 2, width // 2, 2)\n+        latents = latents.permute(0, 2, 4, 1, 3, 5)\n+        latents = latents.reshape(batch_size, (height // 2) * (width // 2), num_channels_latents * 4)\n+\n+        return latents\n+\n+    @staticmethod\n+    # Copied from diffusers.pipelines.qwenimage.pipeline_qwenimage.QwenImagePipeline._unpack_latents\n+    def _unpack_latents(latents, height, width, vae_scale_factor):\n+        batch_size, num_patches, channels = latents.shape\n+\n+        # VAE applies 8x compression on images but we must also account for packing which requires\n+        # latent height and width to be divisible by 2.\n+        height = 2 * (int(height) // (vae_scale_factor * 2))\n+        width = 2 * (int(width) // (vae_scale_factor * 2))\n+\n+        latents = latents.view(batch_size, height // 2, width // 2, channels // 4, 2, 2)\n+        latents = latents.permute(0, 3, 1, 4, 2, 5)\n+\n+        latents = latents.reshape(batch_size, channels // (2 * 2), 1, height, width)\n+\n+        return latents\n+\n+    def enable_vae_slicing(self):\n+        r\"\"\"\n+        Enable sliced VAE decoding. When this option is enabled, the VAE will split the input tensor in slices to\n+        compute decoding in several steps. This is useful to save some memory and allow larger batch sizes.\n+        \"\"\"\n+        self.vae.enable_slicing()\n+\n+    def disable_vae_slicing(self):\n+        r\"\"\"\n+        Disable sliced VAE decoding. If `enable_vae_slicing` was previously enabled, this method will go back to\n+        computing decoding in one step.\n+        \"\"\"\n+        self.vae.disable_slicing()\n+\n+    def enable_vae_tiling(self):\n+        r\"\"\"\n+        Enable tiled VAE decoding. When this option is enabled, the VAE will split the input tensor into tiles to\n+        compute decoding and encoding in several steps. This is useful for saving a large amount of memory and to allow\n+        processing larger images.\n+        \"\"\"\n+        self.vae.enable_tiling()\n+\n+    def disable_vae_tiling(self):\n+        r\"\"\"\n+        Disable tiled VAE decoding. If `enable_vae_tiling` was previously enabled, this method will go back to\n+        computing decoding in one step.\n+        \"\"\"\n+        self.vae.disable_tiling()\n+\n+    # Copied from diffusers.pipelines.qwenimage.pipeline_qwenimage.QwenImagePipeline.prepare_latents\n+    def prepare_latents(\n+        self,\n+        batch_size,\n+        num_channels_latents,\n+        height,\n+        width,\n+        dtype,\n+        device,\n+        generator,\n+        latents=None,\n+    ):\n+        # VAE applies 8x compression on images but we must also account for packing which requires\n+        # latent height and width to be divisible by 2.\n+        height = 2 * (int(height) // (self.vae_scale_factor * 2))\n+        width = 2 * (int(width) // (self.vae_scale_factor * 2))\n+\n+        shape = (batch_size, 1, num_channels_latents, height, width)\n+\n+        if latents is not None:\n+            return latents.to(device=device, dtype=dtype)\n+\n+        if isinstance(generator, list) and len(generator) != batch_size:\n+            raise ValueError(\n+                f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n+                f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n+            )\n+\n+        latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n+        latents = self._pack_latents(latents, batch_size, num_channels_latents, height, width)\n+\n+        return latents\n+\n+    # Copied from diffusers.pipelines.controlnet_sd3.pipeline_stable_diffusion_3_controlnet.StableDiffusion3ControlNetPipeline.prepare_image\n+    def prepare_image(\n+        self,\n+        image,\n+        width,\n+        height,\n+        batch_size,\n+        num_images_per_prompt,\n+        device,\n+        dtype,\n+        do_classifier_free_guidance=False,\n+        guess_mode=False,\n+    ):\n+        if isinstance(image, torch.Tensor):\n+            pass\n+        else:\n+            image = self.image_processor.preprocess(image, height=height, width=width)\n+\n+        image_batch_size = image.shape[0]\n+\n+        if image_batch_size == 1:\n+            repeat_by = batch_size\n+        else:\n+            # image batch size is the same as prompt batch size\n+            repeat_by = num_images_per_prompt\n+\n+        image = image.repeat_interleave(repeat_by, dim=0)\n+\n+        image = image.to(device=device, dtype=dtype)\n+\n+        if do_classifier_free_guidance and not guess_mode:\n+            image = torch.cat([image] * 2)\n+\n+        return image\n+\n+    @property\n+    def guidance_scale(self):\n+        return self._guidance_scale\n+\n+    @property\n+    def attention_kwargs(self):\n+        return self._attention_kwargs\n+\n+    @property\n+    def num_timesteps(self):\n+        return self._num_timesteps\n+\n+    @property\n+    def current_timestep(self):\n+        return self._current_timestep\n+\n+    @property\n+    def interrupt(self):\n+        return self._interrupt\n+\n+    @torch.no_grad()\n+    @replace_example_docstring(EXAMPLE_DOC_STRING)\n+    def __call__(\n+        self,\n+        prompt: Union[str, List[str]] = None,\n+        negative_prompt: Union[str, List[str]] = None,\n+        true_cfg_scale: float = 4.0,\n+        height: Optional[int] = None,\n+        width: Optional[int] = None,\n+        num_inference_steps: int = 50,\n+        sigmas: Optional[List[float]] = None,\n+        guidance_scale: float = 1.0,\n+        control_guidance_start: Union[float, List[float]] = 0.0,\n+        control_guidance_end: Union[float, List[float]] = 1.0,\n+        control_image: PipelineImageInput = None,\n+        controlnet_conditioning_scale: Union[float, List[float]] = 1.0,\n+        num_images_per_prompt: int = 1,\n+        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n+        latents: Optional[torch.Tensor] = None,\n+        prompt_embeds: Optional[torch.Tensor] = None,\n+        prompt_embeds_mask: Optional[torch.Tensor] = None,\n+        negative_prompt_embeds: Optional[torch.Tensor] = None,\n+        negative_prompt_embeds_mask: Optional[torch.Tensor] = None,\n+        output_type: Optional[str] = \"pil\",\n+        return_dict: bool = True,\n+        attention_kwargs: Optional[Dict[str, Any]] = None,\n+        callback_on_step_end: Optional[Callable[[int, int, Dict], None]] = None,\n+        callback_on_step_end_tensor_inputs: List[str] = [\"latents\"],\n+        max_sequence_length: int = 512,\n+    ):\n+        r\"\"\"\n+        Function invoked when calling the pipeline for generation.\n+\n+        Args:\n+            prompt (`str` or `List[str]`, *optional*):\n+                The prompt or prompts to guide the image generation. If not defined, one has to pass `prompt_embeds`.\n+                instead.\n+            negative_prompt (`str` or `List[str]`, *optional*):\n+                The prompt or prompts not to guide the image generation. If not defined, one has to pass\n+                `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `true_cfg_scale` is\n+                not greater than `1`).\n+            true_cfg_scale (`float`, *optional*, defaults to 1.0):\n+                When > 1.0 and a provided `negative_prompt`, enables true classifier-free guidance.\n+            height (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n+                The height in pixels of the generated image. This is set to 1024 by default for the best results.\n+            width (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n+                The width in pixels of the generated image. This is set to 1024 by default for the best results.\n+            num_inference_steps (`int`, *optional*, defaults to 50):\n+                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n+                expense of slower inference.\n+            sigmas (`List[float]`, *optional*):\n+                Custom sigmas to use for the denoising process with schedulers which support a `sigmas` argument in\n+                their `set_timesteps` method. If not defined, the default behavior when `num_inference_steps` is passed\n+                will be used.\n+            guidance_scale (`float`, *optional*, defaults to 3.5):\n+                Guidance scale as defined in [Classifier-Free Diffusion\n+                Guidance](https://huggingface.co/papers/2207.12598). `guidance_scale` is defined as `w` of equation 2.\n+                of [Imagen Paper](https://huggingface.co/papers/2205.11487). Guidance scale is enabled by setting\n+                `guidance_scale > 1`. Higher guidance scale encourages to generate images that are closely linked to\n+                the text `prompt`, usually at the expense of lower image quality.\n+            num_images_per_prompt (`int`, *optional*, defaults to 1):\n+                The number of images to generate per prompt.\n+            generator (`torch.Generator` or `List[torch.Generator]`, *optional*):\n+                One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)\n+                to make generation deterministic.\n+            latents (`torch.Tensor`, *optional*):\n+                Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for image\n+                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents\n+                tensor will be generated by sampling using the supplied random `generator`.\n+            prompt_embeds (`torch.Tensor`, *optional*):\n+                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n+                provided, text embeddings will be generated from `prompt` input argument.\n+            negative_prompt_embeds (`torch.Tensor`, *optional*):\n+                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt\n+                weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt` input\n+                argument.\n+            output_type (`str`, *optional*, defaults to `\"pil\"`):\n+                The output format of the generate image. Choose between\n+                [PIL](https://pillow.readthedocs.io/en/stable/): `PIL.Image.Image` or `np.array`.\n+            return_dict (`bool`, *optional*, defaults to `True`):\n+                Whether or not to return a [`~pipelines.qwenimage.QwenImagePipelineOutput`] instead of a plain tuple.\n+            attention_kwargs (`dict`, *optional*):\n+                A kwargs dictionary that if specified is passed along to the `AttentionProcessor` as defined under\n+                `self.processor` in\n+                [diffusers.models.attention_processor](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).\n+            callback_on_step_end (`Callable`, *optional*):\n+                A function that calls at the end of each denoising steps during the inference. The function is called\n+                with the following arguments: `callback_on_step_end(self: DiffusionPipeline, step: int, timestep: int,\n+                callback_kwargs: Dict)`. `callback_kwargs` will include a list of all tensors as specified by\n+                `callback_on_step_end_tensor_inputs`.\n+            callback_on_step_end_tensor_inputs (`List`, *optional*):\n+                The list of tensor inputs for the `callback_on_step_end` function. The tensors specified in the list\n+                will be passed as `callback_kwargs` argument. You will only be able to include variables listed in the\n+                `._callback_tensor_inputs` attribute of your pipeline class.\n+            max_sequence_length (`int` defaults to 512): Maximum sequence length to use with the `prompt`.\n+\n+        Examples:\n+\n+        Returns:\n+            [`~pipelines.qwenimage.QwenImagePipelineOutput`] or `tuple`:\n+            [`~pipelines.qwenimage.QwenImagePipelineOutput`] if `return_dict` is True, otherwise a `tuple`. When\n+            returning a tuple, the first element is a list with the generated images.\n+        \"\"\"\n+\n+        height = height or self.default_sample_size * self.vae_scale_factor\n+        width = width or self.default_sample_size * self.vae_scale_factor\n+\n+        if not isinstance(control_guidance_start, list) and isinstance(control_guidance_end, list):\n+            control_guidance_start = len(control_guidance_end) * [control_guidance_start]\n+        elif not isinstance(control_guidance_end, list) and isinstance(control_guidance_start, list):\n+            control_guidance_end = len(control_guidance_start) * [control_guidance_end]\n+        elif not isinstance(control_guidance_start, list) and not isinstance(control_guidance_end, list):\n+            mult = len(control_image) if isinstance(self.controlnet, QwenImageMultiControlNetModel) else 1\n+            control_guidance_start, control_guidance_end = (\n+                mult * [control_guidance_start],\n+                mult * [control_guidance_end],\n+            )\n+\n+        # 1. Check inputs. Raise error if not correct\n+        self.check_inputs(\n+            prompt,\n+            height,\n+            width,\n+            negative_prompt=negative_prompt,\n+            prompt_embeds=prompt_embeds,\n+            negative_prompt_embeds=negative_prompt_embeds,\n+            prompt_embeds_mask=prompt_embeds_mask,\n+            negative_prompt_embeds_mask=negative_prompt_embeds_mask,\n+            callback_on_step_end_tensor_inputs=callback_on_step_end_tensor_inputs,\n+            max_sequence_length=max_sequence_length,\n+        )\n+\n+        self._guidance_scale = guidance_scale\n+        self._attention_kwargs = attention_kwargs\n+        self._current_timestep = None\n+        self._interrupt = False\n+\n+        # 2. Define call parameters\n+        if prompt is not None and isinstance(prompt, str):\n+            batch_size = 1\n+        elif prompt is not None and isinstance(prompt, list):\n+            batch_size = len(prompt)\n+        else:\n+            batch_size = prompt_embeds.shape[0]\n+\n+        device = self._execution_device\n+\n+        has_neg_prompt = negative_prompt is not None or (\n+            negative_prompt_embeds is not None and negative_prompt_embeds_mask is not None\n+        )\n+        do_true_cfg = true_cfg_scale > 1 and has_neg_prompt\n+        prompt_embeds, prompt_embeds_mask = self.encode_prompt(\n+            prompt=prompt,\n+            prompt_embeds=prompt_embeds,\n+            prompt_embeds_mask=prompt_embeds_mask,\n+            device=device,\n+            num_images_per_prompt=num_images_per_prompt,\n+            max_sequence_length=max_sequence_length,\n+        )\n+        if do_true_cfg:\n+            negative_prompt_embeds, negative_prompt_embeds_mask = self.encode_prompt(\n+                prompt=negative_prompt,\n+                prompt_embeds=negative_prompt_embeds,\n+                prompt_embeds_mask=negative_prompt_embeds_mask,\n+                device=device,\n+                num_images_per_prompt=num_images_per_prompt,\n+                max_sequence_length=max_sequence_length,\n+            )\n+\n+        # 3. Prepare control image\n+        num_channels_latents = self.transformer.config.in_channels // 4\n+        if isinstance(self.controlnet, QwenImageControlNetModel):\n+            control_image = self.prepare_image(\n+                image=control_image,\n+                width=width,\n+                height=height,\n+                batch_size=batch_size * num_images_per_prompt,\n+                num_images_per_prompt=num_images_per_prompt,\n+                device=device,\n+                dtype=self.vae.dtype,\n+            )\n+            height, width = control_image.shape[-2:]\n+\n+            if control_image.ndim == 4:\n+                control_image = control_image.unsqueeze(2)\n+\n+            # vae encode\n+            self.vae_scale_factor = 2 ** len(self.vae.temperal_downsample)\n+            latents_mean = (torch.tensor(self.vae.config.latents_mean).view(1, self.vae.config.z_dim, 1, 1, 1)).to(\n+                device\n+            )\n+            latents_std = 1.0 / torch.tensor(self.vae.config.latents_std).view(1, self.vae.config.z_dim, 1, 1, 1).to(\n+                device\n+            )\n+\n+            control_image = retrieve_latents(self.vae.encode(control_image), generator=generator)\n+            control_image = (control_image - latents_mean) * latents_std\n+\n+            control_image = control_image.permute(0, 2, 1, 3, 4)\n+\n+            # pack\n+            control_image = self._pack_latents(\n+                control_image,\n+                batch_size=control_image.shape[0],\n+                num_channels_latents=num_channels_latents,\n+                height=control_image.shape[3],\n+                width=control_image.shape[4],\n+            ).to(dtype=prompt_embeds.dtype, device=device)\n+\n+        else:\n+            if isinstance(self.controlnet, QwenImageMultiControlNetModel):\n+                control_images = []\n+                for control_image_ in control_image:\n+                    control_image_ = self.prepare_image(\n+                        image=control_image_,\n+                        width=width,\n+                        height=height,\n+                        batch_size=batch_size * num_images_per_prompt,\n+                        num_images_per_prompt=num_images_per_prompt,\n+                        device=device,\n+                        dtype=self.vae.dtype,\n+                    )\n+\n+                    height, width = control_image_.shape[-2:]\n+\n+                    if control_image_.ndim == 4:\n+                        control_image_ = control_image_.unsqueeze(2)\n+\n+                    # vae encode\n+                    self.vae_scale_factor = 2 ** len(self.vae.temperal_downsample)\n+                    latents_mean = (\n+                        torch.tensor(self.vae.config.latents_mean).view(1, self.vae.config.z_dim, 1, 1, 1)\n+                    ).to(device)\n+                    latents_std = 1.0 / torch.tensor(self.vae.config.latents_std).view(\n+                        1, self.vae.config.z_dim, 1, 1, 1\n+                    ).to(device)\n+\n+                    control_image_ = retrieve_latents(self.vae.encode(control_image_), generator=generator)\n+                    control_image_ = (control_image_ - latents_mean) * latents_std\n+\n+                    control_image_ = control_image_.permute(0, 2, 1, 3, 4)\n+\n+                    # pack\n+                    control_image_ = self._pack_latents(\n+                        control_image_,\n+                        batch_size=control_image_.shape[0],\n+                        num_channels_latents=num_channels_latents,\n+                        height=control_image_.shape[3],\n+                        width=control_image_.shape[4],\n+                    ).to(dtype=prompt_embeds.dtype, device=device)\n+\n+                    control_images.append(control_image_)\n+\n+                control_image = control_images\n+\n+        # 4. Prepare latent variables\n+        num_channels_latents = self.transformer.config.in_channels // 4\n+        latents = self.prepare_latents(\n+            batch_size * num_images_per_prompt,\n+            num_channels_latents,\n+            height,\n+            width,\n+            prompt_embeds.dtype,\n+            device,\n+            generator,\n+            latents,\n+        )\n+        img_shapes = [(1, height // self.vae_scale_factor // 2, width // self.vae_scale_factor // 2)] * batch_size\n+\n+        # 5. Prepare timesteps\n+        sigmas = np.linspace(1.0, 1 / num_inference_steps, num_inference_steps) if sigmas is None else sigmas\n+        image_seq_len = latents.shape[1]\n+        mu = calculate_shift(\n+            image_seq_len,\n+            self.scheduler.config.get(\"base_image_seq_len\", 256),\n+            self.scheduler.config.get(\"max_image_seq_len\", 4096),\n+            self.scheduler.config.get(\"base_shift\", 0.5),\n+            self.scheduler.config.get(\"max_shift\", 1.15),\n+        )\n+        timesteps, num_inference_steps = retrieve_timesteps(\n+            self.scheduler,\n+            num_inference_steps,\n+            device,\n+            sigmas=sigmas,\n+            mu=mu,\n+        )\n+        num_warmup_steps = max(len(timesteps) - num_inference_steps * self.scheduler.order, 0)\n+        self._num_timesteps = len(timesteps)\n+\n+        controlnet_keep = []\n+        for i in range(len(timesteps)):\n+            keeps = [\n+                1.0 - float(i / len(timesteps) < s or (i + 1) / len(timesteps) > e)\n+                for s, e in zip(control_guidance_start, control_guidance_end)\n+            ]\n+            controlnet_keep.append(keeps[0] if isinstance(self.controlnet, QwenImageControlNetModel) else keeps)\n+\n+        # handle guidance\n+        if self.transformer.config.guidance_embeds:\n+            guidance = torch.full([1], guidance_scale, device=device, dtype=torch.float32)\n+            guidance = guidance.expand(latents.shape[0])\n+        else:\n+            guidance = None\n+\n+        if self.attention_kwargs is None:\n+            self._attention_kwargs = {}\n+\n+        # 6. Denoising loop\n+        self.scheduler.set_begin_index(0)\n+        with self.progress_bar(total=num_inference_steps) as progress_bar:\n+            for i, t in enumerate(timesteps):\n+                if self.interrupt:\n+                    continue\n+\n+                self._current_timestep = t\n+                # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n+                timestep = t.expand(latents.shape[0]).to(latents.dtype)\n+\n+                if isinstance(controlnet_keep[i], list):\n+                    cond_scale = [c * s for c, s in zip(controlnet_conditioning_scale, controlnet_keep[i])]\n+                else:\n+                    controlnet_cond_scale = controlnet_conditioning_scale\n+                    if isinstance(controlnet_cond_scale, list):\n+                        controlnet_cond_scale = controlnet_cond_scale[0]\n+                    cond_scale = controlnet_cond_scale * controlnet_keep[i]\n+\n+                # controlnet\n+                controlnet_block_samples = self.controlnet(\n+                    hidden_states=latents,\n+                    controlnet_cond=control_image,\n+                    conditioning_scale=cond_scale,\n+                    timestep=timestep / 1000,\n+                    encoder_hidden_states=prompt_embeds,\n+                    encoder_hidden_states_mask=prompt_embeds_mask,\n+                    img_shapes=img_shapes,\n+                    txt_seq_lens=prompt_embeds_mask.sum(dim=1).tolist(),\n+                    return_dict=False,\n+                )\n+\n+                with self.transformer.cache_context(\"cond\"):\n+                    noise_pred = self.transformer(\n+                        hidden_states=latents,\n+                        timestep=timestep / 1000,\n+                        encoder_hidden_states=prompt_embeds,\n+                        encoder_hidden_states_mask=prompt_embeds_mask,\n+                        img_shapes=img_shapes,\n+                        txt_seq_lens=prompt_embeds_mask.sum(dim=1).tolist(),\n+                        controlnet_block_samples=controlnet_block_samples,\n+                        attention_kwargs=self.attention_kwargs,\n+                        return_dict=False,\n+                    )[0]\n+\n+                if do_true_cfg:\n+                    with self.transformer.cache_context(\"uncond\"):\n+                        neg_noise_pred = self.transformer(\n+                            hidden_states=latents,\n+                            timestep=timestep / 1000,\n+                            guidance=guidance,\n+                            encoder_hidden_states_mask=negative_prompt_embeds_mask,\n+                            encoder_hidden_states=negative_prompt_embeds,\n+                            img_shapes=img_shapes,\n+                            txt_seq_lens=negative_prompt_embeds_mask.sum(dim=1).tolist(),\n+                            controlnet_block_samples=controlnet_block_samples,\n+                            attention_kwargs=self.attention_kwargs,\n+                            return_dict=False,\n+                        )[0]\n+                    comb_pred = neg_noise_pred + true_cfg_scale * (noise_pred - neg_noise_pred)\n+\n+                    cond_norm = torch.norm(noise_pred, dim=-1, keepdim=True)\n+                    noise_norm = torch.norm(comb_pred, dim=-1, keepdim=True)\n+                    noise_pred = comb_pred * (cond_norm / noise_norm)\n+\n+                # compute the previous noisy sample x_t -> x_t-1\n+                latents_dtype = latents.dtype\n+                latents = self.scheduler.step(noise_pred, t, latents, return_dict=False)[0]\n+\n+                if latents.dtype != latents_dtype:\n+                    if torch.backends.mps.is_available():\n+                        # some platforms (eg. apple mps) misbehave due to a pytorch bug: https://github.com/pytorch/pytorch/pull/99272\n+                        latents = latents.to(latents_dtype)\n+\n+                if callback_on_step_end is not None:\n+                    callback_kwargs = {}\n+                    for k in callback_on_step_end_tensor_inputs:\n+                        callback_kwargs[k] = locals()[k]\n+                    callback_outputs = callback_on_step_end(self, i, t, callback_kwargs)\n+\n+                    latents = callback_outputs.pop(\"latents\", latents)\n+                    prompt_embeds = callback_outputs.pop(\"prompt_embeds\", prompt_embeds)\n+\n+                # call the callback, if provided\n+                if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n+                    progress_bar.update()\n+\n+                if XLA_AVAILABLE:\n+                    xm.mark_step()\n+\n+        self._current_timestep = None\n+        if output_type == \"latent\":\n+            image = latents\n+        else:\n+            latents = self._unpack_latents(latents, height, width, self.vae_scale_factor)\n+            latents = latents.to(self.vae.dtype)\n+            latents_mean = (\n+                torch.tensor(self.vae.config.latents_mean)\n+                .view(1, self.vae.config.z_dim, 1, 1, 1)\n+                .to(latents.device, latents.dtype)\n+            )\n+            latents_std = 1.0 / torch.tensor(self.vae.config.latents_std).view(1, self.vae.config.z_dim, 1, 1, 1).to(\n+                latents.device, latents.dtype\n+            )\n+            latents = latents / latents_std + latents_mean\n+            image = self.vae.decode(latents, return_dict=False)[0][:, :, 0]\n+            image = self.image_processor.postprocess(image, output_type=output_type)\n+\n+        # Offload all models\n+        self.maybe_free_model_hooks()\n+\n+        if not return_dict:\n+            return (image,)\n+\n+        return QwenImagePipelineOutput(images=image)"
        },
        {
          "filename": "src/diffusers/utils/dummy_pt_objects.py",
          "status": "modified",
          "additions": 30,
          "deletions": 0,
          "changes": 30,
          "patch": "@@ -1083,6 +1083,36 @@ def from_pretrained(cls, *args, **kwargs):\n         requires_backends(cls, [\"torch\"])\n \n \n+class QwenImageControlNetModel(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+    @classmethod\n+    def from_config(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\"])\n+\n+    @classmethod\n+    def from_pretrained(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\"])\n+\n+\n+class QwenImageMultiControlNetModel(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+    @classmethod\n+    def from_config(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\"])\n+\n+    @classmethod\n+    def from_pretrained(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\"])\n+\n+\n class QwenImageTransformer2DModel(metaclass=DummyObject):\n     _backends = [\"torch\"]\n "
        },
        {
          "filename": "src/diffusers/utils/dummy_torch_and_transformers_objects.py",
          "status": "modified",
          "additions": 15,
          "deletions": 0,
          "changes": 15,
          "patch": "@@ -1757,6 +1757,21 @@ def from_pretrained(cls, *args, **kwargs):\n         requires_backends(cls, [\"torch\", \"transformers\"])\n \n \n+class QwenImageControlNetPipeline(metaclass=DummyObject):\n+    _backends = [\"torch\", \"transformers\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_config(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_pretrained(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+\n class QwenImageEditPipeline(metaclass=DummyObject):\n     _backends = [\"torch\", \"transformers\"]\n "
        }
      ],
      "num_files": 12,
      "scraped_at": "2025-11-16T21:19:45.659849"
    },
    {
      "pr_number": 12209,
      "title": "NPU attention refactor for FLUX",
      "body": "# What does this PR do?\r\n\r\nChange the _attention_backend to NPU by using enable npu flash attention method.\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/diffusers/blob/main/CONTRIBUTING.md)?\r\n- [x] Did you read our [philosophy doc](https://github.com/huggingface/diffusers/blob/main/PHILOSOPHY.md) (important for complex PRs)?\r\n- [ ] Was this discussed/approved via a GitHub issue or the [forum](https://discuss.huggingface.co/c/discussion-related-to-httpsgithubcomhuggingfacediffusers/63)? Please add a link to it if that's the case.\r\n- [x] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/diffusers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/diffusers/tree/main/docs#writing-source-documentation).\r\n- [x] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @.\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nCore library:\r\n\r\n- Schedulers: @yiyixuxu\r\n- Pipelines and pipeline callbacks: @yiyixuxu and @asomoza\r\n- Training examples: @sayakpaul\r\n- Docs: @stevhliu and @sayakpaul\r\n- JAX and MPS: @pcuenca\r\n- Audio: @sanchit-gandhi\r\n- General functionalities: @sayakpaul @yiyixuxu @DN6\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @SunMarc\r\n- PEFT: @sayakpaul @BenjaminBossan\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- transformers: [different repo](https://github.com/huggingface/transformers)\r\n- safetensors: [different repo](https://github.com/huggingface/safetensors)\r\n\r\n-->\r\n",
      "html_url": "https://github.com/huggingface/diffusers/pull/12209",
      "created_at": "2025-08-21T11:22:33Z",
      "merged_at": "2025-08-26T07:23:55Z",
      "merge_commit_sha": "0fd7ee79ea54304a9e04921e5c8c841e1765de73",
      "base_ref": "main",
      "head_sha": "e52f6664c8748511099e4f4553c1a670d29a9b36",
      "user": "leisuzz",
      "files": [
        {
          "filename": "examples/dreambooth/train_dreambooth_flux.py",
          "status": "modified",
          "additions": 8,
          "deletions": 0,
          "changes": 8,
          "patch": "@@ -642,6 +642,7 @@ def parse_args(input_args=None):\n         ],\n         help=\"The image interpolation method to use for resizing images.\",\n     )\n+    parser.add_argument(\"--enable_npu_flash_attention\", action=\"store_true\", help=\"Enabla Flash Attention for NPU\")\n \n     if input_args is not None:\n         args = parser.parse_args(input_args)\n@@ -1182,6 +1183,13 @@ def main(args):\n         text_encoder_one.requires_grad_(False)\n         text_encoder_two.requires_grad_(False)\n \n+    if args.enable_npu_flash_attention:\n+        if is_torch_npu_available():\n+            logger.info(\"npu flash attention enabled.\")\n+            transformer.set_attention_backend(\"_native_npu\")\n+        else:\n+            raise ValueError(\"npu flash attention requires torch_npu extensions and is supported only on npu device \")\n+\n     # For mixed precision training we cast all non-trainable weights (vae, text_encoder and transformer) to half-precision\n     # as these weights are only used for inference, keeping weights in full precision is not required.\n     weight_dtype = torch.float32"
        },
        {
          "filename": "examples/dreambooth/train_dreambooth_lora_flux.py",
          "status": "modified",
          "additions": 9,
          "deletions": 0,
          "changes": 9,
          "patch": "@@ -80,6 +80,7 @@\n     is_wandb_available,\n )\n from diffusers.utils.hub_utils import load_or_create_model_card, populate_model_card\n+from diffusers.utils.import_utils import is_torch_npu_available\n from diffusers.utils.torch_utils import is_compiled_module\n \n \n@@ -686,6 +687,7 @@ def parse_args(input_args=None):\n         ),\n     )\n     parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"For distributed training: local_rank\")\n+    parser.add_argument(\"--enable_npu_flash_attention\", action=\"store_true\", help=\"Enabla Flash Attention for NPU\")\n \n     if input_args is not None:\n         args = parser.parse_args(input_args)\n@@ -1213,6 +1215,13 @@ def main(args):\n     text_encoder_one.requires_grad_(False)\n     text_encoder_two.requires_grad_(False)\n \n+    if args.enable_npu_flash_attention:\n+        if is_torch_npu_available():\n+            logger.info(\"npu flash attention enabled.\")\n+            transformer.set_attention_backend(\"_native_npu\")\n+        else:\n+            raise ValueError(\"npu flash attention requires torch_npu extensions and is supported only on npu device \")\n+\n     # For mixed precision training we cast all non-trainable weights (vae, text_encoder and transformer) to half-precision\n     # as these weights are only used for inference, keeping weights in full precision is not required.\n     weight_dtype = torch.float32"
        },
        {
          "filename": "examples/dreambooth/train_dreambooth_lora_flux_kontext.py",
          "status": "modified",
          "additions": 8,
          "deletions": 0,
          "changes": 8,
          "patch": "@@ -706,6 +706,7 @@ def parse_args(input_args=None):\n         ),\n     )\n     parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"For distributed training: local_rank\")\n+    parser.add_argument(\"--enable_npu_flash_attention\", action=\"store_true\", help=\"Enabla Flash Attention for NPU\")\n \n     if input_args is not None:\n         args = parser.parse_args(input_args)\n@@ -1354,6 +1355,13 @@ def main(args):\n     text_encoder_one.requires_grad_(False)\n     text_encoder_two.requires_grad_(False)\n \n+    if args.enable_npu_flash_attention:\n+        if is_torch_npu_available():\n+            logger.info(\"npu flash attention enabled.\")\n+            transformer.set_attention_backend(\"_native_npu\")\n+        else:\n+            raise ValueError(\"npu flash attention requires torch_npu extensions and is supported only on npu device \")\n+\n     # For mixed precision training we cast all non-trainable weights (vae, text_encoder and transformer) to half-precision\n     # as these weights are only used for inference, keeping weights in full precision is not required.\n     weight_dtype = torch.float32"
        },
        {
          "filename": "src/diffusers/models/transformers/transformer_flux.py",
          "status": "modified",
          "additions": 2,
          "deletions": 15,
          "changes": 17,
          "patch": "@@ -22,8 +22,7 @@\n \n from ...configuration_utils import ConfigMixin, register_to_config\n from ...loaders import FluxTransformer2DLoadersMixin, FromOriginalModelMixin, PeftAdapterMixin\n-from ...utils import USE_PEFT_BACKEND, deprecate, logging, scale_lora_layers, unscale_lora_layers\n-from ...utils.import_utils import is_torch_npu_available\n+from ...utils import USE_PEFT_BACKEND, logging, scale_lora_layers, unscale_lora_layers\n from ...utils.torch_utils import maybe_allow_in_graph\n from ..attention import AttentionMixin, AttentionModuleMixin, FeedForward\n from ..attention_dispatch import dispatch_attention_fn\n@@ -354,25 +353,13 @@ def __init__(self, dim: int, num_attention_heads: int, attention_head_dim: int,\n         self.act_mlp = nn.GELU(approximate=\"tanh\")\n         self.proj_out = nn.Linear(dim + self.mlp_hidden_dim, dim)\n \n-        if is_torch_npu_available():\n-            from ..attention_processor import FluxAttnProcessor2_0_NPU\n-\n-            deprecation_message = (\n-                \"Defaulting to FluxAttnProcessor2_0_NPU for NPU devices will be removed. Attention processors \"\n-                \"should be set explicitly using the `set_attn_processor` method.\"\n-            )\n-            deprecate(\"npu_processor\", \"0.34.0\", deprecation_message)\n-            processor = FluxAttnProcessor2_0_NPU()\n-        else:\n-            processor = FluxAttnProcessor()\n-\n         self.attn = FluxAttention(\n             query_dim=dim,\n             dim_head=attention_head_dim,\n             heads=num_attention_heads,\n             out_dim=dim,\n             bias=True,\n-            processor=processor,\n+            processor=FluxAttnProcessor(),\n             eps=1e-6,\n             pre_only=True,\n         )"
        }
      ],
      "num_files": 4,
      "scraped_at": "2025-11-16T21:19:47.227600"
    }
  ],
  "last_updated": "2025-11-16T21:19:51.320369"
}