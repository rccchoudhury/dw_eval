[
  {
    "pr_number": 17031,
    "title": "ggml webgpu: faster matrix multiplication/matrix-vector multiplication",
    "body": "Adds the following\r\n* Two matrix multiplication implementations: one using register tiling and the other \"subgroup matrices\" (WebGPU's feature to allow access to tensor cores/optimized subgroup (warp) routines on devices that have them). Currently, subgroup matrices are experimental, and on devices where it's not supported, the code will fall back to the register tiling approach\r\n* A somewhat sped up matrix vector multiplication (still needs some work, but it's a decent start I think)\r\n* Support for f32/f16/q4_0 for this code, but set up in a way that I think will make integration of other quantization types easier.\r\n* Updates the dawn version the WebGPU backend is built against\r\n* Moving to a new format for pipeline initialization, with the eventual goal of making initialization lazy/smarter so we don't carry around a ton of compiled shaders that are never used in the browser\r\n\r\nSome preliminary performance numbers on my M3:\r\n\r\n**Llama-3.2-1B-Instruct-F16**\r\n\r\n**WebGPU:**\r\n```\r\n| model                          |       size |     params | backend    | ngl |            test |                  t/s |\r\n| ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: |\r\n| llama 1B F16                   |   2.30 GiB |     1.24 B | WebGPU     |  99 |           pp512 |       1014.17 \u00b1 9.38 |\r\n| llama 1B F16                   |   2.30 GiB |     1.24 B | WebGPU     |  99 |           tg128 |         28.71 \u00b1 0.19 |\r\n```\r\n\r\n**Metal:**\r\n```\r\n| model                          |       size |     params | backend    | ngl |            test |                  t/s |\r\n| ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: |\r\n| llama 1B F16                   |   2.30 GiB |     1.24 B | Metal      |  99 |           pp512 |       1368.47 \u00b1 0.95 |\r\n| llama 1B F16                   |   2.30 GiB |     1.24 B | Metal      |  99 |           tg128 |         35.99 \u00b1 0.78 |\r\n```\r\n\r\n**Llama-3.2-1B-Instruct-Q4_0**\r\n\r\n**WebGPU:**\r\n```\r\n| model                          |       size |     params | backend    | ngl |            test |                  t/s |\r\n| ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: |\r\n| llama 1B Q4_0                  | 729.75 MiB |     1.24 B | WebGPU     |  99 |           pp512 |        960.52 \u00b1 6.05 |\r\n| llama 1B Q4_0                  | 729.75 MiB |     1.24 B | WebGPU     |  99 |           tg128 |         41.76 \u00b1 0.62 |\r\n```\r\n\r\n**Metal:**\r\n```\r\n| model                          |       size |     params | backend    | ngl |            test |                  t/s |\r\n| ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: |\r\n| llama 1B Q4_0                  | 729.75 MiB |     1.24 B | Metal      |  99 |           pp512 |       1346.68 \u00b1 1.21 |\r\n| llama 1B Q4_0                  | 729.75 MiB |     1.24 B | Metal      |  99 |           tg128 |        103.92 \u00b1 0.37 |\r\n```\r\n",
    "html_url": "https://github.com/ggml-org/llama.cpp/pull/17031",
    "created_at": "2025-11-05T17:02:24Z",
    "merged_at": "2025-11-08T03:27:20Z",
    "merge_commit_sha": "647b960bd8017ee882d6633bc2e43e2ae82ee85c",
    "base_ref": "master",
    "head_sha": "7c2b2ef237d0c877ec4115f2429feaa52853c5cc",
    "user": "reeselevine",
    "files": [
      {
        "filename": ".github/workflows/build.yml",
        "status": "modified",
        "additions": 10,
        "deletions": 8,
        "changes": 18,
        "patch": "@@ -161,15 +161,16 @@ jobs:\n       - name: Dawn Dependency\n         id: dawn-depends\n         run: |\n-          DAWN_VERSION=\"v1.0.0\"\n+          DAWN_VERSION=\"v2.0.0\"\n           DAWN_OWNER=\"reeselevine\"\n           DAWN_REPO=\"dawn\"\n-          DAWN_ASSET_NAME=\"Dawn-a1a6b45cced25a3b7f4fb491e0ae70796cc7f22b-macos-latest-Release.tar.gz\"\n+          DAWN_ASSET_NAME=\"Dawn-5e9a4865b1635796ccc77dd30057f2b4002a1355-macos-latest-Release.zip\"\n           echo \"Fetching release asset from https://github.com/${DAWN_OWNER}/${DAWN_REPO}/releases/download/${DAWN_VERSION}/${DAWN_ASSET_NAME}\"\n-          curl -L -o artifact.tar.gz \\\n+          curl -L -o artifact.zip \\\n             \"https://github.com/${DAWN_OWNER}/${DAWN_REPO}/releases/download/${DAWN_VERSION}/${DAWN_ASSET_NAME}\"\n           mkdir dawn\n-          tar -xvf artifact.tar.gz -C dawn --strip-components=1\n+          unzip artifact.zip\n+          tar -xvf Dawn-5e9a4865b1635796ccc77dd30057f2b4002a1355-macos-latest-Release.tar.gz -C dawn --strip-components=1\n \n       - name: Build\n         id: cmake_build\n@@ -521,15 +522,16 @@ jobs:\n         id: dawn-depends\n         run: |\n           sudo apt-get install -y libxrandr-dev libxinerama-dev libxcursor-dev mesa-common-dev libx11-xcb-dev libxi-dev\n-          DAWN_VERSION=\"v1.0.0\"\n+          DAWN_VERSION=\"v2.0.0\"\n           DAWN_OWNER=\"reeselevine\"\n           DAWN_REPO=\"dawn\"\n-          DAWN_ASSET_NAME=\"Dawn-a1a6b45cced25a3b7f4fb491e0ae70796cc7f22b-ubuntu-latest-Release.tar.gz\"\n+          DAWN_ASSET_NAME=\"Dawn-5e9a4865b1635796ccc77dd30057f2b4002a1355-ubuntu-latest-Release.zip\"\n           echo \"Fetching release asset from https://github.com/${DAWN_OWNER}/${DAWN_REPO}/releases/download/${DAWN_VERSION}/${DAWN_ASSET_NAME}\"\n-          curl -L -o artifact.tar.gz \\\n+          curl -L -o artifact.zip \\\n             \"https://github.com/${DAWN_OWNER}/${DAWN_REPO}/releases/download/${DAWN_VERSION}/${DAWN_ASSET_NAME}\"\n           mkdir dawn\n-          tar -xvf artifact.tar.gz -C dawn --strip-components=1\n+          unzip artifact.zip\n+          tar -xvf Dawn-5e9a4865b1635796ccc77dd30057f2b4002a1355-ubuntu-latest-Release.tar.gz -C dawn --strip-components=1\n \n       - name: Build\n         id: cmake_build"
      },
      {
        "filename": "ggml/src/ggml-webgpu/ggml-webgpu.cpp",
        "status": "modified",
        "additions": 313,
        "deletions": 13,
        "changes": 326,
        "patch": "@@ -15,6 +15,7 @@\n #include <condition_variable>\n #include <cstring>\n #include <iostream>\n+#include <map>\n #include <mutex>\n #include <optional>\n #include <string>\n@@ -73,6 +74,30 @@\n // For operations which process a row in parallel, this seems like a reasonable default\n #define WEBGPU_ROW_SPLIT_WG_SIZE 64\n \n+// Matrix multiplication parameters\n+\n+// Register tiling parameters\n+#define WEBGPU_MUL_MAT_TILE_M    8\n+#define WEBGPU_MUL_MAT_TILE_N    8\n+#define WEBGPU_MUL_MAT_WG_SIZE_M 8\n+#define WEBGPU_MUL_MAT_WG_SIZE_N 8\n+#define WEBGPU_MUL_MAT_TILE_K    32\n+\n+// Subgroup matrix parameters\n+// The number of subgroups in the M dimension\n+#define WEBGPU_MUL_MAT_SUBGROUP_M        2\n+// The number of subgroups in the N dimension\n+#define WEBGPU_MUL_MAT_SUBGROUP_N        2\n+// The number of subgroup matrices each subgroup accumulates over\n+#define WEBGPU_MUL_MAT_SUBGROUP_MATRIX_M 4\n+#define WEBGPU_MUL_MAT_SUBGROUP_MATRIX_N 2\n+\n+// Matrix-vector multiplication parameters\n+#define WEBGPU_MUL_MAT_VEC_WG_SIZE        256\n+// Must be multiple of 4 to work with vectorized paths, and must divide mul_mat_vec wg size\n+#define WEBGPU_MUL_MAT_VEC_OUTPUTS_PER_WG 64\n+#define WEBGPU_MUL_MAT_VEC_TILE_K         256\n+\n /* End Constants */\n \n // This is a \"fake\" base pointer, since WebGPU buffers do not have pointers to their locations.\n@@ -236,6 +261,10 @@ struct webgpu_context_struct {\n     wgpu::Queue    queue;\n     wgpu::Limits   limits;\n \n+    bool                       supports_subgroup_matrix = false;\n+    uint32_t                   subgroup_size;\n+    wgpu::SubgroupMatrixConfig subgroup_matrix_config;\n+\n     // Separate this out from limits since on some Metal systems, the limit returned by\n     // querying the limits is higher than the actual allowed maximum.\n     uint32_t max_wg_size_x;\n@@ -247,6 +276,11 @@ struct webgpu_context_struct {\n     webgpu_buf_pool set_rows_error_buf_pool;\n \n     webgpu_pipeline memset_pipeline;\n+\n+    std::map<int, std::map<int, std::map<int, webgpu_pipeline>>> mul_mat_pipelines;  // src0_type, src1_type, vectorized\n+    std::map<int, std::map<int, std::map<int, webgpu_pipeline>>>\n+        mul_mat_vec_pipelines;                                                       // src0_type, src1_type, vectorized\n+\n     webgpu_pipeline mul_mat_pipeline[30][2];\n     webgpu_pipeline set_rows_pipeline[1][2];  // dst->type, vectorized\n     webgpu_pipeline get_rows_pipeline[30];\n@@ -321,6 +355,25 @@ struct ggml_backend_webgpu_buffer_context {\n \n /* WebGPU object initializations */\n \n+// Process a WGSL shader string, replacing tokens of the form {{KEY}} with\n+// the corresponding values provided in `repls`.\n+static std::string ggml_webgpu_process_shader_repls(const char *                               src,\n+                                                    const std::map<std::string, std::string> & repls) {\n+    if (!src) {\n+        return std::string();\n+    }\n+    std::string s = src;\n+    for (const auto & kv : repls) {\n+        std::string token = \"{{\" + kv.first + \"}}\";\n+        size_t      pos   = 0;\n+        while ((pos = s.find(token, pos)) != std::string::npos) {\n+            s.replace(pos, token.length(), kv.second);\n+            pos += kv.second.length();\n+        }\n+    }\n+    return s;\n+}\n+\n static void ggml_webgpu_create_pipeline(wgpu::Device &                           device,\n                                         webgpu_pipeline &                        pipeline,\n                                         const char *                             shader_code,\n@@ -346,6 +399,30 @@ static void ggml_webgpu_create_pipeline(wgpu::Device &\n     pipeline = { device.CreateComputePipeline(&pipeline_desc), label };\n }\n \n+static webgpu_pipeline ggml_webgpu_create_pipeline2(wgpu::Device &                           device,\n+                                                    const char *                             shader_code,\n+                                                    const char *                             label,\n+                                                    const std::vector<wgpu::ConstantEntry> & constants = {}) {\n+    wgpu::ShaderSourceWGSL shader_source;\n+    shader_source.code = shader_code;\n+\n+    wgpu::ShaderModuleDescriptor shader_desc;\n+    shader_desc.nextInChain = &shader_source;\n+\n+    wgpu::ShaderModule shader_module = device.CreateShaderModule(&shader_desc);\n+\n+    wgpu::ComputePipelineDescriptor pipeline_desc;\n+    pipeline_desc.label              = label;\n+    pipeline_desc.compute.module     = shader_module;\n+    pipeline_desc.compute.entryPoint = \"main\";   // Entry point in the WGSL code\n+    pipeline_desc.layout             = nullptr;  // nullptr means auto layout\n+    if (constants.size() > 0) {\n+        pipeline_desc.compute.constants     = constants.data();\n+        pipeline_desc.compute.constantCount = constants.size();\n+    }\n+    return { device.CreateComputePipeline(&pipeline_desc), label };\n+}\n+\n static void ggml_webgpu_create_buffer(wgpu::Device &    device,\n                                       wgpu::Buffer &    buffer,\n                                       size_t            size,\n@@ -512,6 +589,7 @@ static webgpu_command ggml_backend_webgpu_build(webgpu_context &\n                                                 std::vector<uint32_t>             params,\n                                                 std::vector<wgpu::BindGroupEntry> bind_group_entries,\n                                                 uint32_t                          wg_x,\n+                                                uint32_t                          wg_y                = 1,\n                                                 std::optional<webgpu_pool_bufs>   set_rows_error_bufs = std::nullopt) {\n     webgpu_pool_bufs params_bufs = ctx->param_buf_pool.alloc_bufs();\n \n@@ -557,7 +635,7 @@ static webgpu_command ggml_backend_webgpu_build(webgpu_context &\n #endif\n     pass.SetPipeline(pipeline.pipeline);\n     pass.SetBindGroup(0, bind_group);\n-    pass.DispatchWorkgroups(wg_x, 1, 1);\n+    pass.DispatchWorkgroups(wg_x, wg_y, 1);\n     pass.End();\n \n #ifdef GGML_WEBGPU_GPU_PROFILE\n@@ -779,7 +857,7 @@ static std::optional<webgpu_command> ggml_webgpu_set_rows(webgpu_context & ctx,\n \n     uint32_t wg_x = (threads + max_wg_size - 1) / max_wg_size;\n \n-    return ggml_backend_webgpu_build(ctx, pipeline, params, entries, wg_x, error_bufs);\n+    return ggml_backend_webgpu_build(ctx, pipeline, params, entries, wg_x, 1, error_bufs);\n }\n \n static webgpu_command ggml_webgpu_get_rows(webgpu_context & ctx,\n@@ -835,8 +913,8 @@ static webgpu_command ggml_webgpu_mul_mat(webgpu_context & ctx,\n         (uint32_t) (ggml_webgpu_tensor_misalignment(ctx, src0) / ggml_type_size(src0->type)),\n         (uint32_t) (ggml_webgpu_tensor_misalignment(ctx, src1) / ggml_type_size(src1->type)),\n         (uint32_t) (ggml_webgpu_tensor_misalignment(ctx, dst) / ggml_type_size(dst->type)),\n-        (uint32_t) dst->ne[1],                                  // number of rows in result (M)\n-        (uint32_t) dst->ne[0],                                  // number of columns in result (N)\n+        (uint32_t) dst->ne[0],                                  // number of rows in result (M, transposed)\n+        (uint32_t) dst->ne[1],                                  // number of columns in result (N)\n         (uint32_t) src0->ne[0],                                 // number of columns in src0/src1 (K)\n         (uint32_t) (src0->nb[1] / ggml_type_size(src0->type)),  // stride (elements/blocks) of src0 in dimension 1\n         (uint32_t) (src1->nb[1] / ggml_type_size(src1->type)),  // stride (elements/blocks) of src1 in dimension 1\n@@ -865,9 +943,67 @@ static webgpu_command ggml_webgpu_mul_mat(webgpu_context & ctx,\n          .size    = ggml_webgpu_tensor_binding_size(ctx, dst)  },\n     };\n \n+    webgpu_pipeline pipeline = ctx->mul_mat_pipeline[src0->type][src1->type];\n+\n     uint32_t wg_x =\n         (dst->ne[0] * dst->ne[1] * dst->ne[2] * dst->ne[3] + WEBGPU_MUL_MAT_WG_SIZE - 1) / WEBGPU_MUL_MAT_WG_SIZE;\n-    return ggml_backend_webgpu_build(ctx, ctx->mul_mat_pipeline[src0->type][src1->type], params, entries, wg_x);\n+    uint32_t wg_y = 1;\n+\n+    bool use_fast = false;\n+    switch (src1->type) {\n+        case GGML_TYPE_F16:\n+            use_fast = (src0->type == GGML_TYPE_F16);\n+            break;\n+        case GGML_TYPE_F32:\n+            switch (src0->type) {\n+                case GGML_TYPE_F32:\n+                case GGML_TYPE_F16:\n+                case GGML_TYPE_Q4_0:\n+                    use_fast = true;\n+                    break;\n+                default:\n+                    break;\n+            }\n+            break;\n+        default:\n+            break;\n+    }\n+\n+    if (use_fast) {\n+        int vectorized = src0->ne[0] % 4 == 0 && dst->ne[0] % 4 == 0 && dst->ne[1] % 4 == 0;\n+        if (dst->ne[1] == 1) {\n+            // We don't support vectorized mul_mat_vec for quantized types\n+            vectorized       = vectorized && (src0->type < 2);\n+            pipeline         = ctx->mul_mat_vec_pipelines[src0->type][src1->type][vectorized];\n+            uint32_t batches = dst->ne[2] * dst->ne[3];\n+            uint32_t output_groups =\n+                (dst->ne[0] + WEBGPU_MUL_MAT_VEC_OUTPUTS_PER_WG - 1) / WEBGPU_MUL_MAT_VEC_OUTPUTS_PER_WG;\n+            uint32_t total_wg = output_groups * batches;\n+            wg_x              = total_wg % ctx->limits.maxComputeWorkgroupsPerDimension;\n+            wg_y              = (total_wg + ctx->limits.maxComputeWorkgroupsPerDimension - 1) /\n+                   ctx->limits.maxComputeWorkgroupsPerDimension;\n+        } else {\n+            pipeline = ctx->mul_mat_pipelines[src0->type][src1->type][vectorized];\n+            uint32_t wg_m;\n+            uint32_t wg_n;\n+            if (ctx->supports_subgroup_matrix) {\n+                // The total number of subgroups/workgroups needed per matrix.\n+                uint32_t wg_m_sg_tile =\n+                    WEBGPU_MUL_MAT_SUBGROUP_M * WEBGPU_MUL_MAT_SUBGROUP_MATRIX_M * ctx->subgroup_matrix_config.M;\n+                wg_m = (dst->ne[0] + wg_m_sg_tile - 1) / wg_m_sg_tile;\n+                uint32_t wg_n_sg_tile =\n+                    WEBGPU_MUL_MAT_SUBGROUP_N * WEBGPU_MUL_MAT_SUBGROUP_MATRIX_N * ctx->subgroup_matrix_config.N;\n+                wg_n = (dst->ne[1] + wg_n_sg_tile - 1) / wg_n_sg_tile;\n+            } else {\n+                uint32_t tile_m_s = WEBGPU_MUL_MAT_TILE_M * WEBGPU_MUL_MAT_WG_SIZE_M;\n+                uint32_t tile_n_s = WEBGPU_MUL_MAT_TILE_N * WEBGPU_MUL_MAT_WG_SIZE_N;\n+                wg_m              = (dst->ne[0] + tile_m_s - 1) / tile_m_s;\n+                wg_n              = (dst->ne[1] + tile_n_s - 1) / tile_n_s;\n+            }\n+            wg_x = wg_m * wg_n * dst->ne[2] * dst->ne[3];\n+        }\n+    }\n+    return ggml_backend_webgpu_build(ctx, pipeline, params, entries, wg_x, wg_y);\n }\n \n static webgpu_command ggml_webgpu_binary_op(webgpu_context &  ctx,\n@@ -1583,12 +1719,6 @@ static void ggml_webgpu_init_memset_pipeline(webgpu_context & webgpu_ctx) {\n }\n \n static void ggml_webgpu_init_mul_mat_pipeline(webgpu_context & webgpu_ctx) {\n-    ggml_webgpu_create_pipeline(webgpu_ctx->device, webgpu_ctx->mul_mat_pipeline[GGML_TYPE_F32][GGML_TYPE_F32],\n-                                wgsl_mul_mat_f32_f32, \"mul_mat_f32_f32\");\n-    ggml_webgpu_create_pipeline(webgpu_ctx->device, webgpu_ctx->mul_mat_pipeline[GGML_TYPE_F16][GGML_TYPE_F16],\n-                                wgsl_mul_mat_f16_f16, \"mul_mat_f16_f16\");\n-    ggml_webgpu_create_pipeline(webgpu_ctx->device, webgpu_ctx->mul_mat_pipeline[GGML_TYPE_F16][GGML_TYPE_F32],\n-                                wgsl_mul_mat_f16_f32, \"mul_mat_f16_f32\");\n     ggml_webgpu_create_pipeline(webgpu_ctx->device, webgpu_ctx->mul_mat_pipeline[GGML_TYPE_Q4_0][GGML_TYPE_F32],\n                                 wgsl_mul_mat_q4_0_f32, \"mul_mat_q4_0_f32\");\n     ggml_webgpu_create_pipeline(webgpu_ctx->device, webgpu_ctx->mul_mat_pipeline[GGML_TYPE_Q4_1][GGML_TYPE_F32],\n@@ -1627,6 +1757,136 @@ static void ggml_webgpu_init_mul_mat_pipeline(webgpu_context & webgpu_ctx) {\n                                 wgsl_mul_mat_iq4_nl_f32, \"mul_mat_iq4_nl_f32\");\n     ggml_webgpu_create_pipeline(webgpu_ctx->device, webgpu_ctx->mul_mat_pipeline[GGML_TYPE_IQ4_XS][GGML_TYPE_F32],\n                                 wgsl_mul_mat_iq4_xs_f32, \"mul_mat_iq4_xs_f32\");\n+\n+    if (webgpu_ctx->supports_subgroup_matrix) {\n+        std::map<std::string, std::string> sg_matrix_repls;\n+        sg_matrix_repls[\"WEBGPU_MAX_SUBGROUP_SIZE\"] = std::to_string(webgpu_ctx->subgroup_size);\n+        sg_matrix_repls[\"WEBGPU_TILE_K\"]            = std::to_string(WEBGPU_MUL_MAT_TILE_K);\n+        sg_matrix_repls[\"WEBGPU_SUBGROUP_M\"]        = std::to_string(WEBGPU_MUL_MAT_SUBGROUP_M);\n+        sg_matrix_repls[\"WEBGPU_SUBGROUP_N\"]        = std::to_string(WEBGPU_MUL_MAT_SUBGROUP_N);\n+        sg_matrix_repls[\"WEBGPU_SUBGROUP_MATRIX_M\"] = std::to_string(WEBGPU_MUL_MAT_SUBGROUP_MATRIX_M);\n+        sg_matrix_repls[\"WEBGPU_SUBGROUP_MATRIX_N\"] = std::to_string(WEBGPU_MUL_MAT_SUBGROUP_MATRIX_N);\n+        sg_matrix_repls[\"WEBGPU_SG_MAT_M_SIZE\"]     = std::to_string(webgpu_ctx->subgroup_matrix_config.M);\n+        sg_matrix_repls[\"WEBGPU_SG_MAT_N_SIZE\"]     = std::to_string(webgpu_ctx->subgroup_matrix_config.N);\n+        sg_matrix_repls[\"WEBGPU_SG_MAT_K_SIZE\"]     = std::to_string(webgpu_ctx->subgroup_matrix_config.K);\n+\n+        std::string proc_mul_mat_subgroup_matrix_f32_f32 =\n+            ggml_webgpu_process_shader_repls(wgsl_mul_mat_subgroup_matrix_f32_f32, sg_matrix_repls);\n+        std::string proc_mul_mat_subgroup_matrix_f32_f32_vec =\n+            ggml_webgpu_process_shader_repls(wgsl_mul_mat_subgroup_matrix_f32_f32_vec, sg_matrix_repls);\n+        std::string proc_mul_mat_subgroup_matrix_f16_f32 =\n+            ggml_webgpu_process_shader_repls(wgsl_mul_mat_subgroup_matrix_f16_f32, sg_matrix_repls);\n+        std::string proc_mul_mat_subgroup_matrix_f16_f32_vec =\n+            ggml_webgpu_process_shader_repls(wgsl_mul_mat_subgroup_matrix_f16_f32_vec, sg_matrix_repls);\n+        std::string proc_mul_mat_subgroup_matrix_f16_f16 =\n+            ggml_webgpu_process_shader_repls(wgsl_mul_mat_subgroup_matrix_f16_f16, sg_matrix_repls);\n+        std::string proc_mul_mat_subgroup_matrix_f16_f16_vec =\n+            ggml_webgpu_process_shader_repls(wgsl_mul_mat_subgroup_matrix_f16_f16_vec, sg_matrix_repls);\n+        std::string proc_mul_mat_subgroup_matrix_q4_0_f32 =\n+            ggml_webgpu_process_shader_repls(wgsl_mul_mat_subgroup_matrix_q4_0_f32, sg_matrix_repls);\n+        std::string proc_mul_mat_subgroup_matrix_q4_0_f32_vec =\n+            ggml_webgpu_process_shader_repls(wgsl_mul_mat_subgroup_matrix_q4_0_f32_vec, sg_matrix_repls);\n+\n+        webgpu_ctx->mul_mat_pipelines[GGML_TYPE_F32][GGML_TYPE_F32][0] = ggml_webgpu_create_pipeline2(\n+            webgpu_ctx->device, proc_mul_mat_subgroup_matrix_f32_f32.c_str(), \"mul_mat_subgroup_matrix_f32_f32\");\n+        webgpu_ctx->mul_mat_pipelines[GGML_TYPE_F32][GGML_TYPE_F32][1] =\n+            ggml_webgpu_create_pipeline2(webgpu_ctx->device, proc_mul_mat_subgroup_matrix_f32_f32_vec.c_str(),\n+                                         \"mul_mat_subgroup_matrix_f32_f32_vec\");\n+        webgpu_ctx->mul_mat_pipelines[GGML_TYPE_F16][GGML_TYPE_F32][0] = ggml_webgpu_create_pipeline2(\n+            webgpu_ctx->device, proc_mul_mat_subgroup_matrix_f16_f32.c_str(), \"mul_mat_subgroup_matrix_f16_f32\");\n+        webgpu_ctx->mul_mat_pipelines[GGML_TYPE_F16][GGML_TYPE_F32][1] =\n+            ggml_webgpu_create_pipeline2(webgpu_ctx->device, proc_mul_mat_subgroup_matrix_f16_f32_vec.c_str(),\n+                                         \"mul_mat_subgroup_matrix_f16_f32_vec\");\n+        webgpu_ctx->mul_mat_pipelines[GGML_TYPE_F16][GGML_TYPE_F16][0] = ggml_webgpu_create_pipeline2(\n+            webgpu_ctx->device, proc_mul_mat_subgroup_matrix_f16_f16.c_str(), \"mul_mat_subgroup_matrix_f16_f16\");\n+        webgpu_ctx->mul_mat_pipelines[GGML_TYPE_F16][GGML_TYPE_F16][1] =\n+            ggml_webgpu_create_pipeline2(webgpu_ctx->device, proc_mul_mat_subgroup_matrix_f16_f16_vec.c_str(),\n+                                         \"mul_mat_subgroup_matrix_f16_f16_vec\");\n+        webgpu_ctx->mul_mat_pipelines[GGML_TYPE_Q4_0][GGML_TYPE_F32][0] = ggml_webgpu_create_pipeline2(\n+            webgpu_ctx->device, proc_mul_mat_subgroup_matrix_q4_0_f32.c_str(), \"mul_mat_subgroup_matrix_q4_0_f32\");\n+        webgpu_ctx->mul_mat_pipelines[GGML_TYPE_Q4_0][GGML_TYPE_F32][1] =\n+            ggml_webgpu_create_pipeline2(webgpu_ctx->device, proc_mul_mat_subgroup_matrix_q4_0_f32_vec.c_str(),\n+                                         \"mul_mat_subgroup_matrix_q4_0_f32_vec\");\n+    } else {\n+        std::vector<wgpu::ConstantEntry> mul_mat_reg_tile_constants(3);\n+        mul_mat_reg_tile_constants[0].key   = \"TILE_K\";\n+        mul_mat_reg_tile_constants[0].value = WEBGPU_MUL_MAT_TILE_K;\n+        mul_mat_reg_tile_constants[1].key   = \"WORKGROUP_SIZE_M\";\n+        mul_mat_reg_tile_constants[1].value = WEBGPU_MUL_MAT_WG_SIZE_M;\n+        mul_mat_reg_tile_constants[2].key   = \"WORKGROUP_SIZE_N\";\n+        mul_mat_reg_tile_constants[2].value = WEBGPU_MUL_MAT_WG_SIZE_N;\n+\n+        std::map<std::string, std::string> reg_repls;\n+        reg_repls[\"WEBGPU_TILE_M\"] = std::to_string(WEBGPU_MUL_MAT_TILE_M);\n+        reg_repls[\"WEBGPU_TILE_N\"] = std::to_string(WEBGPU_MUL_MAT_TILE_N);\n+\n+        // Process each reg-tile shader with tile replacements.\n+        // Keep the processed strings in-scope so .c_str() remains valid.\n+        std::string proc_mul_mat_reg_tile_f32_f32 =\n+            ggml_webgpu_process_shader_repls(wgsl_mul_mat_reg_tile_f32_f32, reg_repls);\n+        std::string proc_mul_mat_reg_tile_f32_f32_vec =\n+            ggml_webgpu_process_shader_repls(wgsl_mul_mat_reg_tile_f32_f32_vec, reg_repls);\n+        std::string proc_mul_mat_reg_tile_f16_f32 =\n+            ggml_webgpu_process_shader_repls(wgsl_mul_mat_reg_tile_f16_f32, reg_repls);\n+        std::string proc_mul_mat_reg_tile_f16_f32_vec =\n+            ggml_webgpu_process_shader_repls(wgsl_mul_mat_reg_tile_f16_f32_vec, reg_repls);\n+        std::string proc_mul_mat_reg_tile_f16_f16 =\n+            ggml_webgpu_process_shader_repls(wgsl_mul_mat_reg_tile_f16_f16, reg_repls);\n+        std::string proc_mul_mat_reg_tile_f16_f16_vec =\n+            ggml_webgpu_process_shader_repls(wgsl_mul_mat_reg_tile_f16_f16_vec, reg_repls);\n+        std::string proc_mul_mat_reg_tile_q4_0_f32 =\n+            ggml_webgpu_process_shader_repls(wgsl_mul_mat_reg_tile_q4_0_f32, reg_repls);\n+        std::string proc_mul_mat_reg_tile_q4_0_f32_vec =\n+            ggml_webgpu_process_shader_repls(wgsl_mul_mat_reg_tile_q4_0_f32_vec, reg_repls);\n+\n+        webgpu_ctx->mul_mat_pipelines[GGML_TYPE_F32][GGML_TYPE_F32][0] =\n+            ggml_webgpu_create_pipeline2(webgpu_ctx->device, proc_mul_mat_reg_tile_f32_f32.c_str(),\n+                                         \"mul_mat_reg_tile_f32_f32\", mul_mat_reg_tile_constants);\n+        webgpu_ctx->mul_mat_pipelines[GGML_TYPE_F32][GGML_TYPE_F32][1] =\n+            ggml_webgpu_create_pipeline2(webgpu_ctx->device, proc_mul_mat_reg_tile_f32_f32_vec.c_str(),\n+                                         \"mul_mat_reg_tile_f32_f32_vec\", mul_mat_reg_tile_constants);\n+        webgpu_ctx->mul_mat_pipelines[GGML_TYPE_F16][GGML_TYPE_F32][0] =\n+            ggml_webgpu_create_pipeline2(webgpu_ctx->device, proc_mul_mat_reg_tile_f16_f32.c_str(),\n+                                         \"mul_mat_reg_tile_f16_f32\", mul_mat_reg_tile_constants);\n+        webgpu_ctx->mul_mat_pipelines[GGML_TYPE_F16][GGML_TYPE_F32][1] =\n+            ggml_webgpu_create_pipeline2(webgpu_ctx->device, proc_mul_mat_reg_tile_f16_f32_vec.c_str(),\n+                                         \"mul_mat_reg_tile_f16_f32_vec\", mul_mat_reg_tile_constants);\n+        webgpu_ctx->mul_mat_pipelines[GGML_TYPE_F16][GGML_TYPE_F16][0] =\n+            ggml_webgpu_create_pipeline2(webgpu_ctx->device, proc_mul_mat_reg_tile_f16_f16.c_str(),\n+                                         \"mul_mat_reg_tile_f16_f16\", mul_mat_reg_tile_constants);\n+        webgpu_ctx->mul_mat_pipelines[GGML_TYPE_F16][GGML_TYPE_F16][1] =\n+            ggml_webgpu_create_pipeline2(webgpu_ctx->device, proc_mul_mat_reg_tile_f16_f16_vec.c_str(),\n+                                         \"mul_mat_reg_tile_f16_f16_vec\", mul_mat_reg_tile_constants);\n+        webgpu_ctx->mul_mat_pipelines[GGML_TYPE_Q4_0][GGML_TYPE_F32][0] =\n+            ggml_webgpu_create_pipeline2(webgpu_ctx->device, proc_mul_mat_reg_tile_q4_0_f32.c_str(),\n+                                         \"mul_mat_reg_tile_q4_0_f32\", mul_mat_reg_tile_constants);\n+        webgpu_ctx->mul_mat_pipelines[GGML_TYPE_Q4_0][GGML_TYPE_F32][1] =\n+            ggml_webgpu_create_pipeline2(webgpu_ctx->device, proc_mul_mat_reg_tile_q4_0_f32_vec.c_str(),\n+                                         \"mul_mat_reg_tile_q4_0_f32_vec\", mul_mat_reg_tile_constants);\n+    }\n+\n+    std::vector<wgpu::ConstantEntry> mul_mat_vec_constants(3);\n+    mul_mat_vec_constants[0].key   = \"WORKGROUP_SIZE\";\n+    mul_mat_vec_constants[0].value = WEBGPU_MUL_MAT_VEC_WG_SIZE;\n+    mul_mat_vec_constants[1].key   = \"TILE_K\";\n+    mul_mat_vec_constants[1].value = WEBGPU_MUL_MAT_VEC_TILE_K;\n+    mul_mat_vec_constants[2].key   = \"OUTPUTS_PER_WG\";\n+    mul_mat_vec_constants[2].value = WEBGPU_MUL_MAT_VEC_OUTPUTS_PER_WG;\n+\n+    webgpu_ctx->mul_mat_vec_pipelines[GGML_TYPE_F32][GGML_TYPE_F32][0] = ggml_webgpu_create_pipeline2(\n+        webgpu_ctx->device, wgsl_mul_mat_vec_f32_f32, \"mul_mat_vec_f32_f32\", mul_mat_vec_constants);\n+    webgpu_ctx->mul_mat_vec_pipelines[GGML_TYPE_F32][GGML_TYPE_F32][1] = ggml_webgpu_create_pipeline2(\n+        webgpu_ctx->device, wgsl_mul_mat_vec_f32_f32_vec, \"mul_mat_vec_f32_f32_vec\", mul_mat_vec_constants);\n+    webgpu_ctx->mul_mat_vec_pipelines[GGML_TYPE_F16][GGML_TYPE_F32][0] = ggml_webgpu_create_pipeline2(\n+        webgpu_ctx->device, wgsl_mul_mat_vec_f16_f32, \"mul_mat_vec_f16_f32\", mul_mat_vec_constants);\n+    webgpu_ctx->mul_mat_vec_pipelines[GGML_TYPE_F16][GGML_TYPE_F32][1] = ggml_webgpu_create_pipeline2(\n+        webgpu_ctx->device, wgsl_mul_mat_vec_f16_f32_vec, \"mul_mat_vec_f16_f32_vec\", mul_mat_vec_constants);\n+    webgpu_ctx->mul_mat_vec_pipelines[GGML_TYPE_F16][GGML_TYPE_F16][0] = ggml_webgpu_create_pipeline2(\n+        webgpu_ctx->device, wgsl_mul_mat_vec_f16_f16, \"mul_mat_vec_f16_f16\", mul_mat_vec_constants);\n+    webgpu_ctx->mul_mat_vec_pipelines[GGML_TYPE_F16][GGML_TYPE_F16][1] = ggml_webgpu_create_pipeline2(\n+        webgpu_ctx->device, wgsl_mul_mat_vec_f16_f16_vec, \"mul_mat_vec_f16_f16_vec\", mul_mat_vec_constants);\n+    webgpu_ctx->mul_mat_vec_pipelines[GGML_TYPE_Q4_0][GGML_TYPE_F32][0] = ggml_webgpu_create_pipeline2(\n+        webgpu_ctx->device, wgsl_mul_mat_vec_q4_0_f32, \"mul_mat_vec_q4_0_f32\", mul_mat_vec_constants);\n }\n \n static void ggml_webgpu_init_set_rows_pipeline(webgpu_context & webgpu_ctx) {\n@@ -2124,7 +2384,13 @@ static ggml_backend_dev_t ggml_backend_webgpu_reg_get_device(ggml_backend_reg_t\n \n     webgpu_context ctx = reg_ctx->webgpu_ctx;\n \n-    wgpu::RequestAdapterOptions options = {};\n+    // TODO: track need for these toggles: https://issues.chromium.org/issues/42251215\n+    const char * const          adapterEnabledToggles[] = { \"vulkan_enable_f16_on_nvidia\", \"use_vulkan_memory_model\" };\n+    wgpu::DawnTogglesDescriptor adapterTogglesDesc;\n+    adapterTogglesDesc.enabledToggles     = adapterEnabledToggles;\n+    adapterTogglesDesc.enabledToggleCount = 2;\n+    wgpu::RequestAdapterOptions options   = {};\n+    options.nextInChain                   = &adapterTogglesDesc;\n     ctx->instance.WaitAny(ctx->instance.RequestAdapter(\n                               &options, wgpu::CallbackMode::AllowSpontaneous,\n                               [&ctx](wgpu::RequestAdapterStatus status, wgpu::Adapter adapter, const char * message) {\n@@ -2140,12 +2406,46 @@ static ggml_backend_dev_t ggml_backend_webgpu_reg_get_device(ggml_backend_reg_t\n     ctx->adapter.GetLimits(&ctx->limits);\n     ctx->max_wg_size_x = 288;  // default value\n \n-    wgpu::AdapterInfo info{};\n+    wgpu::AdapterInfo                            info{};\n+    wgpu::AdapterPropertiesSubgroupMatrixConfigs subgroup_matrix_configs{};\n+    if (ctx->adapter.HasFeature(wgpu::FeatureName::ChromiumExperimentalSubgroupMatrix)) {\n+        info.nextInChain = &subgroup_matrix_configs;\n+    }\n     ctx->adapter.GetInfo(&info);\n \n+    wgpu::SupportedFeatures features;\n+    ctx->adapter.GetFeatures(&features);\n+    // we require f16 support\n+    GGML_ASSERT(ctx->adapter.HasFeature(wgpu::FeatureName::ShaderF16));\n+\n+    // Only support square f16 matrices of size 8 or 16 for now\n+    bool valid_subgroup_matrix_config = false;\n+    if (ctx->adapter.HasFeature(wgpu::FeatureName::ChromiumExperimentalSubgroupMatrix)) {\n+        for (size_t i = 0; i < subgroup_matrix_configs.configCount; i++) {\n+            const wgpu::SubgroupMatrixConfig config = subgroup_matrix_configs.configs[i];\n+            if (config.M == config.N && config.N == config.K && (config.K == 8 || config.K == 16) &&\n+                config.componentType == wgpu::SubgroupMatrixComponentType::F16 &&\n+                config.resultComponentType == wgpu::SubgroupMatrixComponentType::F16) {\n+                ctx->subgroup_matrix_config  = config;\n+                valid_subgroup_matrix_config = true;\n+                break;\n+            }\n+        }\n+    }\n+\n+    // For subgroup matrix code to be the most efficient, we would like the subgroup size to be consistent and accurate.\n+    // Unfortunately, that is not possible, so we use the maximum subgroup size reported by the adapter.\n+    ctx->subgroup_size            = info.subgroupMaxSize;\n+    ctx->supports_subgroup_matrix = valid_subgroup_matrix_config;\n+\n     // Initialize device\n     std::vector<wgpu::FeatureName> required_features = { wgpu::FeatureName::ShaderF16,\n                                                          wgpu::FeatureName::ImplicitDeviceSynchronization };\n+    if (ctx->supports_subgroup_matrix) {\n+        required_features.push_back(wgpu::FeatureName::Subgroups);\n+        required_features.push_back(wgpu::FeatureName::ChromiumExperimentalSubgroupMatrix);\n+    }\n+\n #ifdef GGML_WEBGPU_GPU_PROFILE\n     required_features.push_back(wgpu::FeatureName::TimestampQuery);\n #endif"
      },
      {
        "filename": "ggml/src/ggml-webgpu/wgsl-shaders/embed_wgsl.py",
        "status": "modified",
        "additions": 6,
        "deletions": 3,
        "changes": 9,
        "patch": "@@ -72,9 +72,12 @@ def generate_variants(fname, input_dir, output_dir, outfile):\n         except ValueError:\n             decls_map = {}\n \n-        with open(os.path.join(input_dir, \"common_decls.tmpl\"), \"r\", encoding=\"utf-8\") as f:\n-            common_decls = f.read()\n-        decls_map.update(parse_decls(common_decls))\n+        for fname in sorted(os.listdir(input_dir)):\n+            if fname.endswith(\".tmpl\"):\n+                tmpl_path = os.path.join(input_dir, fname)\n+                with open(tmpl_path, \"r\", encoding=\"utf-8\") as f_tmpl:\n+                    decls = f_tmpl.read()\n+                    decls_map.update(parse_decls(decls))\n \n         shader_template = extract_block(text, \"SHADER\")\n         for variant in variants:"
      },
      {
        "filename": "ggml/src/ggml-webgpu/wgsl-shaders/mul_mat.tmpl.wgsl",
        "status": "modified",
        "additions": 5,
        "deletions": 5,
        "changes": 10,
        "patch": "@@ -864,8 +864,8 @@ struct MulMatParams {\n     broadcast3: u32\n };\n \n-@group(0) @binding(0) var<storage, read_write> src0: array<{{SRC0_TYPE}}>; // N rows, K columns\n-@group(0) @binding(1) var<storage, read_write> src1: array<{{SRC1_TYPE}}>; // M rows, K columns (transposed)\n+@group(0) @binding(0) var<storage, read_write> src0: array<{{SRC0_TYPE}}>; // M rows, K columns\n+@group(0) @binding(1) var<storage, read_write> src1: array<{{SRC1_TYPE}}>; // K rows, N columns (transposed)\n @group(0) @binding(2) var<storage, read_write> dst: array<f32>; // M rows, N columns\n \n @group(0) @binding(3) var<uniform> params: MulMatParams;\n@@ -891,8 +891,8 @@ fn main(@builtin(global_invocation_id) global_id: vec3<u32>) {\n \n     let dst2_rem = dst3_rem % dst2_stride;\n \n-    let row = dst2_rem / params.n; // output row\n-    let col = dst2_rem % params.n; // output column\n+    let row = dst2_rem / params.m; // output row\n+    let col = dst2_rem % params.m; // output column\n \n     let src0_idx_base = params.offset_src0 + src03_idx * params.stride_03 + src02_idx * params.stride_02 + col * params.stride_01;\n     let src1_idx_base = params.offset_src1 + src13_idx * params.stride_13 + src12_idx * params.stride_12 + row * params.stride_11;\n@@ -901,7 +901,7 @@ fn main(@builtin(global_invocation_id) global_id: vec3<u32>) {\n     for (var i: u32 = 0u; i < params.k/{{BLOCK_SIZE}}; i = i + 1u) {\n         sum += multiply_add(src0_idx_base, src1_idx_base, i);\n     }\n-    dst[params.offset_dst + dst3_idx * dst3_stride + dst2_idx * dst2_stride + row * params.n + col] = sum;\n+    dst[params.offset_dst + dst3_idx * dst3_stride + dst2_idx * dst2_stride + row * params.m + col] = sum;\n }\n \n #end(SHADER)"
      },
      {
        "filename": "ggml/src/ggml-webgpu/wgsl-shaders/mul_mat_decls.tmpl",
        "status": "added",
        "additions": 97,
        "deletions": 0,
        "changes": 97,
        "patch": "@@ -0,0 +1,97 @@\n+#decl(SHMEM_VEC)\n+fn store_shmem(val: vec4<f16>, idx: u32) {\n+    shmem[idx] = val.x;\n+    shmem[idx + 1] = val.y;\n+    shmem[idx + 2] = val.z;\n+    shmem[idx + 3] = val.w;\n+}\n+#enddecl(SHMEM_VEC)\n+\n+#decl(SHMEM_SCALAR)\n+fn store_shmem(val: f16, idx: u32) {\n+    shmem[idx] = val;\n+}\n+#enddecl(SHMEM_SCALAR)\n+\n+#decl(INIT_SRC0_SHMEM_FLOAT)\n+\n+fn init_shmem_src0(thread_id: u32, batch_offset: u32, offset_m: u32, k_outer: u32) {\n+    for (var elem_idx = thread_id * {{VEC_SIZE}}; elem_idx < TILE_SRC0_SHMEM; elem_idx += TOTAL_WORKGROUP_SIZE * {{VEC_SIZE}}) {\n+        let tile_m = elem_idx / TILE_K;\n+        let tile_k = elem_idx % TILE_K;\n+        let global_m = offset_m + tile_m;\n+        let global_k = k_outer + tile_k;\n+        let src0_idx = batch_offset + global_m * params.stride_01 + global_k;\n+        let src0_val = select( // taking a slight performance hit to avoid oob\n+            {{SRC0_TYPE}}(0.0),\n+            src0[src0_idx/{{VEC_SIZE}}],\n+            global_m < params.m && global_k < params.k);\n+        store_shmem({{SHMEM_TYPE}}(src0_val), elem_idx);\n+    }\n+}\n+\n+#enddecl(INIT_SRC0_SHMEM_FLOAT)\n+\n+#decl(INIT_SRC1_SHMEM)\n+\n+fn init_shmem_src1(thread_id: u32, batch_offset: u32, offset_n: u32, k_outer: u32) {\n+    for (var elem_idx = thread_id * {{VEC_SIZE}}; elem_idx < TILE_SRC1_SHMEM; elem_idx += TOTAL_WORKGROUP_SIZE * {{VEC_SIZE}}) {\n+        let tile_n = elem_idx / TILE_K;\n+        let tile_k = elem_idx % TILE_K;\n+        let global_n = offset_n + tile_n;\n+        let global_k = k_outer + tile_k;\n+        let src1_idx = batch_offset + global_n * params.stride_11 + global_k;\n+        let src1_val = select(\n+            {{SRC1_TYPE}}(0.0),\n+            src1[src1_idx/{{VEC_SIZE}}],\n+            global_n < params.n && global_k < params.k);\n+        store_shmem({{SHMEM_TYPE}}(src1_val), TILE_SRC0_SHMEM + elem_idx);\n+    }\n+}\n+\n+#enddecl(INIT_SRC1_SHMEM)\n+\n+#decl(INIT_SRC0_SHMEM_Q4_0)\n+\n+const BLOCK_SIZE = 32u;\n+// the number of blocks per k-tile. Note that this currently only works if TILE_K is a multiple of BLOCK_SIZE, which may need to be rethought for larger quantized types.\n+override BLOCKS_K = TILE_K/BLOCK_SIZE;\n+const NQ = 16u;\n+const F16_PER_BLOCK = 9u; // 1 scale + 8x4 packed weights\n+const WEIGHTS_PER_F16 = 4u; // 4 weights per f16\n+const F16_PER_THREAD = NQ / WEIGHTS_PER_F16;\n+\n+fn init_shmem_src0(thread_id: u32, batch_offset: u32, offset_m: u32, k_outer: u32) {\n+    for (var i = thread_id * NQ; i < TILE_SRC0_SHMEM; i += TOTAL_WORKGROUP_SIZE * NQ) {\n+        let blck_idx = i / BLOCK_SIZE;\n+        let block_offset = (i % BLOCK_SIZE) / WEIGHTS_PER_F16;\n+        let shmem_idx = blck_idx * BLOCK_SIZE + block_offset * 2u;\n+\n+        let tile_m = blck_idx / BLOCKS_K;\n+        let global_m = offset_m + tile_m;\n+        let block_k = blck_idx % BLOCKS_K;\n+        let global_k = k_outer / BLOCK_SIZE + block_k;\n+\n+        if (global_m < params.m && global_k < params.k / BLOCK_SIZE) {\n+            let src0_idx = batch_offset + global_m * params.stride_01 + global_k;\n+            let scale_idx = src0_idx * F16_PER_BLOCK;\n+            let d = src0[scale_idx];\n+\n+            for (var j = 0u; j < F16_PER_THREAD; j += 2) {\n+                let q_0 = src0[scale_idx + 1u + block_offset + j];\n+                let q_1 = src0[scale_idx + 1u + block_offset + j + 1];\n+\n+                let q_packed = bitcast<u32>(vec2(q_0, q_1));\n+                for (var k = 0u; k < 4u; k++) {\n+                    let q_byte = get_byte(q_packed, k);\n+                    let q_hi = (f16((q_byte >> 4) & 0xF) - 8.0) * d;\n+                    let q_lo = (f16(q_byte & 0xF) - 8.0) * d;\n+                    shmem[shmem_idx + j * 2 + k] = q_lo;\n+                    shmem[shmem_idx + j * 2 + k + 16u] = q_hi;\n+                }\n+            }\n+        }\n+    }\n+}\n+\n+#enddecl(INIT_SRC0_SHMEM_Q4_0)"
      },
      {
        "filename": "ggml/src/ggml-webgpu/wgsl-shaders/mul_mat_reg_tile.tmpl.wgsl",
        "status": "added",
        "additions": 247,
        "deletions": 0,
        "changes": 247,
        "patch": "@@ -0,0 +1,247 @@\n+#define(VARIANTS)\n+[\n+  {\n+    \"SHADER_SUFFIX\": \"f32_f32_vec\",\n+    \"REPLS\": {\n+      \"SRC0_TYPE\" : \"vec4<f32>\",\n+      \"SRC1_TYPE\" : \"vec4<f32>\",\n+      \"DST_TYPE\" : \"vec4<f32>\",\n+      \"SHMEM_TYPE\" : \"vec4<f16>\",\n+      \"VEC_SIZE\" : 4,\n+    },\n+    \"DECLS\": [\"VEC\", \"SHMEM_VEC\", \"INIT_SRC0_SHMEM_FLOAT\", \"INIT_SRC1_SHMEM\"]\n+  },\n+  {\n+    \"SHADER_SUFFIX\": \"f32_f32\",\n+    \"REPLS\": {\n+      \"SRC0_TYPE\" : \"f32\",\n+      \"SRC1_TYPE\" : \"f32\",\n+      \"DST_TYPE\" : \"f32\",\n+      \"SHMEM_TYPE\" : \"f16\",\n+      \"VEC_SIZE\" : 1,\n+    },\n+    \"DECLS\": [\"SCALAR\", \"SHMEM_SCALAR\", \"INIT_SRC0_SHMEM_FLOAT\", \"INIT_SRC1_SHMEM\"]\n+  },\n+  {\n+    \"SHADER_SUFFIX\": \"f16_f32_vec\",\n+    \"REPLS\": {\n+      \"SRC0_TYPE\" : \"vec4<f16>\",\n+      \"SRC1_TYPE\" : \"vec4<f32>\",\n+      \"DST_TYPE\" : \"vec4<f32>\",\n+      \"SHMEM_TYPE\" : \"vec4<f16>\",\n+      \"VEC_SIZE\" : 4,\n+    },\n+    \"DECLS\": [\"VEC\", \"SHMEM_VEC\", \"INIT_SRC0_SHMEM_FLOAT\", \"INIT_SRC1_SHMEM\"]\n+  },\n+  {\n+    \"SHADER_SUFFIX\": \"f16_f32\",\n+    \"REPLS\": {\n+      \"SRC0_TYPE\" : \"f16\",\n+      \"SRC1_TYPE\" : \"f32\",\n+      \"DST_TYPE\" : \"f32\",\n+      \"SHMEM_TYPE\" : \"f16\",\n+      \"VEC_SIZE\" : 1,\n+    },\n+    \"DECLS\": [\"SCALAR\", \"SHMEM_SCALAR\", \"INIT_SRC0_SHMEM_FLOAT\", \"INIT_SRC1_SHMEM\"]\n+  },\n+  {\n+    \"SHADER_SUFFIX\": \"f16_f16_vec\",\n+    \"REPLS\": {\n+      \"SRC0_TYPE\" : \"vec4<f16>\",\n+      \"SRC1_TYPE\" : \"vec4<f16>\",\n+      \"DST_TYPE\" : \"vec4<f32>\",\n+      \"SHMEM_TYPE\" : \"vec4<f16>\",\n+      \"VEC_SIZE\" : 4,\n+    },\n+    \"DECLS\": [\"VEC\", \"SHMEM_VEC\", \"INIT_SRC0_SHMEM_FLOAT\", \"INIT_SRC1_SHMEM\"]\n+  },\n+  {\n+    \"SHADER_SUFFIX\": \"f16_f16\",\n+    \"REPLS\": {\n+      \"SRC0_TYPE\" : \"f16\",\n+      \"SRC1_TYPE\" : \"f16\",\n+      \"DST_TYPE\" : \"f32\",\n+      \"SHMEM_TYPE\" : \"f16\",\n+      \"VEC_SIZE\" : 1,\n+    },\n+    \"DECLS\": [\"SCALAR\", \"SHMEM_SCALAR\", \"INIT_SRC0_SHMEM_FLOAT\", \"INIT_SRC1_SHMEM\"]\n+  },\n+  {\n+    \"SHADER_SUFFIX\": \"q4_0_f32_vec\",\n+    \"REPLS\": {\n+      \"SRC0_TYPE\" : \"f16\",\n+      \"SRC1_TYPE\" : \"vec4<f32>\",\n+      \"DST_TYPE\" : \"vec4<f32>\",\n+      \"SHMEM_TYPE\" : \"vec4<f16>\",\n+      \"VEC_SIZE\" : 4,\n+    },\n+    \"DECLS\": [\"BYTE_HELPERS\", \"VEC\", \"SHMEM_VEC\", \"INIT_SRC0_SHMEM_Q4_0\", \"INIT_SRC1_SHMEM\"]\n+  },\n+  {\n+    \"SHADER_SUFFIX\": \"q4_0_f32\",\n+    \"REPLS\": {\n+      \"SRC0_TYPE\" : \"f16\",\n+      \"SRC1_TYPE\" : \"f32\",\n+      \"DST_TYPE\" : \"f32\",\n+      \"SHMEM_TYPE\" : \"f16\",\n+      \"VEC_SIZE\" : 1,\n+    },\n+    \"DECLS\": [\"BYTE_HELPERS\", \"SCALAR\", \"SHMEM_SCALAR\", \"INIT_SRC0_SHMEM_Q4_0\", \"INIT_SRC1_SHMEM\"]\n+  }\n+]\n+\n+#end(VARIANTS)\n+\n+#define(DECLS)\n+\n+#decl(VEC)\n+fn store_val(acc: array<array<f16, TILE_N>, TILE_M>, tn: u32, tm: u32) -> vec4<f32> {\n+    return vec4<f32>(f32(acc[tm][tn]), f32(acc[tm + 1][tn]), f32(acc[tm + 2][tn]), f32(acc[tm + 3][tn]));\n+}\n+#enddecl(VEC)\n+\n+#decl(SCALAR)\n+fn store_val(acc: array<array<f16, TILE_N>, TILE_M>, tn: u32, tm: u32) -> f32 {\n+    return f32(acc[tm][tn]);\n+}\n+#enddecl(SCALAR)\n+\n+#end(DECLS)\n+\n+#define(SHADER)\n+enable f16;\n+\n+struct MulMatParams {\n+    offset_src0: u32,\n+    offset_src1: u32,\n+    offset_dst: u32,\n+    m: u32,\n+    n: u32,\n+    k: u32,\n+    stride_01: u32,\n+    stride_11: u32,\n+    stride_02: u32,\n+    stride_12: u32,\n+    stride_03: u32,\n+    stride_13: u32,\n+    bs02: u32,\n+    bs03: u32,\n+    broadcast2: u32,\n+    broadcast3: u32\n+};\n+\n+@group(0) @binding(0) var<storage, read_write> src0: array<{{SRC0_TYPE}}>; // M rows, K columns\n+@group(0) @binding(1) var<storage, read_write> src1: array<{{SRC1_TYPE}}>; // K rows, N columns (transposed)\n+@group(0) @binding(2) var<storage, read_write> dst: array<{{DST_TYPE}}>; // M rows, N columns (transposed)\n+\n+@group(0) @binding(3) var<uniform> params: MulMatParams;\n+\n+DECLS\n+\n+fn get_local_n(thread_id: u32) -> u32 {\n+    return thread_id / WORKGROUP_SIZE_M;\n+}\n+fn get_local_m(thread_id: u32) -> u32 {\n+    return thread_id % WORKGROUP_SIZE_M;\n+}\n+\n+// TILE_M must be multiple of 4 for vec4 loads\n+const TILE_M = {{WEBGPU_TILE_M}}u;\n+const TILE_N = {{WEBGPU_TILE_N}}u;\n+\n+override WORKGROUP_SIZE_M: u32;\n+override WORKGROUP_SIZE_N: u32;\n+override TILE_K: u32;\n+\n+override TOTAL_WORKGROUP_SIZE = WORKGROUP_SIZE_M * WORKGROUP_SIZE_N;\n+override TILE_SRC0_SHMEM = TILE_K * WORKGROUP_SIZE_M * TILE_M;\n+override TILE_SRC1_SHMEM = TILE_K * WORKGROUP_SIZE_N * TILE_N;\n+\n+var<workgroup> shmem: array<f16, TILE_SRC0_SHMEM + TILE_SRC1_SHMEM>;\n+\n+@compute @workgroup_size(TOTAL_WORKGROUP_SIZE)\n+fn main(@builtin(workgroup_id) wg_id: vec3<u32>,\n+        @builtin(local_invocation_id) local_id: vec3<u32>) {\n+\n+    let thread_id = local_id.x;\n+    let local_m = get_local_m(thread_id);\n+    let local_n = get_local_n(thread_id);\n+\n+    let wg_n_count = (params.n + WORKGROUP_SIZE_N * TILE_N - 1u) / (WORKGROUP_SIZE_N * TILE_N);\n+    let wg_m_count = (params.m + WORKGROUP_SIZE_M * TILE_M - 1u) / (WORKGROUP_SIZE_M * TILE_M);\n+    let wg_per_matrix = wg_m_count * wg_n_count;\n+\n+    let batch_idx = wg_id.x / wg_per_matrix;\n+\n+    let wg_in_batch = wg_id.x % wg_per_matrix;\n+    let wg_m = wg_in_batch % wg_m_count;\n+    let wg_n = wg_in_batch / wg_m_count;\n+\n+    let output_row_base = wg_m * WORKGROUP_SIZE_M * TILE_M + local_m * TILE_M;\n+    let output_col_base = wg_n * WORKGROUP_SIZE_N * TILE_N + local_n * TILE_N;\n+\n+    let dst2_stride = params.m * params.n;\n+    let dst3_stride = dst2_stride * params.bs02 * params.broadcast2;\n+\n+    let dst3_idx = batch_idx / (params.bs02 * params.broadcast2);\n+    let src03_idx = dst3_idx / params.broadcast3;\n+    let src13_idx = dst3_idx;\n+    let dst2_idx = batch_idx % (params.bs02 * params.broadcast2);\n+    let src02_idx = dst2_idx / params.broadcast2;\n+    let src12_idx = dst2_idx;\n+\n+    let src0_batch_offset = params.offset_src0 + src03_idx * params.stride_03 + src02_idx * params.stride_02;\n+    let src1_batch_offset = params.offset_src1 + src13_idx * params.stride_13 + src12_idx * params.stride_12;\n+\n+    let offset_m = wg_m * WORKGROUP_SIZE_M * TILE_M;\n+    let offset_n = wg_n * WORKGROUP_SIZE_N * TILE_N;\n+\n+    var acc: array<array<f16, TILE_N>, TILE_M>;\n+\n+    for (var k_outer = 0u; k_outer < params.k; k_outer += TILE_K) {\n+\n+        // see mul_mat_decls.tmpl\n+        init_shmem_src0(thread_id, src0_batch_offset, offset_m, k_outer);\n+        init_shmem_src1(thread_id, src1_batch_offset, offset_n, k_outer);\n+\n+        workgroupBarrier();\n+\n+        let k_end = min(TILE_K, params.k - k_outer);\n+\n+        for (var k_inner = 0u; k_inner < k_end; k_inner++) {\n+            var src0_tile: array<f16, TILE_M>;\n+            for (var tm = 0u; tm < TILE_M; tm++) {\n+                let src0_m = local_m * TILE_M + tm;\n+                let src0_idx = k_inner + src0_m * TILE_K;\n+                src0_tile[tm] = shmem[src0_idx];\n+            }\n+            for (var tn = 0u; tn < TILE_N; tn++) {\n+                let src1_n = local_n * TILE_N + tn;\n+                let src1_idx = src1_n * TILE_K + k_inner;\n+                let src1_val = shmem[TILE_SRC0_SHMEM + src1_idx];\n+                for (var tm = 0u; tm < TILE_M; tm++) {\n+                      acc[tm][tn] += src0_tile[tm] * src1_val;\n+                }\n+            }\n+        }\n+\n+        workgroupBarrier();\n+    }\n+\n+    let dst_batch_offset = params.offset_dst + dst3_idx * dst3_stride + dst2_idx * dst2_stride;\n+\n+    for (var tn = 0u; tn < TILE_N; tn++) {\n+        let global_col = output_col_base + tn;\n+        if (global_col < params.n) {\n+            for (var tm = 0u; tm < TILE_M; tm += {{VEC_SIZE}}) {\n+                let global_row = output_row_base + tm;\n+                if (global_row < params.m) {\n+                    let dst_idx = dst_batch_offset + global_col * params.m + global_row;\n+                    dst[dst_idx/{{VEC_SIZE}}] = store_val(acc, tn, tm);\n+                }\n+            }\n+        }\n+    }\n+}\n+\n+#end(SHADER)"
      },
      {
        "filename": "ggml/src/ggml-webgpu/wgsl-shaders/mul_mat_subgroup_matrix.tmpl.wgsl",
        "status": "added",
        "additions": 302,
        "deletions": 0,
        "changes": 302,
        "patch": "@@ -0,0 +1,302 @@\n+#define(VARIANTS)\n+[\n+  {\n+    \"SHADER_SUFFIX\": \"f32_f32_vec\",\n+    \"REPLS\": {\n+      \"SRC0_TYPE\" : \"vec4<f32>\",\n+      \"SRC1_TYPE\" : \"vec4<f32>\",\n+      \"DST_TYPE\" : \"vec4<f32>\",\n+      \"SHMEM_TYPE\" : \"vec4<f16>\",\n+      \"VEC_SIZE\" : 4,\n+    },\n+    \"DECLS\": [\"VEC\", \"SHMEM_VEC\", \"INIT_SRC0_SHMEM_FLOAT\", \"INIT_SRC1_SHMEM\"]\n+  },\n+  {\n+    \"SHADER_SUFFIX\": \"f32_f32\",\n+    \"REPLS\": {\n+      \"SRC0_TYPE\" : \"f32\",\n+      \"SRC1_TYPE\" : \"f32\",\n+      \"DST_TYPE\" : \"f32\",\n+      \"SHMEM_TYPE\" : \"f16\",\n+      \"VEC_SIZE\" : 1,\n+    },\n+    \"DECLS\": [\"SCALAR\", \"SHMEM_SCALAR\", \"INIT_SRC0_SHMEM_FLOAT\", \"INIT_SRC1_SHMEM\"]\n+  },\n+  {\n+    \"SHADER_SUFFIX\": \"f16_f32_vec\",\n+    \"REPLS\": {\n+      \"SRC0_TYPE\" : \"vec4<f16>\",\n+      \"SRC1_TYPE\" : \"vec4<f32>\",\n+      \"DST_TYPE\" : \"vec4<f32>\",\n+      \"SHMEM_TYPE\" : \"vec4<f16>\",\n+      \"VEC_SIZE\" : 4,\n+    },\n+    \"DECLS\": [\"VEC\", \"SHMEM_VEC\", \"INIT_SRC0_SHMEM_FLOAT\", \"INIT_SRC1_SHMEM\"]\n+  },\n+  {\n+    \"SHADER_SUFFIX\": \"f16_f32\",\n+    \"REPLS\": {\n+      \"SRC0_TYPE\" : \"f16\",\n+      \"SRC1_TYPE\" : \"f32\",\n+      \"DST_TYPE\" : \"f32\",\n+      \"SHMEM_TYPE\" : \"f16\",\n+      \"VEC_SIZE\" : 1,\n+    },\n+    \"DECLS\": [\"SCALAR\", \"SHMEM_SCALAR\", \"INIT_SRC0_SHMEM_FLOAT\", \"INIT_SRC1_SHMEM\"]\n+  },\n+  {\n+    \"SHADER_SUFFIX\": \"f16_f16_vec\",\n+    \"REPLS\": {\n+      \"SRC0_TYPE\" : \"vec4<f16>\",\n+      \"SRC1_TYPE\" : \"vec4<f16>\",\n+      \"DST_TYPE\" : \"vec4<f32>\",\n+      \"SHMEM_TYPE\" : \"vec4<f16>\",\n+      \"VEC_SIZE\" : 4,\n+    },\n+    \"DECLS\": [\"VEC\", \"SHMEM_VEC\", \"INIT_SRC0_SHMEM_FLOAT\", \"INIT_SRC1_SHMEM\"]\n+  },\n+  {\n+    \"SHADER_SUFFIX\": \"f16_f16\",\n+    \"REPLS\": {\n+      \"SRC0_TYPE\" : \"f16\",\n+      \"SRC1_TYPE\" : \"f16\",\n+      \"DST_TYPE\" : \"f32\",\n+      \"SHMEM_TYPE\" : \"f16\",\n+      \"VEC_SIZE\" : 1,\n+    },\n+    \"DECLS\": [\"SCALAR\", \"SHMEM_SCALAR\", \"INIT_SRC0_SHMEM_FLOAT\", \"INIT_SRC1_SHMEM\"]\n+  },\n+  {\n+    \"SHADER_SUFFIX\": \"q4_0_f32_vec\",\n+    \"REPLS\": {\n+      \"SRC0_TYPE\" : \"f16\",\n+      \"SRC1_TYPE\" : \"vec4<f32>\",\n+      \"DST_TYPE\" : \"vec4<f32>\",\n+      \"SHMEM_TYPE\" : \"vec4<f16>\",\n+      \"VEC_SIZE\" : 4,\n+    },\n+    \"DECLS\": [\"BYTE_HELPERS\", \"VEC\", \"SHMEM_VEC\", \"INIT_SRC0_SHMEM_Q4_0\", \"INIT_SRC1_SHMEM\"]\n+  },\n+  {\n+    \"SHADER_SUFFIX\": \"q4_0_f32\",\n+    \"REPLS\": {\n+      \"SRC0_TYPE\" : \"f16\",\n+      \"SRC1_TYPE\" : \"f32\",\n+      \"DST_TYPE\" : \"f32\",\n+      \"SHMEM_TYPE\" : \"f16\",\n+      \"VEC_SIZE\" : 1,\n+    },\n+    \"DECLS\": [\"BYTE_HELPERS\", \"SCALAR\", \"SHMEM_SCALAR\", \"INIT_SRC0_SHMEM_Q4_0\", \"INIT_SRC1_SHMEM\"]\n+  }\n+]\n+\n+#end(VARIANTS)\n+\n+#define(DECLS)\n+\n+#decl(VEC)\n+fn store_dst(shmem_idx: u32, dst_idx: u32) {\n+    dst[dst_idx] = vec4<f32>(\n+        f32(shmem[shmem_idx]),\n+        f32(shmem[shmem_idx + 1]),\n+        f32(shmem[shmem_idx + 2]),\n+        f32(shmem[shmem_idx + 3])\n+    );\n+}\n+#enddecl(VEC)\n+\n+#decl(SCALAR)\n+fn store_dst(shmem_idx: u32, dst_idx: u32) {\n+    dst[dst_idx] = f32(shmem[shmem_idx]);\n+}\n+#enddecl(SCALAR)\n+\n+#end(DECLS)\n+\n+#define(SHADER)\n+diagnostic(off, chromium.subgroup_matrix_uniformity);\n+enable f16;\n+enable subgroups;\n+enable chromium_experimental_subgroup_matrix;\n+\n+struct MulMatParams {\n+    offset_src0: u32,\n+    offset_src1: u32,\n+    offset_dst: u32,\n+    m: u32,\n+    n: u32,\n+    k: u32,\n+    stride_01: u32,\n+    stride_11: u32,\n+    stride_02: u32,\n+    stride_12: u32,\n+    stride_03: u32,\n+    stride_13: u32,\n+    bs02: u32,\n+    bs03: u32,\n+    broadcast2: u32,\n+    broadcast3: u32\n+};\n+\n+@group(0) @binding(0) var<storage, read_write> src0: array<{{SRC0_TYPE}}>; // M rows, K columns\n+@group(0) @binding(1) var<storage, read_write> src1: array<{{SRC1_TYPE}}>; // K rows, N columns (transposed)\n+@group(0) @binding(2) var<storage, read_write> dst: array<{{DST_TYPE}}>; // M rows, N columns (transposed)\n+\n+@group(0) @binding(3) var<uniform> params: MulMatParams;\n+\n+DECLS\n+\n+// Note: These are string interpolated at build time, cannot use override constants due to limitations in\n+// current Dawn version type definitions/matrix load requirements for constant memory sizes.\n+const SUBGROUP_M = {{WEBGPU_SUBGROUP_M}}u;\n+const SUBGROUP_N = {{WEBGPU_SUBGROUP_N}}u;\n+// For portability we assume the max subgroup size, meaning some subgroups will be masked out if the\n+// runtime subgroup size is smaller.\n+const MAX_SUBGROUP_SIZE = {{WEBGPU_MAX_SUBGROUP_SIZE}}u;\n+\n+const EXPECTED_SUBGROUPS = SUBGROUP_M * SUBGROUP_N;\n+\n+const SUBGROUP_MATRIX_M_SIZE = {{WEBGPU_SG_MAT_M_SIZE}}u;\n+const SUBGROUP_MATRIX_N_SIZE = {{WEBGPU_SG_MAT_N_SIZE}}u;\n+const SUBGROUP_MATRIX_K_SIZE = {{WEBGPU_SG_MAT_K_SIZE}}u;\n+\n+const SUBGROUP_MATRIX_M = {{WEBGPU_SUBGROUP_MATRIX_M}}u;\n+const SUBGROUP_MATRIX_N = {{WEBGPU_SUBGROUP_MATRIX_N}}u;\n+\n+const TILE_K = {{WEBGPU_TILE_K}}u;\n+\n+const WG_M_SG_TILE_SIZE = SUBGROUP_M * SUBGROUP_MATRIX_M * SUBGROUP_MATRIX_M_SIZE;\n+const WG_N_SG_TILE_SIZE = SUBGROUP_N * SUBGROUP_MATRIX_N * SUBGROUP_MATRIX_N_SIZE;\n+\n+const TOTAL_WORKGROUP_SIZE = SUBGROUP_M * SUBGROUP_N * MAX_SUBGROUP_SIZE;\n+const TILE_SRC0_SHMEM = TILE_K * SUBGROUP_M * SUBGROUP_MATRIX_M * SUBGROUP_MATRIX_M_SIZE;\n+const TILE_SRC1_SHMEM = TILE_K * SUBGROUP_N * SUBGROUP_MATRIX_N * SUBGROUP_MATRIX_N_SIZE;\n+\n+const SG_MAT_ACCUM_SHMEM = SUBGROUP_M * SUBGROUP_MATRIX_M * SUBGROUP_N * SUBGROUP_MATRIX_N * SUBGROUP_MATRIX_M_SIZE * SUBGROUP_MATRIX_N_SIZE;\n+\n+// We reuse shmem for accumulation matrices\n+const SHMEM_SIZE = max(TILE_SRC0_SHMEM + TILE_SRC1_SHMEM, SG_MAT_ACCUM_SHMEM);\n+\n+var<workgroup> shmem: array<f16, SHMEM_SIZE>;\n+\n+@compute @workgroup_size(TOTAL_WORKGROUP_SIZE)\n+fn main(@builtin(workgroup_id) wg_id: vec3<u32>,\n+        @builtin(local_invocation_id) local_id: vec3<u32>,\n+        @builtin(subgroup_id) subgroup_id: u32) {\n+\n+    let thread_id = local_id.x;\n+    let subgroup_m = subgroup_id % SUBGROUP_M;\n+    let subgroup_n = subgroup_id / SUBGROUP_M;\n+\n+    let wg_m_count = (params.m + WG_M_SG_TILE_SIZE - 1) / WG_M_SG_TILE_SIZE;\n+    let wg_n_count = (params.n + WG_N_SG_TILE_SIZE - 1) / WG_N_SG_TILE_SIZE;\n+    let wg_per_matrix = wg_m_count * wg_n_count;\n+\n+    let batch_idx = wg_id.x / wg_per_matrix;\n+\n+    let wg_in_batch = wg_id.x % wg_per_matrix;\n+    let wg_m = wg_in_batch % wg_m_count;\n+    let wg_n = wg_in_batch / wg_m_count;\n+\n+    let dst2_stride = params.m * params.n;\n+    let dst3_stride = dst2_stride * params.bs02 * params.broadcast2;\n+\n+    let dst3_idx = batch_idx / (params.bs02 * params.broadcast2);\n+    let src03_idx = dst3_idx / params.broadcast3;\n+    let src13_idx = dst3_idx;\n+    let dst2_idx = batch_idx % (params.bs02 * params.broadcast2);\n+    let src02_idx = dst2_idx / params.broadcast2;\n+    let src12_idx = dst2_idx;\n+\n+    let src0_batch_offset = params.offset_src0 + src03_idx * params.stride_03 + src02_idx * params.stride_02;\n+    let src1_batch_offset = params.offset_src1 + src13_idx * params.stride_13 + src12_idx * params.stride_12;\n+\n+    let offset_m = wg_m * SUBGROUP_M * SUBGROUP_MATRIX_M * SUBGROUP_MATRIX_M_SIZE;\n+    let offset_n = wg_n * SUBGROUP_N * SUBGROUP_MATRIX_N * SUBGROUP_MATRIX_N_SIZE;\n+\n+    var acc_sg_mat : array<array<subgroup_matrix_result<f16, SUBGROUP_MATRIX_N_SIZE, SUBGROUP_MATRIX_M_SIZE>, SUBGROUP_MATRIX_N>, SUBGROUP_MATRIX_M>;\n+\n+    for (var k_outer = 0u; k_outer < params.k; k_outer += TILE_K) {\n+\n+        // see mul_mat_decls.tmpl\n+        init_shmem_src0(thread_id, src0_batch_offset, offset_m, k_outer);\n+        init_shmem_src1(thread_id, src1_batch_offset, offset_n, k_outer);\n+\n+        workgroupBarrier();\n+\n+        if (subgroup_id < EXPECTED_SUBGROUPS) {\n+\n+            for (var k_inner = 0u; k_inner < TILE_K; k_inner += SUBGROUP_MATRIX_K_SIZE) {\n+\n+                let src0_shmem_idx_base = subgroup_m * SUBGROUP_MATRIX_M * SUBGROUP_MATRIX_M_SIZE * TILE_K + k_inner;\n+                var src0_sg_mats: array<subgroup_matrix_left<f16, SUBGROUP_MATRIX_K_SIZE, SUBGROUP_MATRIX_M_SIZE>, SUBGROUP_MATRIX_M>;\n+                for (var m = 0u; m < SUBGROUP_MATRIX_M; m++) {\n+                    src0_sg_mats[m] = subgroupMatrixLoad<subgroup_matrix_left<f16, SUBGROUP_MATRIX_K_SIZE, SUBGROUP_MATRIX_M_SIZE>>(\n+                        &shmem,\n+                        src0_shmem_idx_base + m * SUBGROUP_MATRIX_M_SIZE * TILE_K,\n+                        false,\n+                        TILE_K\n+                    );\n+                }\n+\n+                let src1_shmem_idx_base = TILE_SRC0_SHMEM + subgroup_n * SUBGROUP_MATRIX_N * SUBGROUP_MATRIX_N_SIZE * TILE_K + k_inner;\n+                for (var n = 0u; n < SUBGROUP_MATRIX_N; n++) {\n+                    let src1_sg_mat = subgroupMatrixLoad<subgroup_matrix_right<f16, SUBGROUP_MATRIX_N_SIZE, SUBGROUP_MATRIX_K_SIZE>>(\n+                        &shmem,\n+                        src1_shmem_idx_base + n * SUBGROUP_MATRIX_N_SIZE * TILE_K,\n+                        true,\n+                        TILE_K\n+                    );\n+                    for (var m = 0u; m < SUBGROUP_MATRIX_M; m++) {\n+                        acc_sg_mat[m][n] = subgroupMatrixMultiplyAccumulate(src0_sg_mats[m], src1_sg_mat, acc_sg_mat[m][n]);\n+                    }\n+                }\n+            }\n+        }\n+\n+        workgroupBarrier();\n+    }\n+\n+    let dst_batch_offset = params.offset_dst + dst3_idx * dst3_stride + dst2_idx * dst2_stride;\n+\n+    // Stage the subgroup matrix tiles into shared memory\n+    // This uses WG_M_SG_TILE_SIZE as the stride (number of columns in the workgroup tile).\n+    let WG_TILE_STRIDE = WG_M_SG_TILE_SIZE;\n+    let tile_row_base_local = subgroup_n * SUBGROUP_MATRIX_N * SUBGROUP_MATRIX_N_SIZE;\n+    let tile_col_base_local = subgroup_m * SUBGROUP_MATRIX_M * SUBGROUP_MATRIX_M_SIZE;\n+\n+    if (subgroup_id < EXPECTED_SUBGROUPS) { // 2-5% performance hit :(\n+        for (var n = 0u; n < SUBGROUP_MATRIX_N; n++) {\n+            for (var m = 0u; m < SUBGROUP_MATRIX_M; m++) {\n+                let local_row = tile_row_base_local + n * SUBGROUP_MATRIX_N_SIZE;\n+                let local_col = tile_col_base_local + m * SUBGROUP_MATRIX_M_SIZE;\n+                let out_base = local_row * WG_TILE_STRIDE + local_col;\n+                subgroupMatrixStore(&shmem, out_base, acc_sg_mat[m][n], true, WG_TILE_STRIDE);\n+            }\n+        }\n+    }\n+\n+    workgroupBarrier();\n+\n+    // Cooperative write: iterate over the entire workgroup tile\n+    let tile_rows = WG_N_SG_TILE_SIZE;\n+    let tile_cols = WG_M_SG_TILE_SIZE;\n+    let total_tile_elems = tile_rows * tile_cols;\n+    let tile_dst_row_base = wg_m * SUBGROUP_M * SUBGROUP_MATRIX_M * SUBGROUP_MATRIX_M_SIZE;\n+    let tile_dst_col_base = wg_n * SUBGROUP_N * SUBGROUP_MATRIX_N * SUBGROUP_MATRIX_N_SIZE;\n+\n+    for (var idx = thread_id * {{VEC_SIZE}}; idx < total_tile_elems; idx += TOTAL_WORKGROUP_SIZE * {{VEC_SIZE}}) {\n+        let local_row = idx % WG_TILE_STRIDE;\n+        let local_col = idx / WG_TILE_STRIDE;\n+\n+        let global_row = tile_dst_row_base + local_row;\n+        let global_col = tile_dst_col_base + local_col;\n+\n+        if (global_col < params.n && global_row < params.m) {\n+            let dst_idx = dst_batch_offset + global_col * params.m + global_row;\n+            store_dst(idx, dst_idx/{{VEC_SIZE}});\n+        }\n+    }\n+}\n+\n+#end(SHADER)"
      },
      {
        "filename": "ggml/src/ggml-webgpu/wgsl-shaders/mul_mat_vec.tmpl.wgsl",
        "status": "added",
        "additions": 267,
        "deletions": 0,
        "changes": 267,
        "patch": "@@ -0,0 +1,267 @@\n+#define(VARIANTS)\n+[\n+  {\n+    \"SHADER_SUFFIX\": \"f32_f32_vec\",\n+    \"REPLS\": {\n+      \"SRC0_TYPE\" : \"vec4<f32>\",\n+      \"SRC1_TYPE\" : \"vec4<f32>\",\n+      \"DST_TYPE\": \"vec4<f32>\",\n+      \"VEC_SIZE\" : 4,\n+    },\n+    \"DECLS\": [\"VEC\", \"MUL_ACC_FLOAT\"]\n+  },\n+  {\n+    \"SHADER_SUFFIX\": \"f32_f32\",\n+    \"REPLS\": {\n+      \"SRC0_TYPE\" : \"f32\",\n+      \"SRC1_TYPE\" : \"f32\",\n+      \"DST_TYPE\": \"f32\",\n+      \"VEC_SIZE\" : 1,\n+    },\n+    \"DECLS\": [\"SCALAR\", \"MUL_ACC_FLOAT\"]\n+  },\n+  {\n+    \"SHADER_SUFFIX\": \"f16_f32_vec\",\n+    \"REPLS\": {\n+      \"SRC0_TYPE\" : \"vec4<f16>\",\n+      \"SRC1_TYPE\" : \"vec4<f32>\",\n+      \"DST_TYPE\": \"vec4<f32>\",\n+      \"VEC_SIZE\" : 4,\n+    },\n+    \"DECLS\": [\"VEC\", \"MUL_ACC_FLOAT\"]\n+  },\n+  {\n+    \"SHADER_SUFFIX\": \"f16_f32\",\n+    \"REPLS\": {\n+      \"SRC0_TYPE\" : \"f16\",\n+      \"SRC1_TYPE\" : \"f32\",\n+      \"DST_TYPE\": \"f32\",\n+      \"VEC_SIZE\" : 1,\n+    },\n+    \"DECLS\": [\"SCALAR\", \"MUL_ACC_FLOAT\"]\n+  },\n+  {\n+    \"SHADER_SUFFIX\": \"f16_f16_vec\",\n+    \"REPLS\": {\n+      \"SRC0_TYPE\" : \"vec4<f16>\",\n+      \"SRC1_TYPE\" : \"vec4<f16>\",\n+      \"DST_TYPE\": \"vec4<f32>\",\n+      \"VEC_SIZE\" : 4,\n+    },\n+    \"DECLS\": [\"VEC\", \"MUL_ACC_FLOAT\"]\n+  },\n+  {\n+    \"SHADER_SUFFIX\": \"f16_f16\",\n+    \"REPLS\": {\n+      \"SRC0_TYPE\" : \"f16\",\n+      \"SRC1_TYPE\" : \"f16\",\n+      \"DST_TYPE\": \"f32\",\n+      \"VEC_SIZE\" : 1,\n+    },\n+    \"DECLS\": [\"SCALAR\", \"MUL_ACC_FLOAT\"]\n+  },\n+  {\n+    \"SHADER_SUFFIX\": \"q4_0_f32\",\n+    \"REPLS\": {\n+      \"SRC0_TYPE\" : \"f16\",\n+      \"SRC1_TYPE\" : \"f32\",\n+      \"DST_TYPE\": \"f32\",\n+      \"VEC_SIZE\" : 1,\n+    },\n+    \"DECLS\": [\"BYTE_HELPERS\", \"SCALAR\", \"MUL_ACC_Q4_0\"]\n+  }\n+]\n+\n+#end(VARIANTS)\n+\n+#define(DECLS)\n+\n+#decl(VEC)\n+fn inner_dot(src0_val: {{SRC0_TYPE}}, src1_val: {{SRC1_TYPE}}) -> f32 {\n+    return f32(dot({{SRC1_TYPE}}(src0_val), src1_val));\n+}\n+\n+fn store_val(group_base: u32) -> vec4<f32> {\n+    return vec4<f32>(partial_sums[group_base],\n+                     partial_sums[group_base + THREADS_PER_OUTPUT],\n+                     partial_sums[group_base + THREADS_PER_OUTPUT * 2],\n+                     partial_sums[group_base + THREADS_PER_OUTPUT * 3]);\n+}\n+#enddecl(VEC)\n+\n+#decl(SCALAR)\n+fn inner_dot(src0_val: {{SRC0_TYPE}}, src1_val: {{SRC1_TYPE}}) -> f32 {\n+    return f32(src0_val) * f32(src1_val);\n+}\n+\n+fn store_val(group_base: u32) -> f32 {\n+    return partial_sums[group_base];\n+}\n+#enddecl(SCALAR)\n+\n+#decl(MUL_ACC_FLOAT)\n+\n+fn mul_acc(tig:u32, tile_size: u32, idx_base: u32, k_outer: u32) -> f32 {\n+    var local_sum = 0.0;\n+    for (var i = tig * {{VEC_SIZE}}; i < tile_size; i += THREADS_PER_OUTPUT * {{VEC_SIZE}}) {\n+        let a = src0[(idx_base + k_outer + i) / {{VEC_SIZE}}];\n+        let b = shared_vector[i / {{VEC_SIZE}}];\n+        local_sum += inner_dot(a, b);\n+    }\n+    return local_sum;\n+}\n+\n+#enddecl(MUL_ACC_FLOAT)\n+\n+#decl(MUL_ACC_Q4_0)\n+\n+const BLOCK_SIZE = 32;\n+const NQ = 16u; // number of weights per thread\n+const F16_PER_BLOCK = 9u; // 1 scale + 8x4 packed weights\n+const WEIGHTS_PER_F16 = 4u; // 4 weights per f16\n+const F16_PER_THREAD = NQ / WEIGHTS_PER_F16;\n+\n+fn mul_acc(tig:u32, tile_size: u32, idx_base: u32, k_outer: u32) -> f32 {\n+    var local_sum = 0.0;\n+    for (var i = tig * NQ; i < tile_size; i += THREADS_PER_OUTPUT * NQ) {\n+        let blck_idx = i / BLOCK_SIZE;\n+        let block_offset = (i % BLOCK_SIZE) / WEIGHTS_PER_F16;\n+        let scale_idx = (idx_base + k_outer / BLOCK_SIZE + blck_idx) * F16_PER_BLOCK;\n+        // each f16 contains offsets [block_offset, block_offset + 1] and [block_offset + 16, block_offset + 17]\n+        let shmem_idx = blck_idx * BLOCK_SIZE + block_offset * 2u;\n+        let d = f32(src0[scale_idx]);\n+        for (var j = 0u; j < F16_PER_THREAD; j += 2) {\n+            let q_0 = src0[scale_idx + 1 + block_offset + j];\n+            let q_1 = src0[scale_idx + 1 + block_offset + j + 1];\n+            let q_packed = bitcast<u32>(vec2(q_0, q_1));\n+            for (var k: u32 = 0; k < 4; k++) {\n+                let q_byte = get_byte(q_packed, k);\n+                let q_hi = (f32((q_byte >> 4) & 0xF) - 8.0) * d;\n+                let q_lo = (f32(q_byte & 0xF) - 8.0) * d;\n+                local_sum += q_lo * shared_vector[shmem_idx + j * 2 + k];\n+                local_sum += q_hi * shared_vector[shmem_idx + j * 2 + k + 16];\n+            }\n+        }\n+    }\n+    return local_sum;\n+}\n+\n+#enddecl(MUL_ACC_Q4_0)\n+\n+#end(DECLS)\n+\n+#define(SHADER)\n+enable f16;\n+\n+DECLS\n+\n+struct MulMatParams {\n+    offset_src0: u32,\n+    offset_src1: u32,\n+    offset_dst: u32,\n+    m: u32,\n+    n: u32,\n+    k: u32,\n+    stride_01: u32,\n+    stride_11: u32,\n+    stride_02: u32,\n+    stride_12: u32,\n+    stride_03: u32,\n+    stride_13: u32,\n+    bs02: u32,\n+    bs03: u32,\n+    broadcast2: u32,\n+    broadcast3: u32\n+};\n+\n+@group(0) @binding(0) var<storage, read_write> src0: array<{{SRC0_TYPE}}>; // Matrix (M x K)\n+@group(0) @binding(1) var<storage, read_write> src1: array<{{SRC1_TYPE}}>; // Vector (K x 1, transposed)\n+@group(0) @binding(2) var<storage, read_write> dst: array<{{DST_TYPE}}>;  // Result vector (transposed)\n+\n+@group(0) @binding(3) var<uniform> params: MulMatParams;\n+\n+override WORKGROUP_SIZE: u32;\n+override TILE_K: u32;\n+override OUTPUTS_PER_WG: u32;\n+override THREADS_PER_OUTPUT = WORKGROUP_SIZE / OUTPUTS_PER_WG;\n+\n+// Shared memory for collaborative loading and reduction\n+var<workgroup> shared_vector: array<{{SRC1_TYPE}}, TILE_K/{{VEC_SIZE}}>;  // Cache vector tile\n+var<workgroup> partial_sums: array<f32, WORKGROUP_SIZE>;   // For reduction\n+\n+@compute @workgroup_size(WORKGROUP_SIZE)\n+fn main(\n+    @builtin(local_invocation_id) local_id: vec3<u32>,\n+    @builtin(workgroup_id) wg_id: vec3<u32>,\n+    @builtin(num_workgroups) num_wg: vec3<u32>) {\n+    let thread_id = local_id.x;\n+\n+    // Handle batch dimensions\n+    let total_batches = params.bs02 * params.broadcast2 * params.bs03 * params.broadcast3;\n+    let wg_linear = wg_id.y * num_wg.x + wg_id.x;\n+    let output_groups = (params.m + OUTPUTS_PER_WG - 1u) / OUTPUTS_PER_WG;\n+    let batch_idx = wg_linear / output_groups;\n+    if (batch_idx >= total_batches) {\n+        return;\n+    }\n+\n+    // Which of the outputs does this thread belong to?\n+    let thread_group = thread_id / THREADS_PER_OUTPUT;\n+    let thread_in_group = thread_id % THREADS_PER_OUTPUT;\n+\n+    // Each workgroup computes OUTPUTS_PER_WG consecutive outputs\n+    let output_row = (wg_linear % output_groups) * OUTPUTS_PER_WG + thread_group;\n+\n+    let dst2_stride = params.m * params.n;\n+    let dst2_idx = batch_idx % (params.bs02 * params.broadcast2);\n+    let dst3_stride = dst2_stride * params.bs02 * params.broadcast2;\n+    let dst3_idx = batch_idx / (params.bs02 * params.broadcast2);\n+    let src03_idx = dst3_idx / params.broadcast3;\n+    let src13_idx = dst3_idx;\n+    let src02_idx = dst2_idx / params.broadcast2;\n+    let src12_idx = dst2_idx;\n+\n+    let src0_idx_base = params.offset_src0 + src03_idx * params.stride_03 + src02_idx * params.stride_02 + output_row * params.stride_01;\n+    let src1_idx_base = params.offset_src1 + src13_idx * params.stride_13 + src12_idx * params.stride_12;\n+    let dst_idx = params.offset_dst + dst3_idx * dst3_stride + dst2_idx * dst2_stride + output_row;\n+\n+    var local_sum = 0.0;\n+\n+    // Each thread processes multiple K elements and accumulates\n+    for (var k_tile = 0u; k_tile < params.k; k_tile += TILE_K) {\n+        let tile_size = min(TILE_K, params.k - k_tile);\n+\n+        // Cooperatively load vector tile into shared memory (all threads)\n+        for (var i = thread_id * {{VEC_SIZE}}; i < tile_size; i += WORKGROUP_SIZE * {{VEC_SIZE}}) {\n+            shared_vector[i / {{VEC_SIZE}}] = src1[(src1_idx_base + k_tile + i) / {{VEC_SIZE}}];\n+        }\n+\n+        workgroupBarrier();\n+\n+        if (output_row < params.m) {\n+            local_sum += mul_acc(thread_in_group, tile_size, src0_idx_base, k_tile);\n+        }\n+\n+        workgroupBarrier();\n+    }\n+\n+    // Store partial sums and reduce within each partition\n+    partial_sums[thread_id] = local_sum;\n+    workgroupBarrier();\n+    let group_base = thread_group * THREADS_PER_OUTPUT;\n+    let thread_base = group_base + thread_in_group;\n+    var offset = THREADS_PER_OUTPUT / 2;\n+    while (offset > 0) {\n+        if (thread_in_group < offset) {\n+            partial_sums[thread_base] += partial_sums[thread_base + offset];\n+        }\n+        offset = offset / 2;\n+        workgroupBarrier();\n+    }\n+\n+    // Store back to global memory\n+    if (output_row < params.m && thread_group % {{VEC_SIZE}} == 0 && thread_in_group == 0) {\n+        dst[dst_idx / {{VEC_SIZE}}] = store_val(group_base);\n+    }\n+}\n+#end(SHADER)"
      }
    ],
    "num_files": 8,
    "scraped_at": "2025-11-16T23:28:52.749732"
  },
  {
    "pr_number": 17022,
    "title": "cuda/vulkan : bicubic interpolation",
    "body": "Backend support for `GGML_OP_UPSCALE` with `GGML_SCALE_MODE_BICUBIC`\r\n\r\n* **CUDA**: implemented\r\n* **Vulkan**: implemented\r\n* **OpenCL**: adjusted `ggml_backend_supports_op` to return false for bicubic\r\n\r\nCombined PR so I can add backend-ops tests without having them fail for some backends.",
    "html_url": "https://github.com/ggml-org/llama.cpp/pull/17022",
    "created_at": "2025-11-05T12:11:09Z",
    "merged_at": "2025-11-10T09:19:39Z",
    "merge_commit_sha": "1032256ec95986f60124a62ace6a628106546497",
    "base_ref": "master",
    "head_sha": "0f638ad4003de0930049635ae06ae4393c2440e4",
    "user": "Acly",
    "files": [
      {
        "filename": "ggml/src/ggml-cuda/upscale.cu",
        "status": "modified",
        "additions": 87,
        "deletions": 6,
        "changes": 93,
        "patch": "@@ -81,6 +81,70 @@ static __global__ void upscale_f32_bilinear(const float * x, float * dst,\n     dst[index] = result;\n }\n \n+namespace bicubic_interpolation {\n+// https://en.wikipedia.org/wiki/Bicubic_interpolation#Bicubic_convolution_algorithm\n+__device__ const float a = -0.75f; // use alpha = -0.75 (same as PyTorch)\n+\n+static __device__ float weight1(float x) { return ((a + 2) * x - (a + 3)) * x * x + 1; };\n+static __device__ float weight2(float x) { return ((a * x - 5 * a) * x + 8 * a) * x - 4 * a; };\n+\n+static __device__ float bicubic(float p0, float p1, float p2, float p3, float x) {\n+    const float w0 = weight2(x + 1);\n+    const float w1 = weight1(x + 0);\n+    const float w2 = weight1(1 - x);\n+    const float w3 = weight2(2 - x);\n+    return p0 * w0 + p1 * w1 + p2 * w2 + p3 * w3;\n+};\n+} // namespace bicubic_interpolation\n+\n+static __global__ void upscale_f32_bicubic(const float * x, float * dst,\n+        const int nb00, const int nb01, const int nb02, const int nb03,\n+        const int ne00_src, const int ne01_src,\n+        const int ne10_dst, const int ne11_dst, const int ne12_dst, const int ne13_dst,\n+        const float sf0, const float sf1, const float sf2, const float sf3,\n+        const float pixel_offset) {\n+    using bicubic_interpolation::bicubic;\n+\n+    const int64_t index              = threadIdx.x + blockIdx.x * blockDim.x;\n+    const int64_t dst_total_elements = ne10_dst * ne11_dst * ne12_dst * ne13_dst;\n+\n+    if (index >= dst_total_elements) {\n+        return;\n+    }\n+\n+    const int i10_dst = index % ne10_dst;\n+    const int i11_dst = (index / ne10_dst) % ne11_dst;\n+    const int i12_dst = (index / (ne10_dst * ne11_dst)) % ne12_dst;\n+    const int i13_dst = index / (ne10_dst * ne11_dst * ne12_dst);\n+\n+    const int i02_src = (int)(i12_dst / sf2);\n+    const int i03_src = (int)(i13_dst / sf3);\n+\n+    const float y_src_f = ((float)i11_dst + pixel_offset) / sf1 - pixel_offset;\n+    const int y0_src    = (int)floorf(y_src_f);\n+    const float dy      = y_src_f - (float)y0_src;\n+\n+    const float x_src_f = ((float)i10_dst + pixel_offset) / sf0 - pixel_offset;\n+    const int x0_src    = (int)floorf(x_src_f);\n+    const float dx      = x_src_f - (float)x0_src;\n+\n+    const char * x_base = (const char *)x + (int64_t)i02_src * nb02 + (int64_t)i03_src * nb03;\n+\n+    auto load = [=](int x_off, int y_off) -> float {\n+        int i00_src = max(0, min(x0_src + x_off, ne00_src - 1));\n+        int i01_src = max(0, min(y0_src + y_off, ne01_src - 1));\n+        return *(const float *)(x_base + (int64_t)i00_src * nb00 + (int64_t)i01_src * nb01);\n+    };\n+\n+    const float result = bicubic(\n+        bicubic(load(-1,-1), load(0,-1), load(1,-1), load(2,-1), dx),\n+        bicubic(load(-1, 0), load(0, 0), load(1, 0), load(2, 0), dx),\n+        bicubic(load(-1, 1), load(0, 1), load(1, 1), load(2, 1), dx),\n+        bicubic(load(-1, 2), load(0, 2), load(1, 2), load(2, 2), dx), dy);\n+\n+    dst[index] = result;\n+}\n+\n static void upscale_f32_cuda(const float * x, float * dst,\n         const int nb00, const int nb01, const int nb02, const int nb03,\n         const int ne10, const int ne11, const int ne12, const int ne13,\n@@ -104,6 +168,18 @@ static void upscale_f32_bilinear_cuda(const float * x, float * dst,\n     upscale_f32_bilinear<<<num_blocks, CUDA_UPSCALE_BLOCK_SIZE,0,stream>>>(x, dst, nb00, nb01, nb02, nb03, ne00_src, ne01_src, ne10_dst, ne11_dst, ne12_dst, ne13_dst, sf0, sf1, sf2, sf3, pixel_offset);\n }\n \n+static void upscale_f32_bicubic_cuda(const float * x, float * dst,\n+        const int nb00, const int nb01, const int nb02, const int nb03,\n+        const int ne00_src, const int ne01_src,\n+        const int ne10_dst, const int ne11_dst, const int ne12_dst, const int ne13_dst,\n+        const float sf0, const float sf1, const float sf2, const float sf3,\n+        const float pixel_offset, cudaStream_t stream) {\n+    const int64_t dst_size   = ne10_dst * ne11_dst * ne12_dst * ne13_dst;\n+    const int64_t num_blocks = (dst_size + CUDA_UPSCALE_BLOCK_SIZE - 1) / CUDA_UPSCALE_BLOCK_SIZE;\n+\n+    upscale_f32_bicubic<<<num_blocks, CUDA_UPSCALE_BLOCK_SIZE,0,stream>>>(x, dst, nb00, nb01, nb02, nb03, ne00_src, ne01_src, ne10_dst, ne11_dst, ne12_dst, ne13_dst, sf0, sf1, sf2, sf3, pixel_offset);\n+}\n+\n void ggml_cuda_op_upscale(ggml_backend_cuda_context & ctx, ggml_tensor * dst) {\n     const ggml_tensor * src0 = dst->src[0];\n     const float * src0_d = (const float *)src0->data;\n@@ -121,17 +197,22 @@ void ggml_cuda_op_upscale(ggml_backend_cuda_context & ctx, ggml_tensor * dst) {\n     float sf2 = (float)dst->ne[2]/src0->ne[2];\n     const float sf3 = (float)dst->ne[3]/src0->ne[3];\n \n+    float pixel_offset = 0.5f;\n+    if (mode_flags & GGML_SCALE_FLAG_ALIGN_CORNERS) {\n+        sf0          = dst->ne[0] > 1 && src0->ne[0] > 1 ? (float)(dst->ne[0] - 1) / (src0->ne[0] - 1) : sf0;\n+        sf1          = dst->ne[1] > 1 && src0->ne[1] > 1 ? (float)(dst->ne[1] - 1) / (src0->ne[1] - 1) : sf1;\n+        pixel_offset = 0.0f;\n+    }\n+\n     if (mode == GGML_SCALE_MODE_NEAREST) {\n         upscale_f32_cuda(src0_d, dst_d, src0->nb[0], src0->nb[1], src0->nb[2], src0->nb[3], dst->ne[0], dst->ne[1], dst->ne[2], dst->ne[3], sf0, sf1, sf2, sf3, stream);\n     } else if (mode == GGML_SCALE_MODE_BILINEAR) {\n-        float pixel_offset = 0.5f;\n-        if (mode_flags & GGML_SCALE_FLAG_ALIGN_CORNERS) {\n-            sf0          = dst->ne[0] > 1 && src0->ne[0] > 1 ? (float)(dst->ne[0] - 1) / (src0->ne[0] - 1) : sf0;\n-            sf1          = dst->ne[1] > 1 && src0->ne[1] > 1 ? (float)(dst->ne[1] - 1) / (src0->ne[1] - 1) : sf1;\n-            pixel_offset = 0.0f;\n-        }\n         upscale_f32_bilinear_cuda(src0_d, dst_d, src0->nb[0], src0->nb[1], src0->nb[2], src0->nb[3],\n                                  src0->ne[0], src0->ne[1], dst->ne[0], dst->ne[1], dst->ne[2], dst->ne[3],\n                                  sf0, sf1, sf2, sf3, pixel_offset, stream);\n+    } else if (mode == GGML_SCALE_MODE_BICUBIC) {\n+        upscale_f32_bicubic_cuda(src0_d, dst_d, src0->nb[0], src0->nb[1], src0->nb[2], src0->nb[3],\n+                                 src0->ne[0], src0->ne[1], dst->ne[0], dst->ne[1], dst->ne[2], dst->ne[3],\n+                                 sf0, sf1, sf2, sf3, pixel_offset, stream);\n     }\n }"
      },
      {
        "filename": "ggml/src/ggml-opencl/ggml-opencl.cpp",
        "status": "modified",
        "additions": 5,
        "deletions": 2,
        "changes": 7,
        "patch": "@@ -2944,8 +2944,11 @@ static bool ggml_opencl_supports_op(ggml_backend_dev_t dev, const struct ggml_te\n             return op->src[0]->type == GGML_TYPE_F32 && op->type == GGML_TYPE_F32; // Assuming F32 for now, can be expanded\n         case GGML_OP_PAD:\n             return op->src[0]->type == GGML_TYPE_F32 && op->type == GGML_TYPE_F32;\n-        case GGML_OP_UPSCALE:\n-            return op->src[0]->type == GGML_TYPE_F32 && op->type == GGML_TYPE_F32;\n+        case GGML_OP_UPSCALE: {\n+            ggml_scale_mode mode = (ggml_scale_mode)(ggml_get_op_params_i32(op, 0) & 0xFF);\n+            return op->src[0]->type == GGML_TYPE_F32 && op->type == GGML_TYPE_F32 &&\n+                   (mode == GGML_SCALE_MODE_NEAREST || mode == GGML_SCALE_MODE_BILINEAR);\n+        }\n         case GGML_OP_CONV_2D:\n             return (op->src[0]->type == GGML_TYPE_F16 && op->src[1]->type == GGML_TYPE_F16 && op->type == GGML_TYPE_F16) ||\n                    (op->src[0]->type == GGML_TYPE_F32 && op->src[1]->type == GGML_TYPE_F32 && op->type == GGML_TYPE_F32) ||"
      },
      {
        "filename": "ggml/src/ggml-vulkan/ggml-vulkan.cpp",
        "status": "modified",
        "additions": 4,
        "deletions": 1,
        "changes": 5,
        "patch": "@@ -603,7 +603,7 @@ struct vk_device_struct {\n     vk_pipeline pipeline_add_id_f32;\n \n     vk_pipeline pipeline_concat_f32, pipeline_concat_f16, pipeline_concat_i32;\n-    vk_pipeline pipeline_upscale_nearest_f32, pipeline_upscale_bilinear_f32;\n+    vk_pipeline pipeline_upscale_nearest_f32, pipeline_upscale_bilinear_f32, pipeline_upscale_bicubic_f32;\n     vk_pipeline pipeline_scale_f32;\n     vk_pipeline pipeline_sqr_f32;\n     vk_pipeline pipeline_sqrt_f32;\n@@ -3687,6 +3687,7 @@ static void ggml_vk_load_shaders(vk_device& device) {\n \n     ggml_vk_create_pipeline(device, device->pipeline_upscale_nearest_f32, \"upscale_f32\", upscale_f32_len, upscale_f32_data, \"main\", 2, sizeof(vk_op_upscale_push_constants), {512, 1, 1}, {GGML_SCALE_MODE_NEAREST}, 1);\n     ggml_vk_create_pipeline(device, device->pipeline_upscale_bilinear_f32, \"upscale_f32\", upscale_f32_len, upscale_f32_data, \"main\", 2, sizeof(vk_op_upscale_push_constants), {512, 1, 1}, {GGML_SCALE_MODE_BILINEAR}, 1);\n+    ggml_vk_create_pipeline(device, device->pipeline_upscale_bicubic_f32, \"upscale_f32\", upscale_f32_len, upscale_f32_data, \"main\", 2, sizeof(vk_op_upscale_push_constants), {512, 1, 1}, {GGML_SCALE_MODE_BICUBIC}, 1);\n \n     ggml_vk_create_pipeline(device, device->pipeline_scale_f32, \"scale_f32\", scale_f32_len, scale_f32_data, \"main\", 2, sizeof(vk_op_unary_push_constants), {512, 1, 1}, {}, 1);\n \n@@ -8195,6 +8196,8 @@ static vk_pipeline ggml_vk_op_get_pipeline(ggml_backend_vk_context * ctx, const\n                     return ctx->device->pipeline_upscale_nearest_f32;\n                 case GGML_SCALE_MODE_BILINEAR:\n                     return ctx->device->pipeline_upscale_bilinear_f32;\n+                case GGML_SCALE_MODE_BICUBIC:\n+                    return ctx->device->pipeline_upscale_bicubic_f32;\n                 default:\n                     return nullptr;\n             }"
      },
      {
        "filename": "ggml/src/ggml-vulkan/vulkan-shaders/upscale.comp",
        "status": "modified",
        "additions": 37,
        "deletions": 0,
        "changes": 37,
        "patch": "@@ -20,6 +20,7 @@ layout (binding = 1) writeonly buffer D {D_TYPE data_d[];};\n // from ggml.h: enum ggml_scale_mode, enum ggml_scale_flag\n #define NEAREST  0\n #define BILINEAR 1\n+#define BICUBIC  2\n \n layout (constant_id = 0) const uint scale_mode = 0;\n \n@@ -61,6 +62,39 @@ float interpolate_bilinear(uint i10, uint i11, uint i12, uint i13) {\n     return fetch_bilinear(c0, c1, d, i12, i13);\n }\n \n+// Bicubic interpolation with alpha = -0.75\n+// https://en.wikipedia.org/wiki/Bicubic_interpolation#Bicubic_convolution_algorithm\n+const vec4 bcoeffs1 = vec4( 1.25, -2.25,  0.0, 1.0);\n+const vec4 bcoeffs2 = vec4(-0.75,  3.75, -6.0, 3.0);\n+vec4 powers(float x) { return vec4(x*x*x, x*x, x, 1); }\n+\n+float bicubic(float p0, float p1, float p2, float p3, float x) {\n+    return p0 * dot(bcoeffs2, powers(x + 1)) +\n+           p1 * dot(bcoeffs1, powers(x    )) +\n+           p2 * dot(bcoeffs1, powers(1 - x)) +\n+           p3 * dot(bcoeffs2, powers(2 - x));\n+}\n+\n+#define FETCH(a,b) data_a[base + clamp(i.x+(a), 0, res.x) * p.nb00 + clamp(i.y+(b), 0, res.y) * p.nb01]\n+\n+float interpolate_bicubic(uint i10, uint i11, uint i12, uint i13) {\n+    const ivec2 res = ivec2(p.ne00 - 1, p.ne01 - 1);\n+\n+    const vec2 coord = (vec2(i10, i11) + p.pixel_offset) / vec2(p.sf0, p.sf1) - p.pixel_offset;\n+    const vec2 d = fract(coord);\n+    const ivec2 i = ivec2(floor(coord));\n+\n+    const uint i02 = uint(i12 / p.sf2);\n+    const uint i03 = uint(i13 / p.sf3);\n+    const uint base = p.a_offset + i03 * p.nb03 + i02 * p.nb02;\n+\n+    return bicubic(\n+        bicubic(FETCH(-1,-1), FETCH(0,-1), FETCH(1,-1), FETCH(2,-1), d.x),\n+        bicubic(FETCH(-1, 0), FETCH(0, 0), FETCH(1, 0), FETCH(2, 0), d.x),\n+        bicubic(FETCH(-1, 1), FETCH(0, 1), FETCH(1, 1), FETCH(2, 1), d.x),\n+        bicubic(FETCH(-1, 2), FETCH(0, 2), FETCH(1, 2), FETCH(2, 2), d.x), d.y);\n+}\n+\n void main() {\n     const uint idx = gl_GlobalInvocationID.z * 262144 + gl_GlobalInvocationID.y * 512 + gl_GlobalInvocationID.x;\n \n@@ -81,6 +115,9 @@ void main() {\n         case BILINEAR:\n             result = interpolate_bilinear(i10, i11, i12, i13);\n             break;\n+        case BICUBIC:\n+            result = interpolate_bicubic(i10, i11, i12, i13);\n+            break;\n     }\n \n     data_d[p.d_offset + idx] = D_TYPE(result);"
      },
      {
        "filename": "tests/test-backend-ops.cpp",
        "status": "modified",
        "additions": 15,
        "deletions": 6,
        "changes": 21,
        "patch": "@@ -272,6 +272,10 @@ static double mean_abs_asymm(const float * a, const float * b, const size_t n, c\n \n // utils for printing the variables of the test cases\n \n+static std::string var_to_str(const std::string & x) {\n+    return x;\n+}\n+\n template<typename T>\n static std::string var_to_str(const T & x) {\n     return std::to_string(x);\n@@ -323,7 +327,8 @@ static std::string var_to_str(ggml_scale_mode mode) {\n     switch (mode) {\n         case GGML_SCALE_MODE_NEAREST:  return \"nearest\";\n         case GGML_SCALE_MODE_BILINEAR: return \"bilinear\";\n-        default:                      return std::to_string(mode);\n+        case GGML_SCALE_MODE_BICUBIC:  return \"bicubic\";\n+        default:                       return std::to_string(mode);\n     }\n }\n \n@@ -5165,7 +5170,9 @@ struct test_interpolate : public test_case {\n     const uint32_t mode = GGML_SCALE_MODE_NEAREST;\n \n     std::string vars() override {\n-        return VARS_TO_STR4(type, ne, ne_tgt, mode);\n+        ggml_scale_mode mode = (ggml_scale_mode)(this->mode & 0xFF);\n+        std::string flags = (this->mode & GGML_SCALE_FLAG_ALIGN_CORNERS) ? \"align_corners\" : \"none\";\n+        return VARS_TO_STR5(type, ne, ne_tgt, mode, flags);\n     }\n \n     test_interpolate(ggml_type type = GGML_TYPE_F32,\n@@ -7224,15 +7231,17 @@ static std::vector<std::unique_ptr<test_case>> make_test_cases_eval() {\n         test_cases.emplace_back(new test_argsort(GGML_TYPE_F32, {2, 8, 8192, 1}, order)); // bailingmoe2 (group selection)\n     }\n \n-    for (ggml_scale_mode mode : {GGML_SCALE_MODE_NEAREST, GGML_SCALE_MODE_BILINEAR}) {\n+    for (ggml_scale_mode mode : {GGML_SCALE_MODE_NEAREST, GGML_SCALE_MODE_BILINEAR, GGML_SCALE_MODE_BICUBIC}) {\n         test_cases.emplace_back(new test_upscale(GGML_TYPE_F32, {512, 512, 3, 2}, 2, mode));\n         test_cases.emplace_back(new test_upscale(GGML_TYPE_F32, {512, 512, 3, 2}, 2, mode, true));\n         test_cases.emplace_back(new test_interpolate(GGML_TYPE_F32, {2, 5,  7, 11}, {5, 7, 11, 13}, mode));\n         test_cases.emplace_back(new test_interpolate(GGML_TYPE_F32, {5, 7, 11, 13}, {2, 5,  7, 11}, mode));\n     }\n-    test_cases.emplace_back(new test_interpolate(GGML_TYPE_F32, {2, 5,  7, 11}, {5, 7, 11, 13}, GGML_SCALE_MODE_BILINEAR | GGML_SCALE_FLAG_ALIGN_CORNERS));\n-    test_cases.emplace_back(new test_interpolate(GGML_TYPE_F32, {1, 4, 3, 2}, {2, 8, 3, 2}, GGML_SCALE_MODE_BILINEAR | GGML_SCALE_FLAG_ALIGN_CORNERS));\n-    test_cases.emplace_back(new test_interpolate(GGML_TYPE_F32, {4, 1, 3, 2}, {1, 1, 3, 2}, GGML_SCALE_MODE_BILINEAR | GGML_SCALE_FLAG_ALIGN_CORNERS));\n+    for (ggml_scale_mode mode : {GGML_SCALE_MODE_BILINEAR, GGML_SCALE_MODE_BICUBIC}) {\n+        test_cases.emplace_back(new test_interpolate(GGML_TYPE_F32, {2, 5, 7, 11}, {5, 7, 11, 13}, mode | GGML_SCALE_FLAG_ALIGN_CORNERS));\n+        test_cases.emplace_back(new test_interpolate(GGML_TYPE_F32, {1, 4, 3, 2}, {2, 8, 3, 2}, mode | GGML_SCALE_FLAG_ALIGN_CORNERS));\n+        test_cases.emplace_back(new test_interpolate(GGML_TYPE_F32, {4, 1, 3, 2}, {1, 1, 3, 2}, mode | GGML_SCALE_FLAG_ALIGN_CORNERS));\n+    }\n \n     test_cases.emplace_back(new test_sum());\n     test_cases.emplace_back(new test_sum_rows());"
      }
    ],
    "num_files": 5,
    "scraped_at": "2025-11-16T23:28:53.872236"
  },
  {
    "pr_number": 16993,
    "title": "kleidiai: add optimized per-channel kernels for Q8_0",
    "body": "Benchmarks from MacBook M4:\r\n\r\nW/ KleidiAI\r\n```\r\nGGML_KLEIDIAI_SME=1 ./bin/llama-bench -m ./Llama-3.2-1B-Instruct-Q8_0.gguf -ngl 0 -t 1\r\n| model                          |       size |     params | backend    | threads |            test |                  t/s |\r\n| ------------------------------ | ---------: | ---------: | ---------- | ------: | --------------: | -------------------: |\r\n| llama 1B Q8_0                  |   1.22 GiB |     1.24 B | CPU        |       1 |           pp512 |        504.01 \u00b1 2.70 |\r\n| llama 1B Q8_0                  |   1.22 GiB |     1.24 B | CPU        |       1 |           tg128 |         93.68 \u00b1 0.16 |\r\n\r\nGGML_KLEIDIAI_SME=0 ./bin/llama-bench -m ./Llama-3.2-1B-Instruct-Q8_0.gguf -ngl 0 -t 1,4\r\n| model                          |       size |     params | backend    | threads |            test |                  t/s |\r\n| ------------------------------ | ---------: | ---------: | ---------- | ------: | --------------: | -------------------: |\r\n| llama 1B Q8_0                  |   1.22 GiB |     1.24 B | CPU        |       1 |           pp512 |        193.94 \u00b1 1.22 |\r\n| llama 1B Q8_0                  |   1.22 GiB |     1.24 B | CPU        |       1 |           tg128 |         43.45 \u00b1 0.34 |\r\n| llama 1B Q8_0                  |   1.22 GiB |     1.24 B | CPU        |       4 |           pp512 |        692.11 \u00b1 0.71 |\r\n| llama 1B Q8_0                  |   1.22 GiB |     1.24 B | CPU        |       4 |           tg128 |       132.24 \u00b1 16.44 |\r\n```\r\n\r\nW/O KleidiAI\r\n```\r\n./bin/llama-bench -m ./Llama-3.2-1B-Instruct-Q8_0.gguf -ngl 0 -t 1,4\r\n| model                          |       size |     params | backend    | threads |            test |                  t/s |\r\n| ------------------------------ | ---------: | ---------: | ---------- | ------: | --------------: | -------------------: |\r\n| llama 1B Q8_0                  |   1.22 GiB |     1.24 B | CPU        |       1 |           pp512 |         44.39 \u00b1 0.52 |\r\n| llama 1B Q8_0                  |   1.22 GiB |     1.24 B | CPU        |       1 |           tg128 |         41.61 \u00b1 0.25 |\r\n| llama 1B Q8_0                  |   1.22 GiB |     1.24 B | CPU        |       4 |           pp512 |        156.83 \u00b1 0.62 |\r\n| llama 1B Q8_0                  |   1.22 GiB |     1.24 B | CPU        |       4 |           tg128 |        115.41 \u00b1 1.82 |\r\n```\r\n",
    "html_url": "https://github.com/ggml-org/llama.cpp/pull/16993",
    "created_at": "2025-11-04T10:07:51Z",
    "merged_at": "2025-11-11T11:20:31Z",
    "merge_commit_sha": "8c583242adfe09cbf35e41353aa01fb96da301a0",
    "base_ref": "master",
    "head_sha": "2e03c52819bc0b529e62fba0d888477fd65c16c3",
    "user": "chaxu01",
    "files": [
      {
        "filename": "ggml/src/ggml-cpu/CMakeLists.txt",
        "status": "modified",
        "additions": 15,
        "deletions": 3,
        "changes": 18,
        "patch": "@@ -579,6 +579,7 @@ function(ggml_add_cpu_backend_variant_impl tag_name)\n             ${KLEIDIAI_SRC}/kai/ukernels/\n             ${KLEIDIAI_SRC}/kai/ukernels/matmul/\n             ${KLEIDIAI_SRC}/kai/ukernels/matmul/matmul_clamp_f32_qsi8d32p_qsi4c32p/\n+            ${KLEIDIAI_SRC}/kai/ukernels/matmul/matmul_clamp_f32_qai8dxp_qsi8cxp/\n             ${KLEIDIAI_SRC}/kai/ukernels/matmul/matmul_clamp_fp32_bf16p_bf16p/\n             ${KLEIDIAI_SRC}/kai/ukernels/matmul/pack/)\n \n@@ -597,23 +598,34 @@ function(ggml_add_cpu_backend_variant_impl tag_name)\n             ${KLEIDIAI_SRC}/kai/ukernels/matmul/pack/kai_lhs_quant_pack_qsi8d32p4x8sb_f32_neon.c\n             ${KLEIDIAI_SRC}/kai/ukernels/matmul/pack/kai_rhs_pack_nxk_qsi4c32ps1s0scalef16_qsu4c32s16s0_neon.c\n             ${KLEIDIAI_SRC}/kai/ukernels/matmul/pack/kai_lhs_quant_pack_qsi8d32p_f32_neon.c\n-            ${KLEIDIAI_SRC}/kai/ukernels/matmul/pack/kai_rhs_pack_nxk_qsi4c32pscalef16_qsu4c32s16s0.c)\n+            ${KLEIDIAI_SRC}/kai/ukernels/matmul/pack/kai_rhs_pack_nxk_qsi4c32pscalef16_qsu4c32s16s0.c\n+            ${KLEIDIAI_SRC}/kai/ukernels/matmul/pack/kai_lhs_quant_pack_qai8dxp_f32.c\n+            ${KLEIDIAI_SRC}/kai/ukernels/matmul/pack/kai_rhs_pack_nxk_qsi8cxp_qsi8cx_neon.c)\n \n         if (NOT DOTPROD_ENABLED MATCHES -1)\n             list(APPEND GGML_KLEIDIAI_SOURCES\n                 ${KLEIDIAI_SRC}/kai/ukernels/matmul/matmul_clamp_f32_qsi8d32p_qsi4c32p/kai_matmul_clamp_f32_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod.c\n                 ${KLEIDIAI_SRC}/kai/ukernels/matmul/matmul_clamp_f32_qsi8d32p_qsi4c32p/kai_matmul_clamp_f32_qsi8d32p1x4_qsi4c32p4x4_1x4_neon_dotprod.c\n-                ${KLEIDIAI_SRC}/kai/ukernels/matmul/matmul_clamp_f32_qsi8d32p_qsi4c32p/kai_matmul_clamp_f32_qsi8d32p4x4_qsi4c32p4x4_16x4_neon_dotprod.c)\n+                ${KLEIDIAI_SRC}/kai/ukernels/matmul/matmul_clamp_f32_qsi8d32p_qsi4c32p/kai_matmul_clamp_f32_qsi8d32p4x4_qsi4c32p4x4_16x4_neon_dotprod.c\n+                ${KLEIDIAI_SRC}/kai/ukernels/matmul/matmul_clamp_f32_qai8dxp_qsi8cxp/kai_matmul_clamp_f32_qai8dxp4x4_qsi8cxp4x4_16x4_neon_dotprod.c\n+                ${KLEIDIAI_SRC}/kai/ukernels/matmul/matmul_clamp_f32_qai8dxp_qsi8cxp/kai_matmul_clamp_f32_qai8dxp1x4_qsi8cxp4x4_1x4_neon_dotprod.c\n+                ${KLEIDIAI_SRC}/kai/ukernels/matmul/matmul_clamp_f32_qai8dxp_qsi8cxp/kai_matmul_clamp_f32_qai8dxp1x8_qsi8cxp4x8_1x4_neon_dotprod.c)\n         endif()\n \n         if (NOT I8MM_ENABLED MATCHES -1)\n-            list(APPEND GGML_KLEIDIAI_SOURCES ${KLEIDIAI_SRC}/kai/ukernels/matmul/matmul_clamp_f32_qsi8d32p_qsi4c32p/kai_matmul_clamp_f32_qsi8d32p4x8_qsi4c32p4x8_16x4_neon_i8mm.c)\n+            list(APPEND GGML_KLEIDIAI_SOURCES\n+                ${KLEIDIAI_SRC}/kai/ukernels/matmul/matmul_clamp_f32_qsi8d32p_qsi4c32p/kai_matmul_clamp_f32_qsi8d32p4x8_qsi4c32p4x8_16x4_neon_i8mm.c\n+                ${KLEIDIAI_SRC}/kai/ukernels/matmul/matmul_clamp_f32_qai8dxp_qsi8cxp/kai_matmul_clamp_f32_qai8dxp4x8_qsi8cxp4x8_16x4_neon_i8mm.c)\n         endif()\n \n         if (NOT SME_ENABLED MATCHES -1)\n             list(APPEND GGML_KLEIDIAI_SOURCES\n                 ${KLEIDIAI_SRC}/kai/ukernels/matmul/matmul_clamp_f32_qsi8d32p_qsi4c32p/kai_matmul_clamp_f32_qsi8d32p1vlx4_qsi4c32p4vlx4_1vlx4vl_sme2_mopa.c\n                 ${KLEIDIAI_SRC}/kai/ukernels/matmul/matmul_clamp_f32_qsi8d32p_qsi4c32p/kai_matmul_clamp_f32_qsi8d32p1x4_qsi4c32p4vlx4_1x4vl_sme2_sdot.c\n+                ${KLEIDIAI_SRC}/kai/ukernels/matmul/matmul_clamp_f32_qai8dxp_qsi8cxp/kai_matmul_clamp_f32_qai8dxp1vlx4_qsi8cxp4vlx4_1vlx4vl_sme2_mopa.c\n+                ${KLEIDIAI_SRC}/kai/ukernels/matmul/matmul_clamp_f32_qai8dxp_qsi8cxp/kai_matmul_clamp_f32_qai8dxp1vlx4_qsi8cxp4vlx4_1vlx4vl_sme2_mopa_asm.S\n+                ${KLEIDIAI_SRC}/kai/ukernels/matmul/matmul_clamp_f32_qai8dxp_qsi8cxp/kai_matmul_clamp_f32_qai8dxp1x4_qsi8cxp4vlx4_1x4vl_sme2_dot.c\n+                ${KLEIDIAI_SRC}/kai/ukernels/matmul/matmul_clamp_f32_qai8dxp_qsi8cxp/kai_matmul_clamp_f32_qai8dxp1x4_qsi8cxp4vlx4_1x4vl_sme2_dot_asm.S\n                 ${KLEIDIAI_SRC}/kai/ukernels/matmul/matmul_clamp_fp32_bf16p_bf16p/kai_matmul_clamp_f32_bf16p2vlx2_bf16p2vlx2_2vlx2vl_sme2_mopa.c\n                 ${KLEIDIAI_SRC}/kai/ukernels/matmul/matmul_clamp_fp32_bf16p_bf16p/kai_matmul_clamp_f32_bf16p2vlx2_bf16p2vlx2_2vlx2vl_sme2_mopa_asm.S\n                 ${KLEIDIAI_SRC}/kai/ukernels/matmul/pack/kai_lhs_pack_bf16p2vlx2_f32_sme.c"
      },
      {
        "filename": "ggml/src/ggml-cpu/kleidiai/kernels.cpp",
        "status": "modified",
        "additions": 283,
        "deletions": 0,
        "changes": 283,
        "patch": "@@ -4,27 +4,39 @@\n \n // KleidiAI micro-kernels\n #include \"kai_matmul_clamp_f32_qsi8d32p_qsi4c32p_interface.h\"\n+#include \"kai_matmul_clamp_f32_qai8dxp_qsi8cxp_interface.h\"\n #include \"kai_matmul_clamp_f32_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod.h\"\n #include \"kai_matmul_clamp_f32_qsi8d32p1x4_qsi4c32p4x4_1x4_neon_dotprod.h\"\n #include \"kai_matmul_clamp_f32_qsi8d32p4x4_qsi4c32p4x4_16x4_neon_dotprod.h\"\n #include \"kai_matmul_clamp_f32_qsi8d32p4x8_qsi4c32p4x8_16x4_neon_i8mm.h\"\n #include \"kai_matmul_clamp_f32_qsi8d32p1vlx4_qsi4c32p4vlx4_1vlx4vl_sme2_mopa.h\"\n #include \"kai_matmul_clamp_f32_qsi8d32p1x4_qsi4c32p4vlx4_1x4vl_sme2_sdot.h\"\n #include \"kai_matmul_clamp_f32_bf16p2vlx2_bf16p2vlx2_2vlx2vl_sme2_mopa.h\"\n+#include \"kai_matmul_clamp_f32_qai8dxp1vlx4_qsi8cxp4vlx4_1vlx4vl_sme2_mopa.h\"\n+#include \"kai_matmul_clamp_f32_qai8dxp1x4_qsi8cxp4vlx4_1x4vl_sme2_dot.h\"\n+#include \"kai_matmul_clamp_f32_qai8dxp1x8_qsi8cxp4x8_1x4_neon_dotprod.h\"\n+#include \"kai_matmul_clamp_f32_qai8dxp1x4_qsi8cxp4x4_1x4_neon_dotprod.h\"\n+#include \"kai_matmul_clamp_f32_qai8dxp4x4_qsi8cxp4x4_16x4_neon_dotprod.h\"\n+#include \"kai_matmul_clamp_f32_qai8dxp4x8_qsi8cxp4x8_16x4_neon_i8mm.h\"\n \n #include \"kai_lhs_pack_bf16p2vlx2_f32_sme.h\"\n #include \"kai_lhs_quant_pack_qsi8d32p_f32.h\"\n #include \"kai_lhs_quant_pack_qsi8d32p4x8sb_f32_neon.h\"\n #include \"kai_lhs_quant_pack_qsi8d32p_f32_neon.h\"\n+#include \"kai_lhs_quant_pack_qai8dxp_f32.h\"\n \n #include \"kai_rhs_pack_kxn_bf16p2vlx2b_f32_x32_sme.h\"\n #include \"kai_rhs_pack_nxk_qsi4c32pscalef16_qsu4c32s16s0.h\"\n #include \"kai_rhs_pack_nxk_qsi4c32ps1s0scalef16_qsu4c32s16s0_neon.h\"\n+#include \"kai_rhs_pack_nxk_qsi8cxp_qsi8cx_neon.h\"\n \n #include \"kai_common.h\"\n \n #include \"simd-mappings.h\"\n \n+#define GGML_COMMON_DECL_CPP\n+#include \"ggml-common.h\"\n+\n #include \"kernels.h\"\n \n #define NELEMS(x) sizeof(x) / sizeof(*x)\n@@ -55,6 +67,14 @@ static inline void kernel_run_fn10(size_t m, size_t n, size_t k, size_t /*bl*/,\n     Fn(m, n, k, lhs, rhs, dst, dst_stride_row, dst_stride_col, clamp_min, clamp_max);\n }\n \n+template<void(*Fn)(size_t,size_t,size_t,const void*,const void*,float*,size_t,size_t,float,float)>\n+static inline void kernel_run_float_fn10(size_t m, size_t n, size_t k, size_t /*bl*/,\n+                                     const void* lhs, const void* rhs, void* dst,\n+                                     size_t dst_stride_row, size_t dst_stride_col,\n+                                     float clamp_min, float clamp_max) {\n+    Fn(m, n, k, lhs, rhs, static_cast<float*>(dst), dst_stride_row, dst_stride_col, clamp_min, clamp_max);\n+}\n+\n template<size_t(*Fn)(size_t,size_t,size_t,size_t,size_t,size_t)>\n static inline size_t lhs_ps_fn6(size_t m, size_t k, size_t bl, size_t mr, size_t kr, size_t sr) {\n     return Fn(m, k, bl, mr, kr, sr);\n@@ -93,6 +113,12 @@ static inline void lhs_pack_void_fn9(size_t m, size_t k, size_t /*bl*/, size_t m\n     Fn(m, k, mr, kr, sr, m_idx_start, lhs, lhs_stride, lhs_packed);\n }\n \n+template<void(*Fn)(size_t,size_t,size_t,size_t,size_t,size_t,const float*,size_t,void*)>\n+static inline void lhs_pack_float_fn9_no_bl(size_t m, size_t k, size_t /*bl*/, size_t mr, size_t kr, size_t sr,\n+                                            size_t m_idx_start, const void * lhs, size_t lhs_stride, void * lhs_packed) {\n+    Fn(m, k, mr, kr, sr, m_idx_start, static_cast<const float*>(lhs), lhs_stride, lhs_packed);\n+}\n+\n template<size_t(*Fn)(size_t,size_t,size_t,size_t,size_t)>\n static inline size_t rhs_ps_fn5(size_t n, size_t k, size_t nr, size_t kr, size_t bl) {\n     return Fn(n, k, nr, kr, bl);\n@@ -124,6 +150,18 @@ static inline void rhs_pack_fn12(size_t num_groups, size_t n, size_t k, size_t n\n        static_cast<const kai_rhs_pack_qs4cxs1s0_param*>(params));\n }\n \n+template<void(*Fn)(size_t,size_t,size_t,size_t,size_t,size_t,const int8_t*,const float*,const float*,void*,size_t,const struct kai_rhs_pack_qsi8cx_params*)>\n+static inline void rhs_pack_scale_fn12(size_t num_groups, size_t n, size_t k, size_t nr, size_t kr, size_t sr, size_t /*bl*/,\n+                                               size_t /*rhs_stride*/, const void* rhs, const void* bias, const void* scale,\n+                                               void* rhs_packed, size_t extra_bytes, const void* params) {\n+    Fn(num_groups, n, k, nr, kr, sr,\n+       static_cast<const int8_t*>(rhs),\n+       static_cast<const float*>(bias),\n+       static_cast<const float*>(scale),\n+       rhs_packed, extra_bytes,\n+       static_cast<const kai_rhs_pack_qsi8cx_params*>(params));\n+}\n+\n template<void(*Fn)(size_t,size_t,size_t,size_t,size_t,size_t,size_t,const void*,const void*,const void*,void*,size_t,const void*)>\n static inline void rhs_pack_fn13(size_t num_groups, size_t n, size_t k, size_t nr, size_t kr, size_t sr, size_t /*bl*/,\n                                                size_t rhs_stride, const void* rhs, const void* bias, const void* scale,\n@@ -213,6 +251,57 @@ static void dequantize_row_qsi4c32ps1s0scalef16(\n     GGML_UNUSED(kr);\n }\n \n+static void dequantize_row_qsi8cxp(\n+    const void *packed_data,\n+    int32_t row_idx,\n+    int64_t k,\n+    float *out,\n+    size_t nr,\n+    size_t packed_row_stride,\n+    size_t kr,\n+    size_t bl,\n+    size_t num_bytes_multiplier\n+) {\n+    GGML_UNUSED(bl);\n+    GGML_UNUSED(num_bytes_multiplier);\n+\n+    const size_t k_internal = ((size_t) k + QK8_0 - 1) / QK8_0 * QK8_0;\n+    const size_t group_idx = row_idx / nr;\n+    const size_t row_in_group = row_idx % nr;\n+\n+    const uint8_t * group_ptr = static_cast<const uint8_t *>(packed_data) + group_idx * packed_row_stride;\n+    const int8_t  * data_base = reinterpret_cast<const int8_t *>(group_ptr);\n+\n+    const size_t num_blocks = k_internal / kr;\n+\n+    for (size_t block = 0; block < num_blocks; ++block) {\n+        const int8_t * block_ptr = data_base + (block * nr + row_in_group) * kr;\n+        for (size_t i = 0; i < kr; ++i) {\n+            const size_t k_idx = block * kr + i;\n+            if (k_idx < (size_t) k) {\n+                out[k_idx] = static_cast<float>(block_ptr[i]);\n+            }\n+        }\n+    }\n+\n+    const uint8_t * sums_ptr = group_ptr + nr * k_internal;\n+    GGML_UNUSED(sums_ptr);\n+\n+    const float * scale_ptr = reinterpret_cast<const float *>(sums_ptr + nr * sizeof(int32_t));\n+    const float scale = scale_ptr[row_in_group];\n+\n+    if (scale == 0.0f) {\n+        for (size_t i = 0; i < (size_t) k; ++i) {\n+            out[i] = 0.0f;\n+        }\n+        return;\n+    }\n+\n+    for (size_t i = 0; i < (size_t) k; ++i) {\n+        out[i] *= scale;\n+    }\n+}\n+\n static ggml_kleidiai_kernels gemm_gemv_kernels[] = {\n #if defined(__ARM_FEATURE_SME)\n     {\n@@ -548,6 +637,174 @@ static ggml_kleidiai_kernels gemm_gemv_kernels[] = {\n #endif\n };\n \n+static ggml_kleidiai_kernels gemm_gemv_kernels_q8[] = {\n+#if defined(__ARM_FEATURE_SME)\n+    {\n+        /* SME GEMM */\n+        {\n+            /* .get_m_step            = */ kai_get_m_step_matmul_clamp_f32_qai8dxp1vlx4_qsi8cxp4vlx4_1vlx4vl_sme2_mopa,\n+            /* .get_n_step            = */ kai_get_n_step_matmul_clamp_f32_qai8dxp1vlx4_qsi8cxp4vlx4_1vlx4vl_sme2_mopa,\n+            /* .get_mr                = */ kai_get_mr_matmul_clamp_f32_qai8dxp1vlx4_qsi8cxp4vlx4_1vlx4vl_sme2_mopa,\n+            /* .get_nr                = */ kai_get_nr_matmul_clamp_f32_qai8dxp1vlx4_qsi8cxp4vlx4_1vlx4vl_sme2_mopa,\n+            /* .get_kr                = */ kai_get_kr_matmul_clamp_f32_qai8dxp1vlx4_qsi8cxp4vlx4_1vlx4vl_sme2_mopa,\n+            /* .get_sr                = */ kai_get_sr_matmul_clamp_f32_qai8dxp1vlx4_qsi8cxp4vlx4_1vlx4vl_sme2_mopa,\n+            /* .get_dst_offset        = */ kai_get_dst_offset_matmul_clamp_f32_qai8dxp1vlx4_qsi8cxp4vlx4_1vlx4vl_sme2_mopa,\n+            /* .get_dst_size          = */ kai_get_dst_size_matmul_clamp_f32_qai8dxp1vlx4_qsi8cxp4vlx4_1vlx4vl_sme2_mopa,\n+            /* .get_lhs_offset_ex     = */ &kernel_offs_fn2<kai_get_lhs_packed_offset_matmul_clamp_f32_qai8dxp1vlx4_qsi8cxp4vlx4_1vlx4vl_sme2_mopa>,\n+            /* .get_rhs_packed_offset_ex = */ &kernel_offs_fn2<kai_get_rhs_packed_offset_matmul_clamp_f32_qai8dxp1vlx4_qsi8cxp4vlx4_1vlx4vl_sme2_mopa>,\n+            /* .run_kernel_ex         = */ &kernel_run_float_fn10<kai_run_matmul_clamp_f32_qai8dxp1vlx4_qsi8cxp4vlx4_1vlx4vl_sme2_mopa>,\n+        },\n+        /* .gemm_lhs_info = */ {\n+            /* .get_offset            = */ kai_get_lhs_offset_lhs_quant_pack_qai8dxp_f32,\n+            /* .get_packed_offset_ex  = */ &lhs_offs_fn5<kai_get_lhs_packed_offset_lhs_quant_pack_qai8dxp_f32>,\n+            /* .packed_size_ex        = */ &lhs_ps_fn5<kai_get_lhs_packed_size_lhs_quant_pack_qai8dxp_f32>,\n+            /* .pack_func_ex          = */ &lhs_pack_float_fn9_no_bl<kai_run_lhs_quant_pack_qai8dxp_f32>,\n+        },\n+        /* SME GEMV */\n+        {\n+            /* .get_m_step            = */ kai_get_m_step_matmul_clamp_f32_qai8dxp1x4_qsi8cxp4vlx4_1x4vl_sme2_dot,\n+            /* .get_n_step            = */ kai_get_n_step_matmul_clamp_f32_qai8dxp1x4_qsi8cxp4vlx4_1x4vl_sme2_dot,\n+            /* .get_mr                = */ kai_get_mr_matmul_clamp_f32_qai8dxp1x4_qsi8cxp4vlx4_1x4vl_sme2_dot,\n+            /* .get_nr                = */ kai_get_nr_matmul_clamp_f32_qai8dxp1x4_qsi8cxp4vlx4_1x4vl_sme2_dot,\n+            /* .get_kr                = */ kai_get_kr_matmul_clamp_f32_qai8dxp1x4_qsi8cxp4vlx4_1x4vl_sme2_dot,\n+            /* .get_sr                = */ kai_get_sr_matmul_clamp_f32_qai8dxp1x4_qsi8cxp4vlx4_1x4vl_sme2_dot,\n+            /* .get_dst_offset        = */ kai_get_dst_offset_matmul_clamp_f32_qai8dxp1x4_qsi8cxp4vlx4_1x4vl_sme2_dot,\n+            /* .get_dst_size          = */ kai_get_dst_size_matmul_clamp_f32_qai8dxp1x4_qsi8cxp4vlx4_1x4vl_sme2_dot,\n+            /* .get_lhs_offset_ex     = */ &kernel_offs_fn2<kai_get_lhs_packed_offset_matmul_clamp_f32_qai8dxp1x4_qsi8cxp4vlx4_1x4vl_sme2_dot>,\n+            /* .get_rhs_packed_offset_ex = */ &kernel_offs_fn2<kai_get_rhs_packed_offset_matmul_clamp_f32_qai8dxp1x4_qsi8cxp4vlx4_1x4vl_sme2_dot>,\n+            /* .run_kernel_ex         = */ &kernel_run_float_fn10<kai_run_matmul_clamp_f32_qai8dxp1x4_qsi8cxp4vlx4_1x4vl_sme2_dot>,\n+        },\n+        /* .gemv_lhs_info = */ {\n+            /* .get_offset            = */ kai_get_lhs_offset_lhs_quant_pack_qai8dxp_f32,\n+            /* .get_packed_offset_ex  = */ &lhs_offs_fn5<kai_get_lhs_packed_offset_lhs_quant_pack_qai8dxp_f32>,\n+            /* .packed_size_ex        = */ &lhs_ps_fn5<kai_get_lhs_packed_size_lhs_quant_pack_qai8dxp_f32>,\n+            /* .pack_func_ex          = */ &lhs_pack_float_fn9_no_bl<kai_run_lhs_quant_pack_qai8dxp_f32>,\n+        },\n+        /* .rhs_info = */ {\n+            /* .packed_stride         = */ kai_get_rhs_packed_stride_rhs_pack_nxk_qsi8cxp_qsi8cx_neon,\n+            /* .to_float              = */ dequantize_row_qsi8cxp,\n+            /* .packed_size_ex        = */ &rhs_ps_fn5<kai_get_rhs_packed_size_rhs_pack_nxk_qsi8cxp_qsi8cx_neon>,\n+            /* .packed_stride_ex      = */ &rhs_stride_fn4<kai_get_rhs_packed_stride_rhs_pack_nxk_qsi8cxp_qsi8cx_neon>,\n+            /* .pack_func_ex          = */ &rhs_pack_scale_fn12<kai_run_rhs_pack_nxk_qsi8cxp_qsi8cx_neon>,\n+        },\n+        /* .required_cpu       = */ CPU_FEATURE_SME,\n+        /* .lhs_type           = */ GGML_TYPE_F32,\n+        /* .rhs_type           = */ GGML_TYPE_Q8_0,\n+        /* .op_type            = */ GGML_TYPE_F32,\n+    },\n+#endif\n+#if defined(__ARM_FEATURE_MATMUL_INT8)\n+    {\n+        /* I8MM GEMM */\n+        {\n+            /* .get_m_step            = */ kai_get_m_step_matmul_clamp_f32_qai8dxp4x8_qsi8cxp4x8_16x4_neon_i8mm,\n+            /* .get_n_step            = */ kai_get_n_step_matmul_clamp_f32_qai8dxp4x8_qsi8cxp4x8_16x4_neon_i8mm,\n+            /* .get_mr                = */ kai_get_mr_matmul_clamp_f32_qai8dxp4x8_qsi8cxp4x8_16x4_neon_i8mm,\n+            /* .get_nr                = */ kai_get_nr_matmul_clamp_f32_qai8dxp4x8_qsi8cxp4x8_16x4_neon_i8mm,\n+            /* .get_kr                = */ kai_get_kr_matmul_clamp_f32_qai8dxp4x8_qsi8cxp4x8_16x4_neon_i8mm,\n+            /* .get_sr                = */ kai_get_sr_matmul_clamp_f32_qai8dxp4x8_qsi8cxp4x8_16x4_neon_i8mm,\n+            /* .get_dst_offset        = */ kai_get_dst_offset_matmul_clamp_f32_qai8dxp4x8_qsi8cxp4x8_16x4_neon_i8mm,\n+            /* .get_dst_size          = */ kai_get_dst_size_matmul_clamp_f32_qai8dxp4x8_qsi8cxp4x8_16x4_neon_i8mm,\n+            /* .get_lhs_offset_ex     = */ &kernel_offs_fn2<kai_get_lhs_packed_offset_matmul_clamp_f32_qai8dxp4x8_qsi8cxp4x8_16x4_neon_i8mm>,\n+            /* .get_rhs_packed_offset_ex = */ &kernel_offs_fn2<kai_get_rhs_packed_offset_matmul_clamp_f32_qai8dxp4x8_qsi8cxp4x8_16x4_neon_i8mm>,\n+            /* .run_kernel_ex         = */ &kernel_run_float_fn10<kai_run_matmul_clamp_f32_qai8dxp4x8_qsi8cxp4x8_16x4_neon_i8mm>,\n+        },\n+        /* .gemm_lhs_info = */ {\n+            /* .get_offset            = */ kai_get_lhs_offset_lhs_quant_pack_qai8dxp_f32,\n+            /* .get_packed_offset_ex  = */ &lhs_offs_fn5<kai_get_lhs_packed_offset_lhs_quant_pack_qai8dxp_f32>,\n+            /* .packed_size_ex        = */ &lhs_ps_fn5<kai_get_lhs_packed_size_lhs_quant_pack_qai8dxp_f32>,\n+            /* .pack_func_ex          = */ &lhs_pack_float_fn9_no_bl<kai_run_lhs_quant_pack_qai8dxp_f32>,\n+        },\n+        /* I8MM GEMV (dotprod fallback) */\n+        {\n+            /* .get_m_step            = */ kai_get_m_step_matmul_clamp_f32_qai8dxp1x8_qsi8cxp4x8_1x4_neon_dotprod,\n+            /* .get_n_step            = */ kai_get_n_step_matmul_clamp_f32_qai8dxp1x8_qsi8cxp4x8_1x4_neon_dotprod,\n+            /* .get_mr                = */ kai_get_mr_matmul_clamp_f32_qai8dxp1x8_qsi8cxp4x8_1x4_neon_dotprod,\n+            /* .get_nr                = */ kai_get_nr_matmul_clamp_f32_qai8dxp1x8_qsi8cxp4x8_1x4_neon_dotprod,\n+            /* .get_kr                = */ kai_get_kr_matmul_clamp_f32_qai8dxp1x8_qsi8cxp4x8_1x4_neon_dotprod,\n+            /* .get_sr                = */ kai_get_sr_matmul_clamp_f32_qai8dxp1x8_qsi8cxp4x8_1x4_neon_dotprod,\n+            /* .get_dst_offset        = */ kai_get_dst_offset_matmul_clamp_f32_qai8dxp1x8_qsi8cxp4x8_1x4_neon_dotprod,\n+            /* .get_dst_size          = */ kai_get_dst_size_matmul_clamp_f32_qai8dxp1x8_qsi8cxp4x8_1x4_neon_dotprod,\n+            /* .get_lhs_offset_ex     = */ &kernel_offs_fn2<kai_get_lhs_packed_offset_matmul_clamp_f32_qai8dxp1x8_qsi8cxp4x8_1x4_neon_dotprod>,\n+            /* .get_rhs_packed_offset_ex = */ &kernel_offs_fn2<kai_get_rhs_packed_offset_matmul_clamp_f32_qai8dxp1x8_qsi8cxp4x8_1x4_neon_dotprod>,\n+            /* .run_kernel_ex         = */ &kernel_run_float_fn10<kai_run_matmul_clamp_f32_qai8dxp1x8_qsi8cxp4x8_1x4_neon_dotprod>,\n+        },\n+        /* .gemv_lhs_info = */ {\n+            /* .get_offset            = */ kai_get_lhs_offset_lhs_quant_pack_qai8dxp_f32,\n+            /* .get_packed_offset_ex  = */ &lhs_offs_fn5<kai_get_lhs_packed_offset_lhs_quant_pack_qai8dxp_f32>,\n+            /* .packed_size_ex        = */ &lhs_ps_fn5<kai_get_lhs_packed_size_lhs_quant_pack_qai8dxp_f32>,\n+            /* .pack_func_ex          = */ &lhs_pack_float_fn9_no_bl<kai_run_lhs_quant_pack_qai8dxp_f32>,\n+        },\n+        /* .rhs_info = */ {\n+            /* .packed_stride         = */ kai_get_rhs_packed_stride_rhs_pack_nxk_qsi8cxp_qsi8cx_neon,\n+            /* .to_float              = */ dequantize_row_qsi8cxp,\n+            /* .packed_size_ex        = */ &rhs_ps_fn5<kai_get_rhs_packed_size_rhs_pack_nxk_qsi8cxp_qsi8cx_neon>,\n+            /* .packed_stride_ex      = */ &rhs_stride_fn4<kai_get_rhs_packed_stride_rhs_pack_nxk_qsi8cxp_qsi8cx_neon>,\n+            /* .pack_func_ex          = */ &rhs_pack_scale_fn12<kai_run_rhs_pack_nxk_qsi8cxp_qsi8cx_neon>,\n+        },\n+        /* .required_cpu       = */ CPU_FEATURE_DOTPROD | CPU_FEATURE_I8MM,\n+        /* .lhs_type           = */ GGML_TYPE_F32,\n+        /* .rhs_type           = */ GGML_TYPE_Q8_0,\n+        /* .op_type            = */ GGML_TYPE_F32,\n+    },\n+#endif\n+#if defined(__ARM_FEATURE_DOTPROD)\n+    {\n+        /* DOTPROD GEMM */\n+        {\n+            /* .get_m_step            = */ kai_get_m_step_matmul_clamp_f32_qai8dxp4x4_qsi8cxp4x4_16x4_neon_dotprod,\n+            /* .get_n_step            = */ kai_get_n_step_matmul_clamp_f32_qai8dxp4x4_qsi8cxp4x4_16x4_neon_dotprod,\n+            /* .get_mr                = */ kai_get_mr_matmul_clamp_f32_qai8dxp4x4_qsi8cxp4x4_16x4_neon_dotprod,\n+            /* .get_nr                = */ kai_get_nr_matmul_clamp_f32_qai8dxp4x4_qsi8cxp4x4_16x4_neon_dotprod,\n+            /* .get_kr                = */ kai_get_kr_matmul_clamp_f32_qai8dxp4x4_qsi8cxp4x4_16x4_neon_dotprod,\n+            /* .get_sr                = */ kai_get_sr_matmul_clamp_f32_qai8dxp4x4_qsi8cxp4x4_16x4_neon_dotprod,\n+            /* .get_dst_offset        = */ kai_get_dst_offset_matmul_clamp_f32_qai8dxp4x4_qsi8cxp4x4_16x4_neon_dotprod,\n+            /* .get_dst_size          = */ kai_get_dst_size_matmul_clamp_f32_qai8dxp4x4_qsi8cxp4x4_16x4_neon_dotprod,\n+            /* .get_lhs_offset_ex     = */ &kernel_offs_fn2<kai_get_lhs_packed_offset_matmul_clamp_f32_qai8dxp4x4_qsi8cxp4x4_16x4_neon_dotprod>,\n+            /* .get_rhs_packed_offset_ex = */ &kernel_offs_fn2<kai_get_rhs_packed_offset_matmul_clamp_f32_qai8dxp4x4_qsi8cxp4x4_16x4_neon_dotprod>,\n+            /* .run_kernel_ex         = */ &kernel_run_float_fn10<kai_run_matmul_clamp_f32_qai8dxp4x4_qsi8cxp4x4_16x4_neon_dotprod>,\n+        },\n+        /* .gemm_lhs_info = */ {\n+            /* .get_offset            = */ kai_get_lhs_offset_lhs_quant_pack_qai8dxp_f32,\n+            /* .get_packed_offset_ex  = */ &lhs_offs_fn5<kai_get_lhs_packed_offset_lhs_quant_pack_qai8dxp_f32>,\n+            /* .packed_size_ex        = */ &lhs_ps_fn5<kai_get_lhs_packed_size_lhs_quant_pack_qai8dxp_f32>,\n+            /* .pack_func_ex          = */ &lhs_pack_float_fn9_no_bl<kai_run_lhs_quant_pack_qai8dxp_f32>,\n+        },\n+        /* DOTPROD GEMV */\n+        {\n+            /* .get_m_step            = */ kai_get_m_step_matmul_clamp_f32_qai8dxp1x4_qsi8cxp4x4_1x4_neon_dotprod,\n+            /* .get_n_step            = */ kai_get_n_step_matmul_clamp_f32_qai8dxp1x4_qsi8cxp4x4_1x4_neon_dotprod,\n+            /* .get_mr                = */ kai_get_mr_matmul_clamp_f32_qai8dxp1x4_qsi8cxp4x4_1x4_neon_dotprod,\n+            /* .get_nr                = */ kai_get_nr_matmul_clamp_f32_qai8dxp1x4_qsi8cxp4x4_1x4_neon_dotprod,\n+            /* .get_kr                = */ kai_get_kr_matmul_clamp_f32_qai8dxp1x4_qsi8cxp4x4_1x4_neon_dotprod,\n+            /* .get_sr                = */ kai_get_sr_matmul_clamp_f32_qai8dxp1x4_qsi8cxp4x4_1x4_neon_dotprod,\n+            /* .get_dst_offset        = */ kai_get_dst_offset_matmul_clamp_f32_qai8dxp1x4_qsi8cxp4x4_1x4_neon_dotprod,\n+            /* .get_dst_size          = */ kai_get_dst_size_matmul_clamp_f32_qai8dxp1x4_qsi8cxp4x4_1x4_neon_dotprod,\n+            /* .get_lhs_offset_ex     = */ &kernel_offs_fn2<kai_get_lhs_packed_offset_matmul_clamp_f32_qai8dxp1x4_qsi8cxp4x4_1x4_neon_dotprod>,\n+            /* .get_rhs_packed_offset_ex = */ &kernel_offs_fn2<kai_get_rhs_packed_offset_matmul_clamp_f32_qai8dxp1x4_qsi8cxp4x4_1x4_neon_dotprod>,\n+            /* .run_kernel_ex         = */ &kernel_run_float_fn10<kai_run_matmul_clamp_f32_qai8dxp1x4_qsi8cxp4x4_1x4_neon_dotprod>,\n+        },\n+        /* .gemv_lhs_info = */ {\n+            /* .get_offset            = */ kai_get_lhs_offset_lhs_quant_pack_qai8dxp_f32,\n+            /* .get_packed_offset_ex  = */ &lhs_offs_fn5<kai_get_lhs_packed_offset_lhs_quant_pack_qai8dxp_f32>,\n+            /* .packed_size_ex        = */ &lhs_ps_fn5<kai_get_lhs_packed_size_lhs_quant_pack_qai8dxp_f32>,\n+            /* .pack_func_ex          = */ &lhs_pack_float_fn9_no_bl<kai_run_lhs_quant_pack_qai8dxp_f32>,\n+        },\n+        /* .rhs_info = */ {\n+            /* .packed_stride         = */ kai_get_rhs_packed_stride_rhs_pack_nxk_qsi8cxp_qsi8cx_neon,\n+            /* .to_float              = */ dequantize_row_qsi8cxp,\n+            /* .packed_size_ex        = */ &rhs_ps_fn5<kai_get_rhs_packed_size_rhs_pack_nxk_qsi8cxp_qsi8cx_neon>,\n+            /* .packed_stride_ex      = */ &rhs_stride_fn4<kai_get_rhs_packed_stride_rhs_pack_nxk_qsi8cxp_qsi8cx_neon>,\n+            /* .pack_func_ex          = */ &rhs_pack_scale_fn12<kai_run_rhs_pack_nxk_qsi8cxp_qsi8cx_neon>,\n+        },\n+        /* .required_cpu       = */ CPU_FEATURE_DOTPROD,\n+        /* .lhs_type           = */ GGML_TYPE_F32,\n+        /* .rhs_type           = */ GGML_TYPE_Q8_0,\n+        /* .op_type            = */ GGML_TYPE_F32,\n+    },\n+#endif\n+};\n+\n ggml_kleidiai_kernels * ggml_kleidiai_select_kernels(cpu_feature cpu_features, const ggml_tensor * tensor) {\n     ggml_kleidiai_kernels * kernel = nullptr;\n \n@@ -562,6 +819,17 @@ ggml_kleidiai_kernels * ggml_kleidiai_select_kernels(cpu_feature cpu_features, c\n                 break;\n             }\n         }\n+        if (!kernel) {\n+            for (size_t i = 0; i < NELEMS(gemm_gemv_kernels_q8); ++i) {\n+                if ((cpu_features & gemm_gemv_kernels_q8[i].required_cpu) == gemm_gemv_kernels_q8[i].required_cpu &&\n+                    gemm_gemv_kernels_q8[i].lhs_type == tensor->src[1]->type &&\n+                    gemm_gemv_kernels_q8[i].rhs_type == tensor->src[0]->type &&\n+                    gemm_gemv_kernels_q8[i].op_type  == tensor->type) {\n+                    kernel = &gemm_gemv_kernels_q8[i];\n+                    break;\n+                }\n+            }\n+        }\n #endif\n     }\n \n@@ -582,3 +850,18 @@ ggml_kleidiai_kernels * ggml_kleidiai_select_kernels_q4_0(cpu_feature features)\n \n     return kernels;\n }\n+\n+ggml_kleidiai_kernels * ggml_kleidiai_select_kernels_q8_0(cpu_feature features) {\n+    ggml_kleidiai_kernels * kernels = nullptr;\n+\n+#if defined(__ARM_FEATURE_SME) || defined(__ARM_FEATURE_DOTPROD) || defined(__ARM_FEATURE_MATMUL_INT8)\n+    for (size_t i = 0; i < NELEMS(gemm_gemv_kernels_q8); ++i) {\n+        if ((features & gemm_gemv_kernels_q8[i].required_cpu) == gemm_gemv_kernels_q8[i].required_cpu) {\n+            kernels = &gemm_gemv_kernels_q8[i];\n+            break;\n+        }\n+    }\n+#endif\n+\n+    return kernels;\n+}"
      },
      {
        "filename": "ggml/src/ggml-cpu/kleidiai/kernels.h",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -87,3 +87,4 @@ struct ggml_kleidiai_kernels {\n \n ggml_kleidiai_kernels * ggml_kleidiai_select_kernels(cpu_feature cpu_features, const ggml_tensor * tensor);\n ggml_kleidiai_kernels * ggml_kleidiai_select_kernels_q4_0(cpu_feature features);\n+ggml_kleidiai_kernels * ggml_kleidiai_select_kernels_q8_0(cpu_feature features);"
      },
      {
        "filename": "ggml/src/ggml-cpu/kleidiai/kleidiai.cpp",
        "status": "modified",
        "additions": 235,
        "deletions": 34,
        "changes": 269,
        "patch": "@@ -5,10 +5,13 @@\n #include <assert.h>\n #include <atomic>\n #include <cfloat>\n+#include <cmath>\n+#include <algorithm>\n #include <stdexcept>\n #include <stdint.h>\n #include <string.h>\n #include <string>\n+#include <vector>\n #if defined(__linux__)\n #include <asm/hwcap.h>\n #include <sys/auxv.h>\n@@ -38,8 +41,9 @@\n \n struct ggml_kleidiai_context {\n     cpu_feature features;\n-    ggml_kleidiai_kernels * kernels;\n-} static ctx = { CPU_FEATURE_NONE, NULL };\n+    ggml_kleidiai_kernels * kernels_q4;\n+    ggml_kleidiai_kernels * kernels_q8;\n+} static ctx = { CPU_FEATURE_NONE, NULL, NULL };\n \n static const char* cpu_feature_to_string(cpu_feature f) {\n     switch (f) {\n@@ -73,10 +77,14 @@ static void init_kleidiai_context(void) {\n         if (sme_enabled != 0) {\n             ctx.features |= ggml_cpu_has_sme() ? CPU_FEATURE_SME : CPU_FEATURE_NONE;\n         }\n-        ctx.kernels = ggml_kleidiai_select_kernels_q4_0(ctx.features);\n+        ctx.kernels_q4 = ggml_kleidiai_select_kernels_q4_0(ctx.features);\n+        ctx.kernels_q8 = ggml_kleidiai_select_kernels_q8_0(ctx.features);\n #ifndef NDEBUG\n-        if (ctx.kernels) {\n-            GGML_LOG_DEBUG(\"kleidiai: using kernel with CPU feature %s\\n\", cpu_feature_to_string(ctx.kernels->required_cpu));\n+        if (ctx.kernels_q4) {\n+            GGML_LOG_DEBUG(\"kleidiai: using q4 kernel with CPU feature %s\\n\", cpu_feature_to_string(ctx.kernels_q4->required_cpu));\n+        }\n+        if (ctx.kernels_q8) {\n+            GGML_LOG_DEBUG(\"kleidiai: using q8 kernel with CPU feature %s\\n\", cpu_feature_to_string(ctx.kernels_q8->required_cpu));\n         }\n #endif\n     }\n@@ -130,6 +138,9 @@ class tensor_traits : public ggml::cpu::tensor_traits {\n         if (kernels->rhs_type == GGML_TYPE_Q4_0) {\n             if (!lhs_info->packed_size_ex) return false;\n             size = lhs_info->packed_size_ex(m, k, QK4_0, mr, kr, sr);\n+        } else if (kernels->rhs_type == GGML_TYPE_Q8_0) {\n+            if (!lhs_info->packed_size_ex) return false;\n+            size = lhs_info->packed_size_ex(m, k, QK8_0, mr, kr, sr);\n         } else if (kernels->rhs_type == GGML_TYPE_F16) {\n             if (!lhs_info->packed_size_ex || !kernels->rhs_info.packed_size_ex) return false;\n             const int64_t lhs_batch_size0 = op->src[1]->ne[2];\n@@ -149,11 +160,13 @@ class tensor_traits : public ggml::cpu::tensor_traits {\n         if (dst->op == GGML_OP_MUL_MAT) {\n             if (dst->src[0]->type == GGML_TYPE_Q4_0) {\n                 return compute_forward_q4_0(params, dst);\n+            } else if (dst->src[0]->type == GGML_TYPE_Q8_0) {\n+                return compute_forward_q8_0(params, dst);\n             } else if (dst->src[0]->type == GGML_TYPE_F16) {\n                 return compute_forward_fp16(params, dst);\n             }\n         } else if (dst->op == GGML_OP_GET_ROWS) {\n-            if (dst->src[0]->type == GGML_TYPE_Q4_0) {\n+            if (dst->src[0]->type == GGML_TYPE_Q4_0 || dst->src[0]->type == GGML_TYPE_Q8_0) {\n                 return compute_forward_get_rows(params, dst);\n             }\n         }\n@@ -400,19 +413,120 @@ class tensor_traits : public ggml::cpu::tensor_traits {\n         return true;\n     }\n \n-    bool compute_forward_get_rows(struct ggml_compute_params * params, struct ggml_tensor * dst) {\n-        GGML_ASSERT(dst->src[0]->type == GGML_TYPE_Q4_0);\n-        if (!ctx.kernels) {\n+    bool compute_forward_q8_0(struct ggml_compute_params * params, struct ggml_tensor * dst) {\n+        GGML_ASSERT(dst->src[0]->type == GGML_TYPE_Q8_0);\n+\n+        const ggml_tensor * src0 = dst->src[0];\n+        const ggml_tensor * src1 = dst->src[1];\n+\n+        GGML_TENSOR_BINARY_OP_LOCALS\n+\n+        ggml_kleidiai_kernels *kernels = ggml_kleidiai_select_kernels(ctx.features, dst);\n+        if (!kernels) {\n             return false;\n         }\n \n+        bool is_gemv = src1->ne[1] == 1;\n+        kernel_info * kernel = is_gemv ? &kernels->gemv : &kernels->gemm;\n+        lhs_packing_info * lhs_info = is_gemv ? &kernels->gemv_lhs_info : &kernels->gemm_lhs_info;\n+\n+        if (!kernel || !lhs_info->get_packed_offset_ex || !lhs_info->pack_func_ex ||\n+            !kernel->get_rhs_packed_offset_ex || !kernel->run_kernel_ex || !kernel->get_dst_offset) {\n+            return false;\n+        }\n+\n+        const int ith = params->ith;\n+        const int nth_raw = params->nth;\n+        const int nth = nth_raw > 0 ? nth_raw : 1;\n+\n+        const size_t k = ne00;\n+        const size_t m = ne11;\n+        const size_t n = ne01;\n+\n+        size_t mr = kernel->get_mr();\n+        size_t kr = kernel->get_kr();\n+        size_t sr = kernel->get_sr();\n+\n+        const uint8_t * lhs        = static_cast<const uint8_t *>(src1->data);\n+        uint8_t * lhs_packed       = static_cast<uint8_t *>(params->wdata);\n+        const uint8_t * rhs_packed = static_cast<const uint8_t *>(src0->data);\n+\n+        const size_t n_step = kernel->get_n_step();\n+        const size_t num_n_per_thread = kai_roundup(kai_roundup(n, nth) / nth, n_step);\n+        const size_t n_start = ith * num_n_per_thread;\n+\n+        size_t n_to_process = 0;\n+        if (n_start < n) {\n+            n_to_process = num_n_per_thread;\n+            if ((n_start + n_to_process) > n) {\n+                n_to_process = n - n_start;\n+            }\n+        }\n+\n+        const size_t num_m_per_thread = kai_roundup(m, mr * nth) / nth;\n+        const size_t m_start = ith * num_m_per_thread;\n+        size_t m_to_process = num_m_per_thread;\n+        if ((m_start + m_to_process) > m) {\n+            m_to_process = m - m_start;\n+        }\n+\n+        if (m_start < m) {\n+            const size_t src_stride        = src1->nb[1];\n+            const float * src_ptr          = reinterpret_cast<const float *>(lhs + lhs_info->get_offset(m_start, dst->src[1]->nb[1]));\n+            const size_t lhs_packed_offset = lhs_info->get_packed_offset_ex(m_start, k, 0, mr, kr, sr);\n+            void * lhs_packed_ptr          = static_cast<void *>(lhs_packed + lhs_packed_offset);\n+\n+            lhs_info->pack_func_ex(m_to_process, k, 0, mr, kr, sr, 0, src_ptr, src_stride, lhs_packed_ptr);\n+        }\n+\n+        ggml_barrier(params->threadpool);\n+\n+        const size_t dst_stride        = dst->nb[1];\n+        const size_t lhs_packed_offset = lhs_info->get_packed_offset_ex(0, k, 0, mr, kr, sr);\n+        const size_t rhs_packed_offset = kernel->get_rhs_packed_offset_ex(n_start, k, 0);\n+        const size_t dst_offset        = kernel->get_dst_offset(0, n_start, dst_stride);\n+        const void * rhs_ptr           = static_cast<const void *>(rhs_packed + rhs_packed_offset);\n+        const void * lhs_ptr           = static_cast<const void *>(lhs_packed + lhs_packed_offset);\n+        float * dst_ptr                = reinterpret_cast<float *>(static_cast<uint8_t *>(dst->data) + dst_offset);\n+\n+        if (n_to_process > 0) {\n+            kernel->run_kernel_ex(m, n_to_process, k, 0, lhs_ptr, rhs_ptr, dst_ptr, dst_stride,\n+                                  sizeof(float), -FLT_MAX, FLT_MAX);\n+        }\n+\n+        return true;\n+    }\n+\n+    bool compute_forward_get_rows(struct ggml_compute_params * params, struct ggml_tensor * dst) {\n         const ggml_tensor * src0 = dst->src[0];\n         const ggml_tensor * src1 = dst->src[1];\n \n         GGML_TENSOR_BINARY_OP_LOCALS\n \n-        rhs_packing_info * rhs_info = &ctx.kernels->rhs_info;\n-        kernel_info * kernel        = &ctx.kernels->gemm;\n+        ggml_kleidiai_kernels * kernels = nullptr;\n+        size_t block_len = 0;\n+        size_t num_bytes_multiplier = 0;\n+\n+        if (dst->src[0]->type == GGML_TYPE_Q4_0) {\n+            if (!ctx.kernels_q4) {\n+                return false;\n+            }\n+            kernels = ctx.kernels_q4;\n+            block_len = QK4_0;\n+            num_bytes_multiplier = sizeof(uint16_t);\n+        } else if (dst->src[0]->type == GGML_TYPE_Q8_0) {\n+            if (!ctx.kernels_q8) {\n+                return false;\n+            }\n+            kernels = ctx.kernels_q8;\n+            block_len = QK8_0;\n+            num_bytes_multiplier = sizeof(float);\n+        } else {\n+            return false;\n+        }\n+\n+        rhs_packing_info * rhs_info = &kernels->rhs_info;\n+        kernel_info * kernel        = &kernels->gemm;\n         if (!rhs_info->to_float || !kernel->get_nr) {\n             return false;\n         }\n@@ -423,8 +537,7 @@ class tensor_traits : public ggml::cpu::tensor_traits {\n         const size_t block_rows = kernel->get_nr();\n         const size_t kr         = kernel->get_kr();\n \n-        const size_t num_bytes_multiplier = sizeof(uint16_t);\n-        const size_t packed_stride = rhs_info->packed_stride(nc, block_rows, kr, QK4_0);\n+        const size_t packed_stride = rhs_info->packed_stride(nc, block_rows, kr, block_len);\n \n         const int ith = params->ith;\n         const int nth = params->nth;\n@@ -439,29 +552,99 @@ class tensor_traits : public ggml::cpu::tensor_traits {\n             GGML_ASSERT(row_idx >= 0 && row_idx < src0->ne[1]);\n \n             float *out = (float *)((char *)dst->data + i * nb1);\n-            rhs_info->to_float(src0->data, row_idx, nc, out, block_rows, packed_stride, kr, QK4_0, num_bytes_multiplier);\n+            rhs_info->to_float(src0->data, row_idx, nc, out, block_rows, packed_stride, kr, block_len, num_bytes_multiplier);\n         }\n \n         return true;\n     }\n \n public:\n     int repack(struct ggml_tensor * tensor, const void * data, size_t data_size) {\n-        GGML_ASSERT(tensor->type == GGML_TYPE_Q4_0);\n-        GGML_ASSERT(ctx.kernels);\n         const size_t n = tensor->ne[1];\n         const size_t k = tensor->ne[0];\n-        size_t nr      = ctx.kernels->gemm.get_nr();\n-        size_t kr      = ctx.kernels->gemm.get_kr();\n-        size_t sr      = ctx.kernels->gemm.get_sr();\n \n-        struct kai_rhs_pack_qs4cxs1s0_param params;\n-        params.lhs_zero_point = 1;\n-        params.rhs_zero_point = 8;\n-        ctx.kernels->rhs_info.pack_func_ex(1, n, k, nr, kr, sr, QK4_0, 0, (const uint8_t*)data, nullptr, nullptr, tensor->data, 0, &params);\n+        if (tensor->type == GGML_TYPE_Q4_0) {\n+            if (!ctx.kernels_q4) {\n+                return -1;\n+            }\n+            size_t nr = ctx.kernels_q4->gemm.get_nr();\n+            size_t kr = ctx.kernels_q4->gemm.get_kr();\n+            size_t sr = ctx.kernels_q4->gemm.get_sr();\n+\n+            struct kai_rhs_pack_qs4cxs1s0_param params;\n+            params.lhs_zero_point = 1;\n+            params.rhs_zero_point = 8;\n+            ctx.kernels_q4->rhs_info.pack_func_ex(1, n, k, nr, kr, sr, QK4_0, 0,\n+                                                  static_cast<const uint8_t *>(data),\n+                                                  nullptr, nullptr, tensor->data, 0, &params);\n+            GGML_UNUSED(data_size);\n+            return 0;\n+        } else if (tensor->type == GGML_TYPE_Q8_0) {\n+            if (!ctx.kernels_q8) {\n+                return -1;\n+            }\n+\n+            const size_t row_stride = tensor->nb[1];\n+            const size_t k_blocks   = (k + QK8_0 - 1) / QK8_0;\n+\n+            std::vector<int8_t> qdata(n * k, 0);\n+            std::vector<float> scales(n, 0.0f);\n+\n+            for (size_t row = 0; row < n; ++row) {\n+                const auto * row_blocks = reinterpret_cast<const block_q8_0 *>(\n+                    static_cast<const uint8_t *>(data) + row * row_stride);\n+\n+                float max_abs = 0.0f;\n+                for (size_t block = 0; block < k_blocks; ++block) {\n+                    const block_q8_0 & blk = row_blocks[block];\n+                    const float d = GGML_FP16_TO_FP32(blk.d);\n+                    for (size_t l = 0; l < QK8_0; ++l) {\n+                        const size_t linear_idx = block * QK8_0 + l;\n+                        if (linear_idx >= k) {\n+                            break;\n+                        }\n+                        const float value = d * blk.qs[l];\n+                        max_abs = std::max(max_abs, std::fabs(value));\n+                    }\n+                }\n+\n+                float scale = max_abs > 0.0f ? max_abs / 127.0f : 0.0f;\n+                scales[row] = scale;\n+                const float inv_scale = scale > 0.0f ? 1.0f / scale : 0.0f;\n+\n+                for (size_t block = 0; block < k_blocks; ++block) {\n+                    const block_q8_0 & blk = row_blocks[block];\n+                    const float d = GGML_FP16_TO_FP32(blk.d);\n+                    for (size_t l = 0; l < QK8_0; ++l) {\n+                        const size_t linear_idx = block * QK8_0 + l;\n+                        if (linear_idx >= k) {\n+                            break;\n+                        }\n+                        const float value = d * blk.qs[l];\n+                        int32_t q = scale > 0.0f ? static_cast<int32_t>(std::lround(value * inv_scale)) : 0;\n+                        q = std::clamp(q, -127, 127);\n+                        qdata[row * k + linear_idx] = static_cast<int8_t>(q);\n+                    }\n+                }\n+            }\n+\n+            size_t nr = ctx.kernels_q8->gemm.get_nr();\n+            size_t kr = ctx.kernels_q8->gemm.get_kr();\n+            size_t sr = ctx.kernels_q8->gemm.get_sr();\n+\n+            struct kai_rhs_pack_qsi8cx_params params;\n+            params.lhs_zero_point = 1;\n+            params.scale_multiplier = 1.0f;\n+\n+            ctx.kernels_q8->rhs_info.pack_func_ex(1, n, k, nr, kr, sr, 0, 0,\n+                                                  qdata.data(), nullptr, scales.data(),\n+                                                  tensor->data, 0, &params);\n+            GGML_UNUSED(data_size);\n+            return 0;\n+        }\n \n-        return 0;\n         GGML_UNUSED(data_size);\n+        return -1;\n     }\n };\n \n@@ -518,27 +701,45 @@ static size_t ggml_backend_cpu_kleidiai_buffer_type_get_alignment(ggml_backend_b\n }\n \n static size_t ggml_backend_cpu_kleidiai_buffer_type_get_alloc_size(ggml_backend_buffer_type_t buft, const struct ggml_tensor * tensor) {\n-    GGML_ASSERT(tensor->type == GGML_TYPE_Q4_0);\n-    GGML_ASSERT(ctx.kernels);\n+    GGML_UNUSED(buft);\n \n-    const size_t n  = tensor->ne[1];\n-    const size_t k  = tensor->ne[0];\n-    const size_t nr = ctx.kernels->gemm.get_nr();\n-    const size_t kr = ctx.kernels->gemm.get_kr();\n+    const size_t n = tensor->ne[1];\n+    const size_t k = tensor->ne[0];\n+\n+    ggml_kleidiai_kernels * kernels = nullptr;\n+    size_t block_len = 0;\n+\n+    if (tensor->type == GGML_TYPE_Q4_0) {\n+        GGML_ASSERT(ctx.kernels_q4);\n+        kernels = ctx.kernels_q4;\n+        block_len = QK4_0;\n+    } else if (tensor->type == GGML_TYPE_Q8_0) {\n+        GGML_ASSERT(ctx.kernels_q8);\n+        kernels = ctx.kernels_q8;\n+        block_len = QK8_0;\n+    } else {\n+        return 0;\n+    }\n \n-    return ctx.kernels->rhs_info.packed_size_ex(n, k, nr, kr, QK4_0);\n+    const size_t nr = kernels->gemm.get_nr();\n+    const size_t kr = kernels->gemm.get_kr();\n+    const size_t packed = kernels->rhs_info.packed_size_ex(n, k, nr, kr, block_len);\n+    const size_t raw     = ggml_nbytes(tensor);\n \n-    GGML_UNUSED(buft);\n+    return packed > raw ? packed : raw;\n }\n \n namespace ggml::cpu::kleidiai {\n class extra_buffer_type : ggml::cpu::extra_buffer_type {\n     bool supports_op(ggml_backend_dev_t, const struct ggml_tensor * op) override {\n         if ((op->op == GGML_OP_MUL_MAT || op->op == GGML_OP_GET_ROWS) &&\n-            op->src[0]->type == GGML_TYPE_Q4_0 &&\n+            (op->src[0]->type == GGML_TYPE_Q4_0 || op->src[0]->type == GGML_TYPE_Q8_0) &&\n             op->src[0]->buffer &&\n             (ggml_n_dims(op->src[0]) == 2) &&\n-            op->src[0]->buffer->buft == ggml_backend_cpu_kleidiai_buffer_type() && ctx.kernels) {\n+            op->src[0]->buffer->buft == ggml_backend_cpu_kleidiai_buffer_type()) {\n+            if (((op->src[0]->type == GGML_TYPE_Q4_0) ? ctx.kernels_q4 : ctx.kernels_q8) == nullptr) {\n+                return false;\n+            }\n             if (op->src[1]->buffer && !ggml_backend_buft_is_host(op->src[1]->buffer->buft)) {\n                 return false;\n             }"
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T23:28:58.244859"
  },
  {
    "pr_number": 16988,
    "title": "CUDA: fix crash on uneven context",
    "body": "Fixes https://github.com/ggml-org/llama.cpp/issues/16976 .\r\n\r\nThe problem is that the CUDA kernel selection logic does not check strides, so it's trying to run kernels where the strides don't fit. The tests don't detect this because the strides are always constructed as `2*ne00`.\r\n\r\n@ggerganov I didn't see a warning w.r.t. the KV cache having an inconvenient size, I think it would make sense to add one.",
    "html_url": "https://github.com/ggml-org/llama.cpp/pull/16988",
    "created_at": "2025-11-04T07:54:57Z",
    "merged_at": "2025-11-06T13:05:48Z",
    "merge_commit_sha": "aa374175c30184aeb1813ec71fc68780dd073906",
    "base_ref": "master",
    "head_sha": "41735c24b08c2c4083a7a90a27767a3b8bf03777",
    "user": "JohannesGaessler",
    "files": [
      {
        "filename": "ggml/src/ggml-cuda/ggml-cuda.cu",
        "status": "modified",
        "additions": 6,
        "deletions": 6,
        "changes": 12,
        "patch": "@@ -2113,7 +2113,7 @@ static bool ggml_cuda_should_fuse_mul_mat_vec_f(const ggml_tensor * tensor) {\n         src1->type == GGML_TYPE_F32 && dst->type == GGML_TYPE_F32;\n \n     const int cc      = ggml_cuda_info().devices[ggml_cuda_get_device()].cc;\n-    use_mul_mat_vec_f = use_mul_mat_vec_f && ggml_cuda_should_use_mmvf(src0->type, cc, src0->ne, is_mul_mat_id ? src1->ne[2] : src1->ne[1]);\n+    use_mul_mat_vec_f = use_mul_mat_vec_f && ggml_cuda_should_use_mmvf(src0->type, cc, src0->ne, src0->nb, is_mul_mat_id ? src1->ne[2] : src1->ne[1]);\n \n     const bool split = ggml_backend_buft_is_cuda_split(src0->buffer->buft) ||\n                        ggml_backend_buft_is_cuda_split(src1->buffer->buft);\n@@ -2207,16 +2207,16 @@ static void ggml_cuda_mul_mat(ggml_backend_cuda_context & ctx, const ggml_tensor\n             const int cc            = ggml_cuda_info().devices[id].cc;\n             const int warp_size     = ggml_cuda_info().devices[id].warp_size;\n             use_mul_mat_q           = use_mul_mat_q             && ggml_cuda_should_use_mmq(src0->type, cc, src1->ne[1]);\n-            use_mul_mat_f           = use_mul_mat_f             && ggml_cuda_should_use_mmf(src0->type, cc, warp_size, src0->ne, src1->ne[1], /*mul_mat_id=*/false);\n-            use_mul_mat_vec_f       = use_mul_mat_vec_f         && ggml_cuda_should_use_mmvf(src0->type, cc, src0->ne, src1->ne[1]);\n+            use_mul_mat_f           = use_mul_mat_f             && ggml_cuda_should_use_mmf(src0->type, cc, warp_size, src0->ne, src0->nb, src1->ne[1], /*mul_mat_id=*/false);\n+            use_mul_mat_vec_f       = use_mul_mat_vec_f         && ggml_cuda_should_use_mmvf(src0->type, cc, src0->ne, src0->nb, src1->ne[1]);\n             any_gpus_with_slow_fp16 = any_gpus_with_slow_fp16   || !fast_fp16_hardware_available(cc);\n         }\n     } else {\n         const int cc            = ggml_cuda_info().devices[ctx.device].cc;\n         const int warp_size     = ggml_cuda_info().devices[ctx.device].warp_size;\n         use_mul_mat_q           = use_mul_mat_q             && ggml_cuda_should_use_mmq(src0->type, cc, src1->ne[1]);\n-        use_mul_mat_f           = use_mul_mat_f             && ggml_cuda_should_use_mmf(src0->type, cc, warp_size, src0->ne, src1->ne[1], /*mul_mat_id=*/false);\n-        use_mul_mat_vec_f       = use_mul_mat_vec_f         && ggml_cuda_should_use_mmvf(src0->type, cc, src0->ne, src1->ne[1]);\n+        use_mul_mat_f           = use_mul_mat_f             && ggml_cuda_should_use_mmf(src0->type, cc, warp_size, src0->ne, src0->nb, src1->ne[1], /*mul_mat_id=*/false);\n+        use_mul_mat_vec_f       = use_mul_mat_vec_f         && ggml_cuda_should_use_mmvf(src0->type, cc, src0->ne, src0->nb, src1->ne[1]);\n         any_gpus_with_slow_fp16 = any_gpus_with_slow_fp16   || !fast_fp16_hardware_available(cc);\n     }\n \n@@ -2287,7 +2287,7 @@ static void ggml_cuda_mul_mat_id(ggml_backend_cuda_context & ctx, ggml_tensor *\n             return;\n         }\n \n-        if (ggml_cuda_should_use_mmf(src0->type, cc, WARP_SIZE, src0->ne, src1->ne[2], /*mul_mat_id=*/true)) {\n+        if (ggml_cuda_should_use_mmf(src0->type, cc, WARP_SIZE, src0->ne, src0->nb, src1->ne[2], /*mul_mat_id=*/true)) {\n             ggml_cuda_mul_mat_f(ctx, src0, src1, ids, dst);\n             return;\n         }"
      },
      {
        "filename": "ggml/src/ggml-cuda/mmf.cu",
        "status": "modified",
        "additions": 9,
        "deletions": 3,
        "changes": 12,
        "patch": "@@ -119,15 +119,21 @@ void ggml_cuda_mul_mat_f(ggml_backend_cuda_context & ctx, const ggml_tensor * sr\n     }\n }\n \n-bool ggml_cuda_should_use_mmf(enum ggml_type type, int cc, int warp_size, const int64_t * src0_ne, const int src1_ncols, bool mul_mat_id) {\n-\n+bool ggml_cuda_should_use_mmf(enum ggml_type type, int cc, int warp_size, const int64_t * src0_ne,\n+        const size_t * src0_nb, const int src1_ncols, bool mul_mat_id) {\n     if (ggml_is_quantized(type)) {\n         return false;\n     }\n \n-    if (src0_ne[0] % (warp_size * (4/ggml_type_size(type))) != 0) {\n+    const size_t ts = ggml_type_size(type);\n+    if (src0_ne[0] % (warp_size * (4/ts)) != 0) {\n         return false;\n     }\n+    for (size_t i = 0; i < GGML_MAX_DIMS; ++i) {\n+        if (src0_nb[i] % (2*ts) != 0) {\n+            return false;\n+        }\n+    }\n     if (src0_ne[1] % MMF_ROWS_PER_BLOCK != 0) {\n         return false;\n     }"
      },
      {
        "filename": "ggml/src/ggml-cuda/mmf.cuh",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -17,7 +17,7 @@ struct mmf_ids_data {\n \n void ggml_cuda_mul_mat_f(ggml_backend_cuda_context & ctx, const ggml_tensor * src0, const ggml_tensor * src1, const ggml_tensor * ids, ggml_tensor * dst);\n \n-bool ggml_cuda_should_use_mmf(enum ggml_type type, int cc, int warp_size, const int64_t * scr0_ne, const int src1_ncols, bool mul_mat_id);\n+bool ggml_cuda_should_use_mmf(enum ggml_type type, int cc, int warp_size, const int64_t * scr0_ne, const size_t * src0_nb, const int src1_ncols, bool mul_mat_id);\n \n template <typename T, int rows_per_block, int cols_per_block, int nwarps, bool has_ids>\n __launch_bounds__(ggml_cuda_get_physical_warp_size()*nwarps, 1)"
      },
      {
        "filename": "ggml/src/ggml-cuda/mmvf.cu",
        "status": "modified",
        "additions": 7,
        "deletions": 1,
        "changes": 8,
        "patch": "@@ -716,10 +716,16 @@ void ggml_cuda_op_mul_mat_vec_f(\n     GGML_UNUSED_VARS(ctx, src1, dst, src1_ddq_i, src1_ncols, src1_padded_row_size);\n }\n \n-bool ggml_cuda_should_use_mmvf(enum ggml_type type, int cc, const int64_t * src0_ne, int64_t ne11) {\n+bool ggml_cuda_should_use_mmvf(enum ggml_type type, int cc, const int64_t * src0_ne, const size_t * src0_nb, int64_t ne11) {\n     if (src0_ne[0] % 2 != 0) {\n         return false;\n     }\n+    const size_t ts = ggml_type_size(type);\n+    for (size_t i = 0; i < GGML_MAX_DIMS; ++i) {\n+        if (src0_nb[i] % (2*ts) != 0) {\n+            return false;\n+        }\n+    }\n     switch (type) {\n         case GGML_TYPE_F32:\n             if (GGML_CUDA_CC_IS_NVIDIA(cc)) {"
      },
      {
        "filename": "ggml/src/ggml-cuda/mmvf.cuh",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -9,4 +9,4 @@ void ggml_cuda_op_mul_mat_vec_f(\n     const char * src1_ddq_i, float * dst_dd_i, const int64_t row_low, const int64_t row_high, const int64_t src1_ncols,\n     const int64_t src1_padded_row_size, cudaStream_t stream);\n \n-bool ggml_cuda_should_use_mmvf(enum ggml_type type, int cc, const int64_t * src0_ne, int64_t ne11);\n+bool ggml_cuda_should_use_mmvf(enum ggml_type type, int cc, const int64_t * src0_ne, const size_t * src0_nb, int64_t ne11);"
      },
      {
        "filename": "src/llama-context.cpp",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -21,6 +21,8 @@ llama_context::llama_context(\n               llama_context_params params) :\n     model(model),\n     balloc(std::make_unique<llama_batch_allocr>(model.hparams.n_pos_per_embd())) {\n+    // TODO warning when creating llama_context with awkward ctx size that is not a power of 2,\n+    //     may need to be backend-dependent\n     LLAMA_LOG_INFO(\"%s: constructing llama_context\\n\", __func__);\n \n     t_start_us = model.t_start_us;"
      },
      {
        "filename": "tests/test-backend-ops.cpp",
        "status": "modified",
        "additions": 18,
        "deletions": 26,
        "changes": 44,
        "patch": "@@ -3377,11 +3377,11 @@ struct test_mul_mat : public test_case {\n     const std::array<int64_t, 2> bs;  // dims 3 and 4\n     const std::array<int64_t, 2> nr;  // repeat in dims 3 and 4\n     const std::array<int64_t, 4> per; // permutation of dimensions\n-    const bool v; // whether a and b are non-contiguous views\n+    const int64_t k_v; // size of k in memory, resulting in a non-contiguous view for k_v > k, no view for k_v == 0\n     const uint32_t o; // number of outputs\n \n     std::string vars() override {\n-        return VARS_TO_STR10(type_a, type_b, m, n, k, bs, nr, per, v, o);\n+        return VARS_TO_STR10(type_a, type_b, m, n, k, bs, nr, per, k_v, o);\n     }\n \n     double max_nmse_err() override {\n@@ -3402,8 +3402,8 @@ struct test_mul_mat : public test_case {\n             std::array<int64_t, 2> bs = {10, 10},\n             std::array<int64_t, 2> nr = {2, 2},\n             std::array<int64_t, 4> per = {0, 1, 2, 3},\n-            bool v = false, uint32_t o = 1)\n-        : type_a(type_a), type_b(type_b), m(m), n(n), k(k), bs(bs), nr(nr), per(per), v(v), o(o) {}\n+            int64_t k_v = 0, uint32_t o = 1)\n+        : type_a(type_a), type_b(type_b), m(m), n(n), k(k), bs(bs), nr(nr), per(per), k_v(k_v), o(o) {}\n \n     ggml_tensor * build_graph(ggml_context * ctx) override {\n         // C^T = A * B^T: (k, m) * (k, n) => (m, n)\n@@ -3413,7 +3413,7 @@ struct test_mul_mat : public test_case {\n         const int npermuted = (per[0] != 0) + (per[1] != 1) + (per[2] != 2) + (per[3] != 3);\n         if (npermuted > 0) {\n             GGML_ASSERT(npermuted == 2);\n-            GGML_ASSERT(!v); // not handled\n+            GGML_ASSERT(k_v == 0); // not handled\n             GGML_ASSERT(!ggml_is_quantized(type_a) || per[0] == 0);\n             GGML_ASSERT(!ggml_is_quantized(type_b) || per[0] == 0);\n \n@@ -3437,29 +3437,21 @@ struct test_mul_mat : public test_case {\n             ggml_set_name(a, \"a_permuted\");\n             ggml_set_name(b, \"b_permuted\");\n         } else {\n-            if (v) {\n-                a = ggml_new_tensor_4d(ctx, type_a, k*2, m, bs[0],       bs[1]);\n-                b = ggml_new_tensor_4d(ctx, type_b, k*2, n, bs[0]*nr[0], bs[1]*nr[1]);\n+            const int64_t k_physical = k_v == 0 ? k : k_v;\n+            a = ggml_new_tensor_4d(ctx, type_a, k_physical, m, bs[0],       bs[1]);\n+            b = ggml_new_tensor_4d(ctx, type_b, k_physical, n, bs[0]*nr[0], bs[1]*nr[1]);\n \n-                if (!ggml_is_quantized(type_a)) {\n-                    if (bs[1] == 1 && nr[1] == 1) {\n-                        ggml_set_param(a);\n-                    }\n-                    ggml_set_param(b);\n+            if (!ggml_is_quantized(type_a)) {\n+                if (bs[1] == 1 && nr[1] == 1) {\n+                    ggml_set_param(a);\n                 }\n+                ggml_set_param(b);\n+            }\n \n+            if (k_v != 0) {\n+                GGML_ASSERT(k_v > k);\n                 a = ggml_view_4d(ctx, a, k, m, bs[0],       bs[1],       a->nb[1], a->nb[2], a->nb[3], 0);\n                 b = ggml_view_4d(ctx, b, k, n, bs[0]*nr[0], bs[1]*nr[1], b->nb[1], b->nb[2], b->nb[3], 0);\n-            } else {\n-                a = ggml_new_tensor_4d(ctx, type_a, k, m, bs[0],       bs[1]);\n-                b = ggml_new_tensor_4d(ctx, type_b, k, n, bs[0]*nr[0], bs[1]*nr[1]);\n-\n-                if (!ggml_is_quantized(type_a)) {\n-                    if (bs[1] == 1 && nr[1] == 1) {\n-                        ggml_set_param(a);\n-                    }\n-                    ggml_set_param(b);\n-                }\n             }\n             ggml_set_name(a, \"a\");\n             ggml_set_name(b, \"b\");\n@@ -6886,7 +6878,7 @@ static std::vector<std::unique_ptr<test_case>> make_test_cases_eval() {\n     test_cases.emplace_back(new test_mul_mat(GGML_TYPE_F16, GGML_TYPE_F32, 128, 45,  64, { 8,  1}, {4, 1}));\n     test_cases.emplace_back(new test_mul_mat(GGML_TYPE_F16, GGML_TYPE_F32, 1056, 1, 193, {1,  1}, {4, 1}, {0, 2, 1, 3}));\n     test_cases.emplace_back(new test_mul_mat(GGML_TYPE_F16, GGML_TYPE_F32, 1056, 1, 67,  {1,  1}, {4, 1}, {0, 2, 1, 3}));\n-    test_cases.emplace_back(new test_mul_mat(GGML_TYPE_F32, GGML_TYPE_F32, 16, 32, 32, { 1,  1}, {1, 1}, {0, 1, 2, 3}, true, 3));\n+    test_cases.emplace_back(new test_mul_mat(GGML_TYPE_F32, GGML_TYPE_F32, 16, 32, 32, { 1,  1}, {1, 1}, {0, 1, 2, 3}, 64, 3));\n     test_cases.emplace_back(new test_mul_mat(GGML_TYPE_F32, GGML_TYPE_F32, 64, 77, 77, {12,1}, {1,1}));\n \n #if 0\n@@ -6912,7 +6904,7 @@ static std::vector<std::unique_ptr<test_case>> make_test_cases_eval() {\n                     for (uint32_t k = 0; k < 2; ++k) {\n                         for (ggml_type type: {GGML_TYPE_F16, GGML_TYPE_BF16, GGML_TYPE_F32}) {\n                             test_cases.emplace_back(new test_mul_mat(type, GGML_TYPE_F32, 1056 + m, 1, 128 + k,  {bs,  bs2}, {nr, 1}, {0, 2, 1, 3}));\n-                            test_cases.emplace_back(new test_mul_mat(type, GGML_TYPE_F32, 128 + m,  1, 1056 + k, {bs,  bs2}, {nr, 1}, {0, 1, 2, 3}, true));\n+                            test_cases.emplace_back(new test_mul_mat(type, GGML_TYPE_F32, 128 + m,  1, 1056 + k, {bs,  bs2}, {nr, 1}, {0, 1, 2, 3}, 2*1056 + k));\n                         }\n                     }\n                 }\n@@ -7405,7 +7397,7 @@ static std::vector<std::unique_ptr<test_case>> make_test_cases_perf() {\n     test_cases.emplace_back(new test_pad_reflect_1d(GGML_TYPE_F32, {3000, 384, 4, 1}));\n \n     test_cases.emplace_back(new test_mul_mat(GGML_TYPE_F16, GGML_TYPE_F32, 16416, 1, 128, {8,  1}, {4, 1}, {0, 2, 1, 3}));\n-    test_cases.emplace_back(new test_mul_mat(GGML_TYPE_F16, GGML_TYPE_F32, 128, 1, 16416, {8,  1}, {4, 1}, {0, 1, 2, 3}, true));\n+    test_cases.emplace_back(new test_mul_mat(GGML_TYPE_F16, GGML_TYPE_F32, 128, 1, 16416, {8,  1}, {4, 1}, {0, 1, 2, 3}, 2*16416));\n \n     for (int bs : {1, 2, 3, 4, 5, 8, 512}) {\n         for (ggml_type type_a : all_types) {"
      }
    ],
    "num_files": 7,
    "scraped_at": "2025-11-16T23:28:59.356006"
  },
  {
    "pr_number": 16977,
    "title": "vulkan: fuse rms_norm + mul + rope (+ view + set_rows)",
    "body": "This change combines the rms_norm+mul and rope+view+set_rows fusions to allow fusing the whole sequence together. This comes up in Qwen3, Bailing, and some other models.\r\n\r\nHelps a couple percent on models where it applies.\r\n```\r\nbefore:\r\n\r\nZ:\\github\\jeffbolznv\\llama.cpp\\build\\bin\\RelWithDebInfo>llama-bench.exe -fa 1 -n 128,128,128,128,128 -p 0 -r 20 --prio 1 -m c:\\models\\Qwen_Qwen3-30B-A3B-Q4_K_M.gguf\r\nggml_vulkan: Found 1 Vulkan devices:\r\nggml_vulkan: 0 = NVIDIA GeForce RTX 5090 (NVIDIA) | uma: 0 | fp16: 1 | bf16: 1 | warp size: 32 | shared memory: 49152 | int dot: 1 | matrix cores: NV_coopmat2\r\n| model                          |       size |     params | backend    | ngl | fa |            test |                  t/s |\r\n| ------------------------------ | ---------: | ---------: | ---------- | --: | -: | --------------: | -------------------: |\r\n| qwen3moe 30B.A3B Q4_K - Medium |  17.35 GiB |    30.53 B | Vulkan     |  99 |  1 |           tg128 |       269.45 \u00b1 11.28 |\r\n| qwen3moe 30B.A3B Q4_K - Medium |  17.35 GiB |    30.53 B | Vulkan     |  99 |  1 |           tg128 |        271.92 \u00b1 3.88 |\r\n| qwen3moe 30B.A3B Q4_K - Medium |  17.35 GiB |    30.53 B | Vulkan     |  99 |  1 |           tg128 |        273.37 \u00b1 1.46 |\r\n| qwen3moe 30B.A3B Q4_K - Medium |  17.35 GiB |    30.53 B | Vulkan     |  99 |  1 |           tg128 |        274.13 \u00b1 1.33 |\r\n| qwen3moe 30B.A3B Q4_K - Medium |  17.35 GiB |    30.53 B | Vulkan     |  99 |  1 |           tg128 |        274.37 \u00b1 1.21 |\r\n\r\nbuild: 1ae74882f (6913)\r\n\r\nZ:\\github\\jeffbolznv\\llama.cpp\\build\\bin\\RelWithDebInfo>llama-bench.exe -fa 1 -n 128,128,128,128,128 -p 0 -r 20 --prio 1 -m c:\\models\\Qwen3-14B-Q4_K_M.gguf\r\nggml_vulkan: Found 1 Vulkan devices:\r\nggml_vulkan: 0 = NVIDIA GeForce RTX 5090 (NVIDIA) | uma: 0 | fp16: 1 | bf16: 1 | warp size: 32 | shared memory: 49152 | int dot: 1 | matrix cores: NV_coopmat2\r\n| model                          |       size |     params | backend    | ngl | fa |            test |                  t/s |\r\n| ------------------------------ | ---------: | ---------: | ---------- | --: | -: | --------------: | -------------------: |\r\n| qwen3 14B Q4_K - Medium        |   8.38 GiB |    14.77 B | Vulkan     |  99 |  1 |           tg128 |        134.55 \u00b1 3.63 |\r\n| qwen3 14B Q4_K - Medium        |   8.38 GiB |    14.77 B | Vulkan     |  99 |  1 |           tg128 |        133.55 \u00b1 0.32 |\r\n| qwen3 14B Q4_K - Medium        |   8.38 GiB |    14.77 B | Vulkan     |  99 |  1 |           tg128 |        133.51 \u00b1 0.51 |\r\n| qwen3 14B Q4_K - Medium        |   8.38 GiB |    14.77 B | Vulkan     |  99 |  1 |           tg128 |        133.49 \u00b1 0.43 |\r\n| qwen3 14B Q4_K - Medium        |   8.38 GiB |    14.77 B | Vulkan     |  99 |  1 |           tg128 |        133.57 \u00b1 0.33 |\r\n\r\nbuild: 1ae74882f (6913)\r\n\r\nZ:\\github\\jeffbolznv\\llama.cpp\\build\\bin\\RelWithDebInfo>llama-bench.exe -fa 1 -n 128,128,128,128,128 -p 0 -r 20 --prio 1 -m c:\\models\\Ring-mini-2.0-Q4_K_M.gguf\r\nggml_vulkan: Found 1 Vulkan devices:\r\nggml_vulkan: 0 = NVIDIA GeForce RTX 5090 (NVIDIA) | uma: 0 | fp16: 1 | bf16: 1 | warp size: 32 | shared memory: 49152 | int dot: 1 | matrix cores: NV_coopmat2\r\n| model                          |       size |     params | backend    | ngl | fa |            test |                  t/s |\r\n| ------------------------------ | ---------: | ---------: | ---------- | --: | -: | --------------: | -------------------: |\r\n| bailingmoe2 16B.A1B Q4_K - Medium |   9.22 GiB |    16.26 B | Vulkan     |  99 |  1 |           tg128 |       472.71 \u00b1 38.19 |\r\n| bailingmoe2 16B.A1B Q4_K - Medium |   9.22 GiB |    16.26 B | Vulkan     |  99 |  1 |           tg128 |       478.29 \u00b1 10.31 |\r\n| bailingmoe2 16B.A1B Q4_K - Medium |   9.22 GiB |    16.26 B | Vulkan     |  99 |  1 |           tg128 |       468.20 \u00b1 16.25 |\r\n| bailingmoe2 16B.A1B Q4_K - Medium |   9.22 GiB |    16.26 B | Vulkan     |  99 |  1 |           tg128 |        483.64 \u00b1 2.21 |\r\n| bailingmoe2 16B.A1B Q4_K - Medium |   9.22 GiB |    16.26 B | Vulkan     |  99 |  1 |           tg128 |        484.80 \u00b1 2.20 |\r\n\r\nbuild: 1ae74882f (6913)\r\n\r\nafter:\r\n\r\nZ:\\github\\jeffbolznv\\llama.cpp\\build\\bin\\RelWithDebInfo>llama-bench.exe -fa 1 -n 128,128,128,128,128 -p 0 -r 20 --prio 1 -m c:\\models\\Qwen_Qwen3-30B-A3B-Q4_K_M.gguf\r\nggml_vulkan: Found 1 Vulkan devices:\r\nggml_vulkan: 0 = NVIDIA GeForce RTX 5090 (NVIDIA) | uma: 0 | fp16: 1 | bf16: 1 | warp size: 32 | shared memory: 49152 | int dot: 1 | matrix cores: NV_coopmat2\r\n| model                          |       size |     params | backend    | ngl | fa |            test |                  t/s |\r\n| ------------------------------ | ---------: | ---------: | ---------- | --: | -: | --------------: | -------------------: |\r\n| qwen3moe 30B.A3B Q4_K - Medium |  17.35 GiB |    30.53 B | Vulkan     |  99 |  1 |           tg128 |       269.94 \u00b1 17.36 |\r\n| qwen3moe 30B.A3B Q4_K - Medium |  17.35 GiB |    30.53 B | Vulkan     |  99 |  1 |           tg128 |        275.08 \u00b1 2.82 |\r\n| qwen3moe 30B.A3B Q4_K - Medium |  17.35 GiB |    30.53 B | Vulkan     |  99 |  1 |           tg128 |        276.42 \u00b1 1.60 |\r\n| qwen3moe 30B.A3B Q4_K - Medium |  17.35 GiB |    30.53 B | Vulkan     |  99 |  1 |           tg128 |        276.48 \u00b1 1.57 |\r\n| qwen3moe 30B.A3B Q4_K - Medium |  17.35 GiB |    30.53 B | Vulkan     |  99 |  1 |           tg128 |        277.64 \u00b1 0.90 |\r\n\r\nbuild: b74de9b7b (6915)\r\n\r\nZ:\\github\\jeffbolznv\\llama.cpp\\build\\bin\\RelWithDebInfo>llama-bench.exe -fa 1 -n 128,128,128,128,128 -p 0 -r 20 --prio 1 -m c:\\models\\Qwen3-14B-Q4_K_M.gguf\r\nggml_vulkan: Found 1 Vulkan devices:\r\nggml_vulkan: 0 = NVIDIA GeForce RTX 5090 (NVIDIA) | uma: 0 | fp16: 1 | bf16: 1 | warp size: 32 | shared memory: 49152 | int dot: 1 | matrix cores: NV_coopmat2\r\n| model                          |       size |     params | backend    | ngl | fa |            test |                  t/s |\r\n| ------------------------------ | ---------: | ---------: | ---------- | --: | -: | --------------: | -------------------: |\r\n| qwen3 14B Q4_K - Medium        |   8.38 GiB |    14.77 B | Vulkan     |  99 |  1 |           tg128 |        137.12 \u00b1 3.41 |\r\n| qwen3 14B Q4_K - Medium        |   8.38 GiB |    14.77 B | Vulkan     |  99 |  1 |           tg128 |        137.25 \u00b1 3.01 |\r\n| qwen3 14B Q4_K - Medium        |   8.38 GiB |    14.77 B | Vulkan     |  99 |  1 |           tg128 |        138.57 \u00b1 0.47 |\r\n| qwen3 14B Q4_K - Medium        |   8.38 GiB |    14.77 B | Vulkan     |  99 |  1 |           tg128 |        138.33 \u00b1 0.56 |\r\n| qwen3 14B Q4_K - Medium        |   8.38 GiB |    14.77 B | Vulkan     |  99 |  1 |           tg128 |        138.56 \u00b1 0.53 |\r\n\r\nbuild: b74de9b7b (6915)\r\n\r\nZ:\\github\\jeffbolznv\\llama.cpp\\build\\bin\\RelWithDebInfo>llama-bench.exe -fa 1 -n 128,128,128,128,128 -p 0 -r 20 --prio 1 -m c:\\models\\Ring-mini-2.0-Q4_K_M.gguf\r\nggml_vulkan: Found 1 Vulkan devices:\r\nggml_vulkan: 0 = NVIDIA GeForce RTX 5090 (NVIDIA) | uma: 0 | fp16: 1 | bf16: 1 | warp size: 32 | shared memory: 49152 | int dot: 1 | matrix cores: NV_coopmat2\r\n| model                          |       size |     params | backend    | ngl | fa |            test |                  t/s |\r\n| ------------------------------ | ---------: | ---------: | ---------- | --: | -: | --------------: | -------------------: |\r\n| bailingmoe2 16B.A1B Q4_K - Medium |   9.22 GiB |    16.26 B | Vulkan     |  99 |  1 |           tg128 |       482.35 \u00b1 47.67 |\r\n| bailingmoe2 16B.A1B Q4_K - Medium |   9.22 GiB |    16.26 B | Vulkan     |  99 |  1 |           tg128 |       488.51 \u00b1 11.51 |\r\n| bailingmoe2 16B.A1B Q4_K - Medium |   9.22 GiB |    16.26 B | Vulkan     |  99 |  1 |           tg128 |        487.90 \u00b1 6.93 |\r\n| bailingmoe2 16B.A1B Q4_K - Medium |   9.22 GiB |    16.26 B | Vulkan     |  99 |  1 |           tg128 |        494.89 \u00b1 4.11 |\r\n| bailingmoe2 16B.A1B Q4_K - Medium |   9.22 GiB |    16.26 B | Vulkan     |  99 |  1 |           tg128 |        496.09 \u00b1 5.04 |\r\n\r\nbuild: b74de9b7b (6915)\r\n```",
    "html_url": "https://github.com/ggml-org/llama.cpp/pull/16977",
    "created_at": "2025-11-03T19:05:01Z",
    "merged_at": "2025-11-08T07:52:16Z",
    "merge_commit_sha": "b4e335d8dc503ec0adf76fa4053ab7094b6310dd",
    "base_ref": "master",
    "head_sha": "16b730114af401b8646d206e85e26be12e1a5c4a",
    "user": "jeffbolznv",
    "files": [
      {
        "filename": "ggml/src/ggml-vulkan/ggml-vulkan.cpp",
        "status": "modified",
        "additions": 258,
        "deletions": 32,
        "changes": 290,
        "patch": "@@ -466,6 +466,14 @@ static constexpr std::initializer_list<std::array<int, 3>> rope_view_set_rows_ed\n     { 2, 0, 1 }, // set_rows->src[0] == view\n };\n \n+static constexpr std::initializer_list<std::array<int, 3>> rms_norm_mul_rope_view_set_rows_edges {\n+    { 1, 0, 0 }, // mul->src[0]      == rms\n+    { 2, 0, 1 }, // rope->src[0]     == mul\n+    { 3, 0, 2 }, // view->src[0]     == rope\n+    { 4, 0, 3 }, // set_rows->src[0] == view\n+};\n+\n+\n struct vk_device_struct {\n     std::recursive_mutex mutex;\n \n@@ -617,6 +625,8 @@ struct vk_device_struct {\n     vk_pipeline pipeline_rms_norm_mul_f32;\n     vk_pipeline pipeline_rms_norm_partials_f32;\n     vk_pipeline pipeline_rms_norm_mul_partials_f32;\n+    vk_pipeline pipeline_rms_norm_mul_rope_f32_f32;\n+    vk_pipeline pipeline_rms_norm_mul_rope_f32_f16;\n     vk_pipeline pipeline_rms_norm_back_f32;\n     vk_pipeline pipeline_l2_norm_f32;\n \n@@ -1060,6 +1070,7 @@ struct vk_op_diag_mask_push_constants {\n };\n \n struct vk_op_rope_push_constants {\n+    uint32_t rope_mode;\n     uint32_t ncols;\n     uint32_t n_dims;\n     float freq_scale;\n@@ -1079,6 +1090,12 @@ struct vk_op_rope_push_constants {\n     uint32_t set_rows_stride;\n };\n \n+// For fused rms_norm+mul+rope(+view+set_rows)\n+struct vk_op_rms_norm_mul_rope_push_constants {\n+    vk_op_binary_push_constants bin;\n+    vk_op_rope_push_constants rope;\n+};\n+\n struct vk_op_soft_max_push_constants {\n     uint32_t KX;\n     uint32_t KY;\n@@ -3557,6 +3574,12 @@ static void ggml_vk_load_shaders(vk_device& device) {\n     ggml_vk_create_pipeline(device, device->pipeline_rms_norm_partials_f32, \"rms_norm_partials_f32\", rms_norm_partials_f32_len, rms_norm_partials_f32_data, \"main\", 4, sizeof(vk_op_binary_push_constants), {1, 1, 1}, {0, 0}, 1, true);\n     ggml_vk_create_pipeline(device, device->pipeline_rms_norm_mul_partials_f32, \"rms_norm_mul_partials_f32\", rms_norm_partials_f32_len, rms_norm_partials_f32_data, \"main\", 4, sizeof(vk_op_binary_push_constants), {1, 1, 1}, {0, 1}, 1, true);\n \n+    if (device->float_controls_rte_fp16 &&\n+        sizeof(vk_op_rms_norm_mul_rope_push_constants) <= device->properties.limits.maxPushConstantsSize) {\n+        ggml_vk_create_pipeline(device, device->pipeline_rms_norm_mul_rope_f32_f32, \"rms_norm_mul_rope_f32_f32\", rms_norm_mul_rope_f32_f32_len, rms_norm_mul_rope_f32_f32_data, \"main\", 7, sizeof(vk_op_rms_norm_mul_rope_push_constants), {1, 1, 1}, {0, 1}, 1, true);\n+        ggml_vk_create_pipeline(device, device->pipeline_rms_norm_mul_rope_f32_f16, \"rms_norm_mul_rope_f32_f16\", rms_norm_mul_rope_f32_f16_rte_len, rms_norm_mul_rope_f32_f16_rte_data, \"main\", 7, sizeof(vk_op_rms_norm_mul_rope_push_constants), {1, 1, 1}, {0, 1}, 1, true);\n+    }\n+\n     ggml_vk_create_pipeline(device, device->pipeline_rms_norm_back_f32, \"rms_norm_back_f32\", rms_norm_back_f32_len, rms_norm_back_f32_data, \"main\", 3, sizeof(vk_op_push_constants), {1, 1, 1}, {}, 1);\n     ggml_vk_create_pipeline(device, device->pipeline_l2_norm_f32, \"l2_norm_f32\", l2_norm_f32_len, l2_norm_f32_data, \"main\", 2, sizeof(vk_op_push_constants), {1, 1, 1}, {}, 1);\n \n@@ -9908,21 +9931,149 @@ static uint32_t ggml_vk_rms_partials_size(ggml_backend_vk_context * ctx, const g\n     return num_bytes;\n }\n \n-static void ggml_vk_rms_norm(ggml_backend_vk_context * ctx, vk_context& subctx, const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst, float * op_params) {\n+static vk_op_rope_push_constants ggml_vk_make_rope_constants(const ggml_tensor *dst, const ggml_tensor *src0, const bool has_ff, bool backprop, const uint32_t set_rows_stride) {\n+    const int n_dims        = ((const int32_t *) dst->op_params)[1];\n+    const int mode          = ((const int32_t *) dst->op_params)[2];\n+    // const int n_ctx         = ((const int32_t *) dst->op_params)[3];\n+    const int n_ctx_orig    = ((const int32_t *) dst->op_params)[4];\n+    const float freq_base   = ((const float *)   dst->op_params)[5];\n+    const float freq_scale  = ((const float *)   dst->op_params)[6];\n+    const float ext_factor  = ((const float *)   dst->op_params)[7];\n+    const float attn_factor = ((const float *)   dst->op_params)[8];\n+    const float beta_fast   = ((const float *)   dst->op_params)[9];\n+    const float beta_slow   = ((const float *)   dst->op_params)[10];\n+    int sections[4] {};\n+    if (mode & GGML_ROPE_TYPE_MROPE) {\n+        memcpy(sections, (const int32_t *) dst->op_params + 11, sizeof(int)*4);\n+    }\n+\n+    const bool is_imrope = mode == GGML_ROPE_TYPE_IMROPE;\n+\n+    float corr_dims[2];\n+    ggml_rope_yarn_corr_dims(n_dims, n_ctx_orig, freq_base, beta_fast, beta_slow, corr_dims);\n+\n+    const float theta_scale = powf(freq_base, -2.0f/n_dims);\n+\n+    uint32_t nb01 = src0->nb[1] / ggml_type_size(src0->type);\n+    uint32_t nb02 = src0->nb[2] / ggml_type_size(src0->type);\n+\n+    vk_op_rope_push_constants rope {\n+        (uint32_t)mode, (uint32_t)src0->ne[0], (uint32_t)n_dims, freq_scale, (uint32_t)src0->ne[1],\n+        freq_base, ext_factor, attn_factor, {corr_dims[0], corr_dims[1]}, theta_scale,\n+        has_ff, (uint32_t)src0->ne[2], nb01, nb02,\n+        { sections[0], sections[1], sections[2], sections[3] }, is_imrope, backprop, set_rows_stride,\n+    };\n+\n+    return rope;\n+}\n+\n+static void ggml_vk_rms_norm(ggml_backend_vk_context * ctx, vk_context& subctx, const struct ggml_cgraph * cgraph, int node_idx, float * op_params) {\n+    ggml_tensor * dst;\n+    const ggml_tensor * src0;\n+    const ggml_tensor * src1;\n+\n+    if (ctx->num_additional_fused_ops > 0) {\n+        // fused rms_norm + mul\n+        ggml_tensor *mul = cgraph->nodes[node_idx + 1];\n+        ggml_tensor *other_src = mul->src[0] == cgraph->nodes[node_idx + 0] ? mul->src[1] : mul->src[0];\n+        dst = mul;\n+        src0 = cgraph->nodes[node_idx]->src[0];\n+        src1 = other_src;\n+    } else {\n+        dst = cgraph->nodes[node_idx];\n+        src0 = src1 = dst->src[0];\n+    }\n+\n     const uint32_t src0_type_size = ggml_type_size(src0->type);\n     const uint32_t src1_type_size = ggml_type_size(src1->type);\n     const uint32_t dst_type_size = ggml_type_size(dst->type);\n \n     uint32_t param3 = ctx->do_add_rms_partials ? ggml_vk_rms_num_partials(ctx, dst) : 0;\n \n-    ggml_vk_op_f32<vk_op_binary_push_constants>(ctx, subctx, src0, src1, nullptr, nullptr, dst, GGML_OP_RMS_NORM, {\n+    vk_op_binary_push_constants bin {\n         (uint32_t)ggml_nelements(src0),\n         (uint32_t)src0->ne[0], (uint32_t)src0->ne[1], (uint32_t)src0->ne[2],(uint32_t)src0->ne[3], (uint32_t)src0->nb[0] / src0_type_size, (uint32_t)src0->nb[1] / src0_type_size, (uint32_t)src0->nb[2] / src0_type_size, (uint32_t)src0->nb[3] / src0_type_size,\n         (uint32_t)src1->ne[0], (uint32_t)src1->ne[1], (uint32_t)src1->ne[2],(uint32_t)src1->ne[3], (uint32_t)src1->nb[0] / src1_type_size, (uint32_t)src1->nb[1] / src1_type_size, (uint32_t)src1->nb[2] / src1_type_size, (uint32_t)src1->nb[3] / src1_type_size,\n         (uint32_t) dst->ne[0], (uint32_t) dst->ne[1], (uint32_t) dst->ne[2],(uint32_t) dst->ne[3], (uint32_t) dst->nb[0] /  dst_type_size, (uint32_t) dst->nb[1] /  dst_type_size, (uint32_t) dst->nb[2] /  dst_type_size, (uint32_t) dst->nb[3] /  dst_type_size,\n         0,\n         op_params[0], 0.0f, (int32_t)param3,\n-    });\n+    };\n+\n+    // more than one fused op means rms_norm+mul+rope\n+    if (ctx->num_additional_fused_ops > 1) {\n+        static constexpr uint32_t max_tensors = 7;\n+        const ggml_tensor *tensors[max_tensors] {};\n+\n+        ggml_tensor *rms = cgraph->nodes[node_idx + 0];\n+        ggml_tensor *mul = cgraph->nodes[node_idx + 1];\n+        ggml_tensor *rope = cgraph->nodes[node_idx + 2];\n+\n+        ggml_tensor *other_src = mul->src[0] == rms ? mul->src[1] : mul->src[0];\n+\n+        bool do_set_rows = ctx->num_additional_fused_ops == 4;\n+\n+        tensors[0] = rms->src[0];\n+        tensors[1] = other_src;\n+        tensors[2] = mul;\n+        tensors[3] = rope->src[1]; // pos\n+        tensors[4] = rope->src[2]; // ff\n+        tensors[5] = cgraph->nodes[node_idx + ctx->num_additional_fused_ops]; // dst\n+        tensors[6] = do_set_rows ? tensors[5]->src[1] : nullptr;\n+        const uint32_t set_rows_stride = do_set_rows ? tensors[5]->nb[1] / ggml_type_size(tensors[5]->type) : 0;\n+\n+        vk_op_rms_norm_mul_rope_push_constants pc;\n+        pc.bin = bin;\n+        pc.rope = ggml_vk_make_rope_constants(rope, rope->src[0], tensors[4] != nullptr, false, set_rows_stride);\n+\n+        vk_pipeline pipeline = tensors[5]->type == GGML_TYPE_F16 ? ctx->device->pipeline_rms_norm_mul_rope_f32_f16 : ctx->device->pipeline_rms_norm_mul_rope_f32_f32;\n+\n+        ggml_pipeline_request_descriptor_sets(ctx, pipeline, 1);\n+\n+        ggml_backend_vk_buffer_context * buf_ctx[max_tensors];\n+        vk_buffer buf[max_tensors];\n+        size_t offset[max_tensors];\n+        bool uma[max_tensors];\n+\n+        for (uint32_t i = 0; i < max_tensors; ++i) {\n+            if (!tensors[i]) {\n+                // If any remaining descriptors are unused, just point them at src[0]\n+                buf[i] = buf[0];\n+                offset[i] = 0;\n+                continue;\n+            }\n+            buf_ctx[i] = (ggml_backend_vk_buffer_context *)tensors[i]->buffer->context;\n+            buf[i] = nullptr;\n+            offset[i] = 0;\n+            uma[i] = false;\n+\n+            if (ctx->device->uma) {\n+                ggml_vk_host_get(ctx->device, tensors[i]->data, buf[i], offset[i]);\n+                uma[i] = buf[i] != nullptr;\n+            }\n+            if (!uma[i]) {\n+                buf[i] = buf_ctx[i]->dev_buffer;\n+                offset[i] = vk_tensor_offset(tensors[i]) + tensors[i]->view_offs;\n+            }\n+            GGML_ASSERT(buf[i] != nullptr);\n+        }\n+\n+        std::array<uint32_t, 3> elements;\n+        elements = { (uint32_t)rms->src[0]->ne[1], (uint32_t)rms->src[0]->ne[2], (uint32_t)rms->src[0]->ne[3] };\n+\n+        static_assert(max_tensors == 7);\n+        ggml_vk_dispatch_pipeline(ctx, subctx, pipeline,\n+            {\n+                ggml_vk_subbuffer(ctx, buf[0], offset[0]),\n+                ggml_vk_subbuffer(ctx, buf[1], offset[1]),\n+                ggml_vk_subbuffer(ctx, buf[2], offset[2]),\n+                ggml_vk_subbuffer(ctx, buf[3], offset[3]),\n+                ggml_vk_subbuffer(ctx, buf[4], offset[4]),\n+                ggml_vk_subbuffer(ctx, buf[5], offset[5]),\n+                ggml_vk_subbuffer(ctx, buf[6], offset[6]),\n+            }, pc, elements);\n+    } else {\n+        ggml_vk_op_f32<vk_op_binary_push_constants>(ctx, subctx, src0, src1, nullptr, nullptr, dst, GGML_OP_RMS_NORM, std::move(bin));\n+    }\n \n     if (ctx->do_add_rms_partials_offset_calculation) {\n         ctx->prealloc_size_add_rms_partials_offset += ggml_vk_rms_partials_size(ctx, src0);\n@@ -10117,26 +10268,16 @@ static void ggml_vk_rope(ggml_backend_vk_context * ctx, vk_context& subctx, cons\n     // const int n_ctx         = ((int32_t *) dst->op_params)[3];\n     const int n_ctx_orig    = ((int32_t *) dst->op_params)[4];\n     const float freq_base   = ((float *)   dst->op_params)[5];\n-    const float freq_scale  = ((float *)   dst->op_params)[6];\n-    const float ext_factor  = ((float *)   dst->op_params)[7];\n-    const float attn_factor = ((float *)   dst->op_params)[8];\n     const float beta_fast   = ((float *)   dst->op_params)[9];\n     const float beta_slow   = ((float *)   dst->op_params)[10];\n     int sections[4] {};\n     if (mode & GGML_ROPE_TYPE_MROPE) {\n         memcpy(sections, (int32_t *) dst->op_params + 11, sizeof(int)*4);\n     }\n \n-    const bool is_imrope = mode == GGML_ROPE_TYPE_IMROPE;\n-\n     float corr_dims[2];\n     ggml_rope_yarn_corr_dims(n_dims, n_ctx_orig, freq_base, beta_fast, beta_slow, corr_dims);\n \n-    const float theta_scale = powf(freq_base, -2.0f/n_dims);\n-\n-    uint32_t s1 = src0->nb[1] / ggml_type_size(src0->type);\n-    uint32_t s2 = src0->nb[2] / ggml_type_size(src0->type);\n-\n     uint32_t set_rows_stride = 0;\n     // Fused rope + view + set_rows passes the set_rows destination stride in set_rows_stride\n     // and overrides the dst and sets src3=row_indices\n@@ -10146,12 +10287,8 @@ static void ggml_vk_rope(ggml_backend_vk_context * ctx, vk_context& subctx, cons\n         dst = cgraph->nodes[node_idx + 2];\n     }\n \n-    ggml_vk_op_f32<vk_op_rope_push_constants>(ctx, subctx, src0, src1, src2, src3, dst, GGML_OP_ROPE, {\n-        (uint32_t)src0->ne[0], (uint32_t)n_dims, freq_scale, (uint32_t)src0->ne[1],\n-        freq_base, ext_factor, attn_factor, {corr_dims[0], corr_dims[1]}, theta_scale,\n-        src2 != nullptr, (uint32_t)src0->ne[2], s1, s2,\n-        { sections[0], sections[1], sections[2], sections[3] }, is_imrope, backprop, set_rows_stride,\n-    });\n+    ggml_vk_op_f32<vk_op_rope_push_constants>(ctx, subctx, src0, src1, src2, src3, dst, GGML_OP_ROPE,\n+        ggml_vk_make_rope_constants(cgraph->nodes[node_idx], src0, src2 != nullptr, backprop, set_rows_stride));\n }\n \n static void ggml_vk_argsort(ggml_backend_vk_context * ctx, vk_context& subctx, const ggml_tensor * src0, ggml_tensor * dst) {\n@@ -11666,6 +11803,10 @@ static bool ggml_vk_build_graph(ggml_backend_vk_context * ctx, ggml_cgraph * cgr\n         if (n->op == GGML_OP_GLU) {\n             std::cerr << \" \" << ggml_glu_op_name(ggml_get_glu_op(n)) << \" \" << (n->src[1] ? \"split\" : \"single\") << \" \";\n         }\n+        if (n->op == GGML_OP_ROPE) {\n+            const int mode = ((const int32_t *) n->op_params)[2];\n+            std::cerr << \" rope mode: \" << mode;\n+        }\n         std::cerr << std::endl;\n     }\n #endif\n@@ -11773,14 +11914,7 @@ static bool ggml_vk_build_graph(ggml_backend_vk_context * ctx, ggml_cgraph * cgr\n \n         break;\n     case GGML_OP_RMS_NORM:\n-        if (ctx->num_additional_fused_ops > 0) {\n-            // fused rms_norm + mul\n-            ggml_tensor *mul = cgraph->nodes[node_idx + 1];\n-            ggml_tensor *other_src = mul->src[0] == node ? mul->src[1] : mul->src[0];\n-            ggml_vk_rms_norm(ctx, compute_ctx, src0, other_src, mul, (float *)node->op_params);\n-        } else {\n-            ggml_vk_rms_norm(ctx, compute_ctx, src0, src0, node, (float *)node->op_params);\n-        }\n+        ggml_vk_rms_norm(ctx, compute_ctx, cgraph, node_idx, (float *)node->op_params);\n         break;\n     case GGML_OP_RMS_NORM_BACK:\n         ggml_vk_rms_norm_back(ctx, compute_ctx, src0, src1, node);\n@@ -12766,6 +12900,70 @@ static bool ggml_vk_can_fuse_rope_set_rows(ggml_backend_vk_context * ctx, const\n     return true;\n }\n \n+// Check whether the tensors overlap in memory but are not equal.\n+// Fusions can potenitally overwrite src tensors in ways that are not prevented\n+// by ggml-alloc. If the fusion is entirely elementwise, then it's OK for them\n+// to overlap if they are exactly equal.\n+// XXX TODO this check is probably missing from several fusion optimizations.\n+static bool ggml_vk_tensors_overlap_but_not_equal(const ggml_tensor * a, const ggml_tensor * b) {\n+    ggml_backend_vk_buffer_context * a_buf_ctx = (ggml_backend_vk_buffer_context *)a->buffer->context;\n+    vk_buffer a_buf = a_buf_ctx->dev_buffer;\n+    ggml_backend_vk_buffer_context * b_buf_ctx = (ggml_backend_vk_buffer_context *)b->buffer->context;\n+    vk_buffer b_buf = b_buf_ctx->dev_buffer;\n+    if (a_buf == b_buf) {\n+        auto a_base = vk_tensor_offset(a) + a->view_offs;\n+        auto a_size = ggml_nbytes(a);\n+        auto b_base = vk_tensor_offset(b) + b->view_offs;\n+        auto b_size = ggml_nbytes(b);\n+\n+        if (a_base == b_base && a_size == b_size) {\n+            return false;\n+        }\n+\n+        if ((b_base <= a_base && a_base < b_base + b_size) ||\n+            (a_base <= b_base && b_base < a_base + a_size)) {\n+            return true;\n+        }\n+    }\n+    return false;\n+}\n+\n+static bool ggml_vk_can_fuse_rms_norm_mul_rope(ggml_backend_vk_context * ctx, const struct ggml_cgraph * cgraph,\n+                                               int node_idx) {\n+    GGML_UNUSED(ctx);\n+    const ggml_tensor *rms = cgraph->nodes[node_idx + 0];\n+    const ggml_tensor *mul = cgraph->nodes[node_idx + 1];\n+    const ggml_tensor *rope = cgraph->nodes[node_idx + 2];\n+\n+    const int mode = ((const int32_t *) rope->op_params)[2];\n+\n+    // noncontig tensors aren't tested, and don't seem common in practice\n+    if (!ggml_is_contiguous(rms) ||\n+        !ggml_is_contiguous(mul) ||\n+        !ggml_is_contiguous(rope)) {\n+        return false;\n+    }\n+\n+    // only norm/neox are handled in the shader\n+    if (mode != GGML_ROPE_TYPE_NEOX && mode != GGML_ROPE_TYPE_NORMAL) {\n+        return false;\n+    }\n+\n+    // shared memory size for passing data from mul->rope\n+    if (mul->ne[0] > 1024) {\n+        return false;\n+    }\n+\n+    // must not overwrite srcs in a way that's not elementwise\n+    ggml_tensor *other_src = mul->src[0] == rms ? mul->src[1] : mul->src[0];\n+    if (ggml_vk_tensors_overlap_but_not_equal(rms->src[0], rope) ||\n+        ggml_vk_tensors_overlap_but_not_equal(other_src, rope)) {\n+        return false;\n+    }\n+\n+    return true;\n+}\n+\n static uint32_t ggml_vk_fuse_multi_add(ggml_backend_vk_context * ctx, const struct ggml_cgraph * cgraph, int node_idx) {\n \n     const ggml_tensor *first_node = cgraph->nodes[node_idx];\n@@ -12911,12 +13109,20 @@ static ggml_status ggml_backend_vk_graph_compute(ggml_backend_t backend, ggml_cg\n             uint32_t num_adds = ggml_vk_fuse_multi_add(ctx, cgraph, i);\n             if (num_adds) {\n                 ctx->num_additional_fused_ops = num_adds - 1;\n-            } else if (ggml_vk_can_fuse(ctx, cgraph, i, { GGML_OP_RMS_NORM, GGML_OP_MUL })) {\n-                ctx->num_additional_fused_ops = 1;\n             } else if (ggml_vk_can_fuse(ctx, cgraph, i, { GGML_OP_MUL_MAT, GGML_OP_ADD })) {\n                 ctx->num_additional_fused_ops = 1;\n             } else if (ggml_vk_can_fuse(ctx, cgraph, i, { GGML_OP_MUL_MAT_ID, GGML_OP_ADD_ID })) {\n                 ctx->num_additional_fused_ops = 1;\n+            } else if (ggml_can_fuse_subgraph(cgraph, i, { GGML_OP_RMS_NORM, GGML_OP_MUL, GGML_OP_ROPE, GGML_OP_VIEW, GGML_OP_SET_ROWS }, { i + 4 }) &&\n+                       ggml_check_edges(cgraph, i, rms_norm_mul_rope_view_set_rows_edges) &&\n+                       ggml_vk_can_fuse_rms_norm_mul_rope(ctx, cgraph, i) &&\n+                       ggml_vk_can_fuse_rope_set_rows(ctx, cgraph, i + 2)) {\n+                ctx->num_additional_fused_ops = 4;\n+            } else if (ggml_vk_can_fuse(ctx, cgraph, i, { GGML_OP_RMS_NORM, GGML_OP_MUL, GGML_OP_ROPE })&&\n+                       ggml_vk_can_fuse_rms_norm_mul_rope(ctx, cgraph, i)) {\n+                ctx->num_additional_fused_ops = 2;\n+            } else if (ggml_vk_can_fuse(ctx, cgraph, i, { GGML_OP_RMS_NORM, GGML_OP_MUL })) {\n+                ctx->num_additional_fused_ops = 1;\n             } else if (ggml_can_fuse_subgraph(cgraph, i, { GGML_OP_ROPE, GGML_OP_VIEW, GGML_OP_SET_ROWS }, { i + 2 }) &&\n                        ggml_check_edges(cgraph, i, rope_view_set_rows_edges) &&\n                        ggml_vk_can_fuse_rope_set_rows(ctx, cgraph, i)) {\n@@ -13149,14 +13355,34 @@ static void ggml_vk_graph_optimize(ggml_backend_t backend, struct ggml_cgraph *\n             }\n             if (ok) {\n                 current_set.push_back(j);\n+\n+                int rope_idx = j;\n+\n+                // When we've found RMS_NORM + MUL, try to find a ROPE that uses it\n+                if (j > 0 &&\n+                    graph->nodes[j]->op == GGML_OP_MUL &&\n+                    graph->nodes[j-1]->op == GGML_OP_RMS_NORM) {\n+                    for (int k = j + 1; k < std::min(j + 15, graph->n_nodes); ++k) {\n+                        if (graph->nodes[k]->op == GGML_OP_ROPE &&\n+                            graph->nodes[k]->src[0] == graph->nodes[j] &&\n+                            // Check that other srcs are already valid\n+                            graph->nodes[k]->src[1]->op == GGML_OP_NONE &&\n+                            (graph->nodes[k]->src[2] == nullptr || graph->nodes[k]->src[2]->op == GGML_OP_NONE)) {\n+                            rope_idx = k;\n+                            current_set.push_back(rope_idx);\n+                            used[rope_idx] = true;\n+                            break;\n+                        }\n+                    }\n+                }\n                 // Look for ROPE + VIEW + SET_ROWS and make them consecutive\n-                if (graph->nodes[j]->op == GGML_OP_ROPE) {\n+                if (graph->nodes[rope_idx]->op == GGML_OP_ROPE) {\n                     int view_idx = -1;\n                     int set_rows_idx = -1;\n-                    for (int k = j+1; k < std::min(j + 10, graph->n_nodes); ++k) {\n+                    for (int k = rope_idx+1; k < std::min(rope_idx + 10, graph->n_nodes); ++k) {\n                         if (view_idx == -1 &&\n                             graph->nodes[k]->op == GGML_OP_VIEW &&\n-                            graph->nodes[k]->src[0] == graph->nodes[j]) {\n+                            graph->nodes[k]->src[0] == graph->nodes[rope_idx]) {\n                             view_idx = k;\n                             continue;\n                         }"
      },
      {
        "filename": "ggml/src/ggml-vulkan/vulkan-shaders/generic_binary_head.glsl",
        "status": "modified",
        "additions": 8,
        "deletions": 0,
        "changes": 8,
        "patch": "@@ -3,6 +3,9 @@\n \n #include \"rte.glsl\"\n #include \"utils.glsl\"\n+#if RMS_NORM_ROPE_FUSION\n+#include \"rope_params.glsl\"\n+#endif\n \n layout (push_constant) uniform parameter\n {\n@@ -12,11 +15,16 @@ layout (push_constant) uniform parameter\n     uint ne20; uint ne21; uint ne22; uint ne23; uint nb20; uint nb21; uint nb22; uint nb23;\n     uint misalign_offsets;\n     float param1; float param2; int param3;\n+#if RMS_NORM_ROPE_FUSION\n+    rope_params rope;\n+#endif\n } p;\n \n+#if !RMS_NORM_ROPE_FUSION\n layout (binding = 0) readonly buffer A {A_TYPE data_a[];};\n layout (binding = 1) readonly buffer B {B_TYPE data_b[];};\n layout (binding = 2) writeonly buffer D {D_TYPE data_d[];};\n+#endif\n \n // true if src0/src1 are the same shape and the indices can be reused without additional modulus\n layout(constant_id = 0) const bool norepeat = false;"
      },
      {
        "filename": "ggml/src/ggml-vulkan/vulkan-shaders/rms_norm.comp",
        "status": "modified",
        "additions": 43,
        "deletions": 1,
        "changes": 44,
        "patch": "@@ -3,6 +3,32 @@\n #include \"generic_binary_head.glsl\"\n #include \"types.glsl\"\n \n+#if RMS_NORM_ROPE_FUSION\n+\n+layout (binding = 0) readonly buffer A {A_TYPE data_a[];};\n+layout (binding = 1) readonly buffer B {B_TYPE data_b[];};\n+\n+// data is passed from rms_norm -> rope through shared memory.\n+// rms_norm calls this data_d, rope calls this rope_data_a.\n+// Binding 2 is not used\n+shared FLOAT_TYPE rope_data_a[1024];\n+#define data_d rope_data_a\n+\n+layout (binding = 3) readonly buffer R_Y {int rope_data_pos[];};\n+layout (binding = 4) readonly buffer R_Z {float rope_data_ff[];};\n+layout (binding = 5) writeonly buffer R_D {ROPE_D_TYPE rope_data_d[];};\n+layout (binding = 6) readonly buffer R_I {uvec2 rope_data_i[];}; // indices for set_rows\n+\n+#include \"rope_params.glsl\"\n+#include \"rope_funcs.glsl\"\n+\n+#define GGML_ROPE_TYPE_NORMAL 0\n+#define GGML_ROPE_TYPE_NEOX   2\n+#define GGML_ROPE_TYPE_MROPE  8\n+#define GGML_ROPE_TYPE_VISION 24\n+\n+#endif\n+\n #extension GL_EXT_control_flow_attributes : enable\n #define BLOCK_SIZE 512\n \n@@ -28,8 +54,12 @@ void rms_norm(uint num_iters) {\n \n     uint32_t a_offset = samp*stride_sample + channel*stride_channel + row*stride_row + get_aoffset();\n     uint32_t b_offset = src1_idx(0, row, channel, samp) + get_boffset();\n+#if RMS_NORM_ROPE_FUSION\n+    // Per-row offset in shared memory\n+    uint32_t d_offset = 0;\n+#else\n     uint32_t d_offset = ((samp*nchannels + channel)*nrows + row)*ncols + get_doffset();\n-\n+#endif\n     FLOAT_TYPE sum = FLOAT_TYPE(0.0f); // partial sum for thread in warp\n \n     [[unroll]] for (uint col = tid, idx = 0; idx < num_iters; col += BLOCK_SIZE, ++idx) {\n@@ -79,6 +109,18 @@ void rms_norm(uint num_iters) {\n             data_d[d_offset + col] = D_TYPE(scale * FLOAT_TYPE(data_a[a_offset + col]));\n         }\n     }\n+#if RMS_NORM_ROPE_FUSION\n+    barrier();\n+    rope_params rp = p.rope;\n+    uint rope_row = (samp*nchannels + channel)*nrows + row;\n+    for (uint t = 2*tid; t < ncols; t += 2*BLOCK_SIZE) {\n+        if (rp.rope_mode == GGML_ROPE_TYPE_NEOX) {\n+            rope_neox(t, rope_row, rp);\n+        } else if (rp.rope_mode == GGML_ROPE_TYPE_NORMAL) {\n+            rope_norm(t, rope_row, rp);\n+        }\n+    }\n+#endif\n }\n \n void main() {"
      },
      {
        "filename": "ggml/src/ggml-vulkan/vulkan-shaders/rope_funcs.glsl",
        "status": "added",
        "additions": 227,
        "deletions": 0,
        "changes": 227,
        "patch": "@@ -0,0 +1,227 @@\n+\n+float rope_yarn_ramp(const float low, const float high, const uint i0) {\n+    const float y = (i0 / 2 - low) / max(0.001f, high - low);\n+    return 1.0f - min(1.0f, max(0.0f, y));\n+}\n+\n+uint rope_a_coord(const uint i0, const uint i01, const uint i02, rope_params p) {\n+#if RMS_NORM_ROPE_FUSION\n+    // Per-row offset in shared memory\n+    const uint ix = i0;\n+#else\n+    const uint ix = i02*p.nb02 + i01*p.nb01 + i0;\n+#endif\n+    return ix;\n+}\n+\n+void rope_yarn(const float theta_extrap, const uint i0, out float cos_theta, out float sin_theta, rope_params p) {\n+    float mscale = p.attn_factor;\n+    // Get n-d rotational scaling corrected for extrapolation\n+    float theta_interp = p.freq_scale * theta_extrap;\n+    float theta = theta_interp;\n+    if (p.ext_factor != 0.0f) {\n+        float ramp_mix = rope_yarn_ramp(p.corr_dims[0], p.corr_dims[1], i0) * p.ext_factor;\n+        theta = theta_interp * (1 - ramp_mix) + theta_extrap * ramp_mix;\n+\n+        // Get n-d magnitude scaling corrected for interpolation\n+        mscale *= 1.0f + 0.1f * log(1.0f / p.freq_scale);\n+    }\n+    // Backprogagation uses inverted rotation\n+    if (p.is_back != 0) {\n+        theta = -theta;\n+    }\n+    cos_theta = cos(theta) * mscale;\n+    sin_theta = sin(theta) * mscale;\n+}\n+\n+void rope_norm(const uint i0, const uint i1, rope_params p) {\n+    uint ne0 = p.ncols;\n+    uint ne1 = p.p_delta_rows;\n+\n+    if (i0 >= ne0) {\n+        return;\n+    }\n+\n+    // i1 is actually i2*nb2+i1, but the rows are contiguous\n+    const uint i01 = i1 % ne1;\n+    const uint i02 = i1 / ne1;\n+\n+    uint idst = i1*ne0 + i0;\n+    const uint ix = rope_a_coord(i0, i01, i02, p);\n+\n+    // Fusion optimization: ROPE + VIEW + SET_ROWS..\n+    // The rope output is viewed as a 1D tensor and offset based on a row index in data_i.\n+    if (p.set_rows_stride != 0) {\n+        idst = i01*ne0 + i0;\n+        idst += rope_data_i[i02].x * p.set_rows_stride;\n+    }\n+\n+    if (i0 >= p.n_dims) {\n+        rope_data_d[idst + 0] = ROPE_D_TYPE(rope_data_a[ix + 0]);\n+        rope_data_d[idst + 1] = ROPE_D_TYPE(rope_data_a[ix + 1]);\n+\n+        return;\n+    }\n+\n+    const float theta_base = rope_data_pos[i02] * pow(p.theta_scale, i0/2.0f);\n+\n+    const float freq_factor = p.has_ff != 0 ? rope_data_ff[i0/2] : 1.0f;\n+\n+    float cos_theta, sin_theta;\n+    rope_yarn(theta_base / freq_factor, i0, cos_theta, sin_theta, p);\n+\n+    const float x0 = float(rope_data_a[ix + 0]);\n+    const float x1 = float(rope_data_a[ix + 1]);\n+\n+    rope_data_d[idst + 0] = ROPE_D_TYPE(x0*cos_theta - x1*sin_theta);\n+    rope_data_d[idst + 1] = ROPE_D_TYPE(x0*sin_theta + x1*cos_theta);\n+}\n+\n+void rope_neox(const uint i0, const uint i1, rope_params p) {\n+    uint ne0 = p.ncols;\n+    uint ne1 = p.p_delta_rows;\n+\n+    if (i0 >= ne0) {\n+        return;\n+    }\n+\n+    const uint i01 = i1 % ne1;\n+    const uint i02 = i1 / ne1;\n+\n+    uint idst = i1*ne0 + i0/2;\n+    const uint ix = rope_a_coord(i0/2, i01, i02, p);\n+\n+    // Fusion optimization: ROPE + VIEW + SET_ROWS..\n+    // The rope output is viewed as a 1D tensor and offset based on a row index in rope_data_i.\n+    if (p.set_rows_stride != 0) {\n+        idst = i01*ne0 + i0/2;\n+        idst += rope_data_i[i02].x * p.set_rows_stride;\n+    }\n+\n+    if (i0 >= p.n_dims) {\n+        rope_data_d[idst + i0/2 + 0] = ROPE_D_TYPE(rope_data_a[ix + i0/2 + 0]);\n+        rope_data_d[idst + i0/2 + 1] = ROPE_D_TYPE(rope_data_a[ix + i0/2 + 1]);\n+\n+        return;\n+    }\n+\n+    const float theta_base = rope_data_pos[i02] * pow(p.theta_scale, i0/2.0f);\n+\n+    const float freq_factor = p.has_ff != 0 ? rope_data_ff[i0/2] : 1.0f;\n+\n+    float cos_theta, sin_theta;\n+    rope_yarn(theta_base / freq_factor, i0, cos_theta, sin_theta, p);\n+\n+    const float x0 = float(rope_data_a[ix + 0]);\n+    const float x1 = float(rope_data_a[ix + p.n_dims/2]);\n+\n+    rope_data_d[idst + 0]          = ROPE_D_TYPE(x0*cos_theta - x1*sin_theta);\n+    rope_data_d[idst + p.n_dims/2] = ROPE_D_TYPE(x0*sin_theta + x1*cos_theta);\n+}\n+\n+\n+void rope_multi(const uint i0, const uint i1, rope_params p) {\n+    uint ne0 = p.ncols;\n+    uint ne1 = p.p_delta_rows;\n+    uint ne2 = p.ne02;\n+\n+    if (i0 >= ne0) {\n+        return;\n+    }\n+\n+    const uint i01 = i1 % ne1;\n+    const uint i02 = i1 / ne1;\n+\n+    const uint idst = i1*ne0 + i0/2;\n+    const uint ix = rope_a_coord(i0/2, i01, i02, p);\n+\n+    if (i0 >= p.n_dims) {\n+        rope_data_d[idst + i0/2 + 0] = ROPE_D_TYPE(rope_data_a[ix + i0/2 + 0]);\n+        rope_data_d[idst + i0/2 + 1] = ROPE_D_TYPE(rope_data_a[ix + i0/2 + 1]);\n+\n+        return;\n+    }\n+\n+    const int sect_dims = p.sections[0] + p.sections[1] + p.sections[2] + p.sections[3];\n+    const int sec_w = p.sections[1] + p.sections[0];\n+    const uint sector = (i0 / 2) % sect_dims;\n+\n+    float theta_base = 0.0;\n+    if (p.is_imrope != 0) {\n+        if (sector % 3 == 1 && sector < 3 * p.sections[1]) {\n+            theta_base = rope_data_pos[i02 + ne2 * 1]*pow(p.theta_scale, i0/2.0f);\n+        } else if (sector % 3 == 2 && sector < 3 * p.sections[2]) {\n+            theta_base = rope_data_pos[i02 + ne2 * 2]*pow(p.theta_scale, i0/2.0f);\n+        } else if (sector % 3 == 0 && sector < 3 * p.sections[0]) {\n+            theta_base = rope_data_pos[i02]*pow(p.theta_scale, i0/2.0f);\n+        } else {\n+            theta_base = rope_data_pos[i02 + ne2 * 3]*pow(p.theta_scale, i0/2.0f);\n+        }\n+    } else {\n+        if (sector < p.sections[0]) {\n+            theta_base = rope_data_pos[i02]*pow(p.theta_scale, i0/2.0f);\n+        }\n+        else if (sector >= p.sections[0] && sector < sec_w) {\n+            theta_base = rope_data_pos[i02 + ne2 * 1]*pow(p.theta_scale, i0/2.0f);\n+        }\n+        else if (sector >= sec_w && sector < sec_w + p.sections[2]) {\n+            theta_base = rope_data_pos[i02 + ne2 * 2]*pow(p.theta_scale, i0/2.0f);\n+        }\n+        else if (sector >= sec_w + p.sections[2]) {\n+            theta_base = rope_data_pos[i02 + ne2 * 3]*pow(p.theta_scale, i0/2.0f);\n+        }\n+    }\n+\n+    const float freq_factor = p.has_ff != 0 ? rope_data_ff[i0/2] : 1.0f;\n+\n+    float cos_theta, sin_theta;\n+    rope_yarn(theta_base / freq_factor, i0, cos_theta, sin_theta, p);\n+\n+    const float x0 = float(rope_data_a[ix + 0]);\n+    const float x1 = float(rope_data_a[ix + p.n_dims/2]);\n+\n+    rope_data_d[idst + 0]          = ROPE_D_TYPE(x0*cos_theta - x1*sin_theta);\n+    rope_data_d[idst + p.n_dims/2] = ROPE_D_TYPE(x0*sin_theta + x1*cos_theta);\n+}\n+\n+void rope_vision(const uint i0, const uint i1, rope_params p) {\n+    uint ne0 = p.ncols;\n+    uint ne1 = p.p_delta_rows;\n+    uint ne2 = p.ne02;\n+\n+    if (i0 >= ne0) {\n+        return;\n+    }\n+\n+    const uint i01 = i1 % ne1;\n+    const uint i02 = i1 / ne1;\n+\n+    const uint idst = i1*ne0 + i0/2;\n+    const uint ix = rope_a_coord(i0/2, i01, i02, p);\n+\n+    const int sect_dims = p.sections[0] + p.sections[1];\n+    const int sec_w = p.sections[1] + p.sections[0];\n+    const uint sector = (i0 / 2) % sect_dims;\n+\n+    float theta_base = 0.0;\n+    if (sector < p.sections[0]) {\n+        const uint p0 = sector;\n+        theta_base = rope_data_pos[i02]*pow(p.theta_scale, p0);\n+    }\n+    else if (sector >= p.sections[0] && sector < sec_w) {\n+        const uint p0 = sector - p.sections[0];\n+        theta_base = rope_data_pos[i02 + ne2]*pow(p.theta_scale, p0);\n+    }\n+\n+    const float freq_factor = p.has_ff != 0 ? rope_data_ff[i0/2] : 1.0f;\n+\n+    float cos_theta, sin_theta;\n+    rope_yarn(theta_base / freq_factor, i0, cos_theta, sin_theta, p);\n+\n+    const float x0 = float(rope_data_a[ix + 0]);\n+    const float x1 = float(rope_data_a[ix + p.n_dims]);\n+\n+    rope_data_d[idst + 0]        = ROPE_D_TYPE(x0*cos_theta - x1*sin_theta);\n+    rope_data_d[idst + p.n_dims] = ROPE_D_TYPE(x0*sin_theta + x1*cos_theta);\n+}\n+"
      },
      {
        "filename": "ggml/src/ggml-vulkan/vulkan-shaders/rope_head.glsl",
        "status": "modified",
        "additions": 9,
        "deletions": 47,
        "changes": 56,
        "patch": "@@ -3,56 +3,18 @@\n #extension GL_EXT_shader_16bit_storage : require\n \n #include \"rte.glsl\"\n+#include \"rope_params.glsl\"\n \n layout(local_size_x = 1, local_size_y = 256, local_size_z = 1) in;\n \n-layout (binding = 0) readonly buffer X {A_TYPE data_a[];};\n-layout (binding = 1) readonly buffer Y {int data_pos[];};\n-layout (binding = 2) readonly buffer Z {float data_ff[];};\n-layout (binding = 3) writeonly buffer D {D_TYPE data_d[];};\n-layout (binding = 4) readonly buffer I {uvec2 data_i[];}; // indices for set_rows\n+layout (binding = 0) readonly buffer X {A_TYPE rope_data_a[];};\n+layout (binding = 1) readonly buffer Y {int rope_data_pos[];};\n+layout (binding = 2) readonly buffer Z {float rope_data_ff[];};\n+layout (binding = 3) writeonly buffer D {ROPE_D_TYPE rope_data_d[];};\n+layout (binding = 4) readonly buffer I {uvec2 rope_data_i[];}; // indices for set_rows\n \n-layout (push_constant) uniform parameter {\n-    uint ncols;\n-    uint n_dims;\n-    float freq_scale;\n-    uint p_delta_rows;\n-    float freq_base;\n-    float ext_factor;\n-    float attn_factor;\n-    float corr_dims[2];\n-    float theta_scale;\n-    uint has_ff;\n-    uint ne02;\n-    uint s1;\n-    uint s2;\n-    int sections[4];\n-    uint is_imrope;\n-    uint is_back;\n-    uint set_rows_stride;\n-} p;\n-\n-float rope_yarn_ramp(const float low, const float high, const uint i0) {\n-    const float y = (i0 / 2 - low) / max(0.001f, high - low);\n-    return 1.0f - min(1.0f, max(0.0f, y));\n-}\n \n-void rope_yarn(const float theta_extrap, const uint i0, out float cos_theta, out float sin_theta) {\n-    float mscale = p.attn_factor;\n-    // Get n-d rotational scaling corrected for extrapolation\n-    float theta_interp = p.freq_scale * theta_extrap;\n-    float theta = theta_interp;\n-    if (p.ext_factor != 0.0f) {\n-        float ramp_mix = rope_yarn_ramp(p.corr_dims[0], p.corr_dims[1], i0) * p.ext_factor;\n-        theta = theta_interp * (1 - ramp_mix) + theta_extrap * ramp_mix;\n+layout (push_constant) uniform parameter {\n+    rope_params pc;\n+};\n \n-        // Get n-d magnitude scaling corrected for interpolation\n-        mscale *= 1.0f + 0.1f * log(1.0f / p.freq_scale);\n-    }\n-    // Backprogagation uses inverted rotation\n-    if (p.is_back != 0) {\n-        theta = -theta;\n-    }\n-    cos_theta = cos(theta) * mscale;\n-    sin_theta = sin(theta) * mscale;\n-}"
      },
      {
        "filename": "ggml/src/ggml-vulkan/vulkan-shaders/rope_multi.comp",
        "status": "modified",
        "additions": 4,
        "deletions": 63,
        "changes": 67,
        "patch": "@@ -1,70 +1,11 @@\n #version 450\n \n #include \"rope_head.glsl\"\n+#include \"rope_funcs.glsl\"\n \n void main() {\n     const uint i0 = 2*gl_GlobalInvocationID.y;\n-    uint ne0 = p.ncols;\n-    uint ne1 = p.p_delta_rows;\n-    uint ne2 = p.ne02;\n-\n-    if (i0 >= ne0) {\n-        return;\n-    }\n-\n-    const uint row_dst = gl_GlobalInvocationID.x;\n-\n-    const uint row_x     = row_dst % ne1;\n-    const uint channel_x = row_dst / ne1;\n-\n-    const uint idst = row_dst*ne0 + i0/2;\n-    const uint ix   = channel_x*p.s2 + row_x*p.s1 + i0/2;\n-\n-    if (i0 >= p.n_dims) {\n-        data_d[idst + i0/2 + 0] = data_a[ix + i0/2 + 0];\n-        data_d[idst + i0/2 + 1] = data_a[ix + i0/2 + 1];\n-\n-        return;\n-    }\n-\n-    const int sect_dims = p.sections[0] + p.sections[1] + p.sections[2] + p.sections[3];\n-    const int sec_w = p.sections[1] + p.sections[0];\n-    const uint sector = (i0 / 2) % sect_dims;\n-\n-    float theta_base = 0.0;\n-    if (p.is_imrope != 0) {\n-        if (sector % 3 == 1 && sector < 3 * p.sections[1]) {\n-            theta_base = data_pos[channel_x + ne2 * 1]*pow(p.theta_scale, i0/2.0f);\n-        } else if (sector % 3 == 2 && sector < 3 * p.sections[2]) {\n-            theta_base = data_pos[channel_x + ne2 * 2]*pow(p.theta_scale, i0/2.0f);\n-        } else if (sector % 3 == 0 && sector < 3 * p.sections[0]) {\n-            theta_base = data_pos[channel_x]*pow(p.theta_scale, i0/2.0f);\n-        } else {\n-            theta_base = data_pos[channel_x + ne2 * 3]*pow(p.theta_scale, i0/2.0f);\n-        }\n-    } else {\n-        if (sector < p.sections[0]) {\n-            theta_base = data_pos[channel_x]*pow(p.theta_scale, i0/2.0f);\n-        }\n-        else if (sector >= p.sections[0] && sector < sec_w) {\n-            theta_base = data_pos[channel_x + ne2 * 1]*pow(p.theta_scale, i0/2.0f);\n-        }\n-        else if (sector >= sec_w && sector < sec_w + p.sections[2]) {\n-            theta_base = data_pos[channel_x + ne2 * 2]*pow(p.theta_scale, i0/2.0f);\n-        }\n-        else if (sector >= sec_w + p.sections[2]) {\n-            theta_base = data_pos[channel_x + ne2 * 3]*pow(p.theta_scale, i0/2.0f);\n-        }\n-    }\n-\n-    const float freq_factor = p.has_ff != 0 ? data_ff[i0/2] : 1.0f;\n-\n-    float cos_theta, sin_theta;\n-    rope_yarn(theta_base / freq_factor, i0, cos_theta, sin_theta);\n-\n-    const float x0 = float(data_a[ix + 0]);\n-    const float x1 = float(data_a[ix + p.n_dims/2]);\n-\n-    data_d[idst + 0]          = D_TYPE(x0*cos_theta - x1*sin_theta);\n-    data_d[idst + p.n_dims/2] = D_TYPE(x0*sin_theta + x1*cos_theta);\n+    // i1 is actually i2*nb2+i1, but the rows are contiguous\n+    const uint i1 = gl_GlobalInvocationID.x;\n+    rope_multi(i0, i1, pc);\n }"
      },
      {
        "filename": "ggml/src/ggml-vulkan/vulkan-shaders/rope_neox.comp",
        "status": "modified",
        "additions": 4,
        "deletions": 41,
        "changes": 45,
        "patch": "@@ -1,48 +1,11 @@\n #version 450\n \n #include \"rope_head.glsl\"\n+#include \"rope_funcs.glsl\"\n \n void main() {\n     const uint i0 = 2*gl_GlobalInvocationID.y;\n-    uint ne0 = p.ncols;\n-    uint ne1 = p.p_delta_rows;\n-\n-    if (i0 >= ne0) {\n-        return;\n-    }\n-\n-    const uint row_dst = gl_GlobalInvocationID.x;\n-\n-    const uint row_x     = row_dst % ne1;\n-    const uint channel_x = row_dst / ne1;\n-\n-    uint idst = row_dst*ne0 + i0/2;\n-    const uint ix   = channel_x*p.s2 + row_x*p.s1 + i0/2;\n-\n-    // Fusion optimization: ROPE + VIEW + SET_ROWS..\n-    // The rope output is viewed as a 1D tensor and offset based on a row index in data_i.\n-    if (p.set_rows_stride != 0) {\n-        idst = row_x*ne0 + i0/2;\n-        idst += data_i[channel_x].x * p.set_rows_stride;\n-    }\n-\n-    if (i0 >= p.n_dims) {\n-        data_d[idst + i0/2 + 0] = D_TYPE(data_a[ix + i0/2 + 0]);\n-        data_d[idst + i0/2 + 1] = D_TYPE(data_a[ix + i0/2 + 1]);\n-\n-        return;\n-    }\n-\n-    const float theta_base = data_pos[channel_x] * pow(p.theta_scale, i0/2.0f);\n-\n-    const float freq_factor = p.has_ff != 0 ? data_ff[i0/2] : 1.0f;\n-\n-    float cos_theta, sin_theta;\n-    rope_yarn(theta_base / freq_factor, i0, cos_theta, sin_theta);\n-\n-    const float x0 = float(data_a[ix + 0]);\n-    const float x1 = float(data_a[ix + p.n_dims/2]);\n-\n-    data_d[idst + 0]          = D_TYPE(x0*cos_theta - x1*sin_theta);\n-    data_d[idst + p.n_dims/2] = D_TYPE(x0*sin_theta + x1*cos_theta);\n+    // i1 is actually i2*nb2+i1, but the rows are contiguous\n+    const uint i1 = gl_GlobalInvocationID.x;\n+    rope_neox(i0, i1, pc);\n }"
      },
      {
        "filename": "ggml/src/ggml-vulkan/vulkan-shaders/rope_norm.comp",
        "status": "modified",
        "additions": 4,
        "deletions": 41,
        "changes": 45,
        "patch": "@@ -1,48 +1,11 @@\n #version 450\n \n #include \"rope_head.glsl\"\n+#include \"rope_funcs.glsl\"\n \n void main() {\n     const uint i0 = 2*gl_GlobalInvocationID.y;\n-    uint ne0 = p.ncols;\n-    uint ne1 = p.p_delta_rows;\n-\n-    if (i0 >= ne0) {\n-        return;\n-    }\n-\n-    const uint row_dst = gl_GlobalInvocationID.x;\n-\n-    const uint row_x     = row_dst % ne1;\n-    const uint channel_x = row_dst / ne1;\n-\n-    uint idst = row_dst*ne0 + i0;\n-    const uint ix   = channel_x*p.s2 + row_x*p.s1 + i0;\n-\n-    // Fusion optimization: ROPE + VIEW + SET_ROWS..\n-    // The rope output is viewed as a 1D tensor and offset based on a row index in data_i.\n-    if (p.set_rows_stride != 0) {\n-        idst = row_x*ne0 + i0;\n-        idst += data_i[channel_x].x * p.set_rows_stride;\n-    }\n-\n-    if (i0 >= p.n_dims) {\n-        data_d[idst + 0] = D_TYPE(data_a[ix + 0]);\n-        data_d[idst + 1] = D_TYPE(data_a[ix + 1]);\n-\n-        return;\n-    }\n-\n-    const float theta_base = data_pos[channel_x] * pow(p.theta_scale, i0/2.0f);\n-\n-    const float freq_factor = p.has_ff != 0 ? data_ff[i0/2] : 1.0f;\n-\n-    float cos_theta, sin_theta;\n-    rope_yarn(theta_base / freq_factor, i0, cos_theta, sin_theta);\n-\n-    const float x0 = float(data_a[ix + 0]);\n-    const float x1 = float(data_a[ix + 1]);\n-\n-    data_d[idst + 0] = D_TYPE(x0*cos_theta - x1*sin_theta);\n-    data_d[idst + 1] = D_TYPE(x0*sin_theta + x1*cos_theta);\n+    // i1 is actually i2*nb2+i1, but the rows are contiguous\n+    const uint i1 = gl_GlobalInvocationID.x;\n+    rope_norm(i0, i1, pc);\n }"
      },
      {
        "filename": "ggml/src/ggml-vulkan/vulkan-shaders/rope_params.glsl",
        "status": "added",
        "additions": 27,
        "deletions": 0,
        "changes": 27,
        "patch": "@@ -0,0 +1,27 @@\n+#if !defined(GGML_ROPE_PARAMS)\n+#define GGML_ROPE_PARAMS\n+\n+#include \"rte.glsl\"\n+\n+struct rope_params {\n+    uint rope_mode;\n+    uint ncols;\n+    uint n_dims;\n+    float freq_scale;\n+    uint p_delta_rows;\n+    float freq_base;\n+    float ext_factor;\n+    float attn_factor;\n+    float corr_dims[2];\n+    float theta_scale;\n+    uint has_ff;\n+    uint ne02;\n+    uint nb01;\n+    uint nb02;\n+    int sections[4];\n+    uint is_imrope;\n+    uint is_back;\n+    uint set_rows_stride;\n+};\n+\n+#endif // !defined(GGML_ROPE_PARAMS)"
      },
      {
        "filename": "ggml/src/ggml-vulkan/vulkan-shaders/rope_vision.comp",
        "status": "modified",
        "additions": 4,
        "deletions": 40,
        "changes": 44,
        "patch": "@@ -1,47 +1,11 @@\n #version 450\n \n #include \"rope_head.glsl\"\n+#include \"rope_funcs.glsl\"\n \n void main() {\n     const uint i0 = 2*gl_GlobalInvocationID.y;\n-    uint ne0 = p.ncols;\n-    uint ne1 = p.p_delta_rows;\n-    uint ne2 = p.ne02;\n-\n-    if (i0 >= ne0) {\n-        return;\n-    }\n-\n-    const uint row_dst = gl_GlobalInvocationID.x;\n-\n-    const uint row_x     = row_dst % ne1;\n-    const uint channel_x = row_dst / ne1;\n-\n-    const uint idst = row_dst*ne0 + i0/2;\n-    const uint ix   = channel_x*p.s2 + row_x*p.s1 + i0/2;\n-\n-    const int sect_dims = p.sections[0] + p.sections[1];\n-    const int sec_w = p.sections[1] + p.sections[0];\n-    const uint sector = (i0 / 2) % sect_dims;\n-\n-    float theta_base = 0.0;\n-    if (sector < p.sections[0]) {\n-        const uint p0 = sector;\n-        theta_base = data_pos[channel_x]*pow(p.theta_scale, p0);\n-    }\n-    else if (sector >= p.sections[0] && sector < sec_w) {\n-        const uint p0 = sector - p.sections[0];\n-        theta_base = data_pos[channel_x + ne2]*pow(p.theta_scale, p0);\n-    }\n-\n-    const float freq_factor = p.has_ff != 0 ? data_ff[i0/2] : 1.0f;\n-\n-    float cos_theta, sin_theta;\n-    rope_yarn(theta_base / freq_factor, i0, cos_theta, sin_theta);\n-\n-    const float x0 = float(data_a[ix + 0]);\n-    const float x1 = float(data_a[ix + p.n_dims]);\n-\n-    data_d[idst + 0]        = D_TYPE(x0*cos_theta - x1*sin_theta);\n-    data_d[idst + p.n_dims] = D_TYPE(x0*sin_theta + x1*cos_theta);\n+    // i1 is actually i2*nb2+i1, but the rows are contiguous\n+    const uint i1 = gl_GlobalInvocationID.x;\n+    rope_vision(i0, i1, pc);\n }"
      },
      {
        "filename": "ggml/src/ggml-vulkan/vulkan-shaders/vulkan-shaders-gen.cpp",
        "status": "modified",
        "additions": 21,
        "deletions": 19,
        "changes": 40,
        "patch": "@@ -695,6 +695,8 @@ void process_shaders() {\n     string_to_spv(\"group_norm_f32\", \"group_norm.comp\", merge_maps(base_dict, {{\"A_TYPE\", \"float\"}, {\"D_TYPE\", \"float\"}}));\n     string_to_spv(\"rms_norm_f32\", \"rms_norm.comp\", merge_maps(base_dict, {{\"A_TYPE\", \"float\"}, {\"B_TYPE\", \"float\"}, {\"D_TYPE\", \"float\"}}));\n     string_to_spv(\"rms_norm_partials_f32\", \"rms_norm_partials.comp\", merge_maps(base_dict, {{\"A_TYPE\", \"float\"}, {\"B_TYPE\", \"float\"}, {\"D_TYPE\", \"float\"}}));\n+    string_to_spv(\"rms_norm_mul_rope_f32_f32\", \"rms_norm.comp\", merge_maps(base_dict, {{\"A_TYPE\", \"float\"}, {\"B_TYPE\", \"float\"}, {\"D_TYPE\", \"float\"}, {\"ROPE_D_TYPE\", \"float\"}, {\"RMS_NORM_ROPE_FUSION\", \"1\"}}));\n+    string_to_spv(\"rms_norm_mul_rope_f32_f16_rte\", \"rms_norm.comp\", merge_maps(base_dict, {{\"A_TYPE\", \"float\"}, {\"B_TYPE\", \"float\"}, {\"D_TYPE\", \"float\"}, {\"ROPE_D_TYPE\", \"float16_t\"}, {\"RMS_NORM_ROPE_FUSION\", \"1\"}, {\"RTE16\", \"1\"}}));\n     string_to_spv(\"rms_norm_back_f32\", \"rms_norm_back.comp\", merge_maps(base_dict, {{\"A_TYPE\", \"float\"}, {\"B_TYPE\", \"float\"}, {\"D_TYPE\", \"float\"}}));\n     string_to_spv(\"l2_norm_f32\", \"l2_norm.comp\", merge_maps(base_dict, {{\"A_TYPE\", \"float\"}, {\"D_TYPE\", \"float\"}}));\n \n@@ -840,25 +842,25 @@ void process_shaders() {\n     string_to_spv(\"soft_max_f32_f16\", \"soft_max.comp\", merge_maps(base_dict, {{\"A_TYPE\", \"float\"}, {\"B_TYPE\", \"float16_t\"}, {\"D_TYPE\", \"float\"}}));\n     string_to_spv(\"soft_max_back_f32\", \"soft_max_back.comp\", merge_maps(base_dict, {{\"A_TYPE\", \"float\"}, {\"B_TYPE\", \"float\"}, {\"D_TYPE\", \"float\"}}));\n \n-    string_to_spv(\"rope_norm_f32\", \"rope_norm.comp\", {{\"A_TYPE\", \"float\"}, {\"D_TYPE\", \"float\"}});\n-    string_to_spv(\"rope_norm_f16\", \"rope_norm.comp\", {{\"A_TYPE\", \"float16_t\"}, {\"D_TYPE\", \"float16_t\"}});\n-    string_to_spv(\"rope_norm_f16_rte\", \"rope_norm.comp\", {{\"A_TYPE\", \"float16_t\"}, {\"D_TYPE\", \"float16_t\"}, {\"RTE16\", \"1\"}});\n-    string_to_spv(\"rope_norm_f32_f16\", \"rope_norm.comp\", {{\"A_TYPE\", \"float\"}, {\"D_TYPE\", \"float16_t\"}});\n-    string_to_spv(\"rope_norm_f32_f16_rte\", \"rope_norm.comp\", {{\"A_TYPE\", \"float\"}, {\"D_TYPE\", \"float16_t\"}, {\"RTE16\", \"1\"}});\n-\n-    string_to_spv(\"rope_neox_f32\", \"rope_neox.comp\", {{\"A_TYPE\", \"float\"}, {\"D_TYPE\", \"float\"}});\n-    string_to_spv(\"rope_neox_f16\", \"rope_neox.comp\", {{\"A_TYPE\", \"float16_t\"}, {\"D_TYPE\", \"float16_t\"}});\n-    string_to_spv(\"rope_neox_f16_rte\", \"rope_neox.comp\", {{\"A_TYPE\", \"float16_t\"}, {\"D_TYPE\", \"float16_t\"}, {\"RTE16\", \"1\"}});\n-    string_to_spv(\"rope_neox_f32_f16\", \"rope_neox.comp\", {{\"A_TYPE\", \"float\"}, {\"D_TYPE\", \"float16_t\"}});\n-    string_to_spv(\"rope_neox_f32_f16_rte\", \"rope_neox.comp\", {{\"A_TYPE\", \"float\"}, {\"D_TYPE\", \"float16_t\"}, {\"RTE16\", \"1\"}});\n-\n-    string_to_spv(\"rope_multi_f32\", \"rope_multi.comp\", {{\"A_TYPE\", \"float\"}, {\"D_TYPE\", \"float\"}});\n-    string_to_spv(\"rope_multi_f16\", \"rope_multi.comp\", {{\"A_TYPE\", \"float16_t\"}, {\"D_TYPE\", \"float16_t\"}});\n-    string_to_spv(\"rope_multi_f16_rte\", \"rope_multi.comp\", {{\"A_TYPE\", \"float16_t\"}, {\"D_TYPE\", \"float16_t\"}, {\"RTE16\", \"1\"}});\n-\n-    string_to_spv(\"rope_vision_f32\", \"rope_vision.comp\", {{\"A_TYPE\", \"float\"}, {\"D_TYPE\", \"float\"}});\n-    string_to_spv(\"rope_vision_f16\", \"rope_vision.comp\", {{\"A_TYPE\", \"float16_t\"}, {\"D_TYPE\", \"float16_t\"}});\n-    string_to_spv(\"rope_vision_f16_rte\", \"rope_vision.comp\", {{\"A_TYPE\", \"float16_t\"}, {\"D_TYPE\", \"float16_t\"}, {\"RTE16\", \"1\"}});\n+    string_to_spv(\"rope_norm_f32\", \"rope_norm.comp\", {{\"A_TYPE\", \"float\"}, {\"ROPE_D_TYPE\", \"float\"}});\n+    string_to_spv(\"rope_norm_f16\", \"rope_norm.comp\", {{\"A_TYPE\", \"float16_t\"}, {\"ROPE_D_TYPE\", \"float16_t\"}});\n+    string_to_spv(\"rope_norm_f16_rte\", \"rope_norm.comp\", {{\"A_TYPE\", \"float16_t\"}, {\"ROPE_D_TYPE\", \"float16_t\"}, {\"RTE16\", \"1\"}});\n+    string_to_spv(\"rope_norm_f32_f16\", \"rope_norm.comp\", {{\"A_TYPE\", \"float\"}, {\"ROPE_D_TYPE\", \"float16_t\"}});\n+    string_to_spv(\"rope_norm_f32_f16_rte\", \"rope_norm.comp\", {{\"A_TYPE\", \"float\"}, {\"ROPE_D_TYPE\", \"float16_t\"}, {\"RTE16\", \"1\"}});\n+\n+    string_to_spv(\"rope_neox_f32\", \"rope_neox.comp\", {{\"A_TYPE\", \"float\"}, {\"ROPE_D_TYPE\", \"float\"}});\n+    string_to_spv(\"rope_neox_f16\", \"rope_neox.comp\", {{\"A_TYPE\", \"float16_t\"}, {\"ROPE_D_TYPE\", \"float16_t\"}});\n+    string_to_spv(\"rope_neox_f16_rte\", \"rope_neox.comp\", {{\"A_TYPE\", \"float16_t\"}, {\"ROPE_D_TYPE\", \"float16_t\"}, {\"RTE16\", \"1\"}});\n+    string_to_spv(\"rope_neox_f32_f16\", \"rope_neox.comp\", {{\"A_TYPE\", \"float\"}, {\"ROPE_D_TYPE\", \"float16_t\"}});\n+    string_to_spv(\"rope_neox_f32_f16_rte\", \"rope_neox.comp\", {{\"A_TYPE\", \"float\"}, {\"ROPE_D_TYPE\", \"float16_t\"}, {\"RTE16\", \"1\"}});\n+\n+    string_to_spv(\"rope_multi_f32\", \"rope_multi.comp\", {{\"A_TYPE\", \"float\"}, {\"ROPE_D_TYPE\", \"float\"}});\n+    string_to_spv(\"rope_multi_f16\", \"rope_multi.comp\", {{\"A_TYPE\", \"float16_t\"}, {\"ROPE_D_TYPE\", \"float16_t\"}});\n+    string_to_spv(\"rope_multi_f16_rte\", \"rope_multi.comp\", {{\"A_TYPE\", \"float16_t\"}, {\"ROPE_D_TYPE\", \"float16_t\"}, {\"RTE16\", \"1\"}});\n+\n+    string_to_spv(\"rope_vision_f32\", \"rope_vision.comp\", {{\"A_TYPE\", \"float\"}, {\"ROPE_D_TYPE\", \"float\"}});\n+    string_to_spv(\"rope_vision_f16\", \"rope_vision.comp\", {{\"A_TYPE\", \"float16_t\"}, {\"ROPE_D_TYPE\", \"float16_t\"}});\n+    string_to_spv(\"rope_vision_f16_rte\", \"rope_vision.comp\", {{\"A_TYPE\", \"float16_t\"}, {\"ROPE_D_TYPE\", \"float16_t\"}, {\"RTE16\", \"1\"}});\n \n     string_to_spv(\"argsort_f32\", \"argsort.comp\", {{\"A_TYPE\", \"float\"}});\n "
      },
      {
        "filename": "tests/test-backend-ops.cpp",
        "status": "modified",
        "additions": 89,
        "deletions": 0,
        "changes": 89,
        "patch": "@@ -2294,6 +2294,79 @@ struct test_rope_set_rows : public test_case {\n     }\n };\n \n+// GGML_OP_RMS_NORM + GGML_OP_MUL + GGML_OP_ROPE (+ GGML_OP_VIEW + GGML_OP_SET_ROWS)\n+struct test_rms_norm_mul_rope : public test_case {\n+    const std::array<int64_t, 4> ne;\n+    const float eps;\n+    const bool multi_add; // test a sequence of adds feeding into rms_norm\n+    const bool set_rows;\n+    int mode;\n+\n+    std::string op_desc(ggml_tensor * t) override {\n+        GGML_UNUSED(t);\n+        return \"RMS_NORM_MUL_ROPE\";\n+    }\n+\n+    bool run_whole_graph() override { return true; }\n+\n+    std::string vars() override {\n+        return VARS_TO_STR5(ne, eps, multi_add, set_rows, mode);\n+    }\n+\n+    test_rms_norm_mul_rope(std::array<int64_t, 4> ne, float eps = 1e-6f, bool multi_add = false,\n+                           bool set_rows = false, int mode = GGML_ROPE_TYPE_NORMAL)\n+        : ne(ne), eps(eps), multi_add(multi_add), set_rows(set_rows), mode(mode) {}\n+\n+    ggml_tensor * build_graph(ggml_context * ctx) override {\n+        ggml_tensor * a = ggml_new_tensor_4d(ctx, GGML_TYPE_F32, ne[0], ne[1], ne[2], 1);\n+        ggml_tensor * b = ggml_new_tensor_4d(ctx, GGML_TYPE_F32, ne[0], ne[1], ne[2], 1);\n+        ggml_tensor * c = ggml_new_tensor_4d(ctx, GGML_TYPE_F32, ne[0], ne[1], ne[2], 1);\n+\n+        if (multi_add) {\n+            a = ggml_add(ctx, ggml_add(ctx, a, b), c);\n+        }\n+\n+        a = ggml_mul(ctx, ggml_rms_norm(ctx, a, eps), b);\n+\n+        ggml_tensor * pos = ggml_new_tensor_1d(ctx, GGML_TYPE_I32, ne[2]);\n+\n+        ggml_tensor * rope = ggml_rope(ctx, a, pos, ne[0], mode);\n+\n+        ggml_tensor * out;\n+\n+        if (set_rows) {\n+            ggml_tensor * view = ggml_view_2d(ctx, rope, ne[0] * ne[1], ne[2], rope->nb[2], 0);\n+\n+            ggml_tensor * dst = ggml_new_tensor_4d(ctx, GGML_TYPE_F16, ne[0] * ne[1], ne[2] * ne[3], 1, 1);\n+            ggml_set_name(dst, \"dst\");\n+\n+            ggml_tensor * row_idxs = ggml_new_tensor_3d(ctx, GGML_TYPE_I64, ne[2], 1, 1);\n+            ggml_set_name(row_idxs, \"row_idxs\");\n+\n+            out = ggml_set_rows(ctx, dst, view, row_idxs);\n+            ggml_set_name(out, \"out\");\n+        } else {\n+            out = rope;\n+        }\n+\n+        return out;\n+    }\n+\n+    void initialize_tensors(ggml_context * ctx) override {\n+        for (ggml_tensor * t = ggml_get_first_tensor(ctx); t != NULL; t = ggml_get_next_tensor(ctx, t)) {\n+            if (t->type == GGML_TYPE_I64 || t->type == GGML_TYPE_I32) {\n+                if (ggml_is_view_op(t->op)) {\n+                    continue;\n+                }\n+\n+                init_set_rows_row_ids(t, ne[2]);\n+            } else {\n+                init_tensor_uniform(t);\n+            }\n+        }\n+    }\n+};\n+\n // GGML_OP_ARGMAX\n struct test_argmax : public test_case {\n     const ggml_type type;\n@@ -6743,6 +6816,22 @@ static std::vector<std::unique_ptr<test_case>> make_test_cases_eval() {\n         }\n     }\n \n+    for (auto multi_add : {false, true}) {\n+        for (auto set_rows : {false, true}) {\n+            for (auto rope : {GGML_ROPE_TYPE_NORMAL, GGML_ROPE_TYPE_NEOX}) {\n+                test_cases.emplace_back(new test_rms_norm_mul_rope({768, 1, 1, 1}, 1e-6f, multi_add, set_rows, rope));\n+                test_cases.emplace_back(new test_rms_norm_mul_rope({768, 3, 1, 1}, 1e-6f, multi_add, set_rows, rope));\n+                test_cases.emplace_back(new test_rms_norm_mul_rope({768, 3, 5, 1}, 1e-6f, multi_add, set_rows, rope));\n+                test_cases.emplace_back(new test_rms_norm_mul_rope({128, 32, 2, 1}, 1e-6f, multi_add, set_rows, rope));\n+                test_cases.emplace_back(new test_rms_norm_mul_rope({128, 4, 2, 1}, 1e-6f, multi_add, set_rows, rope));\n+                test_cases.emplace_back(new test_rms_norm_mul_rope({128, 32, 50, 1}, 1e-6f, multi_add, set_rows, rope));\n+                test_cases.emplace_back(new test_rms_norm_mul_rope({128, 4, 50, 1}, 1e-6f, multi_add, set_rows, rope));\n+                test_cases.emplace_back(new test_rms_norm_mul_rope({8192, 2, 2, 1}, 1e-6f, multi_add, set_rows, rope));\n+                test_cases.emplace_back(new test_rms_norm_mul_rope({8192, 2, 2, 1}, 1e-6f, multi_add, set_rows, rope));\n+            }\n+        }\n+    }\n+\n     test_cases.emplace_back(new test_l2_norm(GGML_TYPE_F32, {64, 5, 4, 3}, 1e-12f));\n \n     for (int64_t d_conv : {3, 4}) {"
      }
    ],
    "num_files": 12,
    "scraped_at": "2025-11-16T23:29:00.537463"
  },
  {
    "pr_number": 16962,
    "title": "CUDA: add head size 72 for flash-attn",
    "body": "This PR adds cases for head size 72, used for Qwen3-VL and Gemma 3 vision encoders. Added only the cases for the tile kernel, like head size 40.\r\nThe parameters for the tile kernel is taken from head size 40, will probably require some optimizations. Tested with `test-backend-ops` on a 3060 and a 4060 Ti. Not tested on AMD, though I have added the cases for AMD.\r\nFixes #16950",
    "html_url": "https://github.com/ggml-org/llama.cpp/pull/16962",
    "created_at": "2025-11-03T10:07:35Z",
    "merged_at": "2025-11-03T13:29:11Z",
    "merge_commit_sha": "622cd010ff4a65bef67edbf2f9bf4707c01f98f7",
    "base_ref": "master",
    "head_sha": "72545ce2bebddc9235b553af01252fbb8443a1ba",
    "user": "theo77186",
    "files": [
      {
        "filename": "ggml/src/ggml-cuda/fattn-tile.cu",
        "status": "modified",
        "additions": 4,
        "deletions": 0,
        "changes": 4,
        "patch": "@@ -14,6 +14,10 @@ void ggml_cuda_flash_attn_ext_tile(ggml_backend_cuda_context & ctx, ggml_tensor\n             GGML_ASSERT(V->ne[0] == K->ne[0]);\n             ggml_cuda_flash_attn_ext_tile_case< 64,  64>(ctx, dst);\n         } break;\n+        case  72: {\n+            GGML_ASSERT(V->ne[0] == K->ne[0]);\n+            ggml_cuda_flash_attn_ext_tile_case< 72,  72>(ctx, dst);\n+        } break;\n         case  80: {\n             GGML_ASSERT(V->ne[0] == K->ne[0]);\n             ggml_cuda_flash_attn_ext_tile_case< 80,  80>(ctx, dst);"
      },
      {
        "filename": "ggml/src/ggml-cuda/fattn-tile.cuh",
        "status": "modified",
        "additions": 29,
        "deletions": 2,
        "changes": 31,
        "patch": "@@ -6,7 +6,7 @@\n // nbatch_K == number of K columns to load in parallel for KQ calculation\n \n // TODO optimize kernel parameters for FP16 NVIDIA (P100)\n-// TODO optimize kernel parameters for head sizes 40, 80, 96, 112\n+// TODO optimize kernel parameters for head sizes 40, 72, 80, 96, 112\n \n // The ROCm compiler cannot handle templating in __launch_bounds__.\n // As a workaround, define a macro to package the kernel parameters as uint32_t:\n@@ -32,6 +32,12 @@ static constexpr __host__ __device__ uint32_t ggml_cuda_fattn_tile_get_config_nv\n     GGML_CUDA_FATTN_TILE_CONFIG_CASE( 64,  64, 16, 256, 2,  64,  64)\n     GGML_CUDA_FATTN_TILE_CONFIG_CASE( 64,  64, 32, 256, 2,  64,  64)\n \n+    GGML_CUDA_FATTN_TILE_CONFIG_CASE( 72,  72,  2,  64, 2,  64,  72)\n+    GGML_CUDA_FATTN_TILE_CONFIG_CASE( 72,  72,  4, 128, 2,  64,  72)\n+    GGML_CUDA_FATTN_TILE_CONFIG_CASE( 72,  72,  8, 256, 2,  64,  72)\n+    GGML_CUDA_FATTN_TILE_CONFIG_CASE( 72,  72, 16, 256, 2,  64,  72)\n+    GGML_CUDA_FATTN_TILE_CONFIG_CASE( 72,  72, 32, 256, 2,  64,  72)\n+\n     GGML_CUDA_FATTN_TILE_CONFIG_CASE( 80,  80,  2,  64, 2,  64,  40)\n     GGML_CUDA_FATTN_TILE_CONFIG_CASE( 80,  80,  4, 128, 2,  64,  40)\n     GGML_CUDA_FATTN_TILE_CONFIG_CASE( 80,  80,  8, 256, 2,  64,  40)\n@@ -80,6 +86,12 @@ static constexpr __host__ __device__ uint32_t ggml_cuda_fattn_tile_get_config_nv\n     GGML_CUDA_FATTN_TILE_CONFIG_CASE( 64,  64, 16, 128, 3,  64,  64)\n     GGML_CUDA_FATTN_TILE_CONFIG_CASE( 64,  64, 32, 256, 2,  64,  64)\n \n+    GGML_CUDA_FATTN_TILE_CONFIG_CASE( 72,  72,  2,  64, 2,  32,  72)\n+    GGML_CUDA_FATTN_TILE_CONFIG_CASE( 72,  72,  4, 128, 2,  32,  72)\n+    GGML_CUDA_FATTN_TILE_CONFIG_CASE( 72,  72,  8, 256, 2,  32,  72)\n+    GGML_CUDA_FATTN_TILE_CONFIG_CASE( 72,  72, 16, 256, 2,  32,  72)\n+    GGML_CUDA_FATTN_TILE_CONFIG_CASE( 72,  72, 32, 256, 2,  32,  72)\n+\n     GGML_CUDA_FATTN_TILE_CONFIG_CASE( 80,  80,  2,  64, 2,  32,  40)\n     GGML_CUDA_FATTN_TILE_CONFIG_CASE( 80,  80,  4, 128, 2,  32,  40)\n     GGML_CUDA_FATTN_TILE_CONFIG_CASE( 80,  80,  8, 256, 2,  32,  40)\n@@ -130,6 +142,13 @@ static constexpr __host__ __device__ uint32_t ggml_cuda_fattn_tile_get_config_am\n     GGML_CUDA_FATTN_TILE_CONFIG_CASE( 64,  64, 32, 256, 2,  64,  64)\n     GGML_CUDA_FATTN_TILE_CONFIG_CASE( 64,  64, 64, 256, 2,  64,  64)\n \n+    GGML_CUDA_FATTN_TILE_CONFIG_CASE( 72,  72,  2,  64, 2,  32,  72)\n+    GGML_CUDA_FATTN_TILE_CONFIG_CASE( 72,  72,  4, 128, 2,  32,  72)\n+    GGML_CUDA_FATTN_TILE_CONFIG_CASE( 72,  72,  8, 256, 2,  32,  72)\n+    GGML_CUDA_FATTN_TILE_CONFIG_CASE( 72,  72, 16, 256, 2,  32,  72)\n+    GGML_CUDA_FATTN_TILE_CONFIG_CASE( 72,  72, 32, 256, 2,  32,  72)\n+    GGML_CUDA_FATTN_TILE_CONFIG_CASE( 72,  72, 64, 256, 2,  32,  72)\n+\n     GGML_CUDA_FATTN_TILE_CONFIG_CASE( 80,  80,  2,  64, 2,  32,  40)\n     GGML_CUDA_FATTN_TILE_CONFIG_CASE( 80,  80,  4, 128, 2,  32,  40)\n     GGML_CUDA_FATTN_TILE_CONFIG_CASE( 80,  80,  8, 256, 2,  32,  40)\n@@ -185,6 +204,13 @@ static constexpr __host__ __device__ uint32_t ggml_cuda_fattn_tile_get_config_am\n     GGML_CUDA_FATTN_TILE_CONFIG_CASE( 64,  64, 32, 128, 4,  64,  64)\n     GGML_CUDA_FATTN_TILE_CONFIG_CASE( 64,  64, 64, 128, 5,  64,  64)\n \n+    GGML_CUDA_FATTN_TILE_CONFIG_CASE( 72,  72,  2,  64, 2,  32,  72)\n+    GGML_CUDA_FATTN_TILE_CONFIG_CASE( 72,  72,  4, 128, 2,  32,  72)\n+    GGML_CUDA_FATTN_TILE_CONFIG_CASE( 72,  72,  8, 256, 2,  32,  72)\n+    GGML_CUDA_FATTN_TILE_CONFIG_CASE( 72,  72, 16, 256, 2,  32,  72)\n+    GGML_CUDA_FATTN_TILE_CONFIG_CASE( 72,  72, 32, 256, 2,  32,  72)\n+    GGML_CUDA_FATTN_TILE_CONFIG_CASE( 72,  72, 64, 256, 2,  32,  72)\n+\n     GGML_CUDA_FATTN_TILE_CONFIG_CASE( 80,  80,  2,  64, 2,  32,  40)\n     GGML_CUDA_FATTN_TILE_CONFIG_CASE( 80,  80,  4, 128, 2,  32,  40)\n     GGML_CUDA_FATTN_TILE_CONFIG_CASE( 80,  80,  8, 256, 2,  32,  40)\n@@ -723,7 +749,7 @@ static __global__ void flash_attn_tile(\n \n     if (\n #ifdef GGML_USE_WMMA_FATTN\n-            (ncols2 != 1 && DV != 40 && DV != 512) ||\n+            (ncols2 != 1 && DV != 40 && DV != 72 && DV != 512) ||\n #endif // GGML_USE_WMMA_FATTN\n             (use_logit_softcap && !(DV == 128 || DV == 256))\n     ) {\n@@ -1198,6 +1224,7 @@ void ggml_cuda_flash_attn_ext_tile(ggml_backend_cuda_context & ctx, ggml_tensor\n \n extern DECL_FATTN_TILE_CASE( 40,  40);\n extern DECL_FATTN_TILE_CASE( 64,  64);\n+extern DECL_FATTN_TILE_CASE( 72,  72);\n extern DECL_FATTN_TILE_CASE( 80,  80);\n extern DECL_FATTN_TILE_CASE( 96,  96);\n extern DECL_FATTN_TILE_CASE(112, 112);"
      },
      {
        "filename": "ggml/src/ggml-cuda/fattn.cu",
        "status": "modified",
        "additions": 3,
        "deletions": 2,
        "changes": 5,
        "patch": "@@ -223,6 +223,7 @@ static best_fattn_kernel ggml_cuda_get_best_fattn_kernel(const int device, const\n     switch (K->ne[0]) {\n         case  40:\n         case  64:\n+        case  72:\n         case  80:\n         case  96:\n         case 128:\n@@ -275,7 +276,7 @@ static best_fattn_kernel ggml_cuda_get_best_fattn_kernel(const int device, const\n     const bool can_use_vector_kernel = Q->ne[0] <= 256 && Q->ne[0] % 64 == 0 && K->ne[1] % FATTN_KQ_STRIDE == 0;\n \n     // If Turing tensor cores available, use them:\n-    if (turing_mma_available(cc) && K->ne[1] % FATTN_KQ_STRIDE == 0 && Q->ne[0] != 40) {\n+    if (turing_mma_available(cc) && K->ne[1] % FATTN_KQ_STRIDE == 0 && Q->ne[0] != 40 && Q->ne[0] != 72) {\n         if (can_use_vector_kernel) {\n             if (!ggml_is_quantized(K->type) && !ggml_is_quantized(V->type)) {\n                 if (cc >= GGML_CUDA_CC_ADA_LOVELACE && Q->ne[1] == 1 && Q->ne[3] == 1 && !(gqa_ratio > 4 && K->ne[1] >= 8192)) {\n@@ -301,7 +302,7 @@ static best_fattn_kernel ggml_cuda_get_best_fattn_kernel(const int device, const\n     }\n \n     // Use the WMMA kernel if possible:\n-    if (ggml_cuda_should_use_wmma_fattn(cc) && K->ne[1] % FATTN_KQ_STRIDE == 0 && Q->ne[0] != 40 && Q->ne[0] != 576) {\n+    if (ggml_cuda_should_use_wmma_fattn(cc) && K->ne[1] % FATTN_KQ_STRIDE == 0 && Q->ne[0] != 40 && Q->ne[0] != 72 && Q->ne[0] != 576) {\n         if (can_use_vector_kernel && Q->ne[1] <= 2) {\n             return BEST_FATTN_KERNEL_VEC;\n         }"
      },
      {
        "filename": "ggml/src/ggml-cuda/template-instances/fattn-tile-instance-dkq72-dv72.cu",
        "status": "added",
        "additions": 5,
        "deletions": 0,
        "changes": 5,
        "patch": "@@ -0,0 +1,5 @@\n+// This file has been autogenerated by generate_cu_files.py, do not edit manually.\n+\n+#include \"../fattn-tile.cuh\"\n+\n+DECL_FATTN_TILE_CASE(72, 72);"
      },
      {
        "filename": "ggml/src/ggml-cuda/template-instances/generate_cu_files.py",
        "status": "modified",
        "additions": 3,
        "deletions": 1,
        "changes": 4,
        "patch": "@@ -3,7 +3,7 @@\n from glob import glob\n import os\n \n-HEAD_SIZES_KQ = [40, 64, 80, 96, 112, 128, 256, 576]\n+HEAD_SIZES_KQ = [40, 64, 72, 80, 96, 112, 128, 256, 576]\n \n TYPES_KV = [\"GGML_TYPE_F16\", \"GGML_TYPE_Q4_0\", \"GGML_TYPE_Q4_1\", \"GGML_TYPE_Q5_0\", \"GGML_TYPE_Q5_1\", \"GGML_TYPE_Q8_0\"]\n \n@@ -81,6 +81,8 @@ def get_short_name(long_quant_name):\n             for head_size_kq in HEAD_SIZES_KQ:\n                 if head_size_kq == 40:\n                     continue\n+                if head_size_kq == 72:\n+                    continue\n                 if head_size_kq != 576 and ncols2 == 16:\n                     continue\n                 if head_size_kq == 576 and ncols2 != 16:"
      }
    ],
    "num_files": 5,
    "scraped_at": "2025-11-16T23:29:01.609981"
  },
  {
    "pr_number": 16941,
    "title": "Model: add openPangu-Embedded",
    "body": "Add a new model openPangu-Embedded-1/7B-V1.1.\r\nYu can get the the model from [model path](https://gitcode.com/ascend-tribe).\r\n\r\n",
    "html_url": "https://github.com/ggml-org/llama.cpp/pull/16941",
    "created_at": "2025-11-02T16:06:32Z",
    "merged_at": "2025-11-05T09:28:58Z",
    "merge_commit_sha": "9f052478c2c38ec10cb378109b110a1f7033ce11",
    "base_ref": "master",
    "head_sha": "95280c06477189081964d83c2920ba31cde3383a",
    "user": "Lpzhan931",
    "files": [
      {
        "filename": "convert_hf_to_gguf.py",
        "status": "modified",
        "additions": 36,
        "deletions": 0,
        "changes": 36,
        "patch": "@@ -7187,6 +7187,42 @@ def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None):\n         return super().modify_tensors(data_torch, name, bid)\n \n \n+@ModelBase.register(\"PanguEmbeddedForCausalLM\")\n+class PanguEmbeddedModel(TextModel):\n+    model_arch = gguf.MODEL_ARCH.PANGU_EMBED\n+\n+    def set_vocab(self):\n+        self._set_vocab_sentencepiece()\n+\n+        tokenizer_config_file = self.dir_model / 'tokenizer_config.json'\n+        if tokenizer_config_file.is_file():\n+            with open(tokenizer_config_file, \"r\", encoding=\"utf-8\") as f:\n+                tokenizer_config_json = json.load(f)\n+                if \"add_prefix_space\" in tokenizer_config_json:\n+                    self.gguf_writer.add_add_space_prefix(tokenizer_config_json[\"add_prefix_space\"])\n+\n+    def set_gguf_parameters(self):\n+        super().set_gguf_parameters()\n+        hparams = self.hparams\n+        self.gguf_writer.add_vocab_size(hparams[\"vocab_size\"])\n+\n+        # PanguEmbedded's hparam loaded from config.json without head_dim\n+        if (rope_dim := hparams.get(\"head_dim\")) is None:\n+            rope_dim = hparams[\"hidden_size\"] // hparams[\"num_attention_heads\"]\n+        self.gguf_writer.add_rope_dimension_count(rope_dim)\n+\n+        if hparams.get(\"head_dim\") is None:\n+            self.gguf_writer.add_key_length(rope_dim)\n+            self.gguf_writer.add_value_length(rope_dim)\n+\n+    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n+        if name == \"lm_head.weight\":\n+            if self.hparams.get(\"tie_word_embeddings\", False):\n+                logger.info(\"Skipping tied output layer 'lm_head.weight'\")\n+                return []\n+        return [(self.map_tensor_name(name), data_torch)]\n+\n+\n @ModelBase.register(\"Dots1ForCausalLM\")\n class Dots1Model(Qwen2MoeModel):\n     model_arch = gguf.MODEL_ARCH.DOTS1"
      },
      {
        "filename": "gguf-py/gguf/constants.py",
        "status": "modified",
        "additions": 20,
        "deletions": 0,
        "changes": 20,
        "patch": "@@ -426,6 +426,7 @@ class MODEL_ARCH(IntEnum):\n     APERTUS          = auto()\n     COGVLM           = auto()\n     MINIMAXM2        = auto()\n+    PANGU_EMBED      = auto()\n \n \n class VISION_PROJECTOR_TYPE(IntEnum):\n@@ -793,6 +794,7 @@ class MODEL_TENSOR(IntEnum):\n     MODEL_ARCH.APERTUS:          \"apertus\",\n     MODEL_ARCH.MINIMAXM2:        \"minimax-m2\",\n     MODEL_ARCH.COGVLM:           \"cogvlm\",\n+    MODEL_ARCH.PANGU_EMBED:      \"pangu-embedded\",\n }\n \n VISION_PROJECTOR_TYPE_NAMES: dict[VISION_PROJECTOR_TYPE, str] = {\n@@ -2958,6 +2960,20 @@ class MODEL_TENSOR(IntEnum):\n         MODEL_TENSOR.VISEXP_UP,\n         MODEL_TENSOR.VISEXP_DOWN,\n     ],\n+    MODEL_ARCH.PANGU_EMBED: [\n+        MODEL_TENSOR.TOKEN_EMBD,\n+        MODEL_TENSOR.OUTPUT_NORM,\n+        MODEL_TENSOR.OUTPUT,\n+        MODEL_TENSOR.ATTN_NORM,\n+        MODEL_TENSOR.ATTN_Q,\n+        MODEL_TENSOR.ATTN_K,\n+        MODEL_TENSOR.ATTN_V,\n+        MODEL_TENSOR.ATTN_OUT,\n+        MODEL_TENSOR.FFN_NORM,\n+        MODEL_TENSOR.FFN_GATE,\n+        MODEL_TENSOR.FFN_DOWN,\n+        MODEL_TENSOR.FFN_UP,\n+    ],\n     # TODO\n }\n \n@@ -3013,6 +3029,10 @@ class MODEL_TENSOR(IntEnum):\n     MODEL_ARCH.BAILINGMOE: [\n         MODEL_TENSOR.ROPE_FREQS,\n     ],\n+    MODEL_ARCH.PANGU_EMBED: [\n+        MODEL_TENSOR.ROPE_FREQS,\n+        MODEL_TENSOR.ATTN_ROT_EMBD,\n+    ],\n }\n \n #"
      },
      {
        "filename": "src/CMakeLists.txt",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -99,6 +99,7 @@ add_library(llama\n             models/openai-moe-iswa.cpp\n             models/openelm.cpp\n             models/orion.cpp\n+            models/pangu-embedded.cpp\n             models/phi2.cpp\n             models/phi3.cpp\n             models/plamo.cpp"
      },
      {
        "filename": "src/llama-arch.cpp",
        "status": "modified",
        "additions": 18,
        "deletions": 0,
        "changes": 18,
        "patch": "@@ -107,6 +107,7 @@ static const std::map<llm_arch, const char *> LLM_ARCH_NAMES = {\n     { LLM_ARCH_APERTUS,          \"apertus\"          },\n     { LLM_ARCH_MINIMAX_M2,       \"minimax-m2\"       },\n     { LLM_ARCH_COGVLM,           \"cogvlm\"           },\n+    { LLM_ARCH_PANGU_EMBED,      \"pangu-embedded\"   },\n     { LLM_ARCH_UNKNOWN,          \"(unknown)\"        },\n };\n \n@@ -2377,6 +2378,23 @@ static const std::map<llm_arch, std::map<llm_tensor, const char *>> LLM_TENSOR_N\n             { LLM_TENSOR_FFN_EXP_PROBS_B,    \"blk.%d.exp_probs_b\" },\n         },\n     },\n+    {\n+        LLM_ARCH_PANGU_EMBED,\n+        {\n+            { LLM_TENSOR_TOKEN_EMBD,      \"token_embd\" },\n+            { LLM_TENSOR_OUTPUT_NORM,     \"output_norm\" },\n+            { LLM_TENSOR_OUTPUT,          \"output\" },\n+            { LLM_TENSOR_ATTN_NORM,       \"blk.%d.attn_norm\" },\n+            { LLM_TENSOR_ATTN_Q,          \"blk.%d.attn_q\" },\n+            { LLM_TENSOR_ATTN_K,          \"blk.%d.attn_k\" },\n+            { LLM_TENSOR_ATTN_V,          \"blk.%d.attn_v\" },\n+            { LLM_TENSOR_ATTN_OUT,        \"blk.%d.attn_output\" },\n+            { LLM_TENSOR_FFN_NORM,        \"blk.%d.ffn_norm\" },\n+            { LLM_TENSOR_FFN_GATE,        \"blk.%d.ffn_gate\" },\n+            { LLM_TENSOR_FFN_DOWN,        \"blk.%d.ffn_down\" },\n+            { LLM_TENSOR_FFN_UP,          \"blk.%d.ffn_up\" },\n+        },\n+    },\n     {\n         LLM_ARCH_COGVLM,\n         {"
      },
      {
        "filename": "src/llama-arch.h",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -111,6 +111,7 @@ enum llm_arch {\n     LLM_ARCH_APERTUS,\n     LLM_ARCH_MINIMAX_M2,\n     LLM_ARCH_COGVLM,\n+    LLM_ARCH_PANGU_EMBED,\n     LLM_ARCH_UNKNOWN,\n };\n "
      },
      {
        "filename": "src/llama-chat.cpp",
        "status": "modified",
        "additions": 32,
        "deletions": 0,
        "changes": 32,
        "patch": "@@ -73,6 +73,7 @@ static const std::map<std::string, llm_chat_template> LLM_CHAT_TEMPLATES = {\n     { \"kimi-k2\",           LLM_CHAT_TEMPLATE_KIMI_K2           },\n     { \"seed_oss\",          LLM_CHAT_TEMPLATE_SEED_OSS          },\n     { \"grok-2\",            LLM_CHAT_TEMPLATE_GROK_2            },\n+    { \"pangu-embedded\",    LLM_CHAT_TEMPLATE_PANGU_EMBED       },\n };\n \n llm_chat_template llm_chat_template_from_str(const std::string & name) {\n@@ -213,6 +214,8 @@ llm_chat_template llm_chat_detect_template(const std::string & tmpl) {\n         return LLM_CHAT_TEMPLATE_SEED_OSS;\n     } else if (tmpl_contains(\"'Assistant: '  + message['content'] + '<|separator|>\")) {\n         return LLM_CHAT_TEMPLATE_GROK_2;\n+    } else if (tmpl_contains(LU8(\"[unused9]\u7cfb\u7edf\uff1a[unused10]\"))) {\n+        return LLM_CHAT_TEMPLATE_PANGU_EMBED;\n     }\n     return LLM_CHAT_TEMPLATE_UNKNOWN;\n }\n@@ -813,6 +816,35 @@ int32_t llm_chat_apply_template(\n         if (add_ass) {\n             ss << \"Assistant:\";\n         }\n+    }else if (tmpl == LLM_CHAT_TEMPLATE_PANGU_EMBED) {\n+        // [unused9]\u7cfb\u7edf\uff1axxx[unused10]\n+        // [unused9]\u7528\u6237\uff1axxx[unused10]\n+        // [unused9]\u52a9\u624b\uff1axxx[unused10]\n+        // ...\n+        for (size_t i = 0; i < chat.size(); ++i) {\n+            const auto & msg = chat[i];\n+            const std::string & role = msg->role;\n+            const std::string & content = msg->content;\n+\n+            if (i == 0 && role != \"system\") {\n+                ss << \"[unused9]\u7cfb\u7edf\uff1a[unused10]\";\n+            }\n+\n+            if (role == \"system\") {\n+                ss << \"[unused9]\u7cfb\u7edf\uff1a\" << content << \"[unused10]\";\n+            } else if (role == \"user\") {\n+                ss << \"[unused9]\u7528\u6237\uff1a\" << content << \"[unused10]\";\n+            } else if (role == \"assistant\") {\n+                ss << \"[unused9]\u52a9\u624b\uff1a\" << content << \"[unused10]\";\n+            } else if (role == \"tool\") {\n+                ss << \"[unused9]\u5de5\u5177\uff1a\" << content << \"[unused10]\";\n+            } else if (role == \"function\") {\n+                ss << \"[unused9]\u65b9\u6cd5\uff1a\" << content << \"[unused10]\";\n+            }\n+        }\n+        if (add_ass) {\n+            ss << \"[unused9]\u52a9\u624b\uff1a\";\n+        }\n     } else {\n         // template not supported\n         return -1;"
      },
      {
        "filename": "src/llama-chat.h",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -53,6 +53,7 @@ enum llm_chat_template {\n     LLM_CHAT_TEMPLATE_KIMI_K2,\n     LLM_CHAT_TEMPLATE_SEED_OSS,\n     LLM_CHAT_TEMPLATE_GROK_2,\n+    LLM_CHAT_TEMPLATE_PANGU_EMBED,\n     LLM_CHAT_TEMPLATE_UNKNOWN,\n };\n "
      },
      {
        "filename": "src/llama-model.cpp",
        "status": "modified",
        "additions": 58,
        "deletions": 0,
        "changes": 58,
        "patch": "@@ -2177,6 +2177,15 @@ void llama_model::load_hparams(llama_model_loader & ml) {\n                     default: type = LLM_TYPE_UNKNOWN;\n                 }\n             } break;\n+        case LLM_ARCH_PANGU_EMBED:\n+            {\n+                ml.get_key(LLM_KV_ATTENTION_LAYERNORM_RMS_EPS, hparams.f_norm_rms_eps);\n+                switch (hparams.n_layer) {\n+                    case 26: type = LLM_TYPE_1B; break; // openPangu-Embedded-1B-V1.1\n+                    case 34: type = LLM_TYPE_7B; break; // openPangu-Embedded-7B-V1.1\n+                    default: type = LLM_TYPE_UNKNOWN;\n+                }\n+            } break;\n         default: throw std::runtime_error(\"unsupported model architecture\");\n     }\n \n@@ -6263,6 +6272,50 @@ bool llama_model::load_tensors(llama_model_loader & ml) {\n                         layer.visexp_ffn_up   = create_tensor(tn(LLM_TENSOR_VISEXP_FFN_UP,   \"weight\", i), {n_embd,   n_ff}, 0);\n                     }\n                 } break;\n+            case LLM_ARCH_PANGU_EMBED:\n+                {\n+                    tok_embd = create_tensor(tn(LLM_TENSOR_TOKEN_EMBD, \"weight\"), {n_embd, n_vocab}, 0);\n+\n+                    // output\n+                    output_norm = create_tensor(tn(LLM_TENSOR_OUTPUT_NORM, \"weight\"), {n_embd}, 0);\n+                    output      = create_tensor(tn(LLM_TENSOR_OUTPUT,      \"weight\"), {n_embd, n_vocab}, TENSOR_NOT_REQUIRED);\n+\n+                    // if output is NULL, init from the input tok embed\n+                    if (output == NULL) {\n+                        output = create_tensor(tn(LLM_TENSOR_TOKEN_EMBD, \"weight\"), {n_embd, n_vocab}, TENSOR_DUPLICATED);\n+                    }\n+\n+                    for (int i = 0; i < n_layer; ++i) {\n+                        auto & layer = layers[i];\n+\n+                        layer.attn_norm = create_tensor(tn(LLM_TENSOR_ATTN_NORM, \"weight\", i), {n_embd}, 0);\n+\n+                        // weight tensors\n+                        layer.wq = create_tensor(tn(LLM_TENSOR_ATTN_Q,   \"weight\", i), {n_embd, n_embd_head_k * n_head}, 0);\n+                        layer.wk = create_tensor(tn(LLM_TENSOR_ATTN_K,   \"weight\", i), {n_embd, n_embd_k_gqa}, 0);\n+                        layer.wv = create_tensor(tn(LLM_TENSOR_ATTN_V,   \"weight\", i), {n_embd, n_embd_v_gqa}, 0);\n+                        layer.wo = create_tensor(tn(LLM_TENSOR_ATTN_OUT, \"weight\", i), {n_embd_head_k * n_head, n_embd}, 0);\n+\n+                        // bias tensors\n+                        layer.bq = create_tensor(tn(LLM_TENSOR_ATTN_Q,   \"bias\", i), {n_embd_head_k * n_head}, 0);\n+                        layer.bk = create_tensor(tn(LLM_TENSOR_ATTN_K,   \"bias\", i), {n_embd_gqa}, 0);\n+                        layer.bv = create_tensor(tn(LLM_TENSOR_ATTN_V,   \"bias\", i), {n_embd_gqa}, 0);\n+                        layer.bo = create_tensor(tn(LLM_TENSOR_ATTN_OUT, \"bias\", i), {n_embd}, 0);\n+\n+                        layer.ffn_norm = create_tensor(tn(LLM_TENSOR_FFN_NORM, \"weight\", i), {n_embd}, 0);\n+\n+                        if (hparams.rope_scaling_type_train == LLAMA_ROPE_SCALING_TYPE_LONGROPE) {\n+                            layer.rope_long  = create_tensor(tn(LLM_TENSOR_ROPE_FACTORS_LONG,  \"weight\", i), {n_rot/2}, TENSOR_NOT_REQUIRED | (i != 0 ? TENSOR_DUPLICATED : 0));\n+                            layer.rope_short = create_tensor(tn(LLM_TENSOR_ROPE_FACTORS_SHORT, \"weight\", i), {n_rot/2}, TENSOR_NOT_REQUIRED | (i != 0 ? TENSOR_DUPLICATED : 0));\n+                        } else {\n+                            layer.rope_freqs = create_tensor(tn(LLM_TENSOR_ROPE_FREQS, \"weight\", i), {n_rot/2}, TENSOR_NOT_REQUIRED | (i != 0 ? TENSOR_DUPLICATED : 0));\n+                        }\n+\n+                        layer.ffn_gate = create_tensor(tn(LLM_TENSOR_FFN_GATE, \"weight\", i), {n_embd,   n_ff}, 0);\n+                        layer.ffn_down = create_tensor(tn(LLM_TENSOR_FFN_DOWN, \"weight\", i), {  n_ff, n_embd}, 0);\n+                        layer.ffn_up   = create_tensor(tn(LLM_TENSOR_FFN_UP,   \"weight\", i), {n_embd,   n_ff}, 0);\n+                    }\n+                } break;\n             default:\n                 throw std::runtime_error(\"unknown architecture\");\n         }\n@@ -7260,6 +7313,10 @@ ggml_cgraph * llama_model::build_graph(const llm_graph_params & params) const {\n             {\n                 llm = std::make_unique<llm_build_cogvlm>(*this, params);\n             } break;\n+        case LLM_ARCH_PANGU_EMBED:\n+            {\n+                llm = std::make_unique<llm_build_pangu_embedded>(*this, params);\n+            }break;\n         default:\n             GGML_ABORT(\"fatal error\");\n     }\n@@ -7479,6 +7536,7 @@ llama_rope_type llama_model_rope_type(const llama_model * model) {\n         case LLM_ARCH_APERTUS:\n         case LLM_ARCH_MINIMAX_M2:\n         case LLM_ARCH_COGVLM:\n+        case LLM_ARCH_PANGU_EMBED:\n             return LLAMA_ROPE_TYPE_NEOX;\n \n         case LLM_ARCH_QWEN2VL:"
      },
      {
        "filename": "src/models/models.h",
        "status": "modified",
        "additions": 4,
        "deletions": 0,
        "changes": 4,
        "patch": "@@ -361,6 +361,10 @@ struct llm_build_orion : public llm_graph_context {\n     llm_build_orion(const llama_model & model, const llm_graph_params & params);\n };\n \n+struct llm_build_pangu_embedded : public llm_graph_context {\n+    llm_build_pangu_embedded(const llama_model & model, const llm_graph_params & params);\n+};\n+\n struct llm_build_phi2 : public llm_graph_context {\n     llm_build_phi2(const llama_model & model, const llm_graph_params & params);\n };"
      },
      {
        "filename": "src/models/pangu-embedded.cpp",
        "status": "added",
        "additions": 121,
        "deletions": 0,
        "changes": 121,
        "patch": "@@ -0,0 +1,121 @@\n+#include \"models.h\"\n+\n+\n+llm_build_pangu_embedded::llm_build_pangu_embedded(const llama_model & model, const llm_graph_params & params) : llm_graph_context(params) {\n+    const int64_t n_embd_head = hparams.n_embd_head_v;\n+\n+    GGML_ASSERT(n_embd_head == hparams.n_embd_head_k);\n+    GGML_ASSERT(n_embd_head == hparams.n_rot);\n+\n+    ggml_tensor * cur;\n+    ggml_tensor * inpL;\n+\n+    inpL = build_inp_embd(model.tok_embd);\n+\n+    // inp_pos - contains the positions\n+    ggml_tensor * inp_pos = build_inp_pos();\n+\n+    auto * inp_attn = build_attn_inp_kv();\n+\n+    ggml_tensor * inp_out_ids = build_inp_out_ids();\n+\n+    for (int il = 0; il < n_layer; ++il) {\n+        ggml_tensor * inpSA = inpL;\n+\n+        // norm\n+        cur = build_norm(inpL,\n+                model.layers[il].attn_norm, NULL,\n+                LLM_NORM_RMS, il);\n+        cb(cur, \"attn_norm\", il);\n+\n+        // self attention\n+        {\n+            // compute Q and K and RoPE them\n+            ggml_tensor * Qcur = build_lora_mm(model.layers[il].wq, cur);\n+            Qcur = ggml_add(ctx0, Qcur, model.layers[il].bq);\n+            cb(Qcur, \"Qcur\", il);\n+\n+            ggml_tensor * Kcur = build_lora_mm(model.layers[il].wk, cur);\n+            Kcur = ggml_add(ctx0, Kcur, model.layers[il].bk);\n+            cb(Kcur, \"Kcur\", il);\n+\n+            ggml_tensor * Vcur = build_lora_mm(model.layers[il].wv, cur);\n+            Vcur = ggml_add(ctx0, Vcur, model.layers[il].bv);\n+            cb(Vcur, \"Vcur\", il);\n+\n+            Qcur = ggml_reshape_3d(ctx0, Qcur, n_embd_head, n_head,    n_tokens);\n+            Kcur = ggml_reshape_3d(ctx0, Kcur, n_embd_head, n_head_kv, n_tokens);\n+            Vcur = ggml_reshape_3d(ctx0, Vcur, n_embd_head, n_head_kv, n_tokens);\n+\n+            Qcur = ggml_rope_ext(\n+                    ctx0, Qcur, inp_pos, nullptr,\n+                    n_rot, rope_type, n_ctx_orig, freq_base, freq_scale,\n+                    ext_factor, attn_factor, beta_fast, beta_slow\n+                    );\n+\n+            Kcur = ggml_rope_ext(ctx0, Kcur, inp_pos, nullptr,\n+                    n_rot, rope_type, n_ctx_orig, freq_base, freq_scale,\n+                    ext_factor, attn_factor, beta_fast, beta_slow\n+                    );\n+\n+            cb(Qcur, \"Qcur\", il);\n+            cb(Kcur, \"Kcur\", il);\n+            cb(Vcur, \"Vcur\", il);\n+\n+            cur = build_attn(inp_attn,\n+                    model.layers[il].wo, model.layers[il].bo,\n+                    Qcur, Kcur, Vcur, nullptr, nullptr, nullptr, 1.0f/sqrtf(float(n_embd_head)), il);\n+        }\n+\n+        if (il == n_layer - 1 && inp_out_ids) {\n+            cur   = ggml_get_rows(ctx0,   cur, inp_out_ids);\n+            inpSA = ggml_get_rows(ctx0, inpSA, inp_out_ids);\n+        }\n+\n+        ggml_tensor * ffn_inp = ggml_add(ctx0, cur, inpSA);\n+        cb(ffn_inp, \"ffn_inp\", il);\n+\n+        // feed-forward network\n+        cur = build_norm(ffn_inp,\n+                model.layers[il].ffn_norm, NULL,\n+                LLM_NORM_RMS, il);\n+        cb(cur, \"ffn_norm\", il);\n+\n+        cur = build_ffn(cur,\n+                model.layers[il].ffn_up,   model.layers[il].ffn_up_b,   NULL,\n+                model.layers[il].ffn_gate, model.layers[il].ffn_gate_b, NULL,\n+                model.layers[il].ffn_down, model.layers[il].ffn_down_b, NULL,\n+                NULL,\n+                LLM_FFN_SILU, LLM_FFN_PAR, il);\n+\n+        cur = ggml_add(ctx0, cur, ffn_inp);\n+        cb(cur, \"ffn_out\", il);\n+\n+        cur = build_cvec(cur, il);\n+        cb(cur, \"l_out\", il);\n+\n+        // input for next layer\n+        inpL = cur;\n+    }\n+\n+    cur = inpL;\n+\n+    cur = build_norm(cur,\n+            model.output_norm, NULL,\n+            LLM_NORM_RMS, -1);\n+\n+    cb(cur, \"result_norm\", -1);\n+    res->t_embd = cur;\n+\n+    // lm_head\n+    cur = build_lora_mm(model.output, cur);\n+\n+    if (model.output_b != nullptr) {\n+        cur = ggml_add(ctx0, cur, model.output_b);\n+    }\n+\n+    cb(cur, \"result_output\", -1);\n+    res->t_logits = cur;\n+\n+    ggml_build_forward_expand(gf, cur);\n+}"
      }
    ],
    "num_files": 10,
    "scraped_at": "2025-11-16T23:29:04.104501"
  },
  {
    "pr_number": 16928,
    "title": "hparams : add n_embd_inp() to support extended embed",
    "body": "Required for proper handling of `Qwen3-VL` DeepStack embeds.\r\n\r\nMay change more than currently necessary for future use, f.ex. in `llama-context` (or maybe even not enough), please review carefully!\r\n\r\nFixes #16908",
    "html_url": "https://github.com/ggml-org/llama.cpp/pull/16928",
    "created_at": "2025-11-01T22:34:52Z",
    "merged_at": "2025-11-07T18:27:58Z",
    "merge_commit_sha": "9008027aa376526819415469f74fb9281136224e",
    "base_ref": "master",
    "head_sha": "6faa87f33754d6c5d4af6e26df4e21ff6bc29534",
    "user": "CISC",
    "files": [
      {
        "filename": "include/llama.h",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -482,6 +482,7 @@ extern \"C\" {\n \n     LLAMA_API int32_t llama_model_n_ctx_train(const struct llama_model * model);\n     LLAMA_API int32_t llama_model_n_embd     (const struct llama_model * model);\n+    LLAMA_API int32_t llama_model_n_embd_inp (const struct llama_model * model);\n     LLAMA_API int32_t llama_model_n_layer    (const struct llama_model * model);\n     LLAMA_API int32_t llama_model_n_head     (const struct llama_model * model);\n     LLAMA_API int32_t llama_model_n_head_kv  (const struct llama_model * model);"
      },
      {
        "filename": "src/llama-context.cpp",
        "status": "modified",
        "additions": 3,
        "deletions": 3,
        "changes": 6,
        "patch": "@@ -808,7 +808,7 @@ int llama_context::encode(const llama_batch & batch_inp) {\n \n     const auto & hparams = model.hparams;\n \n-    const int64_t n_embd  = hparams.n_embd;\n+    const int64_t n_embd  = hparams.n_embd_inp();\n     const int64_t n_vocab = model.vocab.n_tokens();\n \n     // note: during encode, we always pass the full sequence starting from pos = 0\n@@ -977,7 +977,7 @@ int llama_context::decode(const llama_batch & batch_inp) {\n     const auto & hparams = model.hparams;\n \n     const int64_t n_vocab = vocab.n_tokens();\n-    const int64_t n_embd  = hparams.n_embd;\n+    const int64_t n_embd  = hparams.n_embd_inp();\n \n     // when computing embeddings, all tokens are output\n     const bool output_all = cparams.embeddings;\n@@ -2135,7 +2135,7 @@ void llama_context::opt_epoch_iter(\n             batch.logits  [pos_batch]    = true;\n         }\n \n-        if (!balloc->init(batch, model.vocab, nullptr, model.hparams.n_embd, cparams.kv_unified ? LLAMA_MAX_SEQ : cparams.n_seq_max, true)) {\n+        if (!balloc->init(batch, model.vocab, nullptr, model.hparams.n_embd_inp(), cparams.kv_unified ? LLAMA_MAX_SEQ : cparams.n_seq_max, true)) {\n             LLAMA_LOG_ERROR(\"%s: failed to initialize batch\\n\", __func__);\n             return;\n         }"
      },
      {
        "filename": "src/llama-graph.cpp",
        "status": "modified",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "patch": "@@ -1142,7 +1142,7 @@ ggml_tensor * llm_graph_context::build_moe_ffn(\n \n // input embeddings with optional lora\n ggml_tensor * llm_graph_context::build_inp_embd(ggml_tensor * tok_embd) const {\n-    const int64_t n_embd = hparams.n_embd;\n+    const int64_t n_embd = hparams.n_embd_inp();\n \n     auto inp = std::make_unique<llm_graph_input_embd>();\n \n@@ -1279,7 +1279,7 @@ ggml_tensor * llm_graph_context::build_inp_cross_embd() const {\n     //    return cur;\n     //}\n \n-    const auto n_embd = !cross->v_embd.empty() ? cross->n_embd : hparams.n_embd;\n+    const auto n_embd = !cross->v_embd.empty() ? cross->n_embd : hparams.n_embd_inp();\n     const auto n_enc  = !cross->v_embd.empty() ? cross->n_enc : hparams.n_ctx_train;\n \n     cur = ggml_new_tensor_2d(ctx0, GGML_TYPE_F32, n_embd, n_enc);"
      },
      {
        "filename": "src/llama-hparams.cpp",
        "status": "modified",
        "additions": 10,
        "deletions": 0,
        "changes": 10,
        "patch": "@@ -60,6 +60,16 @@ uint32_t llama_hparams::n_gqa(uint32_t il) const {\n     return n_head/n_head_kv;\n }\n \n+uint32_t llama_hparams::n_embd_inp() const {\n+    uint32_t n_embd_inp = n_embd;\n+\n+    if (n_deepstack_layers > 0) {\n+        n_embd_inp += n_embd * n_deepstack_layers;\n+    }\n+\n+    return n_embd_inp;\n+}\n+\n uint32_t llama_hparams::n_embd_k_gqa(uint32_t il) const {\n     const uint32_t n_head_kv = this->n_head_kv(il);\n "
      },
      {
        "filename": "src/llama-hparams.h",
        "status": "modified",
        "additions": 3,
        "deletions": 0,
        "changes": 3,
        "patch": "@@ -227,6 +227,9 @@ struct llama_hparams {\n \n     uint32_t n_gqa(uint32_t il = 0) const;\n \n+    // dimension of main + auxiliary input embeddings\n+    uint32_t n_embd_inp() const;\n+\n     // dimension of key embeddings across all k-v heads\n     uint32_t n_embd_k_gqa(uint32_t il = 0) const;\n "
      },
      {
        "filename": "src/llama-model.cpp",
        "status": "modified",
        "additions": 7,
        "deletions": 16,
        "changes": 23,
        "patch": "@@ -276,8 +276,8 @@ static bool weight_buft_supported(const llama_hparams & hparams, ggml_tensor * w\n             } break;\n         case GGML_OP_IM2COL:\n             {\n-                const int n_embd = hparams.n_embd;\n-                ggml_tensor * b = ggml_new_tensor_4d(ctx, GGML_TYPE_F32, n_embd, w->ne[1], 1, 1);\n+                const int n_embd_inp = hparams.n_embd_inp();\n+                ggml_tensor * b = ggml_new_tensor_4d(ctx, GGML_TYPE_F32, n_embd_inp, w->ne[1], 1, 1);\n                 op_tensor = ggml_im2col(ctx, w, b, 1, 0, 0, 0, 1, 0, false, GGML_TYPE_F16);\n             } break;\n         case GGML_OP_SCALE:\n@@ -1039,9 +1039,6 @@ void llama_model::load_hparams(llama_model_loader & ml) {\n                     case 64: type = LLM_TYPE_32B; break;\n                     default: type = LLM_TYPE_UNKNOWN;\n                 }\n-                // since vision model stacks deepstack features along feature dim\n-                // we also create a fake \"n_embd\" for text model to be the main embd + deepstack embds\n-                hparams.n_embd *= hparams.n_deepstack_layers + 1;\n             } break;\n         case LLM_ARCH_QWEN3MOE:\n             {\n@@ -1065,9 +1062,6 @@ void llama_model::load_hparams(llama_model_loader & ml) {\n                     case 94: type = LLM_TYPE_235B_A22B; break;\n                     default: type = LLM_TYPE_UNKNOWN;\n                 }\n-                // since vision model stacks deepstack features along feature dim\n-                // we also create a fake \"n_embd\" for text model to be the main embd + deepstack embds\n-                hparams.n_embd *= hparams.n_deepstack_layers + 1;\n             } break;\n         case LLM_ARCH_PHI2:\n             {\n@@ -3332,10 +3326,6 @@ bool llama_model::load_tensors(llama_model_loader & ml) {\n             case LLM_ARCH_QWEN3:\n             case LLM_ARCH_QWEN3VL:\n                 {\n-                    // for model loading, the weights only have the main embd\n-                    // so we need to divide by the number of deepstack layers + 1\n-                    // n_embd is const int so we declare a new variable\n-                    int64_t n_embd = hparams.n_embd / (hparams.n_deepstack_layers + 1);\n                     tok_embd = create_tensor(tn(LLM_TENSOR_TOKEN_EMBD, \"weight\"), {n_embd, n_vocab}, 0);\n \n                     // output\n@@ -3371,10 +3361,6 @@ bool llama_model::load_tensors(llama_model_loader & ml) {\n             case LLM_ARCH_QWEN3MOE:\n             case LLM_ARCH_QWEN3VLMOE:\n                 {\n-                    // for model loading, the weights only have the main embd\n-                    // so we need to divide by the number of deepstack layers + 1\n-                    // n_embd is const int so we declare a new variable\n-                    int64_t n_embd = hparams.n_embd / (hparams.n_deepstack_layers + 1);\n                     tok_embd = create_tensor(tn(LLM_TENSOR_TOKEN_EMBD, \"weight\"), {n_embd, n_vocab}, 0);\n \n                     // output\n@@ -6482,6 +6468,7 @@ void llama_model::print_info() const {\n     if (!hparams.vocab_only) {\n         LLAMA_LOG_INFO(\"%s: n_ctx_train      = %u\\n\",     __func__, hparams.n_ctx_train);\n         LLAMA_LOG_INFO(\"%s: n_embd           = %u\\n\",     __func__, hparams.n_embd);\n+        LLAMA_LOG_INFO(\"%s: n_embd_inp       = %u\\n\",     __func__, hparams.n_embd_inp());\n         LLAMA_LOG_INFO(\"%s: n_layer          = %u\\n\",     __func__, hparams.n_layer);\n         LLAMA_LOG_INFO(\"%s: n_head           = %s\\n\",     __func__, print_f([&](uint32_t il) { return hparams.n_head(il);    }, hparams.n_layer).c_str());\n         LLAMA_LOG_INFO(\"%s: n_head_kv        = %s\\n\",     __func__, print_f([&](uint32_t il) { return hparams.n_head_kv(il); }, hparams.n_layer).c_str());\n@@ -7329,6 +7316,10 @@ int32_t llama_model_n_embd(const llama_model * model) {\n     return model->hparams.n_embd;\n }\n \n+int32_t llama_model_n_embd_inp(const llama_model * model) {\n+    return model->hparams.n_embd_inp();\n+}\n+\n int32_t llama_model_n_layer(const llama_model * model) {\n     return model->hparams.n_layer;\n }"
      },
      {
        "filename": "src/models/qwen3vl-moe.cpp",
        "status": "modified",
        "additions": 1,
        "deletions": 2,
        "changes": 3,
        "patch": "@@ -1,9 +1,8 @@\n #include \"models.h\"\n \n llm_build_qwen3vlmoe::llm_build_qwen3vlmoe(const llama_model & model, const llm_graph_params & params) : llm_graph_context(params) {\n-    const int64_t n_embd_full = hparams.n_embd; // main embd + deepstack embds\n     const size_t n_deepstack_layers = hparams.n_deepstack_layers;\n-    const int64_t n_embd = n_embd_full / (n_deepstack_layers + 1);\n+    const int64_t n_embd = hparams.n_embd;\n     const int64_t n_embd_head = hparams.n_embd_head_v;\n \n     GGML_ASSERT(n_embd_head == hparams.n_embd_head_k);"
      },
      {
        "filename": "src/models/qwen3vl.cpp",
        "status": "modified",
        "additions": 1,
        "deletions": 4,
        "changes": 5,
        "patch": "@@ -1,13 +1,10 @@\n #include \"models.h\"\n \n llm_build_qwen3vl::llm_build_qwen3vl(const llama_model & model, const llm_graph_params & params) : llm_graph_context(params) {\n-\n-    const int64_t n_embd_full = hparams.n_embd; // main embd + deepstack embds\n     const size_t n_deepstack_layers = hparams.n_deepstack_layers;\n-    const int64_t n_embd = n_embd_full / (n_deepstack_layers + 1);\n+    const int64_t n_embd = hparams.n_embd;\n     const int64_t n_embd_head = hparams.n_embd_head_v;\n \n-\n     GGML_ASSERT(n_embd_head == hparams.n_embd_head_k);\n     GGML_ASSERT(n_embd_head == hparams.n_rot);\n "
      },
      {
        "filename": "tools/mtmd/mtmd.cpp",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -151,7 +151,7 @@ struct mtmd_context {\n         print_timings(ctx_params.print_timings),\n         n_threads    (ctx_params.n_threads),\n         media_marker (ctx_params.media_marker),\n-        n_embd_text  (llama_model_n_embd(text_model))\n+        n_embd_text  (llama_model_n_embd_inp(text_model))\n     {\n         if (std::string(ctx_params.image_marker) != MTMD_DEFAULT_IMAGE_MARKER) {\n             throw std::runtime_error(\"custom image_marker is not supported anymore, use media_marker instead\");"
      }
    ],
    "num_files": 9,
    "scraped_at": "2025-11-16T23:29:05.882841"
  },
  {
    "pr_number": 16921,
    "title": "mtmd: add --image-min/max-tokens",
    "body": "Ref: https://github.com/ggml-org/llama.cpp/issues/16842#issuecomment-3476377710\r\n\r\nChanges in mtmd API:\r\n- Adds `image_min_tokens` and `image_max_tokens` to `mtmd_context_params`\r\n\r\nChanges in `llama-mtmd-cli` and `llama-server`:\r\n- Adds `--image-min-tokens N` and `--image-max-tokens N` arguments",
    "html_url": "https://github.com/ggml-org/llama.cpp/pull/16921",
    "created_at": "2025-11-01T18:50:15Z",
    "merged_at": "2025-11-03T10:11:18Z",
    "merge_commit_sha": "070ff4d5356083d60b807bb34d36b31c3653a29e",
    "base_ref": "master",
    "head_sha": "79b98dbf969e757d339847f9aad1c5e6327667b4",
    "user": "ngxson",
    "files": [
      {
        "filename": "common/arg.cpp",
        "status": "modified",
        "additions": 14,
        "deletions": 0,
        "changes": 14,
        "patch": "@@ -2768,6 +2768,20 @@ common_params_context common_params_parser_init(common_params & params, llama_ex\n             params.image.emplace_back(value);\n         }\n     ).set_examples({LLAMA_EXAMPLE_MTMD}));\n+    add_opt(common_arg(\n+        {\"--image-min-tokens\"}, \"N\",\n+        \"minimum number of tokens each image can take, only used by vision models with dynamic resolution (default: read from model)\",\n+        [](common_params & params, int value) {\n+            params.image_min_tokens = value;\n+        }\n+    ).set_examples(mmproj_examples).set_env(\"LLAMA_ARG_IMAGE_MIN_TOKENS\"));\n+    add_opt(common_arg(\n+        {\"--image-max-tokens\"}, \"N\",\n+        \"maximum number of tokens each image can take, only used by vision models with dynamic resolution (default: read from model)\",\n+        [](common_params & params, int value) {\n+            params.image_max_tokens = value;\n+        }\n+    ).set_examples(mmproj_examples).set_env(\"LLAMA_ARG_IMAGE_MAX_TOKENS\"));\n     if (llama_supports_rpc()) {\n         add_opt(common_arg(\n             {\"--rpc\"}, \"SERVERS\","
      },
      {
        "filename": "common/common.h",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -406,6 +406,8 @@ struct common_params {\n     bool mmproj_use_gpu = true;     // use GPU for multimodal model\n     bool no_mmproj = false;         // explicitly disable multimodal model\n     std::vector<std::string> image; // path to image file(s)\n+    int image_min_tokens = -1;\n+    int image_max_tokens = -1;\n \n     // finetune\n     struct lr_opt lr;"
      },
      {
        "filename": "tools/mtmd/clip.cpp",
        "status": "modified",
        "additions": 34,
        "deletions": 9,
        "changes": 43,
        "patch": "@@ -169,8 +169,8 @@ struct clip_hparams {\n     int32_t n_layer;\n     // idefics3\n     int32_t image_longest_edge = 0;\n-    int32_t image_min_pixels = 0;\n-    int32_t image_max_pixels = 0;\n+    int32_t image_min_pixels = -1;\n+    int32_t image_max_pixels = -1;\n     int32_t n_merge = 0; // number of patch merges **per-side**\n \n     float image_mean[3];\n@@ -203,11 +203,15 @@ struct clip_hparams {\n     int minicpmv_version = 0;\n     int32_t minicpmv_query_num = 0;         // MiniCPM-V query number\n \n+    // custom value provided by user, can be undefined if not set\n+    int32_t custom_image_min_tokens = -1;\n+    int32_t custom_image_max_tokens = -1;\n+\n     void set_limit_image_tokens(int n_tokens_min, int n_tokens_max) {\n         const int cur_merge = n_merge == 0 ? 1 : n_merge;\n         const int patch_area = patch_size * patch_size * cur_merge * cur_merge;\n-        image_min_pixels = n_tokens_min * patch_area;\n-        image_max_pixels = n_tokens_max * patch_area;\n+        image_min_pixels = (custom_image_min_tokens > 0 ? custom_image_min_tokens : n_tokens_min) * patch_area;\n+        image_max_pixels = (custom_image_max_tokens > 0 ? custom_image_max_tokens : n_tokens_max) * patch_area;\n         warmup_image_size = static_cast<int>(std::sqrt(image_max_pixels));\n     }\n \n@@ -216,6 +220,7 @@ struct clip_hparams {\n         GGML_ASSERT(n_tok_per_side * n_tok_per_side == n_tokens && \"n_tokens must be n*n\");\n         const int cur_merge = n_merge == 0 ? 1 : n_merge;\n         warmup_image_size = n_tok_per_side * patch_size * cur_merge;\n+        // TODO: support warmup size for custom token numbers\n     }\n };\n \n@@ -459,6 +464,13 @@ struct clip_ctx {\n             LOG_INF(\"%s: CLIP using CPU backend\\n\", __func__);\n         }\n \n+        if (ctx_params.image_min_tokens > 0) {\n+            model.hparams.custom_image_min_tokens = ctx_params.image_min_tokens;\n+        }\n+        if (ctx_params.image_max_tokens > 0) {\n+            model.hparams.custom_image_max_tokens = ctx_params.image_max_tokens;\n+        }\n+\n         backend_ptrs.push_back(backend_cpu);\n         backend_buft.push_back(ggml_backend_get_default_buffer_type(backend_cpu));\n \n@@ -2777,6 +2789,12 @@ struct clip_model_loader {\n                         //           see: https://github.com/ggml-org/llama.cpp/issues/16842#issuecomment-3475144858\n                         hparams.set_limit_image_tokens(8, 2048);\n                         hparams.set_warmup_n_tokens(256); // avoid OOM on warmup\n+                        const int warn_min_pixels = 1024 * hparams.n_merge * hparams.n_merge * hparams.patch_size * hparams.patch_size;\n+                        if (hparams.image_min_pixels < warn_min_pixels) {\n+                            LOG_WRN(\"%s: Qwen-VL models require at minimum 1024 image tokens to function correctly on grounding tasks\\n\", __func__);\n+                            LOG_WRN(\"%s: if you encounter problems with accuracy, try adding --image-min-tokens 1024\\n\", __func__);\n+                            LOG_WRN(\"%s: more info: https://github.com/ggml-org/llama.cpp/issues/16842\\n\\n\", __func__);\n+                        }\n                     } break;\n                 case PROJECTOR_TYPE_LLAMA4:\n                     {\n@@ -2801,6 +2819,13 @@ struct clip_model_loader {\n                     break;\n             }\n \n+            // sanity check\n+            {\n+                if (hparams.image_max_pixels < hparams.image_min_pixels) {\n+                    throw std::runtime_error(string_format(\"%s: image_max_pixels (%d) is less than image_min_pixels (%d)\\n\", __func__, hparams.image_max_pixels, hparams.image_min_pixels));\n+                }\n+            }\n+\n             LOG_INF(\"%s: projector:          %s\\n\", __func__, proj_type.c_str());\n             LOG_INF(\"%s: n_embd:             %d\\n\", __func__, hparams.n_embd);\n             LOG_INF(\"%s: n_head:             %d\\n\", __func__, hparams.n_head);\n@@ -2817,10 +2842,10 @@ struct clip_model_loader {\n                 LOG_INF(\"%s: n_merge:            %d\\n\", __func__, hparams.n_merge);\n                 LOG_INF(\"%s: n_wa_pattern:       %d\\n\", __func__, hparams.n_wa_pattern);\n                 if (hparams.image_min_pixels > 0) {\n-                    LOG_INF(\"%s: image_min_pixels:   %d\\n\", __func__, hparams.image_min_pixels);\n+                    LOG_INF(\"%s: image_min_pixels:   %d%s\\n\", __func__, hparams.image_min_pixels, hparams.custom_image_min_tokens > 0 ? \" (custom value)\" : \"\");\n                 }\n                 if (hparams.image_max_pixels > 0) {\n-                    LOG_INF(\"%s: image_max_pixels:   %d\\n\", __func__, hparams.image_max_pixels);\n+                    LOG_INF(\"%s: image_max_pixels:   %d%s\\n\", __func__, hparams.image_max_pixels, hparams.custom_image_max_tokens > 0 ? \" (custom value)\" : \"\");\n                 }\n             } else if (is_audio) {\n                 LOG_INF(\"\\n--- audio hparams ---\\n\");\n@@ -4160,7 +4185,7 @@ bool clip_image_preprocess(struct clip_ctx * ctx, const clip_image_u8 * img, str\n         case PROJECTOR_TYPE_QWEN25VL:\n         case PROJECTOR_TYPE_QWEN3VL:\n             {\n-                // step 1: make a blank canvas which aligns to the grid\n+                GGML_ASSERT(params.image_min_pixels > 0 && params.image_max_pixels > 0);\n                 clip_image_u8 resized;\n                 const clip_image_size new_size = img_tool::calc_size_preserved_ratio(\n                     original_size,\n@@ -4253,7 +4278,7 @@ bool clip_image_preprocess(struct clip_ctx * ctx, const clip_image_u8 * img, str\n         case PROJECTOR_TYPE_PIXTRAL:\n         case PROJECTOR_TYPE_LIGHTONOCR:\n             {\n-                GGML_ASSERT(params.image_min_pixels && params.image_max_pixels);\n+                GGML_ASSERT(params.image_min_pixels > 0 && params.image_max_pixels > 0);\n                 clip_image_u8 resized_image;\n                 // the original pixtral model doesn't have n_merge\n                 const int cur_merge = params.n_merge == 0 ? 1 : params.n_merge;\n@@ -4287,7 +4312,7 @@ bool clip_image_preprocess(struct clip_ctx * ctx, const clip_image_u8 * img, str\n         case PROJECTOR_TYPE_LFM2:\n         case PROJECTOR_TYPE_KIMIVL:\n             {\n-                GGML_ASSERT(params.image_min_pixels && params.image_max_pixels);\n+                GGML_ASSERT(params.image_min_pixels > 0 && params.image_max_pixels > 0);\n                 const clip_image_size target_size = img_tool::calc_size_preserved_ratio(\n                     original_size,\n                     params.patch_size * params.n_merge,"
      },
      {
        "filename": "tools/mtmd/clip.h",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -33,6 +33,8 @@ struct clip_context_params {\n     bool use_gpu;\n     enum ggml_log_level verbosity;\n     enum clip_flash_attn_type flash_attn_type;\n+    int image_min_tokens;\n+    int image_max_tokens;\n };\n \n struct clip_init_result {"
      },
      {
        "filename": "tools/mtmd/mtmd-cli.cpp",
        "status": "modified",
        "additions": 7,
        "deletions": 5,
        "changes": 12,
        "patch": "@@ -132,11 +132,13 @@ struct mtmd_cli_context {\n     void init_vision_context(common_params & params) {\n         const char * clip_path = params.mmproj.path.c_str();\n         mtmd_context_params mparams = mtmd_context_params_default();\n-        mparams.use_gpu = params.mmproj_use_gpu;\n-        mparams.print_timings = true;\n-        mparams.n_threads = params.cpuparams.n_threads;\n-        mparams.verbosity = params.verbosity > 0 ? GGML_LOG_LEVEL_DEBUG : GGML_LOG_LEVEL_INFO;\n-        mparams.flash_attn_type = params.flash_attn_type;\n+        mparams.use_gpu          = params.mmproj_use_gpu;\n+        mparams.print_timings    = true;\n+        mparams.n_threads        = params.cpuparams.n_threads;\n+        mparams.verbosity        = params.verbosity > 0 ? GGML_LOG_LEVEL_DEBUG : GGML_LOG_LEVEL_INFO;\n+        mparams.flash_attn_type  = params.flash_attn_type;\n+        mparams.image_min_tokens = params.image_min_tokens;\n+        mparams.image_max_tokens = params.image_max_tokens;\n         ctx_vision.reset(mtmd_init_from_file(clip_path, model, mparams));\n         if (!ctx_vision.get()) {\n             LOG_ERR(\"Failed to load vision model from %s\\n\", clip_path);"
      },
      {
        "filename": "tools/mtmd/mtmd.cpp",
        "status": "modified",
        "additions": 9,
        "deletions": 3,
        "changes": 12,
        "patch": "@@ -109,6 +109,8 @@ mtmd_context_params mtmd_context_params_default() {\n     params.image_marker = MTMD_DEFAULT_IMAGE_MARKER;\n     params.media_marker = mtmd_default_marker();\n     params.flash_attn_type = LLAMA_FLASH_ATTN_TYPE_AUTO;\n+    params.image_min_tokens = -1;\n+    params.image_max_tokens = -1;\n     return params;\n }\n \n@@ -171,9 +173,13 @@ struct mtmd_context {\n         }\n \n         clip_context_params ctx_clip_params;\n-        ctx_clip_params.use_gpu   = ctx_params.use_gpu;\n-        ctx_clip_params.verbosity = ctx_params.verbosity;\n-        ctx_clip_params.flash_attn_type = mtmd_get_clip_flash_attn_type(ctx_params.flash_attn_type);\n+        ctx_clip_params.use_gpu          = ctx_params.use_gpu;\n+        ctx_clip_params.verbosity        = ctx_params.verbosity;\n+        ctx_clip_params.flash_attn_type  = mtmd_get_clip_flash_attn_type(ctx_params.flash_attn_type);\n+        // custom image token limits\n+        ctx_clip_params.image_min_tokens = ctx_params.image_min_tokens;\n+        ctx_clip_params.image_max_tokens = ctx_params.image_max_tokens;\n+\n         auto res = clip_init(mmproj_fname, ctx_clip_params);\n         ctx_v = res.ctx_v;\n         ctx_a = res.ctx_a;"
      },
      {
        "filename": "tools/mtmd/mtmd.h",
        "status": "modified",
        "additions": 4,
        "deletions": 0,
        "changes": 4,
        "patch": "@@ -83,6 +83,10 @@ struct mtmd_context_params {\n     const char * image_marker; // deprecated, use media_marker instead\n     const char * media_marker;\n     enum llama_flash_attn_type flash_attn_type;\n+\n+    // limit number of image tokens, only for vision models with dynamic resolution\n+    int image_min_tokens; // minimum number of tokens for image input (default: read from metadata)\n+    int image_max_tokens; // maximum number of tokens for image input (default: read from metadata)\n };\n \n MTMD_API const char * mtmd_default_marker(void);"
      },
      {
        "filename": "tools/server/server.cpp",
        "status": "modified",
        "additions": 7,
        "deletions": 5,
        "changes": 12,
        "patch": "@@ -2452,11 +2452,13 @@ struct server_context {\n         std::string & mmproj_path = params_base.mmproj.path;\n         if (!mmproj_path.empty()) {\n             mtmd_context_params mparams = mtmd_context_params_default();\n-            mparams.use_gpu       = params_base.mmproj_use_gpu;\n-            mparams.print_timings = false;\n-            mparams.n_threads     = params_base.cpuparams.n_threads;\n-            mparams.verbosity     = params_base.verbosity > 0 ? GGML_LOG_LEVEL_DEBUG : GGML_LOG_LEVEL_INFO;\n-            mparams.flash_attn_type = params_base.flash_attn_type;\n+            mparams.use_gpu          = params_base.mmproj_use_gpu;\n+            mparams.print_timings    = false;\n+            mparams.n_threads        = params_base.cpuparams.n_threads;\n+            mparams.verbosity        = params_base.verbosity > 0 ? GGML_LOG_LEVEL_DEBUG : GGML_LOG_LEVEL_INFO;\n+            mparams.flash_attn_type  = params_base.flash_attn_type;\n+            mparams.image_min_tokens = params_base.image_min_tokens;\n+            mparams.image_max_tokens = params_base.image_max_tokens;\n             mctx = mtmd_init_from_file(mmproj_path.c_str(), model, mparams);\n             if (mctx == nullptr) {\n                 SRV_ERR(\"failed to load multimodal model, '%s'\\n\", mmproj_path.c_str());"
      }
    ],
    "num_files": 8,
    "scraped_at": "2025-11-16T23:29:07.289039"
  },
  {
    "pr_number": 16906,
    "title": "model: add Janus Pro for image understanding",
    "body": "This pull request introduces support for the Janus\u2011Pro 1B and Janus\u2011Pro 7B models within the llama.cpp framework. \r\n\r\nThe focus of this update is on image understanding (i.e., visual-input \u2192 textual or conceptual output). \r\nImage generation is not covered by this PR.\r\n\r\n## Usage & Current Progress\r\n**Convert models to GGUF files**:\r\n```\r\n# Convert the base Janus-Pro 1B model\r\npython convert_hf_to_gguf.py deepseek-community/Janus-Pro-1B \\\r\n    --remote \\\r\n    --outfile janus-pro-1b-f16.gguf \\\r\n    --outtype f16\r\n\r\n# Convert the mmproj component\r\npython convert_hf_to_gguf.py deepseek-community/Janus-Pro-1B \\\r\n    --remote \\\r\n    --outfile mmproj-janus-pro-1b-f16.gguf \\\r\n    --outtype f16 \\\r\n    --mmproj\r\n```\r\n\r\n**The converted GGUF files can be accessed here**: https://huggingface.co/Ericwang/Janus-Pro-1B-GGUF\r\n\r\n**Run the model**:\r\n```\r\n# Build the project:\r\ncmake -B build\r\ncmake --build build --target llama-mtmd-cli\r\n\r\n./build/bin/llama-mtmd-cli \\\r\n    -m janus-pro-1b-f16.gguf \\\r\n    --mmproj mmproj-janus-pro-1b-f16.gguf \\\r\n    --chat-template deepseek\r\n```\r\n\r\n## References\r\nJanus-Pro 1B model card:\r\nhttps://huggingface.co/deepseek-community/Janus-Pro-1B\r\n\r\nJanus-Pro 7B model card:\r\nhttps://huggingface.co/deepseek-community/Janus-Pro-7B\r\n\r\nConfigurations:\r\nhttps://huggingface.co/deepseek-community/Janus-Pro-1B/blob/main/config.json\r\nhttps://huggingface.co/deepseek-community/Janus-Pro-7B/blob/main/config.json\r\n\r\nHF Implementation:\r\nhttps://github.com/huggingface/transformers/tree/main/src/transformers/models/janus",
    "html_url": "https://github.com/ggml-org/llama.cpp/pull/16906",
    "created_at": "2025-10-31T23:24:02Z",
    "merged_at": "2025-11-02T21:08:05Z",
    "merge_commit_sha": "6b9a52422bac0f50dd8f1f8386744fa3ce9783bf",
    "base_ref": "master",
    "head_sha": "63f7cf31546375083815db1e3f53e90bff67d7ad",
    "user": "ravenouse",
    "files": [
      {
        "filename": "convert_hf_to_gguf.py",
        "status": "modified",
        "additions": 107,
        "deletions": 0,
        "changes": 107,
        "patch": "@@ -9802,6 +9802,113 @@ def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iter\n \n         return [(self.map_tensor_name(name), data_torch)]\n \n+\n+@ModelBase.register(\"JanusForConditionalGeneration\")\n+class JanusProModel(LlamaModel):\n+    model_arch = gguf.MODEL_ARCH.LLAMA  # reuse Llama arch\n+\n+    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n+        # Skip vision, aligner, and generation tensors\n+        skip_prefixes = (\n+            'model.vision_model.',\n+            'model.aligner.',\n+            'model.vqmodel.',\n+            'model.generation_embeddings.',\n+            'model.generation_aligner.',\n+            'model.generation_head.',\n+        )\n+        if name.startswith(skip_prefixes):\n+            return []\n+\n+        if name.startswith('model.language_model.'):\n+            name = name.replace('model.language_model.', 'model.')\n+        elif name.startswith('language_model.'):\n+            name = name.replace('language_model.', '')\n+\n+        return super().modify_tensors(data_torch, name, bid)\n+\n+\n+@ModelBase.register(\"JanusForConditionalGeneration\")\n+class JanusProVisionModel(MmprojModel):\n+    def __init__(self, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        assert self.hparams_vision is not None\n+        if \"intermediate_size\" not in self.hparams_vision:\n+            mlp_ratio = self.hparams_vision.get(\"mlp_ratio\")\n+            hidden_size = self.hparams_vision.get(\"hidden_size\")\n+            if mlp_ratio is not None and hidden_size is not None:\n+                self.hparams_vision[\"intermediate_size\"] = int(round(hidden_size * mlp_ratio))\n+\n+    def set_gguf_parameters(self):\n+        super().set_gguf_parameters()\n+        assert self.hparams_vision is not None\n+\n+        self.gguf_writer.add_clip_projector_type(gguf.VisionProjectorType.JANUS_PRO)\n+\n+        self.gguf_writer.add_vision_attention_layernorm_eps(self.hparams_vision.get(\"layer_norm_eps\", 1e-6))\n+\n+        hidden_act = str(self.hparams_vision.get(\"hidden_act\", \"\")).lower()\n+        if hidden_act == \"gelu\":\n+            self.gguf_writer.add_vision_use_gelu(True)\n+        elif hidden_act == \"silu\":\n+            self.gguf_writer.add_vision_use_silu(True)\n+\n+    def _map_aligner_tensor(self, data_torch: Tensor, name: str) -> Iterable[tuple[str, Tensor]]:\n+        \"\"\"Map aligner tensors to projector format\"\"\"\n+        suffix = \".bias\" if name.endswith(\".bias\") else \".weight\"\n+\n+        if name.startswith(\"model.aligner.\"):\n+            local_name = name[len(\"model.aligner.\"):]\n+        elif name.startswith(\"aligner.\"):\n+            local_name = name[len(\"aligner.\"):]\n+        else:\n+            raise ValueError(f\"Unsupported Janus aligner prefix: {name}\")\n+\n+        if local_name.startswith(\"fc1.\"):\n+            mm_index = 0\n+        elif local_name.startswith(\"hidden_layers.\"):\n+            parts = local_name.split(\".\", 2)\n+            if len(parts) < 3:\n+                raise ValueError(f\"Unexpected Janus aligner tensor name: {name}\")\n+            mm_index = int(parts[1]) + 1\n+        else:\n+            raise ValueError(f\"Unsupported Janus aligner tensor: {name}\")\n+\n+        tensor_name = self.format_tensor_name(gguf.MODEL_TENSOR.V_MMPROJ, mm_index, suffix=suffix)\n+        return [(tensor_name, data_torch)]\n+\n+    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n+        del bid  # unused\n+\n+        # Skip language model tensors as they will be handled by `JanusProModel`\n+        if name.startswith(('model.language_model.', 'language_model.')):\n+            return []\n+\n+        # Skip generation-related components\n+        skip_generation_prefixes = (\n+            'model.vqmodel.',\n+            'vqmodel.',\n+            'model.generation_embeddings.',\n+            'generation_embeddings.',\n+            'model.generation_aligner.',\n+            'generation_aligner.',\n+            'model.generation_head.',\n+            'generation_head.',\n+        )\n+        if name.startswith(skip_generation_prefixes):\n+            return []\n+\n+        # Handle aligner tensors\n+        if name.startswith(('model.aligner.', 'aligner.')):\n+            return list(self._map_aligner_tensor(data_torch, name))\n+\n+        # Handle vision tensors\n+        if name.startswith(('model.vision_model.', 'vision_model.')):\n+            return [(self.map_tensor_name(name), data_torch)]\n+\n+        return []\n+\n+\n ###### CONVERSION LOGIC ######\n \n "
      },
      {
        "filename": "gguf-py/gguf/constants.py",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -3186,6 +3186,7 @@ class VisionProjectorType:\n     KIMIVL = \"kimivl\"\n     LIGHTONOCR = \"lightonocr\"\n     COGVLM = \"cogvlm\"\n+    JANUS_PRO = \"janus_pro\"\n \n \n # Items here are (block size, type size)"
      },
      {
        "filename": "gguf-py/gguf/tensor_mapping.py",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -1183,6 +1183,7 @@ class TensorNameMap:\n             \"model.mm_projector.mlp.mlp.{bid}\",\n             \"vision_model.vision_adapter.mlp.fc{bid}\", # llama 4\n             \"mlp1.{bid}\", # InternVL\n+            \"model.aligner.fc1.hidden_layers.{bid}\", # Janus Pro\n         ),\n \n         MODEL_TENSOR.V_MMPROJ_PEG: (\n@@ -1291,6 +1292,7 @@ class TensorNameMap:\n             \"model.vision_tower.encoder.layer.{bid}.attention.projection_layer\", # Intern-S1\n             \"vpm.encoder.layers.{bid}.self_attn.out_proj\",\n             \"model.vision_model.encoder.layers.{bid}.self_attn.out_proj\", # SmolVLM\n+            \"model.vision_model.encoder.layers.{bid}.self_attn.projection_layer\", # Janus Pro\n             \"vision_model.model.layers.{bid}.self_attn.o_proj\", # llama4\n             \"vision_tower.transformer.layers.{bid}.attention.o_proj\", # pixtral-hf\n             \"vision_encoder.transformer.layers.{bid}.attention.wo\", # pixtral"
      },
      {
        "filename": "tools/mtmd/clip-impl.h",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -155,6 +155,7 @@ enum projector_type {\n     PROJECTOR_TYPE_KIMIVL,\n     PROJECTOR_TYPE_LIGHTONOCR,\n     PROJECTOR_TYPE_COGVLM,\n+    PROJECTOR_TYPE_JANUS_PRO,\n     PROJECTOR_TYPE_UNKNOWN,\n };\n \n@@ -180,6 +181,7 @@ static std::map<projector_type, std::string> PROJECTOR_TYPE_NAMES = {\n     { PROJECTOR_TYPE_KIMIVL,    \"kimivl\"},\n     { PROJECTOR_TYPE_LIGHTONOCR,\"lightonocr\"},\n     { PROJECTOR_TYPE_COGVLM,    \"cogvlm\"},\n+    { PROJECTOR_TYPE_JANUS_PRO, \"janus_pro\"},\n };\n \n static projector_type clip_projector_type_from_string(const std::string & str) {"
      },
      {
        "filename": "tools/mtmd/clip.cpp",
        "status": "modified",
        "additions": 35,
        "deletions": 1,
        "changes": 36,
        "patch": "@@ -589,6 +589,15 @@ struct clip_graph {\n             cur = ggml_gelu(ctx0, cur);\n             cur = ggml_mul_mat(ctx0, model.mm_2_w, cur);\n             cur = ggml_add(ctx0, cur, model.mm_2_b);\n+\n+        } else if (ctx->proj_type() == PROJECTOR_TYPE_JANUS_PRO) {\n+            cur = build_ffn(cur,\n+                model.mm_0_w, model.mm_0_b,\n+                nullptr, nullptr,\n+                model.mm_1_w, model.mm_1_b,\n+                hparams.ffn_op,\n+                -1);\n+\n         } else {\n             GGML_ABORT(\"SigLIP: Unsupported projector type\");\n         }\n@@ -1730,7 +1739,6 @@ struct clip_graph {\n \n         return gf;\n     }\n-\n     // whisper encoder with custom projector\n     ggml_cgraph * build_whisper_enc() {\n         const int n_frames = img.nx;\n@@ -2450,6 +2458,10 @@ static ggml_cgraph * clip_image_build_graph(clip_ctx * ctx, const clip_image_f32\n             {\n                 res = graph.build_kimivl();\n             } break;\n+        case PROJECTOR_TYPE_JANUS_PRO:\n+            {\n+                res = graph.build_siglip();\n+            } break;\n         case PROJECTOR_TYPE_COGVLM:\n             {\n                 res = graph.build_cogvlm();\n@@ -3151,6 +3163,13 @@ struct clip_model_loader {\n                     model.mm_boi            = get_tensor(TN_TOK_BOI);\n                     model.mm_eoi            = get_tensor(TN_TOK_EOI);\n                 } break;\n+            case PROJECTOR_TYPE_JANUS_PRO:\n+                {\n+                    model.mm_0_w = get_tensor(string_format(TN_LLAVA_PROJ, 0, \"weight\"));\n+                    model.mm_0_b = get_tensor(string_format(TN_LLAVA_PROJ, 0, \"bias\"));\n+                    model.mm_1_w = get_tensor(string_format(TN_LLAVA_PROJ, 1, \"weight\"));\n+                    model.mm_1_b = get_tensor(string_format(TN_LLAVA_PROJ, 1, \"bias\"));\n+                } break;\n             default:\n                 GGML_ASSERT(false && \"unknown projector type\");\n         }\n@@ -4096,6 +4115,18 @@ bool clip_image_preprocess(struct clip_ctx * ctx, const clip_image_u8 * img, str\n                 res_imgs->entries.push_back(std::move(img_f32));\n             } break;\n \n+        case PROJECTOR_TYPE_JANUS_PRO:\n+            {\n+                // Janus Pro preprocessing: pad to square with gray(127), resize to 384x384\n+                const std::array<uint8_t, 3> pad_color = {127, 127, 127};\n+                clip_image_u8 resized_image;\n+                int sz = params.image_size;\n+                img_tool::resize(*img, resized_image, {sz, sz}, img_tool::RESIZE_ALGO_BILINEAR, true, pad_color);\n+                clip_image_f32_ptr img_f32(clip_image_f32_init());\n+                normalize_image_u8_to_f32(resized_image, *img_f32, params.image_mean, params.image_std);\n+                res_imgs->entries.push_back(std::move(img_f32));\n+            } break;\n+\n         case PROJECTOR_TYPE_PIXTRAL:\n         case PROJECTOR_TYPE_LIGHTONOCR:\n             {\n@@ -4272,6 +4303,7 @@ int clip_n_output_tokens(const struct clip_ctx * ctx, struct clip_image_f32 * im\n     switch (proj) {\n         case PROJECTOR_TYPE_MLP:\n         case PROJECTOR_TYPE_MLP_NORM:\n+        case PROJECTOR_TYPE_JANUS_PRO:\n             {\n                 // do nothing\n             } break;\n@@ -4782,6 +4814,7 @@ bool clip_image_batch_encode(clip_ctx * ctx, const int n_threads, const clip_ima\n         case PROJECTOR_TYPE_ULTRAVOX:\n         case PROJECTOR_TYPE_LFM2:\n         case PROJECTOR_TYPE_VOXTRAL:\n+        case PROJECTOR_TYPE_JANUS_PRO:\n         case PROJECTOR_TYPE_COGVLM:\n             {\n                 // do nothing\n@@ -4870,6 +4903,7 @@ int clip_n_mmproj_embd(const struct clip_ctx * ctx) {\n             return ctx->model.mm_model_mlp_3_w->ne[1];\n         case PROJECTOR_TYPE_QWEN2VL:\n         case PROJECTOR_TYPE_QWEN25VL:\n+        case PROJECTOR_TYPE_JANUS_PRO:\n             return ctx->model.mm_1_b->ne[0];\n         case PROJECTOR_TYPE_QWEN3VL:\n             // main path + deepstack paths"
      }
    ],
    "num_files": 5,
    "scraped_at": "2025-11-16T23:29:09.464776"
  },
  {
    "pr_number": 16901,
    "title": "Add a setting to display message generation statistics",
    "body": "Close #16179\r\n\r\nAdded a setting to display generation statistics for each assistant message \u2014 tokens/s, amount of tokens in a message and generation time.\r\n\r\n### New Setting in the General section\r\n\r\n<img width=\"509\" height=\"172\" alt=\"Zrzut ekranu 2025-10-31 o 19 49 20\" src=\"https://github.com/user-attachments/assets/c962a130-5d47-47ef-a22d-cfab9d8f5dba\" />\r\n\r\n### Statistics at the bottom of the assistant message\r\n\r\n<img width=\"800\" height=\"464\" alt=\"Zrzut ekranu 2025-10-31 o 19 33 46\" src=\"https://github.com/user-attachments/assets/a0fa99a0-7210-4358-a8dd-ba4ee5bdf584\" />\r\n",
    "html_url": "https://github.com/ggml-org/llama.cpp/pull/16901",
    "created_at": "2025-10-31T18:50:44Z",
    "merged_at": "2025-11-01T14:35:57Z",
    "merge_commit_sha": "d8b860a219c2415faac8cc0e50b48b4aa11e3b64",
    "base_ref": "master",
    "head_sha": "ebc93494812aa7199475e795fcb143502c32da83",
    "user": "allozaur",
    "files": [
      {
        "filename": "tools/server/public/index.html.gz",
        "status": "modified",
        "additions": 0,
        "deletions": 0,
        "changes": 0,
        "patch": ""
      },
      {
        "filename": "tools/server/webui/src/lib/components/app/chat/ChatMessages/ChatMessageAssistant.svelte",
        "status": "modified",
        "additions": 59,
        "deletions": 14,
        "changes": 73,
        "patch": "@@ -3,7 +3,16 @@\n \timport { useProcessingState } from '$lib/hooks/use-processing-state.svelte';\n \timport { isLoading } from '$lib/stores/chat.svelte';\n \timport { fade } from 'svelte/transition';\n-\timport { Check, Copy, Package, X } from '@lucide/svelte';\n+\timport {\n+\t\tCheck,\n+\t\tCopy,\n+\t\tPackage,\n+\t\tX,\n+\t\tGauge,\n+\t\tClock,\n+\t\tWholeWord,\n+\t\tChartNoAxesColumn\n+\t} from '@lucide/svelte';\n \timport { Button } from '$lib/components/ui/button';\n \timport { Checkbox } from '$lib/components/ui/checkbox';\n \timport { INPUT_CLASSES } from '$lib/constants/input-classes';\n@@ -160,22 +169,58 @@\n \t\t</div>\n \t{/if}\n \n-\t{#if displayedModel()}\n-\t\t<span class=\"mt-6 mb-4 inline-flex items-center gap-1 text-xs text-muted-foreground\">\n-\t\t\t<Package class=\"h-3.5 w-3.5\" />\n+\t<div class=\"info my-6 grid gap-4\">\n+\t\t{#if displayedModel()}\n+\t\t\t<span class=\"inline-flex items-center gap-2 text-xs text-muted-foreground\">\n+\t\t\t\t<span class=\"inline-flex items-center gap-1\">\n+\t\t\t\t\t<Package class=\"h-3.5 w-3.5\" />\n \n-\t\t\t<span>Model used:</span>\n+\t\t\t\t\t<span>Model used:</span>\n+\t\t\t\t</span>\n \n-\t\t\t<button\n-\t\t\t\tclass=\"inline-flex cursor-pointer items-center gap-1 rounded-sm bg-muted-foreground/15 px-1.5 py-0.75\"\n-\t\t\t\tonclick={handleCopyModel}\n-\t\t\t>\n-\t\t\t\t{displayedModel()}\n+\t\t\t\t<button\n+\t\t\t\t\tclass=\"inline-flex cursor-pointer items-center gap-1 rounded-sm bg-muted-foreground/15 px-1.5 py-0.75\"\n+\t\t\t\t\tonclick={handleCopyModel}\n+\t\t\t\t>\n+\t\t\t\t\t{displayedModel()}\n \n-\t\t\t\t<Copy class=\"ml-1 h-3 w-3 \" />\n-\t\t\t</button>\n-\t\t</span>\n-\t{/if}\n+\t\t\t\t\t<Copy class=\"ml-1 h-3 w-3 \" />\n+\t\t\t\t</button>\n+\t\t\t</span>\n+\t\t{/if}\n+\n+\t\t{#if currentConfig.showMessageStats && message.timings && message.timings.predicted_n && message.timings.predicted_ms}\n+\t\t\t{@const tokensPerSecond = (message.timings.predicted_n / message.timings.predicted_ms) * 1000}\n+\t\t\t<span class=\"inline-flex items-center gap-2 text-xs text-muted-foreground\">\n+\t\t\t\t<span class=\"inline-flex items-center gap-1\">\n+\t\t\t\t\t<ChartNoAxesColumn class=\"h-3.5 w-3.5\" />\n+\n+\t\t\t\t\t<span>Statistics:</span>\n+\t\t\t\t</span>\n+\n+\t\t\t\t<div class=\"inline-flex flex-wrap items-center gap-2 text-xs text-muted-foreground\">\n+\t\t\t\t\t<span\n+\t\t\t\t\t\tclass=\"inline-flex items-center gap-1 rounded-sm bg-muted-foreground/15 px-1.5 py-0.75\"\n+\t\t\t\t\t>\n+\t\t\t\t\t\t<Gauge class=\"h-3 w-3\" />\n+\t\t\t\t\t\t{tokensPerSecond.toFixed(2)} tokens/s\n+\t\t\t\t\t</span>\n+\t\t\t\t\t<span\n+\t\t\t\t\t\tclass=\"inline-flex items-center gap-1 rounded-sm bg-muted-foreground/15 px-1.5 py-0.75\"\n+\t\t\t\t\t>\n+\t\t\t\t\t\t<WholeWord class=\"h-3 w-3\" />\n+\t\t\t\t\t\t{message.timings.predicted_n} tokens\n+\t\t\t\t\t</span>\n+\t\t\t\t\t<span\n+\t\t\t\t\t\tclass=\"inline-flex items-center gap-1 rounded-sm bg-muted-foreground/15 px-1.5 py-0.75\"\n+\t\t\t\t\t>\n+\t\t\t\t\t\t<Clock class=\"h-3 w-3\" />\n+\t\t\t\t\t\t{(message.timings.predicted_ms / 1000).toFixed(2)}s\n+\t\t\t\t\t</span>\n+\t\t\t\t</div>\n+\t\t\t</span>\n+\t\t{/if}\n+\t</div>\n \n \t{#if message.timestamp && !isEditing}\n \t\t<ChatMessageActions"
      },
      {
        "filename": "tools/server/webui/src/lib/components/app/chat/ChatSettings/ChatSettingsDialog.svelte",
        "status": "modified",
        "additions": 5,
        "deletions": 0,
        "changes": 5,
        "patch": "@@ -52,6 +52,11 @@\n \t\t\t\t\t\t{ value: 'dark', label: 'Dark', icon: Moon }\n \t\t\t\t\t]\n \t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tkey: 'showMessageStats',\n+\t\t\t\t\tlabel: 'Show message generation statistics',\n+\t\t\t\t\ttype: 'checkbox'\n+\t\t\t\t},\n \t\t\t\t{\n \t\t\t\t\tkey: 'showTokensPerSecond',\n \t\t\t\t\tlabel: 'Show tokens per second',"
      },
      {
        "filename": "tools/server/webui/src/lib/constants/settings-config.ts",
        "status": "modified",
        "additions": 3,
        "deletions": 0,
        "changes": 3,
        "patch": "@@ -8,6 +8,7 @@ export const SETTING_CONFIG_DEFAULT: Record<string, string | number | boolean> =\n \tshowThoughtInProgress: false,\n \tdisableReasoningFormat: false,\n \tkeepStatsVisible: false,\n+\tshowMessageStats: true,\n \taskForTitleConfirmation: false,\n \tpasteLongTextToFileLen: 2500,\n \tpdfAsImage: false,\n@@ -82,6 +83,8 @@ export const SETTING_CONFIG_INFO: Record<string, string> = {\n \tdisableReasoningFormat:\n \t\t'Show raw LLM output without backend parsing and frontend Markdown rendering to inspect streaming across different models.',\n \tkeepStatsVisible: 'Keep processing statistics visible after generation finishes.',\n+\tshowMessageStats:\n+\t\t'Display generation statistics (tokens/second, token count, duration) below each assistant message.',\n \taskForTitleConfirmation:\n \t\t'Ask for confirmation before automatically changing conversation title when editing the first message.',\n \tpdfAsImage: 'Parse PDF as image instead of text (requires vision-capable model).',"
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T23:29:10.324780"
  },
  {
    "pr_number": 16884,
    "title": "CUDA: fuse rope + set_rows",
    "body": "Based on #16769. \r\n\r\nOn a 4090:\r\n\r\n| Model                 | Test   |   t/s master |   t/s cuda-rope-fusion |   Speedup |\r\n|:----------------------|:-------|-------------:|-----------------------:|----------:|\r\n| llama 8B Q4_K_M       | tg32   |       134.90 |                 136.07 |      1.01 |\r\n| llama 8B Q4_K_M       | tg64   |       131.41 |                 132.84 |      1.01 |\r\n| llama 8B Q4_K_M       | tg128  |       130.54 |                 131.87 |      1.01 |\r\n| qwen3moe 30B.A3B Q4_0 | tg32   |       167.18 |                 168.23 |      1.01 |\r\n| qwen3moe 30B.A3B Q4_0 | tg64   |       161.00 |                 161.90 |      1.01 |\r\n| qwen3moe 30B.A3B Q4_0 | tg128  |       158.84 |                 159.83 |      1.01 |",
    "html_url": "https://github.com/ggml-org/llama.cpp/pull/16884",
    "created_at": "2025-10-31T05:20:51Z",
    "merged_at": "2025-11-13T00:50:01Z",
    "merge_commit_sha": "a90eb94ca9ec19f049a1c8e4958e71d9da777569",
    "base_ref": "master",
    "head_sha": "67624935bce10bc045d455571faf83eb7a1da9b6",
    "user": "am17an",
    "files": [
      {
        "filename": "ggml/src/ggml-cuda/ggml-cuda.cu",
        "status": "modified",
        "additions": 49,
        "deletions": 0,
        "changes": 49,
        "patch": "@@ -2992,6 +2992,36 @@ static void update_cuda_graph_executable(ggml_backend_cuda_context * cuda_ctx) {\n }\n #endif\n \n+static bool ggml_cuda_should_fuse_rope_set_rows(const ggml_tensor * rope,\n+                                                const ggml_tensor * view,\n+                                                const ggml_tensor * set_rows) {\n+    // ne3 not tested\n+    if (rope->src[0]->ne[3] != 1) {\n+        return false;\n+    }\n+\n+    if (set_rows->type != GGML_TYPE_F32 && set_rows->type != GGML_TYPE_F16) {\n+        return false;\n+    }\n+\n+    if (set_rows->src[1]->type != GGML_TYPE_I64) {\n+        return false;\n+    }\n+\n+    // The view should flatten two dims of rope into one dim\n+    if (!ggml_is_contiguous(view) || view->ne[0] != rope->ne[0] * rope->ne[1]) {\n+        return false;\n+    }\n+\n+    // Only norm/neox shaders have the fusion code\n+    const int mode = ((const int32_t *) rope->op_params)[2];\n+    if (mode != GGML_ROPE_TYPE_NORMAL && mode != GGML_ROPE_TYPE_NEOX) {\n+        return false;\n+    }\n+\n+    return true;\n+}\n+\n static bool ggml_cuda_can_fuse(const struct ggml_cgraph * cgraph, int node_idx, std::initializer_list<enum ggml_op> ops, std::initializer_list<enum ggml_unary_op> unary_ops) {\n #ifndef NDEBUG\n     const size_t num_unary = std::count(ops.begin(), ops.end(), GGML_OP_UNARY);\n@@ -3067,6 +3097,16 @@ static bool ggml_cuda_can_fuse(const struct ggml_cgraph * cgraph, int node_idx,\n         }\n     }\n \n+    if (ops.size() == 3 && ggml_can_fuse_subgraph(cgraph, node_idx, ops, { node_idx + 2 })) {\n+        const ggml_tensor * rope     = cgraph->nodes[node_idx];\n+        const ggml_tensor * view     = cgraph->nodes[node_idx + 1];\n+        const ggml_tensor * set_rows = cgraph->nodes[node_idx + 2];\n+\n+        if (ggml_cuda_should_fuse_rope_set_rows(rope, view, set_rows)) {\n+            return true;\n+        }\n+    }\n+\n     if (!ggml_can_fuse(cgraph, node_idx, ops)) {\n         return false;\n     }\n@@ -3196,6 +3236,15 @@ static void evaluate_and_capture_cuda_graph(ggml_backend_cuda_context * cuda_ctx\n                         continue;\n                     }\n \n+                    if (ggml_cuda_can_fuse(cgraph, i, { GGML_OP_ROPE, GGML_OP_VIEW, GGML_OP_SET_ROWS }, {})) {\n+                        ggml_tensor * rope = cgraph->nodes[i];\n+                        ggml_tensor * set_rows = cgraph->nodes[i + 2];\n+\n+                        ggml_cuda_op_rope_fused(*cuda_ctx, rope, set_rows);\n+                        i += 2;\n+                        continue;\n+                    }\n+\n                     if (node->op == GGML_OP_ADD) {\n                         int n_fuse = 0;\n                         ggml_op ops[8];"
      },
      {
        "filename": "ggml/src/ggml-cuda/rope.cu",
        "status": "modified",
        "additions": 162,
        "deletions": 60,
        "changes": 222,
        "patch": "@@ -1,3 +1,6 @@\n+#include \"convert.cuh\"\n+#include \"ggml-cuda/common.cuh\"\n+#include \"ggml.h\"\n #include \"rope.cuh\"\n \n struct rope_corr_dims {\n@@ -37,11 +40,23 @@ static __device__ void rope_yarn(\n     }\n }\n \n-template<bool forward, bool has_ff, typename T>\n-static __global__ void rope_norm(\n-        const T * x, T * dst, const int ne0, const int ne1, const int s1, const int s2, const int n_dims,\n-        const int32_t * pos, const float freq_scale, const float ext_factor, const float attn_factor,\n-        const rope_corr_dims corr_dims, const float theta_scale, const float * freq_factors) {\n+template <bool forward, bool has_ff, typename T, typename D>\n+static __global__ void rope_norm(const T *            x,\n+                                 D *                  dst,\n+                                 const int            ne0,\n+                                 const int            ne1,\n+                                 const int            s1,\n+                                 const int            s2,\n+                                 const int            n_dims,\n+                                 const int32_t *      pos,\n+                                 const float          freq_scale,\n+                                 const float          ext_factor,\n+                                 const float          attn_factor,\n+                                 const rope_corr_dims corr_dims,\n+                                 const float          theta_scale,\n+                                 const float *        freq_factors,\n+                                 const int64_t *      row_indices,\n+                                 const int            set_rows_stride) {\n     const int i0 = 2*(blockDim.y*blockIdx.y + threadIdx.y);\n \n     if (i0 >= ne0) {\n@@ -53,13 +68,27 @@ static __global__ void rope_norm(\n     const int row_x     = row_dst % ne1;\n     const int channel_x = row_dst / ne1;\n \n-    const int idst = row_dst*ne0 + i0;\n+    int       idst = row_dst * ne0 + i0;\n     const int ix   = channel_x*s2 + row_x*s1 + i0;\n \n-    if (i0 >= n_dims) {\n-        dst[idst + 0] = x[ix + 0];\n-        dst[idst + 1] = x[ix + 1];\n+    // Fusion optimization: ROPE + VIEW + SET_ROWS.\n+    // The rope output is viewed as a 1D tensor and offset based on a row index in row_indices.\n+    if (set_rows_stride != 0) {\n+        idst = row_x * ne0 + i0;\n+        idst += row_indices[channel_x] * set_rows_stride;\n+    }\n \n+    const auto & store_coaelsced = [&](float x0, float x1) {\n+        if constexpr (std::is_same_v<float, D>) {\n+            float2 v = make_float2(x0, x1);\n+            ggml_cuda_memcpy_1<8>(dst + idst, &v);\n+        } else if constexpr (std::is_same_v<half, D>) {\n+            half2 v = make_half2(x0, x1);\n+            ggml_cuda_memcpy_1<4>(dst + idst, &v);\n+        }\n+    };\n+    if (i0 >= n_dims) {\n+        store_coaelsced(x[ix + 0], x[ix + 1]);\n         return;\n     }\n \n@@ -75,15 +104,26 @@ static __global__ void rope_norm(\n     const float x0 = x[ix + 0];\n     const float x1 = x[ix + 1];\n \n-    dst[idst + 0] = x0*cos_theta - x1*sin_theta;\n-    dst[idst + 1] = x0*sin_theta + x1*cos_theta;\n+    store_coaelsced(x0 * cos_theta - x1 * sin_theta, x0 * sin_theta + x1 * cos_theta);\n }\n \n-template<bool forward, bool has_ff, typename T>\n-static __global__ void rope_neox(\n-        const T * x, T * dst, const int ne0, const int ne1, const int s1, const int s2, const int n_dims,\n-        const int32_t * pos, const float freq_scale, const float ext_factor, const float attn_factor,\n-        const rope_corr_dims corr_dims, const float theta_scale, const float * freq_factors) {\n+template <bool forward, bool has_ff, typename T, typename D>\n+static __global__ void rope_neox(const T *            x,\n+                                 D *                  dst,\n+                                 const int            ne0,\n+                                 const int            ne1,\n+                                 const int            s1,\n+                                 const int            s2,\n+                                 const int            n_dims,\n+                                 const int32_t *      pos,\n+                                 const float          freq_scale,\n+                                 const float          ext_factor,\n+                                 const float          attn_factor,\n+                                 const rope_corr_dims corr_dims,\n+                                 const float          theta_scale,\n+                                 const float *        freq_factors,\n+                                 const int64_t *      row_indices,\n+                                 const int            set_rows_stride) {\n     const int i0 = 2*(blockDim.y*blockIdx.y + threadIdx.y);\n \n     if (i0 >= ne0) {\n@@ -95,12 +135,19 @@ static __global__ void rope_neox(\n     const int row_x     = row_dst % ne1;\n     const int channel_x = row_dst / ne1;\n \n-    const int idst = row_dst*ne0 + i0/2;\n+    int       idst = row_dst * ne0 + i0 / 2;\n     const int ix   = channel_x*s2 + row_x*s1 + i0/2;\n \n+    // Fusion optimization: ROPE + VIEW + SET_ROWS.\n+    // The rope output is viewed as a 1D tensor and offset based on a row index in row_indices.\n+    if (set_rows_stride != 0) {\n+        idst = row_x * ne0 + i0 / 2;\n+        idst += row_indices[channel_x] * set_rows_stride;\n+    }\n+\n     if (i0 >= n_dims) {\n-        dst[idst + i0/2 + 0] = x[ix + i0/2 + 0];\n-        dst[idst + i0/2 + 1] = x[ix + i0/2 + 1];\n+        dst[idst + i0 / 2 + 0] = ggml_cuda_cast<D>(x[ix + i0 / 2 + 0]);\n+        dst[idst + i0 / 2 + 1] = ggml_cuda_cast<D>(x[ix + i0 / 2 + 1]);\n \n         return;\n     }\n@@ -117,8 +164,8 @@ static __global__ void rope_neox(\n     const float x0 = x[ix + 0];\n     const float x1 = x[ix + n_dims/2];\n \n-    dst[idst + 0]        = x0*cos_theta - x1*sin_theta;\n-    dst[idst + n_dims/2] = x0*sin_theta + x1*cos_theta;\n+    dst[idst + 0]          = ggml_cuda_cast<D>(x0 * cos_theta - x1 * sin_theta);\n+    dst[idst + n_dims / 2] = ggml_cuda_cast<D>(x0 * sin_theta + x1 * cos_theta);\n }\n \n template<bool forward, bool has_ff, typename T>\n@@ -238,11 +285,25 @@ static __global__ void rope_vision(\n     dst[idst + n_dims] = x0*sin_theta + x1*cos_theta;\n }\n \n-template<bool forward, typename T>\n-static void rope_norm_cuda(\n-        const T * x, T * dst, const int ne0, const int ne1, const int s1, const int s2, const int n_dims, const int nr,\n-        const int32_t * pos, const float freq_scale, const float freq_base, const float ext_factor, const float attn_factor,\n-        const rope_corr_dims corr_dims, const float * freq_factors, cudaStream_t stream) {\n+template <bool forward, typename T, typename D>\n+static void rope_norm_cuda(const T *            x,\n+                           D *                  dst,\n+                           const int            ne0,\n+                           const int            ne1,\n+                           const int            s1,\n+                           const int            s2,\n+                           const int            n_dims,\n+                           const int            nr,\n+                           const int32_t *      pos,\n+                           const float          freq_scale,\n+                           const float          freq_base,\n+                           const float          ext_factor,\n+                           const float          attn_factor,\n+                           const rope_corr_dims corr_dims,\n+                           const float *        freq_factors,\n+                           const int64_t *      row_indices,\n+                           const int            set_rows_stride,\n+                           cudaStream_t         stream) {\n     GGML_ASSERT(ne0 % 2 == 0);\n     const dim3 block_dims(1, CUDA_ROPE_BLOCK_SIZE, 1);\n     const int n_blocks_x = (ne0 + 2*CUDA_ROPE_BLOCK_SIZE - 1) / (2*CUDA_ROPE_BLOCK_SIZE);\n@@ -252,20 +313,34 @@ static void rope_norm_cuda(\n \n     if (freq_factors == nullptr) {\n         rope_norm<forward, false><<<block_nums, block_dims, 0, stream>>>(\n-            x, dst, ne0, ne1, s1, s2, n_dims, pos, freq_scale, ext_factor,\n-            attn_factor, corr_dims, theta_scale, freq_factors);\n+            x, dst, ne0, ne1, s1, s2, n_dims, pos, freq_scale, ext_factor, attn_factor, corr_dims, theta_scale,\n+            freq_factors, row_indices, set_rows_stride);\n     } else {\n         rope_norm<forward, true><<<block_nums, block_dims, 0, stream>>>(\n-            x, dst, ne0, ne1, s1, s2, n_dims, pos, freq_scale, ext_factor,\n-            attn_factor, corr_dims, theta_scale, freq_factors);\n+            x, dst, ne0, ne1, s1, s2, n_dims, pos, freq_scale, ext_factor, attn_factor, corr_dims, theta_scale,\n+            freq_factors, row_indices, set_rows_stride);\n     }\n }\n \n-template<bool forward, typename T>\n-static void rope_neox_cuda(\n-        const T * x, T * dst, const int ne0, const int ne1, const int s1, const int s2, const int n_dims, const int nr,\n-        const int32_t * pos, const float freq_scale, const float freq_base, const float ext_factor, const float attn_factor,\n-        const rope_corr_dims corr_dims, const float * freq_factors, cudaStream_t stream) {\n+template <bool forward, typename T, typename D>\n+static void rope_neox_cuda(const T *            x,\n+                           D *                  dst,\n+                           const int            ne0,\n+                           const int            ne1,\n+                           const int            s1,\n+                           const int            s2,\n+                           const int            n_dims,\n+                           const int            nr,\n+                           const int32_t *      pos,\n+                           const float          freq_scale,\n+                           const float          freq_base,\n+                           const float          ext_factor,\n+                           const float          attn_factor,\n+                           const rope_corr_dims corr_dims,\n+                           const float *        freq_factors,\n+                           const int64_t *      row_indices,\n+                           const int            set_rows_stride,\n+                           cudaStream_t         stream) {\n     GGML_ASSERT(ne0 % 2 == 0);\n     const dim3 block_dims(1, CUDA_ROPE_BLOCK_SIZE, 1);\n     const int n_blocks_x = (ne0 + 2*CUDA_ROPE_BLOCK_SIZE - 1) / (2*CUDA_ROPE_BLOCK_SIZE);\n@@ -274,13 +349,13 @@ static void rope_neox_cuda(\n     const float theta_scale = powf(freq_base, -2.0f/n_dims);\n \n     if (freq_factors == nullptr) {\n-        rope_neox<forward, false, T><<<block_nums, block_dims, 0, stream>>>(\n-            x, dst, ne0, ne1, s1, s2, n_dims, pos, freq_scale, ext_factor,\n-            attn_factor, corr_dims, theta_scale, freq_factors);\n+        rope_neox<forward, false><<<block_nums, block_dims, 0, stream>>>(\n+            x, dst, ne0, ne1, s1, s2, n_dims, pos, freq_scale, ext_factor, attn_factor, corr_dims, theta_scale,\n+            freq_factors, row_indices, set_rows_stride);\n     } else {\n-        rope_neox<forward, true, T><<<block_nums, block_dims, 0, stream>>>(\n-            x, dst, ne0, ne1, s1, s2, n_dims, pos, freq_scale, ext_factor,\n-            attn_factor, corr_dims, theta_scale, freq_factors);\n+        rope_neox<forward, true><<<block_nums, block_dims, 0, stream>>>(\n+            x, dst, ne0, ne1, s1, s2, n_dims, pos, freq_scale, ext_factor, attn_factor, corr_dims, theta_scale,\n+            freq_factors, row_indices, set_rows_stride);\n     }\n }\n \n@@ -333,20 +408,35 @@ static void rope_vision_cuda(\n }\n \n template <bool forward>\n-void ggml_cuda_op_rope_impl(ggml_backend_cuda_context & ctx, ggml_tensor * dst) {\n+void ggml_cuda_op_rope_impl(ggml_backend_cuda_context & ctx,\n+                            ggml_tensor *               dst,\n+                            const ggml_tensor *         set_rows = nullptr) {\n     const ggml_tensor * src0 = dst->src[0];\n     const ggml_tensor * src1 = dst->src[1];\n     const ggml_tensor * src2 = dst->src[2];\n \n     const float * src0_d = (const float *)src0->data;\n     const float * src1_d = (const float *)src1->data;\n \n-    float * dst_d = (float *)dst->data;\n+    void *          dst_d           = dst->data;\n+    const int64_t * row_indices     = nullptr;\n+    ggml_type       dst_type        = dst->type;\n+    int             set_rows_stride = 0;\n+\n+    if (set_rows != nullptr) {\n+        GGML_ASSERT(forward);\n+        dst_d           = set_rows->data;\n+        row_indices     = (const int64_t *) set_rows->src[1]->data;\n+        dst_type        = set_rows->type;\n+        set_rows_stride = set_rows->nb[1] / ggml_type_size(set_rows->type);\n+    }\n     cudaStream_t stream = ctx.stream();\n \n     GGML_ASSERT(src0->type == GGML_TYPE_F32 || src0->type == GGML_TYPE_F16);\n     GGML_ASSERT( dst->type == GGML_TYPE_F32 ||  dst->type == GGML_TYPE_F16);\n-    GGML_ASSERT(src0->type == dst->type);\n+    // When not fused, src0 and dst types must match\n+    // When fused (ROPE+VIEW+SET_ROWS), src0 may be F32 and dst may be F16\n+    GGML_ASSERT(src0->type == dst->type || (src0->type == GGML_TYPE_F32 && dst->type == GGML_TYPE_F16));\n \n     const int64_t ne00 = src0->ne[0]; // head dims\n     const int64_t ne01 = src0->ne[1]; // num heads\n@@ -404,14 +494,18 @@ void ggml_cuda_op_rope_impl(ggml_backend_cuda_context & ctx, ggml_tensor * dst)\n \n     // compute\n     if (is_neox) {\n-        if (src0->type == GGML_TYPE_F32) {\n-            rope_neox_cuda<forward>(\n-                (const float *) src0_d, (float *) dst_d, ne00, ne01, s01, s02, n_dims, nr, pos, freq_scale,\n-                freq_base, ext_factor, attn_factor, corr_dims, freq_factors, stream);\n-        } else if (src0->type == GGML_TYPE_F16) {\n-            rope_neox_cuda<forward>(\n-                (const half *) src0_d, (half *) dst_d, ne00, ne01, s01, s02, n_dims, nr, pos, freq_scale,\n-                freq_base, ext_factor, attn_factor, corr_dims, freq_factors, stream);\n+        if (src0->type == GGML_TYPE_F32 && dst_type == GGML_TYPE_F32) {\n+            rope_neox_cuda<forward, float, float>((const float *) src0_d, (float *) dst_d, ne00, ne01, s01, s02, n_dims,\n+                                                  nr, pos, freq_scale, freq_base, ext_factor, attn_factor, corr_dims,\n+                                                  freq_factors, row_indices, set_rows_stride, stream);\n+        } else if (src0->type == GGML_TYPE_F32 && dst_type == GGML_TYPE_F16) {\n+            rope_neox_cuda<forward, float, half>((const float *) src0_d, (half *) dst_d, ne00, ne01, s01, s02, n_dims,\n+                                                 nr, pos, freq_scale, freq_base, ext_factor, attn_factor, corr_dims,\n+                                                 freq_factors, row_indices, set_rows_stride, stream);\n+        } else if (src0->type == GGML_TYPE_F16 && dst_type == GGML_TYPE_F16) {\n+            rope_neox_cuda<forward, half, half>((const half *) src0_d, (half *) dst_d, ne00, ne01, s01, s02, n_dims, nr,\n+                                                pos, freq_scale, freq_base, ext_factor, attn_factor, corr_dims,\n+                                                freq_factors, row_indices, set_rows_stride, stream);\n         } else {\n             GGML_ABORT(\"fatal error\");\n         }\n@@ -440,14 +534,18 @@ void ggml_cuda_op_rope_impl(ggml_backend_cuda_context & ctx, ggml_tensor * dst)\n             GGML_ABORT(\"fatal error\");\n         }\n     } else {\n-        if (src0->type == GGML_TYPE_F32) {\n-            rope_norm_cuda<forward>(\n-                (const float *) src0_d, (float *) dst_d, ne00, ne01, s01, s02, n_dims, nr, pos, freq_scale,\n-                freq_base, ext_factor, attn_factor, corr_dims, freq_factors, stream);\n-        } else if (src0->type == GGML_TYPE_F16) {\n-            rope_norm_cuda<forward>(\n-                (const half *) src0_d, (half *) dst_d, ne00, ne01, s01, s02, n_dims, nr, pos, freq_scale,\n-                freq_base, ext_factor, attn_factor, corr_dims, freq_factors, stream);\n+        if (src0->type == GGML_TYPE_F32 && dst_type == GGML_TYPE_F32) {\n+            rope_norm_cuda<forward, float, float>((const float *) src0_d, (float *) dst_d, ne00, ne01, s01, s02, n_dims,\n+                                                  nr, pos, freq_scale, freq_base, ext_factor, attn_factor, corr_dims,\n+                                                  freq_factors, row_indices, set_rows_stride, stream);\n+        } else if (src0->type == GGML_TYPE_F32 && dst_type == GGML_TYPE_F16) {\n+            rope_norm_cuda<forward, float, half>((const float *) src0_d, (half *) dst_d, ne00, ne01, s01, s02, n_dims,\n+                                                 nr, pos, freq_scale, freq_base, ext_factor, attn_factor, corr_dims,\n+                                                 freq_factors, row_indices, set_rows_stride, stream);\n+        } else if (src0->type == GGML_TYPE_F16 && dst_type == GGML_TYPE_F16) {\n+            rope_norm_cuda<forward, half, half>((const half *) src0_d, (half *) dst_d, ne00, ne01, s01, s02, n_dims, nr,\n+                                                pos, freq_scale, freq_base, ext_factor, attn_factor, corr_dims,\n+                                                freq_factors, row_indices, set_rows_stride, stream);\n         } else {\n             GGML_ABORT(\"fatal error\");\n         }\n@@ -461,3 +559,7 @@ void ggml_cuda_op_rope(ggml_backend_cuda_context & ctx, ggml_tensor * dst) {\n void ggml_cuda_op_rope_back(ggml_backend_cuda_context & ctx, ggml_tensor * dst) {\n     ggml_cuda_op_rope_impl<false>(ctx, dst);\n }\n+\n+void ggml_cuda_op_rope_fused(ggml_backend_cuda_context & ctx, ggml_tensor * rope, ggml_tensor * set_rows) {\n+    ggml_cuda_op_rope_impl<true>(ctx, rope, set_rows);\n+}"
      },
      {
        "filename": "ggml/src/ggml-cuda/rope.cuh",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -5,3 +5,5 @@\n void ggml_cuda_op_rope(ggml_backend_cuda_context & ctx, ggml_tensor * dst);\n \n void ggml_cuda_op_rope_back(ggml_backend_cuda_context & ctx, ggml_tensor * dst);\n+\n+void ggml_cuda_op_rope_fused(ggml_backend_cuda_context & ctx, ggml_tensor * dst, ggml_tensor * set_rows);"
      },
      {
        "filename": "src/llama-graph.cpp",
        "status": "modified",
        "additions": 2,
        "deletions": 1,
        "changes": 3,
        "patch": "@@ -1592,9 +1592,10 @@ ggml_tensor * llm_graph_context::build_attn(\n             int       il) const {\n     // these nodes are added to the graph together so that they are not reordered\n     // by doing so, the number of splits in the graph is reduced\n+    // expand k later to enable rope fusion which directly writes into k-v cache\n     ggml_build_forward_expand(gf, q_cur);\n-    ggml_build_forward_expand(gf, k_cur);\n     ggml_build_forward_expand(gf, v_cur);\n+    ggml_build_forward_expand(gf, k_cur);\n \n     const auto * mctx_cur = inp->mctx;\n "
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T23:29:13.160334"
  },
  {
    "pr_number": 16868,
    "title": "vulkan: fuse mul_mat+add and mul_mat_id+add_id",
    "body": "The fusion is only applied for the mat-vec mul paths.\r\n\r\nI had hesitated to implement this previously because when it kicks in it implicitly disables the add->rmsnorm optimization, but it seems like this is a pretty significant win in some cases. gpt-oss has a significant gain, it uses both mul_mat+add and mul_mat_id+add_id.\r\n\r\n```\r\nbefore:\r\n\r\nZ:\\github\\jeffbolznv\\llama.cpp\\build\\bin\\RelWithDebInfo>llama-bench.exe -fa 1 -n 128 -p 0 -r 10 --prio 1 -m c:\\models\\DeepSeek-R1-Distill-Llama-8B-Q4_K_M.gguf -m c:\\models\\DeepSeek-R1-Distill-Llama-8B-Q6_K.gguf -m c:\\models\\DeepSeek-R1-Distill-Qwen-14B-Q4_K_M.gguf -m c:\\models\\Llama-3.2-1B.Q2_K.gguf -m c:\\models\\Llama-3.2-1B.Q3_K_S.gguf -m c:\\models\\llama-3.2-3b-instruct-q5_k_m.gguf -m c:\\models\\Qwen_Qwen3-30B-A3B-Q2_K.gguf -m c:\\models\\Qwen2.5-7B-Instruct-1M-Q2_K.gguf  -m c:\\models\\\\deepseek-v2-lite-safetensors\\deepseek-v2-lite-Q4_K_M.gguf -m c:\\models\\gpt-oss-20b-mxfp4.gguf -m c:\\models\\Phi-3-mini-4k-instruct-q4.gguf -m c:\\models\\llama-2-7b.Q4_0.gguf -m c:\\models\\llama-3.2-3b-instruct-q8_0.gguf -m c:\\models\\Mistral-22B-v0.2-Q4_K_M.gguf -m c:\\models\\nvidia_Llama-3_3-Nemotron-Super-49B-v1_5-Q4_K_S.gguf\r\nggml_vulkan: Found 1 Vulkan devices:\r\nggml_vulkan: 0 = NVIDIA GeForce RTX 5090 (NVIDIA) | uma: 0 | fp16: 1 | bf16: 1 | warp size: 32 | shared memory: 49152 | int dot: 1 | matrix cores: NV_coopmat2\r\n| model                          |       size |     params | backend    | ngl | fa |            test |                  t/s |\r\n| ------------------------------ | ---------: | ---------: | ---------- | --: | -: | --------------: | -------------------: |\r\n| llama 8B Q4_K - Medium         |   4.58 GiB |     8.03 B | Vulkan     |  99 |  1 |           tg128 |        242.76 \u00b1 1.69 |\r\n| llama 8B Q6_K                  |   6.14 GiB |     8.03 B | Vulkan     |  99 |  1 |           tg128 |        197.42 \u00b1 8.13 |\r\n| qwen2 14B Q4_K - Medium        |   8.37 GiB |    14.77 B | Vulkan     |  99 |  1 |           tg128 |        128.08 \u00b1 5.03 |\r\n| llama 1B Q2_K - Medium         | 546.50 MiB |     1.24 B | Vulkan     |  99 |  1 |           tg128 |       858.07 \u00b1 18.05 |\r\n| llama 1B Q3_K - Small          | 604.50 MiB |     1.24 B | Vulkan     |  99 |  1 |           tg128 |        860.71 \u00b1 5.43 |\r\n| llama 3B Q5_K - Medium         |   2.16 GiB |     3.21 B | Vulkan     |  99 |  1 |           tg128 |        397.72 \u00b1 5.27 |\r\n| qwen3moe 30B.A3B Q2_K - Medium |  10.15 GiB |    30.53 B | Vulkan     |  99 |  1 |           tg128 |        278.15 \u00b1 5.10 |\r\n| qwen2 7B Q2_K - Medium         |   2.80 GiB |     7.62 B | Vulkan     |  99 |  1 |           tg128 |       243.46 \u00b1 14.66 |\r\n| deepseek2 16B Q4_K - Medium    |   9.65 GiB |    15.71 B | Vulkan     |  99 |  1 |           tg128 |       304.32 \u00b1 40.91 |\r\n| gpt-oss 20B MXFP4 MoE          |  11.27 GiB |    20.91 B | Vulkan     |  99 |  1 |           tg128 |       286.50 \u00b1 10.03 |\r\n| phi3 3B Q4_K - Medium          |   2.23 GiB |     3.82 B | Vulkan     |  99 |  1 |           tg128 |        363.21 \u00b1 3.02 |\r\n| llama 7B Q4_0                  |   3.56 GiB |     6.74 B | Vulkan     |  99 |  1 |           tg128 |       271.88 \u00b1 11.31 |\r\n| llama 3B Q8_0                  |   3.18 GiB |     3.21 B | Vulkan     |  99 |  1 |           tg128 |        327.34 \u00b1 2.46 |\r\n| llama ?B Q4_K - Medium         |  12.42 GiB |    22.24 B | Vulkan     |  99 |  1 |           tg128 |         93.66 \u00b1 0.29 |\r\n| deci 70B Q4_K - Small          |  26.66 GiB |    49.87 B | Vulkan     |  99 |  1 |           tg128 |         50.15 \u00b1 0.12 |\r\n\r\nafter:\r\n\r\nZ:\\github\\jeffbolznv\\llama.cpp\\build\\bin\\RelWithDebInfo>llama-bench.exe -fa 1 -n 128 -p 0 -r 10 --prio 1 -m c:\\models\\DeepSeek-R1-Distill-Llama-8B-Q4_K_M.gguf -m c:\\models\\DeepSeek-R1-Distill-Llama-8B-Q6_K.gguf -m c:\\models\\DeepSeek-R1-Distill-Qwen-14B-Q4_K_M.gguf -m c:\\models\\Llama-3.2-1B.Q2_K.gguf -m c:\\models\\Llama-3.2-1B.Q3_K_S.gguf -m c:\\models\\llama-3.2-3b-instruct-q5_k_m.gguf -m c:\\models\\Qwen_Qwen3-30B-A3B-Q2_K.gguf -m c:\\models\\Qwen2.5-7B-Instruct-1M-Q2_K.gguf  -m c:\\models\\\\deepseek-v2-lite-safetensors\\deepseek-v2-lite-Q4_K_M.gguf -m c:\\models\\gpt-oss-20b-mxfp4.gguf -m c:\\models\\Phi-3-mini-4k-instruct-q4.gguf -m c:\\models\\llama-2-7b.Q4_0.gguf -m c:\\models\\llama-3.2-3b-instruct-q8_0.gguf -m c:\\models\\Mistral-22B-v0.2-Q4_K_M.gguf -m c:\\models\\nvidia_Llama-3_3-Nemotron-Super-49B-v1_5-Q4_K_S.gguf\r\nggml_vulkan: Found 1 Vulkan devices:\r\nggml_vulkan: 0 = NVIDIA GeForce RTX 5090 (NVIDIA) | uma: 0 | fp16: 1 | bf16: 1 | warp size: 32 | shared memory: 49152 | int dot: 1 | matrix cores: NV_coopmat2\r\n| model                          |       size |     params | backend    | ngl | fa |            test |                  t/s |\r\n| ------------------------------ | ---------: | ---------: | ---------- | --: | -: | --------------: | -------------------: |\r\n| llama 8B Q4_K - Medium         |   4.58 GiB |     8.03 B | Vulkan     |  99 |  1 |           tg128 |        243.73 \u00b1 3.13 |\r\n| llama 8B Q6_K                  |   6.14 GiB |     8.03 B | Vulkan     |  99 |  1 |           tg128 |        198.43 \u00b1 9.83 |\r\n| qwen2 14B Q4_K - Medium        |   8.37 GiB |    14.77 B | Vulkan     |  99 |  1 |           tg128 |        130.27 \u00b1 4.19 |\r\n| llama 1B Q2_K - Medium         | 546.50 MiB |     1.24 B | Vulkan     |  99 |  1 |           tg128 |       878.72 \u00b1 13.51 |\r\n| llama 1B Q3_K - Small          | 604.50 MiB |     1.24 B | Vulkan     |  99 |  1 |           tg128 |       841.56 \u00b1 12.65 |\r\n| llama 3B Q5_K - Medium         |   2.16 GiB |     3.21 B | Vulkan     |  99 |  1 |           tg128 |        396.98 \u00b1 6.50 |\r\n| qwen3moe 30B.A3B Q2_K - Medium |  10.15 GiB |    30.53 B | Vulkan     |  99 |  1 |           tg128 |        271.83 \u00b1 5.92 |\r\n| qwen2 7B Q2_K - Medium         |   2.80 GiB |     7.62 B | Vulkan     |  99 |  1 |           tg128 |       254.90 \u00b1 17.92 |\r\n| deepseek2 16B Q4_K - Medium    |   9.65 GiB |    15.71 B | Vulkan     |  99 |  1 |           tg128 |        321.27 \u00b1 9.68 |\r\n| gpt-oss 20B MXFP4 MoE          |  11.27 GiB |    20.91 B | Vulkan     |  99 |  1 |           tg128 |       302.79 \u00b1 19.76 |\r\n| phi3 3B Q4_K - Medium          |   2.23 GiB |     3.82 B | Vulkan     |  99 |  1 |           tg128 |       367.65 \u00b1 12.74 |\r\n| llama 7B Q4_0                  |   3.56 GiB |     6.74 B | Vulkan     |  99 |  1 |           tg128 |        276.24 \u00b1 4.54 |\r\n| llama 3B Q8_0                  |   3.18 GiB |     3.21 B | Vulkan     |  99 |  1 |           tg128 |        327.07 \u00b1 3.44 |\r\n| llama ?B Q4_K - Medium         |  12.42 GiB |    22.24 B | Vulkan     |  99 |  1 |           tg128 |         91.18 \u00b1 1.69 |\r\n| deci 70B Q4_K - Small          |  26.66 GiB |    49.87 B | Vulkan     |  99 |  1 |           tg128 |         49.69 \u00b1 0.18 |\r\n```",
    "html_url": "https://github.com/ggml-org/llama.cpp/pull/16868",
    "created_at": "2025-10-30T17:48:31Z",
    "merged_at": "2025-11-01T05:45:28Z",
    "merge_commit_sha": "2e76e013600cb0d51ccf158571ca1d0502952a07",
    "base_ref": "master",
    "head_sha": "c14b77f6c58ca9a4aa61df41f83d36e186b6c632",
    "user": "jeffbolznv",
    "files": [
      {
        "filename": "ggml/src/ggml-vulkan/ggml-vulkan.cpp",
        "status": "modified",
        "additions": 393,
        "deletions": 122,
        "changes": 515,
        "patch": "@@ -792,9 +792,18 @@ struct vk_mat_mat_push_constants {\n     uint32_t padded_N;\n };\n struct vk_mat_vec_push_constants {\n-    uint32_t ncols; uint32_t stride_a; uint32_t stride_b; uint32_t stride_d;\n-    uint32_t batch_stride_a; uint32_t batch_stride_b; uint32_t batch_stride_d;\n-    uint32_t ne02; uint32_t ne12; uint32_t broadcast2; uint32_t broadcast3;\n+    uint32_t ncols;\n+    uint32_t stride_a;\n+    uint32_t stride_b;\n+    uint32_t stride_d;\n+    uint32_t batch_stride_a;\n+    uint32_t batch_stride_b;\n+    uint32_t batch_stride_d;\n+    uint32_t enable_bias;\n+    uint32_t ne02;\n+    uint32_t ne12;\n+    uint32_t broadcast2;\n+    uint32_t broadcast3;\n };\n \n struct vk_mat_mat_id_push_constants {\n@@ -805,9 +814,16 @@ struct vk_mat_mat_id_push_constants {\n     uint32_t padded_N;\n };\n struct vk_mat_vec_id_push_constants {\n-    uint32_t ncols; uint32_t stride_a; uint32_t stride_b; uint32_t stride_d;\n-    uint32_t batch_stride_a; uint32_t batch_stride_b; uint32_t batch_stride_d;\n-    uint32_t nei0; uint32_t ne11;\n+    uint32_t ncols;\n+    uint32_t stride_a;\n+    uint32_t stride_b;\n+    uint32_t stride_d;\n+    uint32_t batch_stride_a;\n+    uint32_t batch_stride_b;\n+    uint32_t batch_stride_d;\n+    uint32_t enable_bias;\n+    uint32_t nei0;\n+    uint32_t ne11;\n };\n \n struct vk_flash_attn_push_constants {\n@@ -3340,92 +3356,92 @@ static void ggml_vk_load_shaders(vk_device& device) {\n                                               SHADER_REDUCTION_MODE_SHMEM;\n \n         for (uint32_t i = 0; i < mul_mat_vec_max_cols; ++i) {\n-            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f32_f32[w][GGML_TYPE_F32 ][i], \"mul_mat_vec_f32_f32_f32\",  arr_dmmv_f32_f32_f32_len[reduc],  arr_dmmv_f32_f32_f32_data[reduc],  \"main\", 3, sizeof(vk_mat_vec_push_constants), {2, 1, 1}, {wg_size_subgroup, 2, i+1}, 1, false, use_subgroups, force_subgroup_size);\n-            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f32_f32[w][GGML_TYPE_F16 ][i], \"mul_mat_vec_f16_f32_f32\",  arr_dmmv_f16_f32_f32_len[reduc],  arr_dmmv_f16_f32_f32_data[reduc],  \"main\", 3, sizeof(vk_mat_vec_push_constants), {2, 1, 1}, {wg_size_subgroup, 2, i+1}, 1, false, use_subgroups, force_subgroup_size);\n-            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f32_f32[w][GGML_TYPE_BF16][i], \"mul_mat_vec_bf16_f32_f32\", arr_dmmv_bf16_f32_f32_len[reduc], arr_dmmv_bf16_f32_f32_data[reduc], \"main\", 3, sizeof(vk_mat_vec_push_constants), {2, 1, 1}, {wg_size_subgroup, 2, i+1}, 1, false, use_subgroups, force_subgroup_size);\n-            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f32_f32[w][GGML_TYPE_Q4_0][i], \"mul_mat_vec_q4_0_f32_f32\", arr_dmmv_q4_0_f32_f32_len[reduc], arr_dmmv_q4_0_f32_f32_data[reduc], \"main\", 3, sizeof(vk_mat_vec_push_constants), {2*rm_stdq, 1, 1}, {wg_size_subgroup, 2*rm_stdq, i+1}, 1, true, use_subgroups, force_subgroup_size);\n-            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f32_f32[w][GGML_TYPE_Q4_1][i], \"mul_mat_vec_q4_1_f32_f32\", arr_dmmv_q4_1_f32_f32_len[reduc], arr_dmmv_q4_1_f32_f32_data[reduc], \"main\", 3, sizeof(vk_mat_vec_push_constants), {2*rm_stdq, 1, 1}, {wg_size_subgroup, 2*rm_stdq, i+1}, 1, true, use_subgroups, force_subgroup_size);\n-            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f32_f32[w][GGML_TYPE_Q5_0][i], \"mul_mat_vec_q5_0_f32_f32\", arr_dmmv_q5_0_f32_f32_len[reduc], arr_dmmv_q5_0_f32_f32_data[reduc], \"main\", 3, sizeof(vk_mat_vec_push_constants), {2*rm_stdq, 1, 1}, {wg_size_subgroup, 2*rm_stdq, i+1}, 1, true, use_subgroups, force_subgroup_size);\n-            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f32_f32[w][GGML_TYPE_Q5_1][i], \"mul_mat_vec_q5_1_f32_f32\", arr_dmmv_q5_1_f32_f32_len[reduc], arr_dmmv_q5_1_f32_f32_data[reduc], \"main\", 3, sizeof(vk_mat_vec_push_constants), {2*rm_stdq, 1, 1}, {wg_size_subgroup, 2*rm_stdq, i+1}, 1, true, use_subgroups, force_subgroup_size);\n-            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f32_f32[w][GGML_TYPE_Q8_0][i], \"mul_mat_vec_q8_0_f32_f32\", arr_dmmv_q8_0_f32_f32_len[reduc], arr_dmmv_q8_0_f32_f32_data[reduc], \"main\", 3, sizeof(vk_mat_vec_push_constants), {1*rm_stdq, 1, 1}, {wg_size_subgroup, 1*rm_stdq, i+1}, 1, true, use_subgroups, force_subgroup_size);\n-            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f32_f32[w][GGML_TYPE_Q2_K][i], \"mul_mat_vec_q2_k_f32_f32\", arr_dmmv_q2_k_f32_f32_len[reduc16], arr_dmmv_q2_k_f32_f32_data[reduc16], \"main\", 3, sizeof(vk_mat_vec_push_constants), {rm_kq, 1, 1}, {wg_size_subgroup16, rm_kq, i+1}, 1, true, use_subgroups16, force_subgroup_size16);\n-            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f32_f32[w][GGML_TYPE_Q3_K][i], \"mul_mat_vec_q3_k_f32_f32\", arr_dmmv_q3_k_f32_f32_len[reduc16], arr_dmmv_q3_k_f32_f32_data[reduc16], \"main\", 3, sizeof(vk_mat_vec_push_constants), {rm_kq, 1, 1}, {wg_size_subgroup16, rm_kq, i+1}, 1, true, use_subgroups16, force_subgroup_size16);\n-            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f32_f32[w][GGML_TYPE_Q4_K][i], \"mul_mat_vec_q4_k_f32_f32\", arr_dmmv_q4_k_f32_f32_len[reduc16], arr_dmmv_q4_k_f32_f32_data[reduc16], \"main\", 3, sizeof(vk_mat_vec_push_constants), {rm_kq, 1, 1}, {wg_size_subgroup16, rm_kq, i+1}, 1, true, use_subgroups16, force_subgroup_size16);\n-            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f32_f32[w][GGML_TYPE_Q5_K][i], \"mul_mat_vec_q5_k_f32_f32\", arr_dmmv_q5_k_f32_f32_len[reduc16], arr_dmmv_q5_k_f32_f32_data[reduc16], \"main\", 3, sizeof(vk_mat_vec_push_constants), {rm_kq, 1, 1}, {wg_size_subgroup16, rm_kq, i+1}, 1, true, use_subgroups16, force_subgroup_size16);\n-            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f32_f32[w][GGML_TYPE_Q6_K][i], \"mul_mat_vec_q6_k_f32_f32\", arr_dmmv_q6_k_f32_f32_len[reduc16], arr_dmmv_q6_k_f32_f32_data[reduc16], \"main\", 3, sizeof(vk_mat_vec_push_constants), {rm_kq, 1, 1}, {wg_size_subgroup16, rm_kq, i+1}, 1, true, use_subgroups16, force_subgroup_size16);\n-            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f32_f32[w][GGML_TYPE_IQ1_S][i],   \"mul_mat_vec_iq1_s_f32_f32\",   arr_dmmv_iq1_s_f32_f32_len[reduc16],   arr_dmmv_iq1_s_f32_f32_data[reduc16],   \"main\", 3, sizeof(vk_mat_vec_push_constants), {rm_iq, 1, 1}, {wg_size_subgroup16, rm_iq, i+1}, 1, true, use_subgroups16, force_subgroup_size16);\n-            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f32_f32[w][GGML_TYPE_IQ1_M][i],   \"mul_mat_vec_iq1_m_f32_f32\",   arr_dmmv_iq1_m_f32_f32_len[reduc16],   arr_dmmv_iq1_m_f32_f32_data[reduc16],   \"main\", 3, sizeof(vk_mat_vec_push_constants), {rm_iq, 1, 1}, {wg_size_subgroup16, rm_iq, i+1}, 1, true, use_subgroups16, force_subgroup_size16);\n-            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f32_f32[w][GGML_TYPE_IQ2_XXS][i], \"mul_mat_vec_iq2_xxs_f32_f32\", arr_dmmv_iq2_xxs_f32_f32_len[reduc16], arr_dmmv_iq2_xxs_f32_f32_data[reduc16], \"main\", 3, sizeof(vk_mat_vec_push_constants), {rm_iq, 1, 1}, {wg_size_subgroup16, rm_iq, i+1}, 1, true, use_subgroups16, force_subgroup_size16);\n-            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f32_f32[w][GGML_TYPE_IQ2_XS][i],  \"mul_mat_vec_iq2_xs_f32_f32\",  arr_dmmv_iq2_xs_f32_f32_len[reduc16],  arr_dmmv_iq2_xs_f32_f32_data[reduc16],  \"main\", 3, sizeof(vk_mat_vec_push_constants), {rm_iq, 1, 1}, {wg_size_subgroup16, rm_iq, i+1}, 1, true, use_subgroups16, force_subgroup_size16);\n-            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f32_f32[w][GGML_TYPE_IQ2_S][i],   \"mul_mat_vec_iq2_s_f32_f32\",   arr_dmmv_iq2_s_f32_f32_len[reduc16],   arr_dmmv_iq2_s_f32_f32_data[reduc16],   \"main\", 3, sizeof(vk_mat_vec_push_constants), {rm_iq, 1, 1}, {wg_size_subgroup16, rm_iq, i+1}, 1, true, use_subgroups16, force_subgroup_size16);\n-            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f32_f32[w][GGML_TYPE_IQ3_XXS][i], \"mul_mat_vec_iq3_xxs_f32_f32\", arr_dmmv_iq3_xxs_f32_f32_len[reduc16], arr_dmmv_iq3_xxs_f32_f32_data[reduc16], \"main\", 3, sizeof(vk_mat_vec_push_constants), {rm_iq, 1, 1}, {wg_size_subgroup16, rm_iq, i+1}, 1, true, use_subgroups16, force_subgroup_size16);\n-            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f32_f32[w][GGML_TYPE_IQ3_S][i],   \"mul_mat_vec_iq3_s_f32_f32\",   arr_dmmv_iq3_s_f32_f32_len[reduc16],   arr_dmmv_iq3_s_f32_f32_data[reduc16],   \"main\", 3, sizeof(vk_mat_vec_push_constants), {rm_iq, 1, 1}, {wg_size_subgroup16, rm_iq, i+1}, 1, true, use_subgroups16, force_subgroup_size16);\n-            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f32_f32[w][GGML_TYPE_IQ4_XS][i],  \"mul_mat_vec_iq4_xs_f32_f32\",  arr_dmmv_iq4_xs_f32_f32_len[reduc16],  arr_dmmv_iq4_xs_f32_f32_data[reduc16],  \"main\", 3, sizeof(vk_mat_vec_push_constants), {rm_iq, 1, 1}, {wg_size_subgroup16, rm_iq, i+1}, 1, true, use_subgroups16, force_subgroup_size16);\n-            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f32_f32[w][GGML_TYPE_IQ4_NL][i],  \"mul_mat_vec_iq4_nl_f32_f32\",  arr_dmmv_iq4_nl_f32_f32_len[reduc16],  arr_dmmv_iq4_nl_f32_f32_data[reduc16],  \"main\", 3, sizeof(vk_mat_vec_push_constants), {rm_iq, 1, 1}, {wg_size_subgroup16, rm_iq, i+1}, 1, true, use_subgroups16, force_subgroup_size16);\n-            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f32_f32[w][GGML_TYPE_MXFP4][i],   \"mul_mat_vec_mxfp4_f32_f32\",   arr_dmmv_mxfp4_f32_f32_len[reduc16],   arr_dmmv_mxfp4_f32_f32_data[reduc16],   \"main\", 3, sizeof(vk_mat_vec_push_constants), {rm_iq, 1, 1}, {wg_size_subgroup16, rm_iq, i+1}, 1, true, use_subgroups16, force_subgroup_size16);\n-\n-            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f16_f32[w][GGML_TYPE_F32 ][i], \"mul_mat_vec_f32_f16_f32\",  arr_dmmv_f32_f16_f32_len[reduc],  arr_dmmv_f32_f16_f32_data[reduc],  \"main\", 3, sizeof(vk_mat_vec_push_constants), {2, 1, 1}, {wg_size_subgroup, 2, i+1}, 1, false, use_subgroups, force_subgroup_size);\n-            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f16_f32[w][GGML_TYPE_F16 ][i], \"mul_mat_vec_f16_f16_f32\",  arr_dmmv_f16_f16_f32_len[reduc],  arr_dmmv_f16_f16_f32_data[reduc],  \"main\", 3, sizeof(vk_mat_vec_push_constants), {2, 1, 1}, {wg_size_subgroup, 2, i+1}, 1, false, use_subgroups, force_subgroup_size);\n-            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f16_f32[w][GGML_TYPE_BF16][i], \"mul_mat_vec_bf16_f16_f32\", arr_dmmv_bf16_f16_f32_len[reduc], arr_dmmv_bf16_f16_f32_data[reduc], \"main\", 3, sizeof(vk_mat_vec_push_constants), {2, 1, 1}, {wg_size_subgroup, 2, i+1}, 1, false, use_subgroups, force_subgroup_size);\n-            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f16_f32[w][GGML_TYPE_Q4_0][i], \"mul_mat_vec_q4_0_f16_f32\", arr_dmmv_q4_0_f16_f32_len[reduc], arr_dmmv_q4_0_f16_f32_data[reduc], \"main\", 3, sizeof(vk_mat_vec_push_constants), {2*rm_stdq, 1, 1}, {wg_size_subgroup, 2*rm_stdq, i+1}, 1, true, use_subgroups, force_subgroup_size);\n-            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f16_f32[w][GGML_TYPE_Q4_1][i], \"mul_mat_vec_q4_1_f16_f32\", arr_dmmv_q4_1_f16_f32_len[reduc], arr_dmmv_q4_1_f16_f32_data[reduc], \"main\", 3, sizeof(vk_mat_vec_push_constants), {2*rm_stdq, 1, 1}, {wg_size_subgroup, 2*rm_stdq, i+1}, 1, true, use_subgroups, force_subgroup_size);\n-            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f16_f32[w][GGML_TYPE_Q5_0][i], \"mul_mat_vec_q5_0_f16_f32\", arr_dmmv_q5_0_f16_f32_len[reduc], arr_dmmv_q5_0_f16_f32_data[reduc], \"main\", 3, sizeof(vk_mat_vec_push_constants), {2*rm_stdq, 1, 1}, {wg_size_subgroup, 2*rm_stdq, i+1}, 1, true, use_subgroups, force_subgroup_size);\n-            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f16_f32[w][GGML_TYPE_Q5_1][i], \"mul_mat_vec_q5_1_f16_f32\", arr_dmmv_q5_1_f16_f32_len[reduc], arr_dmmv_q5_1_f16_f32_data[reduc], \"main\", 3, sizeof(vk_mat_vec_push_constants), {2*rm_stdq, 1, 1}, {wg_size_subgroup, 2*rm_stdq, i+1}, 1, true, use_subgroups, force_subgroup_size);\n-            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f16_f32[w][GGML_TYPE_Q8_0][i], \"mul_mat_vec_q8_0_f16_f32\", arr_dmmv_q8_0_f16_f32_len[reduc], arr_dmmv_q8_0_f16_f32_data[reduc], \"main\", 3, sizeof(vk_mat_vec_push_constants), {1*rm_stdq, 1, 1}, {wg_size_subgroup, 1*rm_stdq, i+1}, 1, true, use_subgroups, force_subgroup_size);\n-            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f16_f32[w][GGML_TYPE_Q2_K][i], \"mul_mat_vec_q2_k_f16_f32\", arr_dmmv_q2_k_f16_f32_len[reduc16], arr_dmmv_q2_k_f16_f32_data[reduc16], \"main\", 3, sizeof(vk_mat_vec_push_constants), {rm_kq, 1, 1}, {wg_size_subgroup16, rm_kq, i+1}, 1, true, use_subgroups16, force_subgroup_size16);\n-            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f16_f32[w][GGML_TYPE_Q3_K][i], \"mul_mat_vec_q3_k_f16_f32\", arr_dmmv_q3_k_f16_f32_len[reduc16], arr_dmmv_q3_k_f16_f32_data[reduc16], \"main\", 3, sizeof(vk_mat_vec_push_constants), {rm_kq, 1, 1}, {wg_size_subgroup16, rm_kq, i+1}, 1, true, use_subgroups16, force_subgroup_size16);\n-            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f16_f32[w][GGML_TYPE_Q4_K][i], \"mul_mat_vec_q4_k_f16_f32\", arr_dmmv_q4_k_f16_f32_len[reduc16], arr_dmmv_q4_k_f16_f32_data[reduc16], \"main\", 3, sizeof(vk_mat_vec_push_constants), {rm_kq, 1, 1}, {wg_size_subgroup16, rm_kq, i+1}, 1, true, use_subgroups16, force_subgroup_size16);\n-            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f16_f32[w][GGML_TYPE_Q5_K][i], \"mul_mat_vec_q5_k_f16_f32\", arr_dmmv_q5_k_f16_f32_len[reduc16], arr_dmmv_q5_k_f16_f32_data[reduc16], \"main\", 3, sizeof(vk_mat_vec_push_constants), {rm_kq, 1, 1}, {wg_size_subgroup16, rm_kq, i+1}, 1, true, use_subgroups16, force_subgroup_size16);\n-            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f16_f32[w][GGML_TYPE_Q6_K][i], \"mul_mat_vec_q6_k_f16_f32\", arr_dmmv_q6_k_f16_f32_len[reduc16], arr_dmmv_q6_k_f16_f32_data[reduc16], \"main\", 3, sizeof(vk_mat_vec_push_constants), {rm_kq, 1, 1}, {wg_size_subgroup16, rm_kq, i+1}, 1, true, use_subgroups16, force_subgroup_size16);\n-            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f16_f32[w][GGML_TYPE_IQ1_S][i],   \"mul_mat_vec_iq1_s_f16_f32\",   arr_dmmv_iq1_s_f16_f32_len[reduc16],   arr_dmmv_iq1_s_f16_f32_data[reduc16],   \"main\", 3, sizeof(vk_mat_vec_push_constants), {rm_iq, 1, 1}, {wg_size_subgroup16, rm_iq, i+1}, 1, true, use_subgroups16, force_subgroup_size16);\n-            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f16_f32[w][GGML_TYPE_IQ1_M][i],   \"mul_mat_vec_iq1_m_f16_f32\",   arr_dmmv_iq1_m_f16_f32_len[reduc16],   arr_dmmv_iq1_m_f16_f32_data[reduc16],   \"main\", 3, sizeof(vk_mat_vec_push_constants), {rm_iq, 1, 1}, {wg_size_subgroup16, rm_iq, i+1}, 1, true, use_subgroups16, force_subgroup_size16);\n-            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f16_f32[w][GGML_TYPE_IQ2_XXS][i], \"mul_mat_vec_iq2_xxs_f16_f32\", arr_dmmv_iq2_xxs_f16_f32_len[reduc16], arr_dmmv_iq2_xxs_f16_f32_data[reduc16], \"main\", 3, sizeof(vk_mat_vec_push_constants), {rm_iq, 1, 1}, {wg_size_subgroup16, rm_iq, i+1}, 1, true, use_subgroups16, force_subgroup_size16);\n-            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f16_f32[w][GGML_TYPE_IQ2_XS][i],  \"mul_mat_vec_iq2_xs_f16_f32\",  arr_dmmv_iq2_xs_f16_f32_len[reduc16],  arr_dmmv_iq2_xs_f16_f32_data[reduc16],  \"main\", 3, sizeof(vk_mat_vec_push_constants), {rm_iq, 1, 1}, {wg_size_subgroup16, rm_iq, i+1}, 1, true, use_subgroups16, force_subgroup_size16);\n-            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f16_f32[w][GGML_TYPE_IQ2_S][i],   \"mul_mat_vec_iq2_s_f16_f32\",   arr_dmmv_iq2_s_f16_f32_len[reduc16],   arr_dmmv_iq2_s_f16_f32_data[reduc16],   \"main\", 3, sizeof(vk_mat_vec_push_constants), {rm_iq, 1, 1}, {wg_size_subgroup16, rm_iq, i+1}, 1, true, use_subgroups16, force_subgroup_size16);\n-            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f16_f32[w][GGML_TYPE_IQ3_XXS][i], \"mul_mat_vec_iq3_xxs_f16_f32\", arr_dmmv_iq3_xxs_f16_f32_len[reduc16], arr_dmmv_iq3_xxs_f16_f32_data[reduc16], \"main\", 3, sizeof(vk_mat_vec_push_constants), {rm_iq, 1, 1}, {wg_size_subgroup16, rm_iq, i+1}, 1, true, use_subgroups16, force_subgroup_size16);\n-            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f16_f32[w][GGML_TYPE_IQ3_S][i],   \"mul_mat_vec_iq3_s_f16_f32\",   arr_dmmv_iq3_s_f16_f32_len[reduc16],   arr_dmmv_iq3_s_f16_f32_data[reduc16],   \"main\", 3, sizeof(vk_mat_vec_push_constants), {rm_iq, 1, 1}, {wg_size_subgroup16, rm_iq, i+1}, 1, true, use_subgroups16, force_subgroup_size16);\n-            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f16_f32[w][GGML_TYPE_IQ4_XS][i],  \"mul_mat_vec_iq4_xs_f16_f32\",  arr_dmmv_iq4_xs_f16_f32_len[reduc16],  arr_dmmv_iq4_xs_f16_f32_data[reduc16],  \"main\", 3, sizeof(vk_mat_vec_push_constants), {rm_iq, 1, 1}, {wg_size_subgroup16, rm_iq, i+1}, 1, true, use_subgroups16, force_subgroup_size16);\n-            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f16_f32[w][GGML_TYPE_IQ4_NL][i],  \"mul_mat_vec_iq4_nl_f16_f32\",  arr_dmmv_iq4_nl_f16_f32_len[reduc16],  arr_dmmv_iq4_nl_f16_f32_data[reduc16],  \"main\", 3, sizeof(vk_mat_vec_push_constants), {rm_iq, 1, 1}, {wg_size_subgroup16, rm_iq, i+1}, 1, true, use_subgroups16, force_subgroup_size16);\n-            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f16_f32[w][GGML_TYPE_MXFP4][i],   \"mul_mat_vec_mxfp4_f16_f32\",   arr_dmmv_mxfp4_f16_f32_len[reduc16],   arr_dmmv_mxfp4_f16_f32_data[reduc16],   \"main\", 3, sizeof(vk_mat_vec_push_constants), {rm_iq, 1, 1}, {wg_size_subgroup16, rm_iq, i+1}, 1, true, use_subgroups16, force_subgroup_size16);\n+            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f32_f32[w][GGML_TYPE_F32 ][i], \"mul_mat_vec_f32_f32_f32\",  arr_dmmv_f32_f32_f32_len[reduc],  arr_dmmv_f32_f32_f32_data[reduc],  \"main\", 4, sizeof(vk_mat_vec_push_constants), {2, 1, 1}, {wg_size_subgroup, 2, i+1}, 1, false, use_subgroups, force_subgroup_size);\n+            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f32_f32[w][GGML_TYPE_F16 ][i], \"mul_mat_vec_f16_f32_f32\",  arr_dmmv_f16_f32_f32_len[reduc],  arr_dmmv_f16_f32_f32_data[reduc],  \"main\", 4, sizeof(vk_mat_vec_push_constants), {2, 1, 1}, {wg_size_subgroup, 2, i+1}, 1, false, use_subgroups, force_subgroup_size);\n+            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f32_f32[w][GGML_TYPE_BF16][i], \"mul_mat_vec_bf16_f32_f32\", arr_dmmv_bf16_f32_f32_len[reduc], arr_dmmv_bf16_f32_f32_data[reduc], \"main\", 4, sizeof(vk_mat_vec_push_constants), {2, 1, 1}, {wg_size_subgroup, 2, i+1}, 1, false, use_subgroups, force_subgroup_size);\n+            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f32_f32[w][GGML_TYPE_Q4_0][i], \"mul_mat_vec_q4_0_f32_f32\", arr_dmmv_q4_0_f32_f32_len[reduc], arr_dmmv_q4_0_f32_f32_data[reduc], \"main\", 4, sizeof(vk_mat_vec_push_constants), {2*rm_stdq, 1, 1}, {wg_size_subgroup, 2*rm_stdq, i+1}, 1, true, use_subgroups, force_subgroup_size);\n+            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f32_f32[w][GGML_TYPE_Q4_1][i], \"mul_mat_vec_q4_1_f32_f32\", arr_dmmv_q4_1_f32_f32_len[reduc], arr_dmmv_q4_1_f32_f32_data[reduc], \"main\", 4, sizeof(vk_mat_vec_push_constants), {2*rm_stdq, 1, 1}, {wg_size_subgroup, 2*rm_stdq, i+1}, 1, true, use_subgroups, force_subgroup_size);\n+            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f32_f32[w][GGML_TYPE_Q5_0][i], \"mul_mat_vec_q5_0_f32_f32\", arr_dmmv_q5_0_f32_f32_len[reduc], arr_dmmv_q5_0_f32_f32_data[reduc], \"main\", 4, sizeof(vk_mat_vec_push_constants), {2*rm_stdq, 1, 1}, {wg_size_subgroup, 2*rm_stdq, i+1}, 1, true, use_subgroups, force_subgroup_size);\n+            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f32_f32[w][GGML_TYPE_Q5_1][i], \"mul_mat_vec_q5_1_f32_f32\", arr_dmmv_q5_1_f32_f32_len[reduc], arr_dmmv_q5_1_f32_f32_data[reduc], \"main\", 4, sizeof(vk_mat_vec_push_constants), {2*rm_stdq, 1, 1}, {wg_size_subgroup, 2*rm_stdq, i+1}, 1, true, use_subgroups, force_subgroup_size);\n+            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f32_f32[w][GGML_TYPE_Q8_0][i], \"mul_mat_vec_q8_0_f32_f32\", arr_dmmv_q8_0_f32_f32_len[reduc], arr_dmmv_q8_0_f32_f32_data[reduc], \"main\", 4, sizeof(vk_mat_vec_push_constants), {1*rm_stdq, 1, 1}, {wg_size_subgroup, 1*rm_stdq, i+1}, 1, true, use_subgroups, force_subgroup_size);\n+            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f32_f32[w][GGML_TYPE_Q2_K][i], \"mul_mat_vec_q2_k_f32_f32\", arr_dmmv_q2_k_f32_f32_len[reduc16], arr_dmmv_q2_k_f32_f32_data[reduc16], \"main\", 4, sizeof(vk_mat_vec_push_constants), {rm_kq, 1, 1}, {wg_size_subgroup16, rm_kq, i+1}, 1, true, use_subgroups16, force_subgroup_size16);\n+            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f32_f32[w][GGML_TYPE_Q3_K][i], \"mul_mat_vec_q3_k_f32_f32\", arr_dmmv_q3_k_f32_f32_len[reduc16], arr_dmmv_q3_k_f32_f32_data[reduc16], \"main\", 4, sizeof(vk_mat_vec_push_constants), {rm_kq, 1, 1}, {wg_size_subgroup16, rm_kq, i+1}, 1, true, use_subgroups16, force_subgroup_size16);\n+            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f32_f32[w][GGML_TYPE_Q4_K][i], \"mul_mat_vec_q4_k_f32_f32\", arr_dmmv_q4_k_f32_f32_len[reduc16], arr_dmmv_q4_k_f32_f32_data[reduc16], \"main\", 4, sizeof(vk_mat_vec_push_constants), {rm_kq, 1, 1}, {wg_size_subgroup16, rm_kq, i+1}, 1, true, use_subgroups16, force_subgroup_size16);\n+            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f32_f32[w][GGML_TYPE_Q5_K][i], \"mul_mat_vec_q5_k_f32_f32\", arr_dmmv_q5_k_f32_f32_len[reduc16], arr_dmmv_q5_k_f32_f32_data[reduc16], \"main\", 4, sizeof(vk_mat_vec_push_constants), {rm_kq, 1, 1}, {wg_size_subgroup16, rm_kq, i+1}, 1, true, use_subgroups16, force_subgroup_size16);\n+            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f32_f32[w][GGML_TYPE_Q6_K][i], \"mul_mat_vec_q6_k_f32_f32\", arr_dmmv_q6_k_f32_f32_len[reduc16], arr_dmmv_q6_k_f32_f32_data[reduc16], \"main\", 4, sizeof(vk_mat_vec_push_constants), {rm_kq, 1, 1}, {wg_size_subgroup16, rm_kq, i+1}, 1, true, use_subgroups16, force_subgroup_size16);\n+            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f32_f32[w][GGML_TYPE_IQ1_S][i],   \"mul_mat_vec_iq1_s_f32_f32\",   arr_dmmv_iq1_s_f32_f32_len[reduc16],   arr_dmmv_iq1_s_f32_f32_data[reduc16],   \"main\", 4, sizeof(vk_mat_vec_push_constants), {rm_iq, 1, 1}, {wg_size_subgroup16, rm_iq, i+1}, 1, true, use_subgroups16, force_subgroup_size16);\n+            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f32_f32[w][GGML_TYPE_IQ1_M][i],   \"mul_mat_vec_iq1_m_f32_f32\",   arr_dmmv_iq1_m_f32_f32_len[reduc16],   arr_dmmv_iq1_m_f32_f32_data[reduc16],   \"main\", 4, sizeof(vk_mat_vec_push_constants), {rm_iq, 1, 1}, {wg_size_subgroup16, rm_iq, i+1}, 1, true, use_subgroups16, force_subgroup_size16);\n+            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f32_f32[w][GGML_TYPE_IQ2_XXS][i], \"mul_mat_vec_iq2_xxs_f32_f32\", arr_dmmv_iq2_xxs_f32_f32_len[reduc16], arr_dmmv_iq2_xxs_f32_f32_data[reduc16], \"main\", 4, sizeof(vk_mat_vec_push_constants), {rm_iq, 1, 1}, {wg_size_subgroup16, rm_iq, i+1}, 1, true, use_subgroups16, force_subgroup_size16);\n+            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f32_f32[w][GGML_TYPE_IQ2_XS][i],  \"mul_mat_vec_iq2_xs_f32_f32\",  arr_dmmv_iq2_xs_f32_f32_len[reduc16],  arr_dmmv_iq2_xs_f32_f32_data[reduc16],  \"main\", 4, sizeof(vk_mat_vec_push_constants), {rm_iq, 1, 1}, {wg_size_subgroup16, rm_iq, i+1}, 1, true, use_subgroups16, force_subgroup_size16);\n+            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f32_f32[w][GGML_TYPE_IQ2_S][i],   \"mul_mat_vec_iq2_s_f32_f32\",   arr_dmmv_iq2_s_f32_f32_len[reduc16],   arr_dmmv_iq2_s_f32_f32_data[reduc16],   \"main\", 4, sizeof(vk_mat_vec_push_constants), {rm_iq, 1, 1}, {wg_size_subgroup16, rm_iq, i+1}, 1, true, use_subgroups16, force_subgroup_size16);\n+            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f32_f32[w][GGML_TYPE_IQ3_XXS][i], \"mul_mat_vec_iq3_xxs_f32_f32\", arr_dmmv_iq3_xxs_f32_f32_len[reduc16], arr_dmmv_iq3_xxs_f32_f32_data[reduc16], \"main\", 4, sizeof(vk_mat_vec_push_constants), {rm_iq, 1, 1}, {wg_size_subgroup16, rm_iq, i+1}, 1, true, use_subgroups16, force_subgroup_size16);\n+            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f32_f32[w][GGML_TYPE_IQ3_S][i],   \"mul_mat_vec_iq3_s_f32_f32\",   arr_dmmv_iq3_s_f32_f32_len[reduc16],   arr_dmmv_iq3_s_f32_f32_data[reduc16],   \"main\", 4, sizeof(vk_mat_vec_push_constants), {rm_iq, 1, 1}, {wg_size_subgroup16, rm_iq, i+1}, 1, true, use_subgroups16, force_subgroup_size16);\n+            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f32_f32[w][GGML_TYPE_IQ4_XS][i],  \"mul_mat_vec_iq4_xs_f32_f32\",  arr_dmmv_iq4_xs_f32_f32_len[reduc16],  arr_dmmv_iq4_xs_f32_f32_data[reduc16],  \"main\", 4, sizeof(vk_mat_vec_push_constants), {rm_iq, 1, 1}, {wg_size_subgroup16, rm_iq, i+1}, 1, true, use_subgroups16, force_subgroup_size16);\n+            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f32_f32[w][GGML_TYPE_IQ4_NL][i],  \"mul_mat_vec_iq4_nl_f32_f32\",  arr_dmmv_iq4_nl_f32_f32_len[reduc16],  arr_dmmv_iq4_nl_f32_f32_data[reduc16],  \"main\", 4, sizeof(vk_mat_vec_push_constants), {rm_iq, 1, 1}, {wg_size_subgroup16, rm_iq, i+1}, 1, true, use_subgroups16, force_subgroup_size16);\n+            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f32_f32[w][GGML_TYPE_MXFP4][i],   \"mul_mat_vec_mxfp4_f32_f32\",   arr_dmmv_mxfp4_f32_f32_len[reduc16],   arr_dmmv_mxfp4_f32_f32_data[reduc16],   \"main\", 4, sizeof(vk_mat_vec_push_constants), {rm_iq, 1, 1}, {wg_size_subgroup16, rm_iq, i+1}, 1, true, use_subgroups16, force_subgroup_size16);\n+\n+            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f16_f32[w][GGML_TYPE_F32 ][i], \"mul_mat_vec_f32_f16_f32\",  arr_dmmv_f32_f16_f32_len[reduc],  arr_dmmv_f32_f16_f32_data[reduc],  \"main\", 4, sizeof(vk_mat_vec_push_constants), {2, 1, 1}, {wg_size_subgroup, 2, i+1}, 1, false, use_subgroups, force_subgroup_size);\n+            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f16_f32[w][GGML_TYPE_F16 ][i], \"mul_mat_vec_f16_f16_f32\",  arr_dmmv_f16_f16_f32_len[reduc],  arr_dmmv_f16_f16_f32_data[reduc],  \"main\", 4, sizeof(vk_mat_vec_push_constants), {2, 1, 1}, {wg_size_subgroup, 2, i+1}, 1, false, use_subgroups, force_subgroup_size);\n+            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f16_f32[w][GGML_TYPE_BF16][i], \"mul_mat_vec_bf16_f16_f32\", arr_dmmv_bf16_f16_f32_len[reduc], arr_dmmv_bf16_f16_f32_data[reduc], \"main\", 4, sizeof(vk_mat_vec_push_constants), {2, 1, 1}, {wg_size_subgroup, 2, i+1}, 1, false, use_subgroups, force_subgroup_size);\n+            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f16_f32[w][GGML_TYPE_Q4_0][i], \"mul_mat_vec_q4_0_f16_f32\", arr_dmmv_q4_0_f16_f32_len[reduc], arr_dmmv_q4_0_f16_f32_data[reduc], \"main\", 4, sizeof(vk_mat_vec_push_constants), {2*rm_stdq, 1, 1}, {wg_size_subgroup, 2*rm_stdq, i+1}, 1, true, use_subgroups, force_subgroup_size);\n+            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f16_f32[w][GGML_TYPE_Q4_1][i], \"mul_mat_vec_q4_1_f16_f32\", arr_dmmv_q4_1_f16_f32_len[reduc], arr_dmmv_q4_1_f16_f32_data[reduc], \"main\", 4, sizeof(vk_mat_vec_push_constants), {2*rm_stdq, 1, 1}, {wg_size_subgroup, 2*rm_stdq, i+1}, 1, true, use_subgroups, force_subgroup_size);\n+            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f16_f32[w][GGML_TYPE_Q5_0][i], \"mul_mat_vec_q5_0_f16_f32\", arr_dmmv_q5_0_f16_f32_len[reduc], arr_dmmv_q5_0_f16_f32_data[reduc], \"main\", 4, sizeof(vk_mat_vec_push_constants), {2*rm_stdq, 1, 1}, {wg_size_subgroup, 2*rm_stdq, i+1}, 1, true, use_subgroups, force_subgroup_size);\n+            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f16_f32[w][GGML_TYPE_Q5_1][i], \"mul_mat_vec_q5_1_f16_f32\", arr_dmmv_q5_1_f16_f32_len[reduc], arr_dmmv_q5_1_f16_f32_data[reduc], \"main\", 4, sizeof(vk_mat_vec_push_constants), {2*rm_stdq, 1, 1}, {wg_size_subgroup, 2*rm_stdq, i+1}, 1, true, use_subgroups, force_subgroup_size);\n+            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f16_f32[w][GGML_TYPE_Q8_0][i], \"mul_mat_vec_q8_0_f16_f32\", arr_dmmv_q8_0_f16_f32_len[reduc], arr_dmmv_q8_0_f16_f32_data[reduc], \"main\", 4, sizeof(vk_mat_vec_push_constants), {1*rm_stdq, 1, 1}, {wg_size_subgroup, 1*rm_stdq, i+1}, 1, true, use_subgroups, force_subgroup_size);\n+            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f16_f32[w][GGML_TYPE_Q2_K][i], \"mul_mat_vec_q2_k_f16_f32\", arr_dmmv_q2_k_f16_f32_len[reduc16], arr_dmmv_q2_k_f16_f32_data[reduc16], \"main\", 4, sizeof(vk_mat_vec_push_constants), {rm_kq, 1, 1}, {wg_size_subgroup16, rm_kq, i+1}, 1, true, use_subgroups16, force_subgroup_size16);\n+            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f16_f32[w][GGML_TYPE_Q3_K][i], \"mul_mat_vec_q3_k_f16_f32\", arr_dmmv_q3_k_f16_f32_len[reduc16], arr_dmmv_q3_k_f16_f32_data[reduc16], \"main\", 4, sizeof(vk_mat_vec_push_constants), {rm_kq, 1, 1}, {wg_size_subgroup16, rm_kq, i+1}, 1, true, use_subgroups16, force_subgroup_size16);\n+            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f16_f32[w][GGML_TYPE_Q4_K][i], \"mul_mat_vec_q4_k_f16_f32\", arr_dmmv_q4_k_f16_f32_len[reduc16], arr_dmmv_q4_k_f16_f32_data[reduc16], \"main\", 4, sizeof(vk_mat_vec_push_constants), {rm_kq, 1, 1}, {wg_size_subgroup16, rm_kq, i+1}, 1, true, use_subgroups16, force_subgroup_size16);\n+            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f16_f32[w][GGML_TYPE_Q5_K][i], \"mul_mat_vec_q5_k_f16_f32\", arr_dmmv_q5_k_f16_f32_len[reduc16], arr_dmmv_q5_k_f16_f32_data[reduc16], \"main\", 4, sizeof(vk_mat_vec_push_constants), {rm_kq, 1, 1}, {wg_size_subgroup16, rm_kq, i+1}, 1, true, use_subgroups16, force_subgroup_size16);\n+            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f16_f32[w][GGML_TYPE_Q6_K][i], \"mul_mat_vec_q6_k_f16_f32\", arr_dmmv_q6_k_f16_f32_len[reduc16], arr_dmmv_q6_k_f16_f32_data[reduc16], \"main\", 4, sizeof(vk_mat_vec_push_constants), {rm_kq, 1, 1}, {wg_size_subgroup16, rm_kq, i+1}, 1, true, use_subgroups16, force_subgroup_size16);\n+            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f16_f32[w][GGML_TYPE_IQ1_S][i],   \"mul_mat_vec_iq1_s_f16_f32\",   arr_dmmv_iq1_s_f16_f32_len[reduc16],   arr_dmmv_iq1_s_f16_f32_data[reduc16],   \"main\", 4, sizeof(vk_mat_vec_push_constants), {rm_iq, 1, 1}, {wg_size_subgroup16, rm_iq, i+1}, 1, true, use_subgroups16, force_subgroup_size16);\n+            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f16_f32[w][GGML_TYPE_IQ1_M][i],   \"mul_mat_vec_iq1_m_f16_f32\",   arr_dmmv_iq1_m_f16_f32_len[reduc16],   arr_dmmv_iq1_m_f16_f32_data[reduc16],   \"main\", 4, sizeof(vk_mat_vec_push_constants), {rm_iq, 1, 1}, {wg_size_subgroup16, rm_iq, i+1}, 1, true, use_subgroups16, force_subgroup_size16);\n+            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f16_f32[w][GGML_TYPE_IQ2_XXS][i], \"mul_mat_vec_iq2_xxs_f16_f32\", arr_dmmv_iq2_xxs_f16_f32_len[reduc16], arr_dmmv_iq2_xxs_f16_f32_data[reduc16], \"main\", 4, sizeof(vk_mat_vec_push_constants), {rm_iq, 1, 1}, {wg_size_subgroup16, rm_iq, i+1}, 1, true, use_subgroups16, force_subgroup_size16);\n+            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f16_f32[w][GGML_TYPE_IQ2_XS][i],  \"mul_mat_vec_iq2_xs_f16_f32\",  arr_dmmv_iq2_xs_f16_f32_len[reduc16],  arr_dmmv_iq2_xs_f16_f32_data[reduc16],  \"main\", 4, sizeof(vk_mat_vec_push_constants), {rm_iq, 1, 1}, {wg_size_subgroup16, rm_iq, i+1}, 1, true, use_subgroups16, force_subgroup_size16);\n+            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f16_f32[w][GGML_TYPE_IQ2_S][i],   \"mul_mat_vec_iq2_s_f16_f32\",   arr_dmmv_iq2_s_f16_f32_len[reduc16],   arr_dmmv_iq2_s_f16_f32_data[reduc16],   \"main\", 4, sizeof(vk_mat_vec_push_constants), {rm_iq, 1, 1}, {wg_size_subgroup16, rm_iq, i+1}, 1, true, use_subgroups16, force_subgroup_size16);\n+            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f16_f32[w][GGML_TYPE_IQ3_XXS][i], \"mul_mat_vec_iq3_xxs_f16_f32\", arr_dmmv_iq3_xxs_f16_f32_len[reduc16], arr_dmmv_iq3_xxs_f16_f32_data[reduc16], \"main\", 4, sizeof(vk_mat_vec_push_constants), {rm_iq, 1, 1}, {wg_size_subgroup16, rm_iq, i+1}, 1, true, use_subgroups16, force_subgroup_size16);\n+            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f16_f32[w][GGML_TYPE_IQ3_S][i],   \"mul_mat_vec_iq3_s_f16_f32\",   arr_dmmv_iq3_s_f16_f32_len[reduc16],   arr_dmmv_iq3_s_f16_f32_data[reduc16],   \"main\", 4, sizeof(vk_mat_vec_push_constants), {rm_iq, 1, 1}, {wg_size_subgroup16, rm_iq, i+1}, 1, true, use_subgroups16, force_subgroup_size16);\n+            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f16_f32[w][GGML_TYPE_IQ4_XS][i],  \"mul_mat_vec_iq4_xs_f16_f32\",  arr_dmmv_iq4_xs_f16_f32_len[reduc16],  arr_dmmv_iq4_xs_f16_f32_data[reduc16],  \"main\", 4, sizeof(vk_mat_vec_push_constants), {rm_iq, 1, 1}, {wg_size_subgroup16, rm_iq, i+1}, 1, true, use_subgroups16, force_subgroup_size16);\n+            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f16_f32[w][GGML_TYPE_IQ4_NL][i],  \"mul_mat_vec_iq4_nl_f16_f32\",  arr_dmmv_iq4_nl_f16_f32_len[reduc16],  arr_dmmv_iq4_nl_f16_f32_data[reduc16],  \"main\", 4, sizeof(vk_mat_vec_push_constants), {rm_iq, 1, 1}, {wg_size_subgroup16, rm_iq, i+1}, 1, true, use_subgroups16, force_subgroup_size16);\n+            ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_f16_f32[w][GGML_TYPE_MXFP4][i],   \"mul_mat_vec_mxfp4_f16_f32\",   arr_dmmv_mxfp4_f16_f32_len[reduc16],   arr_dmmv_mxfp4_f16_f32_data[reduc16],   \"main\", 4, sizeof(vk_mat_vec_push_constants), {rm_iq, 1, 1}, {wg_size_subgroup16, rm_iq, i+1}, 1, true, use_subgroups16, force_subgroup_size16);\n \n #if defined(GGML_VULKAN_INTEGER_DOT_GLSLC_SUPPORT)\n             if (device->integer_dot_product) {\n                 const uint32_t subgroup_size_int = (device->vendor_id == VK_VENDOR_ID_INTEL && device->subgroup_size_control) ? device->subgroup_min_size : device->subgroup_size;\n                 const uint32_t wg_size_subgroup_int = (w == DMMV_WG_SIZE_SUBGROUP) ? subgroup_size_int : (subgroup_size_int * 4);\n \n-                ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_q8_1_f32[w][GGML_TYPE_Q4_0][i], \"mul_mat_vec_q4_0_q8_1_f32\", arr_dmmv_q4_0_q8_1_f32_len[reduc], arr_dmmv_q4_0_q8_1_f32_data[reduc], \"main\", 3, sizeof(vk_mat_vec_push_constants), {2*rm_stdq, 1, 1}, {wg_size_subgroup_int, 2*rm_stdq, i+1}, 1, true, use_subgroups, subgroup_size_int);\n-                ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_q8_1_f32[w][GGML_TYPE_Q4_1][i], \"mul_mat_vec_q4_1_q8_1_f32\", arr_dmmv_q4_1_q8_1_f32_len[reduc], arr_dmmv_q4_1_q8_1_f32_data[reduc], \"main\", 3, sizeof(vk_mat_vec_push_constants), {2*rm_stdq, 1, 1}, {wg_size_subgroup_int, 2*rm_stdq, i+1}, 1, true, use_subgroups, subgroup_size_int);\n-                ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_q8_1_f32[w][GGML_TYPE_Q5_0][i], \"mul_mat_vec_q5_0_q8_1_f32\", arr_dmmv_q5_0_q8_1_f32_len[reduc], arr_dmmv_q5_0_q8_1_f32_data[reduc], \"main\", 3, sizeof(vk_mat_vec_push_constants), {2*rm_stdq, 1, 1}, {wg_size_subgroup_int, 2*rm_stdq, i+1}, 1, true, use_subgroups, subgroup_size_int);\n-                ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_q8_1_f32[w][GGML_TYPE_Q5_1][i], \"mul_mat_vec_q5_1_q8_1_f32\", arr_dmmv_q5_1_q8_1_f32_len[reduc], arr_dmmv_q5_1_q8_1_f32_data[reduc], \"main\", 3, sizeof(vk_mat_vec_push_constants), {2*rm_stdq, 1, 1}, {wg_size_subgroup_int, 2*rm_stdq, i+1}, 1, true, use_subgroups, subgroup_size_int);\n-                ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_q8_1_f32[w][GGML_TYPE_Q8_0][i], \"mul_mat_vec_q8_0_q8_1_f32\", arr_dmmv_q8_0_q8_1_f32_len[reduc], arr_dmmv_q8_0_q8_1_f32_data[reduc], \"main\", 3, sizeof(vk_mat_vec_push_constants), {1*rm_stdq, 1, 1}, {wg_size_subgroup_int, 1*rm_stdq, i+1}, 1, true, use_subgroups, subgroup_size_int);\n+                ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_q8_1_f32[w][GGML_TYPE_Q4_0][i], \"mul_mat_vec_q4_0_q8_1_f32\", arr_dmmv_q4_0_q8_1_f32_len[reduc], arr_dmmv_q4_0_q8_1_f32_data[reduc], \"main\", 4, sizeof(vk_mat_vec_push_constants), {2*rm_stdq, 1, 1}, {wg_size_subgroup_int, 2*rm_stdq, i+1}, 1, true, use_subgroups, subgroup_size_int);\n+                ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_q8_1_f32[w][GGML_TYPE_Q4_1][i], \"mul_mat_vec_q4_1_q8_1_f32\", arr_dmmv_q4_1_q8_1_f32_len[reduc], arr_dmmv_q4_1_q8_1_f32_data[reduc], \"main\", 4, sizeof(vk_mat_vec_push_constants), {2*rm_stdq, 1, 1}, {wg_size_subgroup_int, 2*rm_stdq, i+1}, 1, true, use_subgroups, subgroup_size_int);\n+                ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_q8_1_f32[w][GGML_TYPE_Q5_0][i], \"mul_mat_vec_q5_0_q8_1_f32\", arr_dmmv_q5_0_q8_1_f32_len[reduc], arr_dmmv_q5_0_q8_1_f32_data[reduc], \"main\", 4, sizeof(vk_mat_vec_push_constants), {2*rm_stdq, 1, 1}, {wg_size_subgroup_int, 2*rm_stdq, i+1}, 1, true, use_subgroups, subgroup_size_int);\n+                ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_q8_1_f32[w][GGML_TYPE_Q5_1][i], \"mul_mat_vec_q5_1_q8_1_f32\", arr_dmmv_q5_1_q8_1_f32_len[reduc], arr_dmmv_q5_1_q8_1_f32_data[reduc], \"main\", 4, sizeof(vk_mat_vec_push_constants), {2*rm_stdq, 1, 1}, {wg_size_subgroup_int, 2*rm_stdq, i+1}, 1, true, use_subgroups, subgroup_size_int);\n+                ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_q8_1_f32[w][GGML_TYPE_Q8_0][i], \"mul_mat_vec_q8_0_q8_1_f32\", arr_dmmv_q8_0_q8_1_f32_len[reduc], arr_dmmv_q8_0_q8_1_f32_data[reduc], \"main\", 4, sizeof(vk_mat_vec_push_constants), {1*rm_stdq, 1, 1}, {wg_size_subgroup_int, 1*rm_stdq, i+1}, 1, true, use_subgroups, subgroup_size_int);\n             }\n #endif // GGML_VULKAN_INTEGER_DOT_GLSLC_SUPPORT\n         }\n     }\n \n-    ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_id_f32[GGML_TYPE_F32 ], \"mul_mat_vec_id_f32_f32\",  mul_mat_vec_id_f32_f32_len,  mul_mat_vec_id_f32_f32_data,  \"main\", 4, sizeof(vk_mat_vec_id_push_constants), {2, 1, 1}, {device->subgroup_size, 2}, 1);\n-    ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_id_f32[GGML_TYPE_F16 ], \"mul_mat_vec_id_f16_f32\",  mul_mat_vec_id_f16_f32_len,  mul_mat_vec_id_f16_f32_data,  \"main\", 4, sizeof(vk_mat_vec_id_push_constants), {2, 1, 1}, {device->subgroup_size, 2}, 1);\n-    ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_id_f32[GGML_TYPE_BF16], \"mul_mat_vec_id_bf16_f32\", mul_mat_vec_id_bf16_f32_len, mul_mat_vec_id_bf16_f32_data, \"main\", 4, sizeof(vk_mat_vec_id_push_constants), {2, 1, 1}, {device->subgroup_size, 2}, 1);\n-    ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_id_f32[GGML_TYPE_Q4_0], \"mul_mat_vec_id_q4_0_f32\", mul_mat_vec_id_q4_0_f32_len, mul_mat_vec_id_q4_0_f32_data, \"main\", 4, sizeof(vk_mat_vec_id_push_constants), {2*rm_stdq, 1, 1}, {device->subgroup_size, 2*rm_stdq}, 1, true);\n-    ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_id_f32[GGML_TYPE_Q4_1], \"mul_mat_vec_id_q4_1_f32\", mul_mat_vec_id_q4_1_f32_len, mul_mat_vec_id_q4_1_f32_data, \"main\", 4, sizeof(vk_mat_vec_id_push_constants), {2*rm_stdq, 1, 1}, {device->subgroup_size, 2*rm_stdq}, 1, true);\n-    ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_id_f32[GGML_TYPE_Q5_0], \"mul_mat_vec_id_q5_0_f32\", mul_mat_vec_id_q5_0_f32_len, mul_mat_vec_id_q5_0_f32_data, \"main\", 4, sizeof(vk_mat_vec_id_push_constants), {2*rm_stdq, 1, 1}, {device->subgroup_size, 2*rm_stdq}, 1, true);\n-    ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_id_f32[GGML_TYPE_Q5_1], \"mul_mat_vec_id_q5_1_f32\", mul_mat_vec_id_q5_1_f32_len, mul_mat_vec_id_q5_1_f32_data, \"main\", 4, sizeof(vk_mat_vec_id_push_constants), {2*rm_stdq, 1, 1}, {device->subgroup_size, 2*rm_stdq}, 1, true);\n-    ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_id_f32[GGML_TYPE_Q8_0], \"mul_mat_vec_id_q8_0_f32\", mul_mat_vec_id_q8_0_f32_len, mul_mat_vec_id_q8_0_f32_data, \"main\", 4, sizeof(vk_mat_vec_id_push_constants), {1*rm_stdq, 1, 1}, {device->subgroup_size, 1*rm_stdq}, 1, true);\n-    ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_id_f32[GGML_TYPE_Q2_K], \"mul_mat_vec_id_q2_k_f32\", mul_mat_vec_id_q2_k_f32_len, mul_mat_vec_id_q2_k_f32_data, \"main\", 4, sizeof(vk_mat_vec_id_push_constants), {rm_kq, 1, 1}, {subgroup_size_16, rm_kq}, 1, true);\n-    ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_id_f32[GGML_TYPE_Q3_K], \"mul_mat_vec_id_q3_k_f32\", mul_mat_vec_id_q3_k_f32_len, mul_mat_vec_id_q3_k_f32_data, \"main\", 4, sizeof(vk_mat_vec_id_push_constants), {rm_kq, 1, 1}, {subgroup_size_16, rm_kq}, 1, true);\n-    ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_id_f32[GGML_TYPE_Q4_K], \"mul_mat_vec_id_q4_k_f32\", mul_mat_vec_id_q4_k_f32_len, mul_mat_vec_id_q4_k_f32_data, \"main\", 4, sizeof(vk_mat_vec_id_push_constants), {rm_kq, 1, 1}, {subgroup_size_16, rm_kq}, 1, true);\n-    ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_id_f32[GGML_TYPE_Q5_K], \"mul_mat_vec_id_q5_k_f32\", mul_mat_vec_id_q5_k_f32_len, mul_mat_vec_id_q5_k_f32_data, \"main\", 4, sizeof(vk_mat_vec_id_push_constants), {rm_kq, 1, 1}, {subgroup_size_16, rm_kq}, 1, true);\n-    ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_id_f32[GGML_TYPE_Q6_K], \"mul_mat_vec_id_q6_k_f32\", mul_mat_vec_id_q6_k_f32_len, mul_mat_vec_id_q6_k_f32_data, \"main\", 4, sizeof(vk_mat_vec_id_push_constants), {rm_kq, 1, 1}, {subgroup_size_16, rm_kq}, 1, true);\n-    ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_id_f32[GGML_TYPE_IQ1_S],   \"mul_mat_vec_id_iq1_s_f32\",   mul_mat_vec_id_iq1_s_f32_len,   mul_mat_vec_id_iq1_s_f32_data,   \"main\", 4, sizeof(vk_mat_vec_id_push_constants), {rm_iq, 1, 1}, {subgroup_size_16, rm_iq}, 1, true);\n-    ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_id_f32[GGML_TYPE_IQ1_M],   \"mul_mat_vec_id_iq1_m_f32\",   mul_mat_vec_id_iq1_m_f32_len,   mul_mat_vec_id_iq1_m_f32_data,   \"main\", 4, sizeof(vk_mat_vec_id_push_constants), {rm_iq, 1, 1}, {subgroup_size_16, rm_iq}, 1, true);\n-    ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_id_f32[GGML_TYPE_IQ2_XXS], \"mul_mat_vec_id_iq2_xxs_f32\", mul_mat_vec_id_iq2_xxs_f32_len, mul_mat_vec_id_iq2_xxs_f32_data, \"main\", 4, sizeof(vk_mat_vec_id_push_constants), {rm_iq, 1, 1}, {subgroup_size_16, rm_iq}, 1, true);\n-    ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_id_f32[GGML_TYPE_IQ2_XS],  \"mul_mat_vec_id_iq2_xs_f32\",  mul_mat_vec_id_iq2_xs_f32_len,  mul_mat_vec_id_iq2_xs_f32_data,  \"main\", 4, sizeof(vk_mat_vec_id_push_constants), {rm_iq, 1, 1}, {subgroup_size_16, rm_iq}, 1, true);\n-    ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_id_f32[GGML_TYPE_IQ2_S],   \"mul_mat_vec_id_iq2_s_f32\",   mul_mat_vec_id_iq2_s_f32_len,   mul_mat_vec_id_iq2_s_f32_data,   \"main\", 4, sizeof(vk_mat_vec_id_push_constants), {rm_iq, 1, 1}, {subgroup_size_16, rm_iq}, 1, true);\n-    ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_id_f32[GGML_TYPE_IQ3_XXS], \"mul_mat_vec_id_iq3_xxs_f32\", mul_mat_vec_id_iq3_xxs_f32_len, mul_mat_vec_id_iq3_xxs_f32_data, \"main\", 4, sizeof(vk_mat_vec_id_push_constants), {rm_iq, 1, 1}, {subgroup_size_16, rm_iq}, 1, true);\n-    ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_id_f32[GGML_TYPE_IQ3_S],   \"mul_mat_vec_id_iq3_s_f32\",   mul_mat_vec_id_iq3_s_f32_len,   mul_mat_vec_id_iq3_s_f32_data,   \"main\", 4, sizeof(vk_mat_vec_id_push_constants), {rm_iq, 1, 1}, {subgroup_size_16, rm_iq}, 1, true);\n-    ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_id_f32[GGML_TYPE_IQ4_XS],  \"mul_mat_vec_id_iq4_xs_f32\",  mul_mat_vec_id_iq4_xs_f32_len,  mul_mat_vec_id_iq4_xs_f32_data,  \"main\", 4, sizeof(vk_mat_vec_id_push_constants), {rm_iq, 1, 1}, {subgroup_size_16, rm_iq}, 1, true);\n-    ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_id_f32[GGML_TYPE_IQ4_NL],  \"mul_mat_vec_id_iq4_nl_f32\",  mul_mat_vec_id_iq4_nl_f32_len,  mul_mat_vec_id_iq4_nl_f32_data,  \"main\", 4, sizeof(vk_mat_vec_id_push_constants), {rm_iq, 1, 1}, {subgroup_size_16, rm_iq}, 1, true);\n-    ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_id_f32[GGML_TYPE_MXFP4],   \"mul_mat_vec_id_mxfp4_f32\",   mul_mat_vec_id_mxfp4_f32_len,   mul_mat_vec_id_mxfp4_f32_data,   \"main\", 4, sizeof(vk_mat_vec_id_push_constants), {rm_iq, 1, 1}, {subgroup_size_16, rm_iq}, 1, true);\n+    ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_id_f32[GGML_TYPE_F32 ], \"mul_mat_vec_id_f32_f32\",  mul_mat_vec_id_f32_f32_len,  mul_mat_vec_id_f32_f32_data,  \"main\", 5, sizeof(vk_mat_vec_id_push_constants), {2, 1, 1}, {device->subgroup_size, 2}, 1);\n+    ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_id_f32[GGML_TYPE_F16 ], \"mul_mat_vec_id_f16_f32\",  mul_mat_vec_id_f16_f32_len,  mul_mat_vec_id_f16_f32_data,  \"main\", 5, sizeof(vk_mat_vec_id_push_constants), {2, 1, 1}, {device->subgroup_size, 2}, 1);\n+    ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_id_f32[GGML_TYPE_BF16], \"mul_mat_vec_id_bf16_f32\", mul_mat_vec_id_bf16_f32_len, mul_mat_vec_id_bf16_f32_data, \"main\", 5, sizeof(vk_mat_vec_id_push_constants), {2, 1, 1}, {device->subgroup_size, 2}, 1);\n+    ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_id_f32[GGML_TYPE_Q4_0], \"mul_mat_vec_id_q4_0_f32\", mul_mat_vec_id_q4_0_f32_len, mul_mat_vec_id_q4_0_f32_data, \"main\", 5, sizeof(vk_mat_vec_id_push_constants), {2*rm_stdq, 1, 1}, {device->subgroup_size, 2*rm_stdq}, 1, true);\n+    ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_id_f32[GGML_TYPE_Q4_1], \"mul_mat_vec_id_q4_1_f32\", mul_mat_vec_id_q4_1_f32_len, mul_mat_vec_id_q4_1_f32_data, \"main\", 5, sizeof(vk_mat_vec_id_push_constants), {2*rm_stdq, 1, 1}, {device->subgroup_size, 2*rm_stdq}, 1, true);\n+    ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_id_f32[GGML_TYPE_Q5_0], \"mul_mat_vec_id_q5_0_f32\", mul_mat_vec_id_q5_0_f32_len, mul_mat_vec_id_q5_0_f32_data, \"main\", 5, sizeof(vk_mat_vec_id_push_constants), {2*rm_stdq, 1, 1}, {device->subgroup_size, 2*rm_stdq}, 1, true);\n+    ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_id_f32[GGML_TYPE_Q5_1], \"mul_mat_vec_id_q5_1_f32\", mul_mat_vec_id_q5_1_f32_len, mul_mat_vec_id_q5_1_f32_data, \"main\", 5, sizeof(vk_mat_vec_id_push_constants), {2*rm_stdq, 1, 1}, {device->subgroup_size, 2*rm_stdq}, 1, true);\n+    ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_id_f32[GGML_TYPE_Q8_0], \"mul_mat_vec_id_q8_0_f32\", mul_mat_vec_id_q8_0_f32_len, mul_mat_vec_id_q8_0_f32_data, \"main\", 5, sizeof(vk_mat_vec_id_push_constants), {1*rm_stdq, 1, 1}, {device->subgroup_size, 1*rm_stdq}, 1, true);\n+    ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_id_f32[GGML_TYPE_Q2_K], \"mul_mat_vec_id_q2_k_f32\", mul_mat_vec_id_q2_k_f32_len, mul_mat_vec_id_q2_k_f32_data, \"main\", 5, sizeof(vk_mat_vec_id_push_constants), {rm_kq, 1, 1}, {subgroup_size_16, rm_kq}, 1, true);\n+    ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_id_f32[GGML_TYPE_Q3_K], \"mul_mat_vec_id_q3_k_f32\", mul_mat_vec_id_q3_k_f32_len, mul_mat_vec_id_q3_k_f32_data, \"main\", 5, sizeof(vk_mat_vec_id_push_constants), {rm_kq, 1, 1}, {subgroup_size_16, rm_kq}, 1, true);\n+    ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_id_f32[GGML_TYPE_Q4_K], \"mul_mat_vec_id_q4_k_f32\", mul_mat_vec_id_q4_k_f32_len, mul_mat_vec_id_q4_k_f32_data, \"main\", 5, sizeof(vk_mat_vec_id_push_constants), {rm_kq, 1, 1}, {subgroup_size_16, rm_kq}, 1, true);\n+    ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_id_f32[GGML_TYPE_Q5_K], \"mul_mat_vec_id_q5_k_f32\", mul_mat_vec_id_q5_k_f32_len, mul_mat_vec_id_q5_k_f32_data, \"main\", 5, sizeof(vk_mat_vec_id_push_constants), {rm_kq, 1, 1}, {subgroup_size_16, rm_kq}, 1, true);\n+    ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_id_f32[GGML_TYPE_Q6_K], \"mul_mat_vec_id_q6_k_f32\", mul_mat_vec_id_q6_k_f32_len, mul_mat_vec_id_q6_k_f32_data, \"main\", 5, sizeof(vk_mat_vec_id_push_constants), {rm_kq, 1, 1}, {subgroup_size_16, rm_kq}, 1, true);\n+    ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_id_f32[GGML_TYPE_IQ1_S],   \"mul_mat_vec_id_iq1_s_f32\",   mul_mat_vec_id_iq1_s_f32_len,   mul_mat_vec_id_iq1_s_f32_data,   \"main\", 5, sizeof(vk_mat_vec_id_push_constants), {rm_iq, 1, 1}, {subgroup_size_16, rm_iq}, 1, true);\n+    ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_id_f32[GGML_TYPE_IQ1_M],   \"mul_mat_vec_id_iq1_m_f32\",   mul_mat_vec_id_iq1_m_f32_len,   mul_mat_vec_id_iq1_m_f32_data,   \"main\", 5, sizeof(vk_mat_vec_id_push_constants), {rm_iq, 1, 1}, {subgroup_size_16, rm_iq}, 1, true);\n+    ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_id_f32[GGML_TYPE_IQ2_XXS], \"mul_mat_vec_id_iq2_xxs_f32\", mul_mat_vec_id_iq2_xxs_f32_len, mul_mat_vec_id_iq2_xxs_f32_data, \"main\", 5, sizeof(vk_mat_vec_id_push_constants), {rm_iq, 1, 1}, {subgroup_size_16, rm_iq}, 1, true);\n+    ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_id_f32[GGML_TYPE_IQ2_XS],  \"mul_mat_vec_id_iq2_xs_f32\",  mul_mat_vec_id_iq2_xs_f32_len,  mul_mat_vec_id_iq2_xs_f32_data,  \"main\", 5, sizeof(vk_mat_vec_id_push_constants), {rm_iq, 1, 1}, {subgroup_size_16, rm_iq}, 1, true);\n+    ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_id_f32[GGML_TYPE_IQ2_S],   \"mul_mat_vec_id_iq2_s_f32\",   mul_mat_vec_id_iq2_s_f32_len,   mul_mat_vec_id_iq2_s_f32_data,   \"main\", 5, sizeof(vk_mat_vec_id_push_constants), {rm_iq, 1, 1}, {subgroup_size_16, rm_iq}, 1, true);\n+    ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_id_f32[GGML_TYPE_IQ3_XXS], \"mul_mat_vec_id_iq3_xxs_f32\", mul_mat_vec_id_iq3_xxs_f32_len, mul_mat_vec_id_iq3_xxs_f32_data, \"main\", 5, sizeof(vk_mat_vec_id_push_constants), {rm_iq, 1, 1}, {subgroup_size_16, rm_iq}, 1, true);\n+    ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_id_f32[GGML_TYPE_IQ3_S],   \"mul_mat_vec_id_iq3_s_f32\",   mul_mat_vec_id_iq3_s_f32_len,   mul_mat_vec_id_iq3_s_f32_data,   \"main\", 5, sizeof(vk_mat_vec_id_push_constants), {rm_iq, 1, 1}, {subgroup_size_16, rm_iq}, 1, true);\n+    ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_id_f32[GGML_TYPE_IQ4_XS],  \"mul_mat_vec_id_iq4_xs_f32\",  mul_mat_vec_id_iq4_xs_f32_len,  mul_mat_vec_id_iq4_xs_f32_data,  \"main\", 5, sizeof(vk_mat_vec_id_push_constants), {rm_iq, 1, 1}, {subgroup_size_16, rm_iq}, 1, true);\n+    ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_id_f32[GGML_TYPE_IQ4_NL],  \"mul_mat_vec_id_iq4_nl_f32\",  mul_mat_vec_id_iq4_nl_f32_len,  mul_mat_vec_id_iq4_nl_f32_data,  \"main\", 5, sizeof(vk_mat_vec_id_push_constants), {rm_iq, 1, 1}, {subgroup_size_16, rm_iq}, 1, true);\n+    ggml_vk_create_pipeline(device, device->pipeline_dequant_mul_mat_vec_id_f32[GGML_TYPE_MXFP4],   \"mul_mat_vec_id_mxfp4_f32\",   mul_mat_vec_id_mxfp4_f32_len,   mul_mat_vec_id_mxfp4_f32_data,   \"main\", 5, sizeof(vk_mat_vec_id_push_constants), {rm_iq, 1, 1}, {subgroup_size_16, rm_iq}, 1, true);\n \n     // dequant shaders\n     ggml_vk_create_pipeline(device, device->pipeline_dequant[GGML_TYPE_F32 ], \"f32_to_f16\",   dequant_f32_len,  dequant_f32_data,  \"main\", 2, 5 * sizeof(uint32_t), {256 * 16, 1, 1}, {}, 1);\n@@ -3512,12 +3528,12 @@ static void ggml_vk_load_shaders(vk_device& device) {\n \n     for (uint32_t i = 0; i < p021_max_gqa_ratio; ++i) {\n         if (device->subgroup_arithmetic && device->subgroup_require_full_support) {\n-            ggml_vk_create_pipeline2(device, device->pipeline_mul_mat_vec_p021_f16_f32[i], \"mul_mat_vec_p021_f16_f32\"+std::to_string(i+1), mul_mat_vec_p021_f16_f32_subgroup_add_len, mul_mat_vec_p021_f16_f32_subgroup_add_data, \"main\", 3, 6 * sizeof(uint32_t), {1, 1, 1}, {device->subgroup_size, i + 1}, 1, true, true);\n+            ggml_vk_create_pipeline2(device, device->pipeline_mul_mat_vec_p021_f16_f32[i], \"mul_mat_vec_p021_f16_f32\"+std::to_string(i+1), mul_mat_vec_p021_f16_f32_subgroup_add_len, mul_mat_vec_p021_f16_f32_subgroup_add_data, \"main\", 4, 7 * sizeof(uint32_t), {1, 1, 1}, {device->subgroup_size, i + 1}, 1, true, true);\n         } else {\n-            ggml_vk_create_pipeline2(device, device->pipeline_mul_mat_vec_p021_f16_f32[i], \"mul_mat_vec_p021_f16_f32\"+std::to_string(i+1), mul_mat_vec_p021_f16_f32_len,              mul_mat_vec_p021_f16_f32_data,              \"main\", 3, 6 * sizeof(uint32_t), {1, 1, 1}, {device->subgroup_size, i + 1}, 1, true);\n+            ggml_vk_create_pipeline2(device, device->pipeline_mul_mat_vec_p021_f16_f32[i], \"mul_mat_vec_p021_f16_f32\"+std::to_string(i+1), mul_mat_vec_p021_f16_f32_len,              mul_mat_vec_p021_f16_f32_data,              \"main\", 4, 7 * sizeof(uint32_t), {1, 1, 1}, {device->subgroup_size, i + 1}, 1, true);\n         }\n     }\n-    ggml_vk_create_pipeline(device, device->pipeline_mul_mat_vec_nc_f16_f32, \"mul_mat_vec_nc_f16_f32\", mul_mat_vec_nc_f16_f32_len, mul_mat_vec_nc_f16_f32_data, \"main\", 3, 12 * sizeof(uint32_t), {1, 1, 1}, {}, 1);\n+    ggml_vk_create_pipeline(device, device->pipeline_mul_mat_vec_nc_f16_f32, \"mul_mat_vec_nc_f16_f32\", mul_mat_vec_nc_f16_f32_len, mul_mat_vec_nc_f16_f32_data, \"main\", 4, 13 * sizeof(uint32_t), {1, 1, 1}, {}, 1);\n \n     ggml_vk_create_pipeline(device, device->pipeline_norm_f32, \"norm_f32\", norm_f32_len, norm_f32_data, \"main\", 2, sizeof(vk_op_push_constants), {1, 1, 1}, {}, 1);\n     ggml_vk_create_pipeline(device, device->pipeline_group_norm_f32, \"group_norm_f32\", group_norm_f32_len, group_norm_f32_data, \"main\", 2, sizeof(vk_op_push_constants), {1, 1, 1}, {}, 1);\n@@ -6493,7 +6509,11 @@ static bool ggml_vk_should_use_mmvq(const vk_device& device, uint32_t m, uint32_\n     GGML_UNUSED(k);\n }\n \n-static void ggml_vk_mul_mat_vec_q_f16(ggml_backend_vk_context * ctx, vk_context& subctx, const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst, bool dryrun = false) {\n+static void ggml_vk_mul_mat_vec_q_f16(ggml_backend_vk_context * ctx, vk_context& subctx, const struct ggml_cgraph * cgraph, int node_idx, bool dryrun = false) {\n+    ggml_tensor * dst = cgraph->nodes[node_idx];\n+    const ggml_tensor * src0 = dst->src[0];\n+    const ggml_tensor * src1 = dst->src[1];\n+\n     VK_LOG_DEBUG(\"ggml_vk_mul_mat_vec_q_f16((\" << src0 << \", name=\" << src0->name << \", type=\" << src0->type << \", ne0=\" << src0->ne[0] << \", ne1=\" << src0->ne[1] << \", ne2=\" << src0->ne[2] << \", ne3=\" << src0->ne[3] << \", nb0=\" << src0->nb[0] << \", nb1=\" << src0->nb[1] << \", nb2=\" << src0->nb[2] << \", nb3=\" << src0->nb[3];\n     std::cerr << \"), (\" << src1 << \", name=\" << src1->name << \", type=\" << src1->type << \", ne0=\" << src1->ne[0] << \", ne1=\" << src1->ne[1] << \", ne2=\" << src1->ne[2] << \", ne3=\" << src1->ne[3] << \", nb0=\" << src1->nb[0] << \", nb1=\" << src1->nb[1] << \", nb2=\" << src1->nb[2] << \", nb3=\" << src1->nb[3];\n     std::cerr << \"), (\" << dst << \", name=\" << dst->name << \", type=\" << dst->type << \", ne0=\" << dst->ne[0] << \", ne1=\" << dst->ne[1] << \", ne2=\" << dst->ne[2] << \", ne3=\" << dst->ne[3] << \", nb0=\" << dst->nb[0] << \", nb1=\" << dst->nb[1] << \", nb2=\" << dst->nb[2] << \", nb3=\" << dst->nb[3];\n@@ -6524,7 +6544,6 @@ static void ggml_vk_mul_mat_vec_q_f16(ggml_backend_vk_context * ctx, vk_context&\n     GGML_ASSERT(ne11 == 1 || ne12 * ne13 == 1);\n     bool batch_n = ne11 > 1;\n \n-    ggml_backend_vk_buffer_context * dst_buf_ctx = (ggml_backend_vk_buffer_context *)dst->buffer->context;\n     ggml_backend_vk_buffer_context * src0_buf_ctx = (ggml_backend_vk_buffer_context *)src0->buffer->context;\n     ggml_backend_vk_buffer_context * src1_buf_ctx = (ggml_backend_vk_buffer_context *)src1->buffer->context;\n \n@@ -6626,8 +6645,20 @@ static void ggml_vk_mul_mat_vec_q_f16(ggml_backend_vk_context * ctx, vk_context&\n         return;\n     }\n \n-    vk_buffer d_D = dst_buf_ctx->dev_buffer;\n-    const uint64_t d_buf_offset = vk_tensor_offset(dst) + dst->view_offs;\n+    vk_buffer d_D;\n+    uint64_t d_buf_offset = 0;\n+\n+    if (ctx->num_additional_fused_ops > 0) {\n+        const ggml_tensor * add = cgraph->nodes[node_idx + 1];\n+        ggml_backend_vk_buffer_context * dst_buf_ctx = (ggml_backend_vk_buffer_context *)add->buffer->context;\n+        d_D = dst_buf_ctx->dev_buffer;\n+        d_buf_offset = vk_tensor_offset(add) + add->view_offs;\n+    } else {\n+        ggml_backend_vk_buffer_context * dst_buf_ctx = (ggml_backend_vk_buffer_context *)dst->buffer->context;\n+        d_D = dst_buf_ctx->dev_buffer;\n+        d_buf_offset = vk_tensor_offset(dst) + dst->view_offs;\n+    }\n+\n     GGML_ASSERT(d_D != nullptr);\n     vk_buffer d_X;\n     uint64_t x_buf_offset = 0;\n@@ -6722,14 +6753,43 @@ static void ggml_vk_mul_mat_vec_q_f16(ggml_backend_vk_context * ctx, vk_context&\n         y_sz_total = CEIL_DIV(y_sz_total, 144) * 144;\n     }\n \n+    uint32_t enable_bias = ctx->num_additional_fused_ops > 0;\n+\n+    vk_buffer d_B = d_D;\n+    size_t b_buf_offset = 0;\n+    uint64_t b_sz = 0;\n+\n+    if (enable_bias) {\n+        const ggml_tensor * add = cgraph->nodes[node_idx + 1];\n+        const ggml_tensor * bias = add->src[0] == dst ? add->src[1] : add->src[0];\n+\n+        bool b_uma = false;\n+        if (ctx->device->uma) {\n+            ggml_vk_host_get(ctx->device, bias->data, d_B, b_buf_offset);\n+            b_uma = d_B != nullptr;\n+        }\n+        if(!b_uma) {\n+            ggml_backend_vk_buffer_context * bias_buf_ctx = (ggml_backend_vk_buffer_context *)bias->buffer->context;\n+            d_B = bias_buf_ctx->dev_buffer;\n+            b_buf_offset = vk_tensor_offset(bias) + bias->view_offs;\n+            GGML_ASSERT(d_B != nullptr);\n+            b_sz = ggml_nbytes(bias);\n+        }\n+    }\n+\n     // compute\n     const vk_mat_vec_push_constants pc = {\n         (uint32_t)ne00, (uint32_t)ne10, (uint32_t)ne10, (uint32_t)ne01,\n-        stride_batch_x, stride_batch_y, stride_batch_d,\n+        stride_batch_x, stride_batch_y, stride_batch_d, enable_bias,\n         (uint32_t)ne02, (uint32_t)ne12, (uint32_t)r2, (uint32_t)r3,\n     };\n     ggml_vk_dispatch_pipeline(ctx, subctx, dmmv,\n-                              { vk_subbuffer{ d_X, x_buf_offset, x_sz * ne02 * ne03 }, vk_subbuffer{ d_Y, y_buf_offset, y_sz_total }, vk_subbuffer{ d_D, d_buf_offset, d_sz * ne22 * ne23} },\n+                              {\n+                                vk_subbuffer{ d_X, x_buf_offset, x_sz * ne02 * ne03 },\n+                                vk_subbuffer{ d_Y, y_buf_offset, y_sz_total },\n+                                vk_subbuffer{ d_D, d_buf_offset, d_sz * ne22 * ne23},\n+                                vk_subbuffer{ d_B, b_buf_offset, b_sz },\n+                              },\n                               pc, { groups_x, (uint32_t)(ne12 * ne13), groups_z });\n \n     if (x_non_contig) {\n@@ -6740,7 +6800,10 @@ static void ggml_vk_mul_mat_vec_q_f16(ggml_backend_vk_context * ctx, vk_context&\n     }\n }\n \n-static void ggml_vk_mul_mat_vec_p021_f16_f32(ggml_backend_vk_context * ctx, vk_context& subctx, const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst, bool dryrun = false) {\n+static void ggml_vk_mul_mat_vec_p021_f16_f32(ggml_backend_vk_context * ctx, vk_context& subctx, const struct ggml_cgraph * cgraph, int node_idx, bool dryrun = false) {\n+    ggml_tensor * dst = cgraph->nodes[node_idx];\n+    const ggml_tensor * src0 = dst->src[0];\n+    const ggml_tensor * src1 = dst->src[1];\n     VK_LOG_DEBUG(\"ggml_vk_mul_mat_p021_f16_f32(\" << src0 << \", name=\" << src0->name << \", type=\" << src0->type << \", ne0=\" << src0->ne[0] << \", ne1=\" << src0->ne[1] << \", ne2=\" << src0->ne[2] << \", ne3=\" << src0->ne[3] << \", nb0=\" << src0->nb[0] << \", nb1=\" << src0->nb[1] << \", nb2=\" << src0->nb[2] << \", nb3=\" << src0->nb[3];\n     std::cerr << \"), (\" << src1 << \", name=\" << src1->name << \", type=\" << src1->type << \", ne0=\" << src1->ne[0] << \", ne1=\" << src1->ne[1] << \", ne2=\" << src1->ne[2] << \", ne3=\" << src1->ne[3] << \", nb0=\" << src1->nb[0] << \", nb1=\" << src1->nb[1] << \", nb2=\" << src1->nb[2] << \", nb3=\" << src1->nb[3];\n     std::cerr << \"), (\" << dst << \", name=\" << dst->name << \", type=\" << dst->type << \", ne0=\" << dst->ne[0] << \", ne1=\" << dst->ne[1] << \", ne2=\" << dst->ne[2] << \", ne3=\" << dst->ne[3] << \", nb0=\" << dst->nb[0] << \", nb1=\" << dst->nb[1] << \", nb2=\" << dst->nb[2] << \", nb3=\" << dst->nb[3];\n@@ -6763,7 +6826,6 @@ static void ggml_vk_mul_mat_vec_p021_f16_f32(ggml_backend_vk_context * ctx, vk_c\n \n     GGML_ASSERT(ne11 == 1);\n \n-    ggml_backend_vk_buffer_context * dst_buf_ctx = (ggml_backend_vk_buffer_context *)dst->buffer->context;\n     ggml_backend_vk_buffer_context * src0_buf_ctx = (ggml_backend_vk_buffer_context *)src0->buffer->context;\n     ggml_backend_vk_buffer_context * src1_buf_ctx = (ggml_backend_vk_buffer_context *)src1->buffer->context;\n \n@@ -6797,8 +6859,19 @@ static void ggml_vk_mul_mat_vec_p021_f16_f32(ggml_backend_vk_context * ctx, vk_c\n         return;\n     }\n \n-    vk_buffer d_D = dst_buf_ctx->dev_buffer;\n-    const uint64_t d_buf_offset = vk_tensor_offset(dst) + dst->view_offs;\n+    vk_buffer d_D;\n+    uint64_t d_buf_offset = 0;\n+\n+    if (ctx->num_additional_fused_ops > 0) {\n+        const ggml_tensor * add = cgraph->nodes[node_idx + 1];\n+        ggml_backend_vk_buffer_context * dst_buf_ctx = (ggml_backend_vk_buffer_context *)add->buffer->context;\n+        d_D = dst_buf_ctx->dev_buffer;\n+        d_buf_offset = vk_tensor_offset(add) + add->view_offs;\n+    } else {\n+        ggml_backend_vk_buffer_context * dst_buf_ctx = (ggml_backend_vk_buffer_context *)dst->buffer->context;\n+        d_D = dst_buf_ctx->dev_buffer;\n+        d_buf_offset = vk_tensor_offset(dst) + dst->view_offs;\n+    }\n     GGML_ASSERT(d_D != nullptr);\n     vk_buffer d_Qx = src0_buf_ctx->dev_buffer;\n     const uint64_t qx_buf_offset = vk_tensor_offset(src0) + src0->view_offs;\n@@ -6815,19 +6888,52 @@ static void ggml_vk_mul_mat_vec_p021_f16_f32(ggml_backend_vk_context * ctx, vk_c\n     const uint64_t d_buffer_offset = (d_buf_offset / ctx->device->properties.limits.minStorageBufferOffsetAlignment) * ctx->device->properties.limits.minStorageBufferOffsetAlignment;\n     const uint64_t d_shader_offset = d_buf_offset - d_buffer_offset;\n \n+    uint32_t enable_bias = ctx->num_additional_fused_ops > 0;\n+\n+    vk_buffer d_B = d_D;\n+    size_t b_buf_offset = 0;\n+    uint64_t b_sz = 0;\n+\n+    if (enable_bias) {\n+        const ggml_tensor * add = cgraph->nodes[node_idx + 1];\n+        const ggml_tensor * bias = add->src[0] == dst ? add->src[1] : add->src[0];\n+\n+        bool b_uma = false;\n+        if (ctx->device->uma) {\n+            ggml_vk_host_get(ctx->device, bias->data, d_B, b_buf_offset);\n+            b_uma = d_B != nullptr;\n+        }\n+        if(!b_uma) {\n+            ggml_backend_vk_buffer_context * bias_buf_ctx = (ggml_backend_vk_buffer_context *)bias->buffer->context;\n+            d_B = bias_buf_ctx->dev_buffer;\n+            b_buf_offset = vk_tensor_offset(bias) + bias->view_offs;\n+            GGML_ASSERT(d_B != nullptr);\n+            b_sz = ggml_nbytes(bias);\n+        }\n+    }\n+\n     // compute\n-    const std::array<uint32_t, 6> pc = { (uint32_t)ne00, (uint32_t)ne01, (uint32_t)ne02, (uint32_t)ne12, (uint32_t)(qy_shader_offset / ggml_type_size(src1->type)), (uint32_t)(d_shader_offset / ggml_type_size(dst->type)) };\n+    const std::array<uint32_t, 7> pc = { (uint32_t)ne00, (uint32_t)ne01, (uint32_t)ne02, (uint32_t)ne12, (uint32_t)(qy_shader_offset / ggml_type_size(src1->type)), (uint32_t)(d_shader_offset / ggml_type_size(dst->type)), enable_bias };\n \n     uint32_t workgroups_z = (uint32_t)ne12;\n     // When gqa_ratio > 1, each invocation does multiple rows and we can launch fewer workgroups\n     if (gqa_ratio > 1) {\n         workgroups_z /= gqa_ratio;\n     }\n \n-    ggml_vk_dispatch_pipeline(ctx, subctx, ctx->device->pipeline_mul_mat_vec_p021_f16_f32[gqa_ratio - 1], { vk_subbuffer{ d_Qx, qx_buf_offset, qx_sz }, vk_subbuffer{ d_Qy, qy_buffer_offset, qy_sz + qy_shader_offset }, vk_subbuffer{ d_D, d_buffer_offset, d_sz + d_shader_offset } }, pc, { 1, (uint32_t)ne01, workgroups_z });\n+    ggml_vk_dispatch_pipeline(ctx, subctx, ctx->device->pipeline_mul_mat_vec_p021_f16_f32[gqa_ratio - 1],\n+        {\n+            vk_subbuffer{ d_Qx, qx_buf_offset, qx_sz },\n+            vk_subbuffer{ d_Qy, qy_buffer_offset, qy_sz + qy_shader_offset },\n+            vk_subbuffer{ d_D, d_buffer_offset, d_sz + d_shader_offset },\n+            vk_subbuffer{ d_B, b_buf_offset, b_sz },\n+        }, pc, { 1, (uint32_t)ne01, workgroups_z });\n }\n \n-static void ggml_vk_mul_mat_vec_nc_f16_f32(ggml_backend_vk_context * ctx, vk_context& subctx, const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst, bool dryrun = false) {\n+static void ggml_vk_mul_mat_vec_nc_f16_f32(ggml_backend_vk_context * ctx, vk_context& subctx, const struct ggml_cgraph * cgraph, int node_idx, bool dryrun = false) {\n+    ggml_tensor * dst = cgraph->nodes[node_idx];\n+    const ggml_tensor * src0 = dst->src[0];\n+    const ggml_tensor * src1 = dst->src[1];\n     VK_LOG_DEBUG(\"ggml_vk_mul_mat_nc_f16_f32((\" << src0 << \", name=\" << src0->name << \", type=\" << src0->type << \", ne0=\" << src0->ne[0] << \", ne1=\" << src0->ne[1] << \", ne2=\" << src0->ne[2] << \", ne3=\" << src0->ne[3] << \", nb0=\" << src0->nb[0] << \", nb1=\" << src0->nb[1] << \", nb2=\" << src0->nb[2] << \", nb3=\" << src0->nb[3];\n     std::cerr << \"), (\" << src1 << \", name=\" << src1->name << \", type=\" << src1->type << \", ne0=\" << src1->ne[0] << \", ne1=\" << src1->ne[1] << \", ne2=\" << src1->ne[2] << \", ne3=\" << src1->ne[3] << \", nb0=\" << src1->nb[0] << \", nb1=\" << src1->nb[1] << \", nb2=\" << src1->nb[2] << \", nb3=\" << src1->nb[3];\n     std::cerr << \"), (\" << dst << \", name=\" << dst->name << \", type=\" << dst->type << \", ne0=\" << dst->ne[0] << \", ne1=\" << dst->ne[1] << \", ne2=\" << dst->ne[2] << \", ne3=\" << dst->ne[3] << \", nb0=\" << dst->nb[0] << \", nb1=\" << dst->nb[1] << \", nb2=\" << dst->nb[2] << \", nb3=\" << dst->nb[3];\n@@ -6860,7 +6966,6 @@ static void ggml_vk_mul_mat_vec_nc_f16_f32(ggml_backend_vk_context * ctx, vk_con\n     GGML_ASSERT(ne11 == 1);\n     GGML_ASSERT(src0->ne[3] == src1->ne[3]); // checked in supports_op\n \n-    ggml_backend_vk_buffer_context * dst_buf_ctx = (ggml_backend_vk_buffer_context *)dst->buffer->context;\n     ggml_backend_vk_buffer_context * src0_buf_ctx = (ggml_backend_vk_buffer_context *)src0->buffer->context;\n     ggml_backend_vk_buffer_context * src1_buf_ctx = (ggml_backend_vk_buffer_context *)src1->buffer->context;\n \n@@ -6890,8 +6995,20 @@ static void ggml_vk_mul_mat_vec_nc_f16_f32(ggml_backend_vk_context * ctx, vk_con\n         return;\n     }\n \n-    vk_buffer d_D = dst_buf_ctx->dev_buffer;\n-    const uint64_t d_buf_offset = vk_tensor_offset(dst) + dst->view_offs;\n+    vk_buffer d_D;\n+    uint64_t d_buf_offset = 0;\n+\n+    if (ctx->num_additional_fused_ops > 0) {\n+        const ggml_tensor * add = cgraph->nodes[node_idx + 1];\n+        ggml_backend_vk_buffer_context * dst_buf_ctx = (ggml_backend_vk_buffer_context *)add->buffer->context;\n+        d_D = dst_buf_ctx->dev_buffer;\n+        d_buf_offset = vk_tensor_offset(add) + add->view_offs;\n+    } else {\n+        ggml_backend_vk_buffer_context * dst_buf_ctx = (ggml_backend_vk_buffer_context *)dst->buffer->context;\n+        d_D = dst_buf_ctx->dev_buffer;\n+        d_buf_offset = vk_tensor_offset(dst) + dst->view_offs;\n+    }\n+\n     GGML_ASSERT(d_D != nullptr);\n     vk_buffer d_Qx = src0_buf_ctx->dev_buffer;\n     const uint64_t qx_buf_offset = vk_tensor_offset(src0) + src0->view_offs;\n@@ -6908,13 +7025,45 @@ static void ggml_vk_mul_mat_vec_nc_f16_f32(ggml_backend_vk_context * ctx, vk_con\n     const uint64_t d_buffer_offset = (d_buf_offset / ctx->device->properties.limits.minStorageBufferOffsetAlignment) * ctx->device->properties.limits.minStorageBufferOffsetAlignment;\n     const uint64_t d_shader_offset = d_buf_offset - d_buffer_offset;\n \n+    uint32_t enable_bias = ctx->num_additional_fused_ops > 0;\n+\n+    vk_buffer d_B = d_D;\n+    size_t b_buf_offset = 0;\n+    uint64_t b_sz = 0;\n+\n+    if (enable_bias) {\n+        const ggml_tensor * add = cgraph->nodes[node_idx + 1];\n+        const ggml_tensor * bias = add->src[0] == dst ? add->src[1] : add->src[0];\n+\n+        bool b_uma = false;\n+        if (ctx->device->uma) {\n+            ggml_vk_host_get(ctx->device, bias->data, d_B, b_buf_offset);\n+            b_uma = d_B != nullptr;\n+        }\n+        if(!b_uma) {\n+            ggml_backend_vk_buffer_context * bias_buf_ctx = (ggml_backend_vk_buffer_context *)bias->buffer->context;\n+            d_B = bias_buf_ctx->dev_buffer;\n+            b_buf_offset = vk_tensor_offset(bias) + bias->view_offs;\n+            GGML_ASSERT(d_B != nullptr);\n+            b_sz = ggml_nbytes(bias);\n+        }\n+    }\n+\n     // compute\n-    const std::array<uint32_t, 12> pc = { (uint32_t)ne00, (uint32_t)ne01, row_stride_x, channel_stride_x, channel_stride_y, (uint32_t)(ne12 / ne02), (uint32_t)ne12, (uint32_t)(qy_shader_offset / ggml_type_size(src1->type)), (uint32_t)(d_shader_offset / ggml_type_size(dst->type)), nb03, nb13, nb23 };\n+    const std::array<uint32_t, 13> pc = { (uint32_t)ne00, (uint32_t)ne01, row_stride_x, channel_stride_x, channel_stride_y, (uint32_t)(ne12 / ne02), (uint32_t)ne12, (uint32_t)(qy_shader_offset / ggml_type_size(src1->type)), (uint32_t)(d_shader_offset / ggml_type_size(dst->type)), nb03, nb13, nb23, enable_bias };\n     ggml_vk_dispatch_pipeline(ctx, subctx, ctx->device->pipeline_mul_mat_vec_nc_f16_f32,\n-        { vk_subbuffer{ d_Qx, qx_buf_offset, qx_sz }, vk_subbuffer{ d_Qy, qy_buffer_offset, qy_sz + qy_shader_offset }, vk_subbuffer{ d_D, d_buffer_offset, d_sz + d_shader_offset } }, pc, { (uint32_t)ne03, (uint32_t)ne01, (uint32_t)ne12 });\n+        {\n+            vk_subbuffer{ d_Qx, qx_buf_offset, qx_sz },\n+            vk_subbuffer{ d_Qy, qy_buffer_offset, qy_sz + qy_shader_offset },\n+            vk_subbuffer{ d_D, d_buffer_offset, d_sz + d_shader_offset },\n+            vk_subbuffer{ d_B, b_buf_offset, b_sz },\n+        }, pc, { (uint32_t)ne03, (uint32_t)ne01, (uint32_t)ne12 });\n }\n \n-static void ggml_vk_mul_mat(ggml_backend_vk_context * ctx, vk_context& subctx, ggml_tensor * src0, ggml_tensor * src1, ggml_tensor * dst, bool dryrun = false) {\n+static void ggml_vk_mul_mat(ggml_backend_vk_context * ctx, vk_context& subctx, const struct ggml_cgraph * cgraph, int node_idx, bool dryrun = false) {\n+    ggml_tensor * dst = cgraph->nodes[node_idx];\n+    ggml_tensor * src0 = dst->src[0];\n+    ggml_tensor * src1 = dst->src[1];\n     VK_LOG_DEBUG(\"ggml_vk_mul_mat(\" << src0 << \", \" << src1 << \", \" << dst << \")\");\n \n     // Handle huge A matrix by splitting the M dimensions. This works well for convolution use cases\n@@ -6953,15 +7102,15 @@ static void ggml_vk_mul_mat(ggml_backend_vk_context * ctx, vk_context& subctx, g\n         src1->nb[1] <= src1->nb[3] &&\n         src0->ne[3] == 1 &&\n         src1->ne[3] == 1) {\n-        ggml_vk_mul_mat_vec_p021_f16_f32(ctx, subctx, src0, src1, dst, dryrun);\n+        ggml_vk_mul_mat_vec_p021_f16_f32(ctx, subctx, cgraph, node_idx, dryrun);\n     } else if (src0->type == GGML_TYPE_F16 && !ggml_is_contiguous(src0) && !ggml_is_transposed(src1) && dst->ne[1] == 1 &&\n                !ggml_is_permuted(src0) && !ggml_is_permuted(src1)) {\n-        ggml_vk_mul_mat_vec_nc_f16_f32(ctx, subctx, src0, src1, dst, dryrun);\n+        ggml_vk_mul_mat_vec_nc_f16_f32(ctx, subctx, cgraph, node_idx, dryrun);\n     // mul_mat_vec supports batching ne12*ne13 when ne11==1, or treating ne11 as the batch size (up to four)\n     // when ne12 and ne13 are one.\n     } else if ((dst->ne[1] == 1 || (dst->ne[1] <= mul_mat_vec_max_cols && src1->ne[2] * src1->ne[3] == 1)) &&\n                (src0->type == GGML_TYPE_F32 || src0->type == GGML_TYPE_F16 || src0->type == GGML_TYPE_BF16 || ggml_is_quantized(src0->type))) {\n-        ggml_vk_mul_mat_vec_q_f16(ctx, subctx, src0, src1, dst, dryrun);\n+        ggml_vk_mul_mat_vec_q_f16(ctx, subctx, cgraph, node_idx, dryrun);\n     } else {\n         ggml_vk_mul_mat_q_f16(ctx, subctx, src0, src1, dst, false, dryrun);\n     }\n@@ -7241,7 +7390,11 @@ static void ggml_vk_mul_mat_id_q_f16(ggml_backend_vk_context * ctx, vk_context&\n     }\n }\n \n-static void ggml_vk_mul_mat_vec_id_q_f16(ggml_backend_vk_context * ctx, vk_context& subctx, const ggml_tensor * src0, const ggml_tensor * src1, const ggml_tensor * ids, ggml_tensor * dst, bool dryrun = false) {\n+static void ggml_vk_mul_mat_vec_id_q_f16(ggml_backend_vk_context * ctx, vk_context& subctx, const struct ggml_cgraph * cgraph, int node_idx, bool dryrun = false) {\n+    ggml_tensor * dst = cgraph->nodes[node_idx];\n+    ggml_tensor * src0 = dst->src[0];\n+    ggml_tensor * src1 = dst->src[1];\n+    ggml_tensor * ids = dst->src[2];\n     VK_LOG_DEBUG(\"ggml_vk_mul_mat_vec_id_q_f16((\" << src0 << \", name=\" << src0->name << \", type=\" << src0->type << \", ne0=\" << src0->ne[0] << \", ne1=\" << src0->ne[1] << \", ne2=\" << src0->ne[2] << \", ne3=\" << src0->ne[3] << \", nb0=\" << src0->nb[0] << \", nb1=\" << src0->nb[1] << \", nb2=\" << src0->nb[2] << \", nb3=\" << src0->nb[3];\n     std::cerr << \"), (\" << src1 << \", name=\" << src1->name << \", type=\" << src1->type << \", ne0=\" << src1->ne[0] << \", ne1=\" << src1->ne[1] << \", ne2=\" << src1->ne[2] << \", ne3=\" << src1->ne[3] << \", nb0=\" << src1->nb[0] << \", nb1=\" << src1->nb[1] << \", nb2=\" << src1->nb[2] << \", nb3=\" << src1->nb[3];\n     std::cerr << \"), (\" << ids << \", name=\" << ids->name << \", type=\" << ids->type << \", ne0=\" << ids->ne[0] << \", ne1=\" << ids->ne[1] << \", ne2=\" << ids->ne[2] << \", ne3=\" << ids->ne[3] << \", nb0=\" << ids->nb[0] << \", nb1=\" << ids->nb[1] << \", nb2=\" << ids->nb[2] << \", nb3=\" << ids->nb[3];\n@@ -7273,7 +7426,6 @@ static void ggml_vk_mul_mat_vec_id_q_f16(ggml_backend_vk_context * ctx, vk_conte\n     const uint64_t ne22 = dst->ne[2];\n     const uint64_t ne23 = dst->ne[3];\n \n-    ggml_backend_vk_buffer_context * dst_buf_ctx = (ggml_backend_vk_buffer_context *)dst->buffer->context;\n     ggml_backend_vk_buffer_context * src0_buf_ctx = (ggml_backend_vk_buffer_context *)src0->buffer->context;\n     ggml_backend_vk_buffer_context * src1_buf_ctx = (ggml_backend_vk_buffer_context *)src1->buffer->context;\n     ggml_backend_vk_buffer_context * ids_buf_ctx = (ggml_backend_vk_buffer_context *)ids->buffer->context;\n@@ -7361,8 +7513,20 @@ static void ggml_vk_mul_mat_vec_id_q_f16(ggml_backend_vk_context * ctx, vk_conte\n         return;\n     }\n \n-    vk_buffer d_D = dst_buf_ctx->dev_buffer;\n-    const uint64_t d_buf_offset = vk_tensor_offset(dst) + dst->view_offs;\n+    vk_buffer d_D;\n+    uint64_t d_buf_offset = 0;\n+\n+    if (ctx->num_additional_fused_ops > 0) {\n+        const ggml_tensor * add = cgraph->nodes[node_idx + 1];\n+        ggml_backend_vk_buffer_context * dst_buf_ctx = (ggml_backend_vk_buffer_context *)add->buffer->context;\n+        d_D = dst_buf_ctx->dev_buffer;\n+        d_buf_offset = vk_tensor_offset(add) + add->view_offs;\n+    } else {\n+        ggml_backend_vk_buffer_context * dst_buf_ctx = (ggml_backend_vk_buffer_context *)dst->buffer->context;\n+        d_D = dst_buf_ctx->dev_buffer;\n+        d_buf_offset = vk_tensor_offset(dst) + dst->view_offs;\n+    }\n+\n     GGML_ASSERT(d_D != nullptr);\n     vk_buffer d_X;\n     uint64_t x_buf_offset = 0;\n@@ -7437,15 +7601,46 @@ static void ggml_vk_mul_mat_vec_id_q_f16(ggml_backend_vk_context * ctx, vk_conte\n         groups_x = CEIL_DIV(groups_x, groups_z);\n     }\n \n+    uint32_t enable_bias = ctx->num_additional_fused_ops > 0;\n+\n+    vk_buffer d_B = d_D;\n+    size_t b_buf_offset = 0;\n+    uint64_t b_sz = 0;\n+\n+    if (enable_bias) {\n+        const ggml_tensor * bias = cgraph->nodes[node_idx + 1]->src[1];\n+\n+        bool b_uma = false;\n+        if (ctx->device->uma) {\n+            ggml_vk_host_get(ctx->device, bias->data, d_B, b_buf_offset);\n+            b_uma = d_B != nullptr;\n+        }\n+        if(!b_uma) {\n+            ggml_backend_vk_buffer_context * bias_buf_ctx = (ggml_backend_vk_buffer_context *)bias->buffer->context;\n+            d_B = bias_buf_ctx->dev_buffer;\n+            b_buf_offset = vk_tensor_offset(bias) + bias->view_offs;\n+            GGML_ASSERT(d_B != nullptr);\n+            b_sz = ggml_nbytes(bias);\n+        }\n+    }\n+\n     // compute\n     const vk_mat_vec_id_push_constants pc = {\n         (uint32_t)ne00, (uint32_t)ne10, (uint32_t)ne10, (uint32_t)ne01,\n         (uint32_t)x_ne, stride_batch_y, (uint32_t)(ne20*ne21),\n+\n+        enable_bias,\n+\n         (uint32_t)nei0, (uint32_t)ne11,\n     };\n     ggml_vk_dispatch_pipeline(ctx, subctx, dmmv,\n-        { vk_subbuffer{ d_X, x_buf_offset, x_sz * ne02 * ne03 },\n-        vk_subbuffer{ d_Y, y_buf_offset, y_sz * ne12 * ne13 }, vk_subbuffer{ d_D, d_buf_offset, d_sz * ne22 * ne23}, vk_subbuffer{ d_ids, ids_buf_offset, ids_sz } },\n+        {\n+            vk_subbuffer{ d_X, x_buf_offset, x_sz * ne02 * ne03 },\n+            vk_subbuffer{ d_Y, y_buf_offset, y_sz * ne12 * ne13 },\n+            vk_subbuffer{ d_D, d_buf_offset, d_sz * ne22 * ne23},\n+            vk_subbuffer{ d_B, b_buf_offset, b_sz },\n+            vk_subbuffer{ d_ids, ids_buf_offset, ids_sz },\n+        },\n         pc, { groups_x, (uint32_t)nei0, groups_z });\n \n     if (x_non_contig) {\n@@ -7456,10 +7651,21 @@ static void ggml_vk_mul_mat_vec_id_q_f16(ggml_backend_vk_context * ctx, vk_conte\n     }\n }\n \n-static void ggml_vk_mul_mat_id(ggml_backend_vk_context * ctx, vk_context& subctx, const ggml_tensor * src0, const ggml_tensor * src1, const ggml_tensor * src2, ggml_tensor * dst, bool dryrun = false) {\n+static bool ggml_vk_use_mul_mat_vec_id(const struct ggml_cgraph * cgraph, int node_idx) {\n+    ggml_tensor * dst = cgraph->nodes[node_idx];\n+    ggml_tensor * src0 = dst->src[0];\n+    ggml_tensor * src2 = dst->src[2];\n+    return src2->ne[1] == 1 && (src0->type == GGML_TYPE_F32 || src0->type == GGML_TYPE_F16 || ggml_is_quantized(src0->type));\n+}\n+\n+static void ggml_vk_mul_mat_id(ggml_backend_vk_context * ctx, vk_context& subctx, const struct ggml_cgraph * cgraph, int node_idx, bool dryrun = false) {\n+    ggml_tensor * dst = cgraph->nodes[node_idx];\n+    ggml_tensor * src0 = dst->src[0];\n+    ggml_tensor * src1 = dst->src[1];\n+    ggml_tensor * src2 = dst->src[2];\n     VK_LOG_DEBUG(\"ggml_vk_mul_mat_id(\" << src0 << \", \" << src1 << \", \" << src2 << \", \" << dst << \")\");\n-    if (src2->ne[1] == 1 && (src0->type == GGML_TYPE_F32 || src0->type == GGML_TYPE_F16 || ggml_is_quantized(src0->type))) {\n-        ggml_vk_mul_mat_vec_id_q_f16(ctx, subctx, src0, src1, src2, dst, dryrun);\n+    if (ggml_vk_use_mul_mat_vec_id(cgraph, node_idx)) {\n+        ggml_vk_mul_mat_vec_id_q_f16(ctx, subctx, cgraph, node_idx, dryrun);\n     } else {\n         ggml_vk_mul_mat_id_q_f16(ctx, subctx, src0, src1, src2, dst, dryrun);\n     }\n@@ -8425,7 +8631,7 @@ static bool ggml_vk_op_supports_incontiguous(ggml_op op) {\n     }\n }\n \n-static uint32_t get_misalign_bytes(ggml_backend_vk_context * ctx, const ggml_tensor * t)\n+static uint32_t get_misalign_bytes(const ggml_backend_vk_context * ctx, const ggml_tensor * t)\n {\n     return ((vk_tensor_offset(t) + t->view_offs) & (ctx->device->properties.limits.minStorageBufferOffsetAlignment - 1));;\n }\n@@ -11780,11 +11986,11 @@ static bool ggml_vk_build_graph(ggml_backend_vk_context * ctx, ggml_cgraph * cgr\n \n         break;\n     case GGML_OP_MUL_MAT:\n-        ggml_vk_mul_mat(ctx, compute_ctx, src0, src1, node, dryrun);\n+        ggml_vk_mul_mat(ctx, compute_ctx, cgraph, node_idx, dryrun);\n \n         break;\n     case GGML_OP_MUL_MAT_ID:\n-        ggml_vk_mul_mat_id(ctx, compute_ctx, src0, src1, src2, node, dryrun);\n+        ggml_vk_mul_mat_id(ctx, compute_ctx, cgraph, node_idx, dryrun);\n \n         break;\n \n@@ -12461,7 +12667,7 @@ static bool ggml_vk_is_empty(ggml_tensor * node) {\n     return ggml_is_empty(node) || node->op == GGML_OP_NONE || node->op == GGML_OP_RESHAPE || node->op == GGML_OP_TRANSPOSE || node->op == GGML_OP_VIEW || node->op == GGML_OP_PERMUTE;\n }\n \n-static bool ggml_vk_can_fuse(const struct ggml_cgraph * cgraph, int node_idx, std::initializer_list<enum ggml_op> ops) {\n+static bool ggml_vk_can_fuse(const ggml_backend_vk_context * ctx, const struct ggml_cgraph * cgraph, int node_idx, std::initializer_list<enum ggml_op> ops) {\n     if (!ggml_can_fuse(cgraph, node_idx, ops)) {\n         return false;\n     }\n@@ -12489,6 +12695,61 @@ static bool ggml_vk_can_fuse(const struct ggml_cgraph * cgraph, int node_idx, st\n             return false;\n         }\n     }\n+    if (ops.size() == 2 && ops.begin()[0] == GGML_OP_MUL_MAT && ops.begin()[1] == GGML_OP_ADD) {\n+        // additional constraints specific to this fusion\n+        const ggml_tensor *mul = cgraph->nodes[node_idx];\n+        const ggml_tensor *add = cgraph->nodes[node_idx + 1];\n+        const ggml_tensor *bias = add->src[0] == mul ? add->src[1] : add->src[0];\n+\n+        // mat-vec only\n+        if (ggml_nrows(mul) != 1) {\n+            return false;\n+        }\n+        // shaders assume the types match\n+        if (mul->type != bias->type) {\n+            return false;\n+        }\n+        // shaders reuse the D shape for bias\n+        if (!ggml_are_same_shape(mul, bias) ||\n+            !ggml_are_same_stride(mul, bias)) {\n+            return false;\n+        }\n+        // unaligned bias isn't handled\n+        if (get_misalign_bytes(ctx, bias) != 0) {\n+            return false;\n+        }\n+    }\n+    if (ops.size() == 2 && ops.begin()[0] == GGML_OP_MUL_MAT_ID && ops.begin()[1] == GGML_OP_ADD_ID) {\n+        // additional constraints specific to this fusion\n+        const ggml_tensor *mul = cgraph->nodes[node_idx];\n+        const ggml_tensor *add = cgraph->nodes[node_idx + 1];\n+        const ggml_tensor *bias = add->src[1];\n+\n+        if (mul != add->src[0]) {\n+            return false;\n+        }\n+        // mat-vec only\n+        if (!ggml_vk_use_mul_mat_vec_id(cgraph, node_idx)) {\n+            return false;\n+        }\n+        // shaders assume the types match\n+        if (mul->type != bias->type) {\n+            return false;\n+        }\n+        // shaders assume the bias is contiguous\n+        if (!ggml_is_contiguous(bias)) {\n+            return false;\n+        }\n+        // the ID tensor must be the same for mul_mat_id and add_id\n+        if (mul->src[2] != add->src[2]) {\n+            return false;\n+        }\n+        // unaligned bias isn't handled\n+        if (get_misalign_bytes(ctx, bias) != 0) {\n+            return false;\n+        }\n+    }\n+\n     return true;\n }\n \n@@ -12657,7 +12918,11 @@ static ggml_status ggml_backend_vk_graph_compute(ggml_backend_t backend, ggml_cg\n             uint32_t num_adds = ggml_vk_fuse_multi_add(ctx, cgraph, i);\n             if (num_adds) {\n                 ctx->num_additional_fused_ops = num_adds - 1;\n-            } else if (ggml_vk_can_fuse(cgraph, i, { GGML_OP_RMS_NORM, GGML_OP_MUL })) {\n+            } else if (ggml_vk_can_fuse(ctx, cgraph, i, { GGML_OP_RMS_NORM, GGML_OP_MUL })) {\n+                ctx->num_additional_fused_ops = 1;\n+            } else if (ggml_vk_can_fuse(ctx, cgraph, i, { GGML_OP_MUL_MAT, GGML_OP_ADD })) {\n+                ctx->num_additional_fused_ops = 1;\n+            } else if (ggml_vk_can_fuse(ctx, cgraph, i, { GGML_OP_MUL_MAT_ID, GGML_OP_ADD_ID })) {\n                 ctx->num_additional_fused_ops = 1;\n             } else if (ggml_can_fuse_subgraph(cgraph, i, { GGML_OP_ROPE, GGML_OP_VIEW, GGML_OP_SET_ROWS }, { i + 2 }) &&\n                        ggml_check_edges(cgraph, i, rope_view_set_rows_edges) &&\n@@ -12770,7 +13035,11 @@ static ggml_status ggml_backend_vk_graph_compute(ggml_backend_t backend, ggml_cg\n             uint32_t num_adds = ggml_vk_fuse_multi_add(ctx, cgraph, i);\n             if (num_adds) {\n                 ctx->num_additional_fused_ops = num_adds - 1;\n-            } else if (ggml_vk_can_fuse(cgraph, i, { GGML_OP_RMS_NORM, GGML_OP_MUL })) {\n+            } else if (ggml_vk_can_fuse(ctx, cgraph, i, { GGML_OP_RMS_NORM, GGML_OP_MUL })) {\n+                ctx->num_additional_fused_ops = 1;\n+            } else if (ggml_vk_can_fuse(ctx, cgraph, i, { GGML_OP_MUL_MAT, GGML_OP_ADD })) {\n+                ctx->num_additional_fused_ops = 1;\n+            } else if (ggml_vk_can_fuse(ctx, cgraph, i, { GGML_OP_MUL_MAT_ID, GGML_OP_ADD_ID })) {\n                 ctx->num_additional_fused_ops = 1;\n             } else if (ggml_can_fuse_subgraph(cgraph, i, { GGML_OP_ROPE, GGML_OP_VIEW, GGML_OP_SET_ROWS }, { i + 2 }) &&\n                        ggml_check_edges(cgraph, i, rope_view_set_rows_edges) &&\n@@ -12992,7 +13261,9 @@ static void ggml_vk_graph_optimize(ggml_backend_t backend, struct ggml_cgraph *\n             for (int c = first_unused; c < j; ++c) {\n                 if (!used[c] &&\n                     is_src_of(graph->nodes[j], graph->nodes[c]) &&\n-                    !(j == c+1 && c == current_set.back() && graph->nodes[c]->op == GGML_OP_RMS_NORM && graph->nodes[j]->op == GGML_OP_MUL)) {\n+                    !(j == c+1 && c == current_set.back() && graph->nodes[c]->op == GGML_OP_RMS_NORM && graph->nodes[j]->op == GGML_OP_MUL) &&\n+                    !(j == c+1 && c == current_set.back() && graph->nodes[c]->op == GGML_OP_MUL_MAT && graph->nodes[j]->op == GGML_OP_ADD) &&\n+                    !(j == c+1 && c == current_set.back() && graph->nodes[c]->op == GGML_OP_MUL_MAT_ID && graph->nodes[j]->op == GGML_OP_ADD_ID)) {\n                     ok = false;\n                     break;\n                 }"
      },
      {
        "filename": "ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_vec_base.glsl",
        "status": "modified",
        "additions": 32,
        "deletions": 2,
        "changes": 34,
        "patch": "@@ -28,8 +28,11 @@ layout (binding = 1) readonly buffer BV4 {B_TYPE_VEC4 data_b_v4[];};\n #endif\n \n layout (binding = 2) writeonly buffer D {D_TYPE data_d[];};\n+\n+layout (binding = 3) readonly buffer Bias {D_TYPE data_bias[];};\n+\n #ifdef MUL_MAT_ID\n-layout (binding = 3) readonly buffer IDS {int data_ids[];};\n+layout (binding = 4) readonly buffer IDS {int data_ids[];};\n #endif\n \n #include \"dequant_funcs.glsl\"\n@@ -45,6 +48,8 @@ layout (push_constant) uniform parameter\n     uint batch_stride_b;\n     uint batch_stride_d;\n \n+    uint enable_bias;\n+\n #ifdef MUL_MAT_ID\n     uint nei0;\n     uint ne11;\n@@ -56,6 +61,10 @@ layout (push_constant) uniform parameter\n #endif\n } p;\n \n+#ifdef MUL_MAT_ID\n+uint expert_id;\n+#endif\n+\n void get_offsets(out uint a_offset, out uint b_offset, out uint d_offset) {\n #ifdef MUL_MAT_ID\n     const uint expert_idx = gl_GlobalInvocationID.y;\n@@ -75,7 +84,7 @@ void get_offsets(out uint a_offset, out uint b_offset, out uint d_offset) {\n         batch_idx_a = i03 * p.ne02 + i02;\n     }\n #else\n-    const uint expert_id = data_ids[expert_idx];\n+    expert_id = data_ids[expert_idx];\n #endif\n \n     a_offset =\n@@ -113,6 +122,13 @@ void reduce_result(inout FLOAT_TYPE temp[NUM_COLS][NUM_ROWS], const in uint32_t\n     if (tid == 0) {\n         [[unroll]] for (uint j = 0; j < NUM_COLS; ++j) {\n             [[unroll]] for (uint n = 0; n < num_rows; ++n) {\n+                if (p.enable_bias != 0) {\n+#ifdef MUL_MAT_ID\n+                    temp[j][n] += FLOAT_TYPE(data_bias[expert_id*p.stride_d + first_row + n]);\n+#else\n+                    temp[j][n] += FLOAT_TYPE(data_bias[j*p.batch_stride_d + d_offset + first_row + n]);\n+#endif\n+                }\n                 data_d[j*p.batch_stride_d + d_offset + first_row + n] = D_TYPE(temp[j][n]);\n             }\n         }\n@@ -148,6 +164,13 @@ void reduce_result(FLOAT_TYPE temp[NUM_COLS][NUM_ROWS], const in uint32_t d_offs\n                 [[unroll]] for (uint s = 0; s < gl_NumSubgroups; ++s) {\n                     temp[j][n] += tmpsh[j][n][s];\n                 }\n+                if (p.enable_bias != 0) {\n+#ifdef MUL_MAT_ID\n+                    temp[j][n] += FLOAT_TYPE(data_bias[expert_id*p.stride_d + first_row + n]);\n+#else\n+                    temp[j][n] += FLOAT_TYPE(data_bias[j*p.batch_stride_d + d_offset + first_row + n]);\n+#endif\n+                }\n                 data_d[j*p.batch_stride_d + d_offset + first_row + n] = D_TYPE(temp[j][n]);\n             }\n         }\n@@ -173,6 +196,13 @@ void reduce_result(FLOAT_TYPE temp[NUM_COLS][NUM_ROWS], const in uint32_t d_offs\n     if (tid == 0) {\n         [[unroll]] for (uint j = 0; j < NUM_COLS; ++j) {\n             [[unroll]] for (uint n = 0; n < num_rows; ++n) {\n+                if (p.enable_bias != 0) {\n+#ifdef MUL_MAT_ID\n+                    tmpsh[j][n][0] += FLOAT_TYPE(data_bias[expert_id*p.stride_d + first_row + n]);\n+#else\n+                    tmpsh[j][n][0] += FLOAT_TYPE(data_bias[j*p.batch_stride_d + d_offset + first_row + n]);\n+#endif\n+                }\n                 data_d[j*p.batch_stride_d + d_offset + first_row + n] = D_TYPE(tmpsh[j][n][0]);\n             }\n         }"
      },
      {
        "filename": "ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_vec_nc.comp",
        "status": "modified",
        "additions": 6,
        "deletions": 0,
        "changes": 6,
        "patch": "@@ -15,6 +15,8 @@ layout (binding = 2) writeonly buffer D {D_TYPE dst[];};\n layout (binding = 0) readonly buffer AV4 {A_TYPE_VEC4 data_a_v4[];};\n layout (binding = 1) readonly buffer BV4 {B_TYPE_VEC4 data_b_v4[];};\n \n+layout (binding = 3) readonly buffer Bias {D_TYPE data_bias[];};\n+\n layout (push_constant) uniform parameter\n {\n     uint ncols_x;\n@@ -29,6 +31,7 @@ layout (push_constant) uniform parameter\n     uint nb03;\n     uint nb13;\n     uint nb23;\n+    uint enable_bias;\n } p;\n \n shared FLOAT_TYPE tmp[BLOCK_SIZE];\n@@ -117,6 +120,9 @@ void main() {\n     }\n \n     if (tid == 0) {\n+        if (p.enable_bias != 0) {\n+            tmp[0] += FLOAT_TYPE(data_bias[idst]);\n+        }\n         dst[idst] = tmp[0];\n     }\n }"
      },
      {
        "filename": "ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_vec_p021.comp",
        "status": "modified",
        "additions": 6,
        "deletions": 0,
        "changes": 6,
        "patch": "@@ -17,6 +17,8 @@ layout (binding = 2) writeonly buffer D {D_TYPE dst[];};\n layout (binding = 0) readonly buffer AV4 {A_TYPE_VEC4 data_a_v4[];};\n layout (binding = 1) readonly buffer BV4 {B_TYPE_VEC4 data_b_v4[];};\n \n+layout (binding = 3) readonly buffer Bias {D_TYPE data_bias[];};\n+\n layout(constant_id = 0) const int BLOCK_SIZE = 32;\n // gqa_ratio is in the range [1,8]\n layout(constant_id = 1) const uint gqa_ratio = 1;\n@@ -29,6 +31,7 @@ layout (push_constant) uniform parameter\n     uint nchannels_y;\n     uint b_offset;\n     uint d_offset;\n+    uint enable_bias;\n } p;\n \n #if !USE_SUBGROUP_ADD\n@@ -148,6 +151,9 @@ void main() {\n         [[unroll]] for (uint c = 0; c < gqa_ratio; ++c) {\n             // dst is not transposed and not permuted\n             const uint idst = (channel + c)*nrows_dst + row_dst;\n+            if (p.enable_bias != 0) {\n+                temp[c] += FLOAT_TYPE(data_bias[idst]);\n+            }\n             dst[idst] = temp[c];\n         }\n     }"
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T23:29:15.660103"
  },
  {
    "pr_number": 16857,
    "title": "CUDA: add expert reduce kernel",
    "body": "This PR adds a kernel to fuse the common MoE mul + (n_expert_used-1)*add operations into one. 1-2% TG speed-up depending on num_expert_used\r\n\r\nTested on a 4090\r\n\r\n| Model                 | Test   |   t/s master |   t/s expert-reduce |   Speedup |\r\n|:----------------------|:-------|-------------:|--------------------:|----------:|\r\n| gpt-oss 20B MXFP4 MoE | tg32   |       198.21 |              200.64 |      1.01 |\r\n| gpt-oss 20B MXFP4 MoE | tg64   |       195.64 |              197.59 |      1.01 |\r\n| gpt-oss 20B MXFP4 MoE | tg128  |       193.52 |              194.77 |      1.01 |\r\n| qwen3moe 30B.A3B Q4_0 | tg32   |       167.77 |              171.98 |      1.03 |\r\n| qwen3moe 30B.A3B Q4_0 | tg64   |       161.53 |              165.00 |      1.02 |\r\n| qwen3moe 30B.A3B Q4_0 | tg128  |       159.33 |              162.50 |      1.02 |",
    "html_url": "https://github.com/ggml-org/llama.cpp/pull/16857",
    "created_at": "2025-10-30T08:10:22Z",
    "merged_at": "2025-10-31T12:05:07Z",
    "merge_commit_sha": "4146d6a1a6228711a487a1e3e9ddd120f8d027d7",
    "base_ref": "master",
    "head_sha": "2c10f1c4e375df159018438f6479badced14a51f",
    "user": "am17an",
    "files": [
      {
        "filename": "ggml/src/ggml-cuda/ggml-cuda.cu",
        "status": "modified",
        "additions": 26,
        "deletions": 0,
        "changes": 26,
        "patch": "@@ -27,6 +27,7 @@\n #include \"ggml-cuda/mmq.cuh\"\n #include \"ggml-cuda/mmvf.cuh\"\n #include \"ggml-cuda/mmvq.cuh\"\n+#include \"ggml-cuda/moe-expert-reduce.cuh\"\n #include \"ggml-cuda/norm.cuh\"\n #include \"ggml-cuda/opt-step-adamw.cuh\"\n #include \"ggml-cuda/opt-step-sgd.cuh\"\n@@ -3169,6 +3170,31 @@ static void evaluate_and_capture_cuda_graph(ggml_backend_cuda_context * cuda_ctx\n                         continue;\n                     }\n \n+                    if (node->op == GGML_OP_MUL) {\n+                        int current_node = i + 1;\n+                        int num_views    = 0;\n+                        int num_adds     = 0;\n+                        while (current_node < cgraph->n_nodes && cgraph->nodes[current_node]->op == GGML_OP_VIEW) {\n+                            num_views++;\n+                            current_node++;\n+                        }\n+\n+                        while (current_node < cgraph->n_nodes && cgraph->nodes[current_node]->op == GGML_OP_ADD &&\n+                                num_adds < num_views - 1) {\n+                            num_adds++;\n+                            current_node++;\n+                        }\n+\n+                        if (num_adds == num_views - 1 && num_views > 0) {\n+                            ggml_tensor * dst_node = cgraph->nodes[current_node - 1];\n+                            if (ggml_cuda_should_use_moe_expert_reduce(cgraph, i, current_node)) {\n+                                ggml_cuda_op_moe_expert_reduce(*cuda_ctx, node->src[0], node->src[1], dst_node);\n+                                i += num_views + num_adds;\n+                                continue;\n+                            }\n+                        }\n+                    }\n+\n                     if (node->op == GGML_OP_ADD) {\n                         int n_fuse = 0;\n                         ggml_op ops[8];"
      },
      {
        "filename": "ggml/src/ggml-cuda/moe-expert-reduce.cu",
        "status": "added",
        "additions": 168,
        "deletions": 0,
        "changes": 168,
        "patch": "@@ -0,0 +1,168 @@\n+#include \"moe-expert-reduce.cuh\"\n+\n+// This kernel is a fusion of the expert weight reduce, common in MoE models\n+\n+template <int n_expert_used_template>\n+__global__ void moe_expert_reduce_cuda(const float * __restrict__ experts,\n+                                       const float * __restrict__ weights,\n+                                       float * __restrict__ dst,\n+                                       const int n_expert_used,\n+                                       const int n_cols) {\n+    const int row = blockIdx.x;\n+    const int col = blockIdx.y * blockDim.x + threadIdx.x;\n+    if (col >= n_cols) {\n+        return;\n+    }\n+\n+    experts += row * n_cols * n_expert_used;\n+    weights += row * n_expert_used;\n+    dst += row * n_cols;\n+\n+    float acc = 0.f;\n+    if constexpr (n_expert_used_template == 0) {\n+        for (int expert = 0; expert < n_expert_used; ++expert) {\n+            ggml_cuda_mad(acc, experts[col], weights[expert]);\n+            experts += n_cols;\n+        }\n+        dst[col] = acc;\n+    } else {\n+#pragma unroll\n+        for (int i = 0; i < n_expert_used_template; ++i) {\n+            ggml_cuda_mad(acc, experts[col], weights[i]);\n+            experts += n_cols;\n+        }\n+        dst[col] = acc;\n+    }\n+}\n+\n+static void launch_moe_expert_reduce(ggml_backend_cuda_context & ctx,\n+                                     const float *               experts,\n+                                     const float *               weights,\n+                                     float *                     dst,\n+                                     const int                   n_expert_used,\n+                                     const int                   n_cols,\n+                                     const int                   n_rows) {\n+    const int block_size = 32;\n+\n+    const int n_blocks_x = n_rows;\n+    const int n_blocks_y = (n_cols + block_size - 1) / block_size;\n+\n+    dim3 block_dims(block_size);\n+    dim3 grid_dims(n_blocks_x, n_blocks_y);\n+\n+    cudaStream_t stream = ctx.stream();\n+    switch (n_expert_used) {\n+        case 1:\n+            moe_expert_reduce_cuda<1>\n+                <<<grid_dims, block_dims, 0, stream>>>(experts, weights, dst, n_expert_used, n_cols);\n+            break;\n+        case 2:\n+            moe_expert_reduce_cuda<2>\n+                <<<grid_dims, block_dims, 0, stream>>>(experts, weights, dst, n_expert_used, n_cols);\n+            break;\n+        case 4:\n+            moe_expert_reduce_cuda<4>\n+                <<<grid_dims, block_dims, 0, stream>>>(experts, weights, dst, n_expert_used, n_cols);\n+            break;\n+        case 6:\n+            moe_expert_reduce_cuda<6>\n+                <<<grid_dims, block_dims, 0, stream>>>(experts, weights, dst, n_expert_used, n_cols);\n+            break;\n+        case 8:\n+            moe_expert_reduce_cuda<8>\n+                <<<grid_dims, block_dims, 0, stream>>>(experts, weights, dst, n_expert_used, n_cols);\n+            break;\n+        case 16:\n+            moe_expert_reduce_cuda<16>\n+                <<<grid_dims, block_dims, 0, stream>>>(experts, weights, dst, n_expert_used, n_cols);\n+            break;\n+        case 32:\n+            moe_expert_reduce_cuda<32>\n+                <<<grid_dims, block_dims, 0, stream>>>(experts, weights, dst, n_expert_used, n_cols);\n+            break;\n+        case 64:\n+            moe_expert_reduce_cuda<64>\n+                <<<grid_dims, block_dims, 0, stream>>>(experts, weights, dst, n_expert_used, n_cols);\n+            break;\n+        case 128:\n+            moe_expert_reduce_cuda<128>\n+                <<<grid_dims, block_dims, 0, stream>>>(experts, weights, dst, n_expert_used, n_cols);\n+            break;\n+        default:\n+            moe_expert_reduce_cuda<0>\n+                <<<grid_dims, block_dims, 0, stream>>>(experts, weights, dst, n_expert_used, n_cols);\n+            break;\n+    }\n+}\n+\n+bool ggml_cuda_should_use_moe_expert_reduce(const ggml_cgraph * cgraph, int start_index, int end_index) {\n+    const ggml_tensor * mul = cgraph->nodes[start_index];\n+\n+    if (mul->op != GGML_OP_MUL || !ggml_is_contiguous(mul->src[0]) || !ggml_is_contiguous(mul->src[1])) {\n+        return false;\n+    }\n+\n+    int    current_node   = start_index + 1;\n+    size_t current_offset = 0;\n+\n+    std::vector<const ggml_tensor *> view_nodes;\n+    //check if all are views of the expert in increasing order\n+    while (current_node < end_index && cgraph->nodes[current_node]->op == GGML_OP_VIEW) {\n+        const ggml_tensor * node = cgraph->nodes[current_node];\n+        if (node->view_src != mul) {\n+            return false;\n+        }\n+        if (node->view_offs < current_offset) {\n+            return false;\n+        }\n+        current_offset = node->view_offs;\n+        current_node++;\n+        view_nodes.push_back(node);\n+    }\n+\n+    //check if all the adds are in increasing order\n+    const ggml_tensor * prev_add_src = view_nodes.empty() ? nullptr : view_nodes[0];\n+    int                 num_adds     = 0;\n+    int                 num_views    = view_nodes.size();\n+    while (current_node < end_index && cgraph->nodes[current_node]->op == GGML_OP_ADD) {\n+        const ggml_tensor * add_node = cgraph->nodes[current_node];\n+\n+        bool is_first_op_ok  = num_views > num_adds ? add_node->src[0] == prev_add_src : false;\n+        bool is_second_op_ok = num_views > num_adds ? add_node->src[1] == view_nodes[num_adds + 1] : false;\n+\n+        if (!is_first_op_ok || !is_second_op_ok) {\n+            return false;\n+        }\n+        prev_add_src = add_node;\n+\n+        num_adds++;\n+        current_node++;\n+    }\n+\n+    if (num_views != num_adds + 1) {\n+        return false;\n+    }\n+\n+    return true;\n+}\n+\n+void ggml_cuda_op_moe_expert_reduce(ggml_backend_cuda_context & ctx,\n+                                    const ggml_tensor *         experts,\n+                                    const ggml_tensor *         weights,\n+                                    ggml_tensor *               dst) {\n+    const int n_rows        = experts->ne[2];\n+    const int n_expert_used = experts->ne[1];\n+    const int n_cols        = experts->ne[0];\n+\n+    GGML_ASSERT(experts->type == GGML_TYPE_F32);\n+    GGML_ASSERT(weights->type == GGML_TYPE_F32);\n+    GGML_ASSERT(ggml_is_contiguous(experts));\n+    GGML_ASSERT(ggml_is_contiguous(weights));\n+    GGML_ASSERT(dst->type == GGML_TYPE_F32);\n+\n+    const float * experts_d = (const float *) experts->data;\n+    const float * weights_d = (const float *) weights->data;\n+    float *       dst_d     = (float *) dst->data;\n+\n+    launch_moe_expert_reduce(ctx, experts_d, weights_d, dst_d, n_expert_used, n_cols, n_rows);\n+}"
      },
      {
        "filename": "ggml/src/ggml-cuda/moe-expert-reduce.cuh",
        "status": "added",
        "additions": 11,
        "deletions": 0,
        "changes": 11,
        "patch": "@@ -0,0 +1,11 @@\n+#include \"common.cuh\"\n+#include \"ggml.h\"\n+\n+#include <initializer_list>\n+\n+void ggml_cuda_op_moe_expert_reduce(ggml_backend_cuda_context & ctx,\n+                                    const ggml_tensor *         experts,\n+                                    const ggml_tensor *         weights,\n+                                    ggml_tensor *               dst);\n+\n+bool ggml_cuda_should_use_moe_expert_reduce(const ggml_cgraph * cgraph, int start_index, int end_index);"
      },
      {
        "filename": "tests/test-backend-ops.cpp",
        "status": "modified",
        "additions": 58,
        "deletions": 0,
        "changes": 58,
        "patch": "@@ -4742,6 +4742,60 @@ struct test_topk_moe: public test_case {\n     }\n };\n \n+struct test_moe_expert_reduce : public test_case {\n+    const int64_t n_embd;\n+    const int64_t n_tokens;\n+    const int64_t n_expert_used;\n+\n+    test_moe_expert_reduce(int64_t n_embd = 64, int64_t n_tokens = 5, int64_t n_expert_used = 4)\n+        : n_embd(n_embd), n_tokens(n_tokens), n_expert_used(n_expert_used) {\n+        GGML_ASSERT(n_expert_used > 1);\n+    }\n+\n+    std::string vars() override {\n+        return VARS_TO_STR3(n_embd, n_tokens, n_expert_used);\n+    }\n+\n+    std::string op_desc(ggml_tensor * t) override {\n+        GGML_UNUSED(t);\n+        return \"MOE_EXPERT_REDUCE\";\n+    }\n+\n+    bool run_whole_graph() override { return true; }\n+\n+    ggml_tensor * build_graph(ggml_context * ctx) override {\n+        ggml_tensor * experts = ggml_new_tensor_3d(ctx, GGML_TYPE_F32, n_embd, n_expert_used, n_tokens);\n+        ggml_set_name(experts, \"experts\");\n+\n+        ggml_tensor * weights = ggml_new_tensor_3d(ctx, GGML_TYPE_F32, 1, n_expert_used, n_tokens);\n+        ggml_set_name(weights, \"weights\");\n+\n+        ggml_tensor * weighted = ggml_mul(ctx, experts, weights);\n+        ggml_set_name(weighted, \"weighted_experts\");\n+\n+        std::vector<ggml_tensor *> expert_views(n_expert_used);\n+        for (int64_t i = 0; i < n_expert_used; ++i) {\n+            expert_views[i] = ggml_view_2d(ctx, weighted, n_embd, n_tokens, weighted->nb[2], i * weighted->nb[1]);\n+\n+            std::string name = \"expert_view_\" + std::to_string(i);\n+            ggml_set_name(expert_views[i], name.c_str());\n+            ggml_build_forward_expand(gf, expert_views[i]);\n+        }\n+\n+        ggml_tensor * moe_out = expert_views[0];\n+        for (int64_t i = 1; i < n_expert_used; ++i) {\n+            moe_out = ggml_add(ctx, moe_out, expert_views[i]);\n+\n+            std::string name = \"expert_add_\" + std::to_string(i - 1);\n+            ggml_set_name(moe_out, name.c_str());\n+        }\n+\n+        ggml_set_name(moe_out, \"moe_out\");\n+\n+        return moe_out;\n+    }\n+};\n+\n struct test_mul_mat_vec_fusion : public test_case {\n     const ggml_type type;\n     const ggml_glu_op glu_op;\n@@ -7179,6 +7233,10 @@ static std::vector<std::unique_ptr<test_case>> make_test_cases_eval() {\n     test_cases.emplace_back(new test_topk_moe({ 8, 22, 1, 1 }, 4, /*with_norm*/ false, /*delayed_softmax*/ true));\n     test_cases.emplace_back(new test_topk_moe({ 32, 22, 1, 1 }, 8, /*with_norm*/ false, /*delayed_softmax*/ true));\n \n+    test_cases.emplace_back(new test_moe_expert_reduce(1024, 5, 4));\n+    test_cases.emplace_back(new test_moe_expert_reduce(80, 3, 6));\n+    test_cases.emplace_back(new test_moe_expert_reduce(80, 3, 7));\n+\n #if 0\n     // these tests are disabled to save execution time, sbut they can be handy for debugging\n     test_cases.emplace_back(new test_llama(2, true));"
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T23:29:17.695852"
  },
  {
    "pr_number": 16843,
    "title": "CUDA: Volta tensor core support for MMF",
    "body": "This PR adds support for Volta tensor cores to the `mul_mat_f` kernel. The longer-term goal is to enable these tensor cores also for the MMA FlashAttention kernel.\r\n\r\n<details>\r\n<summary>Performance changes</summary>\r\n\r\n| GPU            | Model             |   Microbatch size | Test   |   t/s master |   t/s 6537cc8a2 |   Speedup |\r\n|:---------------|:------------------|------------------:|:-------|-------------:|----------------:|----------:|\r\n| V100-PCIE-32GB | deepseek2 16B F16 |                 1 | pp512  |        88.54 |           88.39 |      1.00 |\r\n| V100-PCIE-32GB | deepseek2 16B F16 |                 2 | pp512  |        72.62 |          141.05 |      1.94 |\r\n| V100-PCIE-32GB | deepseek2 16B F16 |                 3 | pp512  |        89.10 |          174.72 |      1.96 |\r\n| V100-PCIE-32GB | deepseek2 16B F16 |                 4 | pp512  |       101.98 |          229.25 |      2.25 |\r\n| V100-PCIE-32GB | deepseek2 16B F16 |                 5 | pp512  |       114.94 |          266.92 |      2.32 |\r\n| V100-PCIE-32GB | deepseek2 16B F16 |                 6 | pp512  |       130.69 |          301.50 |      2.31 |\r\n| V100-PCIE-32GB | deepseek2 16B F16 |                 7 | pp512  |       142.80 |          331.81 |      2.32 |\r\n| V100-PCIE-32GB | deepseek2 16B F16 |                 8 | pp512  |       154.34 |          363.39 |      2.35 |\r\n| V100-PCIE-32GB | deepseek2 16B F16 |                 9 | pp512  |       163.69 |          376.59 |      2.30 |\r\n| V100-PCIE-32GB | deepseek2 16B F16 |                10 | pp512  |       179.43 |          409.79 |      2.28 |\r\n| V100-PCIE-32GB | deepseek2 16B F16 |                11 | pp512  |       190.28 |          437.53 |      2.30 |\r\n| V100-PCIE-32GB | deepseek2 16B F16 |                12 | pp512  |       205.59 |          468.86 |      2.28 |\r\n| V100-PCIE-32GB | deepseek2 16B F16 |                13 | pp512  |       206.84 |          477.87 |      2.31 |\r\n| V100-PCIE-32GB | deepseek2 16B F16 |                14 | pp512  |       223.11 |          511.71 |      2.29 |\r\n| V100-PCIE-32GB | deepseek2 16B F16 |                15 | pp512  |       233.90 |          531.07 |      2.27 |\r\n| V100-PCIE-32GB | deepseek2 16B F16 |                16 | pp512  |       241.63 |          548.72 |      2.27 |\r\n| V100-PCIE-32GB | deepseek2 16B F16 |                32 | pp512  |       388.11 |          847.36 |      2.18 |\r\n| V100-PCIE-32GB | deepseek2 16B F16 |                64 | pp512  |       653.26 |         1267.11 |      1.94 |\r\n| V100-PCIE-32GB | deepseek2 16B F16 |               128 | pp512  |      1032.66 |         1690.62 |      1.64 |\r\n| V100-PCIE-32GB | deepseek2 16B F16 |               256 | pp512  |      1651.41 |         1662.73 |      1.01 |\r\n| V100-PCIE-32GB | deepseek2 16B F16 |               512 | pp512  |      2276.55 |         2280.14 |      1.00 |\r\n| V100-PCIE-32GB | llama 8B F16      |                 1 | pp512  |        56.56 |           56.55 |      1.00 |\r\n| V100-PCIE-32GB | llama 8B F16      |                 2 | pp512  |       106.37 |          106.35 |      1.00 |\r\n| V100-PCIE-32GB | llama 8B F16      |                 3 | pp512  |       121.33 |          121.35 |      1.00 |\r\n| V100-PCIE-32GB | llama 8B F16      |                 4 | pp512  |       147.60 |          194.89 |      1.32 |\r\n| V100-PCIE-32GB | llama 8B F16      |                 5 | pp512  |       183.08 |          240.10 |      1.31 |\r\n| V100-PCIE-32GB | llama 8B F16      |                 6 | pp512  |       218.26 |          284.60 |      1.30 |\r\n| V100-PCIE-32GB | llama 8B F16      |                 7 | pp512  |       252.91 |          327.78 |      1.30 |\r\n| V100-PCIE-32GB | llama 8B F16      |                 8 | pp512  |       289.76 |          375.19 |      1.29 |\r\n| V100-PCIE-32GB | llama 8B F16      |                 9 | pp512  |       330.82 |          410.80 |      1.24 |\r\n| V100-PCIE-32GB | llama 8B F16      |                10 | pp512  |       362.85 |          448.04 |      1.23 |\r\n| V100-PCIE-32GB | llama 8B F16      |                11 | pp512  |       387.93 |          489.95 |      1.26 |\r\n| V100-PCIE-32GB | llama 8B F16      |                12 | pp512  |       422.32 |          532.48 |      1.26 |\r\n| V100-PCIE-32GB | llama 8B F16      |                13 | pp512  |       452.50 |          567.58 |      1.25 |\r\n| V100-PCIE-32GB | llama 8B F16      |                14 | pp512  |       486.98 |          609.92 |      1.25 |\r\n| V100-PCIE-32GB | llama 8B F16      |                15 | pp512  |       517.35 |          639.23 |      1.24 |\r\n| V100-PCIE-32GB | llama 8B F16      |                16 | pp512  |       558.80 |          691.65 |      1.24 |\r\n\r\n</details>",
    "html_url": "https://github.com/ggml-org/llama.cpp/pull/16843",
    "created_at": "2025-10-29T13:52:30Z",
    "merged_at": "2025-10-31T14:57:20Z",
    "merge_commit_sha": "31c511a968348281e11d590446bb815048a1e912",
    "base_ref": "master",
    "head_sha": "53badac6c343bf9f6811c84337dacce9934b32ff",
    "user": "JohannesGaessler",
    "files": [
      {
        "filename": "ggml/src/ggml-cuda/common.cuh",
        "status": "modified",
        "additions": 9,
        "deletions": 1,
        "changes": 10,
        "patch": "@@ -224,6 +224,11 @@ static const char * cu_get_error_str(CUresult err) {\n #define AMD_MFMA_AVAILABLE\n #endif // defined(GGML_USE_HIP) && defined(CDNA) && !defined(GGML_HIP_NO_MMQ_MFMA)\n \n+// The Volta instructions are in principle available on Turing or newer but they are effectively unusable:\n+#if !defined(GGML_USE_HIP) && __CUDA_ARCH__ == GGML_CUDA_CC_VOLTA\n+#define VOLTA_MMA_AVAILABLE\n+#endif // !defined(GGML_USE_HIP) && __CUDA_ARCH__ == GGML_CUDA_CC_VOLTA\n+\n #if !defined(GGML_USE_HIP) && __CUDA_ARCH__ >= GGML_CUDA_CC_TURING\n #define TURING_MMA_AVAILABLE\n #endif // !defined(GGML_USE_HIP) && __CUDA_ARCH__ >= GGML_CUDA_CC_TURING\n@@ -278,7 +283,10 @@ static bool amd_mfma_available(const int cc) {\n #endif //!defined(GGML_HIP_NO_MMQ_MFMA)\n }\n \n-// Volta technically had FP16 tensor cores but they work very differently compared to Turing and later.\n+static bool volta_mma_available(const int cc) {\n+    return GGML_CUDA_CC_IS_NVIDIA(cc) && ggml_cuda_highest_compiled_arch(cc) == GGML_CUDA_CC_VOLTA;\n+}\n+\n static bool turing_mma_available(const int cc) {\n     return GGML_CUDA_CC_IS_NVIDIA(cc) && ggml_cuda_highest_compiled_arch(cc) >= GGML_CUDA_CC_TURING;\n }"
      },
      {
        "filename": "ggml/src/ggml-cuda/mma.cuh",
        "status": "modified",
        "additions": 213,
        "deletions": 24,
        "changes": 237,
        "patch": "@@ -18,6 +18,10 @@\n \n #include \"common.cuh\"\n \n+// On Volta each warp is doing 4 8x8 mma operations in parallel.\n+// The basic memory layout for a 32x8 output tile is to stack 4 input tiles in I direction and to mirror the B tile.\n+// However, the i indices in this file are by default permuted to simplify the index calculations.\n+// #define GGML_CUDA_MMA_NO_VOLTA_PERM\n \n #if CUDART_VERSION >= 11080\n \n@@ -73,6 +77,15 @@ namespace ggml_cuda_mma {\n         static constexpr int ne = I * J / 64;\n         T x[ne] = {0};\n \n+        static constexpr __device__ bool supported() {\n+            if (I == 64 && J ==  2) return true;\n+            if (I == 16 && J ==  8) return true;\n+            if (I == 32 && J ==  4) return true;\n+            if (I == 16 && J == 16) return true;\n+            if (I == 32 && J == 32) return true;\n+            return false;\n+        }\n+\n         static __device__ __forceinline__ int get_i(const int l) {\n             if constexpr (I == 64 && J == 2) { // Special tile size to load <16, 4> as <16, 8>\n                 return threadIdx.x % 16;\n@@ -85,7 +98,8 @@ namespace ggml_cuda_mma {\n             } else if constexpr (I == 32 && J == 32) {\n                 return 4 * (threadIdx.x / 32) + 8 * (l / 4) + (l % 4);\n             } else {\n-                static_assert(I == -1 && J == -1, \"template specialization not implemented\");\n+                NO_DEVICE_CODE;\n+                return -1;\n             }\n         }\n \n@@ -101,36 +115,84 @@ namespace ggml_cuda_mma {\n             } else if constexpr (I == 32 && J == 32) {\n                 return threadIdx.x % 32;\n             } else {\n-                static_assert(I == -1 && J == -1, \"template specialization not implemented\");\n+                NO_DEVICE_CODE;\n+                return -1;\n+            }\n+        }\n+#elif __CUDA_ARCH__ == GGML_CUDA_CC_VOLTA\n+        static constexpr int ne = I * J / 32;\n+        T x[ne] = {0};\n+\n+        static constexpr __device__ bool supported() {\n+            if (I == 32 && J ==  8) return true;\n+            return false;\n+        }\n+\n+        static __device__ __forceinline__ int get_i(const int l) {\n+            if constexpr (I == 32 && J == 8) {\n+#ifdef GGML_CUDA_MMA_NO_VOLTA_PERM\n+                return (((threadIdx.x % 16) / 4) * 8) | ((threadIdx.x / 16) * 4) | (l & 2) | (threadIdx.x % 2);\n+#else\n+                return (l & 2) | (threadIdx.x & ~2);\n+#endif // GGML_CUDA_MMA_NO_VOLTA_PERM\n+            } else {\n+                NO_DEVICE_CODE;\n+                return -1;\n+            }\n+        }\n+\n+        static __device__ __forceinline__ int get_j(const int l) {\n+            if constexpr (I == 32 && J == 8) {\n+                return (threadIdx.x & 2) | (l & (4 + 1));\n+            } else {\n+                NO_DEVICE_CODE;\n+                return -1;\n             }\n         }\n #else\n         static constexpr int ne = I * J / 32;\n         T x[ne] = {0};\n \n+        static constexpr __device__ bool supported() {\n+            if (I ==  8 && J ==  4) return true;\n+            if (I ==  8 && J ==  8) return true;\n+            if (I == 16 && J ==  8) return true;\n+            if (I == 16 && J == 16) return true;\n+            if (I == 32 && J ==  8) return true;\n+            return false;\n+        }\n+\n         static __device__ __forceinline__ int get_i(const int l) {\n-            if constexpr (I == 8 && (J == 4 || J == 8)) {\n+            if constexpr (I == 8 && J == 4) {\n+                return threadIdx.x / 4;\n+            } else if constexpr (I == 8 && J == 8) {\n                 return threadIdx.x / 4;\n             } else if constexpr (I == 16 && J == 8) {\n-                return (l / 2) * 8 + threadIdx.x / 4;\n+                return ((l / 2) * 8) | (threadIdx.x / 4);\n             } else if constexpr (I == 16 && J == 16) {\n-                return ((l / 2) % 2) * 8 + threadIdx.x / 4;\n+                return (((l / 2) % 2) * 8) | (threadIdx.x / 4);\n+            } else if constexpr (I == 32 && J == 8) {\n+                return tile<16, 8, T>::get_i(l); // Memory layout simply repeated with same pattern in i direction.\n             } else {\n-                static_assert(I == -1 && J == -1, \"template specialization not implemented\");\n+                NO_DEVICE_CODE;\n+                return -1;\n             }\n         }\n \n         static __device__ __forceinline__ int get_j(const int l) {\n             if constexpr (I == 8 && J == 4) {\n                 return threadIdx.x % 4;\n             } else if constexpr (I == 8 && J == 8) {\n-                return 4 * l + threadIdx.x % 4;\n+                return (l * 4) | (threadIdx.x % 4);\n             } else if constexpr (I == 16 && J == 8) {\n-                return 2 * (threadIdx.x % 4) + l % 2;\n+                return ((threadIdx.x % 4) * 2) | (l % 2);\n             } else if constexpr (I == 16 && J == 16) {\n-                return 8 * (l / 4) + 2 * (threadIdx.x % 4) + l % 2;\n+                return ((l / 4) * 8) | ((threadIdx.x % 4) * 2) | (l % 2);\n+            } else if constexpr (I == 32 && J == 8) {\n+                return tile<16, 8, T>::get_j(l); // Memory layout simply repeated with same pattern in i direction.\n             } else {\n-                static_assert(I == -1 && J == -1, \"template specialization not implemented\");\n+                NO_DEVICE_CODE;\n+                return -1;\n             }\n         }\n #endif // defined(GGML_USE_HIP)\n@@ -140,32 +202,83 @@ namespace ggml_cuda_mma {\n     struct tile<I_, J_, half2> {\n         static constexpr int I  = I_;\n         static constexpr int J  = J_;\n+\n+#if __CUDA_ARCH__ == GGML_CUDA_CC_VOLTA\n+        static constexpr int ne = I == 8 && J == 8 ? I * J / (WARP_SIZE/4) : I * J / WARP_SIZE;\n+        half2 x[ne] = {{0.0f, 0.0f}};\n+\n+        static constexpr __device__ bool supported() {\n+            if (I ==  8 && J ==  8) return true;\n+            if (I == 32 && J ==  8) return true;\n+            return false;\n+        }\n+\n+        static __device__ __forceinline__ int get_i(const int l) {\n+            if constexpr (I == 8 && J == 8) {\n+                return ((threadIdx.x / 16) * 4) | (threadIdx.x % 4);\n+            } else if constexpr (I == 32 && J == 8) {\n+#ifdef GGML_CUDA_MMA_NO_VOLTA_PERM\n+                return (((threadIdx.x % 16) / 4) * 8) | ((threadIdx.x / 16) * 4) | (threadIdx.x % 4);\n+#else\n+                return threadIdx.x;\n+#endif // GGML_CUDA_MMA_NO_VOLTA_PERM\n+            } else {\n+                NO_DEVICE_CODE;\n+                return -1;\n+            }\n+        }\n+\n+        static __device__ __forceinline__ int get_j(const int l) {\n+            if constexpr ((I == 8 || I == 32) && J == 8) {\n+                return l;\n+            } else {\n+                NO_DEVICE_CODE;\n+                return -1;\n+            }\n+        }\n+#else\n         static constexpr int ne = I * J / WARP_SIZE;\n         half2 x[ne] = {{0.0f, 0.0f}};\n \n+        static constexpr __device__ bool supported() {\n+            if (I ==  8 && J ==  4) return true;\n+            if (I ==  8 && J ==  8) return true;\n+            if (I == 16 && J ==  8) return true;\n+            if (I == 16 && J == 16) return true;\n+            if (I == 32 && J ==  8) return true;\n+            return false;\n+        }\n+\n         static __device__ __forceinline__ int get_i(const int l) {\n             if constexpr (I == 8 && J == 8) {\n                 return threadIdx.x / 4;\n             } else if constexpr (I == 16 && J == 4) {\n-                return l * 8 + threadIdx.x / 4;\n+                return (l * 8) | (threadIdx.x / 4);\n             } else if constexpr (I == 16 && J == 8) {\n-                return (l % 2) * 8 + threadIdx.x / 4;\n+                return ((l % 2) * 8) | (threadIdx.x / 4);\n+            } else if constexpr (I == 32 && J == 8) {\n+                return ((l / 4) * 16) | ((l % 2) * 8) | (threadIdx.x / 4);\n             } else {\n-                static_assert(I == -1 && J == -1, \"template specialization not implemented\");\n+                NO_DEVICE_CODE;\n+                return -1;\n             }\n         }\n \n         static __device__ __forceinline__ int get_j(const int l) {\n             if constexpr (I == 8 && J == 8) {\n-                return l * 4 + threadIdx.x % 4;\n+                return (l * 4) | (threadIdx.x % 4);\n             } else if constexpr (I == 16 && J == 4) {\n                 return threadIdx.x % 4;\n             } else if constexpr (I == 16 && J == 8) {\n-                return (l / 2) * 4 + threadIdx.x % 4;\n+                return ((l / 2) * 4) | (threadIdx.x % 4);\n+            } else if constexpr (I == 32 && J == 8) {\n+                return ((l & 2) * 2) | (threadIdx.x % 4);\n             } else {\n-                static_assert(I == -1 && J == -1, \"template specialization not implemented\");\n+                NO_DEVICE_CODE;\n+                return -1;\n             }\n         }\n+#endif // __CUDA_ARCH__ == GGML_CUDA_CC_VOLTA\n     };\n \n     template <int I_, int J_>\n@@ -175,27 +288,36 @@ namespace ggml_cuda_mma {\n         static constexpr int ne = I * J / WARP_SIZE;\n         nv_bfloat162 x[ne] = {{0.0f, 0.0f}};\n \n+        static constexpr __device__ bool supported() {\n+            if (I ==  8 && J ==  8) return true;\n+            if (I == 16 && J ==  4) return true;\n+            if (I == 16 && J ==  8) return true;\n+            return false;\n+        }\n+\n         static __device__ __forceinline__ int get_i(const int l) {\n             if constexpr (I == 8 && J == 8) {\n                 return threadIdx.x / 4;\n             } else if constexpr (I == 16 && J == 4) {\n-                return l * 8 + threadIdx.x / 4;\n+                return (l * 8) | (threadIdx.x / 4);\n             } else if constexpr (I == 16 && J == 8) {\n-                return (l % 2) * 8 + threadIdx.x / 4;\n+                return ((l % 2) * 8) | (threadIdx.x / 4);\n             } else {\n-                static_assert(I == -1 && J == -1, \"template specialization not implemented\");\n+                NO_DEVICE_CODE;\n+                return -1;\n             }\n         }\n \n         static __device__ __forceinline__ int get_j(const int l) {\n             if constexpr (I == 8 && J == 8) {\n-                return l * 4 + threadIdx.x % 4;\n+                return (l * 4) | (threadIdx.x % 4);\n             } else if constexpr (I == 16 && J == 4) {\n                 return threadIdx.x % 4;\n             } else if constexpr (I == 16 && J == 8) {\n-                return (l / 2) * 4 + threadIdx.x % 4;\n+                return ((l / 2) * 4) | (threadIdx.x % 4);\n             } else {\n-                static_assert(I == -1 && J == -1, \"template specialization not implemented\");\n+                NO_DEVICE_CODE;\n+                return -1;\n             }\n         }\n     };\n@@ -263,8 +385,12 @@ namespace ggml_cuda_mma {\n             : \"=r\"(xi[0]), \"=r\"(xi[1])\n             : \"l\"(xs));\n #else\n-        load_generic(xs0, stride);\n-        GGML_UNUSED(t);\n+#if __CUDA_ARCH__ == GGML_CUDA_CC_VOLTA\n+        GGML_UNUSED_VARS(t, xs0, stride);\n+        NO_DEVICE_CODE;\n+#else\n+        load_generic(t, xs0, stride);\n+#endif // __CUDA_ARCH__ == GGML_CUDA_CC_VOLTA\n #endif // TURING_MMA_AVAILABLE\n     }\n \n@@ -277,11 +403,35 @@ namespace ggml_cuda_mma {\n         asm volatile(\"ldmatrix.sync.aligned.m8n8.x4.b16 {%0, %1, %2, %3}, [%4];\"\n             : \"=r\"(xi[0]), \"=r\"(xi[1]), \"=r\"(xi[2]), \"=r\"(xi[3])\n             : \"l\"(xs));\n+#else\n+#if __CUDA_ARCH__ == GGML_CUDA_CC_VOLTA\n+        GGML_UNUSED_VARS(t, xs0, stride);\n+        NO_DEVICE_CODE;\n #else\n         load_generic(t, xs0, stride);\n+#endif // __CUDA_ARCH__ == GGML_CUDA_CC_VOLTA\n #endif // TURING_MMA_AVAILABLE\n     }\n \n+    template <typename T>\n+    static __device__ __forceinline__ void load_ldmatrix(\n+            tile<32, 8, T> & t, const T * __restrict__ xs0, const int stride) {\n+#if __CUDA_ARCH__ == GGML_CUDA_CC_VOLTA\n+#if 1\n+        // TODO: more generic handling\n+        static_assert(sizeof(T) == 4, \"bad type size\");\n+        ggml_cuda_memcpy_1<4*sizeof(T)>(t.x + 0, xs0 + t.get_i(0)*stride + 0);\n+        ggml_cuda_memcpy_1<4*sizeof(T)>(t.x + 4, xs0 + t.get_i(4)*stride + 4);\n+#else\n+        load_generic(t, xs0, stride);\n+#endif // 1\n+#else\n+        tile<16, 8, T> * t16 = (tile<16, 8, T> *) &t;\n+        load_ldmatrix(t16[0], xs0 +  0*stride, stride);\n+        load_ldmatrix(t16[1], xs0 + 16*stride, stride);\n+#endif // __CUDA_ARCH__ == GGML_CUDA_CC_VOLTA\n+    }\n+\n     template <typename T>\n     static __device__ __forceinline__ void load_ldmatrix_trans(\n             tile<16, 8, T> & t, const T * __restrict__ xs0, const int stride) {\n@@ -546,4 +696,43 @@ namespace ggml_cuda_mma {\n         NO_DEVICE_CODE;\n #endif // AMD_MFMA_AVAILABLE\n     }\n+\n+    template <typename T1, typename T2, int J, int K>\n+    static __device__ __forceinline__ void mma(\n+            tile<32, J, T1> & D, const tile<32, K, T2> & A, const tile<J, K, T2> & B) {\n+        tile<16, J, T1> * D16 = (tile<16, J, T1> *) &D;\n+        tile<16, K, T2> * A16 = (tile<16, K, T2> *) &A;\n+        mma(D16[0], A16[0], B);\n+        mma(D16[1], A16[1], B);\n+    }\n+\n+    static __device__ __forceinline__ void mma(\n+            tile<32, 8, float> & D, const tile<32, 8, half2> & A, const tile<8, 8, half2> & B) {\n+#if __CUDA_ARCH__ == GGML_CUDA_CC_VOLTA\n+        const int * Axi = (const int *) A.x;\n+        const int * Bxi = (const int *) B.x;\n+        int       * Dxi = (int       *) D.x;\n+        asm(\"mma.sync.aligned.m8n8k4.row.col.f32.f16.f16.f32 \"\n+            \"{%0, %1, %2, %3, %4, %5, %6, %7}, {%8, %9}, {%10, %11}, {%0, %1, %2, %3, %4, %5, %6, %7};\"\n+            : \"+r\"(Dxi[0]), \"+r\"(Dxi[1]), \"+r\"(Dxi[2]), \"+r\"(Dxi[3]), \"+r\"(Dxi[4]), \"+r\"(Dxi[5]), \"+r\"(Dxi[6]), \"+r\"(Dxi[7])\n+            : \"r\"(Axi[0]), \"r\"(Axi[1]), \"r\"(Bxi[0]), \"r\"(Bxi[1]));\n+        asm(\"mma.sync.aligned.m8n8k4.row.col.f32.f16.f16.f32 \"\n+            \"{%0, %1, %2, %3, %4, %5, %6, %7}, {%8, %9}, {%10, %11}, {%0, %1, %2, %3, %4, %5, %6, %7};\"\n+            : \"+r\"(Dxi[0]), \"+r\"(Dxi[1]), \"+r\"(Dxi[2]), \"+r\"(Dxi[3]), \"+r\"(Dxi[4]), \"+r\"(Dxi[5]), \"+r\"(Dxi[6]), \"+r\"(Dxi[7])\n+            : \"r\"(Axi[2]), \"r\"(Axi[3]), \"r\"(Bxi[2]), \"r\"(Bxi[3]));\n+        asm(\"mma.sync.aligned.m8n8k4.row.col.f32.f16.f16.f32 \"\n+            \"{%0, %1, %2, %3, %4, %5, %6, %7}, {%8, %9}, {%10, %11}, {%0, %1, %2, %3, %4, %5, %6, %7};\"\n+            : \"+r\"(Dxi[0]), \"+r\"(Dxi[1]), \"+r\"(Dxi[2]), \"+r\"(Dxi[3]), \"+r\"(Dxi[4]), \"+r\"(Dxi[5]), \"+r\"(Dxi[6]), \"+r\"(Dxi[7])\n+            : \"r\"(Axi[4]), \"r\"(Axi[5]), \"r\"(Bxi[4]), \"r\"(Bxi[5]));\n+        asm(\"mma.sync.aligned.m8n8k4.row.col.f32.f16.f16.f32 \"\n+            \"{%0, %1, %2, %3, %4, %5, %6, %7}, {%8, %9}, {%10, %11}, {%0, %1, %2, %3, %4, %5, %6, %7};\"\n+            : \"+r\"(Dxi[0]), \"+r\"(Dxi[1]), \"+r\"(Dxi[2]), \"+r\"(Dxi[3]), \"+r\"(Dxi[4]), \"+r\"(Dxi[5]), \"+r\"(Dxi[6]), \"+r\"(Dxi[7])\n+            : \"r\"(Axi[6]), \"r\"(Axi[7]), \"r\"(Bxi[6]), \"r\"(Bxi[7]));\n+#else\n+        tile<16, 8, float> * D16 = (tile<16, 8, float> *) &D;\n+        tile<16, 8, half2> * A16 = (tile<16, 8, half2> *) &A;\n+        mma(D16[0], A16[0], B);\n+        mma(D16[1], A16[1], B);\n+#endif // __CUDA_ARCH__ >= GGML_CUDA_CC_AMPERE\n+    }\n }"
      },
      {
        "filename": "ggml/src/ggml-cuda/mmf.cu",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -148,7 +148,7 @@ bool ggml_cuda_should_use_mmf(enum ggml_type type, int cc, int warp_size, const\n         case GGML_TYPE_F32:\n             return ampere_mma_available(cc);\n         case GGML_TYPE_F16:\n-            return turing_mma_available(cc);\n+            return volta_mma_available(cc) || turing_mma_available(cc);\n         case GGML_TYPE_BF16:\n             return ampere_mma_available(cc);\n         default:"
      },
      {
        "filename": "ggml/src/ggml-cuda/mmf.cuh",
        "status": "modified",
        "additions": 31,
        "deletions": 10,
        "changes": 41,
        "patch": "@@ -28,9 +28,19 @@ static __global__ void mul_mat_f(\n         const int channel_ratio, const int stride_channel_x, const int stride_channel_y, const int stride_channel_dst,\n         const int sample_ratio, const int stride_sample_x, const int stride_sample_y, const int stride_sample_dst) {\n #if !defined(GGML_USE_HIP) && !defined(GGML_USE_MUSA)\n-    typedef tile<16, 8, T>     tile_A;\n-    typedef tile< 8, 8, T>     tile_B;\n-    typedef tile<16, 8, float> tile_C;\n+    constexpr bool I_16_supported = tile<16, 8, T>::supported() && tile<16, 8, float>::supported();\n+    constexpr bool I_32_supported = tile<32, 8, T>::supported() && tile<32, 8, float>::supported();\n+\n+    if (!I_16_supported && !I_32_supported) {\n+        NO_DEVICE_CODE;\n+        return;\n+    }\n+\n+    constexpr int I_preferred = I_16_supported ? 16 : 32; // For Turing MMA both work but 16 is ~1% faster.\n+\n+    typedef tile<I_preferred, 8, T>     tile_A;\n+    typedef tile<8,           8, T>     tile_B;\n+    typedef tile<I_preferred, 8, float> tile_C;\n \n     constexpr int warp_size = ggml_cuda_get_physical_warp_size();\n     constexpr int tile_k_padded = warp_size + 4;\n@@ -232,7 +242,6 @@ static __global__ void mul_mat_f(\n #endif // !defined(GGML_USE_HIP) && !defined(GGML_USE_MUSA)\n }\n \n-\n //This kernel is for larger batch sizes of mul_mat_id\n template <typename T, int rows_per_block, int cols_per_block, int nwarps>\n __launch_bounds__(ggml_cuda_get_physical_warp_size()*nwarps, 1)\n@@ -245,9 +254,19 @@ static __global__ void mul_mat_f_ids(\n         const int sample_ratio, const int stride_sample_x, const int stride_sample_y, const int stride_sample_dst,\n         const uint3 sis1_fd, const uint3 nch_fd) {\n #if !defined(GGML_USE_HIP) && !defined(GGML_USE_MUSA)\n-    typedef tile<16, 8, T>     tile_A;\n-    typedef tile< 8, 8, T>     tile_B;\n-    typedef tile<16, 8, float> tile_C;\n+    constexpr bool I_16_supported = tile<16, 8, T>::supported() && tile<16, 8, float>::supported();\n+    constexpr bool I_32_supported = tile<32, 8, T>::supported() && tile<32, 8, float>::supported();\n+\n+    if (!I_16_supported && !I_32_supported) {\n+        NO_DEVICE_CODE;\n+        return;\n+    }\n+\n+    constexpr int I_preferred = I_16_supported ? 16 : 32; // For Turing MMA both work butr 16 is ~1% faster.\n+\n+    typedef tile<I_preferred, 8, T>     tile_A;\n+    typedef tile<8,           8, T>     tile_B;\n+    typedef tile<I_preferred, 8, float> tile_C;\n \n     constexpr int warp_size = ggml_cuda_get_physical_warp_size();\n     constexpr int tile_k_padded = warp_size + 4;\n@@ -533,7 +552,8 @@ void mul_mat_f_cuda(\n         const int64_t stride_channel_x, const int64_t stride_channel_y, const int64_t stride_channel_dst, const int64_t nsamples_x,\n         const int64_t nsamples_dst, const int64_t stride_sample_x, const int64_t stride_sample_y, const int64_t stride_sample_dst,\n         cudaStream_t stream, const mmf_ids_data * ids_data) {\n-    typedef tile<16, 8, T>     tile_A;\n+    typedef tile<16, 8, T>     tile_A_16;\n+    typedef tile<32, 8, T>     tile_A_32;\n     typedef tile< 8, 8, T>     tile_B;\n \n     GGML_ASSERT(ncols_x      % 2 == 0);\n@@ -544,7 +564,8 @@ void mul_mat_f_cuda(\n     const int64_t channel_ratio = nchannels_dst / nchannels_x;\n     const int64_t sample_ratio  = nsamples_dst  / nsamples_x;\n \n-    const int device = ggml_cuda_get_device();\n+    const int device    = ggml_cuda_get_device();\n+    const int cc        = ggml_cuda_info().devices[device].cc;\n     const int warp_size = ggml_cuda_info().devices[device].warp_size;\n \n     int64_t nwarps_best     = 1;\n@@ -559,7 +580,7 @@ void mul_mat_f_cuda(\n     }\n \n     constexpr int rows_per_block = MMF_ROWS_PER_BLOCK;\n-    const int nbytes_shared_iter = nwarps_best * tile_A::I * (warp_size + 4) * 4;\n+    const int nbytes_shared_iter = nwarps_best * (volta_mma_available(cc) ? tile_A_32::I : tile_A_16::I) * (warp_size + 4) * 4;\n     const int nbytes_shared_combine = GGML_PAD(cols_per_block, tile_B::I) * (nwarps_best*rows_per_block + 4) * 4;\n     const int nbytes_shared = std::max(nbytes_shared_iter, nbytes_shared_combine);\n     const int nbytes_slotmap = ids ? GGML_PAD(cols_per_block, 16) * sizeof(int) : 0;"
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T23:29:21.498664"
  },
  {
    "pr_number": 16837,
    "title": "clip : use FA",
    "body": "ref https://github.com/ggml-org/llama.cpp/pull/13231#issuecomment-3405623958\r\n\r\nSample implementation for using FA in the CLIP. Reduces memory usage and improves performance.\r\n\r\nTesting with Gemma 12B, using `llama-server` and 2 images:\r\n\r\n```bash\r\n# before\r\nalloc_compute_meta:      Metal compute buffer size =  1132.00 MiB\r\nalloc_compute_meta:        CPU compute buffer size =     9.19 MiB\r\n\r\nsrv  process_chun: image processed in 1653 ms\r\nsrv  process_chun: image processed in 1093 ms\r\n\r\n# after\r\nalloc_compute_meta:      Metal compute buffer size =   121.25 MiB\r\nalloc_compute_meta:        CPU compute buffer size =     9.19 MiB\r\n\r\nsrv  process_chun: image processed in 1386 ms\r\nsrv  process_chun: image processed in 810 ms\r\n```\r\n\r\nTODO:\r\n\r\n- [ ] Add FA sizes to other backends (f.ex. Gemma uses non-standard head size of 72)",
    "html_url": "https://github.com/ggml-org/llama.cpp/pull/16837",
    "created_at": "2025-10-29T09:33:55Z",
    "merged_at": "2025-11-02T20:21:48Z",
    "merge_commit_sha": "2f966b8ed87514e74bb96592217226cb6a6974dd",
    "base_ref": "master",
    "head_sha": "d441c31b194503640b03fdd239d0e81a562121b0",
    "user": "ggerganov",
    "files": [
      {
        "filename": "ggml/src/ggml-metal/ggml-metal-device.m",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -707,6 +707,7 @@ bool ggml_metal_device_supports_op(ggml_metal_device_t dev, const struct ggml_te\n             if (op->src[0]->ne[0] != 32 &&\n                 op->src[0]->ne[0] != 40 &&\n                 op->src[0]->ne[0] != 64 &&\n+                op->src[0]->ne[0] != 72 &&\n                 op->src[0]->ne[0] != 80 &&\n                 op->src[0]->ne[0] != 96 &&\n                 op->src[0]->ne[0] != 112 &&"
      },
      {
        "filename": "ggml/src/ggml-metal/ggml-metal.metal",
        "status": "modified",
        "additions": 8,
        "deletions": 0,
        "changes": 8,
        "patch": "@@ -5362,6 +5362,7 @@ typedef decltype(kernel_flash_attn_ext<FA_TYPES, half4x4, 1, dequantize_f16, hal\n template [[host_name(\"kernel_flash_attn_ext_f32_dk32_dv32\"  )]]  kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES_F32, float4x4,   1, dequantize_f32,  float4x4,   1, dequantize_f32,  32,  32>;\n template [[host_name(\"kernel_flash_attn_ext_f32_dk40_dv40\"  )]]  kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES_F32, float4x4,   1, dequantize_f32,  float4x4,   1, dequantize_f32,  40,  40>;\n template [[host_name(\"kernel_flash_attn_ext_f32_dk64_dv64\"  )]]  kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES_F32, float4x4,   1, dequantize_f32,  float4x4,   1, dequantize_f32,  64,  64>;\n+template [[host_name(\"kernel_flash_attn_ext_f32_dk72_dv72\"  )]]  kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES_F32, float4x4,   1, dequantize_f32,  float4x4,   1, dequantize_f32,  72,  72>;\n template [[host_name(\"kernel_flash_attn_ext_f32_dk80_dv80\"  )]]  kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES_F32, float4x4,   1, dequantize_f32,  float4x4,   1, dequantize_f32,  80,  80>;\n template [[host_name(\"kernel_flash_attn_ext_f32_dk96_dv96\"  )]]  kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES_F32, float4x4,   1, dequantize_f32,  float4x4,   1, dequantize_f32,  96,  96>;\n template [[host_name(\"kernel_flash_attn_ext_f32_dk112_dv112\")]]  kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES_F32, float4x4,   1, dequantize_f32,  float4x4,   1, dequantize_f32,  112, 112>;\n@@ -5374,6 +5375,7 @@ template [[host_name(\"kernel_flash_attn_ext_f32_dk576_dv512\")]]  kernel flash_at\n template [[host_name(\"kernel_flash_attn_ext_f16_dk32_dv32\"  )]]  kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    half4x4,    1, dequantize_f16,  half4x4,    1, dequantize_f16,  32,  32>;\n template [[host_name(\"kernel_flash_attn_ext_f16_dk40_dv40\"  )]]  kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    half4x4,    1, dequantize_f16,  half4x4,    1, dequantize_f16,  40,  40>;\n template [[host_name(\"kernel_flash_attn_ext_f16_dk64_dv64\"  )]]  kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    half4x4,    1, dequantize_f16,  half4x4,    1, dequantize_f16,  64,  64>;\n+template [[host_name(\"kernel_flash_attn_ext_f16_dk72_dv72\"  )]]  kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    half4x4,    1, dequantize_f16,  half4x4,    1, dequantize_f16,  72,  72>;\n template [[host_name(\"kernel_flash_attn_ext_f16_dk80_dv80\"  )]]  kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    half4x4,    1, dequantize_f16,  half4x4,    1, dequantize_f16,  80,  80>;\n template [[host_name(\"kernel_flash_attn_ext_f16_dk96_dv96\"  )]]  kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    half4x4,    1, dequantize_f16,  half4x4,    1, dequantize_f16,  96,  96>;\n template [[host_name(\"kernel_flash_attn_ext_f16_dk112_dv112\")]]  kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    half4x4,    1, dequantize_f16,  half4x4,    1, dequantize_f16,  112, 112>;\n@@ -5387,6 +5389,7 @@ template [[host_name(\"kernel_flash_attn_ext_f16_dk576_dv512\")]]  kernel flash_at\n template [[host_name(\"kernel_flash_attn_ext_bf16_dk32_dv32\"  )]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES_BF, bfloat4x4,  1, dequantize_bf16, bfloat4x4,  1, dequantize_bf16, 32,  32>;\n template [[host_name(\"kernel_flash_attn_ext_bf16_dk40_dv40\"  )]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES_BF, bfloat4x4,  1, dequantize_bf16, bfloat4x4,  1, dequantize_bf16, 40,  40>;\n template [[host_name(\"kernel_flash_attn_ext_bf16_dk64_dv64\"  )]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES_BF, bfloat4x4,  1, dequantize_bf16, bfloat4x4,  1, dequantize_bf16, 64,  64>;\n+template [[host_name(\"kernel_flash_attn_ext_bf16_dk72_dv72\"  )]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES_BF, bfloat4x4,  1, dequantize_bf16, bfloat4x4,  1, dequantize_bf16, 72,  72>;\n template [[host_name(\"kernel_flash_attn_ext_bf16_dk80_dv80\"  )]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES_BF, bfloat4x4,  1, dequantize_bf16, bfloat4x4,  1, dequantize_bf16, 80,  80>;\n template [[host_name(\"kernel_flash_attn_ext_bf16_dk96_dv96\"  )]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES_BF, bfloat4x4,  1, dequantize_bf16, bfloat4x4,  1, dequantize_bf16, 96,  96>;\n template [[host_name(\"kernel_flash_attn_ext_bf16_dk112_dv112\")]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES_BF, bfloat4x4,  1, dequantize_bf16, bfloat4x4,  1, dequantize_bf16, 112, 112>;\n@@ -5400,6 +5403,7 @@ template [[host_name(\"kernel_flash_attn_ext_bf16_dk576_dv512\")]] kernel flash_at\n template [[host_name(\"kernel_flash_attn_ext_q4_0_dk32_dv32\"  )]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    block_q4_0, 2, dequantize_q4_0, block_q4_0, 2, dequantize_q4_0, 32,  32>;\n template [[host_name(\"kernel_flash_attn_ext_q4_0_dk40_dv40\"  )]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    block_q4_0, 2, dequantize_q4_0, block_q4_0, 2, dequantize_q4_0, 40,  40>;\n template [[host_name(\"kernel_flash_attn_ext_q4_0_dk64_dv64\"  )]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    block_q4_0, 2, dequantize_q4_0, block_q4_0, 2, dequantize_q4_0, 64,  64>;\n+template [[host_name(\"kernel_flash_attn_ext_q4_0_dk72_dv72\"  )]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    block_q4_0, 2, dequantize_q4_0, block_q4_0, 2, dequantize_q4_0, 72,  72>;\n template [[host_name(\"kernel_flash_attn_ext_q4_0_dk80_dv80\"  )]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    block_q4_0, 2, dequantize_q4_0, block_q4_0, 2, dequantize_q4_0, 80,  80>;\n template [[host_name(\"kernel_flash_attn_ext_q4_0_dk96_dv96\"  )]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    block_q4_0, 2, dequantize_q4_0, block_q4_0, 2, dequantize_q4_0, 96,  96>;\n template [[host_name(\"kernel_flash_attn_ext_q4_0_dk112_dv112\")]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    block_q4_0, 2, dequantize_q4_0, block_q4_0, 2, dequantize_q4_0, 112, 112>;\n@@ -5412,6 +5416,7 @@ template [[host_name(\"kernel_flash_attn_ext_q4_0_dk576_dv512\")]] kernel flash_at\n template [[host_name(\"kernel_flash_attn_ext_q4_1_dk32_dv32\"  )]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    block_q4_1, 2, dequantize_q4_1, block_q4_1, 2, dequantize_q4_1, 32,  32>;\n template [[host_name(\"kernel_flash_attn_ext_q4_1_dk40_dv40\"  )]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    block_q4_1, 2, dequantize_q4_1, block_q4_1, 2, dequantize_q4_1, 40,  40>;\n template [[host_name(\"kernel_flash_attn_ext_q4_1_dk64_dv64\"  )]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    block_q4_1, 2, dequantize_q4_1, block_q4_1, 2, dequantize_q4_1, 64,  64>;\n+template [[host_name(\"kernel_flash_attn_ext_q4_1_dk72_dv72\"  )]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    block_q4_1, 2, dequantize_q4_1, block_q4_1, 2, dequantize_q4_1, 72,  72>;\n template [[host_name(\"kernel_flash_attn_ext_q4_1_dk80_dv80\"  )]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    block_q4_1, 2, dequantize_q4_1, block_q4_1, 2, dequantize_q4_1, 80,  80>;\n template [[host_name(\"kernel_flash_attn_ext_q4_1_dk96_dv96\"  )]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    block_q4_1, 2, dequantize_q4_1, block_q4_1, 2, dequantize_q4_1, 96,  96>;\n template [[host_name(\"kernel_flash_attn_ext_q4_1_dk112_dv112\")]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    block_q4_1, 2, dequantize_q4_1, block_q4_1, 2, dequantize_q4_1, 112, 112>;\n@@ -5424,6 +5429,7 @@ template [[host_name(\"kernel_flash_attn_ext_q4_1_dk576_dv512\")]] kernel flash_at\n template [[host_name(\"kernel_flash_attn_ext_q5_0_dk32_dv32\"  )]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    block_q5_0, 2, dequantize_q5_0, block_q5_0, 2, dequantize_q5_0, 32,  32>;\n template [[host_name(\"kernel_flash_attn_ext_q5_0_dk40_dv40\"  )]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    block_q5_0, 2, dequantize_q5_0, block_q5_0, 2, dequantize_q5_0, 40,  40>;\n template [[host_name(\"kernel_flash_attn_ext_q5_0_dk64_dv64\"  )]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    block_q5_0, 2, dequantize_q5_0, block_q5_0, 2, dequantize_q5_0, 64,  64>;\n+template [[host_name(\"kernel_flash_attn_ext_q5_0_dk72_dv72\"  )]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    block_q5_0, 2, dequantize_q5_0, block_q5_0, 2, dequantize_q5_0, 72,  72>;\n template [[host_name(\"kernel_flash_attn_ext_q5_0_dk80_dv80\"  )]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    block_q5_0, 2, dequantize_q5_0, block_q5_0, 2, dequantize_q5_0, 80,  80>;\n template [[host_name(\"kernel_flash_attn_ext_q5_0_dk96_dv96\"  )]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    block_q5_0, 2, dequantize_q5_0, block_q5_0, 2, dequantize_q5_0, 96,  96>;\n template [[host_name(\"kernel_flash_attn_ext_q5_0_dk112_dv112\")]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    block_q5_0, 2, dequantize_q5_0, block_q5_0, 2, dequantize_q5_0, 112, 112>;\n@@ -5436,6 +5442,7 @@ template [[host_name(\"kernel_flash_attn_ext_q5_0_dk576_dv512\")]] kernel flash_at\n template [[host_name(\"kernel_flash_attn_ext_q5_1_dk32_dv32\"  )]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    block_q5_1, 2, dequantize_q5_1, block_q5_1, 2, dequantize_q5_1, 32,  32>;\n template [[host_name(\"kernel_flash_attn_ext_q5_1_dk40_dv40\"  )]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    block_q5_1, 2, dequantize_q5_1, block_q5_1, 2, dequantize_q5_1, 40,  40>;\n template [[host_name(\"kernel_flash_attn_ext_q5_1_dk64_dv64\"  )]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    block_q5_1, 2, dequantize_q5_1, block_q5_1, 2, dequantize_q5_1, 64,  64>;\n+template [[host_name(\"kernel_flash_attn_ext_q5_1_dk72_dv72\"  )]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    block_q5_1, 2, dequantize_q5_1, block_q5_1, 2, dequantize_q5_1, 72,  72>;\n template [[host_name(\"kernel_flash_attn_ext_q5_1_dk80_dv80\"  )]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    block_q5_1, 2, dequantize_q5_1, block_q5_1, 2, dequantize_q5_1, 80,  80>;\n template [[host_name(\"kernel_flash_attn_ext_q5_1_dk96_dv96\"  )]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    block_q5_1, 2, dequantize_q5_1, block_q5_1, 2, dequantize_q5_1, 96,  96>;\n template [[host_name(\"kernel_flash_attn_ext_q5_1_dk112_dv112\")]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    block_q5_1, 2, dequantize_q5_1, block_q5_1, 2, dequantize_q5_1, 112, 112>;\n@@ -5448,6 +5455,7 @@ template [[host_name(\"kernel_flash_attn_ext_q5_1_dk576_dv512\")]] kernel flash_at\n template [[host_name(\"kernel_flash_attn_ext_q8_0_dk32_dv32\"  )]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    block_q8_0, 2, dequantize_q8_0, block_q8_0, 2, dequantize_q8_0, 32,  32>;\n template [[host_name(\"kernel_flash_attn_ext_q8_0_dk40_dv40\"  )]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    block_q8_0, 2, dequantize_q8_0, block_q8_0, 2, dequantize_q8_0, 40,  40>;\n template [[host_name(\"kernel_flash_attn_ext_q8_0_dk64_dv64\"  )]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    block_q8_0, 2, dequantize_q8_0, block_q8_0, 2, dequantize_q8_0, 64,  64>;\n+template [[host_name(\"kernel_flash_attn_ext_q8_0_dk72_dv72\"  )]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    block_q8_0, 2, dequantize_q8_0, block_q8_0, 2, dequantize_q8_0, 72,  72>;\n template [[host_name(\"kernel_flash_attn_ext_q8_0_dk80_dv80\"  )]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    block_q8_0, 2, dequantize_q8_0, block_q8_0, 2, dequantize_q8_0, 80,  80>;\n template [[host_name(\"kernel_flash_attn_ext_q8_0_dk96_dv96\"  )]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    block_q8_0, 2, dequantize_q8_0, block_q8_0, 2, dequantize_q8_0, 96,  96>;\n template [[host_name(\"kernel_flash_attn_ext_q8_0_dk112_dv112\")]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    block_q8_0, 2, dequantize_q8_0, block_q8_0, 2, dequantize_q8_0, 112, 112>;"
      },
      {
        "filename": "tests/test-backend-ops.cpp",
        "status": "modified",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "patch": "@@ -7225,8 +7225,8 @@ static std::vector<std::unique_ptr<test_case>> make_test_cases_eval() {\n         test_cases.emplace_back(new test_pad_ext(GGML_TYPE_F32, {11, 22, 33, 44}, 1, 2, 3, 4, 5, 6, 7, 8, v));\n     }\n \n-    for (int hsk : { 40, 64, 80, 96, 128, 192, 256, 576 }) {\n-        for (int hsv : { 40, 64, 80, 96, 128, 192, 256, 512 }) {\n+    for (int hsk : { 40, 64, 72, 80, 96, 128, 192, 256, 576 }) {\n+        for (int hsv : { 40, 64, 72, 80, 96, 128, 192, 256, 512 }) {\n             if (hsk != 192 && hsk != 576 && hsk != hsv) continue;\n             if (hsk == 192 && (hsv != 128 && hsv != 192)) continue;\n             if (hsk == 576 && hsv != 512) continue; // DeepSeek MLA"
      },
      {
        "filename": "tools/mtmd/clip.cpp",
        "status": "modified",
        "additions": 160,
        "deletions": 37,
        "changes": 197,
        "patch": "@@ -6,7 +6,6 @@\n #include \"clip-impl.h\"\n #include \"ggml.h\"\n #include \"ggml-cpp.h\"\n-#include \"ggml-cpu.h\"\n #include \"ggml-alloc.h\"\n #include \"ggml-backend.h\"\n #include \"gguf.h\"\n@@ -17,17 +16,15 @@\n #include <cstring>\n #include <fstream>\n #include <map>\n-#include <regex>\n #include <stdexcept>\n #include <unordered_set>\n #include <vector>\n-#include <sstream>\n #include <cinttypes>\n #include <limits>\n #include <array>\n-#include <numeric>\n #include <functional>\n \n+// TODO: allow to pass callback from user code\n struct clip_logger_state g_logger_state = {GGML_LOG_LEVEL_CONT, clip_log_callback_default, NULL};\n \n enum ffn_op_type {\n@@ -426,12 +423,14 @@ struct clip_ctx {\n \n     int max_nodes = 8192;\n     ggml_backend_sched_ptr sched;\n+    clip_flash_attn_type flash_attn_type = CLIP_FLASH_ATTN_TYPE_AUTO;\n \n     // for debugging\n     bool debug_graph = false;\n     std::vector<ggml_tensor *> debug_print_tensors;\n \n     clip_ctx(clip_context_params & ctx_params) {\n+        flash_attn_type = ctx_params.flash_attn_type;\n         debug_graph = std::getenv(\"MTMD_DEBUG_GRAPH\") != nullptr;\n         backend_cpu = ggml_backend_init_by_type(GGML_BACKEND_DEVICE_TYPE_CPU, nullptr);\n         if (!backend_cpu) {\n@@ -2260,17 +2259,25 @@ struct clip_graph {\n         ggml_tensor * k = ggml_permute(ctx0, k_cur, 0, 2, 1, 3);\n         //cb(k, \"k\", il);\n \n-        ggml_tensor * v = ggml_permute(ctx0, v_cur, 1, 2, 0, 3);\n-        v = ggml_cont(ctx0, v);\n-        //cb(k, \"v\", il);\n-\n         ggml_tensor * cur;\n \n-        // TODO @ngxson : support flash attention\n-        {\n+        if (ctx->flash_attn_type == CLIP_FLASH_ATTN_TYPE_ENABLED) {\n+            ggml_tensor * v = ggml_permute(ctx0, v_cur, 0, 2, 1, 3);\n+\n+            k = ggml_cast(ctx0, k, GGML_TYPE_F16);\n+            v = ggml_cast(ctx0, v, GGML_TYPE_F16);\n+\n+            cur = ggml_flash_attn_ext(ctx0, q, k, v, kq_mask, kq_scale, 0.0f, 0.0f);\n+            ggml_flash_attn_ext_set_prec(cur, GGML_PREC_F32);\n+\n+            cur = ggml_reshape_2d(ctx0, cur, cur->ne[0]*cur->ne[1], cur->ne[2]*cur->ne[3]);\n+\n+        } else {\n+            ggml_tensor * v = ggml_permute(ctx0, v_cur, 1, 2, 0, 3);\n+            v = ggml_cont(ctx0, v);\n+\n             const auto n_tokens = q->ne[1];\n             const auto n_head   = q->ne[2];\n-            // const auto n_kv     = k->ne[1]; // for flash attention\n \n             ggml_tensor * kq = ggml_mul_mat(ctx0, k, q);\n             // F32 may not needed for vision encoders?\n@@ -3192,7 +3199,87 @@ struct clip_model_loader {\n         }\n     }\n \n-    void alloc_compute_meta(clip_ctx & ctx_clip) {\n+    struct support_info_op {\n+        ggml_tensor * op;\n+\n+        // true if the op runs on the accelerated ctx_clip.backend\n+        bool is_accel = true;\n+    };\n+\n+    struct support_info_graph {\n+        // whether the clip_ctx.backend supports flash attention\n+        bool fattn = true;\n+        ggml_tensor * fattn_op = nullptr; // for debugging\n+\n+        std::vector<support_info_op> ops;\n+    };\n+\n+    static void warmup(clip_ctx & ctx_clip) {\n+        support_info_graph info;\n+\n+        if (ctx_clip.flash_attn_type == CLIP_FLASH_ATTN_TYPE_AUTO) {\n+            // try to enable flash attention to see if it's supported\n+            ctx_clip.flash_attn_type = CLIP_FLASH_ATTN_TYPE_ENABLED;\n+            info = alloc_compute_meta(ctx_clip);\n+            if (!info.fattn && info.fattn_op) {\n+                auto op = info.fattn_op;\n+                LOG_WRN(\"%s: *****************************************************************\\n\", __func__);\n+                LOG_WRN(\"%s: WARNING: flash attention not supported by %s, memory usage will increase\\n\", __func__, ggml_backend_name(ctx_clip.backend));\n+                LOG_WRN(\"%s: op params: \\n\", __func__);\n+                static auto print_shape = [](const char * fn, const char * name, ggml_tensor * t) {\n+                    LOG_WRN(\"%s:   %s: type = %s, ne = [%d %d %d %d], nb = [%d %d %d %d]\\n\", fn,\n+                            name, ggml_type_name(t->type),\n+                            t->ne[0], t->ne[1], t->ne[2], t->ne[3],\n+                            t->nb[0], t->nb[1], t->nb[2], t->nb[3]);\n+                };\n+                print_shape(__func__, \" dst\", op);\n+                print_shape(__func__, \"src0\", op->src[0]);\n+                print_shape(__func__, \"src1\", op->src[1]);\n+                print_shape(__func__, \"src2\", op->src[2]);\n+                LOG_WRN(\"%s: please report this on github as an issue\\n\", __func__);\n+                LOG_WRN(\"%s: *****************************************************************\\n\", __func__);\n+                ctx_clip.flash_attn_type = CLIP_FLASH_ATTN_TYPE_DISABLED;\n+                alloc_compute_meta(ctx_clip);\n+            }\n+        } else {\n+            info = alloc_compute_meta(ctx_clip);\n+            if (!info.fattn && ctx_clip.flash_attn_type == CLIP_FLASH_ATTN_TYPE_ENABLED) {\n+                LOG_WRN(\"%s: flash attention is not supported by the current backend; falling back to CPU (performance will be degraded)\\n\", __func__);\n+            }\n+        }\n+\n+        LOG_INF(\"%s: flash attention is %s\\n\", __func__,\n+            (ctx_clip.flash_attn_type == CLIP_FLASH_ATTN_TYPE_ENABLED) ? \"enabled\" : \"disabled\");\n+\n+        // print ops that are not supported by the GPU backend (if there is one)\n+        if (ctx_clip.backend && ctx_clip.backend != ctx_clip.backend_cpu) {\n+            std::vector<support_info_op> unsupported_ops;\n+            for (const auto & op : info.ops) {\n+                if (!op.is_accel) {\n+                    unsupported_ops.push_back(op);\n+                }\n+            }\n+            if (!unsupported_ops.empty()) {\n+                LOG_WRN(\"%s: *****************************************************************\\n\", __func__);\n+                LOG_WRN(\"%s: WARNING: the CLIP graph uses unsupported operators by the backend\\n\", __func__);\n+                LOG_WRN(\"%s:          the performance will be suboptimal                      \\n\", __func__);\n+                LOG_WRN(\"%s:          list of unsupported ops (backend=%s):\\n\", __func__, ggml_backend_name(ctx_clip.backend));\n+                for (const auto & op : unsupported_ops) {\n+                    LOG_WRN(\"%s: %16s: type = %s, ne = [%d %d %d %d]\\n\", __func__,\n+                            ggml_op_name(op.op->op),\n+                            ggml_type_name(op.op->type),\n+                            op.op->ne[0], op.op->ne[1], op.op->ne[2], op.op->ne[3]);\n+                }\n+                LOG_WRN(\"%s: flash attention is %s\\n\", __func__,\n+                    (ctx_clip.flash_attn_type == CLIP_FLASH_ATTN_TYPE_ENABLED) ? \"enabled\" : \"disabled\");\n+                LOG_WRN(\"%s: please report this on github as an issue\\n\", __func__);\n+                LOG_WRN(\"%s: ref: https://github.com/ggml-org/llama.cpp/pull/16837#issuecomment-3461676118\\n\", __func__);\n+                LOG_WRN(\"%s: *****************************************************************\\n\", __func__);\n+            }\n+        }\n+    }\n+\n+    static support_info_graph alloc_compute_meta(clip_ctx & ctx_clip) {\n         const auto & hparams = ctx_clip.model.hparams;\n         ctx_clip.buf_compute_meta.resize(ctx_clip.max_nodes * ggml_tensor_overhead() + ggml_graph_overhead());\n \n@@ -3223,57 +3310,95 @@ struct clip_model_loader {\n                         size / 1024.0 / 1024.0);\n             }\n         }\n+\n+        const int n_splits = ggml_backend_sched_get_n_splits(ctx_clip.sched.get());\n+        const int n_nodes  = ggml_graph_n_nodes(gf);\n+\n+        LOG_INF(\"%s: graph splits = %d, nodes = %d\\n\", __func__,  n_splits, n_nodes);\n+\n+        support_info_graph res {\n+            /*.fattn    = */ true,\n+            /*.fattn_op = */ nullptr,\n+            /*.ops      = */ {},\n+        };\n+\n+        // check op support\n+        for (int i = 0; i < ggml_graph_n_nodes(gf); i++) {\n+            ggml_tensor * node = ggml_graph_node(gf, i);\n+            res.ops.push_back({node, true});\n+            if (!ggml_backend_supports_op(ctx_clip.backend, node)) {\n+                res.ops.back().is_accel = false;\n+                if (node->op == GGML_OP_FLASH_ATTN_EXT) {\n+                    res.fattn    = false;\n+                    res.fattn_op = node;\n+                }\n+            }\n+        }\n+\n+        return res;\n     }\n \n-    void get_bool(const std::string & key, bool & output, bool required = true) {\n+    void get_bool(const std::string & key, bool & output, bool required = true) const {\n         const int i = gguf_find_key(ctx_gguf.get(), key.c_str());\n         if (i < 0) {\n-            if (required) throw std::runtime_error(\"Key not found: \" + key);\n+            if (required) {\n+                throw std::runtime_error(\"Key not found: \" + key);\n+            }\n             return;\n         }\n         output = gguf_get_val_bool(ctx_gguf.get(), i);\n     }\n \n-    void get_i32(const std::string & key, int & output, bool required = true) {\n+    void get_i32(const std::string & key, int & output, bool required = true) const {\n         const int i = gguf_find_key(ctx_gguf.get(), key.c_str());\n         if (i < 0) {\n-            if (required) throw std::runtime_error(\"Key not found: \" + key);\n+            if (required) {\n+                throw std::runtime_error(\"Key not found: \" + key);\n+            }\n             return;\n         }\n         output = gguf_get_val_i32(ctx_gguf.get(), i);\n     }\n \n-    void get_u32(const std::string & key, int & output, bool required = true) {\n+    void get_u32(const std::string & key, int & output, bool required = true) const {\n         const int i = gguf_find_key(ctx_gguf.get(), key.c_str());\n         if (i < 0) {\n-            if (required) throw std::runtime_error(\"Key not found: \" + key);\n+            if (required) {\n+                throw std::runtime_error(\"Key not found: \" + key);\n+            }\n             return;\n         }\n         output = gguf_get_val_u32(ctx_gguf.get(), i);\n     }\n \n-    void get_f32(const std::string & key, float & output, bool required = true) {\n+    void get_f32(const std::string & key, float & output, bool required = true) const {\n         const int i = gguf_find_key(ctx_gguf.get(), key.c_str());\n         if (i < 0) {\n-            if (required) throw std::runtime_error(\"Key not found: \" + key);\n+            if (required) {\n+                throw std::runtime_error(\"Key not found: \" + key);\n+            }\n             return;\n         }\n         output = gguf_get_val_f32(ctx_gguf.get(), i);\n     }\n \n-    void get_string(const std::string & key, std::string & output, bool required = true) {\n+    void get_string(const std::string & key, std::string & output, bool required = true) const {\n         const int i = gguf_find_key(ctx_gguf.get(), key.c_str());\n         if (i < 0) {\n-            if (required) throw std::runtime_error(\"Key not found: \" + key);\n+            if (required) {\n+                throw std::runtime_error(\"Key not found: \" + key);\n+            }\n             return;\n         }\n         output = std::string(gguf_get_val_str(ctx_gguf.get(), i));\n     }\n \n-    void get_arr_int(const std::string & key, std::vector<int> & output, bool required = true) {\n+    void get_arr_int(const std::string & key, std::vector<int> & output, bool required = true) const {\n         const int i = gguf_find_key(ctx_gguf.get(), key.c_str());\n         if (i < 0) {\n-            if (required) throw std::runtime_error(\"Key not found: \" + key);\n+            if (required) {\n+                throw std::runtime_error(\"Key not found: \" + key);\n+            }\n             return;\n         }\n         int n = gguf_get_arr_n(ctx_gguf.get(), i);\n@@ -3284,7 +3409,7 @@ struct clip_model_loader {\n         }\n     }\n \n-    void set_llava_uhd_res_candidates(clip_model & model, const int max_patches_per_side) {\n+    static void set_llava_uhd_res_candidates(clip_model & model, const int max_patches_per_side) {\n         auto & hparams = model.hparams;\n         for (int x = 1; x <= max_patches_per_side; x++) {\n             for (int y = 1; y <= max_patches_per_side; y++) {\n@@ -3312,24 +3437,22 @@ struct clip_init_result clip_init(const char * fname, struct clip_context_params\n             ctx_vision = new clip_ctx(ctx_params);\n             loader.load_hparams(ctx_vision->model, CLIP_MODALITY_VISION);\n             loader.load_tensors(*ctx_vision);\n-            loader.alloc_compute_meta(*ctx_vision);\n+            loader.warmup(*ctx_vision);\n         }\n \n         if (loader.has_audio) {\n             ctx_audio = new clip_ctx(ctx_params);\n             loader.load_hparams(ctx_audio->model, CLIP_MODALITY_AUDIO);\n             loader.load_tensors(*ctx_audio);\n-            loader.alloc_compute_meta(*ctx_audio);\n+            loader.warmup(*ctx_audio);\n         }\n \n     } catch (const std::exception & e) {\n         LOG_ERR(\"%s: failed to load model '%s': %s\\n\", __func__, fname, e.what());\n-        if (ctx_vision) {\n-            delete ctx_vision;\n-        }\n-        if (ctx_audio) {\n-            delete ctx_audio;\n-        }\n+\n+        delete ctx_vision;\n+        delete ctx_audio;\n+\n         return {nullptr, nullptr};\n     }\n \n@@ -3367,10 +3490,10 @@ void clip_image_size_free(struct clip_image_size * load_image_size) {\n     }\n     delete load_image_size;\n }\n-void clip_image_u8_free(struct clip_image_u8  * img) { if (img) delete img; }\n-void clip_image_f32_free(struct clip_image_f32 * img) { if (img) delete img; }\n-void clip_image_u8_batch_free(struct clip_image_u8_batch * batch) { if (batch) delete batch; }\n-void clip_image_f32_batch_free(struct clip_image_f32_batch * batch) { if (batch) delete batch; }\n+void clip_image_u8_free(struct clip_image_u8  * img) { delete img; }\n+void clip_image_f32_free(struct clip_image_f32 * img) { delete img; }\n+void clip_image_u8_batch_free(struct clip_image_u8_batch * batch) { delete batch; }\n+void clip_image_f32_batch_free(struct clip_image_f32_batch * batch) { delete batch; }\n \n size_t clip_image_f32_batch_n_images(const struct clip_image_f32_batch * batch) {\n     return batch->entries.size();"
      },
      {
        "filename": "tools/mtmd/clip.h",
        "status": "modified",
        "additions": 8,
        "deletions": 0,
        "changes": 8,
        "patch": "@@ -1,6 +1,7 @@\n #pragma once\n \n #include \"ggml.h\"\n+\n #include <stddef.h>\n #include <stdint.h>\n \n@@ -22,9 +23,16 @@ enum clip_modality {\n     CLIP_MODALITY_AUDIO,\n };\n \n+enum clip_flash_attn_type {\n+    CLIP_FLASH_ATTN_TYPE_AUTO     = -1,\n+    CLIP_FLASH_ATTN_TYPE_DISABLED = 0,\n+    CLIP_FLASH_ATTN_TYPE_ENABLED  = 1,\n+};\n+\n struct clip_context_params {\n     bool use_gpu;\n     enum ggml_log_level verbosity;\n+    enum clip_flash_attn_type flash_attn_type;\n };\n \n struct clip_init_result {"
      },
      {
        "filename": "tools/mtmd/mtmd-cli.cpp",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -136,6 +136,7 @@ struct mtmd_cli_context {\n         mparams.print_timings = true;\n         mparams.n_threads = params.cpuparams.n_threads;\n         mparams.verbosity = params.verbosity > 0 ? GGML_LOG_LEVEL_DEBUG : GGML_LOG_LEVEL_INFO;\n+        mparams.flash_attn_type = params.flash_attn_type;\n         ctx_vision.reset(mtmd_init_from_file(clip_path, model, mparams));\n         if (!ctx_vision.get()) {\n             LOG_ERR(\"Failed to load vision model from %s\\n\", clip_path);"
      },
      {
        "filename": "tools/mtmd/mtmd.cpp",
        "status": "modified",
        "additions": 12,
        "deletions": 4,
        "changes": 16,
        "patch": "@@ -19,7 +19,6 @@\n #include <cstdio>\n #include <cstdlib>\n #include <cstring>\n-#include <limits>\n #include <vector>\n \n // represents raw image data, layout is RGBRGBRGB...\n@@ -92,6 +91,15 @@ const char * mtmd_default_marker() {\n     return \"<__media__>\";\n }\n \n+static clip_flash_attn_type mtmd_get_clip_flash_attn_type(enum llama_flash_attn_type flash_attn_type) {\n+    switch (flash_attn_type) {\n+        case LLAMA_FLASH_ATTN_TYPE_AUTO:     return CLIP_FLASH_ATTN_TYPE_AUTO;\n+        case LLAMA_FLASH_ATTN_TYPE_DISABLED: return CLIP_FLASH_ATTN_TYPE_DISABLED;\n+        case LLAMA_FLASH_ATTN_TYPE_ENABLED:  return CLIP_FLASH_ATTN_TYPE_ENABLED;\n+    }\n+    return CLIP_FLASH_ATTN_TYPE_AUTO;\n+}\n+\n mtmd_context_params mtmd_context_params_default() {\n     mtmd_context_params params;\n     params.use_gpu = true;\n@@ -100,6 +108,7 @@ mtmd_context_params mtmd_context_params_default() {\n     params.verbosity = GGML_LOG_LEVEL_INFO;\n     params.image_marker = MTMD_DEFAULT_IMAGE_MARKER;\n     params.media_marker = mtmd_default_marker();\n+    params.flash_attn_type = LLAMA_FLASH_ATTN_TYPE_AUTO;\n     return params;\n }\n \n@@ -164,6 +173,7 @@ struct mtmd_context {\n         clip_context_params ctx_clip_params;\n         ctx_clip_params.use_gpu   = ctx_params.use_gpu;\n         ctx_clip_params.verbosity = ctx_params.verbosity;\n+        ctx_clip_params.flash_attn_type = mtmd_get_clip_flash_attn_type(ctx_params.flash_attn_type);\n         auto res = clip_init(mmproj_fname, ctx_clip_params);\n         ctx_v = res.ctx_v;\n         ctx_a = res.ctx_a;\n@@ -378,9 +388,7 @@ mtmd_context * mtmd_init_from_file(const char * mmproj_fname,\n }\n \n void mtmd_free(mtmd_context * ctx) {\n-    if (ctx) {\n-        delete ctx;\n-    }\n+    delete ctx;\n }\n \n struct mtmd_tokenizer {"
      },
      {
        "filename": "tools/mtmd/mtmd.h",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -82,6 +82,7 @@ struct mtmd_context_params {\n     enum ggml_log_level verbosity;\n     const char * image_marker; // deprecated, use media_marker instead\n     const char * media_marker;\n+    enum llama_flash_attn_type flash_attn_type;\n };\n \n MTMD_API const char * mtmd_default_marker(void);"
      },
      {
        "filename": "tools/server/server.cpp",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -2456,6 +2456,7 @@ struct server_context {\n             mparams.print_timings = false;\n             mparams.n_threads     = params_base.cpuparams.n_threads;\n             mparams.verbosity     = params_base.verbosity > 0 ? GGML_LOG_LEVEL_DEBUG : GGML_LOG_LEVEL_INFO;\n+            mparams.flash_attn_type = params_base.flash_attn_type;\n             mctx = mtmd_init_from_file(mmproj_path.c_str(), model, mparams);\n             if (mctx == nullptr) {\n                 SRV_ERR(\"failed to load multimodal model, '%s'\\n\", mmproj_path.c_str());"
      }
    ],
    "num_files": 9,
    "scraped_at": "2025-11-16T23:29:22.315856"
  },
  {
    "pr_number": 16831,
    "title": "Model: Minimax M2",
    "body": "Implementation for Minimax M2 - not doing the chat template yet because not sure how to handle the interleaving thinking blocks.",
    "html_url": "https://github.com/ggml-org/llama.cpp/pull/16831",
    "created_at": "2025-10-28T23:11:01Z",
    "merged_at": "2025-10-31T20:20:47Z",
    "merge_commit_sha": "0de0a01576772032008a689afc4d7c80685074c4",
    "base_ref": "master",
    "head_sha": "ac0d7081e19ff899c81f276e483fed698511f2fd",
    "user": "pwilkin",
    "files": [
      {
        "filename": "convert_hf_to_gguf.py",
        "status": "modified",
        "additions": 61,
        "deletions": 0,
        "changes": 61,
        "patch": "@@ -1054,6 +1054,9 @@ def get_vocab_base_pre(self, tokenizer) -> str:\n         if chkhsh == \"53e325976a6e142379c19b09afcae354f2f496f147afa8f9e189a33fe4e3024e\":\n             # ref: https://huggingface.co/ibm-granite/granite-docling-258M\n             res = \"granite-docling\"\n+        if chkhsh == \"f4f37b6c8eb9ea29b3eac6bb8c8487c5ab7885f8d8022e67edc1c68ce8403e95\":\n+            # ref: https://huggingface.co/MiniMaxAI/MiniMax-M2\n+            res = \"minimax-m2\"\n \n         if res is None:\n             logger.warning(\"\\n\")\n@@ -7126,6 +7129,64 @@ def prepare_tensors(self):\n                 raise ValueError(f\"Unprocessed experts: {experts}\")\n \n \n+@ModelBase.register(\"MiniMaxM2ForCausalLM\")\n+class MiniMaxM2Model(TextModel):\n+    model_arch = gguf.MODEL_ARCH.MINIMAXM2\n+    _experts_cache: dict[int, dict[str, Tensor]] = {}\n+\n+    def __init__(self, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        self.hparams[\"num_experts\"] = self.hparams[\"num_local_experts\"]\n+\n+    def set_gguf_parameters(self):\n+        super().set_gguf_parameters()\n+        if self.hparams[\"scoring_func\"] == \"sigmoid\":\n+            self.gguf_writer.add_expert_gating_func(gguf.ExpertGatingFuncType.SIGMOID)\n+        elif self.hparams[\"scoring_func\"] == \"softmax\":\n+            self.gguf_writer.add_expert_gating_func(gguf.ExpertGatingFuncType.SOFTMAX)\n+        else:\n+            raise ValueError(f\"Unsupported scoring_func value: {self.hparams['scoring_func']}\")\n+\n+        self.gguf_writer.add_expert_feed_forward_length(self.find_hparam([\"intermediate_size\"]))\n+        self.gguf_writer.add_rope_dimension_count(self.find_hparam([\"rotary_dim\"]))\n+\n+    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None):\n+        if name.endswith(\"e_score_correction_bias\"):\n+            name = name.replace(\"e_score_correction_bias\", \"e_score_correction.bias\")\n+\n+        # merge expert weights\n+        if 'experts' in name:\n+            n_experts = self.hparams[\"num_experts\"]\n+            assert bid is not None\n+\n+            expert_cache = self._experts_cache.setdefault(bid, {})\n+            expert_cache[name] = data_torch\n+            expert_weights = [\"w1\", \"w2\", \"w3\"]\n+\n+            # not enough expert weights to merge\n+            if len(expert_cache) < n_experts * len(expert_weights):\n+                return []\n+\n+            tensors: list[tuple[str, Tensor]] = []\n+            for w_name in expert_weights:\n+                datas: list[Tensor] = []\n+\n+                for xid in range(n_experts):\n+                    ename = f\"model.layers.{bid}.block_sparse_moe.experts.{xid}.{w_name}.weight\"\n+                    datas.append(expert_cache[ename])\n+                    del expert_cache[ename]\n+\n+                data_torch = torch.stack(datas, dim=0)\n+                merged_name = f\"model.layers.{bid}.block_sparse_moe.experts.{w_name}.weight\"\n+                new_name = self.map_tensor_name(merged_name)\n+                tensors.append((new_name, data_torch))\n+\n+            del self._experts_cache[bid]\n+            return tensors\n+\n+        return super().modify_tensors(data_torch, name, bid)\n+\n+\n @ModelBase.register(\"Dots1ForCausalLM\")\n class Dots1Model(Qwen2MoeModel):\n     model_arch = gguf.MODEL_ARCH.DOTS1"
      },
      {
        "filename": "convert_hf_to_gguf_update.py",
        "status": "modified",
        "additions": 2,
        "deletions": 1,
        "changes": 3,
        "patch": "@@ -141,6 +141,7 @@ class TOKENIZER_TYPE(IntEnum):\n     {\"name\": \"mellum\",           \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/JetBrains/Mellum-4b-base\", },\n     {\"name\": \"bailingmoe2\",      \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/inclusionAI/Ling-mini-base-2.0\", },\n     {\"name\": \"granite-docling\",  \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/ibm-granite/granite-docling-258M\", },\n+    {\"name\": \"minimax-m2\",       \"tokt\": TOKENIZER_TYPE.BPE, \"repo\": \"https://huggingface.co/MiniMaxAI/MiniMax-M2\", },\n ]\n \n # some models are known to be broken upstream, so we will skip them as exceptions\n@@ -435,7 +436,7 @@ def get_vocab_base_pre(self, tokenizer) -> str:\n             tokenizer = AutoTokenizer.from_pretrained(f\"models/tokenizers/{name}\", use_fast=False)\n         else:\n             tokenizer = AutoTokenizer.from_pretrained(f\"models/tokenizers/{name}\")\n-    except OSError as e:\n+    except (OSError, TypeError) as e:\n         logger.error(f\"Failed to load tokenizer for model {name}. Error: {e}\")\n         continue  # Skip this model and continue with the next one in the loop\n "
      },
      {
        "filename": "gguf-py/gguf/constants.py",
        "status": "modified",
        "additions": 20,
        "deletions": 0,
        "changes": 20,
        "patch": "@@ -425,6 +425,7 @@ class MODEL_ARCH(IntEnum):\n     GROVEMOE         = auto()\n     APERTUS          = auto()\n     COGVLM           = auto()\n+    MINIMAXM2        = auto()\n \n \n class VISION_PROJECTOR_TYPE(IntEnum):\n@@ -790,6 +791,7 @@ class MODEL_TENSOR(IntEnum):\n     MODEL_ARCH.SEED_OSS:         \"seed_oss\",\n     MODEL_ARCH.GROVEMOE:         \"grovemoe\",\n     MODEL_ARCH.APERTUS:          \"apertus\",\n+    MODEL_ARCH.MINIMAXM2:        \"minimax-m2\",\n     MODEL_ARCH.COGVLM:           \"cogvlm\",\n }\n \n@@ -2921,6 +2923,24 @@ class MODEL_TENSOR(IntEnum):\n         MODEL_TENSOR.FFN_DOWN_CHEXP,\n         MODEL_TENSOR.FFN_UP_CHEXP,\n     ],\n+    MODEL_ARCH.MINIMAXM2: [\n+        MODEL_TENSOR.TOKEN_EMBD,\n+        MODEL_TENSOR.OUTPUT_NORM,\n+        MODEL_TENSOR.OUTPUT,\n+        MODEL_TENSOR.ATTN_NORM,\n+        MODEL_TENSOR.ATTN_Q,\n+        MODEL_TENSOR.ATTN_Q_NORM,\n+        MODEL_TENSOR.ATTN_K,\n+        MODEL_TENSOR.ATTN_K_NORM,\n+        MODEL_TENSOR.ATTN_V,\n+        MODEL_TENSOR.ATTN_OUT,\n+        MODEL_TENSOR.FFN_NORM,\n+        MODEL_TENSOR.FFN_GATE_INP,\n+        MODEL_TENSOR.FFN_GATE_EXP,\n+        MODEL_TENSOR.FFN_DOWN_EXP,\n+        MODEL_TENSOR.FFN_UP_EXP,\n+        MODEL_TENSOR.FFN_EXP_PROBS_B,\n+    ],\n     MODEL_ARCH.COGVLM: [\n         MODEL_TENSOR.TOKEN_EMBD,\n         MODEL_TENSOR.OUTPUT_NORM,"
      },
      {
        "filename": "gguf-py/gguf/tensor_mapping.py",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -381,6 +381,7 @@ class TensorNameMap:\n             \"model.layers.{bid}.mlp.moe_statics.e_score_correction\",        # ernie4.5-moe\n             \"model.layers.{bid}.mlp.gate.expert_bias\",                      # bailingmoe2\n             \"model.layers.{bid}.feed_forward.expert_bias\",                  # lfm2moe\n+            \"model.layers.{bid}.block_sparse_moe.e_score_correction\",       # minimax-m2\n         ),\n \n         # Feed-forward up"
      },
      {
        "filename": "src/llama-arch.cpp",
        "status": "modified",
        "additions": 22,
        "deletions": 0,
        "changes": 22,
        "patch": "@@ -105,6 +105,7 @@ static const std::map<llm_arch, const char *> LLM_ARCH_NAMES = {\n     { LLM_ARCH_SEED_OSS,         \"seed_oss\"         },\n     { LLM_ARCH_GROVEMOE,         \"grovemoe\"         },\n     { LLM_ARCH_APERTUS,          \"apertus\"          },\n+    { LLM_ARCH_MINIMAX_M2,       \"minimax-m2\"       },\n     { LLM_ARCH_COGVLM,           \"cogvlm\"           },\n     { LLM_ARCH_UNKNOWN,          \"(unknown)\"        },\n };\n@@ -2355,6 +2356,27 @@ static const std::map<llm_arch, std::map<llm_tensor, const char *>> LLM_TENSOR_N\n             { LLM_TENSOR_FFN_UP_CHEXPS,      \"blk.%d.ffn_up_chexps\" },\n         },\n     },\n+    {\n+        LLM_ARCH_MINIMAX_M2,\n+        {\n+            { LLM_TENSOR_TOKEN_EMBD,         \"token_embd\" },\n+            { LLM_TENSOR_OUTPUT_NORM,        \"output_norm\" },\n+            { LLM_TENSOR_OUTPUT,             \"output\" },\n+            { LLM_TENSOR_ATTN_NORM,          \"blk.%d.attn_norm\" },\n+            { LLM_TENSOR_ATTN_Q,             \"blk.%d.attn_q\" },\n+            { LLM_TENSOR_ATTN_K,             \"blk.%d.attn_k\" },\n+            { LLM_TENSOR_ATTN_V,             \"blk.%d.attn_v\" },\n+            { LLM_TENSOR_ATTN_OUT,           \"blk.%d.attn_output\" },\n+            { LLM_TENSOR_ATTN_Q_NORM,        \"blk.%d.attn_q_norm\" },\n+            { LLM_TENSOR_ATTN_K_NORM,        \"blk.%d.attn_k_norm\" },\n+            { LLM_TENSOR_FFN_NORM,           \"blk.%d.ffn_norm\" },\n+            { LLM_TENSOR_FFN_GATE_INP,       \"blk.%d.ffn_gate_inp\" },\n+            { LLM_TENSOR_FFN_GATE_EXPS,      \"blk.%d.ffn_gate_exps\" },\n+            { LLM_TENSOR_FFN_DOWN_EXPS,      \"blk.%d.ffn_down_exps\" },\n+            { LLM_TENSOR_FFN_UP_EXPS,        \"blk.%d.ffn_up_exps\" },\n+            { LLM_TENSOR_FFN_EXP_PROBS_B,    \"blk.%d.exp_probs_b\" },\n+        },\n+    },\n     {\n         LLM_ARCH_COGVLM,\n         {"
      },
      {
        "filename": "src/llama-arch.h",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -109,6 +109,7 @@ enum llm_arch {\n     LLM_ARCH_SEED_OSS,\n     LLM_ARCH_GROVEMOE,\n     LLM_ARCH_APERTUS,\n+    LLM_ARCH_MINIMAX_M2,\n     LLM_ARCH_COGVLM,\n     LLM_ARCH_UNKNOWN,\n };"
      },
      {
        "filename": "src/llama-model.cpp",
        "status": "modified",
        "additions": 170,
        "deletions": 0,
        "changes": 170,
        "patch": "@@ -120,6 +120,7 @@ const char * llm_type_name(llm_type type) {\n         case LLM_TYPE_30B_A3B:       return \"30B.A3B\";\n         case LLM_TYPE_100B_A6B:      return \"100B.A6B\";\n         case LLM_TYPE_106B_A12B:     return \"106B.A12B\";\n+        case LLM_TYPE_230B_A10B:     return \"230B.A10B\";\n         case LLM_TYPE_235B_A22B:     return \"235B.A22B\";\n         case LLM_TYPE_300B_A47B:     return \"300B.A47B\";\n         case LLM_TYPE_355B_A32B:     return \"355B.A32B\";\n@@ -2154,6 +2155,17 @@ void llama_model::load_hparams(llama_model_loader & ml) {\n                     default: type = LLM_TYPE_UNKNOWN;\n                 }\n             } break;\n+        case LLM_ARCH_MINIMAX_M2:\n+            {\n+                ml.get_key(LLM_KV_ATTENTION_LAYERNORM_RMS_EPS,  hparams.f_norm_rms_eps);\n+                ml.get_key(LLM_KV_EXPERT_FEED_FORWARD_LENGTH,   hparams.n_ff_exp);\n+                ml.get_key(LLM_KV_EXPERT_GATING_FUNC,           hparams.expert_gating_func, false);\n+\n+                switch (hparams.n_layer) {\n+                    case 62: type = LLM_TYPE_230B_A10B; break;\n+                    default: type = LLM_TYPE_UNKNOWN;\n+                }\n+            } break;\n         case LLM_ARCH_COGVLM:\n             {\n                 ml.get_key(LLM_KV_ATTENTION_LAYERNORM_RMS_EPS, hparams.f_norm_rms_eps);\n@@ -6184,6 +6196,35 @@ bool llama_model::load_tensors(llama_model_loader & ml) {\n                         layer.attn_k_norm_b = create_tensor(tn(LLM_TENSOR_ATTN_K_NORM, \"bias\",   i), { n_embd_head_k }, TENSOR_NOT_REQUIRED);\n                     }\n                 } break;\n+            case LLM_ARCH_MINIMAX_M2:\n+                {\n+                    tok_embd = create_tensor(tn(LLM_TENSOR_TOKEN_EMBD, \"weight\"), {n_embd, n_vocab}, 0);\n+\n+                    // output\n+                    output_norm = create_tensor(tn(LLM_TENSOR_OUTPUT_NORM, \"weight\"), {n_embd}, 0);\n+                    output      = create_tensor(tn(LLM_TENSOR_OUTPUT,      \"weight\"), {n_embd, n_vocab}, 0);\n+\n+                    for (int i = 0; i < n_layer; ++i) {\n+                        auto & layer = layers[i];\n+\n+                        layer.wq = create_tensor(tn(LLM_TENSOR_ATTN_Q, \"weight\", i), { n_embd, n_embd_head_k * n_head }, 0);\n+                        layer.wk = create_tensor(tn(LLM_TENSOR_ATTN_K, \"weight\", i), { n_embd, n_embd_gqa }, 0);\n+                        layer.wv = create_tensor(tn(LLM_TENSOR_ATTN_V, \"weight\", i), { n_embd, n_embd_gqa }, 0);\n+                        layer.wo = create_tensor(tn(LLM_TENSOR_ATTN_OUT, \"weight\", i), { n_embd_head_k * n_head, n_embd }, 0);\n+\n+                        layer.attn_norm = create_tensor(tn(LLM_TENSOR_ATTN_NORM, \"weight\", i), {n_embd}, 0);\n+                        layer.attn_q_norm = create_tensor(tn(LLM_TENSOR_ATTN_Q_NORM, \"weight\", i), {n_embd_head_k * n_head}, 0);\n+                        layer.attn_k_norm = create_tensor(tn(LLM_TENSOR_ATTN_K_NORM, \"weight\", i), {n_embd_k_gqa}, 0);\n+\n+                        layer.ffn_norm = create_tensor(tn(LLM_TENSOR_FFN_NORM, \"weight\", i), {n_embd}, 0);\n+\n+                        layer.ffn_gate_inp = create_tensor(tn(LLM_TENSOR_FFN_GATE_INP, \"weight\", i), {n_embd, n_expert}, 0);\n+                        layer.ffn_gate_exps = create_tensor(tn(LLM_TENSOR_FFN_GATE_EXPS, \"weight\", i), {n_embd, n_ff,   n_expert}, 0);\n+                        layer.ffn_down_exps = create_tensor(tn(LLM_TENSOR_FFN_DOWN_EXPS, \"weight\", i), {n_ff,   n_embd, n_expert}, 0);\n+                        layer.ffn_up_exps   = create_tensor(tn(LLM_TENSOR_FFN_UP_EXPS,   \"weight\", i), {n_embd, n_ff,   n_expert}, 0);\n+                        layer.ffn_exp_probs_b = create_tensor(tn(LLM_TENSOR_FFN_EXP_PROBS_B, \"bias\", i), {n_expert}, 0);\n+                    }\n+                } break;\n             case LLM_ARCH_COGVLM:\n                 {\n                     tok_embd = create_tensor(tn(LLM_TENSOR_TOKEN_EMBD, \"weight\"), {n_embd, n_vocab}, 0);\n@@ -20023,6 +20064,130 @@ struct llm_build_apertus : public llm_graph_context {\n     }\n };\n \n+struct llm_build_minimax_m2 : public llm_graph_context {\n+    llm_build_minimax_m2(const llama_model & model, const llm_graph_params & params) : llm_graph_context(params) {\n+        const int64_t n_embd_head = hparams.n_embd_head_v;\n+\n+        GGML_ASSERT(n_embd_head == hparams.n_embd_head_k);\n+        // GGML_ASSERT(n_embd_head == hparams.n_rot); this is wrong in case of minimax, head_dim = 128, n_rot = 64\n+\n+        ggml_tensor * cur;\n+        ggml_tensor * inpL;\n+\n+        inpL = build_inp_embd(model.tok_embd);\n+\n+        ggml_tensor * inp_pos = build_inp_pos();\n+        auto inp_attn = build_attn_inp_kv();\n+        ggml_tensor * inp_out_ids = build_inp_out_ids();\n+\n+        for (int il = 0; il < n_layer; ++il) {\n+            ggml_tensor * inpSA = inpL;\n+\n+            cur = inpL;\n+\n+            // self_attention\n+            {\n+                cur = build_norm(inpL, model.layers[il].attn_norm, NULL, LLM_NORM_RMS, il);\n+                cb(cur, \"attn_norm\", il);\n+\n+                // compute Q and K and RoPE them\n+                ggml_tensor * Qcur = build_lora_mm(model.layers[il].wq, cur);\n+                cb(Qcur, \"Qcur\", il);\n+\n+                ggml_tensor * Kcur = build_lora_mm(model.layers[il].wk, cur);\n+                cb(Kcur, \"Kcur\", il);\n+\n+                ggml_tensor * Vcur = build_lora_mm(model.layers[il].wv, cur);\n+                cb(Vcur, \"Vcur\", il);\n+\n+                Qcur = build_norm(Qcur, model.layers[il].attn_q_norm, NULL,\n+                        LLM_NORM_RMS, il);\n+                cb(Qcur, \"Qcur_normed\", il);\n+\n+                Kcur = build_norm(Kcur, model.layers[il].attn_k_norm, NULL,\n+                        LLM_NORM_RMS, il);\n+                cb(Kcur, \"Kcur_normed\", il);\n+\n+                Qcur = ggml_reshape_3d(ctx0, Qcur, n_embd_head, n_head,    n_tokens);\n+                Kcur = ggml_reshape_3d(ctx0, Kcur, n_embd_head, n_head_kv, n_tokens);\n+                Vcur = ggml_reshape_3d(ctx0, Vcur, n_embd_head, n_head_kv, n_tokens);\n+\n+                Qcur = ggml_rope_ext(\n+                    ctx0, Qcur, inp_pos, nullptr,\n+                    n_rot, rope_type, n_ctx_orig, freq_base, freq_scale,\n+                    ext_factor, attn_factor, beta_fast, beta_slow\n+                    );\n+\n+                Kcur = ggml_rope_ext(\n+                    ctx0, Kcur, inp_pos, nullptr,\n+                    n_rot, rope_type, n_ctx_orig, freq_base, freq_scale,\n+                    ext_factor, attn_factor, beta_fast, beta_slow\n+                    );\n+\n+                cb(Qcur, \"Qcur\", il);\n+                cb(Kcur, \"Kcur\", il);\n+                cb(Vcur, \"Vcur\", il);\n+\n+                cur = build_attn(inp_attn,\n+                        model.layers[il].wo, NULL,\n+                        Qcur, Kcur, Vcur, nullptr, nullptr, nullptr, 1.0f/sqrtf(float(n_embd_head)), il);\n+            }\n+\n+            if (il == n_layer - 1 && inp_out_ids) {\n+                cur   = ggml_get_rows(ctx0,   cur, inp_out_ids);\n+                inpSA = ggml_get_rows(ctx0, inpSA, inp_out_ids);\n+            }\n+\n+            ggml_tensor * ffn_inp = ggml_add(ctx0, cur, inpSA);\n+            cb(ffn_inp, \"ffn_inp\", il);\n+\n+            // MoE branch\n+            cur = build_norm(ffn_inp,\n+                    model.layers[il].ffn_norm, NULL,\n+                    LLM_NORM_RMS, il);\n+            cb(cur, \"ffn_norm\", il);\n+\n+            cur = build_moe_ffn(cur,\n+                    model.layers[il].ffn_gate_inp,\n+                    model.layers[il].ffn_up_exps,\n+                    model.layers[il].ffn_gate_exps,\n+                    model.layers[il].ffn_down_exps,\n+                    model.layers[il].ffn_exp_probs_b,\n+                    n_expert, n_expert_used,\n+                    LLM_FFN_SILU, true,\n+                    false, 0.0,\n+                    (llama_expert_gating_func_type) hparams.expert_gating_func,\n+                    il);\n+            cb(cur, \"ffn_moe_out\", il);\n+\n+            cur = ggml_add(ctx0, cur, ffn_inp);\n+\n+            cur = build_cvec(cur, il);\n+            cb(cur, \"l_out\", il);\n+\n+            // input for next layer\n+            inpL = cur;\n+        }\n+\n+        cur = inpL;\n+\n+        cur = build_norm(cur,\n+                model.output_norm, NULL,\n+                LLM_NORM_RMS, -1);\n+\n+        cb(cur, \"result_norm\", -1);\n+        res->t_embd = cur;\n+\n+        // lm_head\n+        cur = build_lora_mm(model.output, cur);\n+\n+        cb(cur, \"result_output\", -1);\n+        res->t_logits = cur;\n+\n+        ggml_build_forward_expand(gf, cur);\n+    }\n+};\n+\n struct llm_build_cogvlm : public llm_graph_context {\n     llm_build_cogvlm(const llama_model & model, const llm_graph_params & params) : llm_graph_context(params) {\n         const int64_t n_embd_head = hparams.n_embd_head_v;\n@@ -20653,6 +20818,10 @@ ggml_cgraph * llama_model::build_graph(const llm_graph_params & params) const {\n             {\n                 llm = std::make_unique<llm_build_apertus>(*this, params);\n             } break;\n+        case LLM_ARCH_MINIMAX_M2:\n+            {\n+                llm = std::make_unique<llm_build_minimax_m2>(*this, params);\n+            } break;\n         case LLM_ARCH_COGVLM:\n             {\n                 llm = std::make_unique<llm_build_cogvlm>(*this, params);\n@@ -20874,6 +21043,7 @@ llama_rope_type llama_model_rope_type(const llama_model * model) {\n         case LLM_ARCH_SEED_OSS:\n         case LLM_ARCH_GROVEMOE:\n         case LLM_ARCH_APERTUS:\n+        case LLM_ARCH_MINIMAX_M2:\n         case LLM_ARCH_COGVLM:\n             return LLAMA_ROPE_TYPE_NEOX;\n "
      },
      {
        "filename": "src/llama-model.h",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -114,6 +114,7 @@ enum llm_type {\n     LLM_TYPE_30B_A3B,\n     LLM_TYPE_100B_A6B,\n     LLM_TYPE_106B_A12B, // GLM-4.5-Air\n+    LLM_TYPE_230B_A10B, // Minimax M2\n     LLM_TYPE_235B_A22B,\n     LLM_TYPE_300B_A47B, // Ernie MoE big\n     LLM_TYPE_355B_A32B, // GLM-4.5"
      },
      {
        "filename": "src/llama-vocab.cpp",
        "status": "modified",
        "additions": 5,
        "deletions": 0,
        "changes": 5,
        "patch": "@@ -401,6 +401,7 @@ struct llm_tokenizer_bpe : llm_tokenizer {\n                 };\n                 break;\n             case LLAMA_VOCAB_PRE_TYPE_GPT4O:\n+            case LLAMA_VOCAB_PRE_TYPE_MINIMAX_M2:\n                 regex_exprs = {\n                     // original regex from tokenizer.json\n                     // \"[^\\\\r\\\\n\\\\p{L}\\\\p{N}]?[\\\\p{Lu}\\\\p{Lt}\\\\p{Lm}\\\\p{Lo}\\\\p{M}]*[\\\\p{Ll}\\\\p{Lm}\\\\p{Lo}\\\\p{M}]+(?i:'s|'t|'re|'ve|'m|'ll|'d)?|[^\\\\r\\\\n\\\\p{L}\\\\p{N}]?[\\\\p{Lu}\\\\p{Lt}\\\\p{Lm}\\\\p{Lo}\\\\p{M}]+[\\\\p{Ll}\\\\p{Lm}\\\\p{Lo}\\\\p{M}]*(?i:'s|'t|'re|'ve|'m|'ll|'d)?|\\\\p{N}{1,3}| ?[^\\\\s\\\\p{L}\\\\p{N}]+[\\\\r\\\\n/]*|\\\\s*[\\\\r\\\\n]+|\\\\s+(?!\\\\S)|\\\\s+\",\n@@ -1992,6 +1993,10 @@ void llama_vocab::impl::load(llama_model_loader & ml, const LLM_KV & kv) {\n                 tokenizer_pre == \"grok-2\") {\n                 pre_type = LLAMA_VOCAB_PRE_TYPE_GROK_2;\n                 clean_spaces = false;\n+            } else if (\n+                tokenizer_pre == \"minimax-m2\") {\n+                pre_type = LLAMA_VOCAB_PRE_TYPE_MINIMAX_M2;\n+                clean_spaces = false;\n             } else {\n                 throw std::runtime_error(format(\"unknown pre-tokenizer type: '%s'\", tokenizer_pre.c_str()));\n             }"
      },
      {
        "filename": "src/llama-vocab.h",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -49,6 +49,7 @@ enum llama_vocab_pre_type {\n     LLAMA_VOCAB_PRE_TYPE_HUNYUAN_DENSE   = 38,\n     LLAMA_VOCAB_PRE_TYPE_GROK_2          = 39,\n     LLAMA_VOCAB_PRE_TYPE_GRANITE_DOCLING = 40,\n+    LLAMA_VOCAB_PRE_TYPE_MINIMAX_M2      = 41,\n };\n \n struct LLM_KV;"
      }
    ],
    "num_files": 10,
    "scraped_at": "2025-11-16T23:29:23.823343"
  },
  {
    "pr_number": 16825,
    "title": "llama: store mrope data in KV cell",
    "body": "Supersede https://github.com/ggml-org/llama.cpp/pull/16822\r\n\r\nFix https://github.com/ggml-org/llama.cpp/issues/13694 (hopefully this time for real)\r\n\r\nThe idea is to store the M-RoPE (x,y,t) positions inside KV cells. This will allow the causal mask to be constructed correctly based on (x,y,t) positions.\r\n\r\nThe benefit is that this introduce no breaking changes, compared to other proposals.\r\n\r\n---\r\n\r\nThis should now give the same output as https://github.com/ggml-org/llama.cpp/pull/16822 across multiple values of `-b`\r\n\r\n```sh\r\n./build/bin/llama-mtmd-cli \\\r\n    -m \"../models/Qwen2.5-VL-7B-Instruct-Q4_K_M.gguf\" \\\r\n    --mmproj \"../models/mmproj-Qwen2.5-VL-7B-Instruct-Q8_0.gguf\" \\\r\n    --image \"../models/0_bbox.png\" \\\r\n    -p \"Please first output bbox coordinates and colors of every rectangle in this image in JSON format, and then answer how many rectangles are there in the image.\" \\\r\n    --temp 0 -n 128\r\n```\r\n\r\n```\r\n[\r\n        {\"bbox_2d\": [168, 679, 462, 837], \"color\": \"red\"},\r\n        {\"bbox_2d\": [312, 575, 480, 765], \"color\": \"green\"},\r\n        {\"bbox_2d\": [601, 708, 672, 775], \"color\": \"black\"}\r\n]\r\n```\r\n\r\nImage draw using this script: https://gist.github.com/ngxson/039024fb2bdaf2e3c15db702f9fddaff\r\n\r\n<img width=\"311\" height=\"313\" alt=\"image\" src=\"https://github.com/user-attachments/assets/eebd25c5-dea3-489e-8b80-093f652c257b\" />\r\n\r\n---\r\n\r\nTODO:\r\n- fix KV save/load with mrope --> follow-up PR\r\n- ~~also fix the `mtmd_image_tokens_get_n_pos`~~\r\n",
    "html_url": "https://github.com/ggml-org/llama.cpp/pull/16825",
    "created_at": "2025-10-28T18:27:44Z",
    "merged_at": "2025-10-29T17:09:18Z",
    "merge_commit_sha": "e3af5563bd049141e036b50f843196db33d23e97",
    "base_ref": "master",
    "head_sha": "45d60e17aeb6145ffcf1f6b92102afed2b79e59e",
    "user": "ngxson",
    "files": [
      {
        "filename": "src/llama-batch.cpp",
        "status": "modified",
        "additions": 41,
        "deletions": 26,
        "changes": 67,
        "patch": "@@ -215,6 +215,7 @@ bool llama_batch_allocr::init(\n             /*.n_seq_tokens =*/ (uint32_t) 1,\n             /*.n_seqs       =*/ (uint32_t) batch.n_tokens,\n             /*.n_seqs_unq   =*/ (uint32_t) this->seq_id_unq.size(),\n+            /*.n_pos        =*/ n_pos_per_embd,\n             /*.token        =*/ batch.token,\n             /*.embd         =*/ batch.embd,\n             /*.pos          =*/ batch.pos,\n@@ -251,46 +252,58 @@ bool llama_batch_allocr::init(\n     // consistency checks\n     //\n \n-    for (uint32_t s = 0; s < n_seq_max; ++s) {\n-        if (seq_pos[s].empty()) {\n-            continue;\n+    if (n_pos_per_embd > 1) {\n+        // M-RoPE case: allow position to \"jump\" forward only (non-continuous positions are allowed)\n+        for (uint32_t s = 0; s < n_seq_max; ++s) {\n+            if (seq_pos[s].empty()) {\n+                continue;\n+            }\n+\n+            const llama_pos p0 = memory ? memory->seq_pos_max(s) : -1;\n+\n+            if (p0 >= 0 && p0 >= seq_pos_min(s)) {\n+                LLAMA_LOG_ERROR(\n+                        \"%s: the tokens of sequence %d in the input batch have inconsistent sequence positions:\\n\"\n+                        \" - the last position stored in the memory module of the context (i.e. the KV cache) for sequence %d is X = %d\\n\"\n+                        \" - the tokens for sequence %d in the input batch have a starting position of Y = %d\\n\"\n+                        \" for M-RoPE, it is required that the position satisfies: X < Y\\n\",\n+                        __func__, s, s, p0, s, seq_pos_min(s));\n+\n+                return false;\n+            }\n         }\n+    } else {\n+        for (uint32_t s = 0; s < n_seq_max; ++s) {\n+            if (seq_pos[s].empty()) {\n+                continue;\n+            }\n \n-        const llama_pos p0 = memory ? memory->seq_pos_max(s) : -1;\n+            const llama_pos p0 = memory ? memory->seq_pos_max(s) : -1;\n \n-        if (p0 >= 0) {\n-            bool ok = true;\n+            if (p0 >= 0) {\n+                bool ok = true;\n \n-            if (batch.token) {\n                 if (seq_pos_min(s) != p0 + 1) {\n                     ok = false;\n                 }\n-            } else {\n-                assert(batch.embd);\n \n-                // for embeddings (typically used as vision input), we allow them to have repeating positions\n-                // ref: https://github.com/ggml-org/llama.cpp/issues/13694#issuecomment-2983871762\n-                if (seq_pos_min(s) != p0 && seq_pos_min(s) != p0 + 1) {\n-                    ok = false;\n+                if (!ok) {\n+                    LLAMA_LOG_ERROR(\n+                            \"%s: the tokens of sequence %d in the input batch have inconsistent sequence positions:\\n\"\n+                            \" - the last position stored in the memory module of the context (i.e. the KV cache) for sequence %d is X = %d\\n\"\n+                            \" - the tokens for sequence %d in the input batch have a starting position of Y = %d\\n\"\n+                            \" it is required that the sequence positions remain consecutive: Y = X + 1\\n\",\n+                            __func__, s, s, p0, s, seq_pos_min(s));\n+\n+                    return false;\n                 }\n             }\n \n-            if (!ok) {\n-                LLAMA_LOG_ERROR(\n-                        \"%s: the tokens of sequence %d in the input batch have inconsistent sequence positions:\\n\"\n-                        \" - the last position stored in the memory module of the context (i.e. the KV cache) for sequence %d is X = %d\\n\"\n-                        \" - the tokens for sequence %d in the input batch have a starting position of Y = %d\\n\"\n-                        \" it is required that the sequence positions remain consecutive: Y = X + 1\\n\",\n-                        __func__, s, s, p0, s, seq_pos_min(s));\n-\n+            if (seq_pos_max(s) - seq_pos_min(s) + 1 > (int) seq_pos[s].size()) {\n+                LLAMA_LOG_ERROR(\"%s: sequence %d positions are not continuous\\n\", __func__, s);\n                 return false;\n             }\n         }\n-\n-        if (seq_pos_max(s) - seq_pos_min(s) + 1 > (int) seq_pos[s].size()) {\n-            LLAMA_LOG_ERROR(\"%s: sequence %d positions are not continuous\\n\", __func__, s);\n-            return false;\n-        }\n     }\n \n     if (memory) {\n@@ -389,6 +402,7 @@ llama_ubatch llama_batch_allocr::ubatch_reserve(uint32_t n_seq_tokens, uint32_t\n         /*.n_seq_tokens =*/ n_seq_tokens,\n         /*.n_seqs       =*/ n_seqs,\n         /*.n_seqs_unq   =*/ n_seqs,\n+        /*.n_pos        =*/ n_pos_per_embd,\n \n         /*.token        =*/ udata->token.data(),\n         /*.embd         =*/ nullptr,\n@@ -710,6 +724,7 @@ llama_ubatch llama_batch_allocr::ubatch_add(const std::vector<int32_t> & idxs, u\n         /*.n_seq_tokens =*/ n_tokens/n_seqs,\n         /*.n_seqs       =*/ n_seqs,\n         /*.n_seqs_unq   =*/ (uint32_t) udata->seq_id_unq.size(),\n+        /*.n_pos        =*/ n_pos_per_embd,\n \n         /*.token        =*/ batch.token ? udata->token.data() : nullptr,\n         /*.embd         =*/ batch.embd ? udata->embd.data() : nullptr,"
      },
      {
        "filename": "src/llama-batch.h",
        "status": "modified",
        "additions": 12,
        "deletions": 1,
        "changes": 13,
        "patch": "@@ -17,6 +17,16 @@ struct llama_ubatch {\n         return b_equal_seqs != 0;\n     }\n \n+    // typical for M-RoPE cases:\n+    //   0 - sequantial position of the tokens/embeddings in the sequence\n+    //   1 - y position in the image\n+    //   2 - x position in the image\n+    //   3 - other\n+    bool is_pos_2d() const {\n+        // TODO @ngxson : we may need to check for model arch when more models use >1 positions\n+        return n_pos >= 3;\n+    }\n+\n     uint32_t b_equal_seqs; // note: this is a boolean, but we use an int32_t for alignment\n                            //       otherwise address sanitizer complains\n     // TODO: whole_seqs for embeddings?\n@@ -25,6 +35,7 @@ struct llama_ubatch {\n     uint32_t n_seq_tokens; // tokens per sequence set\n     uint32_t n_seqs;       // sequence sets in the ubatch\n     uint32_t n_seqs_unq;   // unique sequence ids in the ubatch\n+    uint32_t n_pos;        // number of position inputs for each token/embedding\n \n     // seq_id_unq: unique sequence ids in the ubatch\n     // seq_idx:    indices of the unique sequence ids in the ubatch in [0, n_seqs_unq)\n@@ -33,7 +44,7 @@ struct llama_ubatch {\n     //                          // size               | idx | val\n     llama_token  *  token;      // [n_tokens]         | i   | id, token\n     float        *  embd;       // [n_embd, n_tokens] | i   | embd\n-    llama_pos    *  pos;        // [n_tokens]         | i   | pos\n+    llama_pos    *  pos;        // [n_tokens*n_pos]   | i   | pos\n     int32_t      *  n_seq_id;   // [n_tokens]         | i   | -\n     llama_seq_id ** seq_id;     // [n_tokens]         | s   | s0, s1, seq_id\n     llama_seq_id *  seq_id_unq; // [n_seqs_unq]       | s   | seq_id"
      },
      {
        "filename": "src/llama-kv-cache.cpp",
        "status": "modified",
        "additions": 32,
        "deletions": 0,
        "changes": 32,
        "patch": "@@ -338,6 +338,8 @@ void llama_kv_cache::seq_cp(llama_seq_id seq_id_src, llama_seq_id seq_id_dst, ll\n             llama_pos pos   = v_cells[s0].pos_get(i);\n             llama_pos shift = v_cells[s0].get_shift(i);\n \n+            llama_kv_cell_ext ext = v_cells[s0].ext_get(i);\n+\n             if (shift != 0) {\n                 pos -= shift;\n                 assert(pos >= 0);\n@@ -349,6 +351,8 @@ void llama_kv_cache::seq_cp(llama_seq_id seq_id_src, llama_seq_id seq_id_dst, ll\n             if (shift != 0) {\n                 v_cells[s1].pos_add(i, shift);\n             }\n+\n+            v_cells[s1].ext_set(i, ext);\n         }\n     }\n \n@@ -383,6 +387,7 @@ void llama_kv_cache::seq_keep(llama_seq_id seq_id) {\n \n void llama_kv_cache::seq_add(llama_seq_id seq_id, llama_pos p0, llama_pos p1, llama_pos shift) {\n     GGML_ASSERT(seq_id >= 0 && (size_t) seq_id < seq_to_stream.size());\n+    GGML_ASSERT(hparams.n_pos_per_embd() == 1 && \"seq_add() is only supported for n_pos_per_embd() == 1\");\n \n     auto & cells = v_cells[seq_to_stream[seq_id]];\n     auto & head  = v_heads[seq_to_stream[seq_id]];\n@@ -427,6 +432,7 @@ void llama_kv_cache::seq_add(llama_seq_id seq_id, llama_pos p0, llama_pos p1, ll\n \n void llama_kv_cache::seq_div(llama_seq_id seq_id, llama_pos p0, llama_pos p1, int d) {\n     GGML_ASSERT(seq_id >= 0 && (size_t) seq_id < seq_to_stream.size());\n+    GGML_ASSERT(hparams.n_pos_per_embd() == 1 && \"seq_div() is only supported for n_pos_per_embd() == 1\");\n \n     auto & cells = v_cells[seq_to_stream[seq_id]];\n \n@@ -900,6 +906,14 @@ void llama_kv_cache::apply_ubatch(const slot_info & sinfo, const llama_ubatch &\n \n             cells.pos_set(idx, ubatch.pos[i]);\n \n+            if (ubatch.is_pos_2d()) {\n+                llama_kv_cell_ext ext {\n+                    /*.x =*/ ubatch.pos[i + ubatch.n_tokens*2],\n+                    /*.y =*/ ubatch.pos[i + ubatch.n_tokens],\n+                };\n+                cells.ext_set(idx, ext);\n+            }\n+\n             for (int32_t s = 0; s < ubatch.n_seq_id[i]; s++) {\n                 cells.seq_add(idx, ubatch.seq_id[i][s]);\n             }\n@@ -1247,6 +1261,11 @@ void llama_kv_cache::set_input_kq_mask(ggml_tensor * dst, const llama_ubatch * u\n \n                 const llama_pos p1 = ubatch->pos[i];\n \n+                // for M-RoPE\n+                const bool is_2d = ubatch->is_pos_2d();\n+                const llama_pos p1_x = is_2d ? ubatch->pos[i + ubatch->n_tokens*2] : 0;\n+                const llama_pos p1_y = is_2d ? ubatch->pos[i + ubatch->n_tokens]   : 0;\n+\n                 const uint64_t idst = n_kv*(h*n_stream*n_tps_pad + s*n_tps_pad + ii);\n \n                 for (uint32_t j = 0; j < n_kv; ++j) {\n@@ -1266,6 +1285,14 @@ void llama_kv_cache::set_input_kq_mask(ggml_tensor * dst, const llama_ubatch * u\n                         continue;\n                     }\n \n+                    // M-RoPE causal mask\n+                    if (causal_attn && is_2d && p0 == p1) {\n+                        const auto & p0_ext = cells.ext_get(j);\n+                        if (p0_ext.is_2d_gt(p1_x, p1_y)) {\n+                            continue;\n+                        }\n+                    }\n+\n                     // apply SWA if any\n                     if (is_masked_swa(p0, p1)) {\n                         continue;\n@@ -1559,6 +1586,9 @@ void llama_kv_cache::state_write_meta(llama_io_write_i & io, const cell_ranges_t\n             io.write(&pos,      sizeof(pos));\n             io.write(&n_seq_id, sizeof(n_seq_id));\n \n+            // TODO: we also need to save llama_kv_cell_ext when apply_ubatch() support loading it\n+            //       see: https://github.com/ggml-org/llama.cpp/pull/16825#issuecomment-3460868350\n+\n             for (const auto & seq_id : seq_ids) {\n                 io.write(&seq_id, sizeof(seq_id));\n             }\n@@ -1704,6 +1734,8 @@ bool llama_kv_cache::state_read_meta(llama_io_read_i & io, uint32_t strm, uint32\n             return false;\n         }\n \n+        // TODO: we cannot yet restore llama_kv_cell_ext as the apply_ubatch() does not support it yet\n+        //       see: https://github.com/ggml-org/llama.cpp/pull/16825#issuecomment-3460868350\n         apply_ubatch(sinfo, ubatch);\n \n         const auto head_cur = sinfo.head();"
      },
      {
        "filename": "src/llama-kv-cells.h",
        "status": "modified",
        "additions": 44,
        "deletions": 2,
        "changes": 46,
        "patch": "@@ -5,9 +5,27 @@\n \n #include <bitset>\n #include <cassert>\n-#include <vector>\n-#include <set>\n+#include <cstring>\n #include <map>\n+#include <set>\n+#include <vector>\n+\n+struct llama_kv_cell_ext {\n+    // 2D spatial positions, typically used for M-RoPE\n+    llama_pos x = 0;\n+    llama_pos y = 0;\n+\n+    // return true if the current 2D spatial position is greater than other\n+    bool is_2d_gt(llama_pos ox, llama_pos oy) const {\n+        return (y > oy) || (y == oy && x > ox);\n+    }\n+\n+    void reset() {\n+        static_assert(std::is_trivially_copyable_v<llama_kv_cell_ext>);\n+\n+        memset(this, 0, sizeof(*this));\n+    }\n+};\n \n // meta information about KV cells that can be part of multiple sequences at the same time\n // TODO: add unit tests\n@@ -16,6 +34,7 @@ class llama_kv_cells {\n     void reset() {\n         for (uint32_t i = 0; i < pos.size(); ++i) {\n             pos[i]   = -1;\n+            ext[i].reset();\n             shift[i] =  0;\n             seq[i].reset();\n         }\n@@ -43,6 +62,7 @@ class llama_kv_cells {\n \n     void resize(uint32_t n) {\n         pos.resize(n);\n+        ext.resize(n);\n         shift.resize(n);\n         seq.resize(n);\n \n@@ -108,6 +128,7 @@ class llama_kv_cells {\n             const auto idx = i + j;\n \n             res.pos[j] = pos[idx];\n+            res.ext[j] = ext[idx];\n             res.seq[j] = seq[idx];\n \n             assert(shift[idx] == 0);\n@@ -126,6 +147,7 @@ class llama_kv_cells {\n             const auto idx = idxs[j];\n \n             res.pos[j] = pos[idx];\n+            res.ext[j] = ext[idx];\n             res.seq[j] = seq[idx];\n \n             assert(shift[idx] == 0);\n@@ -154,6 +176,7 @@ class llama_kv_cells {\n             }\n \n             pos[idx] = other.pos[j];\n+            ext[idx] = other.ext[j];\n             seq[idx] = other.seq[j];\n \n             if (pos[idx] != -1) {\n@@ -184,6 +207,7 @@ class llama_kv_cells {\n             }\n \n             pos[idx] = other.pos[j];\n+            ext[idx] = other.ext[j];\n             seq[idx] = other.seq[j];\n \n             if (pos[idx] != -1) {\n@@ -203,6 +227,7 @@ class llama_kv_cells {\n         seq[i].reset();\n \n         pos[i] = -1;\n+        ext[i].reset();\n         shift[i] = 0;\n \n         used.erase(i);\n@@ -221,6 +246,7 @@ class llama_kv_cells {\n \n         if (seq[i].none()) {\n             pos[i] = -1;\n+            ext[i].reset();\n             shift[i] = 0;\n \n             used.erase(i);\n@@ -250,6 +276,7 @@ class llama_kv_cells {\n             seq[i].reset();\n \n             pos[i] = -1;\n+            ext[i].reset();\n             shift[i] = 0;\n \n             used.erase(i);\n@@ -340,6 +367,13 @@ class llama_kv_cells {\n         return pos[i];\n     }\n \n+    const llama_kv_cell_ext & ext_get(uint32_t i) const {\n+        assert(i < pos.size());\n+        assert(pos[i] != -1);\n+\n+        return ext[i];\n+    }\n+\n     // note: call only if the cell is not empty\n     llama_pos get_shift(uint32_t i) const {\n         assert(i < pos.size());\n@@ -368,6 +402,11 @@ class llama_kv_cells {\n         used.insert(i);\n     }\n \n+    void ext_set(uint32_t i, llama_kv_cell_ext p) {\n+        assert(i < ext.size());\n+        ext[i] = p;\n+    }\n+\n     // pos[i] = pos[i] + d\n     // sets \"has_shift\" to true\n     // note: call only if the cell is not empty\n@@ -424,6 +463,9 @@ class llama_kv_cells {\n \n     std::vector<llama_pos> pos;\n \n+    // stores extra info per cell\n+    std::vector<llama_kv_cell_ext> ext;\n+\n     // this array accumulates any applied shifts to the pos array since the last reset_shift() call\n     // this is used to queue multiple updates to the pos array, which in the end can be applied in one go:\n     //"
      },
      {
        "filename": "tools/mtmd/mtmd.cpp",
        "status": "modified",
        "additions": 12,
        "deletions": 1,
        "changes": 13,
        "patch": "@@ -5,6 +5,15 @@\n \n #include \"llama.h\"\n \n+// fix problem with std::min and std::max\n+#if defined(_WIN32)\n+#define WIN32_LEAN_AND_MEAN\n+#ifndef NOMINMAX\n+#   define NOMINMAX\n+#endif\n+#include <windows.h>\n+#endif\n+\n #include <algorithm>\n #include <cerrno>\n #include <cstdio>\n@@ -1031,7 +1040,9 @@ const char * mtmd_image_tokens_get_id(const mtmd_image_tokens * image_tokens) {\n \n llama_pos mtmd_image_tokens_get_n_pos(const mtmd_image_tokens * image_tokens) {\n     if (image_tokens->use_mrope_pos) {\n-        return 1; // for M-RoPE, the whole image is 1 in temporal dimension\n+        // for M-RoPE, temporal dimension = max(t,h,w)\n+        // t is omitted as we don't support video input\n+        return std::max(image_tokens->nx, image_tokens->ny);\n     }\n     return image_tokens->n_tokens();\n }"
      },
      {
        "filename": "tools/mtmd/mtmd.h",
        "status": "modified",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "patch": "@@ -153,7 +153,7 @@ MTMD_API const mtmd_image_tokens *  mtmd_input_chunk_get_tokens_image(const mtmd\n MTMD_API size_t                     mtmd_input_chunk_get_n_tokens    (const mtmd_input_chunk * chunk);\n // returns nullptr for ID on text chunk\n MTMD_API const char *               mtmd_input_chunk_get_id          (const mtmd_input_chunk * chunk);\n-// number of temporal positions (always 1 for M-RoPE, n_tokens otherwise)\n+// number of temporal positions (equals to max(t,h,w) for M-RoPE; equals to n_tokens otherwise)\n MTMD_API llama_pos                  mtmd_input_chunk_get_n_pos       (const mtmd_input_chunk * chunk);\n \n // in case you want to use custom logic to handle the chunk (i.e. KV cache management)\n@@ -171,7 +171,7 @@ MTMD_API size_t       mtmd_image_tokens_get_n_tokens(const mtmd_image_tokens * i\n MTMD_API size_t       mtmd_image_tokens_get_nx      (const mtmd_image_tokens * image_tokens);\n MTMD_API size_t       mtmd_image_tokens_get_ny      (const mtmd_image_tokens * image_tokens);\n MTMD_API const char * mtmd_image_tokens_get_id      (const mtmd_image_tokens * image_tokens); // TODO: deprecate\n-// number of temporal positions (always 1 for M-RoPE, n_tokens otherwise)\n+// number of temporal positions (equals to max(t,h,w) for M-RoPE; equals to n_tokens otherwise)\n MTMD_API llama_pos    mtmd_image_tokens_get_n_pos   (const mtmd_image_tokens * image_tokens); // TODO: deprecate\n \n // tokenize an input text prompt and a list of bitmaps (images/audio)"
      }
    ],
    "num_files": 6,
    "scraped_at": "2025-11-16T23:29:25.230594"
  },
  {
    "pr_number": 16820,
    "title": "Hexagon Op queue & dispatch optimizations",
    "body": "Optimize dspqueue (used for sending Op requests to the NPU) by doing all response processing in-place.\r\nThis removes the need for the dedicated read threads (started internally by the dspqueue library) and essentially \r\neliminates all polling in that path.\r\n\r\n(For the curious, the dspqueue CPU side sources can be found here \r\nhttps://github.com/qualcomm/fastrpc/tree/main/src/dspqueue)\r\n\r\nWe can also bump the CPU backend thread counts now for the default use-cases since we still rely on the \r\nCPU for Flash Attention and a few other Ops. ",
    "html_url": "https://github.com/ggml-org/llama.cpp/pull/16820",
    "created_at": "2025-10-28T15:47:33Z",
    "merged_at": "2025-10-29T13:29:12Z",
    "merge_commit_sha": "3eb2be1ca5f37480aeb16102970d9e65f43347fe",
    "base_ref": "master",
    "head_sha": "d884764eb238a32f1e21e17733e38b378c3dea7f",
    "user": "max-krasnyansky",
    "files": [
      {
        "filename": "CODEOWNERS",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -65,7 +65,7 @@\n /ggml/src/ggml-impl.h                   @ggerganov @slaren\n /ggml/src/ggml-metal/                   @ggerganov\n /ggml/src/ggml-opencl/                  @lhez @max-krasnyansky\n-/ggml/src/ggml-hexagon/                 @max-krasnyansky\n+/ggml/src/ggml-hexagon/                 @max-krasnyansky @lhez\n /ggml/src/ggml-opt.cpp                  @JohannesGaessler\n /ggml/src/ggml-quants.*                 @ggerganov\n /ggml/src/ggml-rpc/                     @rgerganov"
      },
      {
        "filename": "ggml/src/ggml-hexagon/ggml-hexagon.cpp",
        "status": "modified",
        "additions": 79,
        "deletions": 191,
        "changes": 270,
        "patch": "@@ -217,6 +217,9 @@ struct ggml_hexagon_session {\n     void allocate(int dev_id) noexcept(false);\n     void release() noexcept(true);\n \n+    void enqueue(struct htp_general_req &req, struct dspqueue_buffer *bufs, uint32_t n_bufs, bool sync = false);\n+    void flush();\n+\n     ggml_backend_buffer_type buffer_type;\n     ggml_backend_buffer_type repack_buffer_type;\n \n@@ -237,38 +240,61 @@ struct ggml_hexagon_session {\n     uint32_t         prof_pkts;\n };\n \n-// Packet callback\n-static void htp_packet_callback(dspqueue_t queue, AEEResult error, void * context) {\n-    auto sess = static_cast<ggml_hexagon_session *>(context);\n+void ggml_hexagon_session::enqueue(struct htp_general_req &req, struct dspqueue_buffer *bufs, uint32_t n_bufs, bool sync) {\n+    // Bump pending flag (cleared in the session::flush once we get the responce)\n+    this->op_pending++;  // atomic inc\n+\n+    int err = dspqueue_write(this->queue,\n+                             0,                       // flags - the framework will autoset this\n+                             n_bufs,                  // number of buffers\n+                             bufs,                    // buffer references\n+                             sizeof(req),\n+                             (const uint8_t *) &req,  // Message\n+                             1000000                  // Timeout\n+    );\n+\n+    if (err != 0) {\n+        GGML_ABORT(\"ggml-hex: %s dspqueue_write failed: 0x%08x\\n\", this->name.c_str(), (unsigned) err);\n+    }\n+\n+    if (sync) {\n+        flush();\n+    }\n+}\n+\n+// Flush HTP response queue i.e wait for all outstanding requests to complete\n+void ggml_hexagon_session::flush() {\n+    dspqueue_t q = this->queue;\n \n     // Repeatedly read packets from the queue until it's empty. We don't\n     // necessarily get a separate callback for each packet, and new packets\n     // may arrive while we're processing the previous one.\n \n-    while (1) {\n+    while (this->op_pending) {\n         struct htp_general_rsp rsp;\n         uint32_t               rsp_size;\n         uint32_t               flags;\n \n         struct dspqueue_buffer bufs[HTP_MAX_PACKET_BUFFERS];\n         uint32_t               n_bufs;\n \n-        // Read packet from queue\n-        int err = dspqueue_read_noblock(queue, &flags,\n-                                        HTP_MAX_PACKET_BUFFERS,  // Maximum number of buffer references\n-                                        &n_bufs,                 // Number of buffer references\n-                                        bufs,                    // Buffer references\n-                                        sizeof(rsp),             // Max message length\n-                                        &rsp_size,               // Message length\n-                                        (uint8_t *) &rsp);\n-\n-        if (err == AEE_EWOULDBLOCK) {\n-            // Consumed all packets available for now\n-            return;\n+        // Read response packet from queue\n+        int err = dspqueue_read(q, &flags,\n+                                   HTP_MAX_PACKET_BUFFERS,  // Maximum number of buffer references\n+                                   &n_bufs,                 // Number of buffer references\n+                                   bufs,                    // Buffer references\n+                                   sizeof(rsp),             // Max message length\n+                                   &rsp_size,               // Message length\n+                                   (uint8_t *) &rsp,\n+                                   1000000);                // Timeout\n+\n+        if (err == AEE_EEXPIRED) {\n+            // TODO: might need to bail out if the HTP is stuck on something\n+            continue;\n         }\n \n         if (err != 0) {\n-            GGML_ABORT(\"ggml-hex: dspqueue_read_noblock failed: 0x%08x\\n\", (unsigned) err);\n+            GGML_ABORT(\"ggml-hex: dspqueue_read failed: 0x%08x\\n\", (unsigned) err);\n         }\n \n         // Basic sanity checks\n@@ -281,21 +307,15 @@ static void htp_packet_callback(dspqueue_t queue, AEEResult error, void * contex\n             // TODO: handle errors\n         }\n \n-        // FIXME: update profiling implementation\n-        sess->prof_usecs  = rsp.prof_usecs;\n-        sess->prof_cycles = rsp.prof_cycles;\n-        sess->prof_pkts   = rsp.prof_pkts;\n+        // TODO: update profiling implementation, currently only works for opt_opsync mode\n+        this->prof_usecs  = rsp.prof_usecs;\n+        this->prof_cycles = rsp.prof_cycles;\n+        this->prof_pkts   = rsp.prof_pkts;\n \n-        sess->op_pending--;  // atomic dec\n+        this->op_pending--;  // atomic dec\n     }\n }\n \n-// Error callback - simply terminates with an error. Used where we don't\n-// expect errors.\n-[[noreturn]] static void htp_error_callback(dspqueue_t queue, AEEResult error, void * context) {\n-    GGML_ABORT(\"ggml-hex: dspcall general error 0x%x: for queue %p\\n\", error, (void *) queue);\n-}\n-\n // ** backend buffers\n \n struct ggml_backend_hexagon_buffer_type_context {\n@@ -1564,7 +1584,8 @@ void ggml_hexagon_session::allocate(int dev_id) noexcept(false) {\n                           0,              // Flags\n                           128 * 1024,     // Request  queue size (in bytes)\n                           64 * 1024,      // Response queue size (in bytes)\n-                          htp_packet_callback, htp_error_callback,\n+                          nullptr,        // Read packet callback (we handle reads explicitly)\n+                          nullptr,        // Error callback (we handle errors during reads)\n                           (void *) this,  // Callback context\n                           &queue);\n     if (err != 0) {\n@@ -2205,7 +2226,7 @@ static void ggml_hexagon_mul_mat(const struct ggml_tensor * op, uint32_t flags)\n     bufs[0].ptr    = src0->data;\n     bufs[0].offset = (uint8_t *) src0->data - src0_buf->base;\n     bufs[0].size   = ggml_nbytes(src0);\n-    bufs[0].flags  = DSPQUEUE_BUFFER_FLAG_REF;\n+    bufs[0].flags  = 0;\n \n     // Second buffer Input Activations. This is a buffer that the CPU\n     // writes and the DSP reads, so we'll need to flush CPU caches and\n@@ -2215,8 +2236,7 @@ static void ggml_hexagon_mul_mat(const struct ggml_tensor * op, uint32_t flags)\n     bufs[1].ptr    = src1->data;\n     bufs[1].offset = (uint8_t *) src1->data - src1_buf->base;\n     bufs[1].size   = ggml_nbytes(src1);\n-    bufs[1].flags  = (DSPQUEUE_BUFFER_FLAG_REF |                   // Take a reference\n-                     DSPQUEUE_BUFFER_FLAG_FLUSH_SENDER |          // Flush CPU\n+    bufs[1].flags  = (DSPQUEUE_BUFFER_FLAG_FLUSH_SENDER |         // Flush CPU\n                      DSPQUEUE_BUFFER_FLAG_INVALIDATE_RECIPIENT);  // Invalidate DSP\n \n     // Third buffer Output Activations. We'll handle DSP\n@@ -2227,7 +2247,7 @@ static void ggml_hexagon_mul_mat(const struct ggml_tensor * op, uint32_t flags)\n     bufs[2].ptr    = dst->data;\n     bufs[2].offset = (uint8_t *) dst->data - dst_buf->base;\n     bufs[2].size   = ggml_nbytes(dst);\n-    bufs[2].flags  = (DSPQUEUE_BUFFER_FLAG_REF | DSPQUEUE_BUFFER_FLAG_FLUSH_SENDER);\n+    bufs[2].flags  = (DSPQUEUE_BUFFER_FLAG_FLUSH_SENDER);\n \n     // Primary DSP session from the src0 (normally weight) tensor\n     auto sess = src0_buf->sess;\n@@ -2255,27 +2275,7 @@ static void ggml_hexagon_mul_mat(const struct ggml_tensor * op, uint32_t flags)\n     }\n \n     if ((opt_opmask & HTP_OPMASK_QUEUE)) {\n-        // Bump pending flag (cleared in the callback once we get the responce)\n-        sess->op_pending++;  // atomic inc\n-\n-        int err = dspqueue_write(sess->queue,\n-                                 0,                       // flags - the framework will autoset this\n-                                 3,                       // number of buffers\n-                                 bufs,                    // buffer references\n-                                 sizeof(req),\n-                                 (const uint8_t *) &req,  // Message\n-                                 1000000                  // Timeout\n-        );\n-\n-        if (err != 0) {\n-            GGML_ABORT(\"ggml-hex: %s dspqueue_write failed: 0x%08x\\n\", sess->name.c_str(), (unsigned) err);\n-        }\n-    }\n-\n-    if (opt_opsync) {\n-        while (sess->op_pending) {\n-            ;\n-        }\n+        sess->enqueue(req, bufs, 3, opt_opsync);\n     }\n \n     t2 = ggml_time_us();\n@@ -2331,7 +2331,7 @@ static void ggml_hexagon_mul_mat_id(const struct ggml_tensor * op, uint32_t flag\n     bufs[0].ptr    = src0->data;\n     bufs[0].offset = (uint8_t *) src0->data - src0_buf->base;\n     bufs[0].size   = ggml_nbytes(src0);\n-    bufs[0].flags  = DSPQUEUE_BUFFER_FLAG_REF;\n+    bufs[0].flags  = 0;\n \n     // Second buffer Input Activations. This is a buffer that the CPU\n     // writes and the DSP reads, so we'll need to flush CPU caches and\n@@ -2341,8 +2341,7 @@ static void ggml_hexagon_mul_mat_id(const struct ggml_tensor * op, uint32_t flag\n     bufs[1].ptr    = src1->data;\n     bufs[1].offset = (uint8_t *) src1->data - src1_buf->base;\n     bufs[1].size   = ggml_nbytes(src1);\n-    bufs[1].flags  = (DSPQUEUE_BUFFER_FLAG_REF |                   // Take a reference\n-                     DSPQUEUE_BUFFER_FLAG_FLUSH_SENDER |          // Flush CPU\n+    bufs[1].flags  = (DSPQUEUE_BUFFER_FLAG_FLUSH_SENDER |         // Flush CPU\n                      DSPQUEUE_BUFFER_FLAG_INVALIDATE_RECIPIENT);  // Invalidate DSP\n \n     // Third buffer expert IDs. This is a buffer that the CPU\n@@ -2353,8 +2352,7 @@ static void ggml_hexagon_mul_mat_id(const struct ggml_tensor * op, uint32_t flag\n     bufs[2].ptr    = src2->data;\n     bufs[2].offset = (uint8_t *) src2->data - src2_buf->base;\n     bufs[2].size   = ggml_nbytes(src2);\n-    bufs[2].flags  = (DSPQUEUE_BUFFER_FLAG_REF |                   // Take a reference\n-                     DSPQUEUE_BUFFER_FLAG_FLUSH_SENDER |          // Flush CPU\n+    bufs[2].flags  = (DSPQUEUE_BUFFER_FLAG_FLUSH_SENDER |         // Flush CPU\n                      DSPQUEUE_BUFFER_FLAG_INVALIDATE_RECIPIENT);  // Invalidate DSP\n \n     // Forth buffer Output Activations. We'll handle DSP\n@@ -2365,7 +2363,7 @@ static void ggml_hexagon_mul_mat_id(const struct ggml_tensor * op, uint32_t flag\n     bufs[3].ptr    = dst->data;\n     bufs[3].offset = (uint8_t *) dst->data - dst_buf->base;\n     bufs[3].size   = ggml_nbytes(dst);\n-    bufs[3].flags  = (DSPQUEUE_BUFFER_FLAG_REF | DSPQUEUE_BUFFER_FLAG_FLUSH_SENDER);\n+    bufs[3].flags  = (DSPQUEUE_BUFFER_FLAG_FLUSH_SENDER);\n \n     // Primary DSP session from the src0 (normally weight) tensor\n     auto sess = src0_buf->sess;\n@@ -2394,27 +2392,7 @@ static void ggml_hexagon_mul_mat_id(const struct ggml_tensor * op, uint32_t flag\n     }\n \n     if ((opt_opmask & HTP_OPMASK_QUEUE)) {\n-        // Bump pending flag (cleared in the callback once we get the responce)\n-        sess->op_pending++;  // atomic inc\n-\n-        int err = dspqueue_write(sess->queue,\n-                                 0,                       // flags - the framework will autoset this\n-                                 4,                       // number of buffers\n-                                 bufs,                    // buffer references\n-                                 sizeof(req),\n-                                 (const uint8_t *) &req,  // Message\n-                                 1000000                  // Timeout\n-        );\n-\n-        if (err != 0) {\n-            GGML_ABORT(\"ggml-hex: %s dspqueue_write failed: 0x%08x\\n\", sess->name.c_str(), (unsigned) err);\n-        }\n-    }\n-\n-    if (opt_opsync) {\n-        while (sess->op_pending) {\n-            ;\n-        }\n+        sess->enqueue(req, bufs, 4, opt_opsync);\n     }\n \n     t2 = ggml_time_us();\n@@ -2487,8 +2465,7 @@ static void ggml_hexagon_binary(const struct ggml_tensor * op, uint32_t flags) {\n     bufs[0].ptr    = src0->data;\n     bufs[0].offset = (uint8_t *) src0->data - src0_buf->base;\n     bufs[0].size   = ggml_nbytes(src0);\n-    bufs[0].flags  = (DSPQUEUE_BUFFER_FLAG_REF |                   // Take a reference\n-                     DSPQUEUE_BUFFER_FLAG_FLUSH_SENDER |          // Flush CPU\n+    bufs[0].flags  = (DSPQUEUE_BUFFER_FLAG_FLUSH_SENDER |         // Flush CPU\n                      DSPQUEUE_BUFFER_FLAG_INVALIDATE_RECIPIENT);  // Invalidate DSP;\n \n     // Second buffer = Second Operand of Binary op\n@@ -2500,8 +2477,7 @@ static void ggml_hexagon_binary(const struct ggml_tensor * op, uint32_t flags) {\n     bufs[1].ptr    = src1->data;\n     bufs[1].offset = (uint8_t *) src1->data - src1_buf->base;\n     bufs[1].size   = ggml_nbytes(src1);\n-    bufs[1].flags  = (DSPQUEUE_BUFFER_FLAG_REF |                   // Take a reference\n-                     DSPQUEUE_BUFFER_FLAG_FLUSH_SENDER |          // Flush CPU\n+    bufs[1].flags  = (DSPQUEUE_BUFFER_FLAG_FLUSH_SENDER |         // Flush CPU\n                      DSPQUEUE_BUFFER_FLAG_INVALIDATE_RECIPIENT);  // Invalidate DSP\n \n     // Third buffer = Output Activations. We'll handle DSP\n@@ -2512,7 +2488,7 @@ static void ggml_hexagon_binary(const struct ggml_tensor * op, uint32_t flags) {\n     bufs[2].ptr    = dst->data;\n     bufs[2].offset = (uint8_t *) dst->data - dst_buf->base;\n     bufs[2].size   = ggml_nbytes(dst);\n-    bufs[2].flags  = (DSPQUEUE_BUFFER_FLAG_REF | DSPQUEUE_BUFFER_FLAG_FLUSH_SENDER);\n+    bufs[2].flags  = (DSPQUEUE_BUFFER_FLAG_FLUSH_SENDER);\n \n     // Primary DSP session from the src0 tensor\n     ggml_hexagon_session * sess = src0_buf->sess;\n@@ -2540,26 +2516,7 @@ static void ggml_hexagon_binary(const struct ggml_tensor * op, uint32_t flags) {\n     }\n \n     if ((opt_opmask & HTP_OPMASK_QUEUE)) {\n-        // Bump pending flag (cleared in the callback once we get the responce)\n-        sess->op_pending++;  // atomic inc\n-\n-        int err = dspqueue_write(sess->queue,\n-                                 0,                       // flags - the framework will autoset this\n-                                 3,                       // number of buffers\n-                                 bufs,                    // buffer references\n-                                 sizeof(req),\n-                                 (const uint8_t *) &req,  // Message\n-                                 1000000);                // Timeout\n-\n-        if (0 != err) {\n-            GGML_ABORT(\"ggml-hex: %s dspqueue_write failed: 0x%08x\\n\", sess->name.c_str(), (unsigned) err);\n-        }\n-    }\n-\n-    if (opt_opsync) {\n-        while (sess->op_pending) {\n-            ;\n-        }\n+        sess->enqueue(req, bufs, 3, opt_opsync);\n     }\n \n     t2 = ggml_time_us();\n@@ -2624,34 +2581,31 @@ static void ggml_hexagon_add_id(const struct ggml_tensor * op, uint32_t flags) {\n     bufs[0].ptr    = src0->data;\n     bufs[0].offset = (uint8_t *) src0->data - src0_buf->base;\n     bufs[0].size   = ggml_nbytes(src0);\n-    bufs[0].flags  = (DSPQUEUE_BUFFER_FLAG_REF |                   // Take a reference\n-                     DSPQUEUE_BUFFER_FLAG_FLUSH_SENDER |          // Flush CPU\n+    bufs[0].flags  = (DSPQUEUE_BUFFER_FLAG_FLUSH_SENDER |         // Flush CPU\n                      DSPQUEUE_BUFFER_FLAG_INVALIDATE_RECIPIENT);  // Invalidate DSP;\n \n     // Second buffer = experts bias\n     bufs[1].fd     = src1_buf->fd;\n     bufs[1].ptr    = src1->data;\n     bufs[1].offset = (uint8_t *) src1->data - src1_buf->base;\n     bufs[1].size   = ggml_nbytes(src1);\n-    bufs[1].flags  = (DSPQUEUE_BUFFER_FLAG_REF |                   // Take a reference\n-                     DSPQUEUE_BUFFER_FLAG_FLUSH_SENDER |          // Flush CPU\n+    bufs[1].flags  = (DSPQUEUE_BUFFER_FLAG_FLUSH_SENDER |         // Flush CPU\n                      DSPQUEUE_BUFFER_FLAG_INVALIDATE_RECIPIENT);  // Invalidate DSP\n \n     // Third buffer = activated experts\n     bufs[2].fd     = src2_buf->fd;\n     bufs[2].ptr    = src2->data;\n     bufs[2].offset = (uint8_t *) src2->data - src2_buf->base;\n     bufs[2].size   = ggml_nbytes(src2);\n-    bufs[2].flags  = (DSPQUEUE_BUFFER_FLAG_REF |                   // Take a reference\n-                     DSPQUEUE_BUFFER_FLAG_FLUSH_SENDER |          // Flush CPU\n+    bufs[2].flags  = (DSPQUEUE_BUFFER_FLAG_FLUSH_SENDER |         // Flush CPU\n                      DSPQUEUE_BUFFER_FLAG_INVALIDATE_RECIPIENT);  // Invalidate DSP\n \n     // Forth buffer = output activations\n     bufs[3].fd     = dst_buf->fd;\n     bufs[3].ptr    = dst->data;\n     bufs[3].offset = (uint8_t *) dst->data - dst_buf->base;\n     bufs[3].size   = ggml_nbytes(dst);\n-    bufs[3].flags  = (DSPQUEUE_BUFFER_FLAG_REF | DSPQUEUE_BUFFER_FLAG_FLUSH_SENDER);\n+    bufs[3].flags  = (DSPQUEUE_BUFFER_FLAG_FLUSH_SENDER);\n \n     // Primary DSP session from the src0 tensor\n     ggml_hexagon_session * sess = src0_buf->sess;\n@@ -2681,26 +2635,7 @@ static void ggml_hexagon_add_id(const struct ggml_tensor * op, uint32_t flags) {\n     }\n \n     if ((opt_opmask & HTP_OPMASK_QUEUE)) {\n-        // Bump pending flag (cleared in the callback once we get the responce)\n-        sess->op_pending++;  // atomic inc\n-\n-        int err = dspqueue_write(sess->queue,\n-                                 0,                       // flags - the framework will autoset this\n-                                 4,                       // number of buffers\n-                                 bufs,                    // buffer references\n-                                 sizeof(req),\n-                                 (const uint8_t *) &req,  // Message\n-                                 1000000);                // Timeout\n-\n-        if (0 != err) {\n-            GGML_ABORT(\"ggml-hex: %s dspqueue_write failed: 0x%08x\\n\", sess->name.c_str(), (unsigned) err);\n-        }\n-    }\n-\n-    if (opt_opsync) {\n-        while (sess->op_pending) {\n-            ;\n-        }\n+        sess->enqueue(req, bufs, 4, opt_opsync);\n     }\n \n     t2 = ggml_time_us();\n@@ -2798,8 +2733,7 @@ static void ggml_hexagon_unary(const struct ggml_tensor * op, uint32_t flags) {\n     bufs[n_bufs].ptr    = src0->data;\n     bufs[n_bufs].offset = (uint8_t *) src0->data - src0_buf->base;\n     bufs[n_bufs].size   = ggml_nbytes(src0);\n-    bufs[n_bufs].flags  = (DSPQUEUE_BUFFER_FLAG_REF |                   // Take a reference\n-                          DSPQUEUE_BUFFER_FLAG_FLUSH_SENDER |          // Flush CPU\n+    bufs[n_bufs].flags  = (DSPQUEUE_BUFFER_FLAG_FLUSH_SENDER |         // Flush CPU\n                           DSPQUEUE_BUFFER_FLAG_INVALIDATE_RECIPIENT);  // Invalidate DSP;\n     ++n_bufs;\n \n@@ -2814,8 +2748,7 @@ static void ggml_hexagon_unary(const struct ggml_tensor * op, uint32_t flags) {\n         bufs[n_bufs].ptr    = src1->data;\n         bufs[n_bufs].offset = (uint8_t *) src1->data - src1_buf->base;\n         bufs[n_bufs].size   = ggml_nbytes(src1);\n-        bufs[n_bufs].flags  = (DSPQUEUE_BUFFER_FLAG_REF |                   // Take a reference\n-                              DSPQUEUE_BUFFER_FLAG_FLUSH_SENDER |          // Flush CPU\n+        bufs[n_bufs].flags  = (DSPQUEUE_BUFFER_FLAG_FLUSH_SENDER |         // Flush CPU\n                               DSPQUEUE_BUFFER_FLAG_INVALIDATE_RECIPIENT);  // Invalidate DSP\n         ++n_bufs;\n     }\n@@ -2830,7 +2763,7 @@ static void ggml_hexagon_unary(const struct ggml_tensor * op, uint32_t flags) {\n     bufs[n_bufs].ptr    = dst->data;\n     bufs[n_bufs].offset = (uint8_t *) dst->data - dst_buf->base;\n     bufs[n_bufs].size   = ggml_nbytes(dst);\n-    bufs[n_bufs].flags  = (DSPQUEUE_BUFFER_FLAG_REF | DSPQUEUE_BUFFER_FLAG_FLUSH_SENDER);\n+    bufs[n_bufs].flags  = (DSPQUEUE_BUFFER_FLAG_FLUSH_SENDER);\n     ++n_bufs;\n \n     // Primary DSP session from the src0 tensor\n@@ -2863,26 +2796,7 @@ static void ggml_hexagon_unary(const struct ggml_tensor * op, uint32_t flags) {\n     }\n \n     if ((opt_opmask & HTP_OPMASK_QUEUE)) {\n-        // Bump pending flag (cleared in the callback once we get the responce)\n-        sess->op_pending++;  // atomic inc\n-\n-        int err = dspqueue_write(sess->queue,\n-                                 0,                       // flags - the framework will autoset this\n-                                 n_bufs,                  // number of buffers\n-                                 bufs,                    // buffer references\n-                                 sizeof(req),\n-                                 (const uint8_t *) &req,  // Message\n-                                 1000000);                // Timeout\n-\n-        if (0 != err) {\n-            GGML_ABORT(\"ggml-hex: %s dspqueue_write failed: 0x%08x\\n\", sess->name.c_str(), (unsigned) err);\n-        }\n-    }\n-\n-    if (opt_opsync) {\n-        while (sess->op_pending) {\n-            ;\n-        }\n+        sess->enqueue(req, bufs, n_bufs, opt_opsync);\n     }\n \n     t2 = ggml_time_us();\n@@ -2956,8 +2870,7 @@ static void ggml_hexagon_rope(const struct ggml_tensor * op, uint32_t flags) {\n     bufs[n_bufs].ptr    = src0->data;\n     bufs[n_bufs].offset = (uint8_t *) src0->data - src0_buf->base;\n     bufs[n_bufs].size   = ggml_nbytes(src0);\n-    bufs[n_bufs].flags  = (DSPQUEUE_BUFFER_FLAG_REF |                   // Take a reference\n-                          DSPQUEUE_BUFFER_FLAG_FLUSH_SENDER |          // Flush CPU\n+    bufs[n_bufs].flags  = (DSPQUEUE_BUFFER_FLAG_FLUSH_SENDER |         // Flush CPU\n                           DSPQUEUE_BUFFER_FLAG_INVALIDATE_RECIPIENT);  // Invalidate DSP;\n     ++n_bufs;\n \n@@ -2971,8 +2884,7 @@ static void ggml_hexagon_rope(const struct ggml_tensor * op, uint32_t flags) {\n     bufs[n_bufs].ptr    = src1->data;\n     bufs[n_bufs].offset = (uint8_t *) src1->data - src1_buf->base;\n     bufs[n_bufs].size   = ggml_nbytes(src1);\n-    bufs[n_bufs].flags  = (DSPQUEUE_BUFFER_FLAG_REF |                   // Take a reference\n-                          DSPQUEUE_BUFFER_FLAG_FLUSH_SENDER |          // Flush CPU\n+    bufs[n_bufs].flags  = (DSPQUEUE_BUFFER_FLAG_FLUSH_SENDER |         // Flush CPU\n                           DSPQUEUE_BUFFER_FLAG_INVALIDATE_RECIPIENT);  // Invalidate DSP\n     ++n_bufs;\n \n@@ -2987,8 +2899,7 @@ static void ggml_hexagon_rope(const struct ggml_tensor * op, uint32_t flags) {\n         bufs[n_bufs].ptr    = src2->data;\n         bufs[n_bufs].offset = (uint8_t *) src2->data - src2_buf->base;\n         bufs[n_bufs].size   = ggml_nbytes(src2);\n-        bufs[n_bufs].flags  = (DSPQUEUE_BUFFER_FLAG_REF |                   // Take a reference\n-                              DSPQUEUE_BUFFER_FLAG_FLUSH_SENDER |          // Flush CPU\n+        bufs[n_bufs].flags  = (DSPQUEUE_BUFFER_FLAG_FLUSH_SENDER |         // Flush CPU\n                               DSPQUEUE_BUFFER_FLAG_INVALIDATE_RECIPIENT);  // Invalidate DSP\n         ++n_bufs;\n     }\n@@ -3003,7 +2914,7 @@ static void ggml_hexagon_rope(const struct ggml_tensor * op, uint32_t flags) {\n     bufs[n_bufs].ptr    = dst->data;\n     bufs[n_bufs].offset = (uint8_t *) dst->data - dst_buf->base;\n     bufs[n_bufs].size   = ggml_nbytes(dst);\n-    bufs[n_bufs].flags  = (DSPQUEUE_BUFFER_FLAG_REF | DSPQUEUE_BUFFER_FLAG_FLUSH_SENDER);\n+    bufs[n_bufs].flags  = (DSPQUEUE_BUFFER_FLAG_FLUSH_SENDER);\n     ++n_bufs;\n \n     // Primary DSP session from the src0 tensor\n@@ -3036,26 +2947,7 @@ static void ggml_hexagon_rope(const struct ggml_tensor * op, uint32_t flags) {\n     }\n \n     if ((opt_opmask & HTP_OPMASK_QUEUE)) {\n-        // Bump pending flag (cleared in the callback once we get the responce)\n-        sess->op_pending++;  // atomic inc\n-\n-        int err = dspqueue_write(sess->queue,\n-                                 0,                       // flags - the framework will autoset this\n-                                 n_bufs,                  // number of buffers\n-                                 bufs,                    // buffer references\n-                                 sizeof(req),\n-                                 (const uint8_t *) &req,  // Message\n-                                 1000000);                // Timeout\n-\n-        if (0 != err) {\n-            GGML_ABORT(\"ggml-hex: %s dspqueue_write failed: 0x%08x\\n\", sess->name.c_str(), (unsigned) err);\n-        }\n-    }\n-\n-    if (opt_opsync) {\n-        while (sess->op_pending) {\n-            ;\n-        }\n+        sess->enqueue(req, bufs, n_bufs, opt_opsync);\n     }\n \n     t2 = ggml_time_us();\n@@ -3200,9 +3092,7 @@ static ggml_status ggml_backend_hexagon_graph_compute(ggml_backend_t backend, gg\n     }\n \n     // Wait until all pending ops complete\n-    while (sess->op_pending) {\n-        ;\n-    }\n+    sess->flush();\n \n     return GGML_STATUS_SUCCESS;\n }\n@@ -3213,9 +3103,7 @@ static void ggml_backend_hexagon_synchronize(ggml_backend_t backend) {\n     HEX_VERBOSE(\"ggml-hex: %s synchronize\\n\", sess->name.c_str());\n \n     // Wait until all pending ops complete\n-    while (sess->op_pending) {\n-        ;\n-    }\n+    sess->flush();\n }\n \n struct node_info {"
      },
      {
        "filename": "ggml/src/ggml-hexagon/htp/main.c",
        "status": "modified",
        "additions": 50,
        "deletions": 166,
        "changes": 216,
        "patch": "@@ -395,28 +395,14 @@ static void proc_matmul_req(struct htp_context *     ctx,\n                             struct htp_general_req * req,\n                             struct dspqueue_buffer * bufs,\n                             size_t                   n_bufs) {\n-    // Prep response buffer structs (needed for error responses, etc)\n-    struct dspqueue_buffer rsp_bufs[HTP_MAX_PACKET_BUFFERS];\n-    memset(rsp_bufs, 0, sizeof(rsp_bufs));\n-    rsp_bufs[0].fd     = bufs[0].fd;\n-    rsp_bufs[0].ptr    = bufs[0].ptr;\n-    rsp_bufs[0].size   = bufs[0].size;\n-    rsp_bufs[0].offset = bufs[0].offset;\n-    rsp_bufs[0].flags  = DSPQUEUE_BUFFER_FLAG_DEREF;  // Release reference\n-\n-    rsp_bufs[1].fd     = bufs[1].fd;\n-    rsp_bufs[1].ptr    = bufs[1].ptr;\n-    rsp_bufs[1].size   = bufs[1].size;\n-    rsp_bufs[1].offset = bufs[1].offset;\n-    rsp_bufs[1].flags  = DSPQUEUE_BUFFER_FLAG_DEREF;  // Release reference\n+    struct dspqueue_buffer rsp_bufs[1];\n \n     // We had written to the output buffer, we'd also need to flush it\n-    rsp_bufs[2].fd     = bufs[2].fd;\n-    rsp_bufs[2].ptr    = bufs[2].ptr;\n-    rsp_bufs[2].size   = bufs[2].size;\n-    rsp_bufs[2].offset = bufs[2].offset;\n-    rsp_bufs[2].flags  = (DSPQUEUE_BUFFER_FLAG_DEREF |                 // Release reference\n-                         DSPQUEUE_BUFFER_FLAG_FLUSH_SENDER |          // Flush NSP\n+    rsp_bufs[0].fd     = bufs[2].fd;\n+    rsp_bufs[0].ptr    = bufs[2].ptr;\n+    rsp_bufs[0].size   = bufs[2].size;\n+    rsp_bufs[0].offset = bufs[2].offset;\n+    rsp_bufs[0].flags  = (DSPQUEUE_BUFFER_FLAG_FLUSH_SENDER |         // Flush HTP\n                          DSPQUEUE_BUFFER_FLAG_INVALIDATE_RECIPIENT);  // Invalidate CPU\n \n     // Setup Op context\n@@ -444,41 +430,21 @@ static void proc_matmul_req(struct htp_context *     ctx,\n     }\n \n     profile_stop(&prof);\n-    send_htp_rsp(ctx, req->op, rsp_status, rsp_bufs, 3, &prof);\n+    send_htp_rsp(ctx, req->op, rsp_status, rsp_bufs, 1, &prof);\n }\n \n static void proc_matmul_id_req(struct htp_context *     ctx,\n                                struct htp_general_req * req,\n                                struct dspqueue_buffer * bufs,\n                                size_t                   n_bufs) {\n-    // Prep response buffer structs (needed for error responses, etc)\n-    struct dspqueue_buffer rsp_bufs[HTP_MAX_PACKET_BUFFERS];\n-    memset(rsp_bufs, 0, sizeof(rsp_bufs));\n-    rsp_bufs[0].fd     = bufs[0].fd;\n-    rsp_bufs[0].ptr    = bufs[0].ptr;\n-    rsp_bufs[0].size   = bufs[0].size;\n-    rsp_bufs[0].offset = bufs[0].offset;\n-    rsp_bufs[0].flags  = DSPQUEUE_BUFFER_FLAG_DEREF;  // Release reference\n-\n-    rsp_bufs[1].fd     = bufs[1].fd;\n-    rsp_bufs[1].ptr    = bufs[1].ptr;\n-    rsp_bufs[1].size   = bufs[1].size;\n-    rsp_bufs[1].offset = bufs[1].offset;\n-    rsp_bufs[1].flags  = DSPQUEUE_BUFFER_FLAG_DEREF;  // Release reference\n-\n-    rsp_bufs[2].fd     = bufs[2].fd;\n-    rsp_bufs[2].ptr    = bufs[2].ptr;\n-    rsp_bufs[2].size   = bufs[2].size;\n-    rsp_bufs[2].offset = bufs[2].offset;\n-    rsp_bufs[2].flags  = DSPQUEUE_BUFFER_FLAG_DEREF;  // Release reference\n+    struct dspqueue_buffer rsp_bufs[1];\n \n     // We had written to the output buffer, we'd also need to flush it\n-    rsp_bufs[3].fd     = bufs[3].fd;\n-    rsp_bufs[3].ptr    = bufs[3].ptr;\n-    rsp_bufs[3].size   = bufs[3].size;\n-    rsp_bufs[3].offset = bufs[3].offset;\n-    rsp_bufs[3].flags  = (DSPQUEUE_BUFFER_FLAG_DEREF |                 // Release reference\n-                         DSPQUEUE_BUFFER_FLAG_FLUSH_SENDER |          // Flush NSP\n+    rsp_bufs[0].fd     = bufs[3].fd;\n+    rsp_bufs[0].ptr    = bufs[3].ptr;\n+    rsp_bufs[0].size   = bufs[3].size;\n+    rsp_bufs[0].offset = bufs[3].offset;\n+    rsp_bufs[0].flags  = (DSPQUEUE_BUFFER_FLAG_FLUSH_SENDER |         // Flush HTP\n                          DSPQUEUE_BUFFER_FLAG_INVALIDATE_RECIPIENT);  // Invalidate CPU\n \n     // Setup Op context\n@@ -508,32 +474,18 @@ static void proc_matmul_id_req(struct htp_context *     ctx,\n     }\n \n     profile_stop(&prof);\n-    send_htp_rsp(ctx, req->op, rsp_status, rsp_bufs, 4, &prof);\n+    send_htp_rsp(ctx, req->op, rsp_status, rsp_bufs, 1, &prof);\n }\n \n static void proc_binary_req(struct htp_context * ctx, struct htp_general_req * req, struct dspqueue_buffer * bufs) {\n-    struct dspqueue_buffer rsp_bufs[HTP_MAX_PACKET_BUFFERS];\n-    memset(rsp_bufs, 0, sizeof(rsp_bufs));\n-\n-    rsp_bufs[0].fd     = bufs[0].fd;\n-    rsp_bufs[0].ptr    = bufs[0].ptr;\n-    rsp_bufs[0].offset = bufs[0].offset;\n-    rsp_bufs[0].size   = bufs[0].size;\n-    rsp_bufs[0].flags  = DSPQUEUE_BUFFER_FLAG_DEREF;  // Release reference\n-\n-    rsp_bufs[1].fd     = bufs[1].fd;\n-    rsp_bufs[1].ptr    = bufs[1].ptr;\n-    rsp_bufs[1].offset = bufs[1].offset;\n-    rsp_bufs[1].size   = bufs[1].size;\n-    rsp_bufs[1].flags  = DSPQUEUE_BUFFER_FLAG_DEREF;  // Release reference\n+    struct dspqueue_buffer rsp_bufs[1];\n \n     // We had written to the output buffer, we'd also need to flush it\n-    rsp_bufs[2].fd     = bufs[2].fd;\n-    rsp_bufs[2].ptr    = bufs[2].ptr;\n-    rsp_bufs[2].offset = bufs[2].offset;\n-    rsp_bufs[2].size   = bufs[2].size;\n-    rsp_bufs[2].flags  = (DSPQUEUE_BUFFER_FLAG_DEREF |                 // Release reference\n-                         DSPQUEUE_BUFFER_FLAG_FLUSH_SENDER |          // Flush NSP\n+    rsp_bufs[0].fd     = bufs[2].fd;\n+    rsp_bufs[0].ptr    = bufs[2].ptr;\n+    rsp_bufs[0].offset = bufs[2].offset;\n+    rsp_bufs[0].size   = bufs[2].size;\n+    rsp_bufs[0].flags  = (DSPQUEUE_BUFFER_FLAG_FLUSH_SENDER |         // Flush HTP\n                          DSPQUEUE_BUFFER_FLAG_INVALIDATE_RECIPIENT);  // Invalidate CPU\n \n     // Setup Op context\n@@ -561,38 +513,18 @@ static void proc_binary_req(struct htp_context * ctx, struct htp_general_req * r\n     }\n \n     profile_stop(&prof);\n-    send_htp_rsp(ctx, req->op, rsp_status, rsp_bufs, 3, &prof);\n+    send_htp_rsp(ctx, req->op, rsp_status, rsp_bufs, 1, &prof);\n }\n \n static void proc_add_id_req(struct htp_context * ctx, struct htp_general_req * req, struct dspqueue_buffer * bufs) {\n-    struct dspqueue_buffer rsp_bufs[HTP_MAX_PACKET_BUFFERS];\n-    memset(rsp_bufs, 0, sizeof(rsp_bufs));\n-\n-    rsp_bufs[0].fd     = bufs[0].fd;\n-    rsp_bufs[0].ptr    = bufs[0].ptr;\n-    rsp_bufs[0].offset = bufs[0].offset;\n-    rsp_bufs[0].size   = bufs[0].size;\n-    rsp_bufs[0].flags  = DSPQUEUE_BUFFER_FLAG_DEREF;  // Release reference\n-\n-    rsp_bufs[1].fd     = bufs[1].fd;\n-    rsp_bufs[1].ptr    = bufs[1].ptr;\n-    rsp_bufs[1].offset = bufs[1].offset;\n-    rsp_bufs[1].size   = bufs[1].size;\n-    rsp_bufs[1].flags  = DSPQUEUE_BUFFER_FLAG_DEREF;  // Release reference\n-\n-    rsp_bufs[2].fd     = bufs[2].fd;\n-    rsp_bufs[2].ptr    = bufs[2].ptr;\n-    rsp_bufs[2].offset = bufs[2].offset;\n-    rsp_bufs[2].size   = bufs[2].size;\n-    rsp_bufs[2].flags  = DSPQUEUE_BUFFER_FLAG_DEREF;  // Release reference\n+    struct dspqueue_buffer rsp_bufs[1];\n \n     // We had written to the output buffer, we'd also need to flush it\n-    rsp_bufs[3].fd     = bufs[3].fd;\n-    rsp_bufs[3].ptr    = bufs[3].ptr;\n-    rsp_bufs[3].offset = bufs[3].offset;\n-    rsp_bufs[3].size   = bufs[3].size;\n-    rsp_bufs[3].flags  = (DSPQUEUE_BUFFER_FLAG_DEREF |                 // Release reference\n-                         DSPQUEUE_BUFFER_FLAG_FLUSH_SENDER |          // Flush NSP\n+    rsp_bufs[0].fd     = bufs[3].fd;\n+    rsp_bufs[0].ptr    = bufs[3].ptr;\n+    rsp_bufs[0].offset = bufs[3].offset;\n+    rsp_bufs[0].size   = bufs[3].size;\n+    rsp_bufs[0].flags  = (DSPQUEUE_BUFFER_FLAG_FLUSH_SENDER |         // Flush HTP\n                          DSPQUEUE_BUFFER_FLAG_INVALIDATE_RECIPIENT);  // Invalidate CPU\n \n     // Setup Op context\n@@ -622,26 +554,18 @@ static void proc_add_id_req(struct htp_context * ctx, struct htp_general_req * r\n     }\n \n     profile_stop(&prof);\n-    send_htp_rsp(ctx, req->op, rsp_status, rsp_bufs, 4, &prof);\n+    send_htp_rsp(ctx, req->op, rsp_status, rsp_bufs, 1, &prof);\n }\n \n static void proc_unary_req(struct htp_context * ctx, struct htp_general_req * req, struct dspqueue_buffer * bufs) {\n     struct dspqueue_buffer rsp_bufs[HTP_MAX_PACKET_BUFFERS];\n-    memset(rsp_bufs, 0, sizeof(rsp_bufs));\n-\n-    rsp_bufs[0].fd     = bufs[0].fd;\n-    rsp_bufs[0].ptr    = bufs[0].ptr;\n-    rsp_bufs[0].offset = bufs[0].offset;\n-    rsp_bufs[0].size   = bufs[0].size;\n-    rsp_bufs[0].flags  = DSPQUEUE_BUFFER_FLAG_DEREF;  // Release reference\n \n     // We had written to the output buffer, we'd also need to flush it\n-    rsp_bufs[1].fd     = bufs[1].fd;\n-    rsp_bufs[1].ptr    = bufs[1].ptr;\n-    rsp_bufs[1].offset = bufs[1].offset;\n-    rsp_bufs[1].size   = bufs[1].size;\n-    rsp_bufs[1].flags  = (DSPQUEUE_BUFFER_FLAG_DEREF |                 // Release reference\n-                         DSPQUEUE_BUFFER_FLAG_FLUSH_SENDER |          // Flush NSP\n+    rsp_bufs[0].fd     = bufs[1].fd;\n+    rsp_bufs[0].ptr    = bufs[1].ptr;\n+    rsp_bufs[0].offset = bufs[1].offset;\n+    rsp_bufs[0].size   = bufs[1].size;\n+    rsp_bufs[0].flags  = (DSPQUEUE_BUFFER_FLAG_FLUSH_SENDER |         // Flush HTP\n                          DSPQUEUE_BUFFER_FLAG_INVALIDATE_RECIPIENT);  // Invalidate CPU\n \n     // Setup Op context\n@@ -669,41 +593,24 @@ static void proc_unary_req(struct htp_context * ctx, struct htp_general_req * re\n     }\n \n     profile_stop(&prof);\n-    send_htp_rsp(ctx, req->op, rsp_status, rsp_bufs, 2, &prof);\n+    send_htp_rsp(ctx, req->op, rsp_status, rsp_bufs, 1, &prof);\n }\n \n static void proc_activations_req(struct htp_context *     ctx,\n                                  struct htp_general_req * req,\n                                  struct dspqueue_buffer * bufs,\n                                  uint32_t                 n_bufs) {\n     struct dspqueue_buffer rsp_bufs[HTP_MAX_PACKET_BUFFERS];\n-    memset(rsp_bufs, 0, sizeof(rsp_bufs));\n-\n-    rsp_bufs[0].fd     = bufs[0].fd;\n-    rsp_bufs[0].ptr    = bufs[0].ptr;\n-    rsp_bufs[0].offset = bufs[0].offset;\n-    rsp_bufs[0].size   = bufs[0].size;\n-    rsp_bufs[0].flags  = DSPQUEUE_BUFFER_FLAG_DEREF;  // Release reference\n \n-    int write_idx = 1;\n-    if (3 == n_bufs) {\n-        rsp_bufs[1].fd     = bufs[1].fd;\n-        rsp_bufs[1].ptr    = bufs[1].ptr;\n-        rsp_bufs[1].offset = bufs[1].offset;\n-        rsp_bufs[1].size   = bufs[1].size;\n-        rsp_bufs[1].flags  = DSPQUEUE_BUFFER_FLAG_DEREF;  // Release reference\n-\n-        write_idx = 2;\n-    }\n+    int write_idx = (n_bufs == 3) ? 2 : 1;\n \n     // We had written to the output buffer, we'd also need to flush it\n-    rsp_bufs[write_idx].fd     = bufs[write_idx].fd;\n-    rsp_bufs[write_idx].ptr    = bufs[write_idx].ptr;\n-    rsp_bufs[write_idx].offset = bufs[write_idx].offset;\n-    rsp_bufs[write_idx].size   = bufs[write_idx].size;\n-    rsp_bufs[write_idx].flags  = (DSPQUEUE_BUFFER_FLAG_DEREF |                 // Release reference\n-                                 DSPQUEUE_BUFFER_FLAG_FLUSH_SENDER |          // Flush NSP\n-                                 DSPQUEUE_BUFFER_FLAG_INVALIDATE_RECIPIENT);  // Invalidate CPU\n+    rsp_bufs[0].fd     = bufs[write_idx].fd;\n+    rsp_bufs[0].ptr    = bufs[write_idx].ptr;\n+    rsp_bufs[0].offset = bufs[write_idx].offset;\n+    rsp_bufs[0].size   = bufs[write_idx].size;\n+    rsp_bufs[0].flags  = (DSPQUEUE_BUFFER_FLAG_FLUSH_SENDER |         // Flush HTP\n+                          DSPQUEUE_BUFFER_FLAG_INVALIDATE_RECIPIENT); // Invalidate CPU\n \n     // Setup Op context\n     struct htp_ops_context octx = { 0 };\n@@ -742,47 +649,24 @@ static void proc_activations_req(struct htp_context *     ctx,\n     }\n \n     profile_stop(&prof);\n-    send_htp_rsp(ctx, req->op, rsp_status, rsp_bufs, n_bufs, &prof);\n+    send_htp_rsp(ctx, req->op, rsp_status, rsp_bufs, 1, &prof);\n }\n \n static void proc_rope_req(struct htp_context *     ctx,\n                           struct htp_general_req * req,\n                           struct dspqueue_buffer * bufs,\n                           uint32_t                 n_bufs) {\n     struct dspqueue_buffer rsp_bufs[HTP_MAX_PACKET_BUFFERS];\n-    memset(rsp_bufs, 0, sizeof(rsp_bufs));\n-\n-    rsp_bufs[0].fd     = bufs[0].fd;\n-    rsp_bufs[0].ptr    = bufs[0].ptr;\n-    rsp_bufs[0].offset = bufs[0].offset;\n-    rsp_bufs[0].size   = bufs[0].size;\n-    rsp_bufs[0].flags  = DSPQUEUE_BUFFER_FLAG_DEREF;  // Release reference\n \n-    rsp_bufs[1].fd     = bufs[1].fd;\n-    rsp_bufs[1].ptr    = bufs[1].ptr;\n-    rsp_bufs[1].offset = bufs[1].offset;\n-    rsp_bufs[1].size   = bufs[1].size;\n-    rsp_bufs[1].flags  = DSPQUEUE_BUFFER_FLAG_DEREF;  // Release reference\n-\n-    int write_idx = 2;\n-    if (4 == n_bufs) {\n-        rsp_bufs[write_idx].fd     = bufs[write_idx].fd;\n-        rsp_bufs[write_idx].ptr    = bufs[write_idx].ptr;\n-        rsp_bufs[write_idx].offset = bufs[write_idx].offset;\n-        rsp_bufs[write_idx].size   = bufs[write_idx].size;\n-        rsp_bufs[write_idx].flags  = DSPQUEUE_BUFFER_FLAG_DEREF;  // Release reference\n-\n-        write_idx++;\n-    }\n+    int write_idx = (n_bufs == 4) ? 3 : 2;\n \n     // We had written to the output buffer, we'd also need to flush it\n-    rsp_bufs[write_idx].fd     = bufs[write_idx].fd;\n-    rsp_bufs[write_idx].ptr    = bufs[write_idx].ptr;\n-    rsp_bufs[write_idx].offset = bufs[write_idx].offset;\n-    rsp_bufs[write_idx].size   = bufs[write_idx].size;\n-    rsp_bufs[write_idx].flags  = (DSPQUEUE_BUFFER_FLAG_DEREF |                 // Release reference\n-                                 DSPQUEUE_BUFFER_FLAG_FLUSH_SENDER |          // Flush NSP\n-                                 DSPQUEUE_BUFFER_FLAG_INVALIDATE_RECIPIENT);  // Invalidate CPU\n+    rsp_bufs[0].fd     = bufs[write_idx].fd;\n+    rsp_bufs[0].ptr    = bufs[write_idx].ptr;\n+    rsp_bufs[0].offset = bufs[write_idx].offset;\n+    rsp_bufs[0].size   = bufs[write_idx].size;\n+    rsp_bufs[0].flags  = (DSPQUEUE_BUFFER_FLAG_FLUSH_SENDER |         // Flush HTP\n+                          DSPQUEUE_BUFFER_FLAG_INVALIDATE_RECIPIENT); // Invalidate CPU\n \n     // Setup Op context\n     struct htp_ops_context octx = { 0 };\n@@ -819,7 +703,7 @@ static void proc_rope_req(struct htp_context *     ctx,\n     }\n \n     profile_stop(&prof);\n-    send_htp_rsp(ctx, req->op, rsp_status, rsp_bufs, n_bufs, &prof);\n+    send_htp_rsp(ctx, req->op, rsp_status, rsp_bufs, 1, &prof);\n }\n \n static void htp_packet_callback(dspqueue_t queue, int error, void * context) {"
      },
      {
        "filename": "scripts/snapdragon/adb/run-bench.sh",
        "status": "modified",
        "additions": 2,
        "deletions": 1,
        "changes": 3,
        "patch": "@@ -35,5 +35,6 @@ adb $adbserial shell \" \\\n   LD_LIBRARY_PATH=$basedir/$branch/lib   \\\n   ADSP_LIBRARY_PATH=$basedir/$branch/lib \\\n     $ndev $nhvx $opmask ./$branch/bin/llama-bench --device $device --mmap 0 -m $basedir/../gguf/$model \\\n-        -t 4 --batch-size 128 -ngl 99 $@ \\\n+        --poll 1000 -t 6 --cpu-mask 0xfc --cpu-strict 1 \\\n+        --batch-size 128 -ngl 99 $@ \\\n \""
      },
      {
        "filename": "scripts/snapdragon/adb/run-cli.sh",
        "status": "modified",
        "additions": 4,
        "deletions": 3,
        "changes": 7,
        "patch": "@@ -45,8 +45,9 @@ adb $adbserial shell \" \\\n   cd $basedir; ulimit -c unlimited;        \\\n     LD_LIBRARY_PATH=$basedir/$branch/lib   \\\n     ADSP_LIBRARY_PATH=$basedir/$branch/lib \\\n-    $verbose $experimental $sched $opmask $profile $nhvx $ndev           \\\n-      ./$branch/bin/llama-cli --no-mmap -m $basedir/../gguf/$model       \\\n-         -t 4 --ctx-size 8192 --batch-size 128 -ctk q8_0 -ctv q8_0 -fa on \\\n+    $verbose $experimental $sched $opmask $profile $nhvx $ndev       \\\n+      ./$branch/bin/llama-cli --no-mmap -m $basedir/../gguf/$model   \\\n+         --poll 1000 -t 6 --cpu-mask 0xfc --cpu-strict 1             \\\n+         --ctx-size 8192 --batch-size 128 -ctk q8_0 -ctv q8_0 -fa on \\\n          -ngl 99 --device $device $cli_opts $@ \\\n \""
      }
    ],
    "num_files": 5,
    "scraped_at": "2025-11-16T23:29:26.609860"
  },
  {
    "pr_number": 16812,
    "title": "memory : remove KV cache size padding",
    "body": "ref https://github.com/ggml-org/llama.cpp/pull/16736#discussion_r2455431214\r\n\r\nSimplify the logic during memory module creation. Before, for KV caches, we used to pad the buffer size up to 256 cells since flash attention implementations did not support arbitrary K, V sizes. After the improvements in #16148 and related, we no longer need to explicitly do this padding.\r\n\r\nFor now, keeping support for `llama_kv_cache` size padding via the constructor's `n_pad` argument, although it is not currently used anymore.\r\n\r\nNote that we continue to pad `n_kv` - this is the tensor shape for the K and V tensors for each graph:\r\n\r\nhttps://github.com/ggml-org/llama.cpp/blob/1473d59a7bb0716aee4ab104ab09ecf7d978bd40/src/llama-kv-cache.cpp#L957-L972\r\n\r\nWe need to do this in order to reuse most of the compute graphs during the text generation phase. Additionally, this helps the performance with some of the backends.\r\n\r\nAlso, `llama_model::create_memory()` no longer mutates `cparams`.\r\n\r\nNext, will rebase https://github.com/ggml-org/llama.cpp/pull/16736 on top of this change and finish it.",
    "html_url": "https://github.com/ggml-org/llama.cpp/pull/16812",
    "created_at": "2025-10-28T07:32:53Z",
    "merged_at": "2025-10-28T18:19:44Z",
    "merge_commit_sha": "85a7d8677bf2200981e52f744a21d5267964ffcf",
    "base_ref": "master",
    "head_sha": "edd2e8cfc7b8c124617085e30e28aff5e87af833",
    "user": "ggerganov",
    "files": [
      {
        "filename": "src/llama-kv-cache.cpp",
        "status": "modified",
        "additions": 5,
        "deletions": 6,
        "changes": 11,
        "patch": "@@ -957,10 +957,14 @@ bool llama_kv_cache::get_has_shift() const {\n uint32_t llama_kv_cache::get_n_kv(const slot_info & sinfo) const {\n     uint32_t result = 0;\n \n+    // pad the n_kv value so that the graph remains constant across batches and can be reused\n+    // note: this also helps some backends with performance (f.ex https://github.com/ggml-org/llama.cpp/pull/16812#issuecomment-3455112220)\n+    const uint32_t n_pad_cur = std::max(n_pad, 256u);\n+\n     for (uint32_t s = 0; s < sinfo.n_stream(); ++s) {\n         const auto & cells = v_cells[sinfo.strm[s]];\n \n-        result = std::max(std::min(cells.size(), std::max(n_pad, GGML_PAD(cells.used_max_p1(), n_pad))), result);\n+        result = std::max(std::min(cells.size(), std::max(n_pad_cur, GGML_PAD(cells.used_max_p1(), n_pad_cur))), result);\n     }\n \n     return result;\n@@ -2010,8 +2014,3 @@ void llama_kv_cache_context::set_input_kq_mask(ggml_tensor * dst, const llama_ub\n void llama_kv_cache_context::set_input_pos_bucket(ggml_tensor * dst, const llama_ubatch * ubatch) const {\n     kv->set_input_pos_bucket(dst, ubatch);\n }\n-\n-uint32_t llama_kv_cache::get_padding(const llama_cparams & cparams) {\n-    // the FA kernels require padding to avoid extra runtime boundary checks\n-    return cparams.flash_attn ? 256u : 32u;\n-}"
      },
      {
        "filename": "src/llama-kv-cache.h",
        "status": "modified",
        "additions": 0,
        "deletions": 2,
        "changes": 2,
        "patch": "@@ -19,8 +19,6 @@ struct llama_context;\n \n class llama_kv_cache : public llama_memory_i {\n public:\n-    static uint32_t get_padding(const llama_cparams & cparams);\n-\n     struct stream_copy_info {\n         bool empty() const {\n             assert(ssrc.size() == sdst.size());"
      },
      {
        "filename": "src/llama-model.cpp",
        "status": "modified",
        "additions": 4,
        "deletions": 19,
        "changes": 23,
        "patch": "@@ -19641,7 +19641,7 @@ struct llm_build_apertus : public llm_graph_context {\n     }\n };\n \n-llama_memory_i * llama_model::create_memory(const llama_memory_params & params, llama_cparams & cparams) const {\n+llama_memory_i * llama_model::create_memory(const llama_memory_params & params, const llama_cparams & cparams) const {\n     llama_memory_i * res;\n \n     switch (arch) {\n@@ -19692,17 +19692,13 @@ llama_memory_i * llama_model::create_memory(const llama_memory_params & params,\n                         };\n                     }\n \n-                    const auto padding = llama_kv_cache::get_padding(cparams);\n-\n-                    cparams.n_ctx = GGML_PAD(cparams.n_ctx, padding);\n-\n                     res = new llama_memory_hybrid(\n                         /* model             */ *this,\n                         /* attn_type_k       */ params.type_k,\n                         /* attn_type_v       */ params.type_v,\n                         /* attn_v_trans      */ !cparams.flash_attn,\n                         /* attn_kv_size      */ cparams.n_ctx,\n-                        /* attn_n_pad        */ padding,\n+                        /* attn_n_pad        */ 1,\n                         /* attn_n_swa        */ hparams.n_swa,\n                         /* attn_swa_type     */ hparams.swa_type,\n                         /* recurrent_type_k  */ GGML_TYPE_F32,\n@@ -19714,23 +19710,12 @@ llama_memory_i * llama_model::create_memory(const llama_memory_params & params,\n                         /* filter_attn       */ std::move(filter_attn),\n                         /* filter_recr       */ std::move(filter_recr));\n                 } else {\n-                    const auto padding = llama_kv_cache::get_padding(cparams);\n-\n                     uint32_t n_ctx_per_stream = cparams.n_ctx;\n \n                     if (!cparams.kv_unified) {\n                         n_ctx_per_stream = (cparams.n_ctx + cparams.n_seq_max - 1)/cparams.n_seq_max;\n-                        n_ctx_per_stream = GGML_PAD(n_ctx_per_stream, padding);\n-\n-                        cparams.n_ctx = n_ctx_per_stream*cparams.n_seq_max;\n-                    } else {\n-                        n_ctx_per_stream = GGML_PAD(n_ctx_per_stream, padding);\n-\n-                        cparams.n_ctx = n_ctx_per_stream;\n                     }\n \n-                    LLAMA_LOG_DEBUG(\"%s: n_ctx = %u (padded)\\n\", __func__, cparams.n_ctx);\n-\n                     llama_memory_i::layer_reuse_cb reuse = nullptr;\n \n                     if (arch == LLM_ARCH_GEMMA3N) {\n@@ -19757,7 +19742,7 @@ llama_memory_i * llama_model::create_memory(const llama_memory_params & params,\n                                 n_ctx_per_stream,\n                                 cparams.n_seq_max,\n                                 cparams.n_ubatch,\n-                                padding,\n+                                1,\n                                 nullptr,\n                                 reuse);\n                     } else {\n@@ -19772,7 +19757,7 @@ llama_memory_i * llama_model::create_memory(const llama_memory_params & params,\n                                 cparams.kv_unified,\n                                 n_ctx_per_stream,\n                                 cparams.n_seq_max,\n-                                padding,\n+                                1,\n                                 hparams.n_swa,\n                                 hparams.swa_type,\n                                 nullptr,"
      },
      {
        "filename": "src/llama-model.h",
        "status": "modified",
        "additions": 1,
        "deletions": 2,
        "changes": 3,
        "patch": "@@ -500,9 +500,8 @@ struct llama_model {\n \n     ggml_tensor * get_rope_factors(const llama_cparams & cparams, int il) const;\n \n-    // note: can mutate `cparams`\n     // TODO: move this to new llm_arch_model_i interface\n-    llama_memory_i * create_memory(const llama_memory_params & params, llama_cparams & cparams) const;\n+    llama_memory_i * create_memory(const llama_memory_params & params, const llama_cparams & cparams) const;\n \n     // TODO: move this to new llm_arch_model_i interface\n     ggml_cgraph * build_graph(const llm_graph_params & params) const;"
      },
      {
        "filename": "tools/server/server.cpp",
        "status": "modified",
        "additions": 3,
        "deletions": 24,
        "changes": 27,
        "patch": "@@ -2866,10 +2866,12 @@ struct server_context {\n \n         // if context shifting is disabled, make sure that we don't run out of context\n         if (!params_base.ctx_shift && slot.n_past + 1 >= slot.n_ctx) {\n+            slot.truncated      = true;\n             slot.stop           = STOP_TYPE_LIMIT;\n             slot.has_next_token = false;\n \n-            SLT_DBG(slot, \"stopped due to running out of context, n_past = %d, n_ctx = %d\\n\", slot.n_past, slot.n_ctx);\n+            SLT_DBG(slot, \"stopped due to running out of context capacity, n_past = %d, n_prompt_tokens = %d, n_decoded = %d, n_ctx = %d\\n\",\n+                    slot.n_decoded, slot.n_prompt_tokens(), slot.n_past, slot.n_ctx);\n         }\n \n         // check the limits\n@@ -2929,36 +2931,13 @@ struct server_context {\n             }\n         }\n \n-        // if context shift is disabled, we stop when it reaches the context limit\n-        if (slot.n_past >= slot.n_ctx) {\n-            slot.truncated      = true;\n-            slot.stop           = STOP_TYPE_LIMIT;\n-            slot.has_next_token = false;\n-\n-            SLT_DBG(slot, \"stopped due to running out of context capacity, n_past = %d, n_prompt_tokens = %d, n_decoded = %d, n_ctx = %d\\n\",\n-                    slot.n_decoded, slot.n_prompt_tokens(), slot.n_past, slot.n_ctx);\n-        }\n-\n         if (llama_vocab_is_eog(vocab, result.tok)) {\n             slot.stop           = STOP_TYPE_EOS;\n             slot.has_next_token = false;\n \n             SLT_DBG(slot, \"%s\", \"stopped by EOS\\n\");\n         }\n \n-        const auto n_ctx_train = llama_model_n_ctx_train(model);\n-\n-        if (slot.task->params.n_predict < 1 && slot.n_prompt_tokens() + slot.n_decoded >= n_ctx_train) {\n-            slot.truncated      = true;\n-            slot.stop           = STOP_TYPE_LIMIT;\n-            slot.has_next_token = false; // stop prediction\n-\n-            SLT_WRN(slot,\n-                    \"n_predict (%d) is set for infinite generation. \"\n-                    \"Limiting generated tokens to n_ctx_train (%d) to avoid EOS-less generation infinite loop\\n\",\n-                    slot.task->params.n_predict, n_ctx_train);\n-        }\n-\n         SLT_DBG(slot, \"n_decoded = %d, n_remaining = %d, next token: %5d '%s'\\n\", slot.n_decoded, slot.n_remaining, result.tok, token_str.c_str());\n \n         return slot.has_next_token; // continue"
      },
      {
        "filename": "tools/server/tests/unit/test_ctx_shift.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -45,7 +45,7 @@ def test_ctx_shift_enabled():\n \n @pytest.mark.parametrize(\"n_predict,n_token_output,truncated\", [\n     (64, 64, False),\n-    (-1, 120, True),\n+    (-1, 248, True), # 8 tokens prompt + 248 tokens generated = 256 tokens total\n ])\n def test_ctx_shift_disabled_short_prompt(n_predict: int, n_token_output: int, truncated: bool):\n     global server"
      }
    ],
    "num_files": 6,
    "scraped_at": "2025-11-16T23:29:27.964205"
  },
  {
    "pr_number": 16808,
    "title": "sycl: add RMS_NORM_BACK operation support",
    "body": "## Summary\r\n\r\nAdd SYCL backend support for `RMS_NORM_BACK` using a single FP32 compensated parallel reduction path.  \r\nNo changes to the public API. Default numerical accuracy is preserved; a fast opt-in macro is also available.\r\n\r\n---\r\n\r\n## Implementation\r\n\r\n### Algorithm (onsistent with existing backend behavior)\r\n\r\n- inv_r  = 1 / sqrt( (\u03a3 x\u00b2) / D + eps )\r\n- coeff  = \u2212 (\u03a3 x\u00b7dz) / (\u03a3 x\u00b2 + D\u00b7eps)\r\n- dx[i]  = (dz[i] + coeff * x[i]) * inv_r\r\n\r\n### What was implemented\r\n\r\n- Per-thread accumulation of `\u03a3 x\u00b2` and `\u03a3 x\u00b7dz` with Kahan-style compensation.\r\n- Warp (`sub_group`) reduction via `warp_reduce_sum`.\r\n- Cross-warp reduction using local memory (one value per warp) with a single barrier.\r\n- `group_broadcast` used to distribute `inv_r` and `coeff` across the work-group.\r\n- Work-group size: multiple of `WARP_SIZE`, capped by device limit (\u2264256), not larger than `D`.\r\n\r\n### Optional fast path\r\n\r\n- Define `GGML_SYCL_RMS_BACK_FAST` to disable compensated summation and use plain FP32 accumulation.\r\n- Default remains high-accuracy compensated mode.\r\n\r\n---\r\n\r\n## Validation\r\n\r\nFocused tests executed locally:\r\n\r\n| Test Suite                            | Result        |\r\n|--------------------------------------|---------------|\r\n| RMS_NORM_BACK (CPU)                  | 4 / 4 passed  |\r\n| RMS_NORM_BACK (SYCL host/GPU)        | 4 / 4 passed  |\r\n| Sanity check (NMSE)                  | \u2248 1e-11       |\r\n\r\nBuild is warning-free for this code path.\r\n\r\n---\r\n\r\n## Reproduce (build + test)\r\n\r\n```bash\r\n# Configure & build with SYCL\r\ncmake -B build -DGGML_SYCL=ON && cmake --build build -j\"$(nproc)\"\r\n\r\n# Focused RMS_NORM_BACK tests\r\n./build/bin/test-backend-ops test -o RMS_NORM_BACK -b CPU\r\nSYCL_DEVICE_FILTER=host ./build/bin/test-backend-ops test -o RMS_NORM_BACK -b SYCL0\r\n./build/bin/test-backend-ops test -o RMS_NORM_BACK -b SYCL0\r\n\r\n# Optional fast path (less numerically stable)\r\n# add -DGGML_SYCL_RMS_BACK_FAST to your compiler definitions\r\n```\r\n\r\n---\r\n\r\n## Files Changed (minimal scope only)\r\n\r\n| File                                       | Purpose                                      |\r\n|--------------------------------------------|----------------------------------------------|\r\n| `ggml/src/ggml-sycl/norm.cpp`              | Implementation of `ggml_sycl_op_rms_norm_back` |\r\n| `ggml/src/ggml-sycl/ggml-sycl.cpp`         | Operation dispatch registration              |\r\n| `ggml/src/ggml-sycl/norm.hpp`              | Function declaration                         |\r\n| `docs/ops.md`                              | Mark RMS_NORM_BACK as \u2705 for SYCL            |\r\n| `docs/ops/SYCL.csv`                        | Mark RMS_NORM_BACK entries as supported      |\r\n\r\nNo unrelated files or personal data included.\r\n\r\n---\r\n\r\n## Notes & Risks\r\n\r\n- Default path gives high numerical accuracy using compensated FP32 sums.\r\n- Fast path is fully optional (disabled by default).\r\n- Reduction order on GPUs is not bitwise-identical to CPU, but produces NMSE \u2248 1e-11.\r\n\r\n---\r\n\r\n## Reviewers\r\n\r\ncc @CISC @NeoZhangJianyu  \r\nLooking forward to your feedback. Thanks in advance!",
    "html_url": "https://github.com/ggml-org/llama.cpp/pull/16808",
    "created_at": "2025-10-27T19:45:57Z",
    "merged_at": "2025-10-29T06:14:39Z",
    "merge_commit_sha": "338074c383c81366320d176d83b94b0a567ee0c2",
    "base_ref": "master",
    "head_sha": "c7bc7f036616cfd5f7176df0a0f55af38718a72f",
    "user": "YaelLogic",
    "files": [
      {
        "filename": "docs/ops.md",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -79,7 +79,7 @@ Legend:\n |                           REPEAT | \u274c | \u2705 | \u2705 | \ud83d\udfe1 | \u2705 | \ud83d\udfe1 | \u2705 | \ud83d\udfe1 | \u274c |\n |                      REPEAT_BACK | \u274c | \u274c | \u2705 | \u2705 | \u274c | \u274c | \u274c | \u2705 | \u274c |\n |                         RMS_NORM | \u274c | \u2705 | \u2705 | \u2705 | \ud83d\udfe1 | \u2705 | \u2705 | \u2705 | \u274c |\n-|                    RMS_NORM_BACK | \u274c | \u274c | \u2705 | \u2705 | \u274c | \u274c | \u274c | \u2705 | \u274c |\n+|                    RMS_NORM_BACK | \u274c | \u274c | \u2705 | \u2705 | \u274c | \u274c | \u2705 | \u2705 | \u274c |\n |                 RMS_NORM_MUL_ADD | \u274c | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u274c |\n |                             ROLL | \u274c | \u274c | \u2705 | \u274c | \u274c | \u274c | \u274c | \u2705 | \u274c |\n |                             ROPE | \u274c | \ud83d\udfe1 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u274c |"
      },
      {
        "filename": "docs/ops/SYCL.csv",
        "status": "modified",
        "additions": 4,
        "deletions": 4,
        "changes": 8,
        "patch": "@@ -5637,25 +5637,25 @@\n \"SYCL0\",\"RMS_NORM\",\"type=f32,ne=[64,5,4,3],v=0,eps=0.000000,inplace=0\",\"support\",\"1\",\"yes\",\"SYCL\"\n \"SYCL0\",\"NORM\",\"type=f32,ne=[64,5,4,3],v=1,eps=0.000000\",\"support\",\"1\",\"yes\",\"SYCL\"\n \"SYCL0\",\"RMS_NORM\",\"type=f32,ne=[64,5,4,3],v=1,eps=0.000000,inplace=0\",\"support\",\"1\",\"yes\",\"SYCL\"\n-\"SYCL0\",\"RMS_NORM_BACK\",\"type=f32,ne=[64,5,4,3],eps=0.000000\",\"support\",\"0\",\"no\",\"SYCL\"\n+\"SYCL0\",\"RMS_NORM_BACK\",\"type=f32,ne=[64,5,4,3],eps=0.000000\",\"support\",\"1\",\"yes\",\"SYCL\"\n \"SYCL0\",\"L2_NORM\",\"type=f32,ne=[64,5,4,3]\",\"support\",\"1\",\"yes\",\"SYCL\"\n \"SYCL0\",\"NORM\",\"type=f32,ne=[64,5,4,3],v=0,eps=0.000001\",\"support\",\"1\",\"yes\",\"SYCL\"\n \"SYCL0\",\"RMS_NORM\",\"type=f32,ne=[64,5,4,3],v=0,eps=0.000001,inplace=0\",\"support\",\"1\",\"yes\",\"SYCL\"\n \"SYCL0\",\"NORM\",\"type=f32,ne=[64,5,4,3],v=1,eps=0.000001\",\"support\",\"1\",\"yes\",\"SYCL\"\n \"SYCL0\",\"RMS_NORM\",\"type=f32,ne=[64,5,4,3],v=1,eps=0.000001,inplace=0\",\"support\",\"1\",\"yes\",\"SYCL\"\n-\"SYCL0\",\"RMS_NORM_BACK\",\"type=f32,ne=[64,5,4,3],eps=0.000001\",\"support\",\"0\",\"no\",\"SYCL\"\n+\"SYCL0\",\"RMS_NORM_BACK\",\"type=f32,ne=[64,5,4,3],eps=0.000001\",\"support\",\"1\",\"yes\",\"SYCL\"\n \"SYCL0\",\"L2_NORM\",\"type=f32,ne=[64,5,4,3]\",\"support\",\"1\",\"yes\",\"SYCL\"\n \"SYCL0\",\"NORM\",\"type=f32,ne=[64,5,4,3],v=0,eps=0.000100\",\"support\",\"1\",\"yes\",\"SYCL\"\n \"SYCL0\",\"RMS_NORM\",\"type=f32,ne=[64,5,4,3],v=0,eps=0.000100,inplace=0\",\"support\",\"1\",\"yes\",\"SYCL\"\n \"SYCL0\",\"NORM\",\"type=f32,ne=[64,5,4,3],v=1,eps=0.000100\",\"support\",\"1\",\"yes\",\"SYCL\"\n \"SYCL0\",\"RMS_NORM\",\"type=f32,ne=[64,5,4,3],v=1,eps=0.000100,inplace=0\",\"support\",\"1\",\"yes\",\"SYCL\"\n-\"SYCL0\",\"RMS_NORM_BACK\",\"type=f32,ne=[64,5,4,3],eps=0.000100\",\"support\",\"0\",\"no\",\"SYCL\"\n+\"SYCL0\",\"RMS_NORM_BACK\",\"type=f32,ne=[64,5,4,3],eps=0.000100\",\"support\",\"1\",\"yes\",\"SYCL\"\n \"SYCL0\",\"L2_NORM\",\"type=f32,ne=[64,5,4,3]\",\"support\",\"1\",\"yes\",\"SYCL\"\n \"SYCL0\",\"NORM\",\"type=f32,ne=[64,5,4,3],v=0,eps=0.100000\",\"support\",\"1\",\"yes\",\"SYCL\"\n \"SYCL0\",\"RMS_NORM\",\"type=f32,ne=[64,5,4,3],v=0,eps=0.100000,inplace=0\",\"support\",\"1\",\"yes\",\"SYCL\"\n \"SYCL0\",\"NORM\",\"type=f32,ne=[64,5,4,3],v=1,eps=0.100000\",\"support\",\"1\",\"yes\",\"SYCL\"\n \"SYCL0\",\"RMS_NORM\",\"type=f32,ne=[64,5,4,3],v=1,eps=0.100000,inplace=0\",\"support\",\"1\",\"yes\",\"SYCL\"\n-\"SYCL0\",\"RMS_NORM_BACK\",\"type=f32,ne=[64,5,4,3],eps=0.100000\",\"support\",\"0\",\"no\",\"SYCL\"\n+\"SYCL0\",\"RMS_NORM_BACK\",\"type=f32,ne=[64,5,4,3],eps=0.100000\",\"support\",\"1\",\"yes\",\"SYCL\"\n \"SYCL0\",\"L2_NORM\",\"type=f32,ne=[64,5,4,3]\",\"support\",\"1\",\"yes\",\"SYCL\"\n \"SYCL0\",\"RMS_NORM\",\"type=f32,ne=[64,5,4,3],v=0,eps=0.000001,inplace=1\",\"support\",\"1\",\"yes\",\"SYCL\"\n \"SYCL0\",\"RMS_NORM_MUL_ADD\",\"type=f32,ne=[64,5,4,3],eps=0.000000,broadcast=0,multi_add=0\",\"support\",\"1\",\"yes\",\"SYCL\""
      },
      {
        "filename": "ggml/src/ggml-sycl/ggml-sycl.cpp",
        "status": "modified",
        "additions": 11,
        "deletions": 0,
        "changes": 11,
        "patch": "@@ -42,6 +42,7 @@\n #include \"ggml-sycl/backend.hpp\"\n #include \"ggml-sycl/common.hpp\"\n #include \"ggml-sycl/element_wise.hpp\"\n+#include \"ggml-sycl/norm.hpp\"\n #include \"ggml-sycl/presets.hpp\"\n #include \"ggml-sycl/gemm.hpp\"\n #include \"ggml-sycl/set_rows.hpp\"\n@@ -2636,6 +2637,11 @@ static void ggml_sycl_rms_norm(ggml_backend_sycl_context & ctx, ggml_tensor * ds\n     ggml_sycl_op_rms_norm(ctx, dst);\n }\n \n+static void ggml_sycl_rms_norm_back(ggml_backend_sycl_context & ctx, ggml_tensor * dst) {\n+    scope_op_debug_print scope_dbg_print(__func__, dst, /*num_src=*/2);\n+    ggml_sycl_op_rms_norm_back(ctx, dst);\n+}\n+\n static void ggml_sycl_l2_norm(ggml_backend_sycl_context & ctx, ggml_tensor * dst) {\n     scope_op_debug_print scope_dbg_print(__func__, dst, /*num_src=*/1);\n     ggml_sycl_op_l2_norm(ctx, dst);\n@@ -3826,6 +3832,9 @@ static bool ggml_sycl_compute_forward(ggml_backend_sycl_context & ctx, struct gg\n         case GGML_OP_LEAKY_RELU:\n             ggml_sycl_leaky_relu(ctx, dst);\n             break;\n+        case GGML_OP_RMS_NORM_BACK:\n+            ggml_sycl_rms_norm_back(ctx, dst);\n+            break;\n         case GGML_OP_RMS_NORM:\n             ggml_sycl_rms_norm(ctx, dst);\n             break;\n@@ -4568,6 +4577,8 @@ static bool ggml_backend_sycl_device_supports_op(ggml_backend_dev_t dev, const g\n             return ggml_is_contiguous(op->src[0]);\n         case GGML_OP_RMS_NORM:\n             return ((op->src[0]->ne[0] % WARP_SIZE) == 0);\n+        case GGML_OP_RMS_NORM_BACK:\n+            return ((op->src[0]->ne[0] % WARP_SIZE) == 0);\n         case GGML_OP_SCALE:\n             return true;\n         case GGML_OP_CONT:"
      },
      {
        "filename": "ggml/src/ggml-sycl/norm.cpp",
        "status": "modified",
        "additions": 156,
        "deletions": 0,
        "changes": 156,
        "patch": "@@ -480,6 +480,162 @@ void ggml_sycl_op_rms_norm(ggml_backend_sycl_context & ctx, ggml_tensor * dst) {\n     rms_norm_f32_sycl(src0_dd, dst_dd, ne00, ne01, ne02, ne03, s01, s02, s03, eps, main_stream, ctx.device);\n }\n \n+void ggml_sycl_op_rms_norm_back(ggml_backend_sycl_context & ctx, ggml_tensor * dst) {\n+    scope_op_debug_print scope_dbg_print(__func__, dst, /*num_src=*/2);\n+\n+    GGML_ASSERT(dst->src[0]->type == GGML_TYPE_F32); // dz\n+    GGML_ASSERT(dst->src[1]->type == GGML_TYPE_F32); // x\n+    GGML_ASSERT(dst->type         == GGML_TYPE_F32);\n+\n+    float eps = 1e-5f;\n+    std::memcpy(&eps, dst->op_params, sizeof(float));\n+    if (!(eps > 0.0f) || !std::isfinite(eps)) eps = 1e-5f;\n+\n+    const float * g_base  = static_cast<const float *>(dst->src[0]->data); // dz\n+    const float * x_base  = static_cast<const float *>(dst->src[1]->data); // x\n+          float * dx_base = static_cast<      float *>(dst->data);\n+\n+    const int64_t D  = dst->ne[0];\n+    const int64_t n1 = dst->ne[1], n2 = dst->ne[2], n3 = dst->ne[3]; (void) n3;\n+    const int64_t N  = ggml_nrows(dst);\n+    if (D == 0 || N == 0) return;\n+\n+    const ggml_tensor *G = dst->src[0];\n+    const ggml_tensor *X = dst->src[1];\n+    const int ts = (int) ggml_type_size(X->type);\n+    GGML_ASSERT((size_t) X->nb[0]   == (size_t) ts);\n+    GGML_ASSERT((size_t) G->nb[0]   == (size_t) ts);\n+    GGML_ASSERT((size_t) dst->nb[0] == (size_t) ts);\n+\n+    const int64_t xs1 = X->nb[1] / ts, xs2 = X->nb[2] / ts, xs3 = X->nb[3] / ts;\n+    const int64_t gs1 = G->nb[1] / ts, gs2 = G->nb[2] / ts, gs3 = G->nb[3] / ts;\n+    const int64_t ds1 = dst->nb[1] / ts, ds2 = dst->nb[2] / ts, ds3 = dst->nb[3] / ts;\n+\n+    dpct::queue_ptr q = ctx.stream();\n+\n+    // work-group size: multiple of WARP_SIZE, capped by device and 256, and not larger than D\n+    const int device_max_wg = ggml_sycl_info().max_work_group_sizes[ctx.device];\n+    auto roundup = [](int v, int m) { return ((v + m - 1) / m) * m; };\n+    int wg_cap = 256;\n+    if (device_max_wg > 0) wg_cap = std::min(wg_cap, device_max_wg);\n+    int WG = std::max(WARP_SIZE, std::min(roundup((int)std::min<int64_t>(D, wg_cap), WARP_SIZE), wg_cap));\n+\n+    // FP32 path: per-thread compensated accumulation + hierarchical reduction\n+    q->submit([&](sycl::handler &cgh) {\n+        const int nwarps_loc = std::max(1, WG / WARP_SIZE);\n+        // store one partial value per warp (xx and xg) for cross-warp reduction\n+        auto l_xx   = sycl::local_accessor<sycl::float2, 1>(sycl::range<1>(nwarps_loc), cgh);\n+        auto l_xg   = sycl::local_accessor<sycl::float2, 1>(sycl::range<1>(nwarps_loc), cgh);\n+\n+        cgh.parallel_for(\n+            sycl::nd_range<3>(sycl::range<3>(1, 1, N) * sycl::range<3>(1, 1, WG),\n+                              sycl::range<3>(1, 1, WG)),\n+            [=](sycl::nd_item<3> item_ct1) [[sycl::reqd_sub_group_size(WARP_SIZE)]] {\n+                const int row = item_ct1.get_group(2);\n+                const int tid = item_ct1.get_local_id(2);\n+\n+                const int64_t i1 = row % n1;\n+                const int64_t i2 = (row / n1) % n2;\n+                const int64_t i3 = row / (n1 * n2);\n+\n+                const float *__restrict x_row = x_base + i3 * xs3 + i2 * xs2 + i1 * xs1;\n+                const float *__restrict g_row = g_base + i3 * gs3 + i2 * gs2 + i1 * gs1;\n+                float *__restrict d_row       = dx_base + i3 * ds3 + i2 * ds2 + i1 * ds1;\n+\n+                // per-thread accumulation (compensated by default)\n+                float sum_xx = 0.f, sum_xg = 0.f;\n+#ifndef GGML_SYCL_RMS_BACK_FAST\n+                float c_xx = 0.f, c_xg = 0.f;\n+#endif\n+                for (int64_t col = tid; col < D; col += WG) {\n+                    const float xv = x_row[col];\n+                    const float gv = g_row[col];\n+#ifdef GGML_SYCL_RMS_BACK_FAST\n+                    sum_xx += xv * xv;\n+                    sum_xg += xv * gv;\n+#else\n+                    float y1 = xv * xv - c_xx;\n+                    float t1 = sum_xx + y1;\n+                    c_xx = (t1 - sum_xx) - y1;\n+                    sum_xx = t1;\n+\n+                    float y2 = xv * gv - c_xg;\n+                    float t2 = sum_xg + y2;\n+                    c_xg = (t2 - sum_xg) - y2;\n+                    sum_xg = t2;\n+#endif\n+                }\n+\n+                // warp-level reduction\n+                sycl::float2 xx = sycl::float2(sum_xx,\n+#ifndef GGML_SYCL_RMS_BACK_FAST\n+                    c_xx\n+#else\n+                    0.f\n+#endif\n+                );\n+                sycl::float2 xg = sycl::float2(sum_xg,\n+#ifndef GGML_SYCL_RMS_BACK_FAST\n+                    c_xg\n+#else\n+                    0.f\n+#endif\n+                );\n+                xx = warp_reduce_sum(xx, item_ct1);\n+                xg = warp_reduce_sum(xg, item_ct1);\n+\n+                // cross-warp reduction using local memory (single barrier)\n+                const auto sub_group = item_ct1.get_sub_group();\n+                const auto sg_id     = sub_group.get_group_linear_id();\n+                const auto wi_in_sg  = sub_group.get_local_linear_id();\n+                const int nthreads   = item_ct1.get_local_range(2);\n+                const int nwarps     = nthreads / WARP_SIZE;\n+\n+                sycl::float2 xx_total = xx;\n+                sycl::float2 xg_total = xg;\n+                if (nwarps > 1) {\n+                    if (wi_in_sg == 0) {\n+                        l_xx[sg_id] = xx;\n+                        l_xg[sg_id] = xg;\n+                    }\n+                    item_ct1.barrier(sycl::access::fence_space::local_space);\n+\n+                    if (sg_id == 0) {\n+                        const unsigned wi_u = wi_in_sg;\n+                        sycl::float2 xx_first = (wi_u < static_cast<unsigned>(nwarps)) ? l_xx[wi_u] : sycl::float2(0.f, 0.f);\n+                        sycl::float2 xg_first = (wi_u < static_cast<unsigned>(nwarps)) ? l_xg[wi_u] : sycl::float2(0.f, 0.f);\n+                        xx_total = warp_reduce_sum(xx_first, item_ct1);\n+                        xg_total = warp_reduce_sum(xg_first, item_ct1);\n+                    } else {\n+                        // other subgroups keep their local totals; they'll be ignored\n+                        xx_total = xx;\n+                        xg_total = xg;\n+                    }\n+                    // ensure all threads see the first-subgroup result via broadcast below\n+                }\n+\n+                // compute inv_r and coeff once per row and broadcast to the whole work-group\n+                float inv_r = 0.f;\n+                float coeff = 0.f;\n+                if (tid == 0) {\n+                    const float sum_xx_f  = xx_total.x() + xx_total.y();\n+                    const float sum_xdz_f = xg_total.x() + xg_total.y();\n+                    const float mean_eps  = sum_xx_f / (float) D + eps;\n+                    const float sum_eps   = sum_xx_f + eps * (float) D;\n+                    inv_r = sycl::rsqrt(mean_eps);\n+                    coeff = -sum_xdz_f / sum_eps;\n+                }\n+                inv_r = sycl::group_broadcast(item_ct1.get_group(), inv_r);\n+                coeff = sycl::group_broadcast(item_ct1.get_group(), coeff);\n+\n+                for (int64_t col = tid; col < D; col += WG) {\n+                    d_row[col] = (g_row[col] + coeff * x_row[col]) * inv_r;\n+                }\n+            });\n+    });\n+\n+}\n+\n void ggml_sycl_op_l2_norm(ggml_backend_sycl_context& ctx, ggml_tensor* dst) {\n \n     GGML_ASSERT(dst->src[0]->type == GGML_TYPE_F32);"
      },
      {
        "filename": "ggml/src/ggml-sycl/norm.hpp",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -19,6 +19,8 @@ void ggml_sycl_op_norm(ggml_backend_sycl_context& ctx, ggml_tensor* dst);\n \n void ggml_sycl_op_rms_norm(ggml_backend_sycl_context& ctx, ggml_tensor* dst);\n \n+void ggml_sycl_op_rms_norm_back(ggml_backend_sycl_context& ctx, ggml_tensor* dst);\n+\n void ggml_sycl_op_group_norm(ggml_backend_sycl_context& ctx, ggml_tensor* dst);\n \n void ggml_sycl_op_l2_norm(ggml_backend_sycl_context& ctx, ggml_tensor* dst);"
      }
    ],
    "num_files": 5,
    "scraped_at": "2025-11-16T23:29:28.517519"
  },
  {
    "pr_number": 16800,
    "title": "sycl: add SSM_CONV operation support",
    "body": "### Summary\r\n\r\nImplements the **SSM_CONV** operator for the SYCL backend, enabling 1D state-space model convolution on SYCL devices (Intel GPUs).  \r\nProvides efficient per-channel sliding-window convolution following the CPU reference.  \r\nThe changes are focused and aligned with existing SYCL backend patterns.\r\n\r\n---\r\n\r\n### Changes\r\n\r\n- Added `SSM_CONV` kernel in `ssm_conv.cpp` implementing 1D causal convolution  \r\n- Integrated `ggml_sycl_ssm_conv()` dispatch in the SYCL backend  \r\n- Added debug log for runtime validation (`[SSM_CONV SYCL]`)\r\n\r\n---\r\n\r\n### Implementation\r\n\r\n- Processes 4D tensor layout `[d_inner, n_t, n_s]` with convolution weights `[d_conv, d_inner]`  \r\n- Performs per-channel dot-product across a sliding input window of size `d_conv`  \r\n- Uses flat 1D `nd_range` kernel where each work-item handles `(channel, token, sequence)`  \r\n- Includes full dimension validation and stride consistency checks  \r\n- Optimized for sequential memory access along the time dimension\r\n\r\n---\r\n\r\n### Testing\r\n\r\n- Verified against CPU reference implementation for identical numerical results  \r\n- Tested with multiple convolution window sizes and sequence lengths  \r\n- Validated correctness for different batch sizes (`n_s`) and inner channels (`d_inner`)  \r\n- Error handling tested for invalid shapes and stride mismatches  \r\n\r\n---\r\n\r\n### Performance\r\n\r\n- Lightweight kernel design using direct global memory access  \r\n- No redundant copies \u2014 data accessed directly from SYCL device memory  \r\n- Straightforward per-thread dot-product; ready for future vectorization  \r\n- Low launch overhead; uses 256-thread work-groups  \r\n\r\n---\r\n\r\n### Compatibility\r\n\r\n- Supports `GGML_TYPE_F32` tensors  \r\n- Compatible with both **OpenCL** and **Level Zero** SYCL backends  \r\n- Matches CPU operator semantics for `SSM_CONV`  \r\n- Prepares ground for future optimization (local memory tiling, `float4` loads, etc.)\r\n",
    "html_url": "https://github.com/ggml-org/llama.cpp/pull/16800",
    "created_at": "2025-10-27T12:05:38Z",
    "merged_at": "2025-10-28T01:50:34Z",
    "merge_commit_sha": "ad8d36beffd791db10c94eb9e964afb891e3ca55",
    "base_ref": "master",
    "head_sha": "5f15b8f7432f1b7d300ecfbc6edeef6665ea4da1",
    "user": "tamarPal",
    "files": [
      {
        "filename": "ggml/src/ggml-sycl/backend.hpp",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -35,6 +35,7 @@\n #include \"roll.hpp\"\n #include \"rope.hpp\"\n #include \"set_rows.hpp\"\n+#include \"ssm_conv.hpp\"\n #include \"softmax.hpp\"\n #include \"tsembd.hpp\"\n #include \"wkv.hpp\""
      },
      {
        "filename": "ggml/src/ggml-sycl/ggml-sycl.cpp",
        "status": "modified",
        "additions": 7,
        "deletions": 0,
        "changes": 7,
        "patch": "@@ -50,6 +50,7 @@\n #include \"ggml-sycl/getrows.hpp\"\n #include \"ggml-sycl/repeat_back.hpp\"\n #include \"ggml-sycl/quantize.hpp\"\n+#include \"ggml-sycl/ssm_conv.hpp\"\n #include \"ggml.h\"\n \n static bool g_sycl_loaded = false;\n@@ -3921,6 +3922,8 @@ static bool ggml_sycl_compute_forward(ggml_backend_sycl_context & ctx, struct gg\n         case GGML_OP_GATED_LINEAR_ATTN:\n             ggml_sycl_op_gated_linear_attn(ctx, dst);\n             break;\n+        case GGML_OP_SSM_CONV:\n+            ggml_sycl_ssm_conv(ctx, dst);\n         case GGML_OP_ROLL:\n             ggml_sycl_roll(ctx, dst);\n             break;\n@@ -4602,6 +4605,10 @@ static bool ggml_backend_sycl_device_supports_op(ggml_backend_dev_t dev, const g\n         case GGML_OP_RWKV_WKV7:\n         case GGML_OP_GATED_LINEAR_ATTN:\n             return true;\n+        case GGML_OP_SSM_CONV:\n+            return op->type == GGML_TYPE_F32 &&\n+                   op->src[0]->type == GGML_TYPE_F32 &&\n+                   op->src[1]->type == GGML_TYPE_F32;\n         case GGML_OP_ROLL:\n             return op->type == GGML_TYPE_F32;\n         case GGML_OP_ARANGE:"
      },
      {
        "filename": "ggml/src/ggml-sycl/ssm_conv.cpp",
        "status": "added",
        "additions": 127,
        "deletions": 0,
        "changes": 127,
        "patch": "@@ -0,0 +1,127 @@\n+#include \"ssm_conv.hpp\"\n+#include \"common.hpp\"\n+\n+#include <cstdio>\n+\n+using namespace sycl;\n+\n+static void kernel_ssm_conv(\n+    queue &q,\n+    const float *src_data,\n+    const float *weights,\n+    float *dst_data,\n+    int d_conv,\n+    int d_inner,\n+    int n_t,\n+    int n_s,\n+    int ncs __attribute__((unused)),\n+    int src_stride_inner,\n+    int src_stride_seq,\n+    int dst_stride_token,\n+    int dst_stride_seq\n+) {\n+    const size_t total_work = static_cast<size_t>(d_inner) * static_cast<size_t>(n_t) * static_cast<size_t>(n_s);\n+    const size_t work_group_size = 256;\n+    const size_t num_work_groups = (total_work + work_group_size - 1) / work_group_size;\n+\n+    const range<1> global_range(num_work_groups * work_group_size);\n+    const range<1> local_range(work_group_size);\n+\n+    q.submit([&](handler &h) {\n+        h.parallel_for(\n+            nd_range<1>(global_range, local_range),\n+            [=](nd_item<1> item) {\n+                const size_t idx = item.get_global_id(0);\n+                if (idx >= total_work) {\n+                    return;\n+                }\n+\n+                const int channel = static_cast<int>(idx % d_inner);\n+                const int token   = static_cast<int>((idx / d_inner) % n_t);\n+                const int seq     = static_cast<int>(idx / (static_cast<size_t>(d_inner) * static_cast<size_t>(n_t)));\n+\n+                const float *s = src_data\n+                    + static_cast<size_t>(seq) * static_cast<size_t>(src_stride_seq)\n+                    + static_cast<size_t>(channel) * static_cast<size_t>(src_stride_inner)\n+                    + static_cast<size_t>(token);\n+\n+                const float *c = weights + static_cast<size_t>(channel) * static_cast<size_t>(d_conv);\n+\n+                float sumf = 0.0f;\n+                for (int i0 = 0; i0 < d_conv; ++i0) {\n+                    sumf += s[i0] * c[i0];\n+                }\n+\n+                const size_t dst_idx =\n+                    static_cast<size_t>(seq) * static_cast<size_t>(dst_stride_seq) +\n+                    static_cast<size_t>(token) * static_cast<size_t>(dst_stride_token) +\n+                    static_cast<size_t>(channel);\n+\n+                dst_data[dst_idx] = sumf;\n+            }\n+        );\n+    });\n+}\n+\n+void ggml_sycl_ssm_conv(ggml_backend_sycl_context & ctx, ggml_tensor * dst) {\n+    ggml_tensor * src0 = dst->src[0];\n+    ggml_tensor * src1 = dst->src[1];\n+\n+    GGML_ASSERT(src0->type == GGML_TYPE_F32);\n+    GGML_ASSERT(src1->type == GGML_TYPE_F32);\n+    GGML_ASSERT(dst->type  == GGML_TYPE_F32);\n+\n+    const int d_conv   = src1->ne[0];\n+    const int ncs      = src0->ne[0];\n+    const int d_inner  = src0->ne[1];\n+    const int n_t      = dst->ne[1];\n+    const int n_s      = dst->ne[2];\n+\n+    GGML_ASSERT(src0->ne[0] == d_conv - 1 + n_t);\n+    GGML_ASSERT(src0->ne[1] == d_inner);\n+    GGML_ASSERT(src1->ne[1] == d_inner);\n+\n+    GGML_ASSERT(dst->ne[0] == d_inner);\n+    GGML_ASSERT(dst->ne[1] == n_t);\n+    GGML_ASSERT(dst->ne[2] == n_s);\n+\n+    GGML_ASSERT(src0->nb[0] == sizeof(float));\n+    GGML_ASSERT(src1->nb[0] == sizeof(float));\n+\n+    GGML_ASSERT(src0->nb[1] == src0->ne[0] * static_cast<int>(sizeof(float)));\n+\n+    const int src_stride_inner = ncs;\n+    const int src_stride_seq   = ncs * d_inner;\n+    const int dst_stride_token = d_inner;\n+    const int dst_stride_seq   = d_inner * n_t;\n+\n+    try {\n+        queue *q = ctx.stream();\n+\n+        const float *src_data = static_cast<const float *>(src0->data);\n+        const float *weights  = static_cast<const float *>(src1->data);\n+        float *dst_data       = static_cast<float *>(dst->data);\n+\n+        GGML_ASSERT(src_data && weights && dst_data);\n+\n+        kernel_ssm_conv(\n+            *q,\n+            src_data,\n+            weights,\n+            dst_data,\n+            d_conv,\n+            d_inner,\n+            n_t,\n+            n_s,\n+            ncs,\n+            src_stride_inner,\n+            src_stride_seq,\n+            dst_stride_token,\n+            dst_stride_seq\n+        );\n+\n+    } catch (const std::exception &e) {\n+        std::fprintf(stderr, \"[SYCL-SSM_CONV] ERROR: %s\\n\", e.what());\n+        throw;\n+    }\n+}"
      },
      {
        "filename": "ggml/src/ggml-sycl/ssm_conv.hpp",
        "status": "added",
        "additions": 5,
        "deletions": 0,
        "changes": 5,
        "patch": "@@ -0,0 +1,5 @@\n+#pragma once\n+\n+#include \"common.hpp\"\n+\n+void ggml_sycl_ssm_conv(ggml_backend_sycl_context & ctx, ggml_tensor * dst);"
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T23:29:30.222394"
  },
  {
    "pr_number": 16792,
    "title": "grammar : support array references in json schema",
    "body": "fixes #16790\r\n\r\nrelated:  https://github.com/ggml-org/llama.cpp/issues/16714#issuecomment-3431179568.\r\n\r\nThe JSON schema to grammar conversion does not support referencing array items. It appears zod or the MCP library may do this to reuse schemas instead of creating a separate definition.\r\n\r\n<details>\r\n<summary>Example</summary>\r\n\r\n```bash\r\ncurl http://localhost:8080/v1/chat/completions -d '{\r\n  \"model\": \"gpt-oss-20b\",\r\n  \"messages\": [\r\n    {\r\n      \"role\": \"user\",\r\n      \"content\": \"Build a binary tree that matches (A (B C D) E)\"\r\n    }\r\n  ],\r\n  \"tools\": [\r\n    {\r\n      \"type\": \"function\",\r\n      \"function\": {\r\n        \"name\": \"build_binary_tree\",\r\n        \"description\": \"build a binary tree. The left/right fields may be either a string value or another tree.\",\r\n        \"parameters\": {\r\n          \"type\": \"object\",\r\n          \"properties\": {\r\n            \"tree\": {\r\n              \"type\": \"object\",\r\n              \"properties\": {\r\n                \"left\": {\r\n                  \"anyOf\": [\r\n                    {\r\n                      \"type\": \"string\"\r\n                    },\r\n                    {\r\n                      \"$ref\": \"#/properties/tree\"\r\n                    }\r\n                  ]\r\n                },\r\n                \"right\": {\r\n                  \"anyOf\": [\r\n                    {\r\n                      \"$ref\": \"#/properties/tree/properties/left/anyOf/0\"\r\n                    },\r\n                    {\r\n                      \"$ref\": \"#/properties/tree\"\r\n                    }\r\n                  ]\r\n                }\r\n              },\r\n              \"additionalProperties\": false\r\n            }\r\n          },\r\n          \"required\": [\r\n            \"tree\"\r\n          ],\r\n          \"additionalProperties\": false,\r\n          \"$schema\": \"http://json-schema.org/draft-07/schema#\"\r\n        }\r\n      }\r\n    }\r\n  ],\r\n  \"tool_choice\": \"auto\"\r\n}'\r\n```\r\n</details>\r\n\r\nThe example above will fail with:\r\n\r\n```json\r\n{\r\n  \"error\": {\r\n    \"code\": 500,\r\n    \"message\": \"JSON schema conversion failed:\\nError resolving ref #/properties/tree/properties/left/anyOf/0: 0 not in [{\\\"type\\\":\\\"string\\\"},{\\\"$ref\\\":\\\"#/properties/tree\\\"}]\",\r\n    \"type\": \"server_error\"\r\n  }\r\n}\r\n```\r\n\r\nThis PR adds support for referencing array items.\r\n\r\nSince indexes are less unique than object keys, it also renames the grammar rules derived from references.\r\n\r\nCurrently, the rules are named after the last component of the reference. E.g. `#/properties/tree => tree`. That doesn't work well with indexes, so instead this PR names them as follows:\r\n\r\n1. Extract the content after `#`.\r\n2. Replace non-alphanumeric characters with a dash `-`.\r\n3. Prefix with `ref`\r\n\r\nThis results in `#/properties/tree => ref-properties-tree` as the grammar rule name.\r\n\r\n@ochafik I would like your opinion on the rule naming. I don't know if there is any additional impact I am not seeing.\r\n\r\nHere is the same example against this PR:\r\n\r\n```json\r\n{\r\n  \"model\": \"unsloth/gpt-oss-20b\",\r\n  \"choices\": [\r\n    {\r\n      \"finish_reason\": \"tool_calls\",\r\n      \"index\": 0,\r\n      \"message\": {\r\n        \"role\": \"assistant\",\r\n        \"reasoning_content\": \"We need to use the function build_binary_tree. The f...\",\r\n        \"content\": null,\r\n        \"tool_calls\": [\r\n          {\r\n            \"type\": \"function\",\r\n            \"function\": {\r\n              \"name\": \"build_binary_tree\",\r\n              \"arguments\": \"{\\\"tree\\\":{\\\"left\\\":{\\\"left\\\":\\\"C\\\",\\\"right\\\":\\\"D\\\"},\\\"right\\\":\\\"E\\\"}}\"\r\n            },\r\n            \"id\": \"NHd0M0zGWTx1XG6DN2NalUqMT0b8GVb3\"\r\n          }\r\n        ]\r\n      }\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\nAnd the grammar rules generated:\r\n\r\n```\r\nbuild-binary-tree-args ::= \"{\" space build-binary-tree-args-tree-kv \"}\" space\r\nbuild-binary-tree-args-tree ::= \"{\" space  (build-binary-tree-args-tree-left-kv build-binary-tree-args-tree-left-rest | build-binary-tree-args-tree-right-kv )? \"}\" space\r\nbuild-binary-tree-args-tree-kv ::= \"\\\"tree\\\"\" space \":\" space build-binary-tree-args-tree\r\nbuild-binary-tree-args-tree-left ::= string | build-binary-tree-args-tree-left-1\r\nbuild-binary-tree-args-tree-left-1 ::= ref-properties-tree\r\nbuild-binary-tree-args-tree-left-kv ::= \"\\\"left\\\"\" space \":\" space build-binary-tree-args-tree-left\r\nbuild-binary-tree-args-tree-left-rest ::= ( \",\" space build-binary-tree-args-tree-right-kv )?\r\nbuild-binary-tree-args-tree-right ::= build-binary-tree-args-tree-right-0 | build-binary-tree-args-tree-right-1\r\nbuild-binary-tree-args-tree-right-0 ::= string\r\nbuild-binary-tree-args-tree-right-1 ::= ref-properties-tree\r\nbuild-binary-tree-args-tree-right-kv ::= \"\\\"right\\\"\" space \":\" space build-binary-tree-args-tree-right\r\nbuild-binary-tree-call ::= \"build_binary_tree\"channel \" <|constrain|>json\"? \"<|message|>\" build-binary-tree-args\r\nbuild-binary-tree-call0 ::= \"build_binary_tree\" \" <|constrain|>json\"? \"<|message|>\" build-binary-tree-args\r\nchannel ::= \"<|channel|>\" ( \"commentary\" | \"analysis\" )\r\nchar ::= [^\"\\\\\\x7F\\x00-\\x1F] | [\\\\] ([\"\\\\bfnrt] | \"u\" [0-9a-fA-F]{4})\r\nrecipient-in-channel ::= channel \" to=functions.\" ( build-binary-tree-call0 )\r\nrecipient-in-role ::= \"<|start|>assistant\"? \" to=functions.\" ( build-binary-tree-call )\r\nref-properties-tree ::= \"{\" space  (ref-properties-tree-left-kv ref-properties-tree-left-rest | ref-properties-tree-right-kv )? \"}\" space\r\nref-properties-tree-left ::= string | ref-properties-tree-left-1\r\nref-properties-tree-left-1 ::= ref-properties-tree\r\nref-properties-tree-left-kv ::= \"\\\"left\\\"\" space \":\" space ref-properties-tree-left\r\nref-properties-tree-left-rest ::= ( \",\" space ref-properties-tree-right-kv )?\r\nref-properties-tree-right ::= ref-properties-tree-right-0 | ref-properties-tree-right-1\r\nref-properties-tree-right-0 ::= string\r\nref-properties-tree-right-1 ::= ref-properties-tree\r\nref-properties-tree-right-kv ::= \"\\\"right\\\"\" space \":\" space ref-properties-tree-right\r\nroot ::= recipient-in-role | recipient-in-channel\r\nspace ::= | \" \" | \"\\n\"{1,2} [ \\t]{0,20}\r\nstring ::= \"\\\"\" char* \"\\\"\" space\r\n```",
    "html_url": "https://github.com/ggml-org/llama.cpp/pull/16792",
    "created_at": "2025-10-26T20:37:05Z",
    "merged_at": "2025-10-28T08:37:52Z",
    "merge_commit_sha": "280d97be9660e7a5feaa28a6e7a299bc73dd83fc",
    "base_ref": "master",
    "head_sha": "927e069d54706ec82fcb9d5b8279949f49872ed2",
    "user": "aldehir",
    "files": [
      {
        "filename": "common/json-schema-to-grammar.cpp",
        "status": "modified",
        "additions": 19,
        "deletions": 3,
        "changes": 22,
        "patch": "@@ -601,7 +601,10 @@ class SchemaConverter {\n     }\n \n     std::string _resolve_ref(const std::string & ref) {\n-        std::string ref_name = ref.substr(ref.find_last_of('/') + 1);\n+        auto it = ref.find('#');\n+        std::string ref_fragment = it != std::string::npos ? ref.substr(it + 1) : ref;\n+        static const std::regex nonalphanumeric_regex(R\"([^a-zA-Z0-9-]+)\");\n+        std::string ref_name = \"ref\" + std::regex_replace(ref_fragment, nonalphanumeric_regex, \"-\");\n         if (_rules.find(ref_name) == _rules.end() && _refs_being_resolved.find(ref) == _refs_being_resolved.end()) {\n             _refs_being_resolved.insert(ref);\n             json resolved = _refs[ref];\n@@ -774,11 +777,24 @@ class SchemaConverter {\n                         std::vector<std::string> tokens = string_split(pointer, \"/\");\n                         for (size_t i = 1; i < tokens.size(); ++i) {\n                             std::string sel = tokens[i];\n-                            if (target.is_null() || !target.contains(sel)) {\n+                            if (target.is_object() && target.contains(sel)) {\n+                                target = target[sel];\n+                            } else if (target.is_array()) {\n+                                size_t sel_index;\n+                                try {\n+                                    sel_index = std::stoul(sel);\n+                                } catch (const std::invalid_argument & e) {\n+                                    sel_index = target.size();\n+                                }\n+                                if (sel_index >= target.size()) {\n+                                    _errors.push_back(\"Error resolving ref \" + ref + \": \" + sel + \" not in \" + target.dump());\n+                                    return;\n+                                }\n+                                target = target[sel_index];\n+                            } else {\n                                 _errors.push_back(\"Error resolving ref \" + ref + \": \" + sel + \" not in \" + target.dump());\n                                 return;\n                             }\n-                            target = target[sel];\n                         }\n                         _refs[ref] = target;\n                     }"
      },
      {
        "filename": "examples/json_schema_to_grammar.py",
        "status": "modified",
        "additions": 13,
        "deletions": 3,
        "changes": 16,
        "patch": "@@ -371,8 +371,17 @@ def visit(n: dict):\n                         raise ValueError(f'Unsupported ref {ref}')\n \n                     for sel in ref.split('#')[-1].split('/')[1:]:\n-                        assert target is not None and sel in target, f'Error resolving ref {ref}: {sel} not in {target}'\n-                        target = target[sel]\n+                        assert target is not None, f'Error resolving ref {ref}: {sel} not in {target}'\n+                        if isinstance(target, list):\n+                            try:\n+                                sel_index = int(sel)\n+                            except ValueError:\n+                                raise ValueError(f'Error resolving ref {ref}: {sel} not in {target}')\n+                            assert 0 <= sel_index < len(target), f'Error resolving ref {ref}: {sel} not in {target}'\n+                            target = target[sel_index]\n+                        else:\n+                            assert sel in target, f'Error resolving ref {ref}: {sel} not in {target}'\n+                            target = target[sel]\n \n                     self._refs[ref] = target\n                 else:\n@@ -547,7 +556,8 @@ def join_seq():\n \n \n     def _resolve_ref(self, ref):\n-        ref_name = ref.split('/')[-1]\n+        ref_fragment = ref.split('#')[-1]\n+        ref_name = 'ref' + re.sub(r'[^a-zA-Z0-9-]+', '-', ref_fragment)\n         if ref_name not in self._rules and ref not in self._refs_being_resolved:\n             self._refs_being_resolved.add(ref)\n             resolved = self._refs[ref]"
      },
      {
        "filename": "tests/test-json-schema-to-grammar.cpp",
        "status": "modified",
        "additions": 47,
        "deletions": 9,
        "changes": 56,
        "patch": "@@ -1124,9 +1124,9 @@ static void test_all(const std::string & lang, std::function<void(const TestCase\n         })\"\"\",\n         R\"\"\"(\n             char ::= [^\"\\\\\\x7F\\x00-\\x1F] | [\\\\] ([\"\\\\bfnrt] | \"u\" [0-9a-fA-F]{4})\n-            foo ::= \"{\" space foo-a-kv \"}\" space\n-            foo-a-kv ::= \"\\\"a\\\"\" space \":\" space string\n-            root ::= foo\n+            ref-definitions-foo ::= \"{\" space ref-definitions-foo-a-kv \"}\" space\n+            ref-definitions-foo-a-kv ::= \"\\\"a\\\"\" space \":\" space string\n+            root ::= ref-definitions-foo\n             space ::= | \" \" | \"\\n\"{1,2} [ \\t]{0,20}\n             string ::= \"\\\"\" char* \"\\\"\" space\n         )\"\"\"\n@@ -1151,20 +1151,58 @@ static void test_all(const std::string & lang, std::function<void(const TestCase\n             \"type\": \"object\"\n         })\"\"\",\n         R\"\"\"(\n-            alternative-0 ::= foo\n-            alternative-1 ::= bar\n-            bar ::= \"{\" space  (bar-b-kv )? \"}\" space\n-            bar-b-kv ::= \"\\\"b\\\"\" space \":\" space number\n+            alternative-0 ::= ref-definitions-foo\n+            alternative-1 ::= ref-definitions-bar\n             decimal-part ::= [0-9]{1,16}\n-            foo ::= \"{\" space  (foo-a-kv )? \"}\" space\n-            foo-a-kv ::= \"\\\"a\\\"\" space \":\" space number\n             integral-part ::= [0] | [1-9] [0-9]{0,15}\n             number ::= (\"-\"? integral-part) (\".\" decimal-part)? ([eE] [-+]? integral-part)? space\n+            ref-definitions-bar ::= \"{\" space  (ref-definitions-bar-b-kv )? \"}\" space\n+            ref-definitions-bar-b-kv ::= \"\\\"b\\\"\" space \":\" space number\n+            ref-definitions-foo ::= \"{\" space  (ref-definitions-foo-a-kv )? \"}\" space\n+            ref-definitions-foo-a-kv ::= \"\\\"a\\\"\" space \":\" space number\n             root ::= alternative-0 | alternative-1\n             space ::= | \" \" | \"\\n\"{1,2} [ \\t]{0,20}\n         )\"\"\"\n     });\n \n+    test({\n+        SUCCESS,\n+        \"anyOf $ref\",\n+        R\"\"\"({\n+            \"properties\": {\n+                \"a\": {\n+                    \"anyOf\": [\n+                        {\"type\": \"string\"},\n+                        {\"type\": \"number\"}\n+                    ]\n+                },\n+                \"b\": {\n+                    \"anyOf\": [\n+                        {\"$ref\": \"#/properties/a/anyOf/0\"},\n+                        {\"type\": \"boolean\"}\n+                    ]\n+                }\n+            },\n+            \"type\": \"object\"\n+        })\"\"\",\n+        R\"\"\"(\n+            a ::= string | number\n+            a-kv ::= \"\\\"a\\\"\" space \":\" space a\n+            a-rest ::= ( \",\" space b-kv )?\n+            b ::= b-0 | boolean\n+            b-0 ::= string\n+            b-kv ::= \"\\\"b\\\"\" space \":\" space b\n+            boolean ::= (\"true\" | \"false\") space\n+            char ::= [^\"\\\\\\x7F\\x00-\\x1F] | [\\\\] ([\"\\\\bfnrt] | \"u\" [0-9a-fA-F]{4})\n+            decimal-part ::= [0-9]{1,16}\n+            integral-part ::= [0] | [1-9] [0-9]{0,15}\n+            number ::= (\"-\"? integral-part) (\".\" decimal-part)? ([eE] [-+]? integral-part)? space\n+            root ::= \"{\" space  (a-kv a-rest | b-kv )? \"}\" space\n+            space ::= | \" \" | \"\\n\"{1,2} [ \\t]{0,20}\n+            string ::= \"\\\"\" char* \"\\\"\" space\n+        )\"\"\"\n+    });\n+\n     test({\n         SUCCESS,\n         \"mix of allOf, anyOf and $ref (similar to https://json.schemastore.org/tsconfig.json)\","
      },
      {
        "filename": "tools/server/public_legacy/json-schema-to-grammar.mjs",
        "status": "modified",
        "additions": 8,
        "deletions": 3,
        "changes": 11,
        "patch": "@@ -345,10 +345,14 @@ export class SchemaConverter {\n \n           const selectors = ref.split('#')[1].split('/').slice(1);\n           for (const sel of selectors) {\n-            if (!target || !(sel in target)) {\n+            const selIndex = parseInt(sel, 10);\n+            if (target && sel in target) {\n+              target = target[sel];\n+            } else if (target && selIndex in target) {\n+              target = target[selIndex];\n+            } else {\n               throw new Error(`Error resolving ref ${ref}: ${sel} not in ${JSON.stringify(target)}`);\n             }\n-            target = target[sel];\n           }\n \n           this._refs[ref] = target;\n@@ -594,7 +598,8 @@ export class SchemaConverter {\n   }\n \n   _resolveRef(ref) {\n-    let refName = ref.split('/').pop();\n+    let refFragment = ref.split('#').pop();\n+    let refName = 'ref' + refFragment.replace(/[^a-zA-Z0-9-]+/g, '-');\n     if (!(refName in this._rules) && !this._refsBeingResolved.has(ref)) {\n       this._refsBeingResolved.add(ref);\n       const resolved = this._refs[ref];"
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T23:29:31.013317"
  },
  {
    "pr_number": 16784,
    "title": "webui: auto-refresh /props on inference start to resync model metadata",
    "body": "- Add no-cache headers to /props and /slots\r\n- Throttle slot checks to 30s\r\n- Prevent concurrent fetches with promise guard\r\n- Trigger refresh from chat streaming for legacy and ModelSelector\r\n- Show dynamic serverWarning when using cached data\r\n\r\n- Updated assistant message bubbles to show each message's stored model when available,\r\n  falling back to the current server model only when the per-message value is missing\r\n\r\n- When the model selector is disabled, now fetches /props and prioritizes that model name\r\n  over chunk metadata, then persists it with the streamed message so legacy mode properly\r\n  reflects the backend configuration\r\n\r\nCmdline used on legacy (Raspberry Pi 5) :\r\n```\r\n/root/llama.cpp/build/bin/llama-server \\\r\n -m /root/ia/models/mradermacher/OLMoE-1B-7B-0125-Instruct-i1-GGUF/OLMoE-1B-7B-0125-Instruct.i1-Q6_K.gguf \\\r\n -ctk q8_0 -ctv q8_0 -fa on \\\r\n --jinja --ctx-size 8192 --mlock --port 8081\r\n\r\n/root/llama.cpp/build/bin/llama-server \\\r\n -m /root/ia/models/mradermacher/OLMoE-1B-7B-0125-SFT-i1-GGUF/OLMoE-1B-7B-0125-SFT.i1-Q6_K.gguf \\\r\n -ctk q8_0 -ctv q8_0 -fa on \\\r\n --jinja --ctx-size 8192 --mlock --port 8081\r\n\r\n/root/llama.cpp/build/bin/llama-server \\\r\n -m /root/ia/models/mradermacher/Qwen3-30B-A3B-Instruct-2507-i1-GGUF/Qwen3-30B-A3B-Instruct-2507.i1-Q4_K_M.gguf \\\r\n --temp 0.7 --top-p 0.8 --top-k 20 --min-p 0 \\\r\n -ctk q8_0 -ctv q8_0 -fa on \\\r\n --jinja --ctx-size 4096 --port 8081\r\n```\r\n\r\nFixes https://github.com/ggml-org/llama.cpp/issues/16771\r\n\r\nEDIT : I've recorded a video that specifically targets the original issue.\r\nTesting video, Raspberry Pi 5 + master branch + this PR :\r\n\r\nhttps://github.com/user-attachments/assets/c91e875e-aede-4323-be94-0ee79e6e3d6b\r\n\r\nAnd another one to show there's no regression when the model selector is enabled,\r\nalso demonstrating the multimodal function updates:\r\n\r\nhttps://github.com/user-attachments/assets/bcb1bf46-4b7e-4213-999f-6c60b79918a9\r\n\r\n",
    "html_url": "https://github.com/ggml-org/llama.cpp/pull/16784",
    "created_at": "2025-10-26T15:01:49Z",
    "merged_at": "2025-11-01T18:49:51Z",
    "merge_commit_sha": "2f68ce7cfd20e9e7098514bf730e5389b7bba908",
    "base_ref": "master",
    "head_sha": "d2af7f02ec4cb5f93dee088658ec47e0fd302100",
    "user": "ServeurpersoCom",
    "files": [
      {
        "filename": "tools/server/public/index.html.gz",
        "status": "modified",
        "additions": 0,
        "deletions": 0,
        "changes": 0,
        "patch": ""
      },
      {
        "filename": "tools/server/webui/src/lib/components/app/chat/ChatMessages/ChatMessageAssistant.svelte",
        "status": "modified",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "patch": "@@ -85,8 +85,8 @@\n \tlet displayedModel = $derived((): string | null => {\n \t\tif (!currentConfig.showModelInfo) return null;\n \n-\t\tif (currentConfig.modelSelectorEnabled) {\n-\t\t\treturn message.model ?? null;\n+\t\tif (message.model) {\n+\t\t\treturn message.model;\n \t\t}\n \n \t\treturn serverModel;"
      },
      {
        "filename": "tools/server/webui/src/lib/services/chat.ts",
        "status": "modified",
        "additions": 16,
        "deletions": 4,
        "changes": 20,
        "patch": "@@ -54,6 +54,7 @@ export class ChatService {\n \t\t\tonError,\n \t\t\tonReasoningChunk,\n \t\t\tonModel,\n+\t\t\tonFirstValidChunk,\n \t\t\t// Generation parameters\n \t\t\ttemperature,\n \t\t\tmax_tokens,\n@@ -201,6 +202,7 @@ export class ChatService {\n \t\t\t\t\tonError,\n \t\t\t\t\tonReasoningChunk,\n \t\t\t\t\tonModel,\n+\t\t\t\t\tonFirstValidChunk,\n \t\t\t\t\tconversationId,\n \t\t\t\t\tabortController.signal\n \t\t\t\t);\n@@ -267,6 +269,7 @@ export class ChatService {\n \t\tonError?: (error: Error) => void,\n \t\tonReasoningChunk?: (chunk: string) => void,\n \t\tonModel?: (model: string) => void,\n+\t\tonFirstValidChunk?: () => void,\n \t\tconversationId?: string,\n \t\tabortSignal?: AbortSignal\n \t): Promise<void> {\n@@ -283,6 +286,7 @@ export class ChatService {\n \t\tlet lastTimings: ChatMessageTimings | undefined;\n \t\tlet streamFinished = false;\n \t\tlet modelEmitted = false;\n+\t\tlet firstValidChunkEmitted = false;\n \n \t\ttry {\n \t\t\tlet chunk = '';\n@@ -311,17 +315,25 @@ export class ChatService {\n \t\t\t\t\t\ttry {\n \t\t\t\t\t\t\tconst parsed: ApiChatCompletionStreamChunk = JSON.parse(data);\n \n-\t\t\t\t\t\t\tconst chunkModel = this.extractModelName(parsed);\n-\t\t\t\t\t\t\tif (chunkModel && !modelEmitted) {\n-\t\t\t\t\t\t\t\tmodelEmitted = true;\n-\t\t\t\t\t\t\t\tonModel?.(chunkModel);\n+\t\t\t\t\t\t\tif (!firstValidChunkEmitted && parsed.object === 'chat.completion.chunk') {\n+\t\t\t\t\t\t\t\tfirstValidChunkEmitted = true;\n+\n+\t\t\t\t\t\t\t\tif (!abortSignal?.aborted) {\n+\t\t\t\t\t\t\t\t\tonFirstValidChunk?.();\n+\t\t\t\t\t\t\t\t}\n \t\t\t\t\t\t\t}\n \n \t\t\t\t\t\t\tconst content = parsed.choices[0]?.delta?.content;\n \t\t\t\t\t\t\tconst reasoningContent = parsed.choices[0]?.delta?.reasoning_content;\n \t\t\t\t\t\t\tconst timings = parsed.timings;\n \t\t\t\t\t\t\tconst promptProgress = parsed.prompt_progress;\n \n+\t\t\t\t\t\t\tconst chunkModel = this.extractModelName(parsed);\n+\t\t\t\t\t\t\tif (chunkModel && !modelEmitted) {\n+\t\t\t\t\t\t\t\tmodelEmitted = true;\n+\t\t\t\t\t\t\t\tonModel?.(chunkModel);\n+\t\t\t\t\t\t\t}\n+\n \t\t\t\t\t\t\tif (timings || promptProgress) {\n \t\t\t\t\t\t\t\tthis.updateProcessingState(timings, promptProgress, conversationId);\n \t\t\t\t\t\t\t\tif (timings) {"
      },
      {
        "filename": "tools/server/webui/src/lib/stores/chat.svelte.ts",
        "status": "modified",
        "additions": 52,
        "deletions": 2,
        "changes": 54,
        "patch": "@@ -1,6 +1,7 @@\n import { DatabaseStore } from '$lib/stores/database';\n import { chatService, slotsService } from '$lib/services';\n import { config } from '$lib/stores/settings.svelte';\n+import { serverStore } from '$lib/stores/server.svelte';\n import { normalizeModelName } from '$lib/utils/model-names';\n import { filterByLeafNodeId, findLeafNode, findDescendantMessages } from '$lib/utils/branching';\n import { browser } from '$app/environment';\n@@ -362,9 +363,41 @@ class ChatStore {\n \n \t\tlet resolvedModel: string | null = null;\n \t\tlet modelPersisted = false;\n+\t\tconst currentConfig = config();\n+\t\tconst preferServerPropsModel = !currentConfig.modelSelectorEnabled;\n+\t\tlet serverPropsRefreshed = false;\n+\t\tlet updateModelFromServerProps: ((persistImmediately?: boolean) => void) | null = null;\n+\n+\t\tconst refreshServerPropsOnce = () => {\n+\t\t\tif (serverPropsRefreshed) {\n+\t\t\t\treturn;\n+\t\t\t}\n+\n+\t\t\tserverPropsRefreshed = true;\n+\n+\t\t\tconst hasExistingProps = serverStore.serverProps !== null;\n \n-\t\tconst recordModel = (modelName: string, persistImmediately = true): void => {\n-\t\t\tconst normalizedModel = normalizeModelName(modelName);\n+\t\t\tserverStore\n+\t\t\t\t.fetchServerProps({ silent: hasExistingProps })\n+\t\t\t\t.then(() => {\n+\t\t\t\t\tupdateModelFromServerProps?.(true);\n+\t\t\t\t})\n+\t\t\t\t.catch((error) => {\n+\t\t\t\t\tconsole.warn('Failed to refresh server props after streaming started:', error);\n+\t\t\t\t});\n+\t\t};\n+\n+\t\tconst recordModel = (modelName: string | null | undefined, persistImmediately = true): void => {\n+\t\t\tconst serverModelName = serverStore.modelName;\n+\t\t\tconst preferredModelSource = preferServerPropsModel\n+\t\t\t\t? (serverModelName ?? modelName ?? null)\n+\t\t\t\t: (modelName ?? serverModelName ?? null);\n+\n+\t\t\tif (!preferredModelSource) {\n+\t\t\t\treturn;\n+\t\t\t}\n+\n+\t\t\tconst normalizedModel = normalizeModelName(preferredModelSource);\n \n \t\t\tif (!normalizedModel || normalizedModel === resolvedModel) {\n \t\t\t\treturn;\n@@ -388,6 +421,20 @@ class ChatStore {\n \t\t\t}\n \t\t};\n \n+\t\tif (preferServerPropsModel) {\n+\t\t\tupdateModelFromServerProps = (persistImmediately = true) => {\n+\t\t\t\tconst currentServerModel = serverStore.modelName;\n+\n+\t\t\t\tif (!currentServerModel) {\n+\t\t\t\t\treturn;\n+\t\t\t\t}\n+\n+\t\t\t\trecordModel(currentServerModel, persistImmediately);\n+\t\t\t};\n+\n+\t\t\tupdateModelFromServerProps(false);\n+\t\t}\n+\n \t\tslotsService.startStreaming();\n \t\tslotsService.setActiveConversation(assistantMessage.convId);\n \n@@ -396,6 +443,9 @@ class ChatStore {\n \t\t\t{\n \t\t\t\t...this.getApiOptions(),\n \n+\t\t\t\tonFirstValidChunk: () => {\n+\t\t\t\t\trefreshServerPropsOnce();\n+\t\t\t\t},\n \t\t\t\tonChunk: (chunk: string) => {\n \t\t\t\t\tstreamedContent += chunk;\n \t\t\t\t\tthis.setConversationStreaming("
      },
      {
        "filename": "tools/server/webui/src/lib/stores/server.svelte.ts",
        "status": "modified",
        "additions": 110,
        "deletions": 64,
        "changes": 174,
        "patch": "@@ -52,6 +52,7 @@ class ServerStore {\n \tprivate _error = $state<string | null>(null);\n \tprivate _serverWarning = $state<string | null>(null);\n \tprivate _slotsEndpointAvailable = $state<boolean | null>(null);\n+\tprivate fetchServerPropsPromise: Promise<void> | null = null;\n \n \tprivate readCachedServerProps(): ApiLlamaCppServerProps | null {\n \t\tif (!browser) return null;\n@@ -171,88 +172,132 @@ class ServerStore {\n \t/**\n \t * Fetches server properties from the server\n \t */\n-\tasync fetchServerProps(): Promise<void> {\n-\t\tthis._loading = true;\n-\t\tthis._error = null;\n-\t\tthis._serverWarning = null;\n+\tasync fetchServerProps(options: { silent?: boolean } = {}): Promise<void> {\n+\t\tconst { silent = false } = options;\n+\t\tconst isSilent = silent && this._serverProps !== null;\n \n-\t\ttry {\n-\t\t\tconsole.log('Fetching server properties...');\n-\t\t\tconst props = await ChatService.getServerProps();\n-\t\t\tthis._serverProps = props;\n-\t\t\tthis.persistServerProps(props);\n-\t\t\tconsole.log('Server properties loaded:', props);\n-\n-\t\t\t// Check slots endpoint availability after server props are loaded\n-\t\t\tawait this.checkSlotsEndpointAvailability();\n-\t\t} catch (error) {\n-\t\t\tconst hadCachedProps = this._serverProps !== null;\n-\t\t\tlet errorMessage = 'Failed to connect to server';\n-\t\t\tlet isOfflineLikeError = false;\n-\t\t\tlet isServerSideError = false;\n-\n-\t\t\tif (error instanceof Error) {\n-\t\t\t\t// Handle specific error types with user-friendly messages\n-\t\t\t\tif (error.name === 'TypeError' && error.message.includes('fetch')) {\n-\t\t\t\t\terrorMessage = 'Server is not running or unreachable';\n-\t\t\t\t\tisOfflineLikeError = true;\n-\t\t\t\t} else if (error.message.includes('ECONNREFUSED')) {\n-\t\t\t\t\terrorMessage = 'Connection refused - server may be offline';\n-\t\t\t\t\tisOfflineLikeError = true;\n-\t\t\t\t} else if (error.message.includes('ENOTFOUND')) {\n-\t\t\t\t\terrorMessage = 'Server not found - check server address';\n-\t\t\t\t\tisOfflineLikeError = true;\n-\t\t\t\t} else if (error.message.includes('ETIMEDOUT')) {\n-\t\t\t\t\terrorMessage = 'Request timed out - the server took too long to respond';\n-\t\t\t\t\tisOfflineLikeError = true;\n-\t\t\t\t} else if (error.message.includes('503')) {\n-\t\t\t\t\terrorMessage = 'Server temporarily unavailable - try again shortly';\n-\t\t\t\t\tisServerSideError = true;\n-\t\t\t\t} else if (error.message.includes('500')) {\n-\t\t\t\t\terrorMessage = 'Server error - check server logs';\n-\t\t\t\t\tisServerSideError = true;\n-\t\t\t\t} else if (error.message.includes('404')) {\n-\t\t\t\t\terrorMessage = 'Server endpoint not found';\n-\t\t\t\t} else if (error.message.includes('403') || error.message.includes('401')) {\n-\t\t\t\t\terrorMessage = 'Access denied';\n+\t\tif (this.fetchServerPropsPromise) {\n+\t\t\treturn this.fetchServerPropsPromise;\n+\t\t}\n+\n+\t\tif (!isSilent) {\n+\t\t\tthis._loading = true;\n+\t\t\tthis._error = null;\n+\t\t\tthis._serverWarning = null;\n+\t\t}\n+\n+\t\tconst hadProps = this._serverProps !== null;\n+\n+\t\tconst fetchPromise = (async () => {\n+\t\t\ttry {\n+\t\t\t\tconst props = await ChatService.getServerProps();\n+\t\t\t\tthis._serverProps = props;\n+\t\t\t\tthis.persistServerProps(props);\n+\t\t\t\tthis._error = null;\n+\t\t\t\tthis._serverWarning = null;\n+\t\t\t\tawait this.checkSlotsEndpointAvailability();\n+\t\t\t} catch (error) {\n+\t\t\t\tif (isSilent && hadProps) {\n+\t\t\t\t\tconsole.warn('Silent server props refresh failed, keeping cached data:', error);\n+\t\t\t\t\treturn;\n+\t\t\t\t}\n+\n+\t\t\t\tthis.handleFetchServerPropsError(error, hadProps);\n+\t\t\t} finally {\n+\t\t\t\tif (!isSilent) {\n+\t\t\t\t\tthis._loading = false;\n \t\t\t\t}\n+\n+\t\t\t\tthis.fetchServerPropsPromise = null;\n \t\t\t}\n+\t\t})();\n+\n+\t\tthis.fetchServerPropsPromise = fetchPromise;\n+\n+\t\tawait fetchPromise;\n+\t}\n \n-\t\t\tlet cachedProps: ApiLlamaCppServerProps | null = null;\n+\t/**\n+\t * Handles fetch failures by attempting to recover cached server props and\n+\t * updating the user-facing error or warning state appropriately.\n+\t */\n+\tprivate handleFetchServerPropsError(error: unknown, hadProps: boolean): void {\n+\t\tconst { errorMessage, isOfflineLikeError, isServerSideError } = this.normalizeFetchError(error);\n \n-\t\t\tif (!hadCachedProps) {\n-\t\t\t\tcachedProps = this.readCachedServerProps();\n-\t\t\t\tif (cachedProps) {\n-\t\t\t\t\tthis._serverProps = cachedProps;\n-\t\t\t\t\tthis._error = null;\n+\t\tlet cachedProps: ApiLlamaCppServerProps | null = null;\n \n-\t\t\t\t\tif (isOfflineLikeError || isServerSideError) {\n-\t\t\t\t\t\tthis._serverWarning = errorMessage;\n-\t\t\t\t\t}\n+\t\tif (!hadProps) {\n+\t\t\tcachedProps = this.readCachedServerProps();\n \n-\t\t\t\t\tconsole.warn(\n-\t\t\t\t\t\t'Failed to refresh server properties, using cached values from localStorage:',\n-\t\t\t\t\t\terrorMessage\n-\t\t\t\t\t);\n-\t\t\t\t} else {\n-\t\t\t\t\tthis._error = errorMessage;\n-\t\t\t\t}\n-\t\t\t} else {\n+\t\t\tif (cachedProps) {\n+\t\t\t\tthis._serverProps = cachedProps;\n \t\t\t\tthis._error = null;\n \n \t\t\t\tif (isOfflineLikeError || isServerSideError) {\n \t\t\t\t\tthis._serverWarning = errorMessage;\n \t\t\t\t}\n \n \t\t\t\tconsole.warn(\n-\t\t\t\t\t'Failed to refresh server properties, continuing with cached values:',\n+\t\t\t\t\t'Failed to refresh server properties, using cached values from localStorage:',\n \t\t\t\t\terrorMessage\n \t\t\t\t);\n+\t\t\t} else {\n+\t\t\t\tthis._error = errorMessage;\n+\t\t\t}\n+\t\t} else {\n+\t\t\tthis._error = null;\n+\n+\t\t\tif (isOfflineLikeError || isServerSideError) {\n+\t\t\t\tthis._serverWarning = errorMessage;\n \t\t\t}\n-\t\t\tconsole.error('Error fetching server properties:', error);\n-\t\t} finally {\n-\t\t\tthis._loading = false;\n+\n+\t\t\tconsole.warn(\n+\t\t\t\t'Failed to refresh server properties, continuing with cached values:',\n+\t\t\t\terrorMessage\n+\t\t\t);\n \t\t}\n+\n+\t\tconsole.error('Error fetching server properties:', error);\n+\t}\n+\n+\tprivate normalizeFetchError(error: unknown): {\n+\t\terrorMessage: string;\n+\t\tisOfflineLikeError: boolean;\n+\t\tisServerSideError: boolean;\n+\t} {\n+\t\tlet errorMessage = 'Failed to connect to server';\n+\t\tlet isOfflineLikeError = false;\n+\t\tlet isServerSideError = false;\n+\n+\t\tif (error instanceof Error) {\n+\t\t\tconst message = error.message || '';\n+\n+\t\t\tif (error.name === 'TypeError' && message.includes('fetch')) {\n+\t\t\t\terrorMessage = 'Server is not running or unreachable';\n+\t\t\t\tisOfflineLikeError = true;\n+\t\t\t} else if (message.includes('ECONNREFUSED')) {\n+\t\t\t\terrorMessage = 'Connection refused - server may be offline';\n+\t\t\t\tisOfflineLikeError = true;\n+\t\t\t} else if (message.includes('ENOTFOUND')) {\n+\t\t\t\terrorMessage = 'Server not found - check server address';\n+\t\t\t\tisOfflineLikeError = true;\n+\t\t\t} else if (message.includes('ETIMEDOUT')) {\n+\t\t\t\terrorMessage = 'Request timed out - the server took too long to respond';\n+\t\t\t\tisOfflineLikeError = true;\n+\t\t\t} else if (message.includes('503')) {\n+\t\t\t\terrorMessage = 'Server temporarily unavailable - try again shortly';\n+\t\t\t\tisServerSideError = true;\n+\t\t\t} else if (message.includes('500')) {\n+\t\t\t\terrorMessage = 'Server error - check server logs';\n+\t\t\t\tisServerSideError = true;\n+\t\t\t} else if (message.includes('404')) {\n+\t\t\t\terrorMessage = 'Server endpoint not found';\n+\t\t\t} else if (message.includes('403') || message.includes('401')) {\n+\t\t\t\terrorMessage = 'Access denied';\n+\t\t\t}\n+\t\t}\n+\n+\t\treturn { errorMessage, isOfflineLikeError, isServerSideError };\n \t}\n \n \t/**\n@@ -264,6 +309,7 @@ class ServerStore {\n \t\tthis._serverWarning = null;\n \t\tthis._loading = false;\n \t\tthis._slotsEndpointAvailable = null;\n+\t\tthis.fetchServerPropsPromise = null;\n \t\tthis.persistServerProps(null);\n \t}\n }"
      },
      {
        "filename": "tools/server/webui/src/lib/types/api.d.ts",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -186,6 +186,7 @@ export interface ApiChatCompletionRequest {\n }\n \n export interface ApiChatCompletionStreamChunk {\n+\tobject?: string;\n \tmodel?: string;\n \tchoices: Array<{\n \t\tmodel?: string;"
      },
      {
        "filename": "tools/server/webui/src/lib/types/settings.d.ts",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -42,6 +42,7 @@ export interface SettingsChatServiceOptions {\n \tonChunk?: (chunk: string) => void;\n \tonReasoningChunk?: (chunk: string) => void;\n \tonModel?: (model: string) => void;\n+\tonFirstValidChunk?: () => void;\n \tonComplete?: (response: string, reasoningContent?: string, timings?: ChatMessageTimings) => void;\n \tonError?: (error: Error) => void;\n }"
      }
    ],
    "num_files": 7,
    "scraped_at": "2025-11-16T23:29:32.167658"
  },
  {
    "pr_number": 16774,
    "title": "ggml: add s390x cpu-feats",
    "body": "ref: https://github.com/ggml-org/llama.cpp/pull/16664#issuecomment-3421463445\r\n\r\nThis PR introduces the CPU features detection for the s390x platform and allows for dynamic backend loading when compiled with `-DGGML_NATIVE=OFF -DGGML_BACKEND_DL=ON -DGGML_CPU_ALL_VARIANTS=ON`.\r\n\r\nTested `release.yml` and it seems to be working as intended as well: https://github.com/ggml-org/llama.cpp/actions/runs/18814223900/job/53680143680.",
    "html_url": "https://github.com/ggml-org/llama.cpp/pull/16774",
    "created_at": "2025-10-26T06:45:30Z",
    "merged_at": "2025-11-02T00:48:23Z",
    "merge_commit_sha": "d38d9f0877a5872daa3c5f06fb9a86376bf15d50",
    "base_ref": "master",
    "head_sha": "b62c93efc811e4302ac16535c7332f6e79c71db8",
    "user": "taronaeo",
    "files": [
      {
        "filename": ".github/workflows/release.yml",
        "status": "modified",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "patch": "@@ -134,8 +134,8 @@ jobs:\n         include:\n           - build: 'x64'\n             os: ubuntu-22.04\n-          - build: 's390x-z15' # z15 because our CI runners are on z15\n-            os: ubuntu-22.04-s390x\n+          - build: 's390x'\n+            os: ubuntu-24.04-s390x\n           # GGML_BACKEND_DL and GGML_CPU_ALL_VARIANTS are not currently supported on arm\n           # - build: 'arm64'\n           #   os: ubuntu-22.04-arm"
      },
      {
        "filename": "ggml/src/CMakeLists.txt",
        "status": "modified",
        "additions": 6,
        "deletions": 3,
        "changes": 9,
        "patch": "@@ -308,6 +308,10 @@ function(ggml_add_cpu_backend_variant tag_name)\n             set(GGML_INTERNAL_${feat} ON)\n         endforeach()\n     elseif (GGML_SYSTEM_ARCH STREQUAL \"s390x\")\n+        foreach (feat VXE2 NNPA)\n+            set(GGML_INTERNAL_${feat} OFF)\n+        endforeach()\n+\n         foreach (feat ${ARGN})\n             set(GGML_INTERNAL_${feat} ON)\n         endforeach()\n@@ -377,9 +381,8 @@ if (GGML_CPU_ALL_VARIANTS)\n         endif()\n     elseif (GGML_SYSTEM_ARCH STREQUAL \"s390x\")\n         if (CMAKE_SYSTEM_NAME MATCHES \"Linux\")\n-            ggml_add_cpu_backend_variant(s390x_z15  Z15 VXE)\n-            # ggml_add_cpu_backend_variant(s390x_z16  Z16 VXE)\n-            # ggml_add_cpu_backend_variant(s390x_z17  Z17 VXE)\n+            ggml_add_cpu_backend_variant(z15    Z15 VXE2)\n+            ggml_add_cpu_backend_variant(z16    Z16 VXE2 NNPA)\n         else()\n             message(FATAL_ERROR \"Unsupported s390x target OS: ${CMAKE_SYSTEM_NAME}\")\n         endif()"
      },
      {
        "filename": "ggml/src/ggml-cpu/CMakeLists.txt",
        "status": "modified",
        "additions": 10,
        "deletions": 3,
        "changes": 13,
        "patch": "@@ -504,11 +504,18 @@ function(ggml_add_cpu_backend_variant_impl tag_name)\n             endforeach()\n         endif()\n \n-        if (GGML_VXE OR GGML_INTERNAL_VXE)\n-            message(STATUS \"VX/VXE/VXE2 enabled\")\n+        if (GGML_VXE OR GGML_INTERNAL_VXE2)\n+            message(STATUS \"VXE2 enabled\")\n             list(APPEND ARCH_FLAGS -mvx -mzvector)\n-            list(APPEND ARCH_DEFINITIONS GGML_VXE)\n+            list(APPEND ARCH_DEFINITIONS GGML_USE_VXE2)\n         endif()\n+\n+        if (GGML_INTERNAL_NNPA)\n+            message(STATUS \"NNPA enabled\")\n+            list(APPEND ARCH_DEFINITIONS GGML_USE_NNPA)\n+        endif()\n+\n+        ggml_add_cpu_backend_features(${GGML_CPU_NAME} s390 ${ARCH_DEFINITIONS})\n     elseif (CMAKE_SYSTEM_PROCESSOR MATCHES \"wasm\")\n         message(STATUS \"Wasm detected\")\n         list (APPEND GGML_CPU_SOURCES ggml-cpu/arch/wasm/quants.c)"
      },
      {
        "filename": "ggml/src/ggml-cpu/arch/s390/cpu-feats.cpp",
        "status": "added",
        "additions": 50,
        "deletions": 0,
        "changes": 50,
        "patch": "@@ -0,0 +1,50 @@\n+#include \"ggml-backend-impl.h\"\n+\n+#if defined(__s390x__)\n+#include <sys/auxv.h>\n+\n+// find hwcap bits in asm/elf.h\n+#ifndef HWCAP_VXRS_EXT2\n+#define HWCAP_VXRS_EXT2 (1 << 15)\n+#endif\n+\n+#ifndef HWCAP_NNPA\n+#define HWCAP_NNPA (1 << 20)\n+#endif\n+\n+struct s390x_features {\n+    bool has_vxe2 = false;\n+    bool has_nnpa = false;\n+\n+    s390x_features() {\n+        uint32_t hwcap = getauxval(AT_HWCAP);\n+        // NOTE: use hwcap2 with DFLT for z17 and later\n+        // uint32_t hwcap2 = getauxval(AT_HWCAP2);\n+\n+        has_vxe2 = !!(hwcap & HWCAP_VXRS_EXT2);\n+        has_nnpa = !!(hwcap & HWCAP_NNPA);\n+    }\n+};\n+\n+static int ggml_backend_cpu_s390x_score() {\n+    int score = 1;\n+    s390x_features sf;\n+\n+// IBM z15 / LinuxONE 3\n+#ifdef GGML_USE_VXE2\n+    if (!sf.has_vxe2) { return 0; }\n+    score += 1 << 1;\n+#endif\n+\n+// IBM z16 / LinuxONE 4 and z17 / LinuxONE 5\n+#ifdef GGML_USE_NNPA\n+    if (!sf.has_nnpa) { return 0; }\n+    score += 1 << 2;\n+#endif\n+\n+    return score;\n+}\n+\n+GGML_BACKEND_DL_SCORE_IMPL(ggml_backend_cpu_s390x_score)\n+\n+#endif  // __s390x__"
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T23:29:33.588230"
  },
  {
    "pr_number": 16769,
    "title": "vulkan: Fuse rope+set_rows",
    "body": "This pattern appears in a lot of models, the rope operation is applied right before storing into the KV cache (usually on the K tensor).\r\n\r\nAdd a path to some of the rope shaders that computes the destination address based on the set_rows tensor. Compile variants of the shader with D_TYPE of f16 (the usual KV cache type).\r\n\r\nAdd a src3 operand to ggml_vk_op_f32 - sometimes rope uses three srcs and needs the fourth for the row indices.\r\n\r\nAdd fused_ops_write_mask to indicate which intermediate tensors need to write their results to memory. Skipping writing the roped K value helps to allow more nodes to run concurrently.\r\n\r\nAdd logic to ggml_vk_graph_optimize to make ROPE+VIEW+SET_ROWS consecutive. It rarely starts out that way in the graph.\r\n\r\nAdd new backend tests.\r\n\r\nMy test system has been giving less stable results today, so here's just one from llama2, but this helps quite a few models.\r\n\r\n```\r\nbefore:\r\n| llama 7B Q4_0                  |   3.56 GiB |     6.74 B | Vulkan     |  99 |  1 |           tg128 |        269.90 \u00b1 1.69 |\r\n\r\nafter:\r\n| llama 7B Q4_0                  |   3.56 GiB |     6.74 B | Vulkan     |  99 |  1 |           tg128 |        275.67 \u00b1 0.76 |\r\n```\r\n",
    "html_url": "https://github.com/ggml-org/llama.cpp/pull/16769",
    "created_at": "2025-10-25T20:37:01Z",
    "merged_at": "2025-10-29T20:13:10Z",
    "merge_commit_sha": "b9ce94017729465895402cbcfffb51fa926c15e3",
    "base_ref": "master",
    "head_sha": "406432fd1d08d7d11de566794d8e9840fc5ce543",
    "user": "jeffbolznv",
    "files": [
      {
        "filename": "ggml/src/ggml-vulkan/ggml-vulkan.cpp",
        "status": "modified",
        "additions": 248,
        "deletions": 86,
        "changes": 334,
        "patch": "@@ -456,6 +456,11 @@ static topk_moe_mode ggml_vk_num_additional_ops_to_topk_moe_mode(uint32_t num) {\n     return mode;\n }\n \n+static constexpr std::initializer_list<std::array<int, 3>> rope_view_set_rows_edges {\n+    { 1, 0, 0 }, // view->src[0]     == rope\n+    { 2, 0, 1 }, // set_rows->src[0] == view\n+};\n+\n struct vk_device_struct {\n     std::recursive_mutex mutex;\n \n@@ -638,8 +643,8 @@ struct vk_device_struct {\n     vk_pipeline pipeline_soft_max_f32, pipeline_soft_max_f32_f16;\n     vk_pipeline pipeline_soft_max_f32_wg512, pipeline_soft_max_f32_f16_wg512;\n     vk_pipeline pipeline_soft_max_back_f32;\n-    vk_pipeline pipeline_rope_norm_f32, pipeline_rope_norm_f16;\n-    vk_pipeline pipeline_rope_neox_f32, pipeline_rope_neox_f16;\n+    vk_pipeline pipeline_rope_norm_f32, pipeline_rope_norm_f16, pipeline_rope_norm_f32_f16;\n+    vk_pipeline pipeline_rope_neox_f32, pipeline_rope_neox_f16, pipeline_rope_neox_f32_f16;\n     vk_pipeline pipeline_rope_multi_f32, pipeline_rope_multi_f16;\n     vk_pipeline pipeline_rope_vision_f32, pipeline_rope_vision_f16;\n     vk_pipeline pipeline_argsort_f32[num_argsort_pipelines];\n@@ -1052,6 +1057,7 @@ struct vk_op_rope_push_constants {\n     uint32_t s2;\n     int32_t sections[4];\n     uint32_t is_back;\n+    uint32_t set_rows_stride;\n };\n \n struct vk_op_soft_max_push_constants {\n@@ -1562,6 +1568,10 @@ struct ggml_backend_vk_context {\n     // number of additional consecutive nodes that are being fused with the\n     // node currently being processed\n     int num_additional_fused_ops {};\n+    // Bitmask of which fused ops need to write an intermediate value to memory.\n+    // Bit 'i' means nodes[start_of_fusion + i] writes to memory.\n+    // If there's no fusion, bit 0 is still set.\n+    int fused_ops_write_mask {};\n };\n \n static void * const vk_ptr_base = (void *)(uintptr_t) 0x1000;  // NOLINT\n@@ -3695,21 +3705,27 @@ static void ggml_vk_load_shaders(vk_device& device) {\n     ggml_vk_create_pipeline(device, device->pipeline_soft_max_f32_f16_wg512, \"soft_max_f32_f16_wg512\", soft_max_f32_f16_len, soft_max_f32_f16_data, \"main\", 4, sizeof(vk_op_soft_max_push_constants), {1, 1, 1}, { 512 }, 1);\n     ggml_vk_create_pipeline(device, device->pipeline_soft_max_back_f32, \"soft_max_back_f32\", soft_max_back_f32_len, soft_max_back_f32_data, \"main\", 3, sizeof(vk_op_push_constants), {1, 1, 1}, { device->subgroup_size }, 1, true);\n \n-    ggml_vk_create_pipeline(device, device->pipeline_rope_norm_f32, \"rope_norm_f32\", rope_norm_f32_len, rope_norm_f32_data, \"main\", 4, sizeof(vk_op_rope_push_constants), {1, 512, 1}, {}, 1);\n-    ggml_vk_create_pipeline(device, device->pipeline_rope_neox_f32, \"rope_neox_f32\", rope_neox_f32_len, rope_neox_f32_data, \"main\", 4, sizeof(vk_op_rope_push_constants), {1, 512, 1}, {}, 1);\n-    ggml_vk_create_pipeline(device, device->pipeline_rope_multi_f32, \"rope_multi_f32\", rope_multi_f32_len, rope_multi_f32_data, \"main\", 4, sizeof(vk_op_rope_push_constants), {1, 512, 1}, {}, 1);\n-    ggml_vk_create_pipeline(device, device->pipeline_rope_vision_f32, \"rope_vision_f32\", rope_vision_f32_len, rope_vision_f32_data, \"main\", 4, sizeof(vk_op_rope_push_constants), {1, 512, 1}, {}, 1);\n+    ggml_vk_create_pipeline(device, device->pipeline_rope_norm_f32, \"rope_norm_f32\", rope_norm_f32_len, rope_norm_f32_data, \"main\", 5, sizeof(vk_op_rope_push_constants), {1, 512, 1}, {}, 1);\n+    ggml_vk_create_pipeline(device, device->pipeline_rope_neox_f32, \"rope_neox_f32\", rope_neox_f32_len, rope_neox_f32_data, \"main\", 5, sizeof(vk_op_rope_push_constants), {1, 512, 1}, {}, 1);\n+    ggml_vk_create_pipeline(device, device->pipeline_rope_multi_f32, \"rope_multi_f32\", rope_multi_f32_len, rope_multi_f32_data, \"main\", 5, sizeof(vk_op_rope_push_constants), {1, 512, 1}, {}, 1);\n+    ggml_vk_create_pipeline(device, device->pipeline_rope_vision_f32, \"rope_vision_f32\", rope_vision_f32_len, rope_vision_f32_data, \"main\", 5, sizeof(vk_op_rope_push_constants), {1, 512, 1}, {}, 1);\n \n     if (device->float_controls_rte_fp16) {\n-        ggml_vk_create_pipeline(device, device->pipeline_rope_norm_f16, \"rope_norm_f16\", rope_norm_f16_rte_len, rope_norm_f16_rte_data, \"main\", 4, sizeof(vk_op_rope_push_constants), {1, 512, 1}, {}, 1);\n-        ggml_vk_create_pipeline(device, device->pipeline_rope_neox_f16, \"rope_neox_f16\", rope_neox_f16_rte_len, rope_neox_f16_rte_data, \"main\", 4, sizeof(vk_op_rope_push_constants), {1, 512, 1}, {}, 1);\n-        ggml_vk_create_pipeline(device, device->pipeline_rope_multi_f16, \"rope_multi_f16\", rope_multi_f16_rte_len, rope_multi_f16_rte_data, \"main\", 4, sizeof(vk_op_rope_push_constants), {1, 512, 1}, {}, 1);\n-        ggml_vk_create_pipeline(device, device->pipeline_rope_vision_f16, \"rope_vision_f16\", rope_vision_f16_rte_len, rope_vision_f16_rte_data, \"main\", 4, sizeof(vk_op_rope_push_constants), {1, 512, 1}, {}, 1);\n+        ggml_vk_create_pipeline(device, device->pipeline_rope_norm_f16, \"rope_norm_f16\", rope_norm_f16_rte_len, rope_norm_f16_rte_data, \"main\", 5, sizeof(vk_op_rope_push_constants), {1, 512, 1}, {}, 1);\n+        ggml_vk_create_pipeline(device, device->pipeline_rope_neox_f16, \"rope_neox_f16\", rope_neox_f16_rte_len, rope_neox_f16_rte_data, \"main\", 5, sizeof(vk_op_rope_push_constants), {1, 512, 1}, {}, 1);\n+        ggml_vk_create_pipeline(device, device->pipeline_rope_multi_f16, \"rope_multi_f16\", rope_multi_f16_rte_len, rope_multi_f16_rte_data, \"main\", 5, sizeof(vk_op_rope_push_constants), {1, 512, 1}, {}, 1);\n+        ggml_vk_create_pipeline(device, device->pipeline_rope_vision_f16, \"rope_vision_f16\", rope_vision_f16_rte_len, rope_vision_f16_rte_data, \"main\", 5, sizeof(vk_op_rope_push_constants), {1, 512, 1}, {}, 1);\n+\n+        ggml_vk_create_pipeline(device, device->pipeline_rope_norm_f32_f16, \"rope_norm_f32_f16\", rope_norm_f32_f16_rte_len, rope_norm_f32_f16_rte_data, \"main\", 5, sizeof(vk_op_rope_push_constants), {1, 512, 1}, {}, 1);\n+        ggml_vk_create_pipeline(device, device->pipeline_rope_neox_f32_f16, \"rope_neox_f32_f16\", rope_neox_f32_f16_rte_len, rope_neox_f32_f16_rte_data, \"main\", 5, sizeof(vk_op_rope_push_constants), {1, 512, 1}, {}, 1);\n     } else {\n-        ggml_vk_create_pipeline(device, device->pipeline_rope_norm_f16, \"rope_norm_f16\", rope_norm_f16_len, rope_norm_f16_data, \"main\", 4, sizeof(vk_op_rope_push_constants), {1, 512, 1}, {}, 1);\n-        ggml_vk_create_pipeline(device, device->pipeline_rope_neox_f16, \"rope_neox_f16\", rope_neox_f16_len, rope_neox_f16_data, \"main\", 4, sizeof(vk_op_rope_push_constants), {1, 512, 1}, {}, 1);\n-        ggml_vk_create_pipeline(device, device->pipeline_rope_multi_f16, \"rope_multi_f16\", rope_multi_f16_len, rope_multi_f16_data, \"main\", 4, sizeof(vk_op_rope_push_constants), {1, 512, 1}, {}, 1);\n-        ggml_vk_create_pipeline(device, device->pipeline_rope_vision_f16, \"rope_vision_f16\", rope_vision_f16_len, rope_vision_f16_data, \"main\", 4, sizeof(vk_op_rope_push_constants), {1, 512, 1}, {}, 1);\n+        ggml_vk_create_pipeline(device, device->pipeline_rope_norm_f16, \"rope_norm_f16\", rope_norm_f16_len, rope_norm_f16_data, \"main\", 5, sizeof(vk_op_rope_push_constants), {1, 512, 1}, {}, 1);\n+        ggml_vk_create_pipeline(device, device->pipeline_rope_neox_f16, \"rope_neox_f16\", rope_neox_f16_len, rope_neox_f16_data, \"main\", 5, sizeof(vk_op_rope_push_constants), {1, 512, 1}, {}, 1);\n+        ggml_vk_create_pipeline(device, device->pipeline_rope_multi_f16, \"rope_multi_f16\", rope_multi_f16_len, rope_multi_f16_data, \"main\", 5, sizeof(vk_op_rope_push_constants), {1, 512, 1}, {}, 1);\n+        ggml_vk_create_pipeline(device, device->pipeline_rope_vision_f16, \"rope_vision_f16\", rope_vision_f16_len, rope_vision_f16_data, \"main\", 5, sizeof(vk_op_rope_push_constants), {1, 512, 1}, {}, 1);\n+\n+        ggml_vk_create_pipeline(device, device->pipeline_rope_norm_f32_f16, \"rope_norm_f32_f16\", rope_norm_f32_f16_len, rope_norm_f32_f16_data, \"main\", 5, sizeof(vk_op_rope_push_constants), {1, 512, 1}, {}, 1);\n+        ggml_vk_create_pipeline(device, device->pipeline_rope_neox_f32_f16, \"rope_neox_f32_f16\", rope_neox_f32_f16_len, rope_neox_f32_f16_data, \"main\", 5, sizeof(vk_op_rope_push_constants), {1, 512, 1}, {}, 1);\n     }\n \n     for (uint32_t i = 0; i < num_argsort_pipelines; ++i) {\n@@ -8168,7 +8184,8 @@ static vk_pipeline ggml_vk_op_get_pipeline(ggml_backend_vk_context * ctx, const\n     case GGML_OP_ROPE:\n     case GGML_OP_ROPE_BACK:\n         {\n-            const int mode = ((const int32_t *) dst->op_params)[2];\n+            const ggml_tensor *rope = ctx->num_additional_fused_ops == 2 ? dst->src[0]->src[0] : dst;\n+            const int mode = ((const int32_t *) rope->op_params)[2];\n             const bool is_neox = mode & GGML_ROPE_TYPE_NEOX;\n             const bool is_mrope = mode & GGML_ROPE_TYPE_MROPE;\n             const bool is_vision = mode == GGML_ROPE_TYPE_VISION;\n@@ -8177,6 +8194,9 @@ static vk_pipeline ggml_vk_op_get_pipeline(ggml_backend_vk_context * ctx, const\n                 if (src0->type == GGML_TYPE_F32 && dst->type == GGML_TYPE_F32) {\n                     return ctx->device->pipeline_rope_neox_f32;\n                 }\n+                if (src0->type == GGML_TYPE_F32 && dst->type == GGML_TYPE_F16) {\n+                    return ctx->device->pipeline_rope_neox_f32_f16;\n+                }\n                 if (src0->type == GGML_TYPE_F16 && dst->type == GGML_TYPE_F16) {\n                     return ctx->device->pipeline_rope_neox_f16;\n                 }\n@@ -8198,6 +8218,9 @@ static vk_pipeline ggml_vk_op_get_pipeline(ggml_backend_vk_context * ctx, const\n                 if (src0->type == GGML_TYPE_F32 && dst->type == GGML_TYPE_F32) {\n                     return ctx->device->pipeline_rope_norm_f32;\n                 }\n+                if (src0->type == GGML_TYPE_F32 && dst->type == GGML_TYPE_F16) {\n+                    return ctx->device->pipeline_rope_norm_f32_f16;\n+                }\n                 if (src0->type == GGML_TYPE_F16 && dst->type == GGML_TYPE_F16) {\n                     return ctx->device->pipeline_rope_norm_f16;\n                 }\n@@ -8407,60 +8430,66 @@ static uint32_t get_misalign_bytes(ggml_backend_vk_context * ctx, const ggml_ten\n     return ((vk_tensor_offset(t) + t->view_offs) & (ctx->device->properties.limits.minStorageBufferOffsetAlignment - 1));;\n }\n \n-template <typename T> void init_pushconst_tensor_offsets(ggml_backend_vk_context * ctx, T &p, const ggml_tensor * src0, const ggml_tensor * src1, const ggml_tensor * src2, ggml_tensor * dst) {\n+template <typename T> void init_pushconst_tensor_offsets(ggml_backend_vk_context * ctx, T &p, const ggml_tensor * src0, const ggml_tensor * src1, const ggml_tensor * src2, const ggml_tensor * src3, ggml_tensor * dst) {\n     GGML_UNUSED(p);\n     GGML_UNUSED(src0);\n     GGML_UNUSED(src1);\n     GGML_UNUSED(src2);\n+    GGML_UNUSED(src3);\n     GGML_UNUSED(dst);\n     static_assert(!std::is_const<T>::value, \"unexpected type\");\n     GGML_ASSERT(!src0 || get_misalign_bytes(ctx, src0) == 0);\n     GGML_ASSERT(!src1 || get_misalign_bytes(ctx, src1) == 0);\n     GGML_ASSERT(!src2 || get_misalign_bytes(ctx, src2) == 0);\n+    GGML_ASSERT(!src3 || get_misalign_bytes(ctx, src3) == 0);\n     GGML_ASSERT(!dst  || get_misalign_bytes(ctx, dst) == 0);\n }\n \n-template <> void init_pushconst_tensor_offsets(ggml_backend_vk_context * ctx, vk_op_unary_push_constants &p, const ggml_tensor * src0, const ggml_tensor * src1, const ggml_tensor * src2, ggml_tensor * dst) {\n+template <> void init_pushconst_tensor_offsets(ggml_backend_vk_context * ctx, vk_op_unary_push_constants &p, const ggml_tensor * src0, const ggml_tensor * src1, const ggml_tensor * src2, const ggml_tensor * src3, ggml_tensor * dst) {\n     const uint32_t a_offset = get_misalign_bytes(ctx, src0) / ggml_type_size(src0->type);\n     const uint32_t d_offset = get_misalign_bytes(ctx, dst) / ggml_type_size(dst->type);\n \n     p.misalign_offsets = (a_offset << 16) | d_offset;\n \n     GGML_UNUSED(src1);\n     GGML_UNUSED(src2);\n+    GGML_UNUSED(src3);\n }\n \n-template <> void init_pushconst_tensor_offsets(ggml_backend_vk_context * ctx, vk_op_sum_rows_push_constants &p, const ggml_tensor * src0, const ggml_tensor * src1, const ggml_tensor * src2, ggml_tensor * dst) {\n+template <> void init_pushconst_tensor_offsets(ggml_backend_vk_context * ctx, vk_op_sum_rows_push_constants &p, const ggml_tensor * src0, const ggml_tensor * src1, const ggml_tensor * src2, const ggml_tensor * src3, ggml_tensor * dst) {\n     const uint32_t a_offset = get_misalign_bytes(ctx, src0) / ggml_type_size(src0->type);\n     const uint32_t d_offset = get_misalign_bytes(ctx, dst) / ggml_type_size(dst->type);\n \n     p.misalign_offsets = (a_offset << 16) | d_offset;\n \n     GGML_UNUSED(src1);\n     GGML_UNUSED(src2);\n+    GGML_UNUSED(src3);\n }\n \n-template <> void init_pushconst_tensor_offsets(ggml_backend_vk_context * ctx, vk_op_pad_push_constants &p, const ggml_tensor * src0, const ggml_tensor * src1, const ggml_tensor * src2, ggml_tensor * dst) {\n+template <> void init_pushconst_tensor_offsets(ggml_backend_vk_context * ctx, vk_op_pad_push_constants &p, const ggml_tensor * src0, const ggml_tensor * src1, const ggml_tensor * src2, const ggml_tensor * src3, ggml_tensor * dst) {\n     const uint32_t a_offset = get_misalign_bytes(ctx, src0) / ggml_type_size(src0->type);\n     const uint32_t d_offset = get_misalign_bytes(ctx, dst) / ggml_type_size(dst->type);\n \n     p.misalign_offsets = (a_offset << 16) | d_offset;\n \n     GGML_UNUSED(src1);\n     GGML_UNUSED(src2);\n+    GGML_UNUSED(src3);\n }\n \n-template <> void init_pushconst_tensor_offsets(ggml_backend_vk_context * ctx, vk_op_im2col_3d_push_constants &p, const ggml_tensor * src0, const ggml_tensor * src1, const ggml_tensor * src2, ggml_tensor * dst) {\n+template <> void init_pushconst_tensor_offsets(ggml_backend_vk_context * ctx, vk_op_im2col_3d_push_constants &p, const ggml_tensor * src0, const ggml_tensor * src1, const ggml_tensor * src2, const ggml_tensor * src3, ggml_tensor * dst) {\n     const uint32_t a_offset = get_misalign_bytes(ctx, src1) / ggml_type_size(src1->type);\n     const uint32_t d_offset = get_misalign_bytes(ctx, dst) / ggml_type_size(dst->type);\n \n     p.misalign_offsets = (a_offset << 16) | d_offset;\n \n     GGML_UNUSED(src0);\n     GGML_UNUSED(src2);\n+    GGML_UNUSED(src3);\n }\n \n-template <> void init_pushconst_tensor_offsets(ggml_backend_vk_context * ctx, vk_op_binary_push_constants &p, const ggml_tensor * src0, const ggml_tensor * src1, const ggml_tensor * src2, ggml_tensor * dst) {\n+template <> void init_pushconst_tensor_offsets(ggml_backend_vk_context * ctx, vk_op_binary_push_constants &p, const ggml_tensor * src0, const ggml_tensor * src1, const ggml_tensor * src2, const ggml_tensor * src3, ggml_tensor * dst) {\n     const uint32_t a_offset = get_misalign_bytes(ctx, src0) / ggml_type_size(src0->type);\n     const uint32_t b_offset = get_misalign_bytes(ctx, src1) / ggml_type_size(src1->type);\n     const uint32_t d_offset = get_misalign_bytes(ctx, dst) / ggml_type_size(dst->type);\n@@ -8470,9 +8499,10 @@ template <> void init_pushconst_tensor_offsets(ggml_backend_vk_context * ctx, vk\n     p.misalign_offsets = (a_offset << 16) | (b_offset << 8) | d_offset;\n \n     GGML_UNUSED(src2);\n+    GGML_UNUSED(src3);\n }\n \n-template <> void init_pushconst_tensor_offsets(ggml_backend_vk_context * ctx, vk_op_upscale_push_constants &p, const ggml_tensor * src0, const ggml_tensor * src1, const ggml_tensor * src2, ggml_tensor * dst) {\n+template <> void init_pushconst_tensor_offsets(ggml_backend_vk_context * ctx, vk_op_upscale_push_constants &p, const ggml_tensor * src0, const ggml_tensor * src1, const ggml_tensor * src2, const ggml_tensor * src3, ggml_tensor * dst) {\n     const uint32_t a_offset = get_misalign_bytes(ctx, src0) / ggml_type_size(src0->type);\n     const uint32_t d_offset = get_misalign_bytes(ctx, dst) / ggml_type_size(dst->type);\n \n@@ -8481,17 +8511,21 @@ template <> void init_pushconst_tensor_offsets(ggml_backend_vk_context * ctx, vk\n \n     GGML_UNUSED(src1);\n     GGML_UNUSED(src2);\n+    GGML_UNUSED(src3);\n }\n \n template<typename PC>\n-static void ggml_vk_op_f32(ggml_backend_vk_context * ctx, vk_context& subctx, const ggml_tensor * src0, const ggml_tensor * src1, const ggml_tensor * src2, ggml_tensor * dst, ggml_op op, PC&& pc, bool dryrun = false) {\n+static void ggml_vk_op_f32(ggml_backend_vk_context * ctx, vk_context& subctx, const ggml_tensor * src0, const ggml_tensor * src1, const ggml_tensor * src2, const ggml_tensor * src3, ggml_tensor * dst, ggml_op op, PC&& pc, bool dryrun = false) {\n     VK_LOG_DEBUG(\"ggml_vk_op_f32((\" << src0 << \", name=\" << src0->name << \", type=\" << src0->type << \", ne0=\" << src0->ne[0] << \", ne1=\" << src0->ne[1] << \", ne2=\" << src0->ne[2] << \", ne3=\" << src0->ne[3] << \", nb0=\" << src0->nb[0] << \", nb1=\" << src0->nb[1] << \", nb2=\" << src0->nb[2] << \", nb3=\" << src0->nb[3];\n     if (src1 != nullptr) {\n         std::cerr << \"), (\" << src1 << \", name=\" << src1->name << \", type=\" << src1->type << \", ne0=\" << src1->ne[0] << \", ne1=\" << src1->ne[1] << \", ne2=\" << src1->ne[2] << \", ne3=\" << src1->ne[3] << \", nb0=\" << src1->nb[0] << \", nb1=\" << src1->nb[1] << \", nb2=\" << src1->nb[2] << \", nb3=\" << src1->nb[3];\n     }\n     if (src2 != nullptr) {\n         std::cerr << \"), (\" << src2 << \", name=\" << src2->name << \", type=\" << src2->type << \", ne0=\" << src2->ne[0] << \", ne1=\" << src2->ne[1] << \", ne2=\" << src2->ne[2] << \", ne3=\" << src2->ne[3] << \", nb0=\" << src2->nb[0] << \", nb1=\" << src2->nb[1] << \", nb2=\" << src2->nb[2] << \", nb3=\" << src2->nb[3];\n     }\n+    if (src3 != nullptr) {\n+        std::cerr << \"), (\" << src3 << \", name=\" << src3->name << \", type=\" << src3->type << \", ne0=\" << src3->ne[0] << \", ne1=\" << src3->ne[1] << \", ne2=\" << src3->ne[2] << \", ne3=\" << src3->ne[3] << \", nb0=\" << src3->nb[0] << \", nb1=\" << src3->nb[1] << \", nb2=\" << src3->nb[2] << \", nb3=\" << src3->nb[3];\n+    }\n     std::cerr << \"), (\" << dst << \", name=\" << dst->name << \", type=\" << dst->type << \", ne0=\" << dst->ne[0] << \", ne1=\" << dst->ne[1] << \", ne2=\" << dst->ne[2] << \", ne3=\" << dst->ne[3] << \", nb0=\" << dst->nb[0] << \", nb1=\" << dst->nb[1] << \", nb2=\" << dst->nb[2] << \", nb3=\" << dst->nb[3];\n     std::cerr << \"), \" << ggml_op_name(op) << \", \" << (dryrun ? \"dryrun\" : \"\") << \")\");\n     GGML_ASSERT(op == GGML_OP_GET_ROWS || op == GGML_OP_CPY || (!ggml_is_quantized(src0->type) && (src1 == nullptr || !ggml_is_quantized(src1->type))));  // NOLINT\n@@ -8518,6 +8552,13 @@ static void ggml_vk_op_f32(ggml_backend_vk_context * ctx, vk_context& subctx, co\n     const uint64_t ne23 = use_src2 ? src2->ne[3] : 0;\n     const uint64_t ne2 = ne20 * ne21;\n \n+    const bool use_src3 = src3 != nullptr;\n+    const uint64_t ne30 = use_src3 ? src3->ne[0] : 0;\n+    const uint64_t ne31 = use_src3 ? src3->ne[1] : 0;\n+    const uint64_t ne32 = use_src3 ? src3->ne[2] : 0;\n+    const uint64_t ne33 = use_src3 ? src3->ne[3] : 0;\n+    const uint64_t ne3 = ne30 * ne31;\n+\n     const uint64_t ned0 = dst->ne[0];\n     const uint64_t ned1 = dst->ne[1];\n     const uint64_t ned2 = dst->ne[2];\n@@ -8548,17 +8589,21 @@ static void ggml_vk_op_f32(ggml_backend_vk_context * ctx, vk_context& subctx, co\n     ggml_backend_vk_buffer_context * src0_buf_ctx = (ggml_backend_vk_buffer_context *)src0->buffer->context;\n     ggml_backend_vk_buffer_context * src1_buf_ctx = use_src1 ? (ggml_backend_vk_buffer_context *)src1->buffer->context : nullptr;\n     ggml_backend_vk_buffer_context * src2_buf_ctx = use_src2 ? (ggml_backend_vk_buffer_context *)src2->buffer->context : nullptr;\n+    ggml_backend_vk_buffer_context * src3_buf_ctx = use_src3 ? (ggml_backend_vk_buffer_context *)src3->buffer->context : nullptr;\n \n     vk_buffer d_X = nullptr;\n     size_t x_buf_offset = 0;\n     vk_buffer d_Y = nullptr;\n     size_t y_buf_offset = 0;\n     vk_buffer d_Z = nullptr;\n     size_t z_buf_offset = 0;\n+    vk_buffer d_W = nullptr;\n+    size_t w_buf_offset = 0;\n \n     bool src0_uma = false;\n     bool src1_uma = false;\n     bool src2_uma = false;\n+    bool src3_uma = false;\n \n     if (ctx->device->uma) {\n         ggml_vk_host_get(ctx->device, src0->data, d_X, x_buf_offset);\n@@ -8571,6 +8616,10 @@ static void ggml_vk_op_f32(ggml_backend_vk_context * ctx, vk_context& subctx, co\n             ggml_vk_host_get(ctx->device, src2->data, d_Z, z_buf_offset);\n             src2_uma = d_Z != nullptr;\n         }\n+        if (use_src3) {\n+            ggml_vk_host_get(ctx->device, src3->data, d_W, w_buf_offset);\n+            src3_uma = d_W != nullptr;\n+        }\n     }\n \n     vk_buffer d_D = dst_buf_ctx->dev_buffer;\n@@ -8592,11 +8641,17 @@ static void ggml_vk_op_f32(ggml_backend_vk_context * ctx, vk_context& subctx, co\n         z_buf_offset = vk_tensor_offset(src2) + src2->view_offs;\n         GGML_ASSERT(d_Z != nullptr);\n     }\n+    if (use_src3 && !src3_uma) {\n+        d_W = src3_buf_ctx->dev_buffer;\n+        w_buf_offset = vk_tensor_offset(src3) + src3->view_offs;\n+        GGML_ASSERT(d_W != nullptr);\n+    }\n     // Compute misalignment offset for descriptors and store it in in push constants, then align the descriptor offsets.\n-    init_pushconst_tensor_offsets(ctx, pc, src0, src1, src2, dst);\n+    init_pushconst_tensor_offsets(ctx, pc, src0, src1, src2, src3, dst);\n     x_buf_offset &= ~(ctx->device->properties.limits.minStorageBufferOffsetAlignment - 1);\n     y_buf_offset &= ~(ctx->device->properties.limits.minStorageBufferOffsetAlignment - 1);\n     z_buf_offset &= ~(ctx->device->properties.limits.minStorageBufferOffsetAlignment - 1);\n+    w_buf_offset &= ~(ctx->device->properties.limits.minStorageBufferOffsetAlignment - 1);\n     d_buf_offset &= ~(ctx->device->properties.limits.minStorageBufferOffsetAlignment - 1);\n \n     std::array<uint32_t, 3> elements;\n@@ -8797,12 +8852,13 @@ static void ggml_vk_op_f32(ggml_backend_vk_context * ctx, vk_context& subctx, co\n         break;\n     }\n \n-    uint64_t x_sz, y_sz, z_sz, d_sz;\n+    uint64_t x_sz, y_sz, z_sz, w_sz, d_sz;\n \n     if (op_supports_incontiguous) {\n         x_sz = ggml_nbytes(src0) + get_misalign_bytes(ctx, src0);\n         y_sz = use_src1 ? ggml_nbytes(src1) + get_misalign_bytes(ctx, src1) : 0;\n         z_sz = use_src2 ? ggml_nbytes(src2) + get_misalign_bytes(ctx, src2) : 0;\n+        w_sz = use_src3 ? ggml_nbytes(src3) + get_misalign_bytes(ctx, src3) : 0;\n         d_sz = ggml_nbytes(dst) + get_misalign_bytes(ctx, dst);\n \n         if (x_buf_offset + x_sz >= d_X->size) {\n@@ -8814,13 +8870,17 @@ static void ggml_vk_op_f32(ggml_backend_vk_context * ctx, vk_context& subctx, co\n         if (use_src2 && z_buf_offset + z_sz >= d_Z->size) {\n             z_sz = ggml_vk_get_max_buffer_range(ctx, d_Z, z_buf_offset);\n         }\n+        if (use_src3 && w_buf_offset + w_sz >= d_W->size) {\n+            w_sz = ggml_vk_get_max_buffer_range(ctx, d_W, w_buf_offset);\n+        }\n         if (d_buf_offset + d_sz >= d_D->size) {\n             d_sz = ggml_vk_get_max_buffer_range(ctx, d_D, d_buf_offset);\n         }\n     } else {\n         x_sz = ggml_type_size(src0->type)/ggml_blck_size(src0->type) * ne0 * ne02 * ne03;\n         y_sz = use_src1 ? ggml_type_size(src1->type) * ne1 * ne12 * ne13 : 0;\n         z_sz = use_src2 ? ggml_type_size(src2->type) * ne2 * ne22 * ne23 : 0;\n+        w_sz = use_src3 ? ggml_type_size(src3->type) * ne3 * ne32 * ne33 : 0;\n         d_sz = ggml_type_size(dst->type) * ned * ned2 * ned3;\n     }\n \n@@ -8862,14 +8922,19 @@ static void ggml_vk_op_f32(ggml_backend_vk_context * ctx, vk_context& subctx, co\n         ggml_vk_dispatch_pipeline(ctx, subctx, pipeline, { vk_subbuffer{ d_X, x_buf_offset, x_sz }, subbuf_y, subbuf_z, vk_subbuffer{ d_D, d_buf_offset, d_sz } }, pc, elements);\n     } else if (op == GGML_OP_ROPE || op == GGML_OP_ROPE_BACK) {\n         // Empty src2 is possible in rope, but the shader needs a buffer\n-        vk_subbuffer subbuf_z;\n+        vk_subbuffer subbuf_z, subbuf_w;\n         if (use_src2) {\n             subbuf_z = { d_Z, z_buf_offset, z_sz };\n         } else {\n             subbuf_z = { d_X, 0, x_sz };\n         }\n+        if (use_src3) {\n+            subbuf_w = { d_W, w_buf_offset, w_sz };\n+        } else {\n+            subbuf_w = { d_X, 0, x_sz };\n+        }\n \n-        ggml_vk_dispatch_pipeline(ctx, subctx, pipeline, { vk_subbuffer{ d_X, x_buf_offset, x_sz }, vk_subbuffer{ d_Y, y_buf_offset, y_sz }, subbuf_z, vk_subbuffer{ d_D, d_buf_offset, d_sz } }, pc, elements);\n+        ggml_vk_dispatch_pipeline(ctx, subctx, pipeline, { vk_subbuffer{ d_X, x_buf_offset, x_sz }, vk_subbuffer{ d_Y, y_buf_offset, y_sz }, subbuf_z, vk_subbuffer{ d_D, d_buf_offset, d_sz }, subbuf_w }, pc, elements);\n     } else if (op == GGML_OP_IM2COL || op == GGML_OP_IM2COL_3D) {\n         if (ctx->device->shader_int64 && ctx->device->buffer_device_address) {\n             // buffer device address path doesn't use dst buffer\n@@ -8885,6 +8950,8 @@ static void ggml_vk_op_f32(ggml_backend_vk_context * ctx, vk_context& subctx, co\n     } else if (op == GGML_OP_OPT_STEP_SGD) {\n         // OPT_STEP_SGD works on src0, it does not need dst\n         ggml_vk_dispatch_pipeline(ctx, subctx, pipeline, { vk_subbuffer{ d_X, x_buf_offset, x_sz }, vk_subbuffer{ d_Y, y_buf_offset, y_sz }, vk_subbuffer{ d_Z, z_buf_offset, z_sz } }, pc, elements);\n+    } else if (use_src3) {\n+        ggml_vk_dispatch_pipeline(ctx, subctx, pipeline, { vk_subbuffer{ d_X, x_buf_offset, x_sz }, vk_subbuffer{ d_Y, y_buf_offset, y_sz }, vk_subbuffer{ d_Z, z_buf_offset, z_sz }, vk_subbuffer{ d_W, w_buf_offset, w_sz }, vk_subbuffer{ d_D, d_buf_offset, d_sz } }, pc, elements);\n     } else if (use_src2) {\n         ggml_vk_dispatch_pipeline(ctx, subctx, pipeline, { vk_subbuffer{ d_X, x_buf_offset, x_sz }, vk_subbuffer{ d_Y, y_buf_offset, y_sz }, vk_subbuffer{ d_Z, z_buf_offset, z_sz }, vk_subbuffer{ d_D, d_buf_offset, d_sz } }, pc, elements);\n     } else if (use_src1) {\n@@ -8899,7 +8966,7 @@ static void ggml_vk_get_rows(ggml_backend_vk_context * ctx, vk_context& subctx,\n     const uint32_t src1_type_size = ggml_type_size(src1->type);\n     const uint32_t dst_type_size = ggml_type_size(dst->type);\n \n-    ggml_vk_op_f32<vk_op_binary_push_constants>(ctx, subctx, src0, src1, nullptr, dst, GGML_OP_GET_ROWS, {\n+    ggml_vk_op_f32<vk_op_binary_push_constants>(ctx, subctx, src0, src1, nullptr, nullptr, dst, GGML_OP_GET_ROWS, {\n         (uint32_t)ggml_nelements(src0),\n         (uint32_t)src0->ne[0], (uint32_t)src0->ne[1], (uint32_t)src0->ne[2],(uint32_t)src0->ne[3], (uint32_t)src0->nb[0] / src0_type_size, (uint32_t)src0->nb[1] / src0_type_size, (uint32_t)src0->nb[2] / src0_type_size, (uint32_t)src0->nb[3] / src0_type_size,\n         (uint32_t)src1->ne[0], (uint32_t)src1->ne[1], (uint32_t)src1->ne[2],(uint32_t)src1->ne[3], (uint32_t)src1->nb[0] / src1_type_size, (uint32_t)src1->nb[1] / src1_type_size, (uint32_t)src1->nb[2] / src1_type_size, (uint32_t)src1->nb[3] / src1_type_size,\n@@ -8919,7 +8986,7 @@ static void ggml_vk_acc(ggml_backend_vk_context * ctx, vk_context& subctx, const\n     // int nb3 = dst->op_params[2] / 4; // 4 bytes of float32 - unused\n     int offset = dst->op_params[3] / 4; // offset in bytes\n \n-    ggml_vk_op_f32<vk_op_binary_push_constants>(ctx, subctx, src0, src1, nullptr, dst, GGML_OP_ACC, {\n+    ggml_vk_op_f32<vk_op_binary_push_constants>(ctx, subctx, src0, src1, nullptr, nullptr, dst, GGML_OP_ACC, {\n         (uint32_t)ggml_nelements(src0),\n         (uint32_t)src0->ne[0], (uint32_t)src0->ne[1], (uint32_t)src0->ne[2],(uint32_t)src0->ne[3], (uint32_t)src0->nb[0] / src0_type_size, (uint32_t)nb1, (uint32_t)nb2, (uint32_t)src0->nb[3] / src0_type_size,\n         (uint32_t)src1->ne[0], (uint32_t)src1->ne[1], (uint32_t)src1->ne[2],(uint32_t)src1->ne[3], (uint32_t)src1->nb[0] / src1_type_size, (uint32_t)src1->nb[1] / src1_type_size, (uint32_t)src1->nb[2] / src1_type_size, (uint32_t)src1->nb[3] / src1_type_size,\n@@ -9044,7 +9111,7 @@ static void ggml_vk_add(ggml_backend_vk_context * ctx, vk_context& subctx, const\n     const uint32_t src1_type_size = ggml_type_size(src1->type);\n     const uint32_t dst_type_size = ggml_type_size(dst->type);\n \n-    ggml_vk_op_f32<vk_op_binary_push_constants>(ctx, subctx, src0, src1, nullptr, dst, GGML_OP_ADD, {\n+    ggml_vk_op_f32<vk_op_binary_push_constants>(ctx, subctx, src0, src1, nullptr, nullptr, dst, GGML_OP_ADD, {\n         (uint32_t)ggml_nelements(src0),\n         (uint32_t)src0->ne[0], (uint32_t)src0->ne[1], (uint32_t)src0->ne[2],(uint32_t)src0->ne[3], (uint32_t)src0->nb[0] / src0_type_size, (uint32_t)src0->nb[1] / src0_type_size, (uint32_t)src0->nb[2] / src0_type_size, (uint32_t)src0->nb[3] / src0_type_size,\n         (uint32_t)src1->ne[0], (uint32_t)src1->ne[1], (uint32_t)src1->ne[2],(uint32_t)src1->ne[3], (uint32_t)src1->nb[0] / src1_type_size, (uint32_t)src1->nb[1] / src1_type_size, (uint32_t)src1->nb[2] / src1_type_size, (uint32_t)src1->nb[3] / src1_type_size,\n@@ -9059,7 +9126,7 @@ static void ggml_vk_sub(ggml_backend_vk_context * ctx, vk_context& subctx, const\n     const uint32_t src1_type_size = ggml_type_size(src1->type);\n     const uint32_t dst_type_size = ggml_type_size(dst->type);\n \n-    ggml_vk_op_f32<vk_op_binary_push_constants>(ctx, subctx, src0, src1, nullptr, dst, GGML_OP_SUB, {\n+    ggml_vk_op_f32<vk_op_binary_push_constants>(ctx, subctx, src0, src1, nullptr, nullptr, dst, GGML_OP_SUB, {\n         (uint32_t)ggml_nelements(src0),\n         (uint32_t)src0->ne[0], (uint32_t)src0->ne[1], (uint32_t)src0->ne[2],(uint32_t)src0->ne[3], (uint32_t)src0->nb[0] / src0_type_size, (uint32_t)src0->nb[1] / src0_type_size, (uint32_t)src0->nb[2] / src0_type_size, (uint32_t)src0->nb[3] / src0_type_size,\n         (uint32_t)src1->ne[0], (uint32_t)src1->ne[1], (uint32_t)src1->ne[2],(uint32_t)src1->ne[3], (uint32_t)src1->nb[0] / src1_type_size, (uint32_t)src1->nb[1] / src1_type_size, (uint32_t)src1->nb[2] / src1_type_size, (uint32_t)src1->nb[3] / src1_type_size,\n@@ -9074,7 +9141,7 @@ static void ggml_vk_mul(ggml_backend_vk_context * ctx, vk_context& subctx, const\n     const uint32_t src1_type_size = ggml_type_size(src1->type);\n     const uint32_t dst_type_size = ggml_type_size(dst->type);\n \n-    ggml_vk_op_f32<vk_op_binary_push_constants>(ctx, subctx, src0, src1, nullptr, dst, GGML_OP_MUL, {\n+    ggml_vk_op_f32<vk_op_binary_push_constants>(ctx, subctx, src0, src1, nullptr, nullptr, dst, GGML_OP_MUL, {\n         (uint32_t)ggml_nelements(src0),\n         (uint32_t)src0->ne[0], (uint32_t)src0->ne[1], (uint32_t)src0->ne[2],(uint32_t)src0->ne[3], (uint32_t)src0->nb[0] / src0_type_size, (uint32_t)src0->nb[1] / src0_type_size, (uint32_t)src0->nb[2] / src0_type_size, (uint32_t)src0->nb[3] / src0_type_size,\n         (uint32_t)src1->ne[0], (uint32_t)src1->ne[1], (uint32_t)src1->ne[2],(uint32_t)src1->ne[3], (uint32_t)src1->nb[0] / src1_type_size, (uint32_t)src1->nb[1] / src1_type_size, (uint32_t)src1->nb[2] / src1_type_size, (uint32_t)src1->nb[3] / src1_type_size,\n@@ -9089,7 +9156,7 @@ static void ggml_vk_div(ggml_backend_vk_context * ctx, vk_context& subctx, const\n     const uint32_t src1_type_size = ggml_type_size(src1->type);\n     const uint32_t dst_type_size = ggml_type_size(dst->type);\n \n-    ggml_vk_op_f32<vk_op_binary_push_constants>(ctx, subctx, src0, src1, nullptr, dst, GGML_OP_DIV, {\n+    ggml_vk_op_f32<vk_op_binary_push_constants>(ctx, subctx, src0, src1, nullptr, nullptr, dst, GGML_OP_DIV, {\n         (uint32_t)ggml_nelements(src0),\n         (uint32_t)src0->ne[0], (uint32_t)src0->ne[1], (uint32_t)src0->ne[2],(uint32_t)src0->ne[3], (uint32_t)src0->nb[0] / src0_type_size, (uint32_t)src0->nb[1] / src0_type_size, (uint32_t)src0->nb[2] / src0_type_size, (uint32_t)src0->nb[3] / src0_type_size,\n         (uint32_t)src1->ne[0], (uint32_t)src1->ne[1], (uint32_t)src1->ne[2],(uint32_t)src1->ne[3], (uint32_t)src1->nb[0] / src1_type_size, (uint32_t)src1->nb[1] / src1_type_size, (uint32_t)src1->nb[2] / src1_type_size, (uint32_t)src1->nb[3] / src1_type_size,\n@@ -9104,7 +9171,7 @@ static void ggml_vk_add_id(ggml_backend_vk_context * ctx, vk_context& subctx, co\n     const uint32_t src1_type_size = ggml_type_size(src1->type);\n     const uint32_t src2_type_size = ggml_type_size(src2->type);\n \n-    ggml_vk_op_f32<vk_op_add_id_push_constants>(ctx, subctx, src0, src1, src2, dst, GGML_OP_ADD_ID, {\n+    ggml_vk_op_f32<vk_op_add_id_push_constants>(ctx, subctx, src0, src1, src2, nullptr, dst, GGML_OP_ADD_ID, {\n         (uint32_t)dst->ne[0],\n         (uint32_t)dst->ne[1],\n         (uint32_t)src0->nb[1] / src0_type_size,\n@@ -9337,7 +9404,7 @@ static void ggml_vk_ssm_conv(ggml_backend_vk_context * ctx, vk_context& subctx,\n     const ggml_tensor * src0 = dst->src[0];\n     const ggml_tensor * src1 = dst->src[1];\n \n-    ggml_vk_op_f32<vk_op_ssm_conv_push_constants>(ctx, subctx, src0, src1, nullptr, dst, GGML_OP_SSM_CONV, {\n+    ggml_vk_op_f32<vk_op_ssm_conv_push_constants>(ctx, subctx, src0, src1, nullptr, nullptr, dst, GGML_OP_SSM_CONV, {\n         (uint32_t)src0->nb[1], (uint32_t)src0->nb[2],\n         (uint32_t)src1->nb[1],\n         (uint32_t)dst->nb[0], (uint32_t)dst->nb[1], (uint32_t)dst->nb[2],\n@@ -9455,7 +9522,7 @@ static void ggml_vk_opt_step_adamw(ggml_backend_vk_context * ctx, vk_context& su\n static void ggml_vk_opt_step_sgd(ggml_backend_vk_context * ctx, vk_context& subctx, const ggml_tensor * src0, const ggml_tensor * src1, const ggml_tensor * src2, ggml_tensor * dst, bool dryrun = false) {\n     const size_t n = ggml_nelements(dst->src[0]);\n \n-    ggml_vk_op_f32<vk_op_push_constants>(ctx, subctx, src0, src1, src2, dst, GGML_OP_OPT_STEP_SGD, { (uint32_t)n, 0, 0.0f, 0.0f }, dryrun);\n+    ggml_vk_op_f32<vk_op_push_constants>(ctx, subctx, src0, src1, src2, nullptr, dst, GGML_OP_OPT_STEP_SGD, { (uint32_t)n, 0, 0.0f, 0.0f }, dryrun);\n }\n \n static void ggml_vk_concat(ggml_backend_vk_context * ctx, vk_context& subctx, const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst, bool dryrun = false) {\n@@ -9465,7 +9532,7 @@ static void ggml_vk_concat(ggml_backend_vk_context * ctx, vk_context& subctx, co\n     const uint32_t src1_type_size = ggml_type_size(src1->type);\n     const uint32_t dst_type_size = ggml_type_size(dst->type);\n \n-    ggml_vk_op_f32<vk_op_binary_push_constants>(ctx, subctx, src0, src1, nullptr, dst, GGML_OP_CONCAT, {\n+    ggml_vk_op_f32<vk_op_binary_push_constants>(ctx, subctx, src0, src1, nullptr, nullptr, dst, GGML_OP_CONCAT, {\n         (uint32_t)ggml_nelements(dst),\n         (uint32_t)src0->ne[0], (uint32_t)src0->ne[1], (uint32_t)src0->ne[2],(uint32_t)src0->ne[3], (uint32_t)src0->nb[0] / src0_type_size, (uint32_t)src0->nb[1] / src0_type_size, (uint32_t)src0->nb[2] / src0_type_size, (uint32_t)src0->nb[3] / src0_type_size,\n         (uint32_t)src1->ne[0], (uint32_t)src1->ne[1], (uint32_t)src1->ne[2],(uint32_t)src1->ne[3], (uint32_t)src1->nb[0] / src1_type_size, (uint32_t)src1->nb[1] / src1_type_size, (uint32_t)src1->nb[2] / src1_type_size, (uint32_t)src1->nb[3] / src1_type_size,\n@@ -9493,7 +9560,7 @@ static void ggml_vk_upscale(ggml_backend_vk_context * ctx, vk_context& subctx, c\n         pixel_offset = 0.0f;\n     }\n \n-    ggml_vk_op_f32<vk_op_upscale_push_constants>(ctx, subctx, src0, nullptr, nullptr, dst, GGML_OP_UPSCALE, {\n+    ggml_vk_op_f32<vk_op_upscale_push_constants>(ctx, subctx, src0, nullptr, nullptr, nullptr, dst, GGML_OP_UPSCALE, {\n         (uint32_t)ggml_nelements(dst), 0, 0,\n         (uint32_t)ne00, (uint32_t)ne01,\n         (uint32_t)nb00 / src0_type_size, (uint32_t)nb01 / src0_type_size, (uint32_t)nb02 / src0_type_size, (uint32_t)nb03 / src0_type_size,\n@@ -9507,36 +9574,36 @@ static void ggml_vk_scale(ggml_backend_vk_context * ctx, vk_context& subctx, con\n     p.param1 = ggml_get_op_params_f32(dst, 0);\n     p.param2 = ggml_get_op_params_f32(dst, 1);\n \n-    ggml_vk_op_f32(ctx, subctx, src0, nullptr, nullptr, dst, GGML_OP_SCALE, std::move(p), dryrun);\n+    ggml_vk_op_f32(ctx, subctx, src0, nullptr, nullptr, nullptr, dst, GGML_OP_SCALE, std::move(p), dryrun);\n }\n \n static void ggml_vk_sqr(ggml_backend_vk_context * ctx, vk_context& subctx, const ggml_tensor * src0, ggml_tensor * dst, bool dryrun = false) {\n-    ggml_vk_op_f32(ctx, subctx, src0, nullptr, nullptr, dst, GGML_OP_SQR, vk_op_unary_push_constants_init(src0, dst), dryrun);\n+    ggml_vk_op_f32(ctx, subctx, src0, nullptr, nullptr, nullptr, dst, GGML_OP_SQR, vk_op_unary_push_constants_init(src0, dst), dryrun);\n }\n \n static void ggml_vk_sqrt(ggml_backend_vk_context * ctx, vk_context& subctx, const ggml_tensor * src0, ggml_tensor * dst, bool dryrun = false) {\n-    ggml_vk_op_f32(ctx, subctx, src0, nullptr, nullptr, dst, GGML_OP_SQRT, vk_op_unary_push_constants_init(src0, dst), dryrun);\n+    ggml_vk_op_f32(ctx, subctx, src0, nullptr, nullptr, nullptr, dst, GGML_OP_SQRT, vk_op_unary_push_constants_init(src0, dst), dryrun);\n }\n \n static void ggml_vk_sin(ggml_backend_vk_context * ctx, vk_context& subctx, const ggml_tensor * src0, ggml_tensor * dst, bool dryrun = false) {\n-    ggml_vk_op_f32(ctx, subctx, src0, nullptr, nullptr, dst, GGML_OP_SIN, vk_op_unary_push_constants_init(src0, dst), dryrun);\n+    ggml_vk_op_f32(ctx, subctx, src0, nullptr, nullptr, nullptr, dst, GGML_OP_SIN, vk_op_unary_push_constants_init(src0, dst), dryrun);\n }\n \n static void ggml_vk_cos(ggml_backend_vk_context * ctx, vk_context& subctx, const ggml_tensor * src0, ggml_tensor * dst, bool dryrun = false) {\n-    ggml_vk_op_f32(ctx, subctx, src0, nullptr, nullptr, dst, GGML_OP_COS, vk_op_unary_push_constants_init(src0, dst), dryrun);\n+    ggml_vk_op_f32(ctx, subctx, src0, nullptr, nullptr, nullptr, dst, GGML_OP_COS, vk_op_unary_push_constants_init(src0, dst), dryrun);\n }\n \n static void ggml_vk_clamp(ggml_backend_vk_context * ctx, vk_context& subctx, const ggml_tensor * src0, ggml_tensor * dst, bool dryrun = false) {\n     vk_op_unary_push_constants p = vk_op_unary_push_constants_init(src0, dst);\n     p.param1 = ggml_get_op_params_f32(dst, 0);\n     p.param2 = ggml_get_op_params_f32(dst, 1);\n \n-    ggml_vk_op_f32(ctx, subctx, src0, nullptr, nullptr, dst, GGML_OP_CLAMP, std::move(p), dryrun);\n+    ggml_vk_op_f32(ctx, subctx, src0, nullptr, nullptr, nullptr, dst, GGML_OP_CLAMP, std::move(p), dryrun);\n }\n \n static void ggml_vk_pad(ggml_backend_vk_context * ctx, vk_context& subctx, const ggml_tensor * src0, ggml_tensor * dst, bool dryrun = false) {\n     vk_op_pad_push_constants p = vk_op_pad_push_constants_init(src0, dst);\n-    ggml_vk_op_f32(ctx, subctx, src0, nullptr, nullptr, dst, GGML_OP_PAD, std::move(p), dryrun);\n+    ggml_vk_op_f32(ctx, subctx, src0, nullptr, nullptr, nullptr, dst, GGML_OP_PAD, std::move(p), dryrun);\n }\n \n static void ggml_vk_roll(ggml_backend_vk_context * ctx, vk_context& subctx, const ggml_tensor * src0, ggml_tensor * dst, bool dryrun = false) {\n@@ -9551,17 +9618,17 @@ static void ggml_vk_roll(ggml_backend_vk_context * ctx, vk_context& subctx, cons\n     memcpy(&p.param1, &s01_packed, sizeof(float));\n     memcpy(&p.param2, &s23_packed, sizeof(float));\n \n-    ggml_vk_op_f32(ctx, subctx, src0, nullptr, nullptr, dst, GGML_OP_ROLL, std::move(p), dryrun);\n+    ggml_vk_op_f32(ctx, subctx, src0, nullptr, nullptr, nullptr, dst, GGML_OP_ROLL, std::move(p), dryrun);\n }\n \n static void ggml_vk_repeat(ggml_backend_vk_context * ctx, vk_context& subctx, const ggml_tensor * src0, ggml_tensor * dst, bool dryrun = false) {\n     vk_op_unary_push_constants p = vk_op_unary_push_constants_init(src0, dst, ggml_nelements(dst));\n-    ggml_vk_op_f32(ctx, subctx, src0, nullptr, nullptr, dst, GGML_OP_REPEAT, std::move(p), dryrun);\n+    ggml_vk_op_f32(ctx, subctx, src0, nullptr, nullptr, nullptr, dst, GGML_OP_REPEAT, std::move(p), dryrun);\n }\n \n static void ggml_vk_repeat_back(ggml_backend_vk_context * ctx, vk_context& subctx, const ggml_tensor * src0, ggml_tensor * dst, bool dryrun = false) {\n     vk_op_unary_push_constants p = vk_op_unary_push_constants_init(src0, dst, ggml_nelements(dst));\n-    ggml_vk_op_f32(ctx, subctx, src0, nullptr, nullptr, dst, GGML_OP_REPEAT_BACK, std::move(p), dryrun);\n+    ggml_vk_op_f32(ctx, subctx, src0, nullptr, nullptr, nullptr, dst, GGML_OP_REPEAT_BACK, std::move(p), dryrun);\n }\n \n static void ggml_vk_cpy(ggml_backend_vk_context * ctx, vk_context& subctx, const ggml_tensor * src0, ggml_tensor * dst, bool dryrun = false) {\n@@ -9577,7 +9644,7 @@ static void ggml_vk_cpy(ggml_backend_vk_context * ctx, vk_context& subctx, const\n     }\n \n     vk_op_unary_push_constants p = vk_op_unary_push_constants_init(src0, dst, ne);\n-    ggml_vk_op_f32(ctx, subctx, src0, nullptr, nullptr, dst, GGML_OP_CPY, std::move(p), dryrun);\n+    ggml_vk_op_f32(ctx, subctx, src0, nullptr, nullptr, nullptr, dst, GGML_OP_CPY, std::move(p), dryrun);\n }\n \n static void ggml_vk_set_rows(ggml_backend_vk_context * ctx, vk_context& subctx, const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst, bool dryrun = false) {\n@@ -9592,7 +9659,7 @@ static void ggml_vk_set_rows(ggml_backend_vk_context * ctx, vk_context& subctx,\n         return;\n     }\n \n-    ggml_vk_op_f32<vk_op_binary_push_constants>(ctx, subctx, src0, src1, nullptr, dst, GGML_OP_SET_ROWS, {\n+    ggml_vk_op_f32<vk_op_binary_push_constants>(ctx, subctx, src0, src1, nullptr, nullptr, dst, GGML_OP_SET_ROWS, {\n         (uint32_t)ggml_nelements(src0),\n         (uint32_t)src0->ne[0], (uint32_t)src0->ne[1], (uint32_t)src0->ne[2],(uint32_t)src0->ne[3], (uint32_t)src0->nb[0] / src0_type_size, (uint32_t)src0->nb[1] / src0_type_size, (uint32_t)src0->nb[2] / src0_type_size, (uint32_t)src0->nb[3] / src0_type_size,\n         (uint32_t)src1->ne[0], (uint32_t)src1->ne[1], (uint32_t)src1->ne[2],(uint32_t)src1->ne[3], (uint32_t)src1->nb[0] / src1_type_size, (uint32_t)src1->nb[1] / src1_type_size, (uint32_t)src1->nb[2] / src1_type_size, (uint32_t)src1->nb[3] / src1_type_size,\n@@ -9603,13 +9670,13 @@ static void ggml_vk_set_rows(ggml_backend_vk_context * ctx, vk_context& subctx,\n }\n \n static void ggml_vk_silu_back(ggml_backend_vk_context * ctx, vk_context& subctx, const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst, bool dryrun = false) {\n-    ggml_vk_op_f32<vk_op_push_constants>(ctx, subctx, src0, src1, nullptr, dst, GGML_OP_SILU_BACK, { (uint32_t)ggml_nelements(src0), 0, 0.0f, 0.0f }, dryrun);\n+    ggml_vk_op_f32<vk_op_push_constants>(ctx, subctx, src0, src1, nullptr, nullptr, dst, GGML_OP_SILU_BACK, { (uint32_t)ggml_nelements(src0), 0, 0.0f, 0.0f }, dryrun);\n }\n \n static void ggml_vk_norm(ggml_backend_vk_context * ctx, vk_context& subctx, const ggml_tensor * src0, ggml_tensor * dst, bool dryrun = false) {\n     float * op_params = (float *)dst->op_params;\n \n-    ggml_vk_op_f32<vk_op_push_constants>(ctx, subctx, src0, nullptr, nullptr, dst, GGML_OP_NORM, { (uint32_t)src0->ne[0], (uint32_t)src0->ne[1], op_params[0], 0.0f }, dryrun);\n+    ggml_vk_op_f32<vk_op_push_constants>(ctx, subctx, src0, nullptr, nullptr, nullptr, dst, GGML_OP_NORM, { (uint32_t)src0->ne[0], (uint32_t)src0->ne[1], op_params[0], 0.0f }, dryrun);\n }\n \n static void ggml_vk_group_norm(ggml_backend_vk_context * ctx, vk_context& subctx, const ggml_tensor * src0, ggml_tensor * dst, bool dryrun = false) {\n@@ -9620,7 +9687,7 @@ static void ggml_vk_group_norm(ggml_backend_vk_context * ctx, vk_context& subctx\n     const float eps = float_op_params[1];\n     const uint32_t group_size = src0->ne[0] * src0->ne[1] * ((src0->ne[2] + num_groups - 1) / num_groups);\n \n-    ggml_vk_op_f32<vk_op_push_constants>(ctx, subctx, src0, nullptr, nullptr, dst, GGML_OP_GROUP_NORM, { group_size, 0, eps, 0.0f }, dryrun);\n+    ggml_vk_op_f32<vk_op_push_constants>(ctx, subctx, src0, nullptr, nullptr, nullptr, dst, GGML_OP_GROUP_NORM, { group_size, 0, eps, 0.0f }, dryrun);\n }\n \n static uint32_t ggml_vk_rms_num_partials(ggml_backend_vk_context * ctx, const ggml_tensor *node) {\n@@ -9643,7 +9710,7 @@ static void ggml_vk_rms_norm(ggml_backend_vk_context * ctx, vk_context& subctx,\n \n     uint32_t param3 = ctx->do_add_rms_partials ? ggml_vk_rms_num_partials(ctx, dst) : 0;\n \n-    ggml_vk_op_f32<vk_op_binary_push_constants>(ctx, subctx, src0, src1, nullptr, dst, GGML_OP_RMS_NORM, {\n+    ggml_vk_op_f32<vk_op_binary_push_constants>(ctx, subctx, src0, src1, nullptr, nullptr, dst, GGML_OP_RMS_NORM, {\n         (uint32_t)ggml_nelements(src0),\n         (uint32_t)src0->ne[0], (uint32_t)src0->ne[1], (uint32_t)src0->ne[2],(uint32_t)src0->ne[3], (uint32_t)src0->nb[0] / src0_type_size, (uint32_t)src0->nb[1] / src0_type_size, (uint32_t)src0->nb[2] / src0_type_size, (uint32_t)src0->nb[3] / src0_type_size,\n         (uint32_t)src1->ne[0], (uint32_t)src1->ne[1], (uint32_t)src1->ne[2],(uint32_t)src1->ne[3], (uint32_t)src1->nb[0] / src1_type_size, (uint32_t)src1->nb[1] / src1_type_size, (uint32_t)src1->nb[2] / src1_type_size, (uint32_t)src1->nb[3] / src1_type_size,\n@@ -9660,16 +9727,16 @@ static void ggml_vk_rms_norm(ggml_backend_vk_context * ctx, vk_context& subctx,\n \n static void ggml_vk_rms_norm_back(ggml_backend_vk_context * ctx, vk_context& subctx, const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst, bool dryrun = false) {\n     float * op_params = (float *)dst->op_params;\n-    ggml_vk_op_f32<vk_op_push_constants>(ctx, subctx, src0, src1, nullptr, dst, GGML_OP_RMS_NORM_BACK, { (uint32_t)src0->ne[0], (uint32_t)src0->ne[1], op_params[0], 0.0f }, dryrun);\n+    ggml_vk_op_f32<vk_op_push_constants>(ctx, subctx, src0, src1, nullptr, nullptr, dst, GGML_OP_RMS_NORM_BACK, { (uint32_t)src0->ne[0], (uint32_t)src0->ne[1], op_params[0], 0.0f }, dryrun);\n }\n \n static void ggml_vk_l2_norm(ggml_backend_vk_context * ctx, vk_context& subctx, const ggml_tensor * src0, ggml_tensor * dst, bool dryrun = false) {\n     float * op_params = (float *)dst->op_params;\n-    ggml_vk_op_f32<vk_op_push_constants>(ctx, subctx, src0, nullptr, nullptr, dst, GGML_OP_L2_NORM, { (uint32_t)src0->ne[0], (uint32_t)src0->ne[1], op_params[0], 0.0f }, dryrun);\n+    ggml_vk_op_f32<vk_op_push_constants>(ctx, subctx, src0, nullptr, nullptr, nullptr, dst, GGML_OP_L2_NORM, { (uint32_t)src0->ne[0], (uint32_t)src0->ne[1], op_params[0], 0.0f }, dryrun);\n }\n \n static void ggml_vk_unary(ggml_backend_vk_context * ctx, vk_context& subctx, const ggml_tensor * src0, ggml_tensor * dst, bool dryrun = false) {\n-    ggml_vk_op_f32<vk_op_push_constants>(ctx, subctx, src0, nullptr, nullptr, dst, GGML_OP_UNARY, { (uint32_t)ggml_nelements(src0), 0, 0.0f, 0.0f }, dryrun);\n+    ggml_vk_op_f32<vk_op_push_constants>(ctx, subctx, src0, nullptr, nullptr, nullptr, dst, GGML_OP_UNARY, { (uint32_t)ggml_nelements(src0), 0, 0.0f, 0.0f }, dryrun);\n }\n \n static void ggml_vk_glu(ggml_backend_vk_context * ctx, vk_context& subctx, const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst, bool dryrun = false) {\n@@ -9692,7 +9759,7 @@ static void ggml_vk_glu(ggml_backend_vk_context * ctx, vk_context& subctx, const\n \n     const uint32_t mode = split ? 2 : (swapped ? 1 : 0);\n \n-    ggml_vk_op_f32<vk_op_glu_push_constants>(ctx, subctx, src0, src1, nullptr, dst, GGML_OP_GLU,\n+    ggml_vk_op_f32<vk_op_glu_push_constants>(ctx, subctx, src0, src1, nullptr, nullptr, dst, GGML_OP_GLU,\n         {\n             (uint32_t)ggml_nelements(dst),\n             (uint32_t)src0->ne[0],\n@@ -9705,7 +9772,7 @@ static void ggml_vk_glu(ggml_backend_vk_context * ctx, vk_context& subctx, const\n \n static void ggml_vk_diag_mask_inf(ggml_backend_vk_context * ctx, vk_context& subctx, const ggml_tensor * src0, ggml_tensor * dst, bool dryrun = false) {\n     int32_t * op_params = (int32_t *)dst->op_params;\n-    ggml_vk_op_f32<vk_op_diag_mask_push_constants>(ctx, subctx, src0, nullptr, nullptr, dst, GGML_OP_DIAG_MASK_INF, { (uint32_t)src0->ne[0], (uint32_t)src0->ne[1], op_params[0] }, dryrun);\n+    ggml_vk_op_f32<vk_op_diag_mask_push_constants>(ctx, subctx, src0, nullptr, nullptr, nullptr, dst, GGML_OP_DIAG_MASK_INF, { (uint32_t)src0->ne[0], (uint32_t)src0->ne[1], op_params[0] }, dryrun);\n }\n \n static void ggml_vk_soft_max(ggml_backend_vk_context * ctx, vk_context& subctx, const ggml_tensor * src0, const ggml_tensor * src1, const ggml_tensor * src2, ggml_tensor * dst, bool dryrun = false) {\n@@ -9730,7 +9797,7 @@ static void ggml_vk_soft_max(ggml_backend_vk_context * ctx, vk_context& subctx,\n     const float m0 = powf(2.0f, -(max_bias       ) / n_head_log2);\n     const float m1 = powf(2.0f, -(max_bias / 2.0f) / n_head_log2);\n \n-    ggml_vk_op_f32<vk_op_soft_max_push_constants>(ctx, subctx, src0, src1, src2, dst, GGML_OP_SOFT_MAX, {\n+    ggml_vk_op_f32<vk_op_soft_max_push_constants>(ctx, subctx, src0, src1, src2, nullptr, dst, GGML_OP_SOFT_MAX, {\n         ncols,\n         src1 != nullptr ? nrows_y : (uint32_t)0,\n         (uint32_t)src0->ne[0], (uint32_t)src0->ne[1], (uint32_t)src0->ne[2],\n@@ -9746,7 +9813,7 @@ static void ggml_vk_soft_max(ggml_backend_vk_context * ctx, vk_context& subctx,\n \n static void ggml_vk_soft_max_back(ggml_backend_vk_context * ctx, vk_context& subctx, const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst, bool dryrun = false) {\n     float * op_params = (float *)dst->op_params;\n-    ggml_vk_op_f32<vk_op_push_constants>(ctx, subctx, src0, src1, nullptr, dst, GGML_OP_SOFT_MAX_BACK, { (uint32_t)src0->ne[0], (uint32_t)ggml_nrows(src0), op_params[0], op_params[1] }, dryrun);\n+    ggml_vk_op_f32<vk_op_push_constants>(ctx, subctx, src0, src1, nullptr, nullptr, dst, GGML_OP_SOFT_MAX_BACK, { (uint32_t)src0->ne[0], (uint32_t)ggml_nrows(src0), op_params[0], op_params[1] }, dryrun);\n }\n \n static void ggml_vk_topk_moe(ggml_backend_vk_context * ctx, vk_context& subctx, ggml_cgraph * cgraph, int node_idx, bool dryrun = false) {\n@@ -9837,7 +9904,12 @@ static void ggml_vk_topk_moe(ggml_backend_vk_context * ctx, vk_context& subctx,\n         }, pc, elements);\n }\n \n-static void ggml_vk_rope(ggml_backend_vk_context * ctx, vk_context& subctx, const ggml_tensor * src0, const ggml_tensor * src1, const ggml_tensor * src2, ggml_tensor * dst, bool backprop, bool dryrun = false) {\n+static void ggml_vk_rope(ggml_backend_vk_context * ctx, vk_context& subctx, const ggml_cgraph * cgraph, int node_idx, bool backprop, bool dryrun = false) {\n+    ggml_tensor * dst = cgraph->nodes[node_idx];\n+    const ggml_tensor * src0 = dst->src[0];\n+    const ggml_tensor * src1 = dst->src[1];\n+    const ggml_tensor * src2 = dst->src[2];\n+    const ggml_tensor * src3 = nullptr;\n     const int n_dims        = ((int32_t *) dst->op_params)[1];\n     const int mode          = ((int32_t *) dst->op_params)[2];\n     // const int n_ctx         = ((int32_t *) dst->op_params)[3];\n@@ -9861,11 +9933,20 @@ static void ggml_vk_rope(ggml_backend_vk_context * ctx, vk_context& subctx, cons\n     uint32_t s1 = src0->nb[1] / ggml_type_size(src0->type);\n     uint32_t s2 = src0->nb[2] / ggml_type_size(src0->type);\n \n-    ggml_vk_op_f32<vk_op_rope_push_constants>(ctx, subctx, src0, src1, src2, dst, GGML_OP_ROPE, {\n+    uint32_t set_rows_stride = 0;\n+    // Fused rope + view + set_rows passes the set_rows destination stride in set_rows_stride\n+    // and overrides the dst and sets src3=row_indices\n+    if (ctx->num_additional_fused_ops > 0) {\n+        set_rows_stride = cgraph->nodes[node_idx + 2]->nb[1] / ggml_type_size(cgraph->nodes[node_idx + 2]->type);\n+        src3 = cgraph->nodes[node_idx + 2]->src[1];\n+        dst = cgraph->nodes[node_idx + 2];\n+    }\n+\n+    ggml_vk_op_f32<vk_op_rope_push_constants>(ctx, subctx, src0, src1, src2, src3, dst, GGML_OP_ROPE, {\n         (uint32_t)src0->ne[0], (uint32_t)n_dims, freq_scale, (uint32_t)src0->ne[1],\n         freq_base, ext_factor, attn_factor, {corr_dims[0], corr_dims[1]}, theta_scale,\n         src2 != nullptr, (uint32_t)src0->ne[2], s1, s2,\n-        { sections[0], sections[1], sections[2], sections[3] }, backprop\n+        { sections[0], sections[1], sections[2], sections[3] }, backprop, set_rows_stride,\n     }, dryrun);\n }\n \n@@ -9874,34 +9955,34 @@ static void ggml_vk_argsort(ggml_backend_vk_context * ctx, vk_context& subctx, c\n \n     uint32_t ncols = src0->ne[0];\n \n-    ggml_vk_op_f32<vk_op_argsort_push_constants>(ctx, subctx, src0, nullptr, nullptr, dst, GGML_OP_ARGSORT, {\n+    ggml_vk_op_f32<vk_op_argsort_push_constants>(ctx, subctx, src0, nullptr, nullptr, nullptr, dst, GGML_OP_ARGSORT, {\n         ncols,\n         op_params[0],\n     }, dryrun);\n }\n \n static void ggml_vk_sum(ggml_backend_vk_context * ctx, vk_context& subctx, const ggml_tensor * src0, ggml_tensor * dst, bool dryrun = false) {\n     vk_op_sum_rows_push_constants p = vk_op_sum_rows_push_constants_init(src0, dst, ggml_nelements(src0));\n-    ggml_vk_op_f32(ctx, subctx, src0, nullptr, nullptr, dst, GGML_OP_SUM, p, dryrun);\n+    ggml_vk_op_f32(ctx, subctx, src0, nullptr, nullptr, nullptr, dst, GGML_OP_SUM, p, dryrun);\n }\n \n static void ggml_vk_sum_rows(ggml_backend_vk_context * ctx, vk_context& subctx, const ggml_tensor * src0, ggml_tensor * dst, bool dryrun = false) {\n     vk_op_sum_rows_push_constants p = vk_op_sum_rows_push_constants_init(src0, dst, src0->ne[0]);\n-    ggml_vk_op_f32(ctx, subctx, src0, nullptr, nullptr, dst, GGML_OP_SUM_ROWS, p, dryrun);\n+    ggml_vk_op_f32(ctx, subctx, src0, nullptr, nullptr, nullptr, dst, GGML_OP_SUM_ROWS, p, dryrun);\n }\n \n static void ggml_vk_mean(ggml_backend_vk_context * ctx, vk_context& subctx, const ggml_tensor * src0, ggml_tensor * dst, bool dryrun = false) {\n     vk_op_sum_rows_push_constants p = vk_op_sum_rows_push_constants_init(src0, dst, src0->ne[0]);\n     p.weight = 1.0f / (float)src0->ne[0];\n-    ggml_vk_op_f32(ctx, subctx, src0, nullptr, nullptr, dst, GGML_OP_MEAN, p, dryrun);\n+    ggml_vk_op_f32(ctx, subctx, src0, nullptr, nullptr, nullptr, dst, GGML_OP_MEAN, p, dryrun);\n }\n \n static void ggml_vk_argmax(ggml_backend_vk_context * ctx, vk_context& subctx, const ggml_tensor * src0, ggml_tensor * dst, bool dryrun = false) {\n-    ggml_vk_op_f32<vk_op_push_constants>(ctx, subctx, src0, nullptr, nullptr, dst, GGML_OP_ARGMAX, { (uint32_t)src0->ne[0], (uint32_t)src0->ne[1], 0.0f, 0.0f }, dryrun);\n+    ggml_vk_op_f32<vk_op_push_constants>(ctx, subctx, src0, nullptr, nullptr, nullptr, dst, GGML_OP_ARGMAX, { (uint32_t)src0->ne[0], (uint32_t)src0->ne[1], 0.0f, 0.0f }, dryrun);\n }\n \n static void ggml_vk_count_equal(ggml_backend_vk_context * ctx, vk_context& subctx, const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst, bool dryrun = false) {\n-    ggml_vk_op_f32<vk_op_push_constants>(ctx, subctx, src0, src1, nullptr, dst, GGML_OP_COUNT_EQUAL, { (uint32_t)ggml_nelements(src0), 0, 0.0f, 0.0f }, dryrun);\n+    ggml_vk_op_f32<vk_op_push_constants>(ctx, subctx, src0, src1, nullptr, nullptr, dst, GGML_OP_COUNT_EQUAL, { (uint32_t)ggml_nelements(src0), 0, 0.0f, 0.0f }, dryrun);\n }\n \n static void ggml_vk_im2col(ggml_backend_vk_context * ctx, vk_context& subctx, const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst, bool dryrun = false) {\n@@ -9934,7 +10015,7 @@ static void ggml_vk_im2col(ggml_backend_vk_context * ctx, vk_context& subctx, co\n \n     const vk::DeviceAddress dst_addr = d_buf->bda_addr + vk_tensor_offset(dst) + dst->view_offs;\n \n-    ggml_vk_op_f32<vk_op_im2col_push_constants>(ctx, subctx, src0, src1, nullptr, dst, GGML_OP_IM2COL, {\n+    ggml_vk_op_f32<vk_op_im2col_push_constants>(ctx, subctx, src0, src1, nullptr, nullptr, dst, GGML_OP_IM2COL, {\n         dst_addr,\n         batch_offset, offset_delta,\n         IC, IW, IH, OW, OH, KW, KH,\n@@ -10007,15 +10088,15 @@ static void ggml_vk_im2col_3d(ggml_backend_vk_context * ctx, vk_context& subctx,\n     pc.OH_OW_IC_KD_KH_KW = OH*OW*IC*KD*KH*KW;\n     pc.OW_IC_KD_KH_KW = OW*IC*KD*KH*KW;\n \n-    ggml_vk_op_f32<vk_op_im2col_3d_push_constants>(ctx, subctx, src0, src1, nullptr, dst, GGML_OP_IM2COL_3D, std::move(pc), dryrun);\n+    ggml_vk_op_f32<vk_op_im2col_3d_push_constants>(ctx, subctx, src0, src1, nullptr, nullptr, dst, GGML_OP_IM2COL_3D, std::move(pc), dryrun);\n }\n \n static void ggml_vk_timestep_embedding(ggml_backend_vk_context * ctx, vk_context& subctx, const ggml_tensor * src0, ggml_tensor * dst, bool dryrun = false) {\n     const uint32_t dim = dst->op_params[0];\n     const uint32_t max_period = dst->op_params[1];\n     const uint32_t nb1 = dst->nb[1] / ggml_type_size(dst->type);\n \n-    ggml_vk_op_f32<vk_op_timestep_embedding_push_constants>(ctx, subctx, src0, nullptr, nullptr, dst, GGML_OP_TIMESTEP_EMBEDDING, {\n+    ggml_vk_op_f32<vk_op_timestep_embedding_push_constants>(ctx, subctx, src0, nullptr, nullptr, nullptr, dst, GGML_OP_TIMESTEP_EMBEDDING, {\n         nb1, dim, max_period,\n     }, dryrun);\n }\n@@ -10048,7 +10129,7 @@ static void ggml_vk_conv_transpose_1d(ggml_backend_vk_context * ctx, vk_context&\n     p.nb1 = static_cast<uint32_t>(nb1 / nb0);\n     p.s0 = static_cast<uint32_t>(s0);\n \n-    ggml_vk_op_f32(ctx, subctx, src0, src1, nullptr, dst, GGML_OP_CONV_TRANSPOSE_1D, std::move(p), dryrun);\n+    ggml_vk_op_f32(ctx, subctx, src0, src1, nullptr, nullptr, dst, GGML_OP_CONV_TRANSPOSE_1D, std::move(p), dryrun);\n }\n \n static void ggml_vk_pool_2d(ggml_backend_vk_context * ctx, vk_context& subctx, const ggml_tensor * src0, ggml_tensor * dst, bool dryrun = false) {\n@@ -10071,7 +10152,7 @@ static void ggml_vk_pool_2d(ggml_backend_vk_context * ctx, vk_context& subctx, c\n \n     const uint32_t parallel_elements = N * OC * OH * OW;\n \n-    ggml_vk_op_f32<vk_op_pool2d_push_constants>(ctx, subctx, src0, nullptr, nullptr, dst, GGML_OP_POOL_2D, {\n+    ggml_vk_op_f32<vk_op_pool2d_push_constants>(ctx, subctx, src0, nullptr, nullptr, nullptr, dst, GGML_OP_POOL_2D, {\n         IW, IH, OW, OH, OC,\n         parallel_elements,\n         op,\n@@ -10125,7 +10206,7 @@ static void ggml_vk_conv_2d(ggml_backend_vk_context * ctx, vk_context & subctx,\n     GGML_ASSERT(ne03 == ne2);\n     GGML_ASSERT(ne02 == ne12);\n \n-    ggml_vk_op_f32(ctx, subctx, src0, src1, nullptr, dst, GGML_OP_CONV_2D, std::move(p), dryrun);\n+    ggml_vk_op_f32(ctx, subctx, src0, src1, nullptr, nullptr, dst, GGML_OP_CONV_2D, std::move(p), dryrun);\n }\n \n static void ggml_vk_conv_transpose_2d(ggml_backend_vk_context * ctx, vk_context & subctx, const ggml_tensor * src0,\n@@ -10174,7 +10255,7 @@ static void ggml_vk_conv_transpose_2d(ggml_backend_vk_context * ctx, vk_context\n     GGML_ASSERT(ne02 == ne2);\n     GGML_ASSERT(ne03 == ne12);\n \n-    ggml_vk_op_f32(ctx, subctx, src0, src1, nullptr, dst, GGML_OP_CONV_TRANSPOSE_2D, std::move(p), dryrun);\n+    ggml_vk_op_f32(ctx, subctx, src0, src1, nullptr, nullptr, dst, GGML_OP_CONV_TRANSPOSE_2D, std::move(p), dryrun);\n }\n \n static void ggml_vk_conv_2d_dw(ggml_backend_vk_context * ctx, vk_context& subctx, const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst, bool dryrun = false) {\n@@ -10198,12 +10279,12 @@ static void ggml_vk_conv_2d_dw(ggml_backend_vk_context * ctx, vk_context& subctx\n     GGML_ASSERT(src0->ne[3] == p.channels);\n     GGML_ASSERT(src1->ne[3] == p.batches);\n \n-    ggml_vk_op_f32(ctx, subctx, src0, src1, nullptr, dst, GGML_OP_CONV_2D_DW, std::move(p), dryrun);\n+    ggml_vk_op_f32(ctx, subctx, src0, src1, nullptr, nullptr, dst, GGML_OP_CONV_2D_DW, std::move(p), dryrun);\n }\n \n static void ggml_vk_leaky_relu(ggml_backend_vk_context * ctx, vk_context& subctx, const ggml_tensor * src0, ggml_tensor * dst, bool dryrun = false) {\n     const float * op_params = (const float *)dst->op_params;\n-    ggml_vk_op_f32<vk_op_push_constants>(ctx, subctx, src0, nullptr, nullptr, dst, GGML_OP_LEAKY_RELU, { (uint32_t)ggml_nelements(src0), 0, op_params[0], 0.0f }, dryrun);\n+    ggml_vk_op_f32<vk_op_push_constants>(ctx, subctx, src0, nullptr, nullptr, nullptr, dst, GGML_OP_LEAKY_RELU, { (uint32_t)ggml_nelements(src0), 0, op_params[0], 0.0f }, dryrun);\n }\n \n #ifdef GGML_VULKAN_RUN_TESTS\n@@ -11329,7 +11410,6 @@ static bool ggml_vk_build_graph(ggml_backend_vk_context * ctx, ggml_cgraph * cgr\n         case GGML_OP_DIAG_MASK_INF:\n         case GGML_OP_SOFT_MAX:\n         case GGML_OP_SOFT_MAX_BACK:\n-        case GGML_OP_ROPE:\n         case GGML_OP_ROPE_BACK:\n         case GGML_OP_ARGSORT:\n         case GGML_OP_SUM:\n@@ -11403,9 +11483,12 @@ static bool ggml_vk_build_graph(ggml_backend_vk_context * ctx, ggml_cgraph * cgr\n         // nodes require synchronization.\n         for (int32_t i = 0; i < ctx->num_additional_fused_ops + 1 && !need_sync; ++i) {\n             const ggml_tensor *cur_node = cgraph->nodes[node_idx + i];\n-            if (overlaps_unsynced(cur_node, ctx->unsynced_nodes_read) || overlaps_unsynced(cur_node, ctx->unsynced_nodes_written)) {\n-                need_sync = true;\n-                break;\n+            // If the node actually writes to memory, then check if it needs to sync\n+            if (ctx->fused_ops_write_mask & (1 << i)) {\n+                if (overlaps_unsynced(cur_node, ctx->unsynced_nodes_read) || overlaps_unsynced(cur_node, ctx->unsynced_nodes_written)) {\n+                    need_sync = true;\n+                    break;\n+                }\n             }\n             for (uint32_t j = 0; j < GGML_MAX_SRC; ++j) {\n                 if (!cur_node->src[j]) {\n@@ -11432,7 +11515,9 @@ static bool ggml_vk_build_graph(ggml_backend_vk_context * ctx, ggml_cgraph * cgr\n         for (int32_t i = 0; i < ctx->num_additional_fused_ops + 1; ++i) {\n             const ggml_tensor *cur_node = cgraph->nodes[node_idx + i];\n             // Multiple outputs could be written, e.g. in topk_moe. Add them all to the list.\n-            ctx->unsynced_nodes_written.push_back(cur_node);\n+            if (ctx->fused_ops_write_mask & (1 << i)) {\n+                ctx->unsynced_nodes_written.push_back(cur_node);\n+            }\n             for (uint32_t j = 0; j < GGML_MAX_SRC; ++j) {\n                 if (!cur_node->src[j]) {\n                     continue;\n@@ -11623,11 +11708,11 @@ static bool ggml_vk_build_graph(ggml_backend_vk_context * ctx, ggml_cgraph * cgr\n \n         break;\n     case GGML_OP_ROPE:\n-        ggml_vk_rope(ctx, compute_ctx, src0, src1, src2, node, false, dryrun);\n+        ggml_vk_rope(ctx, compute_ctx, cgraph, node_idx, false, dryrun);\n \n         break;\n     case GGML_OP_ROPE_BACK:\n-        ggml_vk_rope(ctx, compute_ctx, src0, src1, src2, node, true, dryrun);\n+        ggml_vk_rope(ctx, compute_ctx, cgraph, node_idx, true, dryrun);\n \n         break;\n     case GGML_OP_ARGSORT:\n@@ -12464,6 +12549,41 @@ static bool ggml_vk_can_fuse_topk_moe(ggml_backend_vk_context * ctx, const struc\n     return true;\n }\n \n+static bool ggml_vk_can_fuse_rope_set_rows(ggml_backend_vk_context * ctx, const struct ggml_cgraph * cgraph,\n+                                           int node_idx) {\n+    GGML_UNUSED(ctx);\n+    const ggml_tensor *rope = cgraph->nodes[node_idx + 0];\n+    const ggml_tensor *view = cgraph->nodes[node_idx + 1];\n+    const ggml_tensor *set_rows = cgraph->nodes[node_idx + 2];\n+\n+    // ne3 not tested\n+    if (rope->src[0]->ne[3] != 1) {\n+        return false;\n+    }\n+\n+    if (set_rows->type != GGML_TYPE_F32 && set_rows->type != GGML_TYPE_F16) {\n+        return false;\n+    }\n+\n+    if (set_rows->src[1]->type != GGML_TYPE_I64) {\n+        return false;\n+    }\n+\n+    // The view should flatten two dims of rope into one dim\n+    if (!ggml_is_contiguous(view) ||\n+        view->ne[0] != rope->ne[0] * rope->ne[1]) {\n+        return false;\n+    }\n+\n+    // Only norm/neox shaders have the fusion code\n+    const int mode = ((const int32_t *) rope->op_params)[2];\n+    if (mode != GGML_ROPE_TYPE_NORMAL && mode != GGML_ROPE_TYPE_NEOX) {\n+        return false;\n+    }\n+\n+    return true;\n+}\n+\n static uint32_t ggml_vk_fuse_multi_add(ggml_backend_vk_context * ctx, const struct ggml_cgraph * cgraph, int node_idx) {\n \n     const ggml_tensor *first_node = cgraph->nodes[node_idx];\n@@ -12539,6 +12659,10 @@ static ggml_status ggml_backend_vk_graph_compute(ggml_backend_t backend, ggml_cg\n                 ctx->num_additional_fused_ops = num_adds - 1;\n             } else if (ggml_vk_can_fuse(cgraph, i, { GGML_OP_RMS_NORM, GGML_OP_MUL })) {\n                 ctx->num_additional_fused_ops = 1;\n+            } else if (ggml_can_fuse_subgraph(cgraph, i, { GGML_OP_ROPE, GGML_OP_VIEW, GGML_OP_SET_ROWS }, { i + 2 }) &&\n+                       ggml_check_edges(cgraph, i, rope_view_set_rows_edges) &&\n+                       ggml_vk_can_fuse_rope_set_rows(ctx, cgraph, i)) {\n+                ctx->num_additional_fused_ops = 2;\n             } else if (ggml_can_fuse_subgraph(cgraph, i, topk_moe_early_softmax_norm, { i + 3, i + 9 }) &&\n                        ggml_check_edges(cgraph, i, topk_moe_early_softmax_norm_edges) &&\n                        ggml_vk_can_fuse_topk_moe(ctx, cgraph, i, TOPK_MOE_EARLY_SOFTMAX_NORM)) {\n@@ -12648,20 +12772,31 @@ static ggml_status ggml_backend_vk_graph_compute(ggml_backend_t backend, ggml_cg\n                 ctx->num_additional_fused_ops = num_adds - 1;\n             } else if (ggml_vk_can_fuse(cgraph, i, { GGML_OP_RMS_NORM, GGML_OP_MUL })) {\n                 ctx->num_additional_fused_ops = 1;\n+            } else if (ggml_can_fuse_subgraph(cgraph, i, { GGML_OP_ROPE, GGML_OP_VIEW, GGML_OP_SET_ROWS }, { i + 2 }) &&\n+                       ggml_check_edges(cgraph, i, rope_view_set_rows_edges) &&\n+                       ggml_vk_can_fuse_rope_set_rows(ctx, cgraph, i)) {\n+                ctx->num_additional_fused_ops = 2;\n             } else if (ggml_can_fuse_subgraph(cgraph, i, topk_moe_early_softmax_norm, { i + 3, i + 9 }) &&\n                        ggml_check_edges(cgraph, i, topk_moe_early_softmax_norm_edges) &&\n                        ggml_vk_can_fuse_topk_moe(ctx, cgraph, i, TOPK_MOE_EARLY_SOFTMAX_NORM)) {\n                 ctx->num_additional_fused_ops = topk_moe_early_softmax_norm.size() - 1;\n+                // view of argsort writes to memory\n+                ctx->fused_ops_write_mask |= 1 << 3;\n             } else if (ggml_can_fuse_subgraph(cgraph, i, topk_moe_early_softmax, { i + 3, i + 4 }) &&\n                        ggml_check_edges(cgraph, i, topk_moe_early_softmax_edges) &&\n                        ggml_vk_can_fuse_topk_moe(ctx, cgraph, i, TOPK_MOE_EARLY_SOFTMAX)) {\n                 ctx->num_additional_fused_ops = topk_moe_early_softmax.size() - 1;\n+                // view of argsort writes to memory\n+                ctx->fused_ops_write_mask |= 1 << 3;\n             } else if (ggml_can_fuse_subgraph(cgraph, i, topk_moe_late_softmax, { i + 1, i + 5 }) &&\n                        ggml_check_edges(cgraph, i, topk_moe_late_softmax_edges) &&\n                        ggml_vk_can_fuse_topk_moe(ctx, cgraph, i, TOPK_MOE_LATE_SOFTMAX)) {\n                 ctx->num_additional_fused_ops = topk_moe_late_softmax.size() - 1;\n+                // view of argsort writes to memory\n+                ctx->fused_ops_write_mask |= 1 << 1;\n             }\n         }\n+        ctx->fused_ops_write_mask |= 1 << ctx->num_additional_fused_ops;\n \n         // Signal the almost_ready fence when the graph is mostly complete (< 20% remaining)\n         bool almost_ready = (cgraph->n_nodes - i) < cgraph->n_nodes / 5;\n@@ -12707,6 +12842,7 @@ static ggml_status ggml_backend_vk_graph_compute(ggml_backend_t backend, ggml_cg\n         }\n         i += ctx->num_additional_fused_ops;\n         ctx->num_additional_fused_ops = 0;\n+        ctx->fused_ops_write_mask = 0;\n     }\n \n     if (vk_perf_logger_enabled) {\n@@ -12863,6 +12999,32 @@ static void ggml_vk_graph_optimize(ggml_backend_t backend, struct ggml_cgraph *\n             }\n             if (ok) {\n                 current_set.push_back(j);\n+                // Look for ROPE + VIEW + SET_ROWS and make them consecutive\n+                if (graph->nodes[j]->op == GGML_OP_ROPE) {\n+                    int view_idx = -1;\n+                    int set_rows_idx = -1;\n+                    for (int k = j+1; k < std::min(j + 10, graph->n_nodes); ++k) {\n+                        if (view_idx == -1 &&\n+                            graph->nodes[k]->op == GGML_OP_VIEW &&\n+                            graph->nodes[k]->src[0] == graph->nodes[j]) {\n+                            view_idx = k;\n+                            continue;\n+                        }\n+                        if (view_idx != -1 &&\n+                            set_rows_idx == -1 &&\n+                            graph->nodes[k]->op == GGML_OP_SET_ROWS &&\n+                            graph->nodes[k]->src[0] == graph->nodes[view_idx]) {\n+                            set_rows_idx = k;\n+                            break;\n+                        }\n+                    }\n+                    if (set_rows_idx != -1) {\n+                        current_set.push_back(view_idx);\n+                        current_set.push_back(set_rows_idx);\n+                        used[view_idx] = true;\n+                        used[set_rows_idx] = true;\n+                    }\n+                }\n             }\n         }\n         // Second pass grabs view nodes."
      },
      {
        "filename": "ggml/src/ggml-vulkan/vulkan-shaders/rope_head.glsl",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -10,6 +10,7 @@ layout (binding = 0) readonly buffer X {A_TYPE data_a[];};\n layout (binding = 1) readonly buffer Y {int data_pos[];};\n layout (binding = 2) readonly buffer Z {float data_ff[];};\n layout (binding = 3) writeonly buffer D {D_TYPE data_d[];};\n+layout (binding = 4) readonly buffer I {uvec2 data_i[];}; // indices for set_rows\n \n layout (push_constant) uniform parameter {\n     uint ncols;\n@@ -27,6 +28,7 @@ layout (push_constant) uniform parameter {\n     uint s2;\n     int sections[4];\n     uint is_back;\n+    uint set_rows_stride;\n } p;\n \n float rope_yarn_ramp(const float low, const float high, const uint i0) {"
      },
      {
        "filename": "ggml/src/ggml-vulkan/vulkan-shaders/rope_neox.comp",
        "status": "modified",
        "additions": 10,
        "deletions": 3,
        "changes": 13,
        "patch": "@@ -16,12 +16,19 @@ void main() {\n     const uint row_x     = row_dst % ne1;\n     const uint channel_x = row_dst / ne1;\n \n-    const uint idst = row_dst*ne0 + i0/2;\n+    uint idst = row_dst*ne0 + i0/2;\n     const uint ix   = channel_x*p.s2 + row_x*p.s1 + i0/2;\n \n+    // Fusion optimization: ROPE + VIEW + SET_ROWS..\n+    // The rope output is viewed as a 1D tensor and offset based on a row index in data_i.\n+    if (p.set_rows_stride != 0) {\n+        idst = row_x*ne0 + i0/2;\n+        idst += data_i[channel_x].x * p.set_rows_stride;\n+    }\n+\n     if (i0 >= p.n_dims) {\n-        data_d[idst + i0/2 + 0] = data_a[ix + i0/2 + 0];\n-        data_d[idst + i0/2 + 1] = data_a[ix + i0/2 + 1];\n+        data_d[idst + i0/2 + 0] = D_TYPE(data_a[ix + i0/2 + 0]);\n+        data_d[idst + i0/2 + 1] = D_TYPE(data_a[ix + i0/2 + 1]);\n \n         return;\n     }"
      },
      {
        "filename": "ggml/src/ggml-vulkan/vulkan-shaders/rope_norm.comp",
        "status": "modified",
        "additions": 10,
        "deletions": 3,
        "changes": 13,
        "patch": "@@ -16,12 +16,19 @@ void main() {\n     const uint row_x     = row_dst % ne1;\n     const uint channel_x = row_dst / ne1;\n \n-    const uint idst = row_dst*ne0 + i0;\n+    uint idst = row_dst*ne0 + i0;\n     const uint ix   = channel_x*p.s2 + row_x*p.s1 + i0;\n \n+    // Fusion optimization: ROPE + VIEW + SET_ROWS..\n+    // The rope output is viewed as a 1D tensor and offset based on a row index in data_i.\n+    if (p.set_rows_stride != 0) {\n+        idst = row_x*ne0 + i0;\n+        idst += data_i[channel_x].x * p.set_rows_stride;\n+    }\n+\n     if (i0 >= p.n_dims) {\n-        data_d[idst + 0] = data_a[ix + 0];\n-        data_d[idst + 1] = data_a[ix + 1];\n+        data_d[idst + 0] = D_TYPE(data_a[ix + 0]);\n+        data_d[idst + 1] = D_TYPE(data_a[ix + 1]);\n \n         return;\n     }"
      },
      {
        "filename": "ggml/src/ggml-vulkan/vulkan-shaders/vulkan-shaders-gen.cpp",
        "status": "modified",
        "additions": 4,
        "deletions": 0,
        "changes": 4,
        "patch": "@@ -842,10 +842,14 @@ void process_shaders() {\n     string_to_spv(\"rope_norm_f32\", \"rope_norm.comp\", {{\"A_TYPE\", \"float\"}, {\"D_TYPE\", \"float\"}});\n     string_to_spv(\"rope_norm_f16\", \"rope_norm.comp\", {{\"A_TYPE\", \"float16_t\"}, {\"D_TYPE\", \"float16_t\"}});\n     string_to_spv(\"rope_norm_f16_rte\", \"rope_norm.comp\", {{\"A_TYPE\", \"float16_t\"}, {\"D_TYPE\", \"float16_t\"}, {\"RTE16\", \"1\"}});\n+    string_to_spv(\"rope_norm_f32_f16\", \"rope_norm.comp\", {{\"A_TYPE\", \"float\"}, {\"D_TYPE\", \"float16_t\"}});\n+    string_to_spv(\"rope_norm_f32_f16_rte\", \"rope_norm.comp\", {{\"A_TYPE\", \"float\"}, {\"D_TYPE\", \"float16_t\"}, {\"RTE16\", \"1\"}});\n \n     string_to_spv(\"rope_neox_f32\", \"rope_neox.comp\", {{\"A_TYPE\", \"float\"}, {\"D_TYPE\", \"float\"}});\n     string_to_spv(\"rope_neox_f16\", \"rope_neox.comp\", {{\"A_TYPE\", \"float16_t\"}, {\"D_TYPE\", \"float16_t\"}});\n     string_to_spv(\"rope_neox_f16_rte\", \"rope_neox.comp\", {{\"A_TYPE\", \"float16_t\"}, {\"D_TYPE\", \"float16_t\"}, {\"RTE16\", \"1\"}});\n+    string_to_spv(\"rope_neox_f32_f16\", \"rope_neox.comp\", {{\"A_TYPE\", \"float\"}, {\"D_TYPE\", \"float16_t\"}});\n+    string_to_spv(\"rope_neox_f32_f16_rte\", \"rope_neox.comp\", {{\"A_TYPE\", \"float\"}, {\"D_TYPE\", \"float16_t\"}, {\"RTE16\", \"1\"}});\n \n     string_to_spv(\"rope_multi_f32\", \"rope_multi.comp\", {{\"A_TYPE\", \"float\"}, {\"D_TYPE\", \"float\"}});\n     string_to_spv(\"rope_multi_f16\", \"rope_multi.comp\", {{\"A_TYPE\", \"float16_t\"}, {\"D_TYPE\", \"float16_t\"}});"
      },
      {
        "filename": "tests/test-backend-ops.cpp",
        "status": "modified",
        "additions": 97,
        "deletions": 25,
        "changes": 122,
        "patch": "@@ -2125,6 +2125,34 @@ struct test_get_rows_back : public test_case {\n     }\n };\n \n+static void init_set_rows_row_ids(ggml_tensor * t, int num_rows) {\n+    std::random_device rd;\n+    std::default_random_engine rng(rd());\n+    for (int i2 = 0; i2 < t->ne[2]; i2++) {\n+        for (int i1 = 0; i1 < t->ne[1]; i1++) {\n+            // generate a shuffled subset of row indices\n+            std::vector<int64_t> data(num_rows);\n+            for (int i = 0; i < num_rows; i++) {\n+                data[i] = i;\n+            }\n+            std::shuffle(data.begin(), data.end(), rng);\n+            data.resize(t->ne[0]);\n+\n+            const size_t offs = i1*t->nb[1] + i2*t->nb[2];\n+            if (t->type == GGML_TYPE_I32) {\n+                // TODO: Make a template or something\n+                std::vector<int32_t> data_i32(t->ne[0]);\n+                for (int i = 0; i < t->ne[0]; i++) {\n+                    data_i32[i] = static_cast<int32_t>(data[i]);\n+                }\n+                ggml_backend_tensor_set(t, data_i32.data(), offs, t->ne[0]*sizeof(int32_t));\n+            } else {\n+                ggml_backend_tensor_set(t, data.data(), offs, t->ne[0]*sizeof(int64_t));\n+            }\n+        }\n+    }\n+}\n+\n // GGML_OP_SET_ROWS\n struct test_set_rows : public test_case {\n     const ggml_type type;\n@@ -2168,37 +2196,13 @@ struct test_set_rows : public test_case {\n     }\n \n     void initialize_tensors(ggml_context * ctx) override {\n-        std::random_device rd;\n-        std::default_random_engine rng(rd());\n         for (ggml_tensor * t = ggml_get_first_tensor(ctx); t != NULL; t = ggml_get_next_tensor(ctx, t)) {\n             if (t->type == GGML_TYPE_I64 || t->type == GGML_TYPE_I32) {\n                 if (ggml_is_view_op(t->op)) {\n                     continue;\n                 }\n \n-                for (int i2 = 0; i2 < t->ne[2]; i2++) {\n-                    for (int i1 = 0; i1 < t->ne[1]; i1++) {\n-                        // generate a shuffled subset of row indices\n-                        std::vector<int64_t> data(ne[1]);\n-                        for (int i = 0; i < ne[1]; i++) {\n-                            data[i] = i;\n-                        }\n-                        std::shuffle(data.begin(), data.end(), rng);\n-                        data.resize(t->ne[0]);\n-\n-                        const size_t offs = i1*t->nb[1] + i2*t->nb[2];\n-                        if (t->type == GGML_TYPE_I32) {\n-                            // TODO: Make a template or something\n-                            std::vector<int32_t> data_i32(t->ne[0]);\n-                            for (int i = 0; i < t->ne[0]; i++) {\n-                                data_i32[i] = static_cast<int32_t>(data[i]);\n-                            }\n-                            ggml_backend_tensor_set(t, data_i32.data(), offs, t->ne[0]*sizeof(int32_t));\n-                        } else {\n-                            ggml_backend_tensor_set(t, data.data(), offs, t->ne[0]*sizeof(int64_t));\n-                        }\n-                    }\n-                }\n+                init_set_rows_row_ids(t, ne[1]);\n             } else {\n                 init_tensor_uniform(t);\n             }\n@@ -2227,6 +2231,67 @@ struct test_set_rows : public test_case {\n     }\n };\n \n+// GGML_OP_ROPE + GGML_OP_VIEW + GGML_OP_SET_ROWS\n+struct test_rope_set_rows : public test_case {\n+    const ggml_type type;\n+    const ggml_type type_idx;\n+    const std::array<int64_t, 4> ne;\n+    int mode;\n+\n+    std::string vars() override {\n+        return VARS_TO_STR4(type, type_idx, ne, mode);\n+    }\n+\n+    std::string op_desc(ggml_tensor * t) override {\n+        GGML_UNUSED(t);\n+        return \"ROPE_SET_ROWS\";\n+    }\n+\n+    bool run_whole_graph() override { return true; }\n+\n+    test_rope_set_rows(ggml_type type,\n+            ggml_type type_idx,\n+            std::array<int64_t, 4> ne,\n+            int mode)\n+        : type(type), type_idx(type_idx), ne(ne), mode(mode) {}\n+\n+    ggml_tensor * build_graph(ggml_context * ctx) override {\n+        ggml_tensor * src = ggml_new_tensor_4d(ctx, GGML_TYPE_F32, ne[0], ne[1], ne[2], 1);\n+        ggml_set_name(src, \"src\");\n+\n+        ggml_tensor * pos = ggml_new_tensor_1d(ctx, GGML_TYPE_I32, ne[2]);\n+\n+        ggml_tensor * rope = ggml_rope(ctx, src, pos, ne[0], mode);\n+\n+        ggml_tensor * view = ggml_view_2d(ctx, rope, ne[0] * ne[1], ne[2], rope->nb[2], 0);\n+\n+        ggml_tensor * dst = ggml_new_tensor_4d(ctx, type, ne[0] * ne[1], ne[2] * ne[3], 1, 1);\n+        ggml_set_name(dst, \"dst\");\n+\n+        ggml_tensor * row_idxs = ggml_new_tensor_3d(ctx, type_idx, ne[2], 1, 1);\n+        ggml_set_name(row_idxs, \"row_idxs\");\n+\n+        ggml_tensor * out = ggml_set_rows(ctx, dst, view, row_idxs);\n+        ggml_set_name(out, \"out\");\n+\n+        return out;\n+    }\n+\n+    void initialize_tensors(ggml_context * ctx) override {\n+        for (ggml_tensor * t = ggml_get_first_tensor(ctx); t != NULL; t = ggml_get_next_tensor(ctx, t)) {\n+            if (t->type == GGML_TYPE_I64 || t->type == GGML_TYPE_I32) {\n+                if (ggml_is_view_op(t->op)) {\n+                    continue;\n+                }\n+\n+                init_set_rows_row_ids(t, ne[2]);\n+            } else {\n+                init_tensor_uniform(t);\n+            }\n+        }\n+    }\n+};\n+\n // GGML_OP_ARGMAX\n struct test_argmax : public test_case {\n     const ggml_type type;\n@@ -6163,6 +6228,13 @@ static std::vector<std::unique_ptr<test_case>> make_test_cases_eval() {\n         }\n     }\n \n+    for (int mode : { GGML_ROPE_TYPE_NORMAL, GGML_ROPE_TYPE_NEOX }) {\n+        for (ggml_type type : {GGML_TYPE_F16, GGML_TYPE_F32}) {\n+            test_cases.emplace_back(new test_rope_set_rows(type, GGML_TYPE_I64, { 128, 32, 1, 100 }, mode));\n+            test_cases.emplace_back(new test_rope_set_rows(type, GGML_TYPE_I64, { 128, 32, 512, 1 }, mode));\n+        }\n+    }\n+\n     for (ggml_type type_input : {GGML_TYPE_F32}) {\n         for (ggml_op_pool pool_type : {GGML_OP_POOL_AVG, GGML_OP_POOL_MAX}) {\n             for (int k0 : {1, 3}) {"
      }
    ],
    "num_files": 6,
    "scraped_at": "2025-11-16T23:29:33.850051"
  },
  {
    "pr_number": 16764,
    "title": "model : add LightOnOCR-1B model",
    "body": "Seems like the \"OCR model race\" has started. This seems to be one of the few \"low hanging fruits\" that we can easily support in llama.cpp\r\n\r\nThe model features:\r\n- Qwen3 as language model\r\n- Mistral3 as vision encoder (the difference is that LightOnOCR does not use `[IMG_BREAK]` token)\r\n\r\nOriginal model: https://huggingface.co/lightonai/LightOnOCR-1B-1025\r\n\r\nGGUF model: https://huggingface.co/ggml-org/LightOnOCR-1B-1025-GGUF\r\n\r\nTo try it:\r\n\r\n```sh\r\nllama-server -hf ggml-org/LightOnOCR-1B-1025-GGUF -c 8192\r\n\r\n# open https://localhost:8080 and try uploading an image\r\n```\r\n\r\n---\r\n\r\nImportant note: this model requires specific input structure, see the [chat template](https://huggingface.co/lightonai/LightOnOCR-1B-1025/resolve/main/chat_template.jinja)\r\n\r\nThe structure seems to be:\r\n- Starts with an empty system message\r\n- Then, an user message. All images must be contained in this message; No instructions are needed\r\n\r\nExample:\r\n\r\n```json\r\n{\r\n  \"messages\": [{\r\n    \"role\": \"system\",\r\n    \"content\": \"\"\r\n  }, {\r\n    \"role\": \"user\",\r\n    \"content\": [{\r\n      \"type\": \"image_url\",\r\n      \"image_url\": {\"url\": \"data:image/png;base64,......\"}\r\n    }]\r\n  }],\r\n}\r\n```",
    "html_url": "https://github.com/ggml-org/llama.cpp/pull/16764",
    "created_at": "2025-10-24T23:15:35Z",
    "merged_at": "2025-10-27T15:02:58Z",
    "merge_commit_sha": "c55d53acec864f64afa1ba92972203dce1bf88f5",
    "base_ref": "master",
    "head_sha": "4718203a63a00bbbcb6df82650fa38ad6483296e",
    "user": "ngxson",
    "files": [
      {
        "filename": "convert_hf_to_gguf.py",
        "status": "modified",
        "additions": 24,
        "deletions": 2,
        "changes": 26,
        "patch": "@@ -2460,18 +2460,21 @@ def set_gguf_parameters(self):\n )\n class LlavaVisionModel(MmprojModel):\n     img_break_tok_id = -1\n+    use_break_tok = True\n \n     def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n         if self.hparams.get(\"model_type\") == \"pixtral\":\n             # layer_norm_eps is not in config.json, it is hard-coded in modeling_pixtral.py\n             self.hparams[\"layer_norm_eps\"] = self.hparams.get(\"layer_norm_eps\", 1e-5)\n-            self.img_break_tok_id = self.get_token_id(\"[IMG_BREAK]\")\n+            if self.use_break_tok:\n+                self.img_break_tok_id = self.get_token_id(\"[IMG_BREAK]\")\n         elif self.is_mistral_format:\n             # hparams is already vision config here so norm_eps is only defined in global_config.\n             self.hparams[\"norm_eps\"] = self.global_config.get(\"norm_eps\", None)\n             assert self.hparams[\"norm_eps\"] is not None, \"norm_eps not found in params.json\"\n-            self.img_break_tok_id = self.find_vparam([\"image_break_token_id\"])\n+            if self.use_break_tok:\n+                self.img_break_tok_id = self.find_vparam([\"image_break_token_id\"])\n         else:\n             raise ValueError(f\"Unsupported model type: {self.hparams['model_type']}\")\n         logger.info(f\"Image break token id: {self.img_break_tok_id}\")\n@@ -3962,6 +3965,10 @@ def _get_cls_out_tensor(self, data_torch: Tensor) -> Tensor:\n         return torch.stack([true_row, false_row], dim=0)\n \n     def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -> Iterable[tuple[str, Tensor]]:\n+        if \"model.vision_\" in name:\n+            # skip multimodal tensors\n+            return []\n+\n         if self.is_rerank:\n             is_tied_head = self.is_tied_embeddings and \"embed_tokens\" in name\n             is_real_head = not self.is_tied_embeddings and \"lm_head\" in name\n@@ -9435,6 +9442,21 @@ def map_tensor_name(self, name: str, try_suffixes: Sequence[str] = (\".weight\", \"\n         return super().map_tensor_name(name, try_suffixes)\n \n \n+@ModelBase.register(\"LightOnOCRForConditionalGeneration\")\n+class LightOnOCRVisionModel(LlavaVisionModel):\n+    is_mistral_format = False\n+    use_break_tok = False\n+\n+    def set_gguf_parameters(self):\n+        super().set_gguf_parameters()\n+        self.gguf_writer.add_clip_projector_type(gguf.VisionProjectorType.LIGHTONOCR)\n+\n+    def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None):\n+        name = name.replace(\"model.vision_encoder.\", \"vision_tower.\")\n+        name = name.replace(\"model.vision_projection.\", \"multi_modal_projector.\")\n+        return super().modify_tensors(data_torch, name, bid)\n+\n+\n @ModelBase.register(\"KimiVLForConditionalGeneration\")\n class KimiVLModel(MmprojModel):\n     def __init__(self, *args, **kwargs):"
      },
      {
        "filename": "gguf-py/gguf/constants.py",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -3062,6 +3062,7 @@ class VisionProjectorType:\n     VOXTRAL = \"voxtral\"\n     LFM2 = \"lfm2\"\n     KIMIVL = \"kimivl\"\n+    LIGHTONOCR = \"lightonocr\"\n \n \n # Items here are (block size, type size)"
      },
      {
        "filename": "tools/mtmd/clip-impl.h",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -139,6 +139,7 @@ enum projector_type {\n     PROJECTOR_TYPE_VOXTRAL,\n     PROJECTOR_TYPE_LFM2,\n     PROJECTOR_TYPE_KIMIVL,\n+    PROJECTOR_TYPE_LIGHTONOCR,\n     PROJECTOR_TYPE_UNKNOWN,\n };\n \n@@ -161,6 +162,7 @@ static std::map<projector_type, std::string> PROJECTOR_TYPE_NAMES = {\n     { PROJECTOR_TYPE_VOXTRAL,   \"voxtral\"},\n     { PROJECTOR_TYPE_LFM2,      \"lfm2\"},\n     { PROJECTOR_TYPE_KIMIVL,    \"kimivl\"},\n+    { PROJECTOR_TYPE_LIGHTONOCR,\"lightonocr\"},\n };\n \n static projector_type clip_projector_type_from_string(const std::string & str) {"
      },
      {
        "filename": "tools/mtmd/clip.cpp",
        "status": "modified",
        "additions": 23,
        "deletions": 3,
        "changes": 26,
        "patch": "@@ -621,7 +621,7 @@ struct clip_graph {\n         }\n \n         // arrangement of the [IMG_BREAK] token\n-        {\n+        if (model.token_embd_img_break) {\n             // not efficient, but works\n             // the trick is to view the embeddings as a 3D tensor with shape [n_embd, n_patches_per_row, n_rows]\n             // and then concatenate the [IMG_BREAK] token to the end of each row, aka n_patches_per_row dimension\n@@ -2095,6 +2095,7 @@ static ggml_cgraph * clip_image_build_graph(clip_ctx * ctx, const clip_image_f32\n                 res = graph.build_siglip();\n             } break;\n         case PROJECTOR_TYPE_PIXTRAL:\n+        case PROJECTOR_TYPE_LIGHTONOCR:\n             {\n                 res = graph.build_pixtral();\n             } break;\n@@ -2380,6 +2381,7 @@ struct clip_model_loader {\n                         get_u32(KEY_PROJ_SCALE_FACTOR, hparams.proj_scale_factor, false);\n                     } break;\n                 case PROJECTOR_TYPE_PIXTRAL:\n+                case PROJECTOR_TYPE_LIGHTONOCR:\n                     {\n                         hparams.rope_theta = 10000.0f;\n                         hparams.warmup_image_size = hparams.patch_size * 8;\n@@ -2722,6 +2724,15 @@ struct clip_model_loader {\n                     model.mm_input_norm_w   = get_tensor(TN_MM_INP_NORM,     false);\n                     model.mm_patch_merger_w = get_tensor(TN_MM_PATCH_MERGER, false);\n                 } break;\n+            case PROJECTOR_TYPE_LIGHTONOCR:\n+                {\n+                    model.mm_1_w = get_tensor(string_format(TN_LLAVA_PROJ, 1, \"weight\"));\n+                    model.mm_1_b = get_tensor(string_format(TN_LLAVA_PROJ, 1, \"bias\"), false);\n+                    model.mm_2_w = get_tensor(string_format(TN_LLAVA_PROJ, 2, \"weight\"));\n+                    model.mm_2_b = get_tensor(string_format(TN_LLAVA_PROJ, 2, \"bias\"), false);\n+                    model.mm_input_norm_w   = get_tensor(TN_MM_INP_NORM,     false);\n+                    model.mm_patch_merger_w = get_tensor(TN_MM_PATCH_MERGER, false);\n+                } break;\n             case PROJECTOR_TYPE_ULTRAVOX:\n                 {\n                     model.conv1d_1_w = get_tensor(string_format(TN_CONV1D, 1, \"weight\"));\n@@ -3622,7 +3633,9 @@ bool clip_image_preprocess(struct clip_ctx * ctx, const clip_image_u8 * img, str\n         res_imgs->entries.push_back(std::move(img_f32));\n         return true;\n \n-    } else if (ctx->proj_type() == PROJECTOR_TYPE_PIXTRAL) {\n+    } else if (ctx->proj_type() == PROJECTOR_TYPE_PIXTRAL\n+            || ctx->proj_type() == PROJECTOR_TYPE_LIGHTONOCR\n+    ) {\n         clip_image_u8 resized_image;\n         auto new_size = image_manipulation::calc_size_preserved_ratio(original_size, params.patch_size, params.image_size);\n         image_manipulation::bilinear_resize(*img, resized_image, new_size.width, new_size.height);\n@@ -3865,12 +3878,17 @@ int clip_n_output_tokens(const struct clip_ctx * ctx, struct clip_image_f32 * im\n                 n_patches = x_patch * y_patch;\n             } break;\n         case PROJECTOR_TYPE_PIXTRAL:\n+        case PROJECTOR_TYPE_LIGHTONOCR:\n             {\n                 // dynamic size\n                 int n_merge = params.spatial_merge_size;\n                 int n_patches_x = img->nx / patch_size / (n_merge > 0 ? n_merge : 1);\n                 int n_patches_y = img->ny / patch_size / (n_merge > 0 ? n_merge : 1);\n-                n_patches = n_patches_y * n_patches_x + n_patches_y - 1; // + one [IMG_BREAK] per row, except the last row\n+                if (ctx->model.token_embd_img_break) {\n+                    n_patches = n_patches_y * n_patches_x + n_patches_y - 1; // + one [IMG_BREAK] per row, except the last row\n+                } else {\n+                    n_patches = n_patches_y * n_patches_x;\n+                }\n             } break;\n         case PROJECTOR_TYPE_VOXTRAL:\n         case PROJECTOR_TYPE_ULTRAVOX:\n@@ -4247,6 +4265,7 @@ bool clip_image_batch_encode(clip_ctx * ctx, const int n_threads, const clip_ima\n             } break;\n         case PROJECTOR_TYPE_PIXTRAL:\n         case PROJECTOR_TYPE_KIMIVL:\n+        case PROJECTOR_TYPE_LIGHTONOCR:\n             {\n                 // set the 2D positions\n                 int n_patches_per_col = image_size_width / patch_size;\n@@ -4377,6 +4396,7 @@ int clip_n_mmproj_embd(const struct clip_ctx * ctx) {\n             return ctx->model.mm_model_peg_0_b->ne[0];\n         case PROJECTOR_TYPE_MLP:\n         case PROJECTOR_TYPE_PIXTRAL:\n+        case PROJECTOR_TYPE_LIGHTONOCR:\n             return ctx->model.mm_2_w->ne[1];\n         case PROJECTOR_TYPE_MLP_NORM:\n             return ctx->model.mm_3_b->ne[0];"
      },
      {
        "filename": "tools/mtmd/mtmd.cpp",
        "status": "modified",
        "additions": 5,
        "deletions": 0,
        "changes": 5,
        "patch": "@@ -275,6 +275,11 @@ struct mtmd_context {\n             img_beg = \"<img>\";\n             img_end = \"</img>\";\n \n+        } else if (proj == PROJECTOR_TYPE_LIGHTONOCR) {\n+            // <|im_start|> ... (image embeddings) ... <|im_end|>\n+            img_beg = \"<|im_start|>\";\n+            img_end = \"<|im_end|>\";\n+\n         }\n     }\n "
      },
      {
        "filename": "tools/mtmd/tests.sh",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -70,6 +70,7 @@ add_test_vision \"ggml-org/InternVL3-1B-Instruct-GGUF:Q8_0\"\n add_test_vision \"ggml-org/Qwen2.5-Omni-3B-GGUF:Q4_K_M\"\n add_test_vision \"ggml-org/LFM2-VL-450M-GGUF:Q8_0\"\n add_test_vision \"ggml-org/granite-docling-258M-GGUF:Q8_0\"\n+add_test_vision \"ggml-org/LightOnOCR-1B-1025-GGUF:Q8_0\"\n \n add_test_audio  \"ggml-org/ultravox-v0_5-llama-3_2-1b-GGUF:Q8_0\"\n add_test_audio  \"ggml-org/Qwen2.5-Omni-3B-GGUF:Q4_K_M\""
      }
    ],
    "num_files": 6,
    "scraped_at": "2025-11-16T23:29:34.670389"
  },
  {
    "pr_number": 16763,
    "title": "Add LFM2 tool handling",
    "body": "LFM2 model has specific tool calling handling, it only produces python like code as of Oct 2025. \r\n\r\nThe LFM2 requires proper system prompt formatting to trigger the tool calling functionality. \r\n\r\nChanges:\r\n* Add proper system message formatting to render the tools in the form the model could understand them.\r\n* Add additional, but optional, JSON schema following. Due to the Python calling, the model is not generally good at JSON, but when the JSON schema constraint is applied it can follow the JSON generation instructions. To trigger that behavior, add `force json schema.` in the system prompt. By defaults, the output for tools invocation is not forced.\r\n\r\nAttaching testing scripts in case anyone wants to try it out.\r\n* [invoke-function-call-force-json-schema.sh](https://github.com/user-attachments/files/23135902/invoke-function-call-force-json-schema.sh)\r\n* [invoke-function-call.sh](https://github.com/user-attachments/files/23135903/invoke-function-call.sh)\r\n\r\nUpstream LFM2 models chat template will be updated next week. \r\n\r\nTest:\r\n\r\n```shell\r\ncmake --preset arm64-apple-clang-release\r\ncmake --build build-arm64-apple-clang-release -j10\r\n./build-arm64-apple-clang-release/bin/llama-server \\\r\n  -hfr LiquidAI/LFM2-1.2B-GGUF \\\r\n  -hff LFM2-1.2B-Q4_0.gguf \\\r\n  --chat-template-file models/templates/llama-cpp-lfm2.jinja  \\\r\n  --jinja\r\n```",
    "html_url": "https://github.com/ggml-org/llama.cpp/pull/16763",
    "created_at": "2025-10-24T22:41:02Z",
    "merged_at": "2025-10-27T22:54:01Z",
    "merge_commit_sha": "c053e18a66dd95dc340aa61317877c2a41d4e3cf",
    "base_ref": "master",
    "head_sha": "3ed13a84dbb6ee283d487f14fdc7319eb2f683bc",
    "user": "ykhrustalev",
    "files": [
      {
        "filename": "common/chat.cpp",
        "status": "modified",
        "additions": 198,
        "deletions": 0,
        "changes": 198,
        "patch": "@@ -9,8 +9,11 @@\n #include <minja/chat-template.hpp>\n #include <minja/minja.hpp>\n \n+#include <algorithm>\n #include <cstdio>\n+#include <cctype>\n #include <exception>\n+#include <functional>\n #include <iostream>\n #include <optional>\n #include <stdexcept>\n@@ -640,6 +643,7 @@ const char * common_chat_format_name(common_chat_format format) {\n         case COMMON_CHAT_FORMAT_SEED_OSS: return \"Seed-OSS\";\n         case COMMON_CHAT_FORMAT_NEMOTRON_V2: return \"Nemotron V2\";\n         case COMMON_CHAT_FORMAT_APERTUS: return \"Apertus\";\n+        case COMMON_CHAT_FORMAT_LFM2_WITH_JSON_TOOLS: return \"LFM2 with JSON tools\";\n         default:\n             throw std::runtime_error(\"Unknown chat format\");\n     }\n@@ -986,6 +990,126 @@ static common_chat_params common_chat_params_init_mistral_nemo(const common_chat\n     return data;\n }\n \n+\n+// Case-insensitive find\n+static size_t ifind_string(const std::string & haystack, const std::string & needle, size_t pos = 0) {\n+    auto it = std::search(\n+        haystack.begin() + pos, haystack.end(),\n+        needle.begin(), needle.end(),\n+        [](char a, char b) { return std::tolower(a) == std::tolower(b); }\n+    );\n+    return (it == haystack.end()) ? std::string::npos : std::distance(haystack.begin(), it);\n+}\n+\n+static common_chat_params common_chat_params_init_lfm2(const common_chat_template & tmpl, const struct templates_params & inputs) {\n+    common_chat_params data;\n+    const auto is_json_schema_provided = !inputs.json_schema.is_null();\n+    const auto is_grammar_provided = !inputs.grammar.empty();\n+    const auto are_tools_provided = inputs.tools.is_array() && !inputs.tools.empty();\n+\n+    // the logic requires potentially modifying the messages\n+    auto tweaked_messages = inputs.messages;\n+\n+    auto replace_json_schema_marker = [](json & messages) -> bool {\n+        static std::string marker1 = \"force json schema.\\n\";\n+        static std::string marker2 = \"force json schema.\";\n+\n+        if (messages.empty() || messages.at(0).at(\"role\") != \"system\") {\n+            return false;\n+        }\n+\n+        std::string content = messages.at(0).at(\"content\");\n+\n+        for (const auto & marker : {marker1, marker2}) {\n+            const auto pos = ifind_string(content, marker);\n+            if (pos != std::string::npos) {\n+                content.replace(pos, marker.length(), \"\");\n+                // inject modified content back into the messages\n+                messages.at(0).at(\"content\") = content;\n+                return true;\n+            }\n+        }\n+\n+        return false;\n+    };\n+\n+    // Lfm2 model does not natively work with json, but can generally understand the tools structure\n+    //\n+    // Example of the pytorch dialog structure:\n+    //     <|startoftext|><|im_start|>system\n+    //     List of tools: <|tool_list_start|>[{\"name\": \"get_candidate_status\", \"description\": \"Retrieves the current status of a candidate in the recruitment process\", \"parameters\": {\"type\": \"object\", \"properties\": {\"candidate_id\": {\"type\": \"string\", \"description\": \"Unique identifier for the candidate\"}}, \"required\": [\"candidate_id\"]}}]<|tool_list_end|><|im_end|>\n+    //     <|im_start|>user\n+    //     What is the current status of candidate ID 12345?<|im_end|>\n+    //     <|im_start|>assistant\n+    //     <|tool_call_start|>[get_candidate_status(candidate_id=\"12345\")]<|tool_call_end|>Checking the current status of candidate ID 12345.<|im_end|>\n+    //     <|im_start|>tool\n+    //     <|tool_response_start|>{\"candidate_id\": \"12345\", \"status\": \"Interview Scheduled\", \"position\": \"Clinical Research Associate\", \"date\": \"2023-11-20\"}<|tool_response_end|><|im_end|>\n+    //     <|im_start|>assistant\n+    //     The candidate with ID 12345 is currently in the \"Interview Scheduled\" stage for the position of Clinical Research Associate, with an interview date set for 2023-11-20.<|im_end|>\n+    //\n+    // For the llama server compatibility with json tools semantic,\n+    // the client can add \"Follow json schema.\" line into the system message prompt to force the json output.\n+    //\n+    if (are_tools_provided && (is_json_schema_provided || is_grammar_provided)) {\n+        // server/utils.hpp prohibits that branch for the custom grammar anyways\n+        throw std::runtime_error(\"Tools call must not use \\\"json_schema\\\" or \\\"grammar\\\", use non-tool invocation if you want to use custom grammar\");\n+    } else if (are_tools_provided && replace_json_schema_marker(tweaked_messages)) {\n+        LOG_INF(\"%s: Using tools to build a grammar\\n\", __func__);\n+\n+        data.grammar = build_grammar([&](const common_grammar_builder & builder) {\n+            auto schemas = json::array();\n+            foreach_function(inputs.tools, [&](const json & tool) {\n+                const auto & function = tool.at(\"function\");\n+                schemas.push_back({\n+                    {\"type\", \"object\"},\n+                    {\"properties\", {\n+                        {\"name\", {\n+                            {\"type\", \"string\"},\n+                            {\"const\", function.at(\"name\")},\n+                        }},\n+                        {\"arguments\", function.at(\"parameters\")},\n+                    }},\n+                    {\"required\", json::array({\"name\", \"arguments\", \"id\"})},\n+                });\n+            });\n+            auto schema = json {\n+                {\"type\", \"array\"},\n+                {\"items\", schemas.size() == 1 ? schemas[0] : json {{\"anyOf\", schemas}}},\n+                {\"minItems\", 1},\n+            };\n+            if (!inputs.parallel_tool_calls) {\n+                schema[\"maxItems\"] = 1;\n+            }\n+\n+            builder.add_rule(\"root\", \"\\\"<|tool_call_start|>\\\"\" + builder.add_schema(\"tool_calls\", schema) + \"\\\"<|tool_call_end|>\\\"\");\n+        });\n+        // model has no concept of tool selection mode choice,\n+        // if the system prompt rendered correctly it will produce a tool call\n+        // the grammar goes inside the tool call body\n+        data.grammar_lazy = true;\n+        data.grammar_triggers = {{COMMON_GRAMMAR_TRIGGER_TYPE_PATTERN_FULL, \"\\\\s*<\\\\|tool_call_start\\\\|>\\\\s*\\\\[\"}};\n+        data.preserved_tokens = {\"<|tool_call_start|>\", \"<|tool_call_end|>\"};\n+        data.format = COMMON_CHAT_FORMAT_LFM2_WITH_JSON_TOOLS;\n+    } else if (are_tools_provided && (!is_json_schema_provided && !is_grammar_provided)) {\n+        LOG_INF(\"%s: Using tools without json schema or grammar\\n\", __func__);\n+        // output those tokens\n+        data.preserved_tokens = {\"<|tool_call_start|>\", \"<|tool_call_end|>\"};\n+    } else if (is_json_schema_provided) {\n+        LOG_INF(\"%s: Using provided json schema to build a grammar\\n\", __func__);\n+        data.grammar = json_schema_to_grammar(inputs.json_schema);\n+    } else if (is_grammar_provided) {\n+        LOG_INF(\"%s: Using provided grammar\\n\", __func__);\n+        data.grammar = inputs.grammar;\n+    } else {\n+        LOG_INF(\"%s: Using content relying on the template\\n\", __func__);\n+    }\n+\n+    data.prompt = apply(tmpl, inputs, /* messages_override= */ tweaked_messages);\n+    LOG_DBG(\"%s: Prompt: %s\\n\", __func__, data.prompt.c_str());\n+\n+    return data;\n+}\n+\n static common_chat_params common_chat_params_init_magistral(const common_chat_template & tmpl, const struct templates_params & inputs) {\n     common_chat_params data;\n     data.prompt = apply(tmpl, inputs);\n@@ -2499,6 +2623,71 @@ static void common_chat_parse_apertus(common_chat_msg_parser & builder) {\n     builder.add_content(builder.consume_rest());\n }\n \n+\n+static void common_chat_parse_lfm2(common_chat_msg_parser & builder) {\n+    if (!builder.syntax().parse_tool_calls) {\n+        builder.add_content(builder.consume_rest());\n+        return;\n+    }\n+\n+    // LFM2 format: <|tool_call_start|>[{\"name\": \"get_current_time\", \"arguments\": {\"location\": \"Paris\"}}]<|tool_call_end|>\n+    static const common_regex tool_call_start_regex(regex_escape(\"<|tool_call_start|>\"));\n+    static const common_regex tool_call_end_regex(regex_escape(\"<|tool_call_end|>\"));\n+\n+    // Loop through all tool calls\n+    while (auto res = builder.try_find_regex(tool_call_start_regex, std::string::npos, /* add_prelude_to_content= */ true)) {\n+        builder.move_to(res->groups[0].end);\n+\n+        // Parse JSON array format: [{\"name\": \"...\", \"arguments\": {...}}]\n+        auto tool_calls_data = builder.consume_json();\n+\n+        // Consume end marker\n+        builder.consume_spaces();\n+        if (!builder.try_consume_regex(tool_call_end_regex)) {\n+            throw common_chat_msg_partial_exception(\"Expected <|tool_call_end|>\");\n+        }\n+\n+        // Process each tool call in the array\n+        if (tool_calls_data.json.is_array()) {\n+            for (const auto & tool_call : tool_calls_data.json) {\n+                if (!tool_call.is_object()) {\n+                    throw common_chat_msg_partial_exception(\"Tool call must be an object\");\n+                }\n+\n+                if (!tool_call.contains(\"name\")) {\n+                    throw common_chat_msg_partial_exception(\"Tool call missing 'name' field\");\n+                }\n+\n+                std::string function_name = tool_call.at(\"name\");\n+                std::string arguments = \"{}\";\n+\n+                if (tool_call.contains(\"arguments\")) {\n+                    if (tool_call.at(\"arguments\").is_object()) {\n+                        arguments = tool_call.at(\"arguments\").dump();\n+                    } else if (tool_call.at(\"arguments\").is_string()) {\n+                        arguments = tool_call.at(\"arguments\");\n+                    }\n+                }\n+\n+                if (!builder.add_tool_call(function_name, \"\", arguments)) {\n+                    throw common_chat_msg_partial_exception(\"Incomplete tool call\");\n+                }\n+            }\n+        } else {\n+            throw common_chat_msg_partial_exception(\"Expected JSON array for tool calls\");\n+        }\n+\n+        // Consume any trailing whitespace after this tool call\n+        builder.consume_spaces();\n+    }\n+\n+    // Consume any remaining content after all tool calls\n+    auto remaining = builder.consume_rest();\n+    if (!string_strip(remaining).empty()) {\n+        builder.add_content(remaining);\n+    }\n+}\n+\n static void common_chat_parse_seed_oss(common_chat_msg_parser & builder) {\n     // Parse thinking tags first - this handles the main reasoning content\n     builder.try_parse_reasoning(\"<seed:think>\", \"</seed:think>\");\n@@ -2748,6 +2937,12 @@ static common_chat_params common_chat_templates_apply_jinja(\n         return common_chat_params_init_apertus(tmpl, params);\n     }\n \n+    // LFM2 (w/ tools)\n+    if (src.find(\"List of tools: <|tool_list_start|>[\") != std::string::npos &&\n+        src.find(\"]<|tool_list_end|>\") != std::string::npos) {\n+        return common_chat_params_init_lfm2(tmpl, params);\n+    }\n+\n     // Use generic handler when mixing tools + JSON schema.\n     // TODO: support that mix in handlers below.\n     if ((params.tools.is_array() && params.json_schema.is_object())) {\n@@ -2926,6 +3121,9 @@ static void common_chat_parse(common_chat_msg_parser & builder) {\n         case COMMON_CHAT_FORMAT_APERTUS:\n             common_chat_parse_apertus(builder);\n             break;\n+        case COMMON_CHAT_FORMAT_LFM2_WITH_JSON_TOOLS:\n+            common_chat_parse_lfm2(builder);\n+            break;\n         default:\n             throw std::runtime_error(std::string(\"Unsupported format: \") + common_chat_format_name(builder.syntax().format));\n     }"
      },
      {
        "filename": "common/chat.h",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -116,6 +116,7 @@ enum common_chat_format {\n     COMMON_CHAT_FORMAT_SEED_OSS,\n     COMMON_CHAT_FORMAT_NEMOTRON_V2,\n     COMMON_CHAT_FORMAT_APERTUS,\n+    COMMON_CHAT_FORMAT_LFM2_WITH_JSON_TOOLS,\n \n     COMMON_CHAT_FORMAT_COUNT, // Not a format, just the # formats\n };"
      },
      {
        "filename": "models/templates/llama-cpp-lfm2.jinja",
        "status": "added",
        "additions": 37,
        "deletions": 0,
        "changes": 37,
        "patch": "@@ -0,0 +1,37 @@\n+{{- bos_token -}}\n+{%- set system_prompt = \"\" -%}\n+{%- set ns = namespace(system_prompt=\"\") -%}\n+{%- if messages[0][\"role\"] == \"system\" -%}\n+\t{%- set ns.system_prompt = messages[0][\"content\"] -%}\n+\t{%- set messages = messages[1:] -%}\n+{%- endif -%}\n+{%- if tools -%}\n+\t{%- set ns.system_prompt = ns.system_prompt + (\"\\n\" if ns.system_prompt else \"\") + \"List of tools: <|tool_list_start|>[\" -%}\n+\t{%- for tool in tools -%}\n+\t\t{%- if tool is not string -%}\n+\t\t\t{%- set tool = tool | tojson -%}\n+\t\t{%- endif -%}\n+\t\t{%- set ns.system_prompt = ns.system_prompt + tool -%}\n+\t\t{%- if not loop.last -%}\n+\t\t\t{%- set ns.system_prompt = ns.system_prompt + \", \" -%}\n+\t\t{%- endif -%}\n+\t{%- endfor -%}\n+\t{%- set ns.system_prompt = ns.system_prompt + \"]<|tool_list_end|>\" -%}\n+{%- endif -%}\n+{%- if ns.system_prompt -%}\n+\t{{- \"<|im_start|>system\\n\" + ns.system_prompt + \"<|im_end|>\\n\" -}}\n+{%- endif -%}\n+{%- for message in messages -%}\n+\t{{- \"<|im_start|>\" + message[\"role\"] + \"\\n\" -}}\n+\t{%- set content = message[\"content\"] -%}\n+\t{%- if content is not string -%}\n+\t\t{%- set content = content | tojson -%}\n+\t{%- endif -%}\n+\t{%- if message[\"role\"] == \"tool\" -%}\n+\t\t{%- set content = \"<|tool_response_start|>\" + content + \"<|tool_response_end|>\" -%}\n+\t{%- endif -%}\n+\t{{- content + \"<|im_end|>\\n\" -}}\n+{%- endfor -%}\n+{%- if add_generation_prompt -%}\n+\t{{- \"<|im_start|>assistant\\n\" -}}\n+{%- endif -%}"
      },
      {
        "filename": "tests/test-chat.cpp",
        "status": "modified",
        "additions": 149,
        "deletions": 0,
        "changes": 149,
        "patch": "@@ -16,6 +16,7 @@\n \n #include <fstream>\n #include <iostream>\n+#include <functional>\n #include <string>\n \n using json = nlohmann::ordered_json;\n@@ -2138,6 +2139,154 @@ static void test_template_output_parsers() {\n \n         assert_equals(true, common_chat_templates_support_enable_thinking(tmpls.get()));\n     }\n+    {\n+        // LFM2 format tests\n+        auto tmpls = read_templates(\"models/templates/llama-cpp-lfm2.jinja\");\n+        std::vector<std::string> end_tokens{ \"<|im_end|>\" };\n+\n+        auto inputs_tools_forced_json_schema = std::invoke([&]() -> common_chat_templates_inputs {\n+            common_chat_templates_inputs inputs;\n+            inputs.messages = {\n+                std::invoke([&]() -> common_chat_msg {\n+                    common_chat_msg msg;\n+                    msg.role = \"system\";\n+                    msg.content = \"force json schema.\\n\";\n+                    return msg;\n+                }),\n+                message_user,\n+            };\n+            inputs.tools = {special_function_tool};\n+            return inputs;\n+        });\n+\n+        {\n+            auto params = common_chat_templates_apply(tmpls.get(), inputs_no_tools);\n+            assert_equals(COMMON_CHAT_FORMAT_CONTENT_ONLY, params.format);\n+            assert_equals(false, params.grammar_lazy);\n+            assert_equals(std::string(R\"(<|im_start|>user\n+Hey there!<|im_end|>\n+<|im_start|>assistant\n+)\"), params.prompt);\n+        }\n+\n+        {\n+            auto params = common_chat_templates_apply(tmpls.get(), inputs_tools);\n+            assert_equals(COMMON_CHAT_FORMAT_CONTENT_ONLY, params.format);\n+            assert_equals(false, params.grammar_lazy);\n+            assert_equals(std::string(R\"(<|im_start|>system\n+List of tools: <|tool_list_start|>[{\"type\": \"function\", \"function\": {\"name\": \"special_function\", \"description\": \"I'm special\", \"parameters\": {\"type\": \"object\", \"properties\": {\"arg1\": {\"type\": \"integer\", \"description\": \"The arg.\"}}, \"required\": [\"arg1\"]}}}]<|tool_list_end|><|im_end|>\n+<|im_start|>user\n+Hey there!<|im_end|>\n+<|im_start|>assistant\n+)\"), params.prompt);\n+            assert_equals(true, params.grammar.empty());\n+        }\n+\n+        {\n+            auto params = common_chat_templates_apply(tmpls.get(), inputs_tools_forced_json_schema);\n+            assert_equals(COMMON_CHAT_FORMAT_LFM2_WITH_JSON_TOOLS, params.format);\n+            assert_equals(true, params.grammar_lazy);\n+            assert_equals(std::string(R\"(<|im_start|>system\n+List of tools: <|tool_list_start|>[{\"type\": \"function\", \"function\": {\"name\": \"special_function\", \"description\": \"I'm special\", \"parameters\": {\"type\": \"object\", \"properties\": {\"arg1\": {\"type\": \"integer\", \"description\": \"The arg.\"}}, \"required\": [\"arg1\"]}}}]<|tool_list_end|><|im_end|>\n+<|im_start|>user\n+Hey there!<|im_end|>\n+<|im_start|>assistant\n+)\"), params.prompt);\n+            assert_equals(false, params.grammar.empty());\n+        }\n+\n+        // Test parsing regular content\n+        assert_msg_equals(message_assist,\n+            common_chat_parse(\n+                \"Hello, world!\\nWhat's up?\",\n+                /* is_partial= */ false,\n+                {COMMON_CHAT_FORMAT_LFM2_WITH_JSON_TOOLS}));\n+\n+        // Test single tool call with JSON format\n+        common_chat_msg msg_single_tool_call;\n+        msg_single_tool_call.role = \"assistant\";\n+        msg_single_tool_call.tool_calls.push_back({\"special_function\", \"{\\\"arg1\\\":1}\", \"\"});\n+        assert_msg_equals(\n+            msg_single_tool_call,\n+            common_chat_parse(\n+                \"<|tool_call_start|>[{\\\"name\\\": \\\"special_function\\\", \\\"arguments\\\": {\\\"arg1\\\": 1}}]<|tool_call_end|>\",\n+                /* is_partial= */ false,\n+                {COMMON_CHAT_FORMAT_LFM2_WITH_JSON_TOOLS}));\n+\n+        // Test tool call with string argument\n+        common_chat_msg msg_tool_call_string;\n+        msg_tool_call_string.role = \"assistant\";\n+        msg_tool_call_string.tool_calls.push_back({\"get_weather\", \"{\\\"location\\\":\\\"Paris\\\"}\", \"\"});\n+        assert_msg_equals(\n+            msg_tool_call_string,\n+            common_chat_parse(\n+                \"<|tool_call_start|>[{\\\"name\\\": \\\"get_weather\\\", \\\"arguments\\\": {\\\"location\\\": \\\"Paris\\\"}}]<|tool_call_end|>\",\n+                /* is_partial= */ false,\n+                {COMMON_CHAT_FORMAT_LFM2_WITH_JSON_TOOLS}));\n+\n+        // Test tool call with multiple arguments\n+        common_chat_msg msg_multi_args;\n+        msg_multi_args.role = \"assistant\";\n+        msg_multi_args.tool_calls.push_back({\"calculate\", \"{\\\"x\\\":10,\\\"y\\\":20,\\\"operation\\\":\\\"add\\\"}\", \"\"});\n+        assert_msg_equals(\n+            msg_multi_args,\n+            common_chat_parse(\n+                \"<|tool_call_start|>[{\\\"name\\\": \\\"calculate\\\", \\\"arguments\\\": {\\\"x\\\": 10, \\\"y\\\": 20, \\\"operation\\\": \\\"add\\\"}}]<|tool_call_end|>\",\n+                /* is_partial= */ false,\n+                {COMMON_CHAT_FORMAT_LFM2_WITH_JSON_TOOLS}));\n+\n+        // Test multiple tool calls in single array\n+        common_chat_msg msg_multiple_tools;\n+        msg_multiple_tools.role = \"assistant\";\n+        msg_multiple_tools.tool_calls.push_back({\"get_weather\", \"{\\\"location\\\":\\\"Paris\\\"}\", \"\"});\n+        msg_multiple_tools.tool_calls.push_back({\"get_time\", \"{\\\"timezone\\\":\\\"UTC\\\"}\", \"\"});\n+        assert_msg_equals(\n+            msg_multiple_tools,\n+            common_chat_parse(\n+                \"<|tool_call_start|>[{\\\"name\\\": \\\"get_weather\\\", \\\"arguments\\\": {\\\"location\\\": \\\"Paris\\\"}}, {\\\"name\\\": \\\"get_time\\\", \\\"arguments\\\": {\\\"timezone\\\": \\\"UTC\\\"}}]<|tool_call_end|>\",\n+                /* is_partial= */ false,\n+                {COMMON_CHAT_FORMAT_LFM2_WITH_JSON_TOOLS}));\n+\n+        // Test tool call with content before\n+        common_chat_msg msg_content_before_tool;\n+        msg_content_before_tool.role = \"assistant\";\n+        msg_content_before_tool.content = \"Let me check the weather for you.\";\n+        msg_content_before_tool.tool_calls.push_back({\"get_weather\", \"{\\\"location\\\":\\\"Paris\\\"}\", \"\"});\n+        assert_msg_equals(\n+            msg_content_before_tool,\n+            common_chat_parse(\n+                \"Let me check the weather for you.<|tool_call_start|>[{\\\"name\\\": \\\"get_weather\\\", \\\"arguments\\\": {\\\"location\\\": \\\"Paris\\\"}}]<|tool_call_end|>\",\n+                /* is_partial= */ false,\n+                {COMMON_CHAT_FORMAT_LFM2_WITH_JSON_TOOLS}));\n+\n+        // Test tool call with content after\n+        common_chat_msg msg_content_after_tool;\n+        msg_content_after_tool.role = \"assistant\";\n+        msg_content_after_tool.content = \"Here's the result.\";\n+        msg_content_after_tool.tool_calls.push_back({\"get_weather\", \"{\\\"location\\\":\\\"Paris\\\"}\", \"\"});\n+        assert_msg_equals(\n+            msg_content_after_tool,\n+            common_chat_parse(\n+                \"<|tool_call_start|>[{\\\"name\\\": \\\"get_weather\\\", \\\"arguments\\\": {\\\"location\\\": \\\"Paris\\\"}}]<|tool_call_end|>Here's the result.\",\n+                /* is_partial= */ false,\n+                {COMMON_CHAT_FORMAT_LFM2_WITH_JSON_TOOLS}));\n+\n+        // Test tool call with newlines (common in LLM output)\n+        common_chat_msg msg_tool_call_newlines;\n+        msg_tool_call_newlines.role = \"assistant\";\n+        msg_tool_call_newlines.tool_calls.push_back({\"get_current_time\", \"{\\\"location\\\":\\\"Paris\\\"}\", \"\"});\n+        assert_msg_equals(\n+            msg_tool_call_newlines,\n+            common_chat_parse(\n+                \"<|tool_call_start|>[{\\n    \\\"name\\\": \\\"get_current_time\\\",\\n    \\\"arguments\\\": {\\n        \\\"location\\\": \\\"Paris\\\"\\n    }\\n}]<|tool_call_end|>\",\n+                /* is_partial= */ false,\n+                {COMMON_CHAT_FORMAT_LFM2_WITH_JSON_TOOLS}));\n+\n+        // Note: LFM2 uses JSON format for tool calls: [{\"name\": \"...\", \"arguments\": {...}}]\n+        // Unlike other formats, LFM2 template does not render tool calls in conversation history,\n+        // so we don't use test_templates() for tool call generation. Instead, the parsing tests\n+        // above verify edge cases and format variations for the tool call output format.\n+    }\n \n }\n "
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T23:29:34.949314"
  },
  {
    "pr_number": 16746,
    "title": "llama: consistent ctx <-> buf order for KV cache",
    "body": "See https://github.com/ggml-org/llama.cpp/pull/16581 . The KV cache has the same issue which can lead to incorrect results for https://github.com/ggml-org/llama.cpp/pull/16653 .",
    "html_url": "https://github.com/ggml-org/llama.cpp/pull/16746",
    "created_at": "2025-10-23T19:32:22Z",
    "merged_at": "2025-10-28T10:23:54Z",
    "merge_commit_sha": "7a0e900e3615fa46c074a7fdf900b47d3c0a1c7e",
    "base_ref": "master",
    "head_sha": "98c7edd81817aaebcd217427c6b0f95209cada71",
    "user": "JohannesGaessler",
    "files": [
      {
        "filename": "src/llama-kv-cache.cpp",
        "status": "modified",
        "additions": 18,
        "deletions": 14,
        "changes": 32,
        "patch": "@@ -8,6 +8,7 @@\n #include <algorithm>\n #include <cassert>\n #include <cmath>\n+#include <cstring>\n #include <limits>\n #include <map>\n #include <stdexcept>\n@@ -37,8 +38,15 @@ llama_kv_cache::llama_kv_cache(\n \n     const uint32_t n_layer_kv = hparams.n_layer_kv();\n \n+    // define a comparator for the buft -> ctx map to ensure that the order is well-defined:\n+    struct ggml_backend_buft_comparator {\n+        bool operator()(const ggml_backend_buffer_type_t & lhs, const ggml_backend_buffer_type_t & rhs) const {\n+            return strcmp(ggml_backend_buft_name(lhs), ggml_backend_buft_name(rhs)) < 0;\n+        }\n+    };\n+    std::map<ggml_backend_buffer_type_t, ggml_context_ptr, ggml_backend_buft_comparator> ctx_map;\n+\n     // create a context for each buffer type\n-    std::map<ggml_backend_buffer_type_t, ggml_context *> ctx_map;\n     auto ctx_for_buft = [&](ggml_backend_buffer_type_t buft) -> ggml_context * {\n         auto it = ctx_map.find(buft);\n         if (it == ctx_map.end()) {\n@@ -53,13 +61,12 @@ llama_kv_cache::llama_kv_cache(\n                 return nullptr;\n             }\n \n-            ctx_map[buft] = ctx;\n-            ctxs.emplace_back(ctx);\n+            ctx_map.emplace(buft, ctx);\n \n             return ctx;\n         }\n \n-        return it->second;\n+        return it->second.get();\n     };\n \n     GGML_ASSERT(n_stream == 1 || n_stream == n_seq_max);\n@@ -167,19 +174,16 @@ llama_kv_cache::llama_kv_cache(\n     }\n \n     // allocate tensors and initialize the buffers to avoid NaNs in the padding\n-    for (auto it : ctx_map) {\n-        auto * buft = it.first;\n-        auto * ctx  = it.second;\n-\n-        ggml_backend_buffer_t buf = ggml_backend_alloc_ctx_tensors_from_buft(ctx, buft);\n+    for (auto & [buft, ctx] : ctx_map) {\n+        ggml_backend_buffer_t buf = ggml_backend_alloc_ctx_tensors_from_buft(ctx.get(), buft);\n         if (!buf) {\n             throw std::runtime_error(\"failed to allocate buffer for kv cache\");\n         }\n \n         LLAMA_LOG_INFO(\"%s: %10s KV buffer size = %8.2f MiB\\n\", __func__, ggml_backend_buffer_name(buf), ggml_backend_buffer_get_size(buf)/1024.0/1024.0);\n \n         ggml_backend_buffer_clear(buf, 0);\n-        bufs.emplace_back(buf);\n+        ctxs_bufs.emplace_back(std::move(ctx), buf);\n     }\n \n     {\n@@ -203,7 +207,7 @@ void llama_kv_cache::clear(bool data) {\n     }\n \n     if (data) {\n-        for (auto & buf : bufs) {\n+        for (auto & [_, buf] : ctxs_bufs) {\n             ggml_backend_buffer_clear(buf.get(), 0);\n         }\n     }\n@@ -472,8 +476,8 @@ llama_pos llama_kv_cache::seq_pos_max(llama_seq_id seq_id) const {\n \n std::map<ggml_backend_buffer_type_t, size_t> llama_kv_cache::memory_breakdown() const {\n     std::map<ggml_backend_buffer_type_t, size_t> ret;\n-    for (const ggml_backend_buffer_ptr & buf_ptr : bufs) {\n-        ret[ggml_backend_buffer_get_type(buf_ptr.get())] += ggml_backend_buffer_get_size(buf_ptr.get());\n+    for (const auto & [_, buf] : ctxs_bufs) {\n+        ret[ggml_backend_buffer_get_type(buf.get())] += ggml_backend_buffer_get_size(buf.get());\n     }\n     return ret;\n }\n@@ -1298,7 +1302,7 @@ void llama_kv_cache::set_input_pos_bucket(ggml_tensor * dst, const llama_ubatch\n size_t llama_kv_cache::total_size() const {\n     size_t size = 0;\n \n-    for (const auto & buf : bufs) {\n+    for (const auto & [_, buf] : ctxs_bufs) {\n         size += ggml_backend_buffer_get_size(buf.get());\n     }\n "
      },
      {
        "filename": "src/llama-kv-cache.h",
        "status": "modified",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "patch": "@@ -217,8 +217,8 @@ class llama_kv_cache : public llama_memory_i {\n     // this is the SWA type of the cache - not to be confused with the model SWA type\n     const llama_swa_type swa_type = LLAMA_SWA_TYPE_NONE;\n \n-    std::vector<ggml_context_ptr>        ctxs;\n-    std::vector<ggml_backend_buffer_ptr> bufs;\n+    // ggml contexts for the KV cache along with the allocated backend buffers:\n+    std::vector<std::pair<ggml_context_ptr, ggml_backend_buffer_ptr>> ctxs_bufs;\n \n     // the current index from where we start searching for a free slot in the ring buffer of KV cells (see find_slot())\n     // note: this is not part of the KV state and it's only used to speed-up the find_slot() method"
      },
      {
        "filename": "src/llama-memory-recurrent.cpp",
        "status": "modified",
        "additions": 18,
        "deletions": 14,
        "changes": 32,
        "patch": "@@ -7,6 +7,7 @@\n \n #include <algorithm>\n #include <cassert>\n+#include <cstring>\n #include <limits>\n #include <map>\n #include <stdexcept>\n@@ -32,8 +33,15 @@ llama_memory_recurrent::llama_memory_recurrent(\n     cells.clear();\n     cells.resize(mem_size);\n \n+    // define a comparator for the buft -> ctx map to ensure that the order is well-defined:\n+    struct ggml_backend_buft_comparator {\n+        bool operator()(const ggml_backend_buffer_type_t & lhs, const ggml_backend_buffer_type_t & rhs) const {\n+            return strcmp(ggml_backend_buft_name(lhs), ggml_backend_buft_name(rhs)) < 0;\n+        }\n+    };\n+    std::map<ggml_backend_buffer_type_t, ggml_context_ptr, ggml_backend_buft_comparator> ctx_map;\n+\n     // create a context for each buffer type\n-    std::map<ggml_backend_buffer_type_t, ggml_context *> ctx_map;\n     auto ctx_for_buft = [&](ggml_backend_buffer_type_t buft) -> ggml_context * {\n         auto it = ctx_map.find(buft);\n         if (it == ctx_map.end()) {\n@@ -48,13 +56,12 @@ llama_memory_recurrent::llama_memory_recurrent(\n                 return nullptr;\n             }\n \n-            ctx_map[buft] = ctx;\n-            ctxs.emplace_back(ctx);\n+            ctx_map.emplace(buft, ctx);\n \n             return ctx;\n         }\n \n-        return it->second;\n+        return it->second.get();\n     };\n \n     r_l.resize(n_layer);\n@@ -93,17 +100,14 @@ llama_memory_recurrent::llama_memory_recurrent(\n     }\n \n     // allocate tensors and initialize the buffers to avoid NaNs in the padding\n-    for (auto it : ctx_map) {\n-        auto * buft = it.first;\n-        auto * ctx  = it.second;\n-\n-        ggml_backend_buffer_t buf = ggml_backend_alloc_ctx_tensors_from_buft(ctx, buft);\n+    for (auto & [buft, ctx] : ctx_map) {\n+        ggml_backend_buffer_t buf = ggml_backend_alloc_ctx_tensors_from_buft(ctx.get(), buft);\n         if (!buf) {\n             throw std::runtime_error(\"failed to allocate buffer for rs cache\");\n         }\n         ggml_backend_buffer_clear(buf, 0);\n         LLAMA_LOG_INFO(\"%s: %10s RS buffer size = %8.2f MiB\\n\", __func__, ggml_backend_buffer_name(buf), ggml_backend_buffer_get_size(buf)/1024.0/1024.0);\n-        bufs.emplace_back(buf);\n+        ctxs_bufs.emplace_back(std::move(ctx), buf);\n     }\n \n     {\n@@ -129,7 +133,7 @@ void llama_memory_recurrent::clear(bool data) {\n     used = 0;\n \n     if (data) {\n-        for (auto & buf : bufs) {\n+        for (auto & [_, buf] : ctxs_bufs) {\n             ggml_backend_buffer_clear(buf.get(), 0);\n         }\n     }\n@@ -364,8 +368,8 @@ llama_pos llama_memory_recurrent::seq_pos_max(llama_seq_id seq_id) const {\n \n std::map<ggml_backend_buffer_type_t, size_t> llama_memory_recurrent::memory_breakdown() const {\n     std::map<ggml_backend_buffer_type_t, size_t> ret;\n-    for (const ggml_backend_buffer_ptr & buf_ptr : bufs) {\n-        ret[ggml_backend_buffer_get_type(buf_ptr.get())] += ggml_backend_buffer_get_size(buf_ptr.get());\n+    for (const auto & [_, buf] : ctxs_bufs) {\n+        ret[ggml_backend_buffer_get_type(buf.get())] += ggml_backend_buffer_get_size(buf.get());\n     }\n     return ret;\n }\n@@ -662,7 +666,7 @@ bool llama_memory_recurrent::get_can_shift() const {\n \n size_t llama_memory_recurrent::total_size() const {\n     size_t size = 0;\n-    for (const auto & buf : bufs) {\n+    for (const auto & [_, buf] : ctxs_bufs) {\n         size += ggml_backend_buffer_get_size(buf.get());\n     }\n "
      },
      {
        "filename": "src/llama-memory-recurrent.h",
        "status": "modified",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "patch": "@@ -109,8 +109,8 @@ class llama_memory_recurrent : public llama_memory_i {\n \n     const uint32_t n_seq_max = 1;\n \n-    std::vector<ggml_context_ptr>        ctxs;\n-    std::vector<ggml_backend_buffer_ptr> bufs;\n+    // ggml contexts for the KV cache along with the allocated backend buffers:\n+    std::vector<std::pair<ggml_context_ptr, ggml_backend_buffer_ptr>> ctxs_bufs;\n \n     size_t total_size() const;\n "
      },
      {
        "filename": "src/llama-model.cpp",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -2231,7 +2231,7 @@ bool llama_model::load_tensors(llama_model_loader & ml) {\n     // define a comparator for the buft -> ctx map to ensure that the order is well-defined:\n     struct ggml_backend_buft_comparator {\n         bool operator()(const ggml_backend_buffer_type_t & lhs, const ggml_backend_buffer_type_t & rhs) const {\n-            return ggml_backend_buft_name(lhs) < ggml_backend_buft_name(rhs);\n+            return strcmp(ggml_backend_buft_name(lhs), ggml_backend_buft_name(rhs)) < 0;\n         }\n     };\n     std::map<ggml_backend_buffer_type_t, ggml_context_ptr, ggml_backend_buft_comparator> ctx_map;"
      }
    ],
    "num_files": 5,
    "scraped_at": "2025-11-16T23:29:37.469312"
  },
  {
    "pr_number": 16736,
    "title": "server : support unified cache across slots",
    "body": "ref https://github.com/ggml-org/llama.cpp/discussions/4130#discussioncomment-14754307\r\n\r\nCurrent logic in this PR (subject to change):\r\n\r\n- When using unified KV cache with `-kvu`, share the entire context `-c N` among all parallel slots of the server `-np N`\r\n- When we run out of space, try to free some by purging old sequences from idle slots, one by one, in no particular order\r\n- If we still run out of space, terminate all active slots at once\r\n- The `-np N` argument is still utilized to control the max number of parallel jobs, but it is no longer used to change the per-slot context\r\n- By default, start the server using 4 slots and unified KV cache\r\n\r\nExample:\r\n\r\n```bash\r\nllama-server -m model.gguf -c 8192 --jinja\r\n```\r\n\r\nTODO:\r\n\r\n- [x] When we run out of space, terminate the active slots one-by-one and keep trying\r\n- [ ] ~Think about instead of purging, to move the slot into host-memory cache. Not sure that this is really needed thanks to the existing logic from #16391~\r\n- [x] Add tests\r\n\r\nFuture improvements:\r\n\r\n- When run out of space, terminate slots one by one instead of all together\r\n- Update logic for starting a new task to check that it has some extra room for generation (not very sure if needed, current logic will simply purge one of the other slots, so it should be good as it is)",
    "html_url": "https://github.com/ggml-org/llama.cpp/pull/16736",
    "created_at": "2025-10-23T09:31:48Z",
    "merged_at": "2025-11-02T16:14:04Z",
    "merge_commit_sha": "cd5e3b57541ecc52421130742f4d89acbcf77cd4",
    "base_ref": "master",
    "head_sha": "56fceee2cbfba351f3e59cbcdf7fb18123dfd0e6",
    "user": "ggerganov",
    "files": [
      {
        "filename": "include/llama.h",
        "status": "modified",
        "additions": 4,
        "deletions": 1,
        "changes": 5,
        "patch": "@@ -461,7 +461,10 @@ extern \"C\" {\n     LLAMA_API bool llama_supports_gpu_offload(void);\n     LLAMA_API bool llama_supports_rpc        (void);\n \n+    // NOTE: After creating a llama_context, it is recommended to query the actual values using these functions\n+    //       In some cases the requested values via llama_context_params may differ from the actual values used by the context\n     LLAMA_API uint32_t llama_n_ctx      (const struct llama_context * ctx);\n+    LLAMA_API uint32_t llama_n_ctx_seq  (const struct llama_context * ctx);\n     LLAMA_API uint32_t llama_n_batch    (const struct llama_context * ctx);\n     LLAMA_API uint32_t llama_n_ubatch   (const struct llama_context * ctx);\n     LLAMA_API uint32_t llama_n_seq_max  (const struct llama_context * ctx);\n@@ -585,7 +588,7 @@ extern \"C\" {\n     LLAMA_API int32_t llama_adapter_meta_val_str_by_index(const struct llama_adapter_lora * adapter, int32_t i, char * buf, size_t buf_size);\n \n     // Manually free a LoRA adapter\n-    // Note: loaded adapters will be free when the associated model is deleted\n+    // NOTE: loaded adapters will be free when the associated model is deleted\n     LLAMA_API void llama_adapter_lora_free(struct llama_adapter_lora * adapter);\n \n     // Get the invocation tokens if the current lora is an alora"
      },
      {
        "filename": "src/llama-context.cpp",
        "status": "modified",
        "additions": 27,
        "deletions": 10,
        "changes": 37,
        "patch": "@@ -112,11 +112,24 @@ llama_context::llama_context(\n         }\n     }\n \n-    const uint32_t n_ctx_per_seq = cparams.n_ctx / cparams.n_seq_max;\n+    if (cparams.kv_unified) {\n+        cparams.n_ctx_seq = cparams.n_ctx;\n+    } else {\n+        cparams.n_ctx_seq = cparams.n_ctx / cparams.n_seq_max;\n+\n+        if (cparams.n_ctx_seq == 0) {\n+            throw std::runtime_error(\"n_ctx_seq == 0\");\n+        }\n+\n+        if (cparams.n_ctx != cparams.n_ctx_seq * cparams.n_seq_max) {\n+            cparams.n_ctx =  cparams.n_ctx_seq * cparams.n_seq_max;\n+            LLAMA_LOG_WARN(\"%s: n_ctx is not divisible by n_seq_max - rounding down to %u\\n\", __func__, cparams.n_ctx);\n+        }\n+    }\n \n     LLAMA_LOG_INFO(\"%s: n_seq_max     = %u\\n\",   __func__, cparams.n_seq_max);\n     LLAMA_LOG_INFO(\"%s: n_ctx         = %u\\n\",   __func__, cparams.n_ctx);\n-    LLAMA_LOG_INFO(\"%s: n_ctx_per_seq = %u\\n\",   __func__, n_ctx_per_seq);\n+    LLAMA_LOG_INFO(\"%s: n_ctx_seq     = %u\\n\",   __func__, cparams.n_ctx_seq);\n     LLAMA_LOG_INFO(\"%s: n_batch       = %u\\n\",   __func__, cparams.n_batch);\n     LLAMA_LOG_INFO(\"%s: n_ubatch      = %u\\n\",   __func__, cparams.n_ubatch);\n     LLAMA_LOG_INFO(\"%s: causal_attn   = %d\\n\",   __func__, cparams.causal_attn);\n@@ -125,14 +138,14 @@ llama_context::llama_context(\n     LLAMA_LOG_INFO(\"%s: freq_base     = %.1f\\n\", __func__, cparams.rope_freq_base);\n     LLAMA_LOG_INFO(\"%s: freq_scale    = %g\\n\",   __func__, cparams.rope_freq_scale);\n \n-    if (n_ctx_per_seq < hparams.n_ctx_train) {\n-        LLAMA_LOG_WARN(\"%s: n_ctx_per_seq (%u) < n_ctx_train (%u) -- the full capacity of the model will not be utilized\\n\",\n-                __func__, n_ctx_per_seq, hparams.n_ctx_train);\n+    if (cparams.n_ctx_seq < hparams.n_ctx_train) {\n+        LLAMA_LOG_WARN(\"%s: n_ctx_seq (%u) < n_ctx_train (%u) -- the full capacity of the model will not be utilized\\n\",\n+                __func__, cparams.n_ctx_seq, hparams.n_ctx_train);\n     }\n \n-    if (n_ctx_per_seq > hparams.n_ctx_train) {\n-        LLAMA_LOG_WARN(\"%s: n_ctx_per_seq (%u) > n_ctx_train (%u) -- possible training context overflow\\n\",\n-                __func__, n_ctx_per_seq, hparams.n_ctx_train);\n+    if (cparams.n_ctx_seq > hparams.n_ctx_train) {\n+        LLAMA_LOG_WARN(\"%s: n_ctx_seq (%u) > n_ctx_train (%u) -- possible training context overflow\\n\",\n+                __func__, cparams.n_ctx_seq, hparams.n_ctx_train);\n     }\n \n     if (!hparams.vocab_only) {\n@@ -453,8 +466,8 @@ uint32_t llama_context::n_ctx() const {\n     return cparams.n_ctx;\n }\n \n-uint32_t llama_context::n_ctx_per_seq() const {\n-    return cparams.n_ctx / cparams.n_seq_max;\n+uint32_t llama_context::n_ctx_seq() const {\n+    return cparams.n_ctx_seq;\n }\n \n uint32_t llama_context::n_batch() const {\n@@ -2383,6 +2396,10 @@ uint32_t llama_n_ctx(const llama_context * ctx) {\n     return ctx->n_ctx();\n }\n \n+uint32_t llama_n_ctx_seq(const llama_context * ctx) {\n+    return ctx->n_ctx_seq();\n+}\n+\n uint32_t llama_n_batch(const llama_context * ctx) {\n     return ctx->n_batch();\n }"
      },
      {
        "filename": "src/llama-context.h",
        "status": "modified",
        "additions": 5,
        "deletions": 5,
        "changes": 10,
        "patch": "@@ -43,11 +43,11 @@ struct llama_context {\n \n     ggml_backend_sched_t get_sched() const;\n \n-    uint32_t n_ctx()         const;\n-    uint32_t n_ctx_per_seq() const;\n-    uint32_t n_batch()       const;\n-    uint32_t n_ubatch()      const;\n-    uint32_t n_seq_max()     const;\n+    uint32_t n_ctx()     const;\n+    uint32_t n_ctx_seq() const;\n+    uint32_t n_batch()   const;\n+    uint32_t n_ubatch()  const;\n+    uint32_t n_seq_max() const;\n \n     uint32_t n_threads()       const;\n     uint32_t n_threads_batch() const;"
      },
      {
        "filename": "src/llama-cparams.h",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -8,6 +8,7 @@\n \n struct llama_cparams {\n     uint32_t n_ctx;           // context size used during inference\n+    uint32_t n_ctx_seq;       // context for a single sequence\n     uint32_t n_batch;\n     uint32_t n_ubatch;\n     uint32_t n_seq_max;"
      },
      {
        "filename": "src/llama-model.cpp",
        "status": "modified",
        "additions": 4,
        "deletions": 10,
        "changes": 14,
        "patch": "@@ -6712,14 +6712,14 @@ float llama_model::get_rope_freq_scale(const llama_cparams & cparams, int il) co\n }\n \n ggml_tensor * llama_model::get_rope_factors(const llama_cparams & cparams, int il) const {\n-    const uint32_t n_ctx_per_seq = cparams.n_ctx / cparams.n_seq_max;\n+    const uint32_t n_ctx_seq = cparams.n_ctx_seq;\n \n     // choose long/short freq factors based on the context size\n     if (layers[il].rope_freqs != nullptr) {\n         return layers[il].rope_freqs;\n     }\n \n-    if (n_ctx_per_seq > hparams.n_ctx_orig_yarn) {\n+    if (n_ctx_seq > hparams.n_ctx_orig_yarn) {\n         return layers[il].rope_long;\n     }\n \n@@ -6795,12 +6795,6 @@ llama_memory_i * llama_model::create_memory(const llama_memory_params & params,\n                         /* filter_attn       */ std::move(filter_attn),\n                         /* filter_recr       */ std::move(filter_recr));\n                 } else {\n-                    uint32_t n_ctx_per_stream = cparams.n_ctx;\n-\n-                    if (!cparams.kv_unified) {\n-                        n_ctx_per_stream = (cparams.n_ctx + cparams.n_seq_max - 1)/cparams.n_seq_max;\n-                    }\n-\n                     llama_memory_i::layer_reuse_cb reuse = nullptr;\n \n                     if (arch == LLM_ARCH_GEMMA3N) {\n@@ -6824,7 +6818,7 @@ llama_memory_i * llama_model::create_memory(const llama_memory_params & params,\n                                 cparams.offload_kqv,\n                                 params.swa_full,\n                                 cparams.kv_unified,\n-                                n_ctx_per_stream,\n+                                cparams.n_ctx_seq,\n                                 cparams.n_seq_max,\n                                 cparams.n_ubatch,\n                                 1,\n@@ -6840,7 +6834,7 @@ llama_memory_i * llama_model::create_memory(const llama_memory_params & params,\n                                 !cparams.flash_attn,\n                                 cparams.offload_kqv,\n                                 cparams.kv_unified,\n-                                n_ctx_per_stream,\n+                                cparams.n_ctx_seq,\n                                 cparams.n_seq_max,\n                                 1,\n                                 hparams.n_swa,"
      },
      {
        "filename": "tests/test-thread-safety.cpp",
        "status": "modified",
        "additions": 8,
        "deletions": 1,
        "changes": 9,
        "patch": "@@ -131,7 +131,14 @@ int main(int argc, char ** argv) {\n                     }\n \n                     batch = llama_batch_get_one(&token, 1);\n-                    if (llama_decode(ctx.get(), batch)) {\n+\n+                    int ret = llama_decode(ctx.get(), batch);\n+                    if (ret == 1 && i > 0) {\n+                        LOG_INF(\"Context full, stopping generation.\\n\");\n+                        break;\n+                    }\n+\n+                    if (ret != 0) {\n                         LOG_ERR(\"Model %d/%d, Context %d/%d: failed to decode\\n\", m + 1, num_models, c + 1, num_contexts);\n                         failed.store(true);\n                         return;"
      },
      {
        "filename": "tools/server/server.cpp",
        "status": "modified",
        "additions": 72,
        "deletions": 14,
        "changes": 86,
        "patch": "@@ -2407,7 +2407,7 @@ struct server_context {\n \n             params_dft.devices      = params_base.speculative.devices;\n             params_dft.model        = params_base.speculative.model;\n-            params_dft.n_ctx        = params_base.speculative.n_ctx == 0 ? params_base.n_ctx / params_base.n_parallel : params_base.speculative.n_ctx;\n+            params_dft.n_ctx        = params_base.speculative.n_ctx == 0 ? llama_n_ctx_seq(ctx) : params_base.speculative.n_ctx;\n             params_dft.n_gpu_layers = params_base.speculative.n_gpu_layers;\n             params_dft.n_parallel   = 1;\n             params_dft.cache_type_k = params_base.speculative.cache_type_k;\n@@ -2495,10 +2495,16 @@ struct server_context {\n     }\n \n     void init() {\n-        const int32_t n_ctx_slot = n_ctx / params_base.n_parallel;\n-\n         SRV_INF(\"initializing slots, n_slots = %d\\n\", params_base.n_parallel);\n \n+        const int n_ctx_train = llama_model_n_ctx_train(model);\n+\n+        int n_ctx_slot = llama_n_ctx_seq(ctx);\n+        if (n_ctx_slot > n_ctx_train) {\n+            SRV_WRN(\"the slot context (%d) exceeds the training context of the model (%d) - capping\\n\", n_ctx_slot, n_ctx_train);\n+            n_ctx_slot = n_ctx_train;\n+        }\n+\n         for (int i = 0; i < params_base.n_parallel; i++) {\n             server_slot slot;\n \n@@ -2527,7 +2533,7 @@ struct server_context {\n                 }\n             }\n \n-            SLT_INF(slot, \"new slot n_ctx_slot = %d\\n\", slot.n_ctx);\n+            SLT_INF(slot, \"new slot, n_ctx = %d\\n\", slot.n_ctx);\n \n             slot.callback_on_release = [this](int) {\n                 queue_tasks.pop_deferred_task();\n@@ -2699,6 +2705,39 @@ struct server_context {\n         return ret;\n     }\n \n+    // return true if at least one slot has been purged\n+    // TODO: improve logic\n+    //       - smarter decision which slot to purge (LRU or longest prompt?)\n+    //       - move slot to level 2 cache instead of removing?\n+    //       - instead of purging, try to store and resume later?\n+    bool try_purge_idle_slots() {\n+        bool res = false;\n+\n+        if (!params_base.kv_unified) {\n+            return res;\n+        }\n+\n+        for (auto & slot : slots) {\n+            if (slot.is_processing()) {\n+                continue;\n+            }\n+\n+            if (slot.prompt.n_tokens() > 0) {\n+                SRV_WRN(\"purging slot %d with %zu tokens\\n\", slot.id, slot.prompt.tokens.size());\n+\n+                llama_memory_seq_rm(llama_get_memory(ctx), slot.id, -1, -1);\n+                slot.prompt.tokens.clear();\n+\n+                res = true;\n+\n+                // purge slots one by one\n+                break;\n+            }\n+        }\n+\n+        return res;\n+    }\n+\n     bool launch_slot_with_task(server_slot & slot, server_task && task) {\n         slot.reset();\n \n@@ -3635,9 +3674,10 @@ struct server_context {\n         int32_t n_batch  = llama_n_batch(ctx);\n         int32_t n_ubatch = llama_n_ubatch(ctx);\n \n-        // next, batch any pending prompts without exceeding n_batch\n-        float alora_scale = -1.0f;\n+        float  alora_scale       = -1.0f;\n         size_t alora_disabled_id = 0;\n+\n+        // next, batch any pending prompts without exceeding n_batch\n         if (params_base.cont_batching || batch.n_tokens == 0) {\n             for (auto & slot : slots) {\n                 // check if we can batch this slot with the previous one\n@@ -3914,8 +3954,11 @@ struct server_context {\n \n                     // truncate any tokens that are beyond n_past for this slot\n                     const llama_pos p0 = slot.prompt.tokens.pos_next();\n+\n+                    SLT_INF(slot, \"n_tokens = %d, memory_seq_rm [%d, end)\\n\", slot.prompt.n_tokens(), p0);\n+\n                     if (!llama_memory_seq_rm(llama_get_memory(ctx), slot.id, p0, -1)) {\n-                        SLT_WRN(slot, \"failed to truncate tokens with position >= %d\\n\", p0);\n+                        SLT_WRN(slot, \"failed to truncate tokens with position >= %d - clearing the memory\\n\", p0);\n                         llama_memory_seq_rm(llama_get_memory(ctx), slot.id, -1, -1);\n \n                         // there is no common part left\n@@ -3924,8 +3967,6 @@ struct server_context {\n                         slot.prompt.tokens.clear();\n                     }\n \n-                    SLT_INF(slot, \"n_tokens = %d, memory_seq_rm [%d, end)\\n\", slot.prompt.n_tokens(), p0);\n-\n                     // check if we should process the image\n                     if (slot.prompt.n_tokens() < slot.task->n_tokens() && input_tokens[slot.prompt.n_tokens()] == LLAMA_TOKEN_NULL) {\n                         // process the image\n@@ -4126,6 +4167,8 @@ struct server_context {\n                     std::string err;\n \n                     if (n_batch == 1 && ret == 1) {\n+                        // TODO: try to terminate only the largest active slot/sequence and continue with the rest\n+                        //       need to remove the tokens from the current batch too\n                         err = \"Context size has been exceeded.\";\n                     }\n \n@@ -4141,17 +4184,23 @@ struct server_context {\n                     // TODO: handle ret == 2 (abort) when we start aborting\n \n                     if (!err.empty()) {\n-                        SRV_ERR(\"%s, i = %d, n_batch = %d, ret = %d\\n\", err.c_str(), i, n_batch, ret);\n+                        SRV_ERR(\"%s i = %d, n_batch = %d, ret = %d\\n\", err.c_str(), i, n_batch, ret);\n+\n                         for (auto & slot : slots) {\n-                            send_error(slot, err);\n-                            slot.release();\n+                            if (slot.is_processing()) {\n+                                send_error(slot, err);\n+                                slot.release();\n+                            }\n                         }\n+\n                         break;\n                     }\n                 }\n \n                 // retry with half the batch size to try to find a free slot in the KV cache\n-                n_batch /= 2;\n+                if (!try_purge_idle_slots()) {\n+                    n_batch /= 2;\n+                }\n \n                 SRV_WRN(\"failed to find free space in the KV cache, retrying with smaller batch size, i = %d, n_batch = %d, ret = %d\\n\", i, n_batch, ret);\n \n@@ -4391,6 +4440,15 @@ int main(int argc, char ** argv) {\n         return 1;\n     }\n \n+    // TODO: should we have a separate n_parallel parameter for the server?\n+    //       https://github.com/ggml-org/llama.cpp/pull/16736#discussion_r2483763177\n+    if (params.n_parallel == 1 && params.kv_unified == false) {\n+        LOG_WRN(\"%s: setting n_parallel = 4 and kv_unified = true\\n\", __func__);\n+\n+        params.n_parallel = 4;\n+        params.kv_unified = true;\n+    }\n+\n     common_init();\n \n     // struct that contains llama context and inference\n@@ -4944,7 +5002,7 @@ int main(int argc, char ** argv) {\n                 // Everything else, including multimodal completions.\n                 inputs = tokenize_input_prompts(ctx_server.vocab, ctx_server.mctx, prompt, true, true);\n             }\n-            const size_t n_ctx_slot = ctx_server.n_ctx / ctx_server.params_base.n_parallel;\n+            const size_t n_ctx_slot = ctx_server.slots.front().n_ctx;\n             tasks.reserve(inputs.size());\n             for (size_t i = 0; i < inputs.size(); i++) {\n                 auto n_prompt_tokens = inputs[i].size();"
      },
      {
        "filename": "tools/server/tests/unit/test_chat_completion.py",
        "status": "modified",
        "additions": 4,
        "deletions": 4,
        "changes": 8,
        "patch": "@@ -433,21 +433,21 @@ def test_context_size_exceeded_stream():\n @pytest.mark.parametrize(\n     \"n_batch,batch_count,reuse_cache\",\n     [\n-        (64, 15, False),\n+        (64, 3, False),\n         (64, 1, True),\n     ]\n )\n-def test_return_progresssss(n_batch, batch_count, reuse_cache):\n+def test_return_progress(n_batch, batch_count, reuse_cache):\n     global server\n     server.n_batch = n_batch\n-    server.n_ctx = 2048\n+    server.n_ctx = 256\n     server.n_slots = 1\n     server.start()\n     def make_cmpl_request():\n         return server.make_stream_request(\"POST\", \"/chat/completions\", data={\n             \"max_tokens\": 10,\n             \"messages\": [\n-                {\"role\": \"user\", \"content\": \"This is a test\" * 100},\n+                {\"role\": \"user\", \"content\": \"This is a test\" * 10},\n             ],\n             \"stream\": True,\n             \"return_progress\": True,"
      },
      {
        "filename": "tools/server/tests/unit/test_completion.py",
        "status": "modified",
        "additions": 31,
        "deletions": 0,
        "changes": 31,
        "patch": "@@ -368,6 +368,37 @@ def check_slots_status():\n         # assert match_regex(re_content, res.body[\"content\"])\n \n \n+@pytest.mark.parametrize(\n+    \"n_ctx,n_slots,n_predict_vals,expected_success\",\n+    [\n+        (256, 4, [80, 40, 80, 80], [True,  True,  True,  True]),\n+        (256, 4, [70, 70, 70, 70], [False, False, False, False]),\n+        (256, 4, [90, 90, 40, 90], [False, False, True,  False]),\n+        (256, 4, [90, 90, 40, 75], [True,  True,  True,  True]),\n+    ],\n+)\n+def test_completion_unified(n_ctx, n_slots, n_predict_vals, expected_success):\n+    global server\n+    server.n_slots = n_slots\n+    server.kv_unified = True\n+    server.n_ctx = n_ctx\n+    server.start()\n+    prompt = \"A\"\n+    tasks = []\n+    for n_predict in n_predict_vals:\n+        tasks.append((server.make_request, (\"POST\", \"/completion\", {\"prompt\": prompt, \"n_predict\": n_predict})))\n+    results = parallel_function_calls(tasks)\n+    for res, n_predict, expect_ok in zip(results, n_predict_vals, expected_success):\n+        if expect_ok:\n+            assert res.status_code == 200\n+            assert \"content\" in res.body\n+            if \"timings\" in res.body:\n+                assert res.body[\"timings\"][\"predicted_n\"] == n_predict\n+        else:\n+            assert res.status_code == 500\n+            assert \"content\" not in res.body\n+\n+\n @pytest.mark.parametrize(\n     \"prompt,n_predict,response_fields\",\n     ["
      },
      {
        "filename": "tools/server/tests/unit/test_infill.py",
        "status": "modified",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "patch": "@@ -18,7 +18,7 @@ def test_infill_without_input_extra():\n         \"input_suffix\": \"}\\n\",\n     })\n     assert res.status_code == 200\n-    assert match_regex(\"(Ann|small|shiny|Daddy)+\", res.body[\"content\"])\n+    assert match_regex(\"(Ann|small|shiny|Daddy|Jimmy)+\", res.body[\"content\"])\n \n \n def test_infill_with_input_extra():\n@@ -34,7 +34,7 @@ def test_infill_with_input_extra():\n         \"input_suffix\": \"}\\n\",\n     })\n     assert res.status_code == 200\n-    assert match_regex(\"(Dad|excited|park)+\", res.body[\"content\"])\n+    assert match_regex(\"(Dad|excited|park|Jimmy)+\", res.body[\"content\"])\n \n \n @pytest.mark.parametrize(\"input_extra\", ["
      },
      {
        "filename": "tools/server/tests/utils.py",
        "status": "modified",
        "additions": 3,
        "deletions": 0,
        "changes": 3,
        "patch": "@@ -78,6 +78,7 @@ class ServerProcess:\n     server_embeddings: bool | None = False\n     server_reranking: bool | None = False\n     server_metrics: bool | None = False\n+    kv_unified: bool | None = False\n     server_slots: bool | None = False\n     pooling: str | None = None\n     draft: int | None = None\n@@ -159,6 +160,8 @@ def start(self, timeout_seconds: int | None = DEFAULT_HTTP_TIMEOUT) -> None:\n             server_args.append(\"--reranking\")\n         if self.server_metrics:\n             server_args.append(\"--metrics\")\n+        if self.kv_unified:\n+            server_args.append(\"--kv-unified\")\n         if self.server_slots:\n             server_args.append(\"--slots\")\n         else:"
      },
      {
        "filename": "tools/server/utils.hpp",
        "status": "modified",
        "additions": 2,
        "deletions": 1,
        "changes": 3,
        "patch": "@@ -1212,7 +1212,7 @@ struct server_tokens {\n             for (auto it = tokens.map_idx_to_media.begin(); it != tokens.map_idx_to_media.end(); ) {\n                 auto * chunk = tokens.map_idx_to_media[it->first].get();\n                 mtmd::input_chunk_ptr new_chunk(mtmd_input_chunk_copy(chunk));\n-                map_idx_to_media[start_idx+it->first] = std::move(new_chunk);\n+                map_idx_to_media[start_idx + it->first] = std::move(new_chunk);\n             }\n         }\n     }\n@@ -1244,6 +1244,7 @@ struct server_tokens {\n     }\n \n     void clear() {\n+        map_idx_to_media.clear();\n         tokens.clear();\n     }\n "
      }
    ],
    "num_files": 12,
    "scraped_at": "2025-11-16T23:29:39.328547"
  },
  {
    "pr_number": 16715,
    "title": "CUDA: General GEMV fusion",
    "body": "This is a follow up to #16630. This PR adds ability to fuse the following common GEMV operations:\r\n\r\n- GLU \r\n- Bias + GLU\r\n- Bias\r\n\r\nIt uses a template bool to determine if we are in the fusion path, then does runtime checks for which fusion path to take. This PR also splits up `mmvq` (by type) and `mmvf` (by `ncols-dst`) as their compile times were becoming large after this change. This change helps TG (which is IO bound) to almost all class of models. Apart from adding tests to `test-backend-ops` I also spot-checked perplexity on a couple of models and it is unchanged by this change.\r\n\r\nTested on 6x 4090\r\n\r\n| Model                  | Test   |   t/s master |   t/s cuda_fuse_gate |   Speedup |\r\n|:----------------------|:-------|-------------:|---------------------:|----------:|\r\n| gpt-oss 120B MXFP4 MoE | tg32   |       118.72 |               125.14 |      1.05 |\r\n| gpt-oss 120B MXFP4 MoE | tg64   |       116.91 |               123.09 |      1.05 |\r\n| gpt-oss 120B MXFP4 MoE | tg128  |       115.72 |               121.74 |      1.05 |\r\n| gpt-oss 20B MXFP4 MoE | tg32   |       171.60 |               180.07 |      1.05 |\r\n| gpt-oss 20B MXFP4 MoE | tg64   |       169.46 |               177.63 |      1.05 |\r\n| gpt-oss 20B MXFP4 MoE | tg128  |       167.58 |               175.59 |      1.05 |\r\n| qwen3moe 30B.A3B Q4_0 | tg32   |       154.72 |               162.06 |      1.05 |\r\n| qwen3moe 30B.A3B Q4_0 | tg64   |       151.37 |               158.40 |      1.05 |\r\n| qwen3moe 30B.A3B Q4_0 | tg128  |       149.25 |               156.00 |      1.05 |\r\n| qwen3 0.6B F16 | tg32   |       310.61 |               333.92 |      1.08 |\r\n| qwen3 0.6B F16 | tg64   |       306.26 |               325.99 |      1.06 |\r\n| qwen3 0.6B F16 | tg128  |       303.14 |               322.62 |      1.06 |\r\n| glm4moe 106B.A12B IQ4_XS - 4.25 bpw | tg32   |        68.99 |                72.30 |      1.05 |\r\n| glm4moe 106B.A12B IQ4_XS - 4.25 bpw | tg64   |        68.24 |                71.44 |      1.05 |\r\n| glm4moe 106B.A12B IQ4_XS - 4.25 bpw | tg128  |        67.53 |                70.71 |      1.05 |\r\n| llama 8B Q4_0 | tg32   |       133.00 |               137.42 |      1.03 |\r\n| llama 8B Q4_0 | tg64   |       131.89 |               136.47 |      1.03 |\r\n| llama 8B Q4_0 | tg128  |       130.78 |               135.35 |      1.03 |\r\n| gemma 7B Q4_0 | tg32   |       123.23 |               126.88 |      1.03 |\r\n| gemma 7B Q4_0 | tg64   |       122.28 |               125.76 |      1.03 |\r\n| gemma 7B Q4_0 | tg128  |       121.45 |               124.74 |      1.03 |\r\n\r\n",
    "html_url": "https://github.com/ggml-org/llama.cpp/pull/16715",
    "created_at": "2025-10-22T07:59:08Z",
    "merged_at": "2025-10-26T11:28:05Z",
    "merge_commit_sha": "f77c13b91f4d25754b6a0b857f98a6bc922a0aa7",
    "base_ref": "master",
    "head_sha": "975ef381c55d6fac60332fb2c9bc459d811c6eb4",
    "user": "am17an",
    "files": [
      {
        "filename": "ggml/src/ggml-cuda/common.cuh",
        "status": "modified",
        "additions": 13,
        "deletions": 0,
        "changes": 13,
        "patch": "@@ -1005,3 +1005,16 @@ struct ggml_backend_cuda_context {\n         return pool(device);\n     }\n };\n+\n+struct ggml_cuda_mm_fusion_args_host {\n+    const ggml_tensor * x_bias = nullptr;\n+    const ggml_tensor * gate = nullptr;\n+    const ggml_tensor * gate_bias = nullptr;\n+    ggml_glu_op glu_op;\n+};\n+struct ggml_cuda_mm_fusion_args_device {\n+    const void * x_bias = nullptr;\n+    const void * gate = nullptr;\n+    const void * gate_bias = nullptr;\n+    ggml_glu_op glu_op;\n+};"
      },
      {
        "filename": "ggml/src/ggml-cuda/convert.cuh",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -1,3 +1,4 @@\n+#pragma once\n #include \"common.cuh\"\n \n #define CUDA_DEQUANTIZE_BLOCK_SIZE 256"
      },
      {
        "filename": "ggml/src/ggml-cuda/ggml-cuda.cu",
        "status": "modified",
        "additions": 352,
        "deletions": 1,
        "changes": 353,
        "patch": "@@ -2007,6 +2007,147 @@ static void ggml_cuda_mul_mat_batched_cublas(ggml_backend_cuda_context & ctx, co\n     }\n }\n \n+static bool ggml_cuda_should_fuse_mul_mat(const ggml_tensor * ffn_up,\n+                                          const ggml_tensor * ffn_gate,\n+                                          const ggml_tensor * glu,\n+                                          const ggml_tensor * ffn_up_bias = nullptr,\n+                                          const ggml_tensor * ffn_gate_bias = nullptr) {\n+    const bool has_bias = ffn_up_bias != nullptr || ffn_gate_bias != nullptr;\n+\n+    if (has_bias && (!ffn_up_bias || !ffn_gate_bias)) {\n+        return false;\n+    }\n+\n+    const bool is_mul_mat     = ffn_up->op == GGML_OP_MUL_MAT     && ffn_gate->op == GGML_OP_MUL_MAT     && glu->op == GGML_OP_GLU;\n+    const bool is_mul_mat_id  = ffn_up->op == GGML_OP_MUL_MAT_ID  && ffn_gate->op == GGML_OP_MUL_MAT_ID  && glu->op == GGML_OP_GLU;\n+\n+    GGML_ASSERT(ffn_up && ffn_gate && glu);\n+\n+    if (!is_mul_mat && !is_mul_mat_id) {\n+        return false;\n+    }\n+\n+    const ggml_op expected_bias_op = is_mul_mat ? GGML_OP_ADD : GGML_OP_ADD_ID;\n+\n+    if (has_bias) {\n+        if (ffn_up_bias->op != expected_bias_op || ffn_gate_bias->op != expected_bias_op) {\n+            return false;\n+        }\n+\n+        if (glu->src[0] != ffn_gate_bias || glu->src[1] != ffn_up_bias) {\n+            return false;\n+        }\n+\n+        if (expected_bias_op == GGML_OP_ADD) {\n+            const bool up_has_mul   = ffn_up_bias->src[0] == ffn_up || ffn_up_bias->src[1] == ffn_up;\n+            const bool gate_has_mul = ffn_gate_bias->src[0] == ffn_gate || ffn_gate_bias->src[1] == ffn_gate;\n+            if (!up_has_mul || !gate_has_mul) {\n+                return false;\n+            }\n+        } else { // GGML_OP_ADD_ID\n+            if (ffn_up_bias->src[0] != ffn_up || ffn_gate_bias->src[0] != ffn_gate) {\n+                return false;\n+            }\n+            if (ffn_up_bias->src[2] != ffn_up->src[2] || ffn_gate_bias->src[2] != ffn_gate->src[2]) {\n+                return false;\n+            }\n+        }\n+    } else {\n+        if (glu->src[0] != ffn_gate && glu->src[1] != ffn_up) {\n+            return false;\n+        }\n+    }\n+\n+    if (ffn_up->src[0]->type != ffn_gate->src[0]->type || !ggml_are_same_shape(ffn_up->src[0], ffn_gate->src[0]) ||\n+        !ggml_are_same_stride(ffn_up->src[0], ffn_gate->src[0])) {\n+        return false;\n+    }\n+\n+    if (ffn_up->src[1] != ffn_gate->src[1]) {\n+        return false;\n+    }\n+\n+    if (ffn_up->src[2] && (ffn_up->src[2] != ffn_gate->src[2])) {\n+        return false;\n+    }\n+\n+    static constexpr std::array<ggml_glu_op, 3> valid_glu_ops = { GGML_GLU_OP_SWIGLU, GGML_GLU_OP_GEGLU, GGML_GLU_OP_SWIGLU_OAI };\n+\n+    if (std::find(valid_glu_ops.begin(), valid_glu_ops.end(), ggml_get_glu_op(glu)) == valid_glu_ops.end()) {\n+        return false;\n+    }\n+\n+    if (const bool swapped = ggml_get_op_params_i32(glu, 1); swapped) {\n+        return false;\n+    }\n+\n+    const bool split = ggml_backend_buft_is_cuda_split(ffn_up->src[0]->buffer->buft) ||\n+                       ggml_backend_buft_is_cuda_split(ffn_gate->src[0]->buffer->buft);\n+\n+    //TODO: add support for fusion for split buffers\n+    if (split) {\n+        return false;\n+    }\n+\n+    return true;\n+}\n+\n+static bool ggml_cuda_should_fuse_mul_mat_vec_f(const ggml_tensor * tensor) {\n+    ggml_tensor *       src0 = tensor->src[0];\n+    ggml_tensor *       src1 = tensor->src[1];\n+    const ggml_tensor * dst  = tensor;\n+\n+    const bool is_mul_mat_id = tensor->op == GGML_OP_MUL_MAT_ID;\n+\n+    bool use_mul_mat_vec_f =\n+        (src0->type == GGML_TYPE_F32 || src0->type == GGML_TYPE_F16 || src0->type == GGML_TYPE_BF16) &&\n+        src1->type == GGML_TYPE_F32 && dst->type == GGML_TYPE_F32;\n+\n+    const int cc      = ggml_cuda_info().devices[ggml_cuda_get_device()].cc;\n+    use_mul_mat_vec_f = use_mul_mat_vec_f && ggml_cuda_should_use_mmvf(src0->type, cc, src0->ne, is_mul_mat_id ? src1->ne[2] : src1->ne[1]);\n+\n+    //we only support fusion for ncols_dst = 1\n+    if (tensor->op == GGML_OP_MUL_MAT && dst->ne[1] != 1) {\n+        return false;\n+    }\n+\n+    if (tensor->op == GGML_OP_MUL_MAT_ID && dst->ne[2] != 1) {\n+        return false;\n+    }\n+\n+\n+    return use_mul_mat_vec_f;\n+}\n+\n+static bool ggml_cuda_should_fuse_mul_mat_vec_q(const ggml_tensor * tensor) {\n+    ggml_tensor *       src0 = tensor->src[0];\n+    ggml_tensor *       src1 = tensor->src[1];\n+    const ggml_tensor * dst  = tensor;\n+\n+    const bool bad_padding_clear = ggml_backend_buffer_get_usage(src0->buffer) == GGML_BACKEND_BUFFER_USAGE_COMPUTE &&\n+                                   ggml_nbytes(src0) != ggml_backend_buffer_get_alloc_size(src0->buffer, src0) &&\n+                                   src0->view_src;\n+\n+    bool use_mul_mat_vec_q = ggml_is_quantized(src0->type) && !bad_padding_clear && src1->type == GGML_TYPE_F32 &&\n+                             dst->type == GGML_TYPE_F32 && src1->ne[1] <= MMVQ_MAX_BATCH_SIZE;\n+\n+    // fusion is not universally faster on Pascal\n+    const int cc = ggml_cuda_info().devices[ggml_cuda_get_device()].cc;\n+    if (cc <= GGML_CUDA_CC_PASCAL) {\n+        return false;\n+    }\n+    //we only support fusion for ncols_dst = 1\n+    if (tensor->op == GGML_OP_MUL_MAT && dst->ne[1] != 1) {\n+        return false;\n+    }\n+\n+    if (tensor->op == GGML_OP_MUL_MAT_ID && dst->ne[2] != 1) {\n+        return false;\n+    }\n+\n+    return use_mul_mat_vec_q;\n+}\n+\n static void ggml_cuda_mul_mat(ggml_backend_cuda_context & ctx, const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst) {\n     const bool split = ggml_backend_buft_is_cuda_split(src0->buffer->buft);\n \n@@ -2745,7 +2886,7 @@ static bool ggml_graph_node_has_matching_properties(ggml_tensor * node, ggml_gra\n         }\n     }\n \n-    if (node->op == GGML_OP_SCALE &&\n+    if ((node->op == GGML_OP_SCALE || node->op == GGML_OP_GLU) &&\n         memcmp(graph_node_properties->op_params, node->op_params, GGML_MAX_OP_PARAMS) != 0) {\n         return false;\n     }\n@@ -2854,6 +2995,38 @@ static bool ggml_cuda_can_fuse(const struct ggml_cgraph * cgraph, int node_idx,\n         }\n     }\n \n+    std::initializer_list<enum ggml_op> mul_mat_bias_glu_ops    = { GGML_OP_MUL_MAT,    GGML_OP_ADD,    GGML_OP_MUL_MAT,    GGML_OP_ADD,    GGML_OP_GLU };\n+    std::initializer_list<enum ggml_op> mul_mat_id_bias_glu_ops = { GGML_OP_MUL_MAT_ID, GGML_OP_ADD_ID, GGML_OP_MUL_MAT_ID, GGML_OP_ADD_ID, GGML_OP_GLU };\n+\n+    std::initializer_list<enum ggml_op> mul_mat_id_glu_ops = { GGML_OP_MUL_MAT_ID, GGML_OP_MUL_MAT_ID, GGML_OP_GLU };\n+    std::initializer_list<enum ggml_op> mul_mat_glu_ops    = { GGML_OP_MUL_MAT,    GGML_OP_MUL_MAT,    GGML_OP_GLU };\n+\n+    if (ops.size() == 5 && (ggml_can_fuse_subgraph(cgraph, node_idx, ops, {node_idx + 4}) ||\n+                            ggml_can_fuse_subgraph(cgraph, node_idx, ops, {node_idx + 4}))) {\n+\n+        const ggml_tensor * ffn_gate      = cgraph->nodes[node_idx];\n+        const ggml_tensor * ffn_gate_bias = cgraph->nodes[node_idx + 1];\n+        const ggml_tensor * ffn_up        = cgraph->nodes[node_idx + 2];\n+        const ggml_tensor * ffn_up_bias   = cgraph->nodes[node_idx + 3];\n+        const ggml_tensor * glu           = cgraph->nodes[node_idx + 4];\n+\n+        if (ggml_cuda_should_fuse_mul_mat(ffn_up, ffn_gate, glu, ffn_up_bias, ffn_gate_bias)) {\n+            return true;\n+        }\n+    }\n+\n+    if (ops.size() == 3 && (ggml_can_fuse_subgraph(cgraph, node_idx, ops, {node_idx + 2}) ||\n+                            ggml_can_fuse_subgraph(cgraph, node_idx, ops, {node_idx + 2}))) {\n+\n+        const ggml_tensor * ffn_gate = cgraph->nodes[node_idx];\n+        const ggml_tensor * ffn_up   = cgraph->nodes[node_idx + 1];\n+        const ggml_tensor * glu      = cgraph->nodes[node_idx + 2];\n+\n+        if (ggml_cuda_should_fuse_mul_mat(ffn_up, ffn_gate, glu)) {\n+            return true;\n+        }\n+    }\n+\n     if (!ggml_can_fuse(cgraph, node_idx, ops)) {\n         return false;\n     }\n@@ -3004,6 +3177,184 @@ static void evaluate_and_capture_cuda_graph(ggml_backend_cuda_context * cuda_ctx\n                         }\n                     }\n \n+                    bool fused_mul_mat_vec = false;\n+                    int fused_node_count = 0;\n+\n+                    for (ggml_op op : { GGML_OP_MUL_MAT, GGML_OP_MUL_MAT_ID }) {\n+                        const ggml_op bias_op = op == GGML_OP_MUL_MAT ? GGML_OP_ADD : GGML_OP_ADD_ID;\n+\n+                        if (ggml_cuda_can_fuse(cgraph, i, { op, bias_op, op, bias_op, GGML_OP_GLU }, {})) {\n+                            ggml_tensor * glu         = cgraph->nodes[i + 4];\n+                            ggml_tensor * gate_bias_n = glu->src[0];\n+                            ggml_tensor * up_bias_n   = glu->src[1];\n+\n+                            //we don't assume the order for {gate, up}. Instead infer it from the bias tensor\n+                            ggml_tensor * gate_n      = nullptr;\n+                            ggml_tensor * up_n        = nullptr;\n+\n+                            if (gate_bias_n->src[0] == cgraph->nodes[i] || gate_bias_n->src[1] == cgraph->nodes[i]) {\n+                                gate_n = cgraph->nodes[i];\n+                                up_n   = cgraph->nodes[i + 2];\n+                            } else if (gate_bias_n->src[0] == cgraph->nodes[i + 2] || gate_bias_n->src[1] == cgraph->nodes[i + 2]) {\n+                                gate_n = cgraph->nodes[i + 2];\n+                                up_n   = cgraph->nodes[i];\n+                            } else {\n+                                continue;\n+                            }\n+\n+                            auto get_bias_tensor = [](const ggml_tensor * bias_node, const ggml_tensor * mul_node, ggml_op op_bias) {\n+                                if (op_bias == GGML_OP_ADD) {\n+                                    if (bias_node->src[0] == mul_node) {\n+                                        return bias_node->src[1];\n+                                    }\n+                                    if (bias_node->src[1] == mul_node) {\n+                                        return bias_node->src[0];\n+                                    }\n+                                    return (ggml_tensor *) nullptr;\n+                                }\n+                                GGML_ASSERT(op_bias == GGML_OP_ADD_ID);\n+                                GGML_ASSERT(bias_node->src[0] == mul_node);\n+                                return bias_node->src[1];\n+                            };\n+\n+                            ggml_tensor * up_bias_tensor   = get_bias_tensor(up_bias_n, up_n, bias_op);\n+                            ggml_tensor * gate_bias_tensor = get_bias_tensor(gate_bias_n, gate_n, bias_op);\n+\n+                            if (!up_bias_tensor || !gate_bias_tensor) {\n+                                continue;\n+                            }\n+\n+                            const ggml_tensor * src0 = up_n->src[0];\n+                            const ggml_tensor * src1 = up_n->src[1];\n+                            const ggml_tensor * ids  = up_n->src[2];\n+\n+                            if (ggml_cuda_should_fuse_mul_mat_vec_f(up_n)) {\n+                                ggml_cuda_mm_fusion_args_host fusion_data{};\n+                                fusion_data.gate      = gate_n->src[0];\n+                                fusion_data.x_bias    = up_bias_tensor;\n+                                fusion_data.gate_bias = gate_bias_tensor;\n+                                fusion_data.glu_op    = ggml_get_glu_op(glu);\n+\n+                                ggml_cuda_mul_mat_vec_f(*cuda_ctx, src0, src1, ids, glu, &fusion_data);\n+                                fused_mul_mat_vec = true;\n+                                fused_node_count = 5;\n+                                break;\n+                            }\n+\n+                            if (ggml_cuda_should_fuse_mul_mat_vec_q(up_n)) {\n+                                ggml_cuda_mm_fusion_args_host fusion_data{};\n+                                fusion_data.gate      = gate_n->src[0];\n+                                fusion_data.x_bias    = up_bias_tensor;\n+                                fusion_data.gate_bias = gate_bias_tensor;\n+                                fusion_data.glu_op    = ggml_get_glu_op(glu);\n+\n+                                ggml_cuda_mul_mat_vec_q(*cuda_ctx, src0, src1, ids, glu, &fusion_data);\n+                                fused_mul_mat_vec = true;\n+                                fused_node_count = 5;\n+                                break;\n+                            }\n+                        } else if (ggml_cuda_can_fuse(cgraph, i, { op, op, GGML_OP_GLU }, {})) {\n+                            ggml_tensor * glu  = cgraph->nodes[i + 2];\n+                            ggml_tensor * gate = glu->src[0];\n+                            ggml_tensor * up   = glu->src[1];\n+\n+                            bool ok = (gate == cgraph->nodes[i] && up == cgraph->nodes[i + 1])\n+                                || (gate == cgraph->nodes[i + 1] && up == cgraph->nodes[i]);\n+\n+                            if (!ok) continue;\n+\n+                            const ggml_tensor * src0 = up->src[0];\n+                            const ggml_tensor * src1 = up->src[1];\n+                            const ggml_tensor * ids  = up->src[2];\n+\n+                            if (ggml_cuda_should_fuse_mul_mat_vec_f(up)) {\n+                                ggml_cuda_mm_fusion_args_host fusion_data{};\n+                                fusion_data.gate   = gate->src[0];\n+                                fusion_data.glu_op = ggml_get_glu_op(glu);\n+\n+                                ggml_cuda_mul_mat_vec_f(*cuda_ctx, src0, src1, ids, glu, &fusion_data);\n+                                fused_mul_mat_vec = true;\n+                                fused_node_count = 3;\n+                                break;\n+                            }\n+\n+                            if (ggml_cuda_should_fuse_mul_mat_vec_q(up)) {\n+                                ggml_cuda_mm_fusion_args_host fusion_data{};\n+                                fusion_data.gate   = gate->src[0];\n+                                fusion_data.glu_op = ggml_get_glu_op(glu);\n+\n+                                ggml_cuda_mul_mat_vec_q(*cuda_ctx, src0, src1, ids, glu, &fusion_data);\n+                                fused_mul_mat_vec = true;\n+                                fused_node_count = 3;\n+                                break;\n+                            }\n+                        }\n+                    }\n+\n+                    if (fused_mul_mat_vec) {\n+                        i += fused_node_count - 1;\n+                        continue;\n+                    }\n+\n+                    fused_mul_mat_vec = false;\n+                    fused_node_count = 0;\n+\n+                    for (ggml_op op : { GGML_OP_MUL_MAT, GGML_OP_MUL_MAT_ID }) {\n+                        const ggml_op bias_op = op == GGML_OP_MUL_MAT ? GGML_OP_ADD : GGML_OP_ADD_ID;\n+\n+                        if (!ggml_can_fuse(cgraph, i, { op, bias_op })) {\n+                            continue;\n+                        }\n+\n+                        ggml_tensor * mm_node   = cgraph->nodes[i];\n+                        ggml_tensor * bias_node = cgraph->nodes[i + 1];\n+\n+                        ggml_tensor * bias_tensor = nullptr;\n+                        if (bias_op == GGML_OP_ADD) {\n+                            if (bias_node->src[0] == mm_node) {\n+                                bias_tensor = bias_node->src[1];\n+                            } else if (bias_node->src[1] == mm_node) {\n+                                bias_tensor = bias_node->src[0];\n+                            } else {\n+                                continue;\n+                            }\n+                        } else {\n+                            if (bias_node->src[0] != mm_node) {\n+                                continue;\n+                            }\n+                            bias_tensor = bias_node->src[1];\n+                        }\n+\n+                        const ggml_tensor * src0 = mm_node->src[0];\n+                        const ggml_tensor * src1 = mm_node->src[1];\n+                        const ggml_tensor * ids  = mm_node->src[2];\n+\n+                        if (bias_op == GGML_OP_ADD_ID && bias_node->src[2] != ids) {\n+                            continue;\n+                        }\n+\n+                        ggml_cuda_mm_fusion_args_host fusion_data{};\n+                        fusion_data.x_bias = bias_tensor;\n+\n+                        if (ggml_cuda_should_fuse_mul_mat_vec_f(mm_node)) {\n+                            ggml_cuda_mul_mat_vec_f(*cuda_ctx, src0, src1, ids, bias_node, &fusion_data);\n+                            fused_mul_mat_vec = true;\n+                            fused_node_count = 2;\n+                            break;\n+                        }\n+\n+                        if (ggml_cuda_should_fuse_mul_mat_vec_q(mm_node)) {\n+                            ggml_cuda_mul_mat_vec_q(*cuda_ctx, src0, src1, ids, bias_node, &fusion_data);\n+                            fused_mul_mat_vec = true;\n+                            fused_node_count = 2;\n+                            break;\n+                        }\n+                    }\n+\n+                    if (fused_mul_mat_vec) {\n+                        i += fused_node_count - 1;\n+                        continue;\n+                    }\n \n                     if (ggml_cuda_can_fuse(cgraph, i, { GGML_OP_RMS_NORM, GGML_OP_MUL, GGML_OP_ADD}, {})) {\n                         ggml_cuda_op_rms_norm_fused_add(*cuda_ctx, node, cgraph->nodes[i+1], cgraph->nodes[i+2]);"
      },
      {
        "filename": "ggml/src/ggml-cuda/mmvf.cu",
        "status": "modified",
        "additions": 317,
        "deletions": 57,
        "changes": 374,
        "patch": "@@ -1,11 +1,12 @@\n #include \"ggml.h\"\n #include \"common.cuh\"\n-#include \"convert.cuh\"\n+#include \"unary.cuh\"\n #include \"mmvf.cuh\"\n+#include \"convert.cuh\"\n \n-template <typename T, typename type_acc, int ncols_dst, int block_size>\n+template <typename T, typename type_acc, int ncols_dst, int block_size, bool has_fusion = false>\n static __global__ void mul_mat_vec_f(\n-        const T * __restrict__ x, const float * __restrict__ y, const int32_t * __restrict__ ids, float * __restrict__ dst,\n+        const T * __restrict__ x, const float * __restrict__ y, const int32_t * __restrict__ ids, const ggml_cuda_mm_fusion_args_device fusion, float * __restrict__ dst,\n         const int ncols2, const int nchannels_y, const int stride_row, const int stride_col_y2, const int stride_col_dst,\n         const uint3 channel_ratio, const int stride_channel_x, const int stride_channel_y, const int stride_channel_dst,\n         const uint3 sample_ratio, const int stride_sample_x, const int stride_sample_y, const int stride_sample_dst) {\n@@ -24,65 +25,180 @@ static __global__ void mul_mat_vec_f(\n     y   += int64_t(sample_y)  *stride_sample_y   + channel_y  *stride_channel_y;\n     dst += int64_t(sample_dst)*stride_sample_dst + channel_dst*stride_channel_dst;\n \n+    bool use_gate = false;\n+    bool use_bias = false;\n+    bool use_gate_bias = false;\n+    ggml_glu_op glu_op = ggml_glu_op::GGML_GLU_OP_SWIGLU;\n+    const T * gate_x = nullptr;\n+    const float * x_bias = nullptr;\n+    const float * gate_bias = nullptr;\n+\n+    if constexpr (has_fusion) {\n+        use_gate = fusion.gate != nullptr;\n+        use_bias = fusion.x_bias != nullptr;\n+        use_gate_bias = fusion.gate_bias != nullptr;\n+        glu_op = fusion.glu_op;\n+\n+        if (use_gate) {\n+            gate_x = static_cast<const T *>(fusion.gate);\n+        }\n+        if (use_bias) {\n+            x_bias = static_cast<const float *>(fusion.x_bias);\n+        }\n+        if (use_gate_bias) {\n+            gate_bias = static_cast<const float *>(fusion.gate_bias);\n+            use_gate_bias = use_gate;\n+        } else {\n+            use_gate_bias = false;\n+        }\n+    }\n+\n+    if (use_gate) {\n+        gate_x += int64_t(sample_x)  *stride_sample_x   + channel_x  *stride_channel_x   + row*stride_row;\n+    }\n+    if constexpr (has_fusion) {\n+        const int channel_bias = ids ? channel_x : channel_dst;\n+        if (use_bias) {\n+            x_bias += int64_t(sample_dst)*stride_sample_dst + channel_bias*stride_channel_dst;\n+        }\n+        if (use_gate_bias) {\n+            gate_bias += int64_t(sample_dst)*stride_sample_dst + channel_bias*stride_channel_dst;\n+        }\n+    }\n+\n     const float2 * y2 = (const float2 *) y;\n \n     extern __shared__ char data_mmv[];\n     float * buf_iw = (float *) data_mmv;\n+    float * buf_iw_gate = nullptr;\n+    if constexpr (has_fusion) {\n+        buf_iw_gate = (float *) (data_mmv + warp_size*sizeof(float));\n+    }\n \n     if (block_size > warp_size) {\n         if (tid < warp_size) {\n             buf_iw[tid] = 0.0f;\n+            if constexpr (has_fusion) {\n+                if (use_gate) {\n+                    buf_iw_gate[tid] = 0.0f;\n+                }\n+            }\n         }\n         __syncthreads();\n     }\n \n     float sumf[ncols_dst] = {0.0f};\n+    float sumf_gate[ncols_dst];\n+    if constexpr (has_fusion) {\n+#pragma unroll\n+        for (int j = 0; j < ncols_dst; ++j) {\n+            sumf_gate[j] = 0.0f;\n+        }\n+    }\n \n     if constexpr (std::is_same_v<T, float>) {\n         const float2 * x2 = (const float2 *) x;\n+        const float2 * gate_x2 = nullptr;\n+        if constexpr (has_fusion) {\n+            if (use_gate) {\n+                gate_x2 = (const float2 *) gate_x;\n+            }\n+        }\n \n         for (int col2 = tid; col2 < ncols2; col2 += block_size) {\n             const float2 tmpx = x2[col2];\n+            float2 tmpx_gate = make_float2(0.0f, 0.0f);\n+            if constexpr (has_fusion) {\n+                if (use_gate) {\n+                    tmpx_gate = gate_x2[col2];\n+                }\n+            }\n \n #pragma unroll\n             for (int j = 0; j < ncols_dst; ++j) {\n                 const float2 tmpy = y2[j*stride_col_y2 + col2];\n                 ggml_cuda_mad(sumf[j], tmpx.x, tmpy.x);\n                 ggml_cuda_mad(sumf[j], tmpx.y, tmpy.y);\n+\n+                if constexpr (has_fusion) {\n+                    if (use_gate) {\n+                        ggml_cuda_mad(sumf_gate[j], tmpx_gate.x, tmpy.x);\n+                        ggml_cuda_mad(sumf_gate[j], tmpx_gate.y, tmpy.y);\n+                    }\n+                }\n             }\n         }\n     } else if constexpr (std::is_same_v<T, half>) {\n         const half2 * x2 = (const half2 *) x;\n+        const half2 * gate_x2 = nullptr;\n+        if constexpr (has_fusion) {\n+            if (use_gate) {\n+                gate_x2 = (const half2 *) gate_x;\n+            }\n+        }\n \n         if (std::is_same_v<type_acc, float>) {\n             for (int col2 = tid; col2 < ncols2; col2 += block_size) {\n                 const float2 tmpx = __half22float2(x2[col2]);\n-\n+                float2 tmpx_gate = make_float2(0.0f, 0.0f);\n+                if constexpr (has_fusion) {\n+                    if (use_gate) {\n+                        tmpx_gate = __half22float2(gate_x2[col2]);\n+                    }\n+                }\n #pragma unroll\n                 for (int j = 0; j < ncols_dst; ++j) {\n                     const float2 tmpy = y2[j*stride_col_y2 + col2];\n                     ggml_cuda_mad(sumf[j], tmpx.x, tmpy.x);\n                     ggml_cuda_mad(sumf[j], tmpx.y, tmpy.y);\n+\n+                    if constexpr (has_fusion) {\n+                        if (use_gate) {\n+                            ggml_cuda_mad(sumf_gate[j], tmpx_gate.x, tmpy.x);\n+                            ggml_cuda_mad(sumf_gate[j], tmpx_gate.y, tmpy.y);\n+                        }\n+                    }\n                 }\n             }\n         } else {\n #ifdef FP16_AVAILABLE\n             half2 sumh2[ncols_dst] = {{0.0f, 0.0f}};\n+            half2 sumh2_gate[ncols_dst] = {{0.0f, 0.0f}};\n \n             for (int col2 = tid; col2 < ncols2; col2 += block_size) {\n                 const half2 tmpx = x2[col2];\n-\n+                half2 tmpx_gate = make_half2(0.0f, 0.0f);\n+                if constexpr (has_fusion) {\n+                    if (use_gate) {\n+                        tmpx_gate = gate_x2[col2];\n+                    }\n+                }\n #pragma unroll\n                 for (int j = 0; j < ncols_dst; ++j) {\n                     const float2 tmpy = y2[j*stride_col_y2 + col2];\n                     sumh2[j] += tmpx * make_half2(tmpy.x, tmpy.y);\n+\n+                    if constexpr (has_fusion) {\n+                        if (use_gate) {\n+                            sumh2_gate[j] += tmpx_gate * make_half2(tmpy.x, tmpy.y);\n+                        }\n+                    }\n                 }\n             }\n \n #pragma unroll\n             for (int j = 0; j < ncols_dst; ++j) {\n                 sumf[j] = __low2float(sumh2[j]) + __high2float(sumh2[j]);\n             }\n+\n+            if constexpr (has_fusion) {\n+                if (use_gate) {\n+#pragma unroll\n+                    for (int j = 0; j < ncols_dst; ++j) {\n+                        sumf_gate[j] = __low2float(sumh2_gate[j]) + __high2float(sumh2_gate[j]);\n+                    }\n+                }\n+            }\n #else\n             NO_DEVICE_CODE;\n #endif // FP16_AVAILABLE\n@@ -91,26 +207,66 @@ static __global__ void mul_mat_vec_f(\n //TODO: add support for ggml_cuda_mad for hip_bfloat162\n #if defined(GGML_USE_HIP)\n         const int * x2 = (const int *) x;\n+        const int * gate_x2 = nullptr;\n+        if constexpr (has_fusion) {\n+            if (use_gate) {\n+                gate_x2 = (const int *) gate_x;\n+            }\n+        }\n         for (int col2 = tid; col2 < ncols2; col2 += block_size) {\n             const int tmpx = x2[col2];\n+            int tmpx_gate = 0;\n+            if constexpr (has_fusion) {\n+                if (use_gate) {\n+                    tmpx_gate = gate_x2[col2];\n+                }\n+            }\n #pragma unroll\n             for (int j = 0; j < ncols_dst; ++j) {\n                 const float2 tmpy = y2[j*stride_col_y2 + col2];\n                 const float tmpx0 = ggml_cuda_cast<float>(reinterpret_cast<const nv_bfloat16 *>(&tmpx)[0]);\n                 const float tmpx1 = ggml_cuda_cast<float>(reinterpret_cast<const nv_bfloat16 *>(&tmpx)[1]);\n                 ggml_cuda_mad(sumf[j], tmpx0, tmpy.x);\n                 ggml_cuda_mad(sumf[j], tmpx1, tmpy.y);\n+\n+                if constexpr (has_fusion) {\n+                    if (use_gate) {\n+                        const float tmpx0_gate = ggml_cuda_cast<float>(reinterpret_cast<const nv_bfloat16 *>(&tmpx_gate)[0]);\n+                        const float tmpx1_gate = ggml_cuda_cast<float>(reinterpret_cast<const nv_bfloat16 *>(&tmpx_gate)[1]);\n+                        ggml_cuda_mad(sumf_gate[j], tmpx0_gate, tmpy.x);\n+                        ggml_cuda_mad(sumf_gate[j], tmpx1_gate, tmpy.y);\n+                    }\n+                }\n             }\n         }\n #else\n         const nv_bfloat162 * x2 = (const nv_bfloat162 *) x;\n+        const nv_bfloat162 * gate_x2 = nullptr;\n+        if constexpr (has_fusion) {\n+            if (use_gate) {\n+                gate_x2 = (const nv_bfloat162 *) gate_x;\n+            }\n+        }\n         for (int col2 = tid; col2 < ncols2; col2 += block_size) {\n             const nv_bfloat162 tmpx = x2[col2];\n+            nv_bfloat162 tmpx_gate;\n+            if constexpr (has_fusion) {\n+                if (use_gate) {\n+                    tmpx_gate = gate_x2[col2];\n+                }\n+            }\n #pragma unroll\n             for (int j = 0; j < ncols_dst; ++j) {\n                 const float2 tmpy = y2[j*stride_col_y2 + col2];\n                 ggml_cuda_mad(sumf[j], tmpx.x, tmpy.x);\n                 ggml_cuda_mad(sumf[j], tmpx.y, tmpy.y);\n+\n+                if constexpr (has_fusion) {\n+                    if (use_gate) {\n+                        ggml_cuda_mad(sumf_gate[j], tmpx_gate.x, tmpy.x);\n+                        ggml_cuda_mad(sumf_gate[j], tmpx_gate.y, tmpy.y);\n+                    }\n+                }\n             }\n         }\n #endif\n@@ -122,13 +278,31 @@ static __global__ void mul_mat_vec_f(\n     for (int j = 0; j < ncols_dst; ++j) {\n         sumf[j] = warp_reduce_sum<warp_size>(sumf[j]);\n \n+        if constexpr (has_fusion) {\n+            if (use_gate) {\n+                sumf_gate[j] = warp_reduce_sum<warp_size>(sumf_gate[j]);\n+            }\n+        }\n+\n         if (block_size > warp_size) {\n             buf_iw[tid/warp_size] = sumf[j];\n+            if constexpr (has_fusion) {\n+                if (use_gate) {\n+                    buf_iw_gate[tid/warp_size] = sumf_gate[j];\n+                }\n+            }\n             __syncthreads();\n             if (tid < warp_size) {\n                 sumf[j] = buf_iw[tid];\n                 sumf[j] = warp_reduce_sum<warp_size>(sumf[j]);\n+                if constexpr (has_fusion) {\n+                    if (use_gate) {\n+                        sumf_gate[j] = buf_iw_gate[tid];\n+                        sumf_gate[j] = warp_reduce_sum<warp_size>(sumf_gate[j]);\n+                    }\n+                }\n             }\n+\n             if (j < ncols_dst) {\n                 __syncthreads();\n             }\n@@ -139,12 +313,70 @@ static __global__ void mul_mat_vec_f(\n         return;\n     }\n \n-    dst[tid*stride_col_dst + row] = sumf[tid];\n+    float value = sumf[tid];\n+\n+    if constexpr (has_fusion) {\n+        if (use_bias) {\n+            value += x_bias[tid*stride_col_dst + row];\n+        }\n+\n+        if (use_gate) {\n+            float gate_value = sumf_gate[tid];\n+            if (use_gate_bias) {\n+                gate_value += gate_bias[tid*stride_col_dst + row];\n+            }\n+            switch (glu_op) {\n+                case GGML_GLU_OP_SWIGLU:\n+                    value *= ggml_cuda_op_silu_single(gate_value);\n+                    break;\n+                case GGML_GLU_OP_GEGLU:\n+                    value *= ggml_cuda_op_gelu_single(gate_value);\n+                    break;\n+                case GGML_GLU_OP_SWIGLU_OAI: {\n+                    value = ggml_cuda_op_swiglu_oai_single(gate_value, value);\n+                    break;\n+                }\n+                default:\n+                    break;\n+            }\n+        }\n+    }\n+\n+    dst[tid*stride_col_dst + row] = value;\n+}\n+\n+template<typename T, typename type_acc, int ncols_dst, int block_size>\n+static void mul_mat_vec_f_switch_fusion(\n+        const T * x, const float * y, const int32_t * ids, const ggml_cuda_mm_fusion_args_device fusion, float * dst,\n+        const int64_t ncols, const int64_t nrows,\n+        const int64_t stride_row, const int64_t stride_col_y, const int64_t stride_col_dst,\n+        const uint3 channel_ratio, const int stride_channel_x, const int stride_channel_y, const int stride_channel_dst,\n+        const uint3 sample_ratio, const int stride_sample_x, const int stride_sample_y, const int stride_sample_dst,\n+        const dim3 & block_dims, const dim3 & block_nums, const int nbytes_shared, const cudaStream_t stream) {\n+\n+    const bool has_fusion = fusion.gate != nullptr || fusion.x_bias != nullptr || fusion.gate_bias != nullptr;\n+    if constexpr (ncols_dst == 1) {\n+        if (has_fusion) {\n+            mul_mat_vec_f<T, type_acc, ncols_dst, block_size, true><<<block_nums, block_dims, nbytes_shared, stream>>>\n+                (x, y, ids, fusion, dst, ncols, nrows, stride_row, stride_col_y, stride_col_dst,\n+                channel_ratio, stride_channel_x, stride_channel_y, stride_channel_dst,\n+                sample_ratio, stride_sample_x, stride_sample_y, stride_sample_dst);\n+            return;\n+       }\n+    }\n+\n+    GGML_ASSERT(!has_fusion && \"fusion only supported for ncols_dst=1\");\n+\n+    mul_mat_vec_f<T, type_acc, ncols_dst, block_size><<<block_nums, block_dims, nbytes_shared, stream>>>\n+        (x, y, ids, fusion, dst, ncols, nrows, stride_row, stride_col_y, stride_col_dst,\n+        channel_ratio, stride_channel_x, stride_channel_y, stride_channel_dst,\n+        sample_ratio, stride_sample_x, stride_sample_y, stride_sample_dst);\n+\n }\n \n template <typename T, typename type_acc, int ncols_dst>\n-static void launch_mul_mat_vec_f_cuda(\n-        const T * x, const float * y, const int32_t * ids, float * dst,\n+void launch_mul_mat_vec_f_cuda(\n+        const T * x, const float * y, const int32_t * ids, const ggml_cuda_mm_fusion_args_device fusion, float * dst,\n         const int64_t ncols, const int64_t nrows,\n         const int64_t stride_row, const int64_t stride_col_y, const int64_t stride_col_dst,\n         const int64_t nchannels_x, const int64_t nchannels_y, const int64_t nchannels_dst,\n@@ -176,57 +408,59 @@ static void launch_mul_mat_vec_f_cuda(\n         }\n     }\n \n-    const int nbytes_shared = warp_size*sizeof(float);\n+    const bool has_fusion = fusion.gate != nullptr || fusion.x_bias != nullptr || fusion.gate_bias != nullptr;\n+\n+    const int nbytes_shared = warp_size*sizeof(float) + (has_fusion ? warp_size*sizeof(float) : 0);\n     const dim3 block_nums(nrows, nchannels_dst, nsamples_dst);\n     const dim3 block_dims(block_size_best, 1, 1);\n     switch (block_size_best) {\n         case   32: {\n-            mul_mat_vec_f<T, type_acc, ncols_dst,  32><<<block_nums, block_dims, nbytes_shared, stream>>>\n-                (x, y, ids, dst, ncols/2, nchannels_y, stride_row, stride_col_y/2, stride_col_dst,\n+            mul_mat_vec_f_switch_fusion<T, type_acc, ncols_dst, 32>\n+                (x, y, ids, fusion, dst, ncols/2, nchannels_y, stride_row, stride_col_y/2, stride_col_dst,\n                  channel_ratio_fd, stride_channel_x, stride_channel_y, stride_channel_dst,\n-                 sample_ratio_fd, stride_sample_x, stride_sample_y, stride_sample_dst);\n+                 sample_ratio_fd, stride_sample_x, stride_sample_y, stride_sample_dst, block_dims, block_nums, nbytes_shared, stream);\n         } break;\n         case   64: {\n-            mul_mat_vec_f<T, type_acc, ncols_dst,  64><<<block_nums, block_dims, nbytes_shared, stream>>>\n-                (x, y, ids, dst, ncols/2, nchannels_y, stride_row, stride_col_y/2, stride_col_dst,\n+            mul_mat_vec_f_switch_fusion<T, type_acc, ncols_dst, 64>\n+                (x, y, ids, fusion, dst, ncols/2, nchannels_y, stride_row, stride_col_y/2, stride_col_dst,\n                  channel_ratio_fd, stride_channel_x, stride_channel_y, stride_channel_dst,\n-                 sample_ratio_fd, stride_sample_x, stride_sample_y, stride_sample_dst);\n+                 sample_ratio_fd, stride_sample_x, stride_sample_y, stride_sample_dst, block_dims, block_nums, nbytes_shared, stream);\n         } break;\n         case   96: {\n-            mul_mat_vec_f<T, type_acc, ncols_dst,  96><<<block_nums, block_dims, nbytes_shared, stream>>>\n-                (x, y, ids, dst, ncols/2, nchannels_y, stride_row, stride_col_y/2, stride_col_dst,\n+            mul_mat_vec_f_switch_fusion<T, type_acc, ncols_dst, 96>\n+                (x, y, ids, fusion, dst, ncols/2, nchannels_y, stride_row, stride_col_y/2, stride_col_dst,\n                  channel_ratio_fd, stride_channel_x, stride_channel_y, stride_channel_dst,\n-                 sample_ratio_fd, stride_sample_x, stride_sample_y, stride_sample_dst);\n+                 sample_ratio_fd, stride_sample_x, stride_sample_y, stride_sample_dst, block_dims, block_nums, nbytes_shared, stream);\n         } break;\n         case  128: {\n-            mul_mat_vec_f<T, type_acc, ncols_dst, 128><<<block_nums, block_dims, nbytes_shared, stream>>>\n-                (x, y, ids, dst, ncols/2, nchannels_y, stride_row, stride_col_y/2, stride_col_dst,\n+            mul_mat_vec_f_switch_fusion<T, type_acc, ncols_dst, 128>\n+                (x, y, ids, fusion, dst, ncols/2, nchannels_y, stride_row, stride_col_y/2, stride_col_dst,\n                  channel_ratio_fd, stride_channel_x, stride_channel_y, stride_channel_dst,\n-                 sample_ratio_fd, stride_sample_x, stride_sample_y, stride_sample_dst);\n+                 sample_ratio_fd, stride_sample_x, stride_sample_y, stride_sample_dst, block_dims, block_nums, nbytes_shared, stream);\n         } break;\n         case  160: {\n-            mul_mat_vec_f<T, type_acc, ncols_dst, 160><<<block_nums, block_dims, nbytes_shared, stream>>>\n-                (x, y, ids, dst, ncols/2, nchannels_y, stride_row, stride_col_y/2, stride_col_dst,\n+            mul_mat_vec_f_switch_fusion<T, type_acc, ncols_dst, 160>\n+                (x, y, ids, fusion, dst, ncols/2, nchannels_y, stride_row, stride_col_y/2, stride_col_dst,\n                  channel_ratio_fd, stride_channel_x, stride_channel_y, stride_channel_dst,\n-                 sample_ratio_fd, stride_sample_x, stride_sample_y, stride_sample_dst);\n+                 sample_ratio_fd, stride_sample_x, stride_sample_y, stride_sample_dst, block_dims, block_nums, nbytes_shared, stream);\n         } break;\n         case  192: {\n-            mul_mat_vec_f<T, type_acc, ncols_dst, 192><<<block_nums, block_dims, nbytes_shared, stream>>>\n-                (x, y, ids, dst, ncols/2, nchannels_y, stride_row, stride_col_y/2, stride_col_dst,\n+            mul_mat_vec_f_switch_fusion<T, type_acc, ncols_dst, 192>\n+                (x, y, ids, fusion, dst, ncols/2, nchannels_y, stride_row, stride_col_y/2, stride_col_dst,\n                  channel_ratio_fd, stride_channel_x, stride_channel_y, stride_channel_dst,\n-                 sample_ratio_fd, stride_sample_x, stride_sample_y, stride_sample_dst);\n+                 sample_ratio_fd, stride_sample_x, stride_sample_y, stride_sample_dst, block_dims, block_nums, nbytes_shared, stream);\n         } break;\n         case  224: {\n-            mul_mat_vec_f<T, type_acc, ncols_dst, 224><<<block_nums, block_dims, nbytes_shared, stream>>>\n-                (x, y, ids, dst, ncols/2, nchannels_y, stride_row, stride_col_y/2, stride_col_dst,\n+            mul_mat_vec_f_switch_fusion<T, type_acc, ncols_dst, 224>\n+                (x, y, ids, fusion, dst, ncols/2, nchannels_y, stride_row, stride_col_y/2, stride_col_dst,\n                  channel_ratio_fd, stride_channel_x, stride_channel_y, stride_channel_dst,\n-                 sample_ratio_fd, stride_sample_x, stride_sample_y, stride_sample_dst);\n+                 sample_ratio_fd, stride_sample_x, stride_sample_y, stride_sample_dst, block_dims, block_nums, nbytes_shared, stream);\n         } break;\n         case  256: {\n-            mul_mat_vec_f<T, type_acc, ncols_dst, 256><<<block_nums, block_dims, nbytes_shared, stream>>>\n-                (x, y, ids, dst, ncols/2, nchannels_y, stride_row, stride_col_y/2, stride_col_dst,\n+            mul_mat_vec_f_switch_fusion<T, type_acc, ncols_dst, 256>\n+                (x, y, ids, fusion, dst, ncols/2, nchannels_y, stride_row, stride_col_y/2, stride_col_dst,\n                  channel_ratio_fd, stride_channel_x, stride_channel_y, stride_channel_dst,\n-                 sample_ratio_fd, stride_sample_x, stride_sample_y, stride_sample_dst);\n+                 sample_ratio_fd, stride_sample_x, stride_sample_y, stride_sample_dst, block_dims, block_nums, nbytes_shared, stream);\n         } break;\n         default: {\n             GGML_ABORT(\"fatal error\");\n@@ -236,7 +470,7 @@ static void launch_mul_mat_vec_f_cuda(\n \n template <typename T, typename type_acc>\n static void mul_mat_vec_f_cuda_switch_ncols_dst(\n-        const T * x, const float * y, const int32_t * ids, float * dst,\n+        const T * x, const float * y, const int32_t * ids, const ggml_cuda_mm_fusion_args_device fusion, float * dst,\n         const int64_t ncols, const int64_t nrows, const int64_t ncols_dst,\n         const int64_t stride_row, const int64_t stride_col_y, const int64_t stride_col_dst,\n         const int64_t nchannels_x, const int64_t nchannels_y, const int64_t nchannels_dst,\n@@ -246,49 +480,49 @@ static void mul_mat_vec_f_cuda_switch_ncols_dst(\n     switch (ncols_dst) {\n         case 1:\n             launch_mul_mat_vec_f_cuda<T, type_acc, 1>\n-                (x, y, ids, dst, ncols, nrows, stride_row, stride_col_y, stride_col_dst,\n+                (x, y, ids, fusion, dst, ncols, nrows, stride_row, stride_col_y, stride_col_dst,\n                  nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y,\n                  stride_channel_dst, nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, stream);\n             break;\n         case 2:\n             launch_mul_mat_vec_f_cuda<T, type_acc, 2>\n-                (x, y, ids, dst, ncols, nrows, stride_row, stride_col_y, stride_col_dst,\n+                (x, y, ids, fusion, dst, ncols, nrows, stride_row, stride_col_y, stride_col_dst,\n                  nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y,\n                  stride_channel_dst, nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, stream);\n             break;\n         case 3:\n             launch_mul_mat_vec_f_cuda<T, type_acc, 3>\n-                (x, y, ids, dst, ncols, nrows, stride_row, stride_col_y, stride_col_dst,\n+                (x, y, ids, fusion, dst, ncols, nrows, stride_row, stride_col_y, stride_col_dst,\n                  nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y,\n                  stride_channel_dst, nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, stream);\n             break;\n         case 4:\n             launch_mul_mat_vec_f_cuda<T, type_acc, 4>\n-                (x, y, ids, dst, ncols, nrows, stride_row, stride_col_y, stride_col_dst,\n+                (x, y, ids, fusion, dst, ncols, nrows, stride_row, stride_col_y, stride_col_dst,\n                  nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y,\n                  stride_channel_dst, nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, stream);\n             break;\n         case 5:\n             launch_mul_mat_vec_f_cuda<T, type_acc, 5>\n-                (x, y, ids, dst, ncols, nrows, stride_row, stride_col_y, stride_col_dst,\n+                (x, y, ids, fusion, dst, ncols, nrows, stride_row, stride_col_y, stride_col_dst,\n                  nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y,\n                  stride_channel_dst, nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, stream);\n             break;\n         case 6:\n             launch_mul_mat_vec_f_cuda<T, type_acc, 6>\n-                (x, y, ids, dst, ncols, nrows, stride_row, stride_col_y, stride_col_dst,\n+                (x, y, ids, fusion, dst, ncols, nrows, stride_row, stride_col_y, stride_col_dst,\n                  nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y,\n                  stride_channel_dst, nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, stream);\n             break;\n         case 7:\n             launch_mul_mat_vec_f_cuda<T, type_acc, 7>\n-                (x, y, ids, dst, ncols, nrows, stride_row, stride_col_y, stride_col_dst,\n+                (x, y, ids, fusion, dst, ncols, nrows, stride_row, stride_col_y, stride_col_dst,\n                  nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y,\n                  stride_channel_dst, nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, stream);\n             break;\n         case 8:\n             launch_mul_mat_vec_f_cuda<T, type_acc, 8>\n-                (x, y, ids, dst, ncols, nrows, stride_row, stride_col_y, stride_col_dst,\n+                (x, y, ids, fusion, dst, ncols, nrows, stride_row, stride_col_y, stride_col_dst,\n                  nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y,\n                  stride_channel_dst, nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, stream);\n             break;\n@@ -300,29 +534,31 @@ static void mul_mat_vec_f_cuda_switch_ncols_dst(\n \n template<typename T>\n static void mul_mat_vec_f_cuda(\n-        const T * x, const float * y, const int32_t * ids, float * dst,\n+        const T * x, const float * y, const int32_t * ids, const ggml_cuda_mm_fusion_args_device fusion, float * dst,\n         const int64_t ncols, const int64_t nrows, const int64_t ncols_dst,\n         const int64_t stride_row, const int64_t stride_col_y, const int stride_col_dst,\n         const int64_t nchannels_x, const int64_t nchannels_y, const int64_t nchannels_dst,\n         const int64_t stride_channel_x, const int64_t stride_channel_y, const int64_t stride_channel_dst, const int64_t nsamples_x,\n         const int64_t nsamples_dst, const int64_t stride_sample_x, const int64_t stride_sample_y, const int64_t stride_sample_dst,\n         enum ggml_prec prec, cudaStream_t stream) {\n+\n     if constexpr(std::is_same_v<T, half>) {\n         if (prec == GGML_PREC_DEFAULT) {\n             mul_mat_vec_f_cuda_switch_ncols_dst<T, half>\n-                (x, y, ids, dst, ncols, nrows, ncols_dst, stride_row, stride_col_y, stride_col_dst,\n-                 nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y,\n-                 stride_channel_dst, nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, stream);\n+                (x, y, ids, fusion, dst, ncols, nrows, ncols_dst, stride_row, stride_col_y, stride_col_dst,\n+                nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y,\n+                stride_channel_dst, nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, stream);\n             return;\n         }\n     }\n     mul_mat_vec_f_cuda_switch_ncols_dst<T, float>\n-        (x, y, ids, dst, ncols, nrows, ncols_dst, stride_row, stride_col_y, stride_col_dst,\n-         nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y,\n-         stride_channel_dst, nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, stream);\n+        (x, y, ids, fusion, dst, ncols, nrows, ncols_dst, stride_row, stride_col_y, stride_col_dst,\n+        nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y,\n+        stride_channel_dst, nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, stream);\n }\n \n-void ggml_cuda_mul_mat_vec_f(ggml_backend_cuda_context & ctx, const ggml_tensor * src0, const ggml_tensor * src1, const ggml_tensor * ids, ggml_tensor * dst) {\n+void ggml_cuda_mul_mat_vec_f(ggml_backend_cuda_context & ctx, const ggml_tensor * src0, const ggml_tensor * src1, const ggml_tensor * ids, ggml_tensor * dst,\n+    const ggml_cuda_mm_fusion_args_host * fusion) {\n     GGML_ASSERT(        src1->type == GGML_TYPE_F32);\n     GGML_ASSERT(!ids ||  ids->type == GGML_TYPE_I32);\n     GGML_ASSERT(         dst->type == GGML_TYPE_F32);\n@@ -348,6 +584,30 @@ void ggml_cuda_mul_mat_vec_f(ggml_backend_cuda_context & ctx, const ggml_tensor\n     const int32_t *  ids_d = ids ? (const int32_t *)  ids->data : nullptr;\n     float         *  dst_d =       (float         *)  dst->data;\n \n+    ggml_cuda_mm_fusion_args_device fusion_local{};\n+\n+    if (fusion) {\n+        GGML_ASSERT( !ids || dst->ne[2] == 1);\n+        GGML_ASSERT(  ids || dst->ne[1] == 1);\n+        if (fusion->x_bias) {\n+            GGML_ASSERT(fusion->x_bias->type == GGML_TYPE_F32);\n+            GGML_ASSERT(fusion->x_bias->ne[0] == dst->ne[0]);\n+            GGML_ASSERT(!ids || fusion->x_bias->ne[1] == src0->ne[2]);\n+            fusion_local.x_bias = fusion->x_bias->data;\n+        }\n+        if (fusion->gate) {\n+            GGML_ASSERT(fusion->gate->type == src0->type && ggml_are_same_stride(fusion->gate, src0));\n+            fusion_local.gate = fusion->gate->data;\n+        }\n+        if (fusion->gate_bias) {\n+            GGML_ASSERT(fusion->gate_bias->type == GGML_TYPE_F32);\n+            GGML_ASSERT(fusion->gate_bias->ne[0] == dst->ne[0]);\n+            GGML_ASSERT(!ids || fusion->gate_bias->ne[1] == src0->ne[2]);\n+            fusion_local.gate_bias = fusion->gate_bias->data;\n+        }\n+        fusion_local.glu_op = fusion->glu_op;\n+    }\n+\n     const int64_t s01 = src0->nb[1] / ts_src0;\n     const int64_t s11 = src1->nb[1] / ts_src1;\n     const int64_t s1  =  dst->nb[1] / ts_dst;\n@@ -370,19 +630,19 @@ void ggml_cuda_mul_mat_vec_f(ggml_backend_cuda_context & ctx, const ggml_tensor\n     switch (src0->type) {\n         case GGML_TYPE_F32: {\n             const float * src0_d = (const float *) src0->data;\n-            mul_mat_vec_f_cuda(src0_d, src1_d, ids_d, dst_d, ne00, ne01, ncols_dst, s01, s11, s1,\n+            mul_mat_vec_f_cuda(src0_d, src1_d, ids_d, fusion_local, dst_d, ne00, ne01, ncols_dst, s01, s11, s1,\n                 ne02, nchannels_y, nchannels_dst, s02, stride_channel_y, stride_channel_dst,\n                 ne03,              ne3,           s03, s13,              s3,                 prec, ctx.stream());\n         } break;\n         case GGML_TYPE_F16: {\n             const half * src0_d = (const half *) src0->data;\n-            mul_mat_vec_f_cuda(src0_d, src1_d, ids_d, dst_d, ne00, ne01, ncols_dst, s01, s11, s1,\n+            mul_mat_vec_f_cuda(src0_d, src1_d, ids_d, fusion_local, dst_d, ne00, ne01, ncols_dst, s01, s11, s1,\n                 ne02, nchannels_y, nchannels_dst, s02, stride_channel_y, stride_channel_dst,\n                 ne03,              ne3,           s03, s13,              s3,                 prec, ctx.stream());\n         } break;\n         case GGML_TYPE_BF16: {\n             const nv_bfloat16 * src0_d = (const nv_bfloat16 *) src0->data;\n-            mul_mat_vec_f_cuda(src0_d, src1_d, ids_d, dst_d, ne00, ne01, ncols_dst, s01, s11, s1,\n+            mul_mat_vec_f_cuda(src0_d, src1_d, ids_d, fusion_local, dst_d, ne00, ne01, ncols_dst, s01, s11, s1,\n                 ne02, nchannels_y, nchannels_dst, s02, stride_channel_y, stride_channel_dst,\n                 ne03,              ne3,           s03, s13,              s3,                 prec, ctx.stream());\n         } break;\n@@ -409,7 +669,6 @@ void ggml_cuda_op_mul_mat_vec_f(\n     const int cc = ggml_cuda_info().devices[id].cc;\n     const enum ggml_prec prec = fast_fp16_available(cc) ? ggml_prec(dst->op_params[0]) : GGML_PREC_F32;\n \n-\n     // ggml_cuda_op provides single, contiguous matrices\n     const int64_t stride_row         = ne00;\n     const int64_t stride_col_y       = ne10;\n@@ -426,22 +685,23 @@ void ggml_cuda_op_mul_mat_vec_f(\n     const int64_t stride_sample_y    = 0;\n     const int64_t stride_sample_dst  = 0;\n \n+    ggml_cuda_mm_fusion_args_device empty{};\n     switch (src0->type) {\n         case GGML_TYPE_F32: {\n             const float * src0_d = (const float *) src0_dd_i;\n-            mul_mat_vec_f_cuda(src0_d, src1_ddf_i, nullptr, dst_dd_i, ne00, row_diff, src1_ncols, stride_row, stride_col_y, stride_col_dst,\n+            mul_mat_vec_f_cuda(src0_d, src1_ddf_i, nullptr, empty, dst_dd_i, ne00, row_diff, src1_ncols, stride_row, stride_col_y, stride_col_dst,\n                 nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y, stride_channel_dst,\n                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, prec, stream);\n         } break;\n         case GGML_TYPE_F16: {\n             const half * src0_d = (const half *) src0_dd_i;\n-            mul_mat_vec_f_cuda(src0_d, src1_ddf_i, nullptr, dst_dd_i, ne00, row_diff, src1_ncols, stride_row, stride_col_y, stride_col_dst,\n+            mul_mat_vec_f_cuda(src0_d, src1_ddf_i, nullptr, empty, dst_dd_i, ne00, row_diff, src1_ncols, stride_row, stride_col_y, stride_col_dst,\n                 nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y, stride_channel_dst,\n                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, prec, stream);\n         } break;\n         case GGML_TYPE_BF16: {\n             const nv_bfloat16 * src0_d = (const nv_bfloat16 *) src0_dd_i;\n-            mul_mat_vec_f_cuda(src0_d, src1_ddf_i, nullptr, dst_dd_i, ne00, row_diff, src1_ncols, stride_row, stride_col_y, stride_col_dst,\n+            mul_mat_vec_f_cuda(src0_d, src1_ddf_i, nullptr, empty, dst_dd_i, ne00, row_diff, src1_ncols, stride_row, stride_col_y, stride_col_dst,\n                 nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y, stride_channel_dst,\n                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, prec, stream);\n         } break;"
      },
      {
        "filename": "ggml/src/ggml-cuda/mmvf.cuh",
        "status": "modified",
        "additions": 2,
        "deletions": 1,
        "changes": 3,
        "patch": "@@ -1,6 +1,7 @@\n #include \"common.cuh\"\n \n-void ggml_cuda_mul_mat_vec_f(ggml_backend_cuda_context & ctx, const ggml_tensor * src0, const ggml_tensor * src1, const ggml_tensor * ids, ggml_tensor * dst);\n+void ggml_cuda_mul_mat_vec_f(ggml_backend_cuda_context & ctx, const ggml_tensor * src0, const ggml_tensor * src1, const ggml_tensor * ids, ggml_tensor * dst,\n+    const ggml_cuda_mm_fusion_args_host * fusion = nullptr);\n \n void ggml_cuda_op_mul_mat_vec_f(\n     ggml_backend_cuda_context & ctx,"
      },
      {
        "filename": "ggml/src/ggml-cuda/mmvq.cu",
        "status": "modified",
        "additions": 219,
        "deletions": 95,
        "changes": 314,
        "patch": "@@ -1,5 +1,6 @@\n #include \"mmvq.cuh\"\n #include \"quantize.cuh\"\n+#include \"unary.cuh\"\n #include \"vecdotq.cuh\"\n \n #include <cstdint>\n@@ -82,7 +83,7 @@ static __host__ mmvq_parameter_table_id get_device_table_id(int cc) {\n     return MMVQ_PARAMETERS_GENERIC;\n }\n \n-static constexpr __host__ __device__ int calc_nwarps(int ncols_dst,  mmvq_parameter_table_id table_id) {\n+static constexpr __host__ __device__ int calc_nwarps(int ncols_dst, mmvq_parameter_table_id table_id) {\n     if (table_id == MMVQ_PARAMETERS_GENERIC) {\n         switch (ncols_dst) {\n             case 1:\n@@ -136,11 +137,11 @@ static constexpr __host__ __device__ int calc_rows_per_block(int ncols_dst, int\n     return 1;\n }\n \n-template <ggml_type type, int ncols_dst>\n // tell the compiler to use as many registers as it wants, see nwarps definition below\n+template <ggml_type type, int ncols_dst, bool has_fusion>\n __launch_bounds__(calc_nwarps(ncols_dst, get_device_table_id())*ggml_cuda_get_physical_warp_size(), 1)\n static __global__ void mul_mat_vec_q(\n-        const void * __restrict__ vx, const void * __restrict__ vy, const int32_t * __restrict__ ids, float * __restrict__ dst,\n+        const void * __restrict__ vx, const void * __restrict__ vy, const int32_t * __restrict__ ids, const ggml_cuda_mm_fusion_args_device fusion, float * __restrict__ dst,\n         const uint32_t ncols_x, const uint3 nchannels_y, const uint32_t stride_row_x, const uint32_t stride_col_y,\n         const uint32_t stride_col_dst, const uint3 channel_ratio, const uint32_t stride_channel_x,\n         const uint32_t stride_channel_y, const uint32_t stride_channel_dst, const uint3 sample_ratio,\n@@ -169,8 +170,38 @@ static __global__ void mul_mat_vec_q(\n     const uint32_t sample_x    = fastdiv(sample_dst, sample_ratio);\n     const uint32_t sample_y    = sample_dst;\n \n+    bool use_gate = false;\n+    bool use_bias = false;\n+    bool use_gate_bias = false;\n+    const void * vgate = nullptr;\n+    const float * x_bias = nullptr;\n+    const float * gate_bias = nullptr;\n+    ggml_glu_op active_glu;\n+\n+    if constexpr (has_fusion) {\n+        use_gate      = fusion.gate      != nullptr;\n+        use_bias      = fusion.x_bias    != nullptr;\n+        use_gate_bias = fusion.gate_bias != nullptr && use_gate;\n+        vgate         = fusion.gate;\n+        x_bias        = (const float *) fusion.x_bias;\n+        gate_bias     = (const float *) fusion.gate_bias;\n+        active_glu    = fusion.glu_op;\n+    }\n+\n+    const uint32_t channel_bias = ids ? channel_x : channel_dst;\n+\n+    if constexpr (has_fusion) {\n+        if (use_bias) {\n+            x_bias = x_bias + sample_dst*stride_sample_dst + channel_bias*stride_channel_dst + row0;\n+        }\n+        if (use_gate_bias) {\n+            gate_bias = gate_bias + sample_dst*stride_sample_dst + channel_bias*stride_channel_dst + row0;\n+        }\n+    }\n+\n     // partial sum for each thread\n     float tmp[ncols_dst][rows_per_cuda_block] = {{0.0f}};\n+    float tmp_gate[ncols_dst][rows_per_cuda_block] = {{0.0f}};\n \n     const block_q8_1 * y = ((const block_q8_1 *) vy) + sample_y*stride_sample_y + channel_y*stride_channel_y;\n     const int kbx_offset = sample_x*stride_sample_x + channel_x*stride_channel_x + row0*stride_row_x;\n@@ -187,17 +218,35 @@ static __global__ void mul_mat_vec_q(\n             for (int i = 0; i < rows_per_cuda_block; ++i) {\n                 tmp[j][i] += vec_dot_q_cuda(\n                     vx, &y[j*stride_col_y + kby], kbx_offset + i*stride_row_x + kbx, kqs);\n+                if constexpr (has_fusion) {\n+                    if (use_gate) {\n+                        tmp_gate[j][i] += vec_dot_q_cuda(\n+                            vgate, &y[j*stride_col_y + kby], kbx_offset + i*stride_row_x + kbx, kqs);\n+                    }\n+                }\n             }\n         }\n     }\n \n     __shared__ float tmp_shared[nwarps-1 > 0 ? nwarps-1 : 1][ncols_dst][rows_per_cuda_block][warp_size];\n+    __shared__ float tmp_shared_gate[(has_fusion && (nwarps-1 > 0)) ? nwarps-1 : 1][ncols_dst][rows_per_cuda_block][warp_size];\n+    if constexpr (!has_fusion) {\n+        (void) tmp_shared_gate;\n+    } else if (!use_gate) {\n+        (void) tmp_shared_gate;\n+    }\n+\n     if (threadIdx.y > 0) {\n #pragma unroll\n         for (int j = 0; j < ncols_dst; ++j) {\n #pragma unroll\n             for (int i = 0; i < rows_per_cuda_block; ++i) {\n                 tmp_shared[threadIdx.y-1][j][i][threadIdx.x] = tmp[j][i];\n+                if constexpr (has_fusion) {\n+                    if (use_gate) {\n+                        tmp_shared_gate[threadIdx.y-1][j][i][threadIdx.x] = tmp_gate[j][i];\n+                    }\n+                }\n             }\n         }\n     }\n@@ -216,12 +265,49 @@ static __global__ void mul_mat_vec_q(\n #pragma unroll\n             for (int l = 0; l < nwarps-1; ++l) {\n                 tmp[j][i] += tmp_shared[l][j][i][threadIdx.x];\n+                if constexpr (has_fusion) {\n+                    if (use_gate) {\n+                        tmp_gate[j][i] += tmp_shared_gate[l][j][i][threadIdx.x];\n+                    }\n+                }\n             }\n             tmp[j][i] = warp_reduce_sum<warp_size>(tmp[j][i]);\n+            if constexpr (has_fusion) {\n+                if (use_gate) {\n+                    tmp_gate[j][i] = warp_reduce_sum<warp_size>(tmp_gate[j][i]);\n+                }\n+            }\n         }\n \n         if (threadIdx.x < rows_per_cuda_block && (rows_per_cuda_block == 1 || uint32_t(row0 + threadIdx.x) < stride_col_dst)) {\n-            dst[j*stride_col_dst + threadIdx.x] = tmp[j][threadIdx.x];\n+            float result = tmp[j][threadIdx.x];\n+            if constexpr (has_fusion) {\n+                if (use_bias) {\n+                    result += x_bias[j*stride_col_dst + threadIdx.x];\n+                }\n+                if (use_gate) {\n+                    float gate_value = tmp_gate[j][threadIdx.x];\n+                    if (use_gate_bias) {\n+                        gate_value += gate_bias[j*stride_col_dst + threadIdx.x];\n+                    }\n+                    switch (active_glu) {\n+                        case GGML_GLU_OP_SWIGLU:\n+                            result *= ggml_cuda_op_silu_single(gate_value);\n+                            break;\n+                        case GGML_GLU_OP_GEGLU:\n+                            result *= ggml_cuda_op_gelu_single(gate_value);\n+                            break;\n+                        case GGML_GLU_OP_SWIGLU_OAI: {\n+                            result = ggml_cuda_op_swiglu_oai_single(gate_value, result);\n+                            break;\n+                        }\n+                        default:\n+                            result = result * gate_value;\n+                            break;\n+                    }\n+                }\n+            }\n+            dst[j*stride_col_dst + threadIdx.x] = result;\n         }\n     }\n }\n@@ -235,9 +321,37 @@ static std::pair<dim3, dim3> calc_launch_params(\n     return {block_nums, block_dims};\n }\n \n+template<ggml_type type, int c_ncols_dst>\n+static void mul_mat_vec_q_switch_fusion(\n+        const void * vx, const void * vy, const int32_t * ids, const ggml_cuda_mm_fusion_args_device fusion, float * dst,\n+        const uint32_t ncols_x, const uint3 nchannels_y, const uint32_t stride_row_x, const uint32_t stride_col_y,\n+        const uint32_t stride_col_dst, const uint3 channel_ratio, const uint32_t stride_channel_x,\n+        const uint32_t stride_channel_y, const uint32_t stride_channel_dst, const uint3 sample_ratio,\n+        const uint32_t stride_sample_x, const uint32_t stride_sample_y, const uint32_t stride_sample_dst,\n+        const dim3 & block_nums, const dim3 & block_dims, const int nbytes_shared, cudaStream_t stream) {\n+\n+    const bool has_fusion = fusion.gate != nullptr || fusion.x_bias != nullptr || fusion.gate_bias != nullptr;\n+    if constexpr (c_ncols_dst == 1) {\n+        if (has_fusion) {\n+            mul_mat_vec_q<type, c_ncols_dst, true><<<block_nums, block_dims, nbytes_shared, stream>>>\n+                (vx, vy, ids, fusion, dst, ncols_x, nchannels_y, stride_row_x, stride_col_y, stride_col_dst,\n+                 channel_ratio, stride_channel_x, stride_channel_y, stride_channel_dst,\n+                 sample_ratio, stride_sample_x, stride_sample_y, stride_sample_dst);\n+            return;\n+        }\n+    }\n+\n+    GGML_ASSERT(!has_fusion && \"fusion only supported for ncols_dst=1\");\n+\n+    mul_mat_vec_q<type, c_ncols_dst, false><<<block_nums, block_dims, nbytes_shared, stream>>>\n+        (vx, vy, ids, fusion, dst, ncols_x, nchannels_y, stride_row_x, stride_col_y, stride_col_dst,\n+        channel_ratio, stride_channel_x, stride_channel_y, stride_channel_dst,\n+        sample_ratio, stride_sample_x, stride_sample_y, stride_sample_dst);\n+}\n+\n template <ggml_type type>\n static void mul_mat_vec_q_switch_ncols_dst(\n-        const void * vx, const void * vy, const int32_t * ids, float * dst,\n+        const void * vx, const void * vy, const int32_t * ids, const ggml_cuda_mm_fusion_args_device fusion, float * dst,\n         const int ncols_x, const int nrows_x, const int ncols_dst,\n         const int stride_row_x, const int stride_col_y, const int stride_col_dst,\n         const int nchannels_x, const int nchannels_y, const int nchannels_dst,\n@@ -256,80 +370,83 @@ static void mul_mat_vec_q_switch_ncols_dst(\n     const int warp_size = ggml_cuda_info().devices[device].warp_size;\n     const mmvq_parameter_table_id table_id = get_device_table_id(ggml_cuda_info().devices[device].cc);\n \n+    const bool has_fusion = fusion.gate != nullptr || fusion.x_bias != nullptr || fusion.gate_bias != nullptr;\n+\n     GGML_ASSERT(!ids || ncols_dst == 1);\n     switch (ncols_dst) {\n         case 1: {\n             constexpr int c_ncols_dst = 1;\n             std::pair<dim3, dim3> dims = calc_launch_params(c_ncols_dst, nrows_x, nchannels_dst, nsamples_dst, warp_size, table_id);\n-            mul_mat_vec_q<type, c_ncols_dst><<<dims.first, dims.second, 0, stream>>>\n-                (vx, vy, ids, dst, ncols_x, nchannels_y_fd, stride_row_x, stride_col_y, stride_col_dst,\n+            mul_mat_vec_q_switch_fusion<type, c_ncols_dst>(vx, vy, ids, fusion, dst, ncols_x, nchannels_y_fd, stride_row_x, stride_col_y, stride_col_dst,\n                  channel_ratio_fd, stride_channel_x, stride_channel_y, stride_channel_dst,\n-                 sample_ratio_fd, stride_sample_x, stride_sample_y, stride_sample_dst);\n+                 sample_ratio_fd, stride_sample_x, stride_sample_y, stride_sample_dst,\n+                 dims.first, dims.second, 0, stream);\n         } break;\n         case 2: {\n             constexpr int c_ncols_dst = 2;\n             std::pair<dim3, dim3> dims = calc_launch_params(c_ncols_dst, nrows_x, nchannels_dst, nsamples_dst, warp_size, table_id);\n-            mul_mat_vec_q<type, c_ncols_dst><<<dims.first, dims.second, 0, stream>>>\n-                (vx, vy, ids, dst, ncols_x, nchannels_y_fd, stride_row_x, stride_col_y, stride_col_dst,\n+            mul_mat_vec_q_switch_fusion<type, c_ncols_dst>(vx, vy, ids, fusion, dst, ncols_x, nchannels_y_fd, stride_row_x, stride_col_y, stride_col_dst,\n                  channel_ratio_fd, stride_channel_x, stride_channel_y, stride_channel_dst,\n-                 sample_ratio_fd, stride_sample_x, stride_sample_y, stride_sample_dst);\n+                 sample_ratio_fd, stride_sample_x, stride_sample_y, stride_sample_dst,\n+                 dims.first, dims.second, 0, stream);\n         } break;\n         case 3: {\n             constexpr int c_ncols_dst = 3;\n             std::pair<dim3, dim3> dims = calc_launch_params(c_ncols_dst, nrows_x, nchannels_dst, nsamples_dst, warp_size, table_id);\n-            mul_mat_vec_q<type, c_ncols_dst><<<dims.first, dims.second, 0, stream>>>\n-                (vx, vy, ids, dst, ncols_x, nchannels_y_fd, stride_row_x, stride_col_y, stride_col_dst,\n+            mul_mat_vec_q_switch_fusion<type, c_ncols_dst>(vx, vy, ids, fusion, dst, ncols_x, nchannels_y_fd, stride_row_x, stride_col_y, stride_col_dst,\n                  channel_ratio_fd, stride_channel_x, stride_channel_y, stride_channel_dst,\n-                 sample_ratio_fd, stride_sample_x, stride_sample_y, stride_sample_dst);\n+                 sample_ratio_fd, stride_sample_x, stride_sample_y, stride_sample_dst,\n+                 dims.first, dims.second, 0, stream);\n         } break;\n         case 4: {\n             constexpr int c_ncols_dst = 4;\n             std::pair<dim3, dim3> dims = calc_launch_params(c_ncols_dst, nrows_x, nchannels_dst, nsamples_dst, warp_size, table_id);\n-            mul_mat_vec_q<type, c_ncols_dst><<<dims.first, dims.second, 0, stream>>>\n-                (vx, vy, ids, dst, ncols_x, nchannels_y_fd, stride_row_x, stride_col_y, stride_col_dst,\n+            mul_mat_vec_q_switch_fusion<type, c_ncols_dst>(vx, vy, ids, fusion, dst, ncols_x, nchannels_y_fd, stride_row_x, stride_col_y, stride_col_dst,\n                  channel_ratio_fd, stride_channel_x, stride_channel_y, stride_channel_dst,\n-                 sample_ratio_fd, stride_sample_x, stride_sample_y, stride_sample_dst);\n+                 sample_ratio_fd, stride_sample_x, stride_sample_y, stride_sample_dst,\n+                 dims.first, dims.second, 0, stream);\n         } break;\n         case 5: {\n             constexpr int c_ncols_dst = 5;\n             std::pair<dim3, dim3> dims = calc_launch_params(c_ncols_dst, nrows_x, nchannels_dst, nsamples_dst, warp_size, table_id);\n-            mul_mat_vec_q<type, c_ncols_dst><<<dims.first, dims.second, 0, stream>>>\n-                (vx, vy, ids, dst, ncols_x, nchannels_y_fd, stride_row_x, stride_col_y, stride_col_dst,\n+            mul_mat_vec_q_switch_fusion<type, c_ncols_dst>(vx, vy, ids, fusion, dst, ncols_x, nchannels_y_fd, stride_row_x, stride_col_y, stride_col_dst,\n                  channel_ratio_fd, stride_channel_x, stride_channel_y, stride_channel_dst,\n-                 sample_ratio_fd, stride_sample_x, stride_sample_y, stride_sample_dst);\n+                 sample_ratio_fd, stride_sample_x, stride_sample_y, stride_sample_dst,\n+                 dims.first, dims.second, 0, stream);\n         } break;\n         case 6: {\n             constexpr int c_ncols_dst = 6;\n             std::pair<dim3, dim3> dims = calc_launch_params(c_ncols_dst, nrows_x, nchannels_dst, nsamples_dst, warp_size, table_id);\n-            mul_mat_vec_q<type, c_ncols_dst><<<dims.first, dims.second, 0, stream>>>\n-                (vx, vy, ids, dst, ncols_x, nchannels_y_fd, stride_row_x, stride_col_y, stride_col_dst,\n+            mul_mat_vec_q_switch_fusion<type, c_ncols_dst>(vx, vy, ids, fusion, dst, ncols_x, nchannels_y_fd, stride_row_x, stride_col_y, stride_col_dst,\n                  channel_ratio_fd, stride_channel_x, stride_channel_y, stride_channel_dst,\n-                 sample_ratio_fd, stride_sample_x, stride_sample_y, stride_sample_dst);\n+                 sample_ratio_fd, stride_sample_x, stride_sample_y, stride_sample_dst,\n+                 dims.first, dims.second, 0, stream);\n         } break;\n         case 7: {\n             constexpr int c_ncols_dst = 7;\n             std::pair<dim3, dim3> dims = calc_launch_params(c_ncols_dst, nrows_x, nchannels_dst, nsamples_dst, warp_size, table_id);\n-            mul_mat_vec_q<type, c_ncols_dst><<<dims.first, dims.second, 0, stream>>>\n-                (vx, vy, ids, dst, ncols_x, nchannels_y_fd, stride_row_x, stride_col_y, stride_col_dst,\n+            mul_mat_vec_q_switch_fusion<type, c_ncols_dst>(vx, vy, ids, fusion, dst, ncols_x, nchannels_y_fd, stride_row_x, stride_col_y, stride_col_dst,\n                  channel_ratio_fd, stride_channel_x, stride_channel_y, stride_channel_dst,\n-                 sample_ratio_fd, stride_sample_x, stride_sample_y, stride_sample_dst);\n+                 sample_ratio_fd, stride_sample_x, stride_sample_y, stride_sample_dst,\n+                 dims.first, dims.second, 0, stream);\n         } break;\n         case 8: {\n             constexpr int c_ncols_dst = 8;\n             std::pair<dim3, dim3> dims = calc_launch_params(c_ncols_dst, nrows_x, nchannels_dst, nsamples_dst, warp_size, table_id);\n-            mul_mat_vec_q<type, c_ncols_dst><<<dims.first, dims.second, 0, stream>>>\n-                (vx, vy, ids, dst, ncols_x, nchannels_y_fd, stride_row_x, stride_col_y, stride_col_dst,\n+            mul_mat_vec_q_switch_fusion<type, c_ncols_dst>(vx, vy, ids, fusion, dst, ncols_x, nchannels_y_fd, stride_row_x, stride_col_y, stride_col_dst,\n                  channel_ratio_fd, stride_channel_x, stride_channel_y, stride_channel_dst,\n-                 sample_ratio_fd, stride_sample_x, stride_sample_y, stride_sample_dst);\n+                 sample_ratio_fd, stride_sample_x, stride_sample_y, stride_sample_dst,\n+                 dims.first, dims.second, 0, stream);\n         } break;\n         default:\n             GGML_ABORT(\"fatal error\");\n             break;\n     }\n-}\n \n+    GGML_UNUSED(has_fusion);\n+}\n static void mul_mat_vec_q_switch_type(\n-        const void * vx, const ggml_type type_x, const void * vy, const int32_t * ids, float * dst,\n+        const void * vx, const ggml_type type_x, const void * vy, const int32_t * ids, const ggml_cuda_mm_fusion_args_device fusion, float * dst,\n         const int ncols_x, const int nrows_x, const int ncols_dst,\n         const int stride_row_x, const int stride_col_y, const int stride_col_dst,\n         const int nchannels_x, const int nchannels_y, const int nchannels_dst,\n@@ -339,143 +456,123 @@ static void mul_mat_vec_q_switch_type(\n     switch (type_x) {\n         case GGML_TYPE_Q4_0:\n             mul_mat_vec_q_switch_ncols_dst<GGML_TYPE_Q4_0>\n-                (vx, vy, ids, dst, ncols_x, nrows_x, ncols_dst, stride_row_x, stride_col_y, stride_col_dst,\n+                (vx, vy, ids, fusion, dst, ncols_x, nrows_x, ncols_dst, stride_row_x, stride_col_y, stride_col_dst,\n                  nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y, stride_channel_dst,\n-                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst,\n-                 stream);\n+                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, stream);\n             break;\n         case GGML_TYPE_Q4_1:\n             mul_mat_vec_q_switch_ncols_dst<GGML_TYPE_Q4_1>\n-                (vx, vy, ids, dst, ncols_x, nrows_x, ncols_dst, stride_row_x, stride_col_y, stride_col_dst,\n+                (vx, vy, ids, fusion, dst, ncols_x, nrows_x, ncols_dst, stride_row_x, stride_col_y, stride_col_dst,\n                  nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y, stride_channel_dst,\n-                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst,\n-                 stream);\n+                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, stream);\n             break;\n         case GGML_TYPE_Q5_0:\n             mul_mat_vec_q_switch_ncols_dst<GGML_TYPE_Q5_0>\n-                (vx, vy, ids, dst, ncols_x, nrows_x, ncols_dst, stride_row_x, stride_col_y, stride_col_dst,\n+                (vx, vy, ids, fusion, dst, ncols_x, nrows_x, ncols_dst, stride_row_x, stride_col_y, stride_col_dst,\n                  nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y, stride_channel_dst,\n-                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst,\n-                 stream);\n+                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, stream);\n             break;\n         case GGML_TYPE_Q5_1:\n             mul_mat_vec_q_switch_ncols_dst<GGML_TYPE_Q5_1>\n-                (vx, vy, ids, dst, ncols_x, nrows_x, ncols_dst, stride_row_x, stride_col_y, stride_col_dst,\n+                (vx, vy, ids, fusion, dst, ncols_x, nrows_x, ncols_dst, stride_row_x, stride_col_y, stride_col_dst,\n                  nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y, stride_channel_dst,\n-                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst,\n-                 stream);\n+                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, stream);\n             break;\n         case GGML_TYPE_Q8_0:\n             mul_mat_vec_q_switch_ncols_dst<GGML_TYPE_Q8_0>\n-                (vx, vy, ids, dst, ncols_x, nrows_x, ncols_dst, stride_row_x, stride_col_y, stride_col_dst,\n+                (vx, vy, ids, fusion, dst, ncols_x, nrows_x, ncols_dst, stride_row_x, stride_col_y, stride_col_dst,\n                  nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y, stride_channel_dst,\n-                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst,\n-                 stream);\n+                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, stream);\n             break;\n         case GGML_TYPE_MXFP4:\n             mul_mat_vec_q_switch_ncols_dst<GGML_TYPE_MXFP4>\n-                (vx, vy, ids, dst, ncols_x, nrows_x, ncols_dst, stride_row_x, stride_col_y, stride_col_dst,\n+                (vx, vy, ids, fusion, dst, ncols_x, nrows_x, ncols_dst, stride_row_x, stride_col_y, stride_col_dst,\n                  nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y, stride_channel_dst,\n-                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst,\n-                 stream);\n+                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, stream);\n             break;\n         case GGML_TYPE_Q2_K:\n             mul_mat_vec_q_switch_ncols_dst<GGML_TYPE_Q2_K>\n-                (vx, vy, ids, dst, ncols_x, nrows_x, ncols_dst, stride_row_x, stride_col_y, stride_col_dst,\n+                (vx, vy, ids, fusion, dst, ncols_x, nrows_x, ncols_dst, stride_row_x, stride_col_y, stride_col_dst,\n                  nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y, stride_channel_dst,\n-                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst,\n-                 stream);\n+                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, stream);\n             break;\n         case GGML_TYPE_Q3_K:\n             mul_mat_vec_q_switch_ncols_dst<GGML_TYPE_Q3_K>\n-                (vx, vy, ids, dst, ncols_x, nrows_x, ncols_dst, stride_row_x, stride_col_y, stride_col_dst,\n+                (vx, vy, ids, fusion, dst, ncols_x, nrows_x, ncols_dst, stride_row_x, stride_col_y, stride_col_dst,\n                  nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y, stride_channel_dst,\n-                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst,\n-                 stream);\n+                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, stream);\n             break;\n         case GGML_TYPE_Q4_K:\n             mul_mat_vec_q_switch_ncols_dst<GGML_TYPE_Q4_K>\n-                (vx, vy, ids, dst, ncols_x, nrows_x, ncols_dst, stride_row_x, stride_col_y, stride_col_dst,\n+                (vx, vy, ids, fusion, dst, ncols_x, nrows_x, ncols_dst, stride_row_x, stride_col_y, stride_col_dst,\n                  nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y, stride_channel_dst,\n-                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst,\n-                 stream);\n+                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, stream);\n             break;\n         case GGML_TYPE_Q5_K:\n             mul_mat_vec_q_switch_ncols_dst<GGML_TYPE_Q5_K>\n-                (vx, vy, ids, dst, ncols_x, nrows_x, ncols_dst, stride_row_x, stride_col_y, stride_col_dst,\n+                (vx, vy, ids, fusion, dst, ncols_x, nrows_x, ncols_dst, stride_row_x, stride_col_y, stride_col_dst,\n                  nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y, stride_channel_dst,\n-                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst,\n-                 stream);\n+                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, stream);\n             break;\n         case GGML_TYPE_Q6_K:\n             mul_mat_vec_q_switch_ncols_dst<GGML_TYPE_Q6_K>\n-                (vx, vy, ids, dst, ncols_x, nrows_x, ncols_dst, stride_row_x, stride_col_y, stride_col_dst,\n+                (vx, vy, ids, fusion, dst, ncols_x, nrows_x, ncols_dst, stride_row_x, stride_col_y, stride_col_dst,\n                  nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y, stride_channel_dst,\n-                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst,\n-                 stream);\n+                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, stream);\n             break;\n         case GGML_TYPE_IQ2_XXS:\n             mul_mat_vec_q_switch_ncols_dst<GGML_TYPE_IQ2_XXS>\n-                (vx, vy, ids, dst, ncols_x, nrows_x, ncols_dst, stride_row_x, stride_col_y, stride_col_dst,\n+                (vx, vy, ids, fusion, dst, ncols_x, nrows_x, ncols_dst, stride_row_x, stride_col_y, stride_col_dst,\n                  nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y, stride_channel_dst,\n-                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst,\n-                 stream);\n+                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, stream);\n             break;\n         case GGML_TYPE_IQ2_XS:\n             mul_mat_vec_q_switch_ncols_dst<GGML_TYPE_IQ2_XS>\n-                (vx, vy, ids, dst, ncols_x, nrows_x, ncols_dst, stride_row_x, stride_col_y, stride_col_dst,\n+                (vx, vy, ids, fusion, dst, ncols_x, nrows_x, ncols_dst, stride_row_x, stride_col_y, stride_col_dst,\n                  nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y, stride_channel_dst,\n-                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst,\n-                 stream);\n+                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, stream);\n             break;\n         case GGML_TYPE_IQ2_S:\n             mul_mat_vec_q_switch_ncols_dst<GGML_TYPE_IQ2_S>\n-                (vx, vy, ids, dst, ncols_x, nrows_x, ncols_dst, stride_row_x, stride_col_y, stride_col_dst,\n+                (vx, vy, ids, fusion, dst, ncols_x, nrows_x, ncols_dst, stride_row_x, stride_col_y, stride_col_dst,\n                  nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y, stride_channel_dst,\n-                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst,\n-                 stream);\n+                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, stream);\n             break;\n         case GGML_TYPE_IQ3_XXS:\n             mul_mat_vec_q_switch_ncols_dst<GGML_TYPE_IQ3_XXS>\n-                (vx, vy, ids, dst, ncols_x, nrows_x, ncols_dst, stride_row_x, stride_col_y, stride_col_dst,\n+                (vx, vy, ids, fusion, dst, ncols_x, nrows_x, ncols_dst, stride_row_x, stride_col_y, stride_col_dst,\n                  nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y, stride_channel_dst,\n-                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst,\n-                 stream);\n+                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, stream);\n             break;\n         case GGML_TYPE_IQ1_S:\n             mul_mat_vec_q_switch_ncols_dst<GGML_TYPE_IQ1_S>\n-                (vx, vy, ids, dst, ncols_x, nrows_x, ncols_dst, stride_row_x, stride_col_y, stride_col_dst,\n+                (vx, vy, ids, fusion, dst, ncols_x, nrows_x, ncols_dst, stride_row_x, stride_col_y, stride_col_dst,\n                  nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y, stride_channel_dst,\n-                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst,\n-                 stream);\n+                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, stream);\n             break;\n         case GGML_TYPE_IQ1_M:\n             mul_mat_vec_q_switch_ncols_dst<GGML_TYPE_IQ1_M>\n-                (vx, vy, ids, dst, ncols_x, nrows_x, ncols_dst, stride_row_x, stride_col_y, stride_col_dst,\n+                (vx, vy, ids, fusion, dst, ncols_x, nrows_x, ncols_dst, stride_row_x, stride_col_y, stride_col_dst,\n                  nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y, stride_channel_dst,\n-                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst,\n-                 stream);\n+                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, stream);\n             break;\n         case GGML_TYPE_IQ4_NL:\n             mul_mat_vec_q_switch_ncols_dst<GGML_TYPE_IQ4_NL>\n-                (vx, vy, ids, dst, ncols_x, nrows_x, ncols_dst, stride_row_x, stride_col_y, stride_col_dst,\n+                (vx, vy, ids, fusion, dst, ncols_x, nrows_x, ncols_dst, stride_row_x, stride_col_y, stride_col_dst,\n                  nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y, stride_channel_dst,\n-                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst,\n-                 stream);\n+                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, stream);\n             break;\n         case GGML_TYPE_IQ4_XS:\n             mul_mat_vec_q_switch_ncols_dst<GGML_TYPE_IQ4_XS>\n-                (vx, vy, ids, dst, ncols_x, nrows_x, ncols_dst, stride_row_x, stride_col_y, stride_col_dst,\n+                (vx, vy, ids, fusion, dst, ncols_x, nrows_x, ncols_dst, stride_row_x, stride_col_y, stride_col_dst,\n                  nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y, stride_channel_dst,\n-                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst,\n-                 stream);\n+                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, stream);\n             break;\n         case GGML_TYPE_IQ3_S:\n             mul_mat_vec_q_switch_ncols_dst<GGML_TYPE_IQ3_S>\n-                (vx, vy, ids, dst, ncols_x, nrows_x, ncols_dst, stride_row_x, stride_col_y, stride_col_dst,\n+                (vx, vy, ids, fusion, dst, ncols_x, nrows_x, ncols_dst, stride_row_x, stride_col_y, stride_col_dst,\n                  nchannels_x, nchannels_y, nchannels_dst, stride_channel_x, stride_channel_y, stride_channel_dst,\n-                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst,\n-                 stream);\n+                 nsamples_x, nsamples_dst, stride_sample_x, stride_sample_y, stride_sample_dst, stream);\n             break;\n         default:\n             GGML_ABORT(\"fatal error\");\n@@ -484,7 +581,8 @@ static void mul_mat_vec_q_switch_type(\n }\n \n void ggml_cuda_mul_mat_vec_q(\n-        ggml_backend_cuda_context & ctx, const ggml_tensor * src0, const ggml_tensor * src1, const ggml_tensor * ids, ggml_tensor * dst) {\n+        ggml_backend_cuda_context & ctx, const ggml_tensor * src0, const ggml_tensor * src1, const ggml_tensor * ids, ggml_tensor * dst,\n+        const ggml_cuda_mm_fusion_args_host * fusion) {\n     GGML_ASSERT(        src1->type == GGML_TYPE_F32);\n     GGML_ASSERT(        dst->type  == GGML_TYPE_F32);\n     GGML_ASSERT(!ids || ids->type  == GGML_TYPE_I32); // Optional, used for batched GGML_MUL_MAT_ID.\n@@ -508,6 +606,31 @@ void ggml_cuda_mul_mat_vec_q(\n     const int32_t *  ids_d = ids ? (const int32_t *)  ids->data : nullptr;\n     float         *  dst_d =       (float         *)  dst->data;\n \n+    ggml_cuda_mm_fusion_args_device fusion_local{};\n+\n+    if (fusion) {\n+        GGML_ASSERT( !ids || dst->ne[2] == 1);\n+        GGML_ASSERT(  ids || dst->ne[1] == 1);\n+\n+        if (fusion->x_bias) {\n+            GGML_ASSERT(fusion->x_bias->type == GGML_TYPE_F32);\n+            GGML_ASSERT(fusion->x_bias->ne[0] == dst->ne[0]);\n+            GGML_ASSERT(!ids || fusion->x_bias->ne[1] == src0->ne[2]);\n+            fusion_local.x_bias = fusion->x_bias->data;\n+        }\n+        if (fusion->gate) {\n+            GGML_ASSERT(fusion->gate->type == src0->type && ggml_are_same_stride(fusion->gate, src0));\n+            fusion_local.gate = fusion->gate->data;\n+        }\n+        if (fusion->gate_bias) {\n+            GGML_ASSERT(fusion->gate_bias->type == GGML_TYPE_F32);\n+            GGML_ASSERT(fusion->gate_bias->ne[0] == dst->ne[0]);\n+            GGML_ASSERT(!ids || fusion->gate_bias->ne[1] == src0->ne[2]);\n+            fusion_local.gate_bias = fusion->gate_bias->data;\n+        }\n+        fusion_local.glu_op = fusion->glu_op;\n+    }\n+\n     // If src0 is a temporary compute buffer, clear any potential padding.\n     if (ggml_backend_buffer_get_usage(src0->buffer) == GGML_BACKEND_BUFFER_USAGE_COMPUTE) {\n         const size_t size_data  = ggml_nbytes(src0);\n@@ -549,10 +672,10 @@ void ggml_cuda_mul_mat_vec_q(\n     const int64_t stride_channel_y   = ids ? s11  : s12;\n \n     mul_mat_vec_q_switch_type(\n-        src0->data, src0->type, src1_q8_1.get(), ids_d, dst_d, ne00,\n+        src0->data, src0->type, src1_q8_1.get(), ids_d, fusion_local, dst_d, ne00,\n         ne01,              ncols_dst,     s01, stride_col_y,     stride_col_dst,\n         ne02, nchannels_y, nchannels_dst, s02, stride_channel_y, stride_channel_dst,\n-        ne03,              ne3,           s03, s13,              s3,                 stream);\n+        ne03,              ne3,           s03, s13,              s3,               stream);\n }\n \n void ggml_cuda_op_mul_mat_vec_q(\n@@ -578,8 +701,9 @@ void ggml_cuda_op_mul_mat_vec_q(\n     const int stride_row_x = ne00 / ggml_blck_size(src0->type);\n     const int stride_col_y = src1_padded_row_size / QK8_1;\n \n+    ggml_cuda_mm_fusion_args_device fusion_local{};\n     mul_mat_vec_q_switch_type(\n-        src0_dd_i, src0->type, src1_ddq_i, nullptr, dst_dd_i, ne00, row_diff, src1_ncols, stride_row_x, stride_col_y, nrows_dst,\n+        src0_dd_i, src0->type, src1_ddq_i, nullptr, fusion_local, dst_dd_i, ne00, row_diff, src1_ncols, stride_row_x, stride_col_y, nrows_dst,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, stream);\n \n     GGML_UNUSED_VARS(src1, dst, src1_ddf_i, src1_ncols, src1_padded_row_size);"
      },
      {
        "filename": "ggml/src/ggml-cuda/mmvq.cuh",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -3,7 +3,7 @@\n #define MMVQ_MAX_BATCH_SIZE 8 // Max. batch size for which to use MMVQ kernels.\n \n void ggml_cuda_mul_mat_vec_q(ggml_backend_cuda_context & ctx,\n-    const ggml_tensor * src0, const ggml_tensor * src1, const ggml_tensor * ids, ggml_tensor * dst);\n+    const ggml_tensor * src0, const ggml_tensor * src1, const ggml_tensor * ids, ggml_tensor * dst, const ggml_cuda_mm_fusion_args_host * fusion = nullptr);\n \n void ggml_cuda_op_mul_mat_vec_q(\n     ggml_backend_cuda_context & ctx,"
      },
      {
        "filename": "ggml/src/ggml-cuda/unary.cu",
        "status": "modified",
        "additions": 3,
        "deletions": 11,
        "changes": 14,
        "patch": "@@ -18,10 +18,7 @@ static __device__ __forceinline__ float op_step(float x) {\n }\n \n static __device__ __forceinline__ float op_gelu(float x) {\n-    const float GELU_COEF_A    = 0.044715f;\n-    const float SQRT_2_OVER_PI = 0.79788456080286535587989211986876f;\n-\n-    return 0.5f*x*(1.0f + tanhf(SQRT_2_OVER_PI*x*(1.0f + GELU_COEF_A*x*x)));\n+    return ggml_cuda_op_gelu_single(x);\n }\n \n static __device__ __forceinline__ float op_gelu_erf(float x) {\n@@ -37,7 +34,7 @@ static __device__ __forceinline__ float op_gelu_quick(float x) {\n }\n \n static __device__ __forceinline__ float op_silu(float x) {\n-    return x / (1.0f + expf(-x));\n+    return ggml_cuda_op_silu_single(x);\n }\n \n static __device__ __forceinline__ float op_tanh(float x) {\n@@ -317,13 +314,8 @@ static __global__ void swiglu_oai_kernel(const T * x, const T * g, T * dst, cons\n \n     float xi = x[j0];\n     float gi = g[j1];\n-    xi = fminf(xi, limit);\n-    gi = fmaxf(fminf(gi, limit), -limit);\n-\n-    float out_glu = xi / (1.0f + expf(-xi * alpha));\n-    out_glu = out_glu * (1.0f + gi);\n \n-    dst[i] = out_glu;\n+    dst[i] = ggml_cuda_op_swiglu_oai_single(xi, gi, alpha, limit);\n }\n \n template <typename T>"
      },
      {
        "filename": "ggml/src/ggml-cuda/unary.cuh",
        "status": "modified",
        "additions": 21,
        "deletions": 0,
        "changes": 21,
        "patch": "@@ -1,3 +1,4 @@\n+#pragma once\n #include \"common.cuh\"\n \n #define CUDA_NEG_BLOCK_SIZE 256\n@@ -75,3 +76,23 @@ void ggml_cuda_op_geglu_erf(ggml_backend_cuda_context & ctx, ggml_tensor * dst);\n void ggml_cuda_op_geglu_quick(ggml_backend_cuda_context & ctx, ggml_tensor * dst);\n \n void ggml_cuda_op_xielu(ggml_backend_cuda_context & ctx, ggml_tensor * dst);\n+\n+__device__ __forceinline__ float ggml_cuda_op_silu_single(float x) {\n+    return x / (1.0f + expf(-x));\n+}\n+\n+__device__ __forceinline__ float ggml_cuda_op_gelu_single(float x) {\n+    const float GELU_COEF_A    = 0.044715f;\n+    const float SQRT_2_OVER_PI = 0.79788456080286535587989211986876f;\n+\n+    return 0.5f * x * (1.0f + tanhf(SQRT_2_OVER_PI * x * (1.0f + GELU_COEF_A * x * x)));\n+}\n+\n+__device__ __forceinline__ float ggml_cuda_op_swiglu_oai_single(float x, float g, float alpha = 1.702f, float limit = 7.0f) {\n+    x = fminf(x, limit);\n+    g = fmaxf(fminf(g, limit), -limit);\n+\n+    float out_glu = x / (1.0f + expf(-x * alpha));\n+    out_glu = out_glu * (1.0f + g);\n+    return out_glu;\n+}"
      },
      {
        "filename": "src/llama-graph.cpp",
        "status": "modified",
        "additions": 6,
        "deletions": 0,
        "changes": 6,
        "patch": "@@ -810,6 +810,9 @@ ggml_tensor * llm_graph_context::build_ffn(\n             GGML_ABORT(\"fatal error\");\n     }\n \n+    //expand here so that we can fuse ffn gate\n+    ggml_build_forward_expand(gf, cur);\n+\n     if (gate && type_gate == LLM_FFN_PAR) {\n         cur = ggml_mul(ctx0, cur, tmp);\n         cb(cur, \"ffn_gate_par\", il);\n@@ -1091,6 +1094,9 @@ ggml_tensor * llm_graph_context::build_moe_ffn(\n             GGML_ABORT(\"fatal error\");\n     }\n \n+    //expand here so that we can fuse ffn gate\n+    ggml_build_forward_expand(gf, cur);\n+\n     experts = build_lora_mm_id(down_exps, cur, selected_experts); // [n_embd, n_expert_used, n_tokens]\n     cb(experts, \"ffn_moe_down\", il);\n "
      },
      {
        "filename": "tests/test-backend-ops.cpp",
        "status": "modified",
        "additions": 161,
        "deletions": 0,
        "changes": 161,
        "patch": "@@ -4721,6 +4721,140 @@ struct test_topk_moe: public test_case {\n     }\n };\n \n+struct test_mul_mat_vec_fusion : public test_case {\n+    const ggml_type type;\n+    const ggml_glu_op glu_op;\n+    const int64_t m;\n+    const int64_t n;\n+    const int64_t k;\n+    const bool use_id;\n+    const int n_mats;\n+    const int n_used;\n+    const bool b;        // broadcast b matrix (only for use_id)\n+    const bool with_bias;\n+    const bool with_gate;\n+\n+    test_mul_mat_vec_fusion(ggml_type type, ggml_glu_op op, int64_t m, int64_t n, int64_t k,\n+                        bool use_id = false, int n_mats = 1, int n_used = 1, bool b = false, bool with_bias = false, bool with_gate = true)\n+    : type(type), glu_op(op), m(m), n(n), k(k), use_id(use_id), n_mats(n_mats), n_used(n_used), b(b), with_bias(with_bias), with_gate(with_gate) {\n+        if (use_id) {\n+            GGML_ASSERT(n_used <= n_mats);\n+        }\n+    }\n+\n+    std::string vars() override {\n+        return VARS_TO_STR11(type, glu_op, m, n, k, use_id, n_mats, n_used, b, with_bias, with_gate);\n+    }\n+\n+    std::string op_desc(ggml_tensor * t) override {\n+        GGML_UNUSED(t);\n+        return \"MUL_MAT_VEC_FUSION\";\n+    }\n+\n+    bool run_whole_graph() override { return true; }\n+\n+    ggml_tensor * build_gate(ggml_context * ctx, ggml_tensor * ffn_gate, ggml_tensor * ffn_up) {\n+        ggml_tensor * out = nullptr;\n+        if (with_gate) {\n+            if (glu_op == GGML_GLU_OP_SWIGLU_OAI) {\n+                constexpr float alpha = 1.702f;\n+                constexpr float limit = 7.0f;\n+                out = ggml_swiglu_oai(ctx, ffn_gate, ffn_up, alpha, limit);\n+            } else {\n+                out = ggml_glu_split(ctx, ffn_gate, ffn_up, glu_op);\n+            }\n+        }\n+        return out;\n+    }\n+\n+    ggml_tensor * build_graph(ggml_context * ctx) override {\n+        if (!use_id) {\n+            std::array<int64_t, 4> ne = {k, m, 1, 1};\n+            std::array<int64_t, 4> ne0 = {k, n, 1, 1};\n+\n+            ggml_tensor * cur  = ggml_new_tensor(ctx, GGML_TYPE_F32, 4, ne.data());\n+            ggml_tensor * gate = with_gate ? ggml_new_tensor(ctx, type, 4, ne0.data()) : nullptr;\n+            ggml_tensor * up   = ggml_new_tensor(ctx, type, 4, ne0.data());\n+\n+            ggml_tensor * ffn_up = ggml_mul_mat(ctx, up, cur);\n+            if (with_bias) {\n+                std::array<int64_t, 4> bias_ne = {ffn_up->ne[0], 1, 1, 1};\n+                ggml_tensor * up_bias = ggml_new_tensor(ctx, GGML_TYPE_F32, 4, bias_ne.data());\n+                ffn_up = ggml_add(ctx, ffn_up, up_bias);\n+            }\n+\n+            ggml_tensor * ffn_gate = with_gate ? ggml_mul_mat(ctx, gate, cur) : nullptr;\n+            if (with_bias && with_gate) {\n+                std::array<int64_t, 4> bias_ne = {ffn_gate->ne[0], 1, 1, 1};\n+                ggml_tensor * gate_bias = ggml_new_tensor(ctx, GGML_TYPE_F32, 4, bias_ne.data());\n+                ffn_gate = ggml_add(ctx, ffn_gate, gate_bias);\n+            }\n+\n+            ggml_tensor * out = with_gate ? build_gate(ctx, ffn_gate, ffn_up) : ffn_up;\n+            ggml_set_name(out, \"out\");\n+            return out;\n+        } else {\n+            ggml_tensor * gates = ggml_new_tensor_3d(ctx, type, k, n, n_mats);\n+            ggml_tensor * ups   = ggml_new_tensor_3d(ctx, type, k, n, n_mats);\n+            ggml_tensor * ids   = ggml_new_tensor_2d(ctx, GGML_TYPE_I32, n_mats, m);\n+\n+            if (n_used != n_mats) {\n+                ids = ggml_view_2d(ctx, ids, n_used, m, ids->nb[1], 0);\n+            }\n+\n+            ggml_tensor * cur = ggml_new_tensor_3d(ctx, GGML_TYPE_F32, k, this->b ? 1 : n_used, m);\n+            ggml_set_name(cur, \"cur\");\n+\n+            ggml_tensor * ffn_up = ggml_mul_mat_id(ctx, ups, cur, ids);\n+            if (with_bias) {\n+                ggml_tensor * up_bias_param = ggml_new_tensor_2d(ctx, GGML_TYPE_F32, ffn_up->ne[0], n_mats);\n+                ffn_up = ggml_add_id(ctx, ffn_up, up_bias_param, ids);\n+            }\n+\n+            ggml_tensor * ffn_gate = with_gate? ggml_mul_mat_id(ctx, gates, cur, ids) : nullptr;\n+            if (with_bias && with_gate) {\n+                ggml_tensor * gate_bias_param = ggml_new_tensor_2d(ctx, GGML_TYPE_F32, ffn_gate->ne[0], n_mats);\n+                ffn_gate = ggml_add_id(ctx, ffn_gate, gate_bias_param, ids);\n+            }\n+\n+            ggml_tensor * out = with_gate ? build_gate(ctx, ffn_gate, ffn_up) : ffn_up;\n+            ggml_set_name(out, \"out\");\n+            return out;\n+        }\n+    }\n+\n+    void initialize_tensors(ggml_context * ctx) override {\n+        if (!use_id) {\n+            for (ggml_tensor * t = ggml_get_first_tensor(ctx); t != NULL; t = ggml_get_next_tensor(ctx, t)) {\n+                init_tensor_uniform(t);\n+            }\n+        } else {\n+            std::random_device rd;\n+            std::default_random_engine rng(rd());\n+            for (ggml_tensor * t = ggml_get_first_tensor(ctx); t != NULL; t = ggml_get_next_tensor(ctx, t)) {\n+                if (t->type == GGML_TYPE_I32) {\n+                    if (ggml_is_view_op(t->op)) { continue; }\n+                    // ids\n+                    for (int64_t r = 0; r < ggml_nrows(t); r++) {\n+                        std::vector<int32_t> data(t->ne[0]);\n+                        for (int i = 0; i < t->ne[0]; i++) {\n+                            data[i] = i % n_mats;\n+                        }\n+                        std::shuffle(data.begin(), data.end(), rng);\n+                        ggml_backend_tensor_set(t, data.data(), r * t->nb[1], t->ne[0] * sizeof(int32_t));\n+                    }\n+                } else {\n+                    init_tensor_uniform(t);\n+                }\n+            }\n+        }\n+    }\n+\n+    double max_nmse_err() override {\n+        return 5e-3;\n+    }\n+};\n+\n // GGML_OP_SUM\n struct test_sum : public test_case {\n     const ggml_type type;\n@@ -6983,6 +7117,33 @@ static std::vector<std::unique_ptr<test_case>> make_test_cases_eval() {\n     test_cases.emplace_back(new test_opt_step_adamw(GGML_TYPE_F32, {10, 5, 4, 3}));\n     test_cases.emplace_back(new test_opt_step_sgd(GGML_TYPE_F32, {10, 5, 4, 3}));\n \n+    for (ggml_type type : base_types) {\n+        for (bool with_gate : {false, true}) {\n+            for (bool use_id : {false, true}) {\n+                for (bool b : {false, true}) {\n+                    if (!use_id && b) {\n+                        continue;\n+                    }\n+                    for (bool with_bias : {false, true}) {\n+                        if (!with_gate && !with_bias) {\n+                            continue;\n+                        }\n+                        for (ggml_glu_op glu_op : {GGML_GLU_OP_SWIGLU, GGML_GLU_OP_GEGLU}) {\n+                            if (!with_bias && glu_op == GGML_GLU_OP_SWIGLU_OAI) {\n+                                continue;\n+                            }\n+                            if (!with_gate && glu_op != GGML_GLU_OP_SWIGLU) {\n+                                continue;\n+                            }\n+                            test_cases.emplace_back(new test_mul_mat_vec_fusion(type, glu_op, 1, 32, 256,\n+                                use_id, 16, 8, b, with_bias, with_gate));\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+    }\n+\n     for (bool with_norm : {false, true}) {\n         test_cases.emplace_back(new test_topk_moe({8, 22, 1, 1}, 4, with_norm));\n         test_cases.emplace_back(new test_topk_moe({32, 22, 1, 1}, 8, with_norm));"
      }
    ],
    "num_files": 11,
    "scraped_at": "2025-11-16T23:29:41.821496"
  },
  {
    "pr_number": 16700,
    "title": "ggml : fix interpolate with align-corners and ne=1",
    "body": "* Avoid division by zero if one of the spatial dimensions is 1\r\n* Not unlikely to run into this for vision models that support dynamic resolution input and do some downscale steps\r\n* CPU, CUDA, OpenCL returned correct results anyway due to clamp, but I think it's still better to not div-by-zero\r\n* Vulkan didn't clamp, so results were broken. I removed the specialization for align-corners rather than adding more complex pipeline selection and fallback behavior.",
    "html_url": "https://github.com/ggml-org/llama.cpp/pull/16700",
    "created_at": "2025-10-21T09:58:47Z",
    "merged_at": "2025-10-27T20:50:23Z",
    "merge_commit_sha": "10640e31aab0819f31c1e1f2d008b019ee737232",
    "base_ref": "master",
    "head_sha": "19864028f6d4b7811e527463c8c4352dc1005950",
    "user": "Acly",
    "files": [
      {
        "filename": "ggml/src/ggml-cpu/ops.cpp",
        "status": "modified",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "patch": "@@ -7519,8 +7519,8 @@ static void ggml_compute_forward_upscale_f32(\n         float pixel_offset = 0.5f;\n         if (mode_flags & GGML_SCALE_FLAG_ALIGN_CORNERS) {\n             pixel_offset = 0.0f;\n-            sf0 = (float)(ne0 - 1) / (src0->ne[0] - 1);\n-            sf1 = (float)(ne1 - 1) / (src0->ne[1] - 1);\n+            sf0 = ne0 > 1 && ne00 > 1 ? (float)(ne0 - 1) / (ne00 - 1) : sf0;\n+            sf1 = ne1 > 1 && ne01 > 1 ? (float)(ne1 - 1) / (ne01 - 1) : sf1;\n         }\n \n         for (int64_t i3 = 0; i3 < ne3; i3++) {"
      },
      {
        "filename": "ggml/src/ggml-cuda/upscale.cu",
        "status": "modified",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "patch": "@@ -126,8 +126,8 @@ void ggml_cuda_op_upscale(ggml_backend_cuda_context & ctx, ggml_tensor * dst) {\n     } else if (mode == GGML_SCALE_MODE_BILINEAR) {\n         float pixel_offset = 0.5f;\n         if (mode_flags & GGML_SCALE_FLAG_ALIGN_CORNERS) {\n-            sf0          = (float)(dst->ne[0] - 1) / (src0->ne[0] - 1);\n-            sf1          = (float)(dst->ne[1] - 1) / (src0->ne[1] - 1);\n+            sf0          = dst->ne[0] > 1 && src0->ne[0] > 1 ? (float)(dst->ne[0] - 1) / (src0->ne[0] - 1) : sf0;\n+            sf1          = dst->ne[1] > 1 && src0->ne[1] > 1 ? (float)(dst->ne[1] - 1) / (src0->ne[1] - 1) : sf1;\n             pixel_offset = 0.0f;\n         }\n         upscale_f32_bilinear_cuda(src0_d, dst_d, src0->nb[0], src0->nb[1], src0->nb[2], src0->nb[3],"
      },
      {
        "filename": "ggml/src/ggml-opencl/ggml-opencl.cpp",
        "status": "modified",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "patch": "@@ -6156,8 +6156,8 @@ static void ggml_cl_upscale(ggml_backend_t backend, const ggml_tensor * src0, gg\n         CL_CHECK(clSetKernelArg(kernel, 15, sizeof(float),    &sf3));\n     } else if (mode == GGML_SCALE_MODE_BILINEAR) {\n         if (mode_flags & GGML_SCALE_FLAG_ALIGN_CORNERS) {\n-            sf0 = (float)(ne0 - 1) / (ne00 - 1);\n-            sf1 = (float)(ne1 - 1) / (ne01 - 1);\n+            sf0 = ne0 > 1 && ne00 > 1 ? (float)(ne0 - 1) / (ne00 - 1) : sf0;\n+            sf1 = ne1 > 1 && ne01 > 1 ? (float)(ne1 - 1) / (ne01 - 1) : sf1;\n             pixel_offset = 0.0f;\n         }\n "
      },
      {
        "filename": "ggml/src/ggml-vulkan/ggml-vulkan.cpp",
        "status": "modified",
        "additions": 19,
        "deletions": 15,
        "changes": 34,
        "patch": "@@ -525,7 +525,7 @@ struct vk_device_struct {\n     vk_pipeline pipeline_add_id_f32;\n \n     vk_pipeline pipeline_concat_f32, pipeline_concat_f16, pipeline_concat_i32;\n-    vk_pipeline pipeline_upscale_nearest_f32, pipeline_upscale_bilinear_f32, pipeline_upscale_bilinear_ac_f32;\n+    vk_pipeline pipeline_upscale_nearest_f32, pipeline_upscale_bilinear_f32;\n     vk_pipeline pipeline_scale_f32;\n     vk_pipeline pipeline_sqr_f32;\n     vk_pipeline pipeline_sqrt_f32;\n@@ -1240,6 +1240,7 @@ struct vk_op_upscale_push_constants {\n     uint32_t nb00; uint32_t nb01; uint32_t nb02; uint32_t nb03;\n     uint32_t ne10; uint32_t ne11; uint32_t ne12; uint32_t ne13;\n     float sf0; float sf1; float sf2; float sf3;\n+    float pixel_offset;\n };\n \n struct vk_op_sum_rows_push_constants\n@@ -3498,7 +3499,6 @@ static void ggml_vk_load_shaders(vk_device& device) {\n \n     ggml_vk_create_pipeline(device, device->pipeline_upscale_nearest_f32, \"upscale_f32\", upscale_f32_len, upscale_f32_data, \"main\", 2, sizeof(vk_op_upscale_push_constants), {512, 1, 1}, {GGML_SCALE_MODE_NEAREST}, 1);\n     ggml_vk_create_pipeline(device, device->pipeline_upscale_bilinear_f32, \"upscale_f32\", upscale_f32_len, upscale_f32_data, \"main\", 2, sizeof(vk_op_upscale_push_constants), {512, 1, 1}, {GGML_SCALE_MODE_BILINEAR}, 1);\n-    ggml_vk_create_pipeline(device, device->pipeline_upscale_bilinear_ac_f32, \"upscale_f32\", upscale_f32_len, upscale_f32_data, \"main\", 2, sizeof(vk_op_upscale_push_constants), {512, 1, 1}, {GGML_SCALE_MODE_BILINEAR | GGML_SCALE_FLAG_ALIGN_CORNERS}, 1);\n \n     ggml_vk_create_pipeline(device, device->pipeline_scale_f32, \"scale_f32\", scale_f32_len, scale_f32_data, \"main\", 2, sizeof(vk_op_unary_push_constants), {512, 1, 1}, {}, 1);\n \n@@ -7855,14 +7855,14 @@ static vk_pipeline ggml_vk_op_get_pipeline(ggml_backend_vk_context * ctx, const\n         return nullptr;\n     case GGML_OP_UPSCALE:\n         if (src0->type == GGML_TYPE_F32 && dst->type == GGML_TYPE_F32) {\n-            int mode = ggml_get_op_params_i32(dst, 0);\n+            ggml_scale_mode mode = (ggml_scale_mode)(ggml_get_op_params_i32(dst, 0) & 0xFF);\n             switch (mode) {\n                 case GGML_SCALE_MODE_NEAREST:\n                     return ctx->device->pipeline_upscale_nearest_f32;\n                 case GGML_SCALE_MODE_BILINEAR:\n                     return ctx->device->pipeline_upscale_bilinear_f32;\n-                case GGML_SCALE_MODE_BILINEAR | GGML_SCALE_FLAG_ALIGN_CORNERS:\n-                    return ctx->device->pipeline_upscale_bilinear_ac_f32;\n+                default:\n+                    return nullptr;\n             }\n         }\n         return nullptr;\n@@ -9351,22 +9351,26 @@ static void ggml_vk_upscale(ggml_backend_vk_context * ctx, vk_context& subctx, c\n     const uint32_t src0_type_size = ggml_type_size(src0->type);\n     const uint32_t mode = (uint32_t)ggml_get_op_params_i32(dst, 0);\n \n-    float sf0 = (float)dst->ne[0] / src0->ne[0];\n-    float sf1 = (float)dst->ne[1] / src0->ne[1];\n-    float sf2 = (float)dst->ne[2] / src0->ne[2];\n-    float sf3 = (float)dst->ne[3] / src0->ne[3];\n+    GGML_TENSOR_UNARY_OP_LOCALS\n+\n+    float sf0 = (float)ne0 / ne00;\n+    float sf1 = (float)ne1 / ne01;\n+    float sf2 = (float)ne2 / ne02;\n+    float sf3 = (float)ne3 / ne03;\n+    float pixel_offset = 0.5f;\n \n     if (mode & GGML_SCALE_FLAG_ALIGN_CORNERS) {\n-        sf0 = (float)(dst->ne[0] - 1) / (src0->ne[0] - 1);\n-        sf1 = (float)(dst->ne[1] - 1) / (src0->ne[1] - 1);\n+        sf0 = ne0 > 1 && ne00 > 1 ? (float)(ne0 - 1) / (ne00 - 1) : sf0;\n+        sf1 = ne1 > 1 && ne01 > 1 ? (float)(ne1 - 1) / (ne01 - 1) : sf1;\n+        pixel_offset = 0.0f;\n     }\n \n     ggml_vk_op_f32<vk_op_upscale_push_constants>(ctx, subctx, src0, nullptr, nullptr, dst, GGML_OP_UPSCALE, {\n         (uint32_t)ggml_nelements(dst), 0, 0,\n-        (uint32_t)src0->ne[0], (uint32_t)src0->ne[1],\n-        (uint32_t)src0->nb[0] / src0_type_size, (uint32_t)src0->nb[1] / src0_type_size, (uint32_t)src0->nb[2] / src0_type_size, (uint32_t)src0->nb[3] / src0_type_size,\n-        (uint32_t)dst->ne[0], (uint32_t)dst->ne[1], (uint32_t)dst->ne[2],(uint32_t)dst->ne[3],\n-        sf0, sf1, sf2, sf3,\n+        (uint32_t)ne00, (uint32_t)ne01,\n+        (uint32_t)nb00 / src0_type_size, (uint32_t)nb01 / src0_type_size, (uint32_t)nb02 / src0_type_size, (uint32_t)nb03 / src0_type_size,\n+        (uint32_t)ne0, (uint32_t)ne1, (uint32_t)ne2, (uint32_t)ne3,\n+        sf0, sf1, sf2, sf3, pixel_offset\n     }, dryrun);\n }\n "
      },
      {
        "filename": "ggml/src/ggml-vulkan/vulkan-shaders/upscale.comp",
        "status": "modified",
        "additions": 2,
        "deletions": 15,
        "changes": 17,
        "patch": "@@ -7,6 +7,7 @@ layout (push_constant) uniform parameter\n     uint nb00; uint nb01; uint nb02; uint nb03;\n     uint ne10; uint ne11; uint ne12; uint ne13;\n     float sf0; float sf1; float sf2; float sf3;\n+    float pixel_offset;\n } p;\n \n #include \"types.glsl\"\n@@ -19,7 +20,6 @@ layout (binding = 1) writeonly buffer D {D_TYPE data_d[];};\n // from ggml.h: enum ggml_scale_mode, enum ggml_scale_flag\n #define NEAREST  0\n #define BILINEAR 1\n-#define ALIGN_CORNERS (1 << 8)\n \n layout (constant_id = 0) const uint scale_mode = 0;\n \n@@ -52,7 +52,7 @@ float fetch_bilinear(ivec2 c0, ivec2 c1, vec2 d, uint i12, uint i13) {\n float interpolate_bilinear(uint i10, uint i11, uint i12, uint i13) {\n     const ivec2 ne0 = ivec2(p.ne00, p.ne01);\n \n-    const vec2 c = (vec2(i10, i11) + 0.5) / vec2(p.sf0, p.sf1) - 0.5;\n+    const vec2 c = (vec2(i10, i11) + p.pixel_offset) / vec2(p.sf0, p.sf1) - p.pixel_offset;\n     const vec2 c0f = floor(c);\n     const vec2 d = c - c0f;\n     const ivec2 c0 = max(ivec2(c0f), 0);\n@@ -61,16 +61,6 @@ float interpolate_bilinear(uint i10, uint i11, uint i12, uint i13) {\n     return fetch_bilinear(c0, c1, d, i12, i13);\n }\n \n-float interpolate_bilinear_align_corners(uint i10, uint i11, uint i12, uint i13) {\n-    const vec2 c = vec2(i10, i11) / vec2(p.sf0, p.sf1);\n-    const vec2 c0f = floor(c);\n-    const vec2 d = c - c0f;\n-    const ivec2 c0 = ivec2(c0f);\n-    const ivec2 c1 = c0 + 1;\n-\n-    return fetch_bilinear(c0, c1, d, i12, i13);\n-}\n-\n void main() {\n     const uint idx = gl_GlobalInvocationID.z * 262144 + gl_GlobalInvocationID.y * 512 + gl_GlobalInvocationID.x;\n \n@@ -91,9 +81,6 @@ void main() {\n         case BILINEAR:\n             result = interpolate_bilinear(i10, i11, i12, i13);\n             break;\n-        case BILINEAR | ALIGN_CORNERS:\n-            result = interpolate_bilinear_align_corners(i10, i11, i12, i13);\n-            break;\n     }\n \n     data_d[p.d_offset + idx] = D_TYPE(result);"
      },
      {
        "filename": "tests/test-backend-ops.cpp",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -6877,6 +6877,8 @@ static std::vector<std::unique_ptr<test_case>> make_test_cases_eval() {\n         test_cases.emplace_back(new test_interpolate(GGML_TYPE_F32, {5, 7, 11, 13}, {2, 5,  7, 11}, mode));\n     }\n     test_cases.emplace_back(new test_interpolate(GGML_TYPE_F32, {2, 5,  7, 11}, {5, 7, 11, 13}, GGML_SCALE_MODE_BILINEAR | GGML_SCALE_FLAG_ALIGN_CORNERS));\n+    test_cases.emplace_back(new test_interpolate(GGML_TYPE_F32, {1, 4, 3, 2}, {2, 8, 3, 2}, GGML_SCALE_MODE_BILINEAR | GGML_SCALE_FLAG_ALIGN_CORNERS));\n+    test_cases.emplace_back(new test_interpolate(GGML_TYPE_F32, {4, 1, 3, 2}, {1, 1, 3, 2}, GGML_SCALE_MODE_BILINEAR | GGML_SCALE_FLAG_ALIGN_CORNERS));\n \n     test_cases.emplace_back(new test_sum());\n     test_cases.emplace_back(new test_sum_rows());"
      }
    ],
    "num_files": 6,
    "scraped_at": "2025-11-16T23:29:43.480902"
  },
  {
    "pr_number": 16665,
    "title": "sycl: add ROLL operation support",
    "body": "## Summary\r\nImplements the ROLL operator for the SYCL backend, enabling multi-dimensional tensor rolling on SYCL devices (Intel GPUs). Provides efficient circular shifts along tensor axes. The changes are focused and follow SYCL patterns.\r\n\r\n## Changes\r\n- Added ROLL kernel in `roll.cpp` with multi-axis support\r\n- ROLL dispatch in `ggml-sycl.cpp` \r\n- Integrated into the SYCL compute pipeline\r\n\r\n## Implementation\r\n- 4D tensor support: per-axis shifts with normalization\r\n- Parallel processing using `range<3>` following existing SYCL patterns  \r\n- Direct GPU memory access for optimal performance\r\n- Zero-shift optimization: fast memcpy path\r\n\r\n## Testing  \r\n- ROLL operations verified against CPU reference\r\n- Multi-axis scenarios tested with various shift combinations\r\n- In-place safety validated\r\n\r\n## Performance\r\n- GPU memory-optimized kernel design\r\n- Efficient modular arithmetic for shift normalization\r\n- Supports tensors up to 4D with configurable axis shifts\r\n- Zero-overhead for identity operations (all shifts = 0)\r\n\r\n## Compatibility\r\n- F32 tensors, OpenCL and Level Zero backends\r\n- Follows existing SYCL backend conventions",
    "html_url": "https://github.com/ggml-org/llama.cpp/pull/16665",
    "created_at": "2025-10-19T13:13:59Z",
    "merged_at": "2025-10-27T01:20:24Z",
    "merge_commit_sha": "2b9bd9bf4e759c05db629ec1c391dc8aeaa71887",
    "base_ref": "master",
    "head_sha": "b9793080f99b51fb6c6f19ae4aa6f10550eea91d",
    "user": "tamarPal",
    "files": [
      {
        "filename": "ggml/src/ggml-sycl/backend.hpp",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -32,6 +32,7 @@\n #include \"pad.hpp\"\n #include \"quantize.hpp\"\n #include \"quants.hpp\"\n+#include \"roll.hpp\"\n #include \"rope.hpp\"\n #include \"set_rows.hpp\"\n #include \"softmax.hpp\""
      },
      {
        "filename": "ggml/src/ggml-sycl/ggml-sycl.cpp",
        "status": "modified",
        "additions": 5,
        "deletions": 0,
        "changes": 5,
        "patch": "@@ -3836,6 +3836,9 @@ static bool ggml_sycl_compute_forward(ggml_backend_sycl_context & ctx, struct gg\n         case GGML_OP_GATED_LINEAR_ATTN:\n             ggml_sycl_op_gated_linear_attn(ctx, dst);\n             break;\n+        case GGML_OP_ROLL:\n+            ggml_sycl_roll(ctx, dst);\n+            break;\n         case GGML_OP_ARANGE:\n             ggml_sycl_arange(ctx, dst);\n             break;\n@@ -4491,6 +4494,8 @@ static bool ggml_backend_sycl_device_supports_op(ggml_backend_dev_t dev, const g\n         case GGML_OP_RWKV_WKV7:\n         case GGML_OP_GATED_LINEAR_ATTN:\n             return true;\n+        case GGML_OP_ROLL:\n+            return op->type == GGML_TYPE_F32;\n         case GGML_OP_ARANGE:\n             return op->type == GGML_TYPE_F32;\n         default:"
      },
      {
        "filename": "ggml/src/ggml-sycl/roll.cpp",
        "status": "added",
        "additions": 122,
        "deletions": 0,
        "changes": 122,
        "patch": "@@ -0,0 +1,122 @@\n+#include \"roll.hpp\"\n+#include \"common.hpp\"\n+\n+using namespace sycl;\n+\n+static inline int wrap_add(int i, int shift, int n) {\n+\n+    int s = i + shift;\n+    return (s >= n) ? (s - n) : s;\n+}\n+\n+static void kernel_roll_fused_i0_i1(\n+    queue &q,\n+    const float *src_d,\n+    float *dst_d,\n+    int ne0, int ne1, int ne2, int ne3,\n+    int sh0, int sh1, int sh2, int sh3)\n+{\n+    if (ne0 == 0 || ne1 == 0 || ne2 == 0 || ne3 == 0) return;\n+\n+\n+    const int stride1 = ne0;\n+    const int stride2 = ne0 * ne1;\n+    const int stride3 = ne0 * ne1 * ne2;\n+\n+\n+    const int shNe0 = (ne0 - sh0) % ne0;\n+    const int shNe1 = (ne1 - sh1) % ne1;\n+    const int shNe2 = (ne2 - sh2) % ne2;\n+    const int shNe3 = (ne3 - sh3) % ne3;\n+\n+\n+    const size_t g0 = (size_t) ne3;\n+    const size_t g1 = (size_t) ne2;\n+    const size_t g2 = (size_t) (ne1 * ne0);\n+\n+    const range<3> global{ g0, g1, g2 };\n+\n+    q.submit([&](handler &h) {\n+        h.parallel_for(global, [=](id<3> idx) {\n+            const int i3 = (int) idx[0];\n+            const int i2 = (int) idx[1];\n+\n+            const int fused = (int) idx[2];\n+            const int i1 = fused / ne0;\n+            const int i0 = fused - i1 * ne0;  // fused % ne0\n+\n+\n+            const int idx_dst = i0\n+                              + i1 * stride1\n+                              + i2 * stride2\n+                              + i3 * stride3;\n+\n+\n+            const int s0 = wrap_add(i0, shNe0, ne0);\n+            const int s1 = wrap_add(i1, shNe1, ne1);\n+            const int s2 = wrap_add(i2, shNe2, ne2);\n+            const int s3 = wrap_add(i3, shNe3, ne3);\n+\n+            const int idx_src = s0\n+                              + s1 * stride1\n+                              + s2 * stride2\n+                              + s3 * stride3;\n+\n+            dst_d[idx_dst] = src_d[idx_src];\n+        });\n+    });\n+}\n+\n+void ggml_sycl_roll(ggml_backend_sycl_context & ctx, ggml_tensor *dst) {\n+    GGML_ASSERT(dst->type == GGML_TYPE_F32);\n+\n+    const ggml_tensor *src = dst->src[0];\n+    GGML_ASSERT(src && src->type == GGML_TYPE_F32);\n+\n+    const int ne0 = (int) dst->ne[0];\n+    const int ne1 = (int) dst->ne[1];\n+    const int ne2 = (int) dst->ne[2];\n+    const int ne3 = (int) dst->ne[3];\n+\n+    const int32_t *params = (const int32_t *) dst->op_params;\n+    int shift0 = params[0];\n+    int shift1 = params[1];\n+    int shift2 = params[2];\n+    int shift3 = params[3];\n+\n+\n+    if ((shift0 | shift1 | shift2 | shift3) == 0) {\n+        const size_t nb = ggml_nbytes(src);\n+        queue *q = ctx.stream();\n+        SYCL_CHECK(CHECK_TRY_ERROR(q->memcpy(dst->data, src->data, nb)));\n+        return;\n+    }\n+\n+    auto norm = [](int sh, int n) -> int {\n+        if (n <= 0) return 0;\n+        sh %= n;\n+        if (sh < 0) sh += n;\n+        return sh;\n+    };\n+    shift0 = norm(shift0, ne0);\n+    shift1 = norm(shift1, ne1);\n+    shift2 = norm(shift2, ne2);\n+    shift3 = norm(shift3, ne3);\n+\n+    try {\n+        queue *q = ctx.stream();\n+\n+        const float *src_d = (const float *) src->data;\n+        float *dst_d = (float *) dst->data;\n+        GGML_ASSERT(src_d && dst_d);\n+\n+        kernel_roll_fused_i0_i1(\n+            *q, src_d, dst_d,\n+            ne0, ne1, ne2, ne3,\n+            shift0, shift1, shift2, shift3\n+        );\n+    } catch (const std::exception &e) {\n+        std::fprintf(stderr, \"[SYCL-ROLL] ERROR: %s\\n\", e.what());\n+        throw;\n+    }\n+}"
      },
      {
        "filename": "ggml/src/ggml-sycl/roll.hpp",
        "status": "added",
        "additions": 20,
        "deletions": 0,
        "changes": 20,
        "patch": "@@ -0,0 +1,20 @@\n+//\n+// MIT license\n+// Copyright (C) 2024 Intel Corporation\n+// SPDX-License-Identifier: MIT\n+//\n+\n+//\n+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n+// See https://llvm.org/LICENSE.txt for license information.\n+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n+//\n+\n+#ifndef GGML_SYCL_ROLL_HPP\n+#define GGML_SYCL_ROLL_HPP\n+\n+#include \"common.hpp\"\n+\n+void ggml_sycl_roll(ggml_backend_sycl_context & ctx, ggml_tensor *dst);\n+\n+#endif // GGML_SYCL_ROLL_HPP"
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T23:29:47.030482"
  },
  {
    "pr_number": 16649,
    "title": "CUDA: topk-moe: add optional parameter for gpt-oss",
    "body": "While looking at this kernel I realized that it is relatively easy to add it for gpt-oss, which does the softmax after the top-k.\r\n\r\nPerformance on a 4090:\r\n| Model                 | Test   |   t/s master |   t/s cuda_gpt_oss_opt |   Speedup |\r\n|:----------------------|:-------|-------------:|-----------------------:|----------:|\r\n| gpt-oss 20B MXFP4 MoE | tg32   |       170.99 |                 177.68 |      1.04 |\r\n| gpt-oss 20B MXFP4 MoE | tg64   |       168.75 |                 175.36 |      1.04 |\r\n| gpt-oss 20B MXFP4 MoE | tg128  |       167.01 |                 173.33 |      1.04 |",
    "html_url": "https://github.com/ggml-org/llama.cpp/pull/16649",
    "created_at": "2025-10-18T11:24:16Z",
    "merged_at": "2025-10-21T14:40:38Z",
    "merge_commit_sha": "03792ad93609fc67e41041c6347d9aa14e5e0d74",
    "base_ref": "master",
    "head_sha": "17c392796c3090697a01955c31b8ba8a13185f7e",
    "user": "am17an",
    "files": [
      {
        "filename": "ggml/src/ggml-cuda/ggml-cuda.cu",
        "status": "modified",
        "additions": 31,
        "deletions": 4,
        "changes": 35,
        "patch": "@@ -2818,8 +2818,12 @@ static bool ggml_cuda_can_fuse(const struct ggml_cgraph * cgraph, int node_idx,\n #endif\n \n     //TODO: remove special case once ggml_can_fuse can handle empty nodes\n-    std::initializer_list<enum ggml_op> topk_moe_ops           = ggml_cuda_topk_moe_ops(false);\n-    std::initializer_list<enum ggml_op> topk_moe_ops_with_norm = ggml_cuda_topk_moe_ops(true);\n+    std::initializer_list<enum ggml_op> topk_moe_ops =\n+        ggml_cuda_topk_moe_ops(/*with_norm*/ false, /*delayed_softmax=*/false);\n+    std::initializer_list<enum ggml_op> topk_moe_ops_with_norm =\n+        ggml_cuda_topk_moe_ops(/*with_norm=*/true, /*delayed_softmax=*/false);\n+    std::initializer_list<enum ggml_op> topk_moe_ops_delayed_softmax =\n+        ggml_cuda_topk_moe_ops(/*with_norm=*/false, /*delayed_softmax=*/true);\n \n     if (ops.size() == topk_moe_ops_with_norm.size() &&\n         ggml_can_fuse_subgraph(cgraph, node_idx, topk_moe_ops_with_norm, { node_idx + 3, node_idx + 8 })) {\n@@ -2840,6 +2844,16 @@ static bool ggml_cuda_can_fuse(const struct ggml_cgraph * cgraph, int node_idx,\n         }\n     }\n \n+    if (ops.size() == topk_moe_ops_delayed_softmax.size() &&\n+        ggml_can_fuse_subgraph(cgraph, node_idx, topk_moe_ops_delayed_softmax, { node_idx + 2, node_idx + 5 })) {\n+        ggml_tensor * softmax = cgraph->nodes[node_idx + 4];\n+        ggml_tensor * weights = cgraph->nodes[node_idx + 5];\n+\n+        if (ggml_cuda_should_use_topk_moe(softmax, weights)) {\n+            return true;\n+        }\n+    }\n+\n     if (!ggml_can_fuse(cgraph, node_idx, ops)) {\n         return false;\n     }\n@@ -2933,19 +2947,32 @@ static void evaluate_and_capture_cuda_graph(ggml_backend_cuda_context * cuda_ctx\n                     if (ggml_cuda_can_fuse(cgraph, i, ggml_cuda_topk_moe_ops(/*with norm*/ true), {})) {\n                         ggml_tensor * weights = cgraph->nodes[i+8];\n                         ggml_tensor * selected_experts = cgraph->nodes[i+3];\n-                        ggml_cuda_op_topk_moe(*cuda_ctx, node, weights, selected_experts, /*with norm*/ true);\n+                        ggml_cuda_op_topk_moe(*cuda_ctx, node->src[0], weights, selected_experts, /*with norm*/ true,\n+                                              /*delayed softmax*/ false);\n                         i += 8;\n                         continue;\n                     }\n \n                     if (ggml_cuda_can_fuse(cgraph, i, ggml_cuda_topk_moe_ops(/*with norm*/ false), {})) {\n                         ggml_tensor * weights = cgraph->nodes[i+4];\n                         ggml_tensor * selected_experts = cgraph->nodes[i+3];\n-                        ggml_cuda_op_topk_moe(*cuda_ctx, node, weights, selected_experts, /*with norm*/ false);\n+                        ggml_cuda_op_topk_moe(*cuda_ctx, node->src[0], weights, selected_experts, /*with norm*/ false,\n+                                              /*delayed softmax*/ false);\n                         i += 4;\n                         continue;\n                     }\n \n+                    if (ggml_cuda_can_fuse(cgraph, i,\n+                                           ggml_cuda_topk_moe_ops(/*with norm*/ false, /*delayed softmax*/ true), {})) {\n+                        ggml_tensor * weights = cgraph->nodes[i + 5];\n+                        ggml_tensor * ids     = cgraph->nodes[i + 1];\n+\n+                        ggml_cuda_op_topk_moe(*cuda_ctx, node->src[0], weights, ids, /*with norm*/ false,\n+                                              /*delayed_softmax*/ true);\n+                        i += 5;\n+                        continue;\n+                    }\n+\n                     if (node->op == GGML_OP_ADD) {\n                         int n_fuse = 0;\n                         ggml_op ops[8];"
      },
      {
        "filename": "ggml/src/ggml-cuda/topk-moe.cu",
        "status": "modified",
        "additions": 96,
        "deletions": 49,
        "changes": 145,
        "patch": "@@ -4,16 +4,61 @@\n \n #include <initializer_list>\n \n+// Warp-local softmax used for both the pre-top-k logits and the post-top-k delayed path.\n+template <int experts_per_thread, bool use_limit>\n+__device__ void softmax_warp_inplace(float (&vals)[experts_per_thread], const int limit, const int lane) {\n+    float max_val = -INFINITY;\n+\n+#pragma unroll\n+    for (int i = 0; i < experts_per_thread; i++) {\n+        const int  idx    = lane + i * WARP_SIZE;\n+        const bool active = !use_limit || (idx < limit);\n+        if (active) {\n+            max_val = max(max_val, vals[i]);\n+        }\n+    }\n+\n+    max_val = warp_reduce_max(max_val);\n+\n+    float sum = 0.f;\n+\n+#pragma unroll\n+    for (int i = 0; i < experts_per_thread; i++) {\n+        const int  idx    = lane + i * WARP_SIZE;\n+        const bool active = !use_limit || (idx < limit);\n+        if (active) {\n+            const float val = expf(vals[i] - max_val);\n+            vals[i]         = val;\n+            sum += val;\n+        } else {\n+            vals[i] = 0.f;\n+        }\n+    }\n+\n+    sum = warp_reduce_sum(sum);\n+\n+    const float inv_sum = 1.0f / sum;\n+\n+#pragma unroll\n+    for (int i = 0; i < experts_per_thread; i++) {\n+        const int  idx    = lane + i * WARP_SIZE;\n+        const bool active = !use_limit || (idx < limit);\n+        if (active) {\n+            vals[i] *= inv_sum;\n+        }\n+    }\n+}\n+\n /*\n     This kernel does the following:\n-    1. softmax over the logits per token [n_experts, n_tokens]\n+    1. optionally softmax over the logits per token [n_experts, n_tokens]\n     2. argmax reduce over the top-k (n_experts_used) logits\n     3. write weights + ids to global memory\n-    4. optionally normalize the weights\n+    4. optionally normalize the weights or apply softmax over the selected logits\n \n     It is intended as fusion of softmax->top-k->get_rows pipeline for MoE models\n */\n-template <int n_experts, bool with_norm>\n+template <int n_experts, bool with_norm, bool delayed_softmax = false>\n __launch_bounds__(4 * WARP_SIZE, 1) __global__ void topk_moe_cuda(const float * logits,\n                                                                   float *       weights,\n                                                                   int32_t *     ids,\n@@ -30,51 +75,31 @@ __launch_bounds__(4 * WARP_SIZE, 1) __global__ void topk_moe_cuda(const float *\n \n     constexpr int experts_per_thread = (n_experts > WARP_SIZE) ? n_experts / WARP_SIZE : 1;\n \n-    float logits_r[experts_per_thread];\n+    float wt[experts_per_thread];\n \n #pragma unroll\n     for (int i = 0; i < n_experts; i += WARP_SIZE) {\n-        const int expert        = i + threadIdx.x;\n-        logits_r[i / WARP_SIZE] = n_experts % WARP_SIZE == 0 || expert < n_experts ? logits[expert] : -INFINITY;\n+        const int expert  = i + threadIdx.x;\n+        wt[i / WARP_SIZE] = (n_experts % WARP_SIZE == 0 || expert < n_experts) ? logits[expert] : -INFINITY;\n     }\n \n-    float max_val = logits_r[0];\n-\n-#pragma unroll\n-    for (int i = 1; i < experts_per_thread; i++) {\n-        const float val = logits_r[i];\n-        max_val         = max(val, max_val);\n+    if constexpr (!delayed_softmax) {\n+        softmax_warp_inplace<experts_per_thread, false>(wt, n_experts, threadIdx.x);\n     }\n \n-    max_val = warp_reduce_max(max_val);\n-\n-    float wt[experts_per_thread];\n-    float tmp = 0.f;\n-\n-#pragma unroll\n-    for (int i = 0; i < experts_per_thread; i++) {\n-        const float val = logits_r[i];\n-        wt[i]           = expf(val - max_val);\n-        tmp += wt[i];\n-    }\n+    //at this point, each thread holds either a portion of the softmax distribution\n+    //or the raw logits. We do the argmax reduce over n_expert_used, each time marking\n+    //the expert weight as -inf to exclude from the next iteration\n \n-    tmp = warp_reduce_sum(tmp);\n+    float wt_sum = 0.f;\n \n-    const float inv_sum = 1.0f / tmp;\n+    float output_weights[experts_per_thread];\n \n #pragma unroll\n     for (int i = 0; i < experts_per_thread; i++) {\n-        wt[i] = wt[i] * inv_sum;\n+        output_weights[i] = 0.f;\n     }\n \n-    //at this point, each thread holds a portion of softmax,\n-    //we do the argmax reduce over n_expert_used, each time marking\n-    //the expert weight as -inf to exclude from the next iteration\n-\n-    float wt_sum = 0.f;\n-\n-    float output_weights[experts_per_thread];\n-\n     for (int k = 0; k < n_expert_used; k++) {\n         float max_val    = wt[0];\n         int   max_expert = threadIdx.x;\n@@ -121,6 +146,10 @@ __launch_bounds__(4 * WARP_SIZE, 1) __global__ void topk_moe_cuda(const float *\n         }\n     }\n \n+    if constexpr (delayed_softmax) {\n+        softmax_warp_inplace<experts_per_thread, true>(output_weights, n_expert_used, threadIdx.x);\n+    }\n+\n #pragma unroll\n     for (int i = 0; i < experts_per_thread; i++) {\n         const int idx = i * WARP_SIZE + threadIdx.x;\n@@ -130,58 +159,60 @@ __launch_bounds__(4 * WARP_SIZE, 1) __global__ void topk_moe_cuda(const float *\n     }\n }\n \n-template <bool with_norm>\n+template <bool with_norm, bool delayed_softmax = false>\n static void launch_topk_moe_cuda(ggml_backend_cuda_context & ctx,\n                                  const float *               logits,\n                                  float *                     weights,\n                                  int32_t *                   ids,\n                                  const int                   n_rows,\n                                  const int                   n_expert,\n                                  const int                   n_expert_used) {\n+    static_assert(!(with_norm && delayed_softmax), \"delayed softmax is not supported with weight normalization\");\n+\n     const int    rows_per_block = 4;\n     dim3         grid_dims((n_rows + rows_per_block - 1) / rows_per_block, 1, 1);\n     dim3         block_dims(WARP_SIZE, rows_per_block, 1);\n     cudaStream_t stream = ctx.stream();\n \n     switch (n_expert) {\n         case 1:\n-            topk_moe_cuda<1, with_norm>\n+            topk_moe_cuda<1, with_norm, delayed_softmax>\n                 <<<grid_dims, block_dims, 0, stream>>>(logits, weights, ids, n_rows, n_expert_used);\n             break;\n         case 2:\n-            topk_moe_cuda<2, with_norm>\n+            topk_moe_cuda<2, with_norm, delayed_softmax>\n                 <<<grid_dims, block_dims, 0, stream>>>(logits, weights, ids, n_rows, n_expert_used);\n             break;\n         case 4:\n-            topk_moe_cuda<4, with_norm>\n+            topk_moe_cuda<4, with_norm, delayed_softmax>\n                 <<<grid_dims, block_dims, 0, stream>>>(logits, weights, ids, n_rows, n_expert_used);\n             break;\n         case 8:\n-            topk_moe_cuda<8, with_norm>\n+            topk_moe_cuda<8, with_norm, delayed_softmax>\n                 <<<grid_dims, block_dims, 0, stream>>>(logits, weights, ids, n_rows, n_expert_used);\n             break;\n         case 16:\n-            topk_moe_cuda<16, with_norm>\n+            topk_moe_cuda<16, with_norm, delayed_softmax>\n                 <<<grid_dims, block_dims, 0, stream>>>(logits, weights, ids, n_rows, n_expert_used);\n             break;\n         case 32:\n-            topk_moe_cuda<32, with_norm>\n+            topk_moe_cuda<32, with_norm, delayed_softmax>\n                 <<<grid_dims, block_dims, 0, stream>>>(logits, weights, ids, n_rows, n_expert_used);\n             break;\n         case 64:\n-            topk_moe_cuda<64, with_norm>\n+            topk_moe_cuda<64, with_norm, delayed_softmax>\n                 <<<grid_dims, block_dims, 0, stream>>>(logits, weights, ids, n_rows, n_expert_used);\n             break;\n         case 128:\n-            topk_moe_cuda<128, with_norm>\n+            topk_moe_cuda<128, with_norm, delayed_softmax>\n                 <<<grid_dims, block_dims, 0, stream>>>(logits, weights, ids, n_rows, n_expert_used);\n             break;\n         case 256:\n-            topk_moe_cuda<256, with_norm>\n+            topk_moe_cuda<256, with_norm, delayed_softmax>\n                 <<<grid_dims, block_dims, 0, stream>>>(logits, weights, ids, n_rows, n_expert_used);\n             break;\n         case 512:\n-            topk_moe_cuda<512, with_norm>\n+            topk_moe_cuda<512, with_norm, delayed_softmax>\n                 <<<grid_dims, block_dims, 0, stream>>>(logits, weights, ids, n_rows, n_expert_used);\n             break;\n         default:\n@@ -194,15 +225,16 @@ void ggml_cuda_op_topk_moe(ggml_backend_cuda_context & ctx,\n                            const ggml_tensor *         logits,\n                            ggml_tensor *               weights,\n                            ggml_tensor *               ids,\n-                           const bool                  with_norm) {\n+                           const bool                  with_norm,\n+                           const bool                  delayed_softmax) {\n     GGML_ASSERT(logits->type == GGML_TYPE_F32);\n     GGML_ASSERT(weights->type == GGML_TYPE_F32);\n     GGML_ASSERT(ids->type == GGML_TYPE_I32);\n \n     const int n_experts = logits->ne[0];\n     const int n_rows    = logits->ne[1];\n \n-    const float * logits_d  = (const float *) logits->src[0]->data;\n+    const float * logits_d  = (const float *) logits->data;\n     float *       weights_d = (float *) weights->data;\n     int32_t *     ids_d     = (int32_t *) ids->data;\n \n@@ -213,7 +245,11 @@ void ggml_cuda_op_topk_moe(ggml_backend_cuda_context & ctx,\n     if (with_norm) {\n         launch_topk_moe_cuda<true>(ctx, logits_d, weights_d, ids_d, n_rows, n_experts, n_expert_used);\n     } else {\n-        launch_topk_moe_cuda<false>(ctx, logits_d, weights_d, ids_d, n_rows, n_experts, n_expert_used);\n+        if (delayed_softmax) {\n+            launch_topk_moe_cuda<false, true>(ctx, logits_d, weights_d, ids_d, n_rows, n_experts, n_expert_used);\n+        } else {\n+            launch_topk_moe_cuda<false, false>(ctx, logits_d, weights_d, ids_d, n_rows, n_experts, n_expert_used);\n+        }\n     }\n }\n \n@@ -246,16 +282,27 @@ bool ggml_cuda_should_use_topk_moe(const ggml_tensor * softmax, const ggml_tenso\n     return true;\n }\n \n-std::initializer_list<enum ggml_op> ggml_cuda_topk_moe_ops(bool norm) {\n+std::initializer_list<enum ggml_op> ggml_cuda_topk_moe_ops(bool norm, bool delayed_softmax) {\n     static std::initializer_list<enum ggml_op> norm_ops = { GGML_OP_SOFT_MAX, GGML_OP_RESHAPE,  GGML_OP_ARGSORT,\n                                                             GGML_OP_VIEW,     GGML_OP_GET_ROWS, GGML_OP_RESHAPE,\n                                                             GGML_OP_SUM_ROWS, GGML_OP_DIV,      GGML_OP_RESHAPE };\n \n     static std::initializer_list<enum ggml_op> no_norm_ops = { GGML_OP_SOFT_MAX, GGML_OP_RESHAPE, GGML_OP_ARGSORT,\n                                                                GGML_OP_VIEW, GGML_OP_GET_ROWS };\n \n+    static std::initializer_list<enum ggml_op> delayed_softmax_ops = { GGML_OP_ARGSORT,  GGML_OP_VIEW,\n+                                                                       GGML_OP_GET_ROWS, GGML_OP_RESHAPE,\n+                                                                       GGML_OP_SOFT_MAX, GGML_OP_RESHAPE };\n+\n+    GGML_ASSERT(!norm || !delayed_softmax);\n+\n+    if (delayed_softmax) {\n+        return delayed_softmax_ops;\n+    }\n+\n     if (norm) {\n         return norm_ops;\n     }\n+\n     return no_norm_ops;\n }"
      },
      {
        "filename": "ggml/src/ggml-cuda/topk-moe.cuh",
        "status": "modified",
        "additions": 4,
        "deletions": 3,
        "changes": 7,
        "patch": "@@ -6,9 +6,10 @@\n void ggml_cuda_op_topk_moe(ggml_backend_cuda_context & ctx,\n                            const ggml_tensor *         logits,\n                            ggml_tensor *               weights,\n-                           ggml_tensor *               top_k,\n-                           const bool                  with_norm);\n+                           ggml_tensor *               ids,\n+                           const bool                  with_norm,\n+                           const bool                  delayed_softmax = false);\n \n bool ggml_cuda_should_use_topk_moe(const ggml_tensor * softmax, const ggml_tensor * weights);\n \n-std::initializer_list<enum ggml_op> ggml_cuda_topk_moe_ops(bool with_norm);\n+std::initializer_list<enum ggml_op> ggml_cuda_topk_moe_ops(bool with_norm, bool delayed_softmax = false);"
      },
      {
        "filename": "tests/test-backend-ops.cpp",
        "status": "modified",
        "additions": 22,
        "deletions": 6,
        "changes": 28,
        "patch": "@@ -4669,14 +4669,21 @@ struct test_topk_moe: public test_case {\n     const std::array<int64_t, 4> ne;\n     const int n_expert_used;\n     const bool with_norm;\n-    test_topk_moe(std::array<int64_t, 4> ne = {10, 5, 1, 1}, int n_expert_used = 1, bool with_norm = false)\n-    : ne(ne), n_expert_used(n_expert_used), with_norm(with_norm) {\n+    const bool                   delayed_softmax;\n+\n+    test_topk_moe(std::array<int64_t, 4> ne              = { 10, 5, 1, 1 },\n+                  int                    n_expert_used   = 1,\n+                  bool                   with_norm       = false,\n+                  bool                   delayed_softmax = false) :\n+        ne(ne),\n+        n_expert_used(n_expert_used),\n+        with_norm(with_norm),\n+        delayed_softmax(delayed_softmax) {\n         GGML_ASSERT(n_expert_used <= ne[0]);\n+        GGML_ASSERT(!(with_norm && delayed_softmax));\n     }\n \n-    std::string vars() override {\n-        return VARS_TO_STR3(ne, n_expert_used, with_norm);\n-    }\n+    std::string vars() override { return VARS_TO_STR4(ne, n_expert_used, with_norm, delayed_softmax); }\n \n     std::string op_desc(ggml_tensor * t) override {\n         GGML_UNUSED(t);\n@@ -4690,11 +4697,17 @@ struct test_topk_moe: public test_case {\n         const int n_tokens = ne[1];\n \n         ggml_tensor * logits = ggml_new_tensor(ctx, GGML_TYPE_F32, 4, ne.data());\n-        ggml_tensor * probs  = ggml_soft_max(ctx, logits);\n+        ggml_tensor * probs            = delayed_softmax ? logits : ggml_soft_max(ctx, logits);\n         ggml_tensor * selected_experts = ggml_top_k(ctx, probs, n_expert_used); // [n_expert_used, n_tokens]\n \n         ggml_tensor * out = ggml_get_rows(ctx, ggml_reshape_3d(ctx, probs, 1, n_expert, n_tokens), selected_experts); // [1, n_expert_used, n_tokens]\n \n+        if (delayed_softmax) {\n+            out = ggml_reshape_2d(ctx, out, n_expert_used, n_tokens);\n+            out = ggml_soft_max(ctx, out);  // [n_expert_used, n_tokens]\n+            out = ggml_reshape_3d(ctx, out, 1, n_expert_used, n_tokens);\n+        }\n+\n         if (with_norm) {\n             out = ggml_reshape_2d(ctx, out, n_expert_used, n_tokens);\n             ggml_tensor * weights_sum = ggml_sum_rows(ctx, out); // [1, n_tokens]\n@@ -6975,6 +6988,9 @@ static std::vector<std::unique_ptr<test_case>> make_test_cases_eval() {\n         test_cases.emplace_back(new test_topk_moe({128, 1, 1, 1}, 128, with_norm));\n     }\n \n+    test_cases.emplace_back(new test_topk_moe({ 8, 22, 1, 1 }, 4, /*with_norm*/ false, /*delayed_softmax*/ true));\n+    test_cases.emplace_back(new test_topk_moe({ 32, 22, 1, 1 }, 8, /*with_norm*/ false, /*delayed_softmax*/ true));\n+\n #if 0\n     // these tests are disabled to save execution time, sbut they can be handy for debugging\n     test_cases.emplace_back(new test_llama(2, true));"
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T23:29:50.232954"
  },
  {
    "pr_number": 16641,
    "title": "vulkan: Implement topk_moe fused shader, ported from CUDA",
    "body": "This is similar to the CUDA shader from #16130, but doesn't use shared memory and handles different subgroup sizes.\r\n\r\n```\r\nbefore:\r\n\r\nZ:\\github\\jeffbolznv\\llama.cpp\\build\\bin\\RelWithDebInfo>llama-bench.exe -fa 1 -n 128 -p 0 -r 10 --prio 1 -m c:\\models\\Qwen_Qwen3-30B-A3B-Q2_K.gguf -m c:\\models\\\\deepseek-v2-lite-safetensors\\deepseek-v2-lite-Q4_K_M.gguf\r\nggml_vulkan: Found 1 Vulkan devices:\r\nggml_vulkan: 0 = NVIDIA GeForce RTX 5090 (NVIDIA) | uma: 0 | fp16: 1 | bf16: 1 | warp size: 32 | shared memory: 49152 | int dot: 1 | matrix cores: NV_coopmat2\r\n| model                          |       size |     params | backend    | ngl | fa |            test |                  t/s |\r\n| ------------------------------ | ---------: | ---------: | ---------- | --: | -: | --------------: | -------------------: |\r\n| qwen3moe 30B.A3B Q2_K - Medium |  10.15 GiB |    30.53 B | Vulkan     |  99 |  1 |           tg128 |       260.12 \u00b1 24.04 |\r\n| deepseek2 16B Q4_K - Medium    |   9.65 GiB |    15.71 B | Vulkan     |  99 |  1 |           tg128 |        328.18 \u00b1 9.83 |\r\n\r\nbuild: 66b0dbcb2 (6791)\r\n\r\nafter:\r\n\r\nZ:\\github\\jeffbolznv\\llama.cpp\\build\\bin\\RelWithDebInfo>llama-bench.exe -fa 1 -n 128 -p 0 -r 10 --prio 1 -m c:\\models\\Qwen_Qwen3-30B-A3B-Q2_K.gguf -m c:\\models\\\\deepseek-v2-lite-safetensors\\deepseek-v2-lite-Q4_K_M.gguf\r\nggml_vulkan: Found 1 Vulkan devices:\r\nggml_vulkan: 0 = NVIDIA GeForce RTX 5090 (NVIDIA) | uma: 0 | fp16: 1 | bf16: 1 | warp size: 32 | shared memory: 49152 | int dot: 1 | matrix cores: NV_coopmat2\r\n| model                          |       size |     params | backend    | ngl | fa |            test |                  t/s |\r\n| ------------------------------ | ---------: | ---------: | ---------- | --: | -: | --------------: | -------------------: |\r\n| qwen3moe 30B.A3B Q2_K - Medium |  10.15 GiB |    30.53 B | Vulkan     |  99 |  1 |           tg128 |        285.47 \u00b1 7.80 |\r\n| deepseek2 16B Q4_K - Medium    |   9.65 GiB |    15.71 B | Vulkan     |  99 |  1 |           tg128 |       339.16 \u00b1 16.07 |\r\n\r\nbuild: e0f7fa913 (6792)\r\n```",
    "html_url": "https://github.com/ggml-org/llama.cpp/pull/16641",
    "created_at": "2025-10-17T20:14:12Z",
    "merged_at": "2025-10-18T10:22:57Z",
    "merge_commit_sha": "e56abd2098dd2e2b0804691b93c13b48ae421627",
    "base_ref": "master",
    "head_sha": "e0f7fa913d4352d51ca3c26725fe8d284b75cbd0",
    "user": "jeffbolznv",
    "files": [
      {
        "filename": "ggml/src/ggml-impl.h",
        "status": "modified",
        "additions": 11,
        "deletions": 2,
        "changes": 13,
        "patch": "@@ -565,14 +565,23 @@ static inline ggml_bf16_t ggml_compute_fp32_to_bf16(float s) {\n #define GGML_FP32_TO_BF16(x) ggml_compute_fp32_to_bf16(x)\n #define GGML_BF16_TO_FP32(x) ggml_compute_bf16_to_fp32(x)\n \n+static inline int32_t ggml_node_get_use_count(const struct ggml_cgraph * cgraph, int node_idx) {\n+    const struct ggml_tensor * node = cgraph->nodes[node_idx];\n+\n+    size_t hash_pos = ggml_hash_find(&cgraph->visited_hash_set, node);\n+    if (!ggml_bitset_get(cgraph->visited_hash_set.used, hash_pos)) {\n+        return 0;\n+    }\n+    return cgraph->use_counts[hash_pos];\n+}\n+\n // return true if the node's results are only used by N other nodes\n // and can be fused into their calculations.\n static inline bool ggml_node_has_n_uses(const struct ggml_cgraph * cgraph, int node_idx, int32_t n_uses) {\n     const struct ggml_tensor * node = cgraph->nodes[node_idx];\n \n     // check the use count against how many we're replacing\n-    size_t hash_pos = ggml_hash_find(&cgraph->visited_hash_set, node);\n-    if (!ggml_bitset_get(cgraph->visited_hash_set.used, hash_pos) || cgraph->use_counts[hash_pos] != n_uses) {\n+    if (ggml_node_get_use_count(cgraph, node_idx) != n_uses) {\n         return false;\n     }\n "
      },
      {
        "filename": "ggml/src/ggml-vulkan/ggml-vulkan.cpp",
        "status": "modified",
        "additions": 260,
        "deletions": 6,
        "changes": 266,
        "patch": "@@ -385,6 +385,14 @@ enum shader_reduction_mode {\n \n static constexpr uint32_t num_argsort_pipelines = 11;\n static constexpr uint32_t max_argsort_cols = 1 << (num_argsort_pipelines-1);\n+static constexpr uint32_t num_topk_moe_pipelines = 10;\n+\n+static constexpr std::array topk_moe_norm{ GGML_OP_SOFT_MAX, GGML_OP_RESHAPE,  GGML_OP_ARGSORT,\n+                                           GGML_OP_VIEW,     GGML_OP_GET_ROWS, GGML_OP_RESHAPE,\n+                                           GGML_OP_SUM_ROWS, GGML_OP_DIV,      GGML_OP_RESHAPE };\n+static constexpr std::array topk_moe     { GGML_OP_SOFT_MAX, GGML_OP_RESHAPE,  GGML_OP_ARGSORT,\n+                                           GGML_OP_VIEW,     GGML_OP_GET_ROWS };\n+\n \n struct vk_device_struct {\n     std::recursive_mutex mutex;\n@@ -598,6 +606,9 @@ struct vk_device_struct {\n \n     vk_pipeline pipeline_flash_attn_split_k_reduce;\n \n+    // [2] is {!norm, norm}\n+    vk_pipeline pipeline_topk_moe[num_topk_moe_pipelines][2];\n+\n     std::vector<vk_pipeline_ref> all_pipelines;\n \n     std::vector<std::tuple<void*, size_t, vk_buffer>> pinned_memory;\n@@ -941,6 +952,11 @@ struct vk_op_multi_add_push_constants {\n static_assert(MAX_PARAMETER_COUNT == 12);\n static_assert(sizeof(vk_op_multi_add_push_constants) <= 256);\n \n+struct vk_op_topk_moe_push_constants {\n+    uint32_t n_rows;\n+    uint32_t n_expert_used;\n+};\n+\n struct vk_op_add_id_push_constants {\n     uint32_t ne0;\n     uint32_t ne1;\n@@ -3722,6 +3738,11 @@ static void ggml_vk_load_shaders(vk_device& device) {\n     ggml_vk_create_pipeline(device, device->pipeline_conv2d_dw_whcn_f16_f32, \"conv2d_dw_whcn_f16_f32\", conv2d_dw_whcn_f16_f32_len, conv2d_dw_whcn_f16_f32_data, \"main\", 3, sizeof(vk_op_conv2d_dw_push_constants), {512, 1, 1}, {}, 1);\n     ggml_vk_create_pipeline(device, device->pipeline_conv2d_dw_cwhn_f16_f32, \"conv2d_dw_cwhn_f16_f32\", conv2d_dw_cwhn_f16_f32_len, conv2d_dw_cwhn_f16_f32_data, \"main\", 3, sizeof(vk_op_conv2d_dw_push_constants), {512, 1, 1}, {}, 1);\n \n+    for (uint32_t i = 0; i < num_topk_moe_pipelines; ++i) {\n+        ggml_vk_create_pipeline2(device, device->pipeline_topk_moe[i][0], \"topk_moe_f32_\"+std::to_string(i),   topk_moe_f32_len, topk_moe_f32_data, \"main\", 3, sizeof(vk_op_topk_moe_push_constants), {1, 1, 1}, {device->subgroup_size, 1u<<i, 0}, 1, true, true);\n+        ggml_vk_create_pipeline2(device, device->pipeline_topk_moe[i][1], \"topk_moe_f32_\"+std::to_string(i),   topk_moe_f32_len, topk_moe_f32_data, \"main\", 3, sizeof(vk_op_topk_moe_push_constants), {1, 1, 1}, {device->subgroup_size, 1u<<i, 1}, 1, true, true);\n+    }\n+\n     for (auto &c : compiles) {\n         c.wait();\n     }\n@@ -8004,6 +8025,13 @@ static vk_pipeline ggml_vk_op_get_pipeline(ggml_backend_vk_context * ctx, const\n         GGML_ASSERT(!src1 || src1->type == GGML_TYPE_F32 || src1->type == GGML_TYPE_F16);\n         GGML_ASSERT(!src2 || src2->type == GGML_TYPE_F32);\n \n+        if (ctx->num_additional_fused_ops) {\n+            uint32_t idx = (uint32_t)ceilf(log2f(float(dst->ne[0])));\n+            GGML_ASSERT(idx < num_topk_moe_pipelines);\n+            bool with_norm = ctx->num_additional_fused_ops == topk_moe_norm.size() - 1;\n+            return ctx->device->pipeline_topk_moe[idx][with_norm];\n+        }\n+\n         if (src0->type == GGML_TYPE_F32 && (src1 == nullptr || src1->type == GGML_TYPE_F32) && dst->type == GGML_TYPE_F32) {\n             return src0->ne[0] > 1024 ? ctx->device->pipeline_soft_max_f32_wg512 : ctx->device->pipeline_soft_max_f32;\n         }\n@@ -9589,6 +9617,87 @@ static void ggml_vk_soft_max_back(ggml_backend_vk_context * ctx, vk_context& sub\n     ggml_vk_op_f32<vk_op_push_constants>(ctx, subctx, src0, src1, nullptr, dst, GGML_OP_SOFT_MAX_BACK, { (uint32_t)src0->ne[0], (uint32_t)ggml_nrows(src0), op_params[0], op_params[1] }, dryrun);\n }\n \n+static void ggml_vk_topk_moe(ggml_backend_vk_context * ctx, vk_context& subctx, ggml_cgraph * cgraph, int node_idx, bool dryrun = false) {\n+\n+    bool with_norm = ctx->num_additional_fused_ops == topk_moe_norm.size() - 1;\n+    ggml_tensor * logits = cgraph->nodes[node_idx + 0]->src[0];\n+    ggml_tensor * weights = with_norm ? cgraph->nodes[node_idx + 8] : cgraph->nodes[node_idx + 4];\n+    ggml_tensor * ids = cgraph->nodes[node_idx + 3];\n+\n+    GGML_ASSERT(logits->type == GGML_TYPE_F32);\n+    GGML_ASSERT(weights->type == GGML_TYPE_F32);\n+    GGML_ASSERT(ids->type == GGML_TYPE_I32);\n+\n+    const int n_experts = logits->ne[0];\n+    const int n_rows    = logits->ne[1];\n+    const int n_expert_used = weights->ne[1];\n+\n+    GGML_ASSERT(ids->nb[1] / ggml_type_size(ids->type) == (size_t) n_experts);\n+\n+    vk_pipeline pipeline = ggml_vk_op_get_pipeline(ctx, nullptr, nullptr, nullptr, cgraph->nodes[node_idx], GGML_OP_SOFT_MAX);\n+\n+    if (dryrun) {\n+        ggml_pipeline_request_descriptor_sets(ctx, pipeline, 1);\n+        return;\n+    }\n+\n+    ggml_backend_vk_buffer_context * logits_buf_ctx = (ggml_backend_vk_buffer_context *)logits->buffer->context;\n+    ggml_backend_vk_buffer_context * weights_buf_ctx = (ggml_backend_vk_buffer_context *)weights->buffer->context;\n+    ggml_backend_vk_buffer_context * ids_buf_ctx = (ggml_backend_vk_buffer_context *)ids->buffer->context;\n+\n+    vk_buffer d_logits = nullptr;\n+    size_t logits_buf_offset = 0;\n+    vk_buffer d_weights = nullptr;\n+    size_t weights_buf_offset = 0;\n+    vk_buffer d_ids = nullptr;\n+    size_t ids_buf_offset = 0;\n+\n+    bool logits_uma = false;\n+    bool weights_uma = false;\n+    bool ids_uma = false;\n+\n+    if (ctx->device->uma) {\n+        ggml_vk_host_get(ctx->device, logits->data, d_logits, logits_buf_offset);\n+        ggml_vk_host_get(ctx->device, weights->data, d_weights, weights_buf_offset);\n+        ggml_vk_host_get(ctx->device, ids->data, d_ids, ids_buf_offset);\n+        logits_uma = d_logits != nullptr;\n+        weights_uma = d_weights != nullptr;\n+        ids_uma = d_ids != nullptr;\n+    }\n+\n+    if (!logits_uma) {\n+        d_logits = logits_buf_ctx->dev_buffer;\n+        logits_buf_offset = vk_tensor_offset(logits) + logits->view_offs;\n+        GGML_ASSERT(d_logits != nullptr);\n+    }\n+    if (!weights_uma) {\n+        d_weights = weights_buf_ctx->dev_buffer;\n+        weights_buf_offset = vk_tensor_offset(weights) + weights->view_offs;\n+        GGML_ASSERT(d_weights != nullptr);\n+    }\n+    if (!ids_uma) {\n+        d_ids = ids_buf_ctx->dev_buffer;\n+        ids_buf_offset = vk_tensor_offset(ids) + ids->view_offs;\n+        GGML_ASSERT(d_ids != nullptr);\n+    }\n+\n+    vk_op_topk_moe_push_constants pc;\n+    pc.n_rows = n_rows;\n+    pc.n_expert_used = n_expert_used;\n+\n+    GGML_ASSERT(n_expert_used <= n_experts);\n+\n+    const uint32_t rows_per_block = 4;\n+    std::array<uint32_t, 3> elements = { CEIL_DIV(n_rows, rows_per_block), 1, 1 };\n+\n+    ggml_vk_dispatch_pipeline(ctx, subctx, pipeline,\n+        {\n+            ggml_vk_subbuffer(ctx, d_logits, logits_buf_offset),\n+            ggml_vk_subbuffer(ctx, d_weights, weights_buf_offset),\n+            ggml_vk_subbuffer(ctx, d_ids, ids_buf_offset),\n+        }, pc, elements);\n+}\n+\n static void ggml_vk_rope(ggml_backend_vk_context * ctx, vk_context& subctx, const ggml_tensor * src0, const ggml_tensor * src1, const ggml_tensor * src2, ggml_tensor * dst, bool backprop, bool dryrun = false) {\n     const int n_dims        = ((int32_t *) dst->op_params)[1];\n     const int mode          = ((int32_t *) dst->op_params)[2];\n@@ -11174,11 +11283,11 @@ static bool ggml_vk_build_graph(ggml_backend_vk_context * ctx, ggml_cgraph * cgr\n             ctx->unsynced_nodes_read.clear();\n             ggml_vk_sync_buffers(ctx, compute_ctx);\n         }\n-        // Add the last fused node and all fused source nodes to the unsynchronized list.\n-        const ggml_tensor * last_node = cgraph->nodes[node_idx + ctx->num_additional_fused_ops];\n-        ctx->unsynced_nodes_written.push_back(last_node);\n+        // Add all fused nodes to the unsynchronized lists.\n         for (int32_t i = 0; i < ctx->num_additional_fused_ops + 1; ++i) {\n             const ggml_tensor *cur_node = cgraph->nodes[node_idx + i];\n+            // Multiple outputs could be written, e.g. in topk_moe. Add them all to the list.\n+            ctx->unsynced_nodes_written.push_back(cur_node);\n             for (uint32_t j = 0; j < GGML_MAX_SRC; ++j) {\n                 if (!cur_node->src[j]) {\n                     continue;\n@@ -11345,7 +11454,11 @@ static bool ggml_vk_build_graph(ggml_backend_vk_context * ctx, ggml_cgraph * cgr\n \n         break;\n     case GGML_OP_SOFT_MAX:\n-        ggml_vk_soft_max(ctx, compute_ctx, src0, src1, src2, node, dryrun);\n+        if (ctx->num_additional_fused_ops) {\n+            ggml_vk_topk_moe(ctx, compute_ctx, cgraph, node_idx, dryrun);\n+        } else {\n+            ggml_vk_soft_max(ctx, compute_ctx, src0, src1, src2, node, dryrun);\n+        }\n \n         break;\n     case GGML_OP_SOFT_MAX_BACK:\n@@ -12141,6 +12254,120 @@ static bool ggml_vk_can_fuse(const struct ggml_cgraph * cgraph, int node_idx, st\n     return true;\n }\n \n+static bool ggml_vk_can_fuse_topk_moe(ggml_backend_vk_context * ctx, const struct ggml_cgraph * cgraph,\n+                                      int node_idx, bool with_norm) {\n+\n+    if (with_norm) {\n+        if (node_idx + (int)topk_moe_norm.size() > cgraph->n_nodes) {\n+            return false;\n+        }\n+        for (size_t i = 0; i < topk_moe_norm.size(); ++i) {\n+            if (cgraph->nodes[node_idx + i]->op != topk_moe_norm[i]) {\n+                return false;\n+            }\n+        }\n+    } else {\n+        if (node_idx + (int)topk_moe.size() > cgraph->n_nodes) {\n+            return false;\n+        }\n+        for (size_t i = 0; i < topk_moe.size(); ++i) {\n+            if (cgraph->nodes[node_idx + i]->op != topk_moe[i]) {\n+                return false;\n+            }\n+        }\n+    }\n+\n+    const ggml_tensor * softmax =  cgraph->nodes[node_idx + 0];\n+    const ggml_tensor * weights = with_norm ? cgraph->nodes[node_idx + 8] : cgraph->nodes[node_idx + 4];\n+\n+    const float * op_params = (const float *)softmax->op_params;\n+\n+    float scale = op_params[0];\n+    float max_bias = op_params[1];\n+\n+    if (!ggml_is_contiguous(softmax->src[0]) || !ggml_is_contiguous(weights)) {\n+        return false;\n+    }\n+\n+    if (scale != 1.0f || max_bias != 0.0f) {\n+        return false;\n+    }\n+\n+    // don't fuse when masks or sinks are present\n+    if (softmax->src[1] || softmax->src[2]) {\n+        return false;\n+    }\n+\n+    const int n_expert = softmax->ne[0];\n+    // n_expert must be a power of 2\n+    if (!is_pow2(n_expert) || n_expert > (1 << (num_topk_moe_pipelines-1))) {\n+        return false;\n+    }\n+\n+    // Check that the nodes don't have any unexpected uses\n+    const ggml_tensor * reshape1 =  cgraph->nodes[node_idx + 1];\n+    const ggml_tensor * argsort =   cgraph->nodes[node_idx + 2];\n+    const ggml_tensor * view =      cgraph->nodes[node_idx + 3];\n+    const ggml_tensor * get_rows =  cgraph->nodes[node_idx + 4];\n+    const ggml_tensor * reshape5 =  with_norm ? cgraph->nodes[node_idx + 5] : nullptr;\n+    const ggml_tensor * sum_rows =  with_norm ? cgraph->nodes[node_idx + 6] : nullptr;\n+    const ggml_tensor * div =       with_norm ? cgraph->nodes[node_idx + 7] : nullptr;\n+    const ggml_tensor * reshape8 =  with_norm ? cgraph->nodes[node_idx + 8] : nullptr;\n+\n+    // softmax is used by reshape and argsort\n+    if (ggml_node_get_use_count(cgraph, node_idx) != 2 ||\n+        reshape1->src[0] != softmax ||\n+        argsort->src[0] != softmax) {\n+        return false;\n+    }\n+    // reshape is used by get_rows\n+    if (ggml_node_get_use_count(cgraph, node_idx + 1) != 1 ||\n+        get_rows->src[0] != reshape1) {\n+        return false;\n+    }\n+    // argsort is used by view\n+    if (ggml_node_get_use_count(cgraph, node_idx + 2) != 1 ||\n+        view->src[0] != argsort) {\n+        return false;\n+    }\n+    // view is written (via argsort), we can skip checking it\n+\n+    if (with_norm) {\n+        // get_rows is used by reshape\n+        if (ggml_node_get_use_count(cgraph, node_idx + 4) != 1 ||\n+            reshape5->src[0] != get_rows) {\n+            return false;\n+        }\n+\n+        // reshape is used by sum_rows and div\n+        if (ggml_node_get_use_count(cgraph, node_idx + 5) != 2 ||\n+            sum_rows->src[0] != reshape5 ||\n+            div->src[0] != reshape5) {\n+            return false;\n+        }\n+\n+        // sum_rows is used by div\n+        if (ggml_node_get_use_count(cgraph, node_idx + 6) != 1 ||\n+            div->src[1] != sum_rows) {\n+            return false;\n+        }\n+\n+        // div/reshape are written\n+        if (reshape8->src[0] != div) {\n+            return false;\n+        }\n+    }\n+\n+    if (!ctx->device->subgroup_arithmetic ||\n+        !ctx->device->subgroup_shuffle ||\n+        !ctx->device->subgroup_require_full_support ||\n+        ctx->device->disable_fusion) {\n+        return false;\n+    }\n+\n+    return true;\n+}\n+\n static uint32_t ggml_vk_fuse_multi_add(ggml_backend_vk_context * ctx, const struct ggml_cgraph * cgraph, int node_idx) {\n \n     const ggml_tensor *first_node = cgraph->nodes[node_idx];\n@@ -12216,6 +12443,10 @@ static ggml_status ggml_backend_vk_graph_compute(ggml_backend_t backend, ggml_cg\n                 ctx->num_additional_fused_ops = num_adds - 1;\n             } else if (ggml_vk_can_fuse(cgraph, i, { GGML_OP_RMS_NORM, GGML_OP_MUL })) {\n                 ctx->num_additional_fused_ops = 1;\n+            } else if (ggml_vk_can_fuse_topk_moe(ctx, cgraph, i, true)) {\n+                ctx->num_additional_fused_ops = topk_moe_norm.size() - 1;\n+            } else if (ggml_vk_can_fuse_topk_moe(ctx, cgraph, i, false)) {\n+                ctx->num_additional_fused_ops = topk_moe.size() - 1;\n             }\n         }\n         ggml_vk_build_graph(ctx, cgraph, i, nullptr, 0, true, false, false, false);\n@@ -12313,17 +12544,21 @@ static ggml_status ggml_backend_vk_graph_compute(ggml_backend_t backend, ggml_cg\n                 ctx->num_additional_fused_ops = num_adds - 1;\n             } else if (ggml_vk_can_fuse(cgraph, i, { GGML_OP_RMS_NORM, GGML_OP_MUL })) {\n                 ctx->num_additional_fused_ops = 1;\n+            } else if (ggml_vk_can_fuse_topk_moe(ctx, cgraph, i, true)) {\n+                ctx->num_additional_fused_ops = topk_moe_norm.size() - 1;\n+            } else if (ggml_vk_can_fuse_topk_moe(ctx, cgraph, i, false)) {\n+                ctx->num_additional_fused_ops = topk_moe.size() - 1;\n             }\n         }\n \n         // Signal the almost_ready fence when the graph is mostly complete (< 20% remaining)\n         bool almost_ready = (cgraph->n_nodes - i) < cgraph->n_nodes / 5;\n         bool submit = (submitted_nodes >= nodes_per_submit) ||\n                       (mul_mat_bytes >= mul_mat_bytes_per_submit) ||\n-                      (i + ctx->num_additional_fused_ops == last_node) ||\n+                      (i + ctx->num_additional_fused_ops >= last_node) ||\n                       (almost_ready && !ctx->almost_ready_fence_pending);\n \n-        bool enqueued = ggml_vk_build_graph(ctx, cgraph, i, cgraph->nodes[submit_node_idx], submit_node_idx, false, i + ctx->num_additional_fused_ops == last_node, almost_ready, submit);\n+        bool enqueued = ggml_vk_build_graph(ctx, cgraph, i, cgraph->nodes[submit_node_idx], submit_node_idx, false, i + ctx->num_additional_fused_ops >= last_node, almost_ready, submit);\n \n         if (vk_perf_logger_enabled) {\n             if (ctx->compute_ctx.expired()) {\n@@ -12444,6 +12679,25 @@ static void ggml_vk_graph_optimize(ggml_backend_t backend, struct ggml_cgraph *\n     while (first_unused < graph->n_nodes) {\n         std::vector<int> current_set;\n \n+        // Avoid reordering topk_moe_norm\n+        if (first_unused + (int)topk_moe_norm.size() <= graph->n_nodes) {\n+            bool is_topk_moe_norm = true;\n+            for (size_t j = 0; j < topk_moe_norm.size(); ++j) {\n+                if (graph->nodes[first_unused + j]->op != topk_moe_norm[j] || used[first_unused + j]) {\n+                    is_topk_moe_norm = false;\n+                }\n+            }\n+            if (is_topk_moe_norm) {\n+                for (size_t j = 0; j < topk_moe_norm.size(); ++j) {\n+                    new_order.push_back(graph->nodes[first_unused + j]);\n+                    used[first_unused + j] = true;\n+                }\n+                while (first_unused < graph->n_nodes && used[first_unused]) {\n+                    first_unused++;\n+                }\n+                continue;\n+            }\n+        }\n         // First, grab the next unused node.\n         current_set.push_back(first_unused);\n "
      },
      {
        "filename": "ggml/src/ggml-vulkan/vulkan-shaders/topk_moe.comp",
        "status": "added",
        "additions": 139,
        "deletions": 0,
        "changes": 139,
        "patch": "@@ -0,0 +1,139 @@\n+#version 450\n+\n+#extension GL_EXT_control_flow_attributes : require\n+#extension GL_KHR_shader_subgroup_basic : enable\n+#extension GL_KHR_shader_subgroup_arithmetic : enable\n+#extension GL_KHR_shader_subgroup_shuffle : enable\n+\n+#include \"types.glsl\"\n+\n+layout (push_constant) uniform parameter\n+{\n+    uint n_rows;\n+    uint n_expert_used;\n+};\n+\n+layout(local_size_x_id = 0, local_size_y = 4, local_size_z = 1) in;\n+\n+layout(constant_id = 0) const uint WARP_SIZE = 32;\n+layout(constant_id = 1) const uint n_experts = 512;\n+layout(constant_id = 2) const bool with_norm = true;\n+\n+const uint experts_per_thread = (n_experts > WARP_SIZE) ? n_experts / WARP_SIZE : 1;\n+\n+layout (binding = 0, std430) readonly buffer Logits {float logits[];};\n+layout (binding = 1, std430) writeonly buffer Weights {float weights[];};\n+layout (binding = 2, std430) writeonly buffer Ids {uint ids[];};\n+\n+void main() {\n+    const uint row = gl_WorkGroupID.x * gl_WorkGroupSize.y + gl_LocalInvocationID.y;\n+    if (row >= n_rows) {\n+        return;\n+    }\n+\n+    const uint logits_offset = n_experts * row;\n+    const uint weights_offset = n_expert_used * row;\n+    const uint ids_offset = n_experts * row;\n+\n+    float logits_r[experts_per_thread];\n+\n+    const float INFINITY = 1.0 / 0.0;\n+\n+    [[unroll]]\n+    for (uint i = 0; i < n_experts; i += WARP_SIZE) {\n+        const uint expert        = i + gl_LocalInvocationID.x;\n+        logits_r[i / WARP_SIZE] = n_experts % WARP_SIZE == 0 || expert < n_experts ? logits[logits_offset + expert] : -INFINITY;\n+    }\n+\n+    float max_val = logits_r[0];\n+\n+    [[unroll]]\n+    for (int i = 1; i < experts_per_thread; i++) {\n+        const float val = logits_r[i];\n+        max_val         = max(val, max_val);\n+    }\n+\n+    max_val = subgroupMax(max_val);\n+\n+    float wt[experts_per_thread];\n+    float tmp = 0.f;\n+\n+    [[unroll]]\n+    for (int i = 0; i < experts_per_thread; i++) {\n+        const float val = logits_r[i];\n+        wt[i]           = exp(val - max_val);\n+        tmp += wt[i];\n+    }\n+\n+    tmp = subgroupAdd(tmp);\n+\n+    const float inv_sum = 1.0f / tmp;\n+\n+    [[unroll]]\n+    for (int i = 0; i < experts_per_thread; i++) {\n+        wt[i] = wt[i] * inv_sum;\n+    }\n+\n+    // at this point, each thread holds a portion of softmax,\n+    // we do the argmax reduce over n_expert_used, each time marking\n+    // the expert weight as -inf to exclude from the next iteration\n+\n+    float wt_sum = 0.f;\n+\n+    float output_weights[experts_per_thread];\n+\n+    for (int k = 0; k < n_expert_used; k++) {\n+        float max_val    = wt[0];\n+        uint   max_expert = gl_LocalInvocationID.x;\n+\n+        [[unroll]]\n+        for (int i = 1; i < experts_per_thread; i++) {\n+            const uint expert = gl_LocalInvocationID.x + i * WARP_SIZE;\n+            if ((n_experts % WARP_SIZE == 0 || expert < n_experts) && wt[i] > max_val) {\n+                max_val    = wt[i];\n+                max_expert = expert;\n+            }\n+        }\n+\n+        [[unroll]]\n+        for (uint mask = WARP_SIZE / 2; mask > 0; mask /= 2) {\n+            const float val    = subgroupShuffleXor(max_val, mask);\n+            const uint  expert = subgroupShuffleXor(max_expert, mask);\n+            if (val > max_val || (val == max_val && expert < max_expert)) {\n+                max_val    = val;\n+                max_expert = expert;\n+            }\n+        }\n+\n+        if ((k & (WARP_SIZE - 1)) == gl_LocalInvocationID.x) {\n+            output_weights[k / WARP_SIZE] = max_val;\n+        }\n+\n+        if ((max_expert & (WARP_SIZE - 1)) == gl_LocalInvocationID.x) {\n+            wt[max_expert / WARP_SIZE] = -INFINITY;\n+\n+            ids[ids_offset + k] = max_expert;\n+            if (with_norm) {\n+                wt_sum += max_val;\n+            }\n+        }\n+    }\n+\n+    if (with_norm) {\n+        wt_sum              = subgroupAdd(wt_sum);\n+        const float inv_sum = 1.0f / wt_sum;\n+\n+        [[unroll]]\n+        for (uint i = 0; i < experts_per_thread; ++i) {\n+            output_weights[i] *= inv_sum;\n+        }\n+    }\n+\n+    [[unroll]]\n+    for (uint i = 0; i < experts_per_thread; ++i) {\n+        uint idx = i * WARP_SIZE + gl_LocalInvocationID.x;\n+        if (idx < n_expert_used) {\n+            weights[weights_offset + idx] = output_weights[i];\n+        }\n+    }\n+}"
      },
      {
        "filename": "ggml/src/ggml-vulkan/vulkan-shaders/vulkan-shaders-gen.cpp",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -920,6 +920,8 @@ void process_shaders() {\n \n     string_to_spv(\"ssm_conv_f32\", \"ssm_conv.comp\", {{\"A_TYPE\", \"float\"}});\n \n+    string_to_spv(\"topk_moe_f32\", \"topk_moe.comp\", {});\n+\n     for (auto &c : compiles) {\n         c.wait();\n     }"
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T23:29:52.352949"
  },
  {
    "pr_number": 16634,
    "title": "metal : initial Metal4 tensor API support",
    "body": "- Rework matrix-matrix multiplication\r\n- Use Tensor API when available\r\n\r\nTODOs\r\n\r\n- [x] Update `mul_mm_id` kernel\r\n- [x] Test on M5 (looking for volunteers to test as I won't have hardware anytime soon)\r\n- [x] How to handle missing `bfloat` tensor API? https://github.com/ggml-org/llama.cpp/pull/16634#issuecomment-3443085619\r\n- [x] Confirm that using the Tensor API maintains the existing performance without using it on M4 and earlier",
    "html_url": "https://github.com/ggml-org/llama.cpp/pull/16634",
    "created_at": "2025-10-17T13:49:53Z",
    "merged_at": "2025-11-06T12:45:11Z",
    "merge_commit_sha": "5b180c3d60f3df61cd9955bc5c69e64537958f92",
    "base_ref": "master",
    "head_sha": "3d9a497a8b02fba6e84442b3fd40eec4698b4c89",
    "user": "ggerganov",
    "files": [
      {
        "filename": "ggml/src/ggml-metal/ggml-metal-context.m",
        "status": "modified",
        "additions": 1,
        "deletions": 4,
        "changes": 5,
        "patch": "@@ -35,7 +35,6 @@\n     // additional, inference-time compiled pipelines\n     ggml_metal_pipelines_t pipelines_ext;\n \n-    bool use_bfloat;\n     bool use_fusion;\n     bool use_concurrency;\n     bool use_graph_optimize;\n@@ -121,11 +120,10 @@ ggml_metal_t ggml_metal_init(ggml_metal_device_t dev) {\n         }\n     }\n \n-    const struct ggml_metal_device_props * props_dev = ggml_metal_device_get_props(dev);\n+    //const struct ggml_metal_device_props * props_dev = ggml_metal_device_get_props(dev);\n \n     res->d_queue = dispatch_queue_create(\"ggml-metal\", DISPATCH_QUEUE_CONCURRENT);\n \n-    res->use_bfloat      = props_dev->has_bfloat;\n     res->use_fusion      = getenv(\"GGML_METAL_FUSION_DISABLE\") == nil;\n     res->use_concurrency = getenv(\"GGML_METAL_CONCURRENCY_DISABLE\") == nil;\n \n@@ -147,7 +145,6 @@ ggml_metal_t ggml_metal_init(ggml_metal_device_t dev) {\n \n     memset(res->fuse_cnt, 0, sizeof(res->fuse_cnt));\n \n-    GGML_LOG_INFO(\"%s: use bfloat         = %s\\n\", __func__, res->use_bfloat         ? \"true\" : \"false\");\n     GGML_LOG_INFO(\"%s: use fusion         = %s\\n\", __func__, res->use_fusion         ? \"true\" : \"false\");\n     GGML_LOG_INFO(\"%s: use concurrency    = %s\\n\", __func__, res->use_concurrency    ? \"true\" : \"false\");\n     GGML_LOG_INFO(\"%s: use graph optimize = %s\\n\", __func__, res->use_graph_optimize ? \"true\" : \"false\");"
      },
      {
        "filename": "ggml/src/ggml-metal/ggml-metal-device.h",
        "status": "modified",
        "additions": 4,
        "deletions": 1,
        "changes": 5,
        "patch": "@@ -95,7 +95,9 @@ void ggml_metal_encoder_end_encoding(ggml_metal_encoder_t encoder);\n \n typedef struct ggml_metal_library * ggml_metal_library_t;\n \n-ggml_metal_library_t ggml_metal_library_init(ggml_metal_device_t dev);\n+ggml_metal_library_t ggml_metal_library_init            (ggml_metal_device_t dev);\n+ggml_metal_library_t ggml_metal_library_init_from_source(ggml_metal_device_t dev, const char * source, bool verbose);\n+\n void ggml_metal_library_free(ggml_metal_library_t lib);\n \n ggml_metal_pipeline_t ggml_metal_library_get_pipeline    (ggml_metal_library_t lib, const char * name);\n@@ -193,6 +195,7 @@ struct ggml_metal_device_props {\n     bool has_simdgroup_mm;\n     bool has_unified_memory;\n     bool has_bfloat;\n+    bool has_tensor;\n     bool use_residency_sets;\n     bool use_shared_buffers;\n "
      },
      {
        "filename": "ggml/src/ggml-metal/ggml-metal-device.m",
        "status": "modified",
        "additions": 205,
        "deletions": 6,
        "changes": 211,
        "patch": "@@ -21,8 +21,9 @@\n #define GGML_METAL_HAS_RESIDENCY_SETS 1\n #endif\n \n-// overload of MTLGPUFamilyMetal3 (not available in some environments)\n+// overload of MTLGPUFamilyMetalX (not available in some environments)\n static const NSInteger MTLGPUFamilyMetal3_GGML = 5001;\n+static const NSInteger MTLGPUFamilyMetal4_GGML = 5002;\n \n // virtual address for GPU memory allocations\n static atomic_uintptr_t g_addr_device = 0x000000400ULL;\n@@ -261,6 +262,10 @@ ggml_metal_library_t ggml_metal_library_init(ggml_metal_device_t dev) {\n                     [prep setObject:@\"1\" forKey:@\"GGML_METAL_HAS_BF16\"];\n                 }\n \n+                if (ggml_metal_device_get_props(dev)->has_tensor) {\n+                    [prep setObject:@\"1\" forKey:@\"GGML_METAL_HAS_TENSOR\"];\n+                }\n+\n #if GGML_METAL_EMBED_LIBRARY\n                 [prep setObject:@\"1\" forKey:@\"GGML_METAL_EMBED_LIBRARY\"];\n #endif\n@@ -298,6 +303,72 @@ ggml_metal_library_t ggml_metal_library_init(ggml_metal_device_t dev) {\n     return res;\n }\n \n+ggml_metal_library_t ggml_metal_library_init_from_source(ggml_metal_device_t dev, const char * source, bool verbose) {\n+    if (source == NULL) {\n+        GGML_LOG_ERROR(\"%s: source is NULL\\n\", __func__);\n+        return NULL;\n+    }\n+\n+    id<MTLDevice> device = ggml_metal_device_get_obj(dev);\n+    id<MTLLibrary> library = nil;\n+    NSError * error = nil;\n+\n+    const int64_t t_start = ggml_time_us();\n+\n+    NSString * src = [[NSString alloc] initWithBytes:source\n+                                              length:strlen(source)\n+                                            encoding:NSUTF8StringEncoding];\n+    if (!src) {\n+        GGML_LOG_ERROR(\"%s: failed to create NSString from source\\n\", __func__);\n+        return NULL;\n+    }\n+\n+    @autoreleasepool {\n+        NSMutableDictionary * prep = [NSMutableDictionary dictionary];\n+\n+        MTLCompileOptions * options = [MTLCompileOptions new];\n+        options.preprocessorMacros = prep;\n+\n+        library = [device newLibraryWithSource:src options:options error:&error];\n+        if (error) {\n+            if (verbose) {\n+                GGML_LOG_ERROR(\"%s: error compiling source: %s\\n\", __func__, [[error description] UTF8String]);\n+            } else {\n+                GGML_LOG_ERROR(\"%s: error compiling source\\n\", __func__);\n+            }\n+            library = nil;\n+        }\n+\n+        [options release];\n+    }\n+\n+    [src release];\n+\n+    if (!library) {\n+        if (verbose) {\n+            GGML_LOG_ERROR(\"%s: failed to create Metal library from source\\n\", __func__);\n+        }\n+\n+        return NULL;\n+    }\n+\n+    if (verbose) {\n+        GGML_LOG_INFO(\"%s: compiled in %.3f sec\\n\", __func__, (ggml_time_us() - t_start) / 1e6);\n+    }\n+\n+    ggml_metal_library_t res = calloc(1, sizeof(struct ggml_metal_library));\n+    if (!res) {\n+        GGML_LOG_ERROR(\"%s: calloc failed\\n\", __func__);\n+        return NULL;\n+    }\n+\n+    res->obj       = library;\n+    res->device    = device;\n+    res->pipelines = ggml_metal_pipelines_init();\n+\n+    return res;\n+}\n+\n void ggml_metal_library_free(ggml_metal_library_t lib) {\n     if (!lib) {\n         return;\n@@ -345,23 +416,31 @@ ggml_metal_pipeline_t ggml_metal_library_compile_pipeline(ggml_metal_library_t l\n         if (!mtl_function) {\n             ggml_critical_section_end();\n \n-            GGML_LOG_ERROR(\"%s: error: failed to compile pipeline: base = '%s', name = '%s'\\n\", __func__, base, name);\n+            GGML_LOG_ERROR(\"%s: failed to compile pipeline: base = '%s', name = '%s'\\n\", __func__, base, name);\n             if (error) {\n-                GGML_LOG_ERROR(\"%s: error: %s\\n\", __func__, [[error description] UTF8String]);\n+                GGML_LOG_ERROR(\"%s: %s\\n\", __func__, [[error description] UTF8String]);\n             }\n \n             return nil;\n         }\n \n         res->obj = [lib->device newComputePipelineStateWithFunction:mtl_function error:&error];\n \n-        ggml_metal_pipelines_add(lib->pipelines, name, res);\n-\n         [mtl_function release];\n \n         GGML_LOG_DEBUG(\"%s: loaded %-40s %16p | th_max = %4d | th_width = %4d\\n\", __func__, name, (void *) res->obj,\n                 (int) res->obj.maxTotalThreadsPerThreadgroup,\n                 (int) res->obj.threadExecutionWidth);\n+\n+        if (res->obj.maxTotalThreadsPerThreadgroup == 0 || res->obj.threadExecutionWidth == 0) {\n+            ggml_critical_section_end();\n+\n+            GGML_LOG_ERROR(\"%s: incompatible pipeline %s\\n\", __func__, name);\n+\n+            return nil;\n+        }\n+\n+        ggml_metal_pipelines_add(lib->pipelines, name, res);\n     }\n \n     ggml_critical_section_end();\n@@ -469,14 +548,133 @@ ggml_metal_device_t ggml_metal_device_init(void) {\n \n             dev->props.has_bfloat  = [dev->mtl_device supportsFamily:MTLGPUFamilyMetal3_GGML];\n             dev->props.has_bfloat |= [dev->mtl_device supportsFamily:MTLGPUFamilyApple6];\n+            if (getenv(\"GGML_METAL_BF16_DISABLE\") != NULL) {\n+                dev->props.has_bfloat = false;\n+            }\n+\n+            dev->props.has_tensor = [dev->mtl_device supportsFamily:MTLGPUFamilyMetal4_GGML];\n+            if (getenv(\"GGML_METAL_TENSOR_DISABLE\") != NULL) {\n+                dev->props.has_tensor = false;\n+            }\n+\n+            // note: disable the tensor API by default for old chips because with the current implementation it is not useful\n+            // - M2 Ultra:   ~5% slower\n+            // - M4, M4 Max: no significant difference\n+            //\n+            // TODO: try to update the tensor API kernels to at least match the simdgroup performance\n+            if (getenv(\"GGML_METAL_TENSOR_ENABLE\") == NULL &&\n+                ![[dev->mtl_device name] containsString:@\"M5\"] &&\n+                ![[dev->mtl_device name] containsString:@\"M6\"]) {\n+                GGML_LOG_WARN(\"%s: tensor API disabled for pre-M5 device\\n\", __func__);\n+                dev->props.has_tensor = false;\n+            }\n+\n+            // double-check that the tensor API compiles\n+            if (dev->props.has_tensor) {\n+                const char * src_tensor_f16 = \"\\n\"\n+                    \"#include <metal_stdlib> \\n\"\n+                    \"#include <metal_tensor> \\n\"\n+                    \"#include <MetalPerformancePrimitives/MetalPerformancePrimitives.h> \\n\"\n+                    \" \\n\"\n+                    \"using namespace metal; \\n\"\n+                    \"using namespace mpp::tensor_ops; \\n\"\n+                    \" \\n\"\n+                    \"kernel void dummy_kernel( \\n\"\n+                    \"    tensor<device  half, dextents<int32_t, 2>> A [[buffer(0)]], \\n\"\n+                    \"    tensor<device  half, dextents<int32_t, 2>> B [[buffer(1)]], \\n\"\n+                    \"    device float * C [[buffer(2)]], \\n\"\n+                    \"    uint2 tgid [[threadgroup_position_in_grid]]) \\n\"\n+                    \"{ \\n\"\n+                    \"    auto tA = A.slice(0, (int)tgid.y); \\n\"\n+                    \"    auto tB = B.slice((int)tgid.x, 0); \\n\"\n+                    \" \\n\"\n+                    \"    matmul2d< \\n\"\n+                    \"        matmul2d_descriptor(8, 8, dynamic_extent), \\n\"\n+                    \"        execution_simdgroups<4>> mm; \\n\"\n+                    \" \\n\"\n+                    \"    auto cT = mm.get_destination_cooperative_tensor<decltype(tA), decltype(tB), float>(); \\n\"\n+                    \" \\n\"\n+                    \"    auto sA = tA.slice(0, 0); \\n\"\n+                    \"    auto sB = tB.slice(0, 0); \\n\"\n+                    \"    mm.run(sB, sA, cT); \\n\"\n+                    \" \\n\"\n+                    \"    auto tC = tensor<device float, dextents<int32_t, 2>, tensor_inline>(C, dextents<int32_t, 2>(4, 4)); \\n\"\n+                    \" \\n\"\n+                    \"    cT.store(tC); \\n\"\n+                    \"}\";\n+\n+                GGML_LOG_INFO(\"%s: testing tensor API for f16 support\\n\", __func__);\n+                ggml_metal_library_t lib = ggml_metal_library_init_from_source(dev, src_tensor_f16, false);\n+                if (lib == NULL) {\n+                    GGML_LOG_WARN(\"%s: - the tensor API is not supported in this environment - disabling\\n\", __func__);\n+                    dev->props.has_tensor = false;\n+                } else {\n+                    ggml_metal_pipeline_t ppl = ggml_metal_library_compile_pipeline(lib, \"dummy_kernel\", \"dummy_kernel\", nil);\n+                    if (!ppl) {\n+                        GGML_LOG_WARN(\"%s: - the tensor API is not supported in this environment - disabling\\n\", __func__);\n+                        dev->props.has_tensor = false;\n+                    }\n+\n+                    ggml_metal_library_free(lib);\n+                }\n+            }\n+\n+            // try to compile a dummy kernel to determine if the tensor API is supported for bfloat\n+            if (dev->props.has_tensor && dev->props.has_bfloat) {\n+                const char * src_tensor_bf16 = \"\\n\"\n+                    \"#include <metal_stdlib> \\n\"\n+                    \"#include <metal_tensor> \\n\"\n+                    \"#include <MetalPerformancePrimitives/MetalPerformancePrimitives.h> \\n\"\n+                    \" \\n\"\n+                    \"using namespace metal; \\n\"\n+                    \"using namespace mpp::tensor_ops; \\n\"\n+                    \" \\n\"\n+                    \"kernel void dummy_kernel( \\n\"\n+                    \"    tensor<device bfloat, dextents<int32_t, 2>> A [[buffer(0)]], \\n\"\n+                    \"    tensor<device bfloat, dextents<int32_t, 2>> B [[buffer(1)]], \\n\"\n+                    \"    device float * C [[buffer(2)]], \\n\"\n+                    \"    uint2 tgid [[threadgroup_position_in_grid]]) \\n\"\n+                    \"{ \\n\"\n+                    \"    auto tA = A.slice(0, (int)tgid.y); \\n\"\n+                    \"    auto tB = B.slice((int)tgid.x, 0); \\n\"\n+                    \" \\n\"\n+                    \"    matmul2d< \\n\"\n+                    \"        matmul2d_descriptor(8, 8, dynamic_extent), \\n\"\n+                    \"        execution_simdgroups<4>> mm; \\n\"\n+                    \" \\n\"\n+                    \"    auto cT = mm.get_destination_cooperative_tensor<decltype(tA), decltype(tB), float>(); \\n\"\n+                    \" \\n\"\n+                    \"    auto sA = tA.slice(0, 0); \\n\"\n+                    \"    auto sB = tB.slice(0, 0); \\n\"\n+                    \"    mm.run(sB, sA, cT); \\n\"\n+                    \" \\n\"\n+                    \"    auto tC = tensor<device float, dextents<int32_t, 2>, tensor_inline>(C, dextents<int32_t, 2>(4, 4)); \\n\"\n+                    \" \\n\"\n+                    \"    cT.store(tC); \\n\"\n+                    \"}\";\n+\n+                GGML_LOG_INFO(\"%s: testing tensor API for bfloat support\\n\", __func__);\n+                ggml_metal_library_t lib = ggml_metal_library_init_from_source(dev, src_tensor_bf16, false);\n+                if (lib == NULL) {\n+                    GGML_LOG_WARN(\"%s: - the tensor API does not support bfloat - disabling bfloat support\\n\", __func__);\n+                    dev->props.has_bfloat = false;\n+                } else {\n+                    ggml_metal_pipeline_t ppl = ggml_metal_library_compile_pipeline(lib, \"dummy_kernel\", \"dummy_kernel\", nil);\n+                    if (!ppl) {\n+                        GGML_LOG_WARN(\"%s: - the tensor API does not support bfloat - disabling bfloat support\\n\", __func__);\n+                        dev->props.has_bfloat = false;\n+                    }\n+\n+                    ggml_metal_library_free(lib);\n+                }\n+            }\n \n             dev->props.use_residency_sets = true;\n #if defined(GGML_METAL_HAS_RESIDENCY_SETS)\n             dev->props.use_residency_sets = getenv(\"GGML_METAL_NO_RESIDENCY\") == nil;\n #endif\n \n             dev->props.use_shared_buffers = dev->props.has_unified_memory;\n-\n             if (getenv(\"GGML_METAL_SHARED_BUFFERS_DISABLE\") != NULL) {\n                 dev->props.use_shared_buffers = false;\n             }\n@@ -529,6 +727,7 @@ ggml_metal_device_t ggml_metal_device_init(void) {\n             GGML_LOG_INFO(\"%s: simdgroup matrix mul. = %s\\n\", __func__, dev->props.has_simdgroup_mm        ? \"true\" : \"false\");\n             GGML_LOG_INFO(\"%s: has unified memory    = %s\\n\", __func__, dev->props.has_unified_memory      ? \"true\" : \"false\");\n             GGML_LOG_INFO(\"%s: has bfloat            = %s\\n\", __func__, dev->props.has_bfloat              ? \"true\" : \"false\");\n+            GGML_LOG_INFO(\"%s: has tensor            = %s\\n\", __func__, dev->props.has_tensor              ? \"true\" : \"false\");\n             GGML_LOG_INFO(\"%s: use residency sets    = %s\\n\", __func__, dev->props.use_residency_sets      ? \"true\" : \"false\");\n             GGML_LOG_INFO(\"%s: use shared buffers    = %s\\n\", __func__, dev->props.use_shared_buffers      ? \"true\" : \"false\");\n "
      },
      {
        "filename": "ggml/src/ggml-metal/ggml-metal.metal",
        "status": "modified",
        "additions": 396,
        "deletions": 125,
        "changes": 521,
        "patch": "@@ -9,6 +9,12 @@ __embed_ggml-common.h__\n \n #include <metal_stdlib>\n \n+#ifdef GGML_METAL_HAS_TENSOR\n+#include <metal_tensor>\n+\n+#include <MetalPerformancePrimitives/MetalPerformancePrimitives.h>\n+#endif\n+\n using namespace metal;\n \n #define MAX(x, y) ((x) > (y) ? (x) : (y))\n@@ -1742,7 +1748,7 @@ kernel void kernel_op_sum_f32(\n \n     float sumf = 0;\n \n-    for (int64_t i0 = tpitg.x; i0 < args.np; i0 += ntg.x) {\n+    for (uint64_t i0 = tpitg.x; i0 < args.np; i0 += ntg.x) {\n         sumf += src0[i0];\n     }\n \n@@ -5467,6 +5473,7 @@ template [[host_name(\"kernel_flash_attn_ext_q8_0_dk576_dv512\")]] kernel flash_at\n \n #undef FA_TYPES\n #undef FA_TYPES_BF\n+#undef FA_TYPES_F32\n \n constant bool FC_flash_attn_ext_vec_has_mask  [[function_constant(FC_FLASH_ATTN_EXT_VEC + 0)]];\n constant bool FC_flash_attn_ext_vec_has_sinks [[function_constant(FC_FLASH_ATTN_EXT_VEC + 1)]];\n@@ -6088,6 +6095,7 @@ template [[host_name(\"kernel_flash_attn_ext_vec_q5_1_dk576_dv512\")]] kernel flas\n template [[host_name(\"kernel_flash_attn_ext_vec_q8_0_dk576_dv512\")]] kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES,     block_q8_0, 8, dequantize_q8_0_t4, block_q8_0,  8, dequantize_q8_0_t4, 576, 512, 2>;\n \n #undef FA_TYPES\n+#undef FA_TYPES_F32\n \n constant int32_t FC_flash_attn_ext_vec_reduce_DV  [[function_constant(FC_FLASH_ATTN_EXT_VEC_REDUCE + 0)]];\n constant int32_t FC_flash_attn_ext_vec_reduce_NWG [[function_constant(FC_FLASH_ATTN_EXT_VEC_REDUCE + 1)]];\n@@ -8141,17 +8149,6 @@ kernel void kernel_set_rows_f(\n constant bool FC_mul_mm_bc_inp [[function_constant(FC_MUL_MM + 0)]];\n constant bool FC_mul_mm_bc_out [[function_constant(FC_MUL_MM + 1)]];\n \n-#define BLOCK_SIZE_M 64 // 8 simdgroup matrices from matrix A\n-#define BLOCK_SIZE_N 32 // 4 simdgroup matrices from matrix B\n-#define BLOCK_SIZE_K 32\n-#define THREAD_MAT_M 4 // each thread take 4 simdgroup matrices from matrix A\n-#define THREAD_MAT_N 2 // each thread take 2 simdgroup matrices from matrix B\n-#define THREAD_PER_BLOCK 128\n-#define THREAD_PER_ROW 2 // 2 thread for each row in matrix A to load numbers\n-#define THREAD_PER_COL 4 // 4 thread for each row in matrix B to load numbers\n-#define SG_MAT_SIZE 64 // simdgroup matrix is of shape 8x8\n-#define SG_MAT_ROW 8\n-\n // each block_q contains 16*nl weights\n template<typename S0, typename S0_4x4, typename S0_8x8, typename S1, typename S1_2x4, typename S1_8x8, typename block_q, short nl, void (*dequantize_func)(device const block_q *, short, thread S0_4x4 &), typename T0, typename T0_4x4, typename T1, typename T1_2x4>\n kernel void kernel_mul_mm(\n@@ -8167,18 +8164,48 @@ kernel void kernel_mul_mm(\n     threadgroup S0 * sa = (threadgroup S0 *)(shmem);\n     threadgroup S1 * sb = (threadgroup S1 *)(shmem + 4096);\n \n-    const int r0 = tgpig.y;\n-    const int r1 = tgpig.x;\n+    threadgroup float * sc = (threadgroup float *)(shmem);\n+\n+    constexpr int NR0 = 64;\n+    constexpr int NR1 = 32;\n+\n+    constexpr int NK  = 32;\n+    constexpr int NL0 = NK/16;\n+    constexpr int NL1 = NK/8;\n+\n     const int im = tgpig.z;\n+    const int r0 = tgpig.y*NR0;\n+    const int r1 = tgpig.x*NR1;\n \n     // if this block is of 64x32 shape or smaller\n-    const short n_rows = (args.ne0 - r0*BLOCK_SIZE_M < BLOCK_SIZE_M) ? (args.ne0 - r0*BLOCK_SIZE_M) : BLOCK_SIZE_M;\n-    const short n_cols = (args.ne1 - r1*BLOCK_SIZE_N < BLOCK_SIZE_N) ? (args.ne1 - r1*BLOCK_SIZE_N) : BLOCK_SIZE_N;\n+    const short nr0 = (args.ne0 - r0 < NR0) ? (args.ne0 - r0) : NR0;\n+    const short nr1 = (args.ne1 - r1 < NR1) ? (args.ne1 - r1) : NR1;\n \n     // a thread shouldn't load data outside of the matrix\n-    const short thread_row = ((short)tiitg/THREAD_PER_ROW) < n_rows ? ((short)tiitg/THREAD_PER_ROW) : n_rows - 1;\n-    const short thread_col = ((short)tiitg/THREAD_PER_COL) < n_cols ? ((short)tiitg/THREAD_PER_COL) : n_cols - 1;\n+    const short lr0 = ((short)tiitg/NL0) < nr0 ? ((short)tiitg/NL0) : nr0 - 1; // 0 .. 63\n+    const short lr1 = ((short)tiitg/NL1) < nr1 ? ((short)tiitg/NL1) : nr1 - 1; // 0 .. 31\n+\n+    const short il0 = (tiitg % NL0);\n+\n+    short il = il0;\n+\n+    const int i12 = im%args.ne12;\n+    const int i13 = im/args.ne12;\n+\n+    const uint64_t offset0 = (i12/args.r2)*args.nb02 + (i13/args.r3)*args.nb03;\n+    const short    offset1 = il0/nl;\n+\n+    device const block_q * x = (device const block_q *)(src0 + args.nb01*(r0 + lr0) + offset0) + offset1;\n+\n+    const short iy = 8*(tiitg % NL1);\n+\n+    device const T1 * y = (device const T1 *)(src1\n+        + args.nb13*i13\n+        + args.nb12*i12\n+        + args.nb11*(r1 + lr1)\n+        + args.nb10*iy);\n \n+#ifndef GGML_METAL_HAS_TENSOR\n     S0_8x8 ma[4];\n     S1_8x8 mb[2];\n \n@@ -8187,36 +8214,104 @@ kernel void kernel_mul_mm(\n     for (short i = 0; i < 8; i++){\n         mc[i] = make_filled_simdgroup_matrix<float, 8>(0.f);\n     }\n+#else\n+    auto tA = tensor<threadgroup S0, dextents<int32_t, 2>, tensor_inline>(sa, dextents<int32_t, 2>(NK,  NR0));\n+    auto tB = tensor<threadgroup S1, dextents<int32_t, 2>, tensor_inline>(sb, dextents<int32_t, 2>(NR1, NK ));\n \n-    short il = (tiitg % THREAD_PER_ROW);\n+    mpp::tensor_ops::matmul2d<\n+        mpp::tensor_ops::matmul2d_descriptor(NR1, NR0, NK, false, true, false, mpp::tensor_ops::matmul2d_descriptor::mode::multiply_accumulate),\n+        execution_simdgroups<4>> mm;\n \n-    const int i12 = im%args.ne12;\n-    const int i13 = im/args.ne12;\n+    auto cT = mm.get_destination_cooperative_tensor<decltype(tA), decltype(tB), float>();\n+#endif\n \n-    const uint64_t offset0 = (i12/args.r2)*args.nb02 + (i13/args.r3)*args.nb03;\n-    const short    offset1 = il/nl;\n+    for (int loop_k = 0; loop_k < args.ne00; loop_k += NK) {\n+#ifndef GGML_METAL_HAS_TENSOR\n+        // load data and store to threadgroup memory\n+        if (is_same<T0_4x4, block_q>::value && FC_mul_mm_bc_inp) {\n+            threadgroup_barrier(mem_flags::mem_threadgroup);\n \n-    device const block_q * x = (device const block_q *)(src0\n-        + args.nb01*(r0*BLOCK_SIZE_M + thread_row) + offset0) + offset1;\n+            // no need for dequantization\n+            for (short i = 0; i < 16; i++) {\n+                const short sx = 2*il0 + i/8;\n+                const short sy = (tiitg/NL0)/8;\n \n-    const short iy = (BLOCK_SIZE_K / THREAD_PER_COL * (tiitg % THREAD_PER_COL));\n+              //const short lx = i%8;\n+              //const short ly = (tiitg/NL0)%8;\n+                const short lx = (tiitg/NL0)%8;\n+                const short ly = i%8;\n \n-    device const T1 * y = (device const T1 *)(src1\n-        + args.nb13*i13\n-        + args.nb12*i12\n-        + args.nb11*(r1*BLOCK_SIZE_N + thread_col)\n-        + args.nb10*iy);\n+                const short ib = 8*sx + sy;\n+\n+                *(sa + 64*ib + 8*ly + lx) = loop_k + 16*il + i < args.ne00 ? *((device T0 *) x + i) : 0;\n+            }\n+        } else {\n+            S0_4x4 temp_a;\n+            dequantize_func(x, il, temp_a);\n+\n+            threadgroup_barrier(mem_flags::mem_threadgroup);\n \n-    for (int loop_k = 0; loop_k < args.ne00; loop_k += BLOCK_SIZE_K) {\n+            FOR_UNROLL (short i = 0; i < 16; i++) {\n+                const short sx = 2*il0 + i/8;\n+                const short sy = (tiitg/NL0)/8;\n+\n+              //const short lx = i%8;\n+              //const short ly = (tiitg/NL0)%8;\n+                const short lx = (tiitg/NL0)%8;\n+                const short ly = i%8;\n+\n+                const short ib = 8*sx + sy;\n+\n+                // NOTE: this is massively slower.. WTF?\n+                //sa[64*ib + 8*ly + lx] = temp_a[i/4][i%4];\n+\n+                *(sa + 64*ib + 8*ly + lx) = temp_a[i/4][i%4];\n+            }\n+        }\n+\n+        if (FC_mul_mm_bc_inp) {\n+            for (short i = 0; i < 8; ++i) {\n+                const short sx = (tiitg%NL1);\n+                const short sy = (tiitg/NL1)/8;\n+\n+                const short lx = i;\n+                const short ly = (tiitg/NL1)%8;\n+              //const short lx = (tiitg/NL1)%8;\n+              //const short ly = i;\n+\n+                const short ib = 4*sx + sy;\n+\n+                *(sb + 64*ib + 8*ly + lx) = loop_k + iy + i < args.ne00 ? (S1) *((device T1 *) y + i) : 0;\n+            }\n+        } else {\n+            const short sx = (tiitg%NL1);\n+            const short sy = (tiitg/NL1)/8;\n+\n+            const short dx = sx;\n+            const short dy = sy;\n+\n+            const short ly = (tiitg/NL1)%8;\n+\n+            const short ib = 4*sx + sy;\n+\n+            *(threadgroup S1_2x4 *)(sb + 64*ib + 8*ly) = (S1_2x4)(*((device T1_2x4 *) y));\n+        }\n+#else\n         // load data and store to threadgroup memory\n         if (is_same<T0_4x4, block_q>::value && FC_mul_mm_bc_inp) {\n             threadgroup_barrier(mem_flags::mem_threadgroup);\n \n             // no need for dequantization\n             for (short i = 0; i < 16; i++) {\n-                *(sa + SG_MAT_SIZE * ((tiitg/THREAD_PER_ROW/8) \\\n-                +                     (tiitg%THREAD_PER_ROW)*16 + (i/8)*8) \\\n-                +                     (tiitg/THREAD_PER_ROW)%8  + (i&7)*8) = loop_k + 16*il + i < args.ne00 ? ((device T0 *) x)[i] : 0;\n+                const short sx = 2*il0 + i/8;\n+                const short sy = (tiitg/NL0)/8;\n+\n+                const short lx = i%8;\n+                const short ly = (tiitg/NL0)%8;\n+                //const short lx = (tiitg/NL0)%8;\n+                //const short ly = i%8;\n+\n+                *(sa + NK*(8*sy + ly) + 8*sx + lx) = loop_k + 16*il + i < args.ne00 ? *((device T0 *) x + i) : 0;\n             }\n         } else {\n             S0_4x4 temp_a;\n@@ -8225,91 +8320,135 @@ kernel void kernel_mul_mm(\n             threadgroup_barrier(mem_flags::mem_threadgroup);\n \n             FOR_UNROLL (short i = 0; i < 16; i++) {\n-                *(sa + SG_MAT_SIZE * ((tiitg/THREAD_PER_ROW/8) \\\n-                +                     (tiitg%THREAD_PER_ROW)*16 + (i/8)*8) \\\n-                +                     (tiitg/THREAD_PER_ROW)%8  + (i&7)*8) = temp_a[i/4][i%4];\n+                const short sx = 2*il0 + i/8;\n+                const short sy = (tiitg/NL0)/8;\n+\n+                const short lx = i%8;\n+                const short ly = (tiitg/NL0)%8;\n+                //const short lx = (tiitg/NL0)%8;\n+                //const short ly = i%8;\n+\n+                *(sa + NK*(8*sy + ly) + 8*sx + lx) = temp_a[i/4][i%4];\n             }\n         }\n \n         if (FC_mul_mm_bc_inp) {\n             for (short i = 0; i < 8; ++i) {\n-                sb[32*8*(tiitg%THREAD_PER_COL) + 8*(tiitg/THREAD_PER_COL) + i] = loop_k + iy + i < args.ne00 ? (S1) ((device T1 *) y)[i] : 0;\n+                const short sx = (tiitg%NL1);\n+                const short sy = (tiitg/NL1)/8;\n+\n+                const short lx = i;\n+                const short ly = (tiitg/NL1)%8;\n+                //const short lx = (tiitg/NL1)%8;\n+                //const short ly = i;\n+\n+                *(sb + NK*(8*sy + ly) + 8*sx + lx) = loop_k + iy + i < args.ne00 ? (S1) *((device T1 *) y + i) : 0;\n             }\n         } else {\n-            *(threadgroup S1_2x4 *)(sb + 32*8*(tiitg%THREAD_PER_COL) + 8*(tiitg/THREAD_PER_COL)) = (S1_2x4)(*((device T1_2x4 *) y));\n+            const short sx = (tiitg%NL1);\n+            const short sy = (tiitg/NL1)/8;\n+\n+            //const short lx = i;\n+            const short ly = (tiitg/NL1)%8;\n+            //const short lx = (tiitg/NL1)%8;\n+            //const short ly = i;\n+\n+            *(threadgroup S1_2x4 *)(sb + NK*(8*sy + ly) + 8*sx) = (S1_2x4)(*((device T1_2x4 *) y));\n         }\n+#endif\n \n         il = (il + 2 < nl) ? il + 2 : il % 2;\n         x  = (il < 2) ? x + (2 + nl - 1)/nl : x;\n-        y += BLOCK_SIZE_K;\n+\n+        y += NK;\n \n         threadgroup_barrier(mem_flags::mem_threadgroup);\n \n+#ifndef GGML_METAL_HAS_TENSOR\n         // load matrices from threadgroup memory and conduct outer products\n-        threadgroup const S0 * lsma = (sa + THREAD_MAT_M*SG_MAT_SIZE*(sgitg%2));\n-        threadgroup const S1 * lsmb = (sb + THREAD_MAT_N*SG_MAT_SIZE*(sgitg/2));\n+        threadgroup const S0 * lsma = (sa + 4*64*(sgitg%2));\n+        threadgroup const S1 * lsmb = (sb + 2*64*(sgitg/2));\n \n-        #pragma unroll(4)\n-        for (short ik = 0; ik < BLOCK_SIZE_K/8; ik++) {\n+        FOR_UNROLL (short ik = 0; ik < NK/8; ik++) {\n             simdgroup_barrier(mem_flags::mem_none);\n \n-            #pragma unroll(4)\n-            for (short i = 0; i < 4; i++) {\n-                simdgroup_load(ma[i], lsma + SG_MAT_SIZE * i);\n+            FOR_UNROLL (short i = 0; i < 4; i++) {\n+                simdgroup_load(ma[i], lsma + 64*i, 8, 0, false);\n             }\n \n-            #pragma unroll(2)\n-            for (short i = 0; i < 2; i++) {\n-                simdgroup_load(mb[i], lsmb + SG_MAT_SIZE * i);\n+            simdgroup_barrier(mem_flags::mem_none);\n+\n+            FOR_UNROLL (short i = 0; i < 2; i++) {\n+                simdgroup_load(mb[i], lsmb + 64*i, 8, 0, false);\n             }\n \n             simdgroup_barrier(mem_flags::mem_none);\n \n-            #pragma unroll(8)\n-            for (short i = 0; i < 8; i++){\n+            FOR_UNROLL (short i = 0; i < 8; i++){\n                 simdgroup_multiply_accumulate(mc[i], mb[i/4], ma[i%4], mc[i]);\n             }\n \n-            lsma += (BLOCK_SIZE_M/SG_MAT_ROW)*SG_MAT_SIZE;\n-            lsmb += (BLOCK_SIZE_N/SG_MAT_ROW)*SG_MAT_SIZE;\n+            lsma += 8*64;\n+            lsmb += 4*64;\n         }\n+#else\n+        auto sA = tA.slice(0, 0);\n+        auto sB = tB.slice(0, 0);\n+\n+        mm.run(sB, sA, cT);\n+#endif\n     }\n \n-    if (!FC_mul_mm_bc_out || ((r0 + 1) * BLOCK_SIZE_M <= args.ne0 && (r1 + 1) * BLOCK_SIZE_N <= args.ne1)) {\n+    if (!FC_mul_mm_bc_out || (r0 + NR0 <= args.ne0 && r1 + NR1 <= args.ne1)) {\n         // if no bounds checks on the output are needed, we can directly write to device memory\n+#ifdef GGML_METAL_HAS_TENSOR\n+        device float * C = (device float *) dst +\n+            r0 + \\\n+            r1 * args.ne0 + im*args.ne1*args.ne0;\n+\n+        auto tC = tensor<device float, dextents<int32_t, 2>, tensor_inline>(C, dextents<int32_t, 2>(args.ne0, NR1));\n+        cT.store(tC);\n+#else\n         device float * C = (device float *) dst +\n-            (BLOCK_SIZE_M * r0 + 32*(sgitg &  1)) + \\\n-            (BLOCK_SIZE_N * r1 + 16*(sgitg >> 1)) * args.ne0 + im*args.ne1*args.ne0;\n+            (r0 + 32*(sgitg &  1)) + \\\n+            (r1 + 16*(sgitg >> 1)) * args.ne0 + im*args.ne1*args.ne0;\n \n         for (short i = 0; i < 8; i++) {\n-            simdgroup_store(mc[i], C + 8 * (i%4) + 8 * args.ne0 * (i/4), args.ne0);\n+            simdgroup_store(mc[i], C + 8*(i%4) + 8*args.ne0*(i/4), args.ne0, 0, false);\n         }\n+#endif\n     } else {\n         // block is smaller than 64x32, we should avoid writing data outside of the matrix\n         threadgroup_barrier(mem_flags::mem_threadgroup);\n-        threadgroup float * temp_str = ((threadgroup float *) shmem) \\\n-                                     + 32*(sgitg&1) + (16*(sgitg >> 1))*BLOCK_SIZE_M;\n+\n+        threadgroup float * temp_str = ((threadgroup float *) shmem) + 32*(sgitg&1) + (16*(sgitg >> 1))*NR0;\n+\n+#ifdef GGML_METAL_HAS_TENSOR\n+        auto tC = tensor<threadgroup float, dextents<int32_t, 2>, tensor_inline>(sc, dextents<int32_t, 2>(NR0, NR1));\n+        cT.store(tC);\n+#else\n         for (short i = 0; i < 8; i++) {\n-            simdgroup_store(mc[i], temp_str + 8*(i%4) + 8*BLOCK_SIZE_M*(i/4), BLOCK_SIZE_M);\n+            simdgroup_store(mc[i], temp_str + 8*(i%4) + 8*NR0*(i/4), NR0, 0, false);\n         }\n+#endif\n \n         threadgroup_barrier(mem_flags::mem_threadgroup);\n \n         if (sgitg == 0) {\n-            for (int j = tiitg; j < n_cols; j += BLOCK_SIZE_N) {\n-                device float  * D  = (device float  *) dst + (r0*BLOCK_SIZE_M) + (r1*BLOCK_SIZE_N + j)*args.ne0 + im*args.ne1*args.ne0;\n+            for (int j = tiitg; j < nr1; j += NR1) {\n+                device float  * D  = (device float  *) dst + r0 + (r1 + j)*args.ne0 + im*args.ne1*args.ne0;\n                 device float4 * D4 = (device float4 *) D;\n \n-                threadgroup float  * C  = temp_str + (j*BLOCK_SIZE_M);\n+                threadgroup float  * C  = temp_str + (j*NR0);\n                 threadgroup float4 * C4 = (threadgroup float4 *) C;\n \n                 int i = 0;\n-                for (; i < n_rows/4; i++) {\n+                for (; i < nr0/4; i++) {\n                     *(D4 + i) = *(C4 + i);\n                 }\n \n                 i *= 4;\n-                for (; i < n_rows; i++) {\n+                for (; i < nr0; i++) {\n                     *(D + i) = *(C + i);\n                 }\n             }\n@@ -8394,72 +8533,169 @@ kernel void kernel_mul_mm_id(\n         ushort tiitg[[thread_index_in_threadgroup]],\n         ushort tiisg[[thread_index_in_simdgroup]],\n         ushort sgitg[[simdgroup_index_in_threadgroup]]) {\n-\n     threadgroup S0 * sa = (threadgroup S0 *)(shmem);\n     threadgroup S1 * sb = (threadgroup S1 *)(shmem + 4096);\n \n-    const int r0 = tgpig.y;\n-    const int r1 = tgpig.x;\n+    threadgroup float * sc = (threadgroup float *)(shmem);\n+\n+    constexpr int NR0 = 64;\n+    constexpr int NR1 = 32;\n+\n+    constexpr int NK  = 32;\n+    constexpr int NL0 = NK/16;\n+    constexpr int NL1 = NK/8;\n+\n     const int im = tgpig.z; // expert\n+    const int r0 = tgpig.y*NR0;\n+    const int r1 = tgpig.x*NR1;\n \n     device const uint32_t * tpe_u32 = (device const uint32_t *) (htpe);\n     device const int32_t  * ids_i32 = (device const int32_t  *) (hids);\n \n     const int32_t neh1 = tpe_u32[im];\n \n-    if (r1*BLOCK_SIZE_N >= neh1) {\n+    if (r1 >= neh1) {\n         return;\n     }\n \n     // if this block is of 64x32 shape or smaller\n-    const short n_rows = (args.ne0 - r0*BLOCK_SIZE_M < BLOCK_SIZE_M) ? (args.ne0 - r0*BLOCK_SIZE_M) : BLOCK_SIZE_M;\n-    const short n_cols = (    neh1 - r1*BLOCK_SIZE_N < BLOCK_SIZE_N) ? (    neh1 - r1*BLOCK_SIZE_N) : BLOCK_SIZE_N;\n+    const short nr0 = (args.ne0 - r0 < NR0) ? (args.ne0 - r0) : NR0;\n+    const short nr1 = (    neh1 - r1 < NR1) ? (    neh1 - r1) : NR1;\n \n     // a thread shouldn't load data outside of the matrix\n-    const short thread_row = ((short)tiitg/THREAD_PER_ROW) < n_rows ? ((short)tiitg/THREAD_PER_ROW) : n_rows - 1;\n-    const short thread_col = ((short)tiitg/THREAD_PER_COL) < n_cols ? ((short)tiitg/THREAD_PER_COL) : n_cols - 1;\n+    const short lr0 = ((short)tiitg/NL0) < nr0 ? ((short)tiitg/NL0) : nr0 - 1; // 0 .. 63\n+    const short lr1 = ((short)tiitg/NL1) < nr1 ? ((short)tiitg/NL1) : nr1 - 1; // 0 .. 31\n \n-    S0_8x8 ma[4];\n-    S1_8x8 mb[2];\n+    const short il0 = (tiitg % NL0);\n \n-    simdgroup_float8x8 mc[8];\n+    short il = il0;\n \n-    for (short i = 0; i < 8; i++){\n-        mc[i] = make_filled_simdgroup_matrix<float, 8>(0.f);\n-    }\n-\n-    short il = (tiitg % THREAD_PER_ROW);\n-\n-    const int id = ids_i32[im*args.ne21 + r1*BLOCK_SIZE_N + thread_col];\n+    const int id = ids_i32[im*args.ne21 + r1 + lr1];\n \n     const short i11 = (id % args.ne20) % args.ne11;\n     const short i12 = (id / args.ne20);\n     const short i13 = 0;\n \n     const uint64_t offset0 = im*args.nb02 + i13*args.nb03;\n-    const short    offset1 = il/nl;\n+    const short    offset1 = il0/nl;\n \n-    device const block_q * x = (device const block_q *)(src0\n-        + args.nb01*(r0*BLOCK_SIZE_M + thread_row) + offset0) + offset1;\n+    device const block_q * x = (device const block_q *)(src0 + args.nb01*(r0 + lr0) + offset0) + offset1;\n \n-    const short iy = (BLOCK_SIZE_K / THREAD_PER_COL * (tiitg % THREAD_PER_COL));\n+    const short iy = 8*(tiitg % NL1);\n \n     device const T1 * y = (device const T1 *)(src1\n         + args.nb13*i13\n         + args.nb12*i12\n         + args.nb11*i11\n         + args.nb10*iy);\n \n-    for (int loop_k = 0; loop_k < args.ne00; loop_k += BLOCK_SIZE_K) {\n+#ifndef GGML_METAL_HAS_TENSOR\n+    S0_8x8 ma[4];\n+    S1_8x8 mb[2];\n+\n+    simdgroup_float8x8 mc[8];\n+\n+    for (short i = 0; i < 8; i++){\n+        mc[i] = make_filled_simdgroup_matrix<float, 8>(0.f);\n+    }\n+#else\n+    auto tA = tensor<threadgroup S0, dextents<int32_t, 2>, tensor_inline>(sa, dextents<int32_t, 2>(NK,  NR0));\n+    auto tB = tensor<threadgroup S1, dextents<int32_t, 2>, tensor_inline>(sb, dextents<int32_t, 2>(NR1, NK ));\n+\n+    mpp::tensor_ops::matmul2d<\n+        mpp::tensor_ops::matmul2d_descriptor(NR1, NR0, NK, false, true, false, mpp::tensor_ops::matmul2d_descriptor::mode::multiply_accumulate),\n+        execution_simdgroups<4>> mm;\n+\n+    auto cT = mm.get_destination_cooperative_tensor<decltype(tA), decltype(tB), float>();\n+#endif\n+\n+    for (int loop_k = 0; loop_k < args.ne00; loop_k += NK) {\n+#ifndef GGML_METAL_HAS_TENSOR\n+        // load data and store to threadgroup memory\n+        if (is_same<T0_4x4, block_q>::value && FC_mul_mm_bc_inp) {\n+            threadgroup_barrier(mem_flags::mem_threadgroup);\n+\n+            // no need for dequantization\n+            for (short i = 0; i < 16; i++) {\n+                const short sx = 2*il0 + i/8;\n+                const short sy = (tiitg/NL0)/8;\n+\n+              //const short lx = i%8;\n+              //const short ly = (tiitg/NL0)%8;\n+                const short lx = (tiitg/NL0)%8;\n+                const short ly = i%8;\n+\n+                const short ib = 8*sx + sy;\n+\n+                *(sa + 64*ib + 8*ly + lx) = loop_k + 16*il + i < args.ne00 ? *((device T0 *) x + i) : 0;\n+            }\n+        } else {\n+            S0_4x4 temp_a;\n+            dequantize_func(x, il, temp_a);\n+\n+            threadgroup_barrier(mem_flags::mem_threadgroup);\n+\n+            FOR_UNROLL (short i = 0; i < 16; i++) {\n+                const short sx = 2*il0 + i/8;\n+                const short sy = (tiitg/NL0)/8;\n+\n+              //const short lx = i%8;\n+              //const short ly = (tiitg/NL0)%8;\n+                const short lx = (tiitg/NL0)%8;\n+                const short ly = i%8;\n+\n+                const short ib = 8*sx + sy;\n+\n+                // NOTE: this is massively slower.. WTF?\n+                //sa[64*ib + 8*ly + lx] = temp_a[i/4][i%4];\n+\n+                *(sa + 64*ib + 8*ly + lx) = temp_a[i/4][i%4];\n+            }\n+        }\n+\n+        if (FC_mul_mm_bc_inp) {\n+            for (short i = 0; i < 8; ++i) {\n+                const short sx = (tiitg%NL1);\n+                const short sy = (tiitg/NL1)/8;\n+\n+                const short lx = i;\n+                const short ly = (tiitg/NL1)%8;\n+              //const short lx = (tiitg/NL1)%8;\n+              //const short ly = i;\n+\n+                const short ib = 4*sx + sy;\n+\n+                *(sb + 64*ib + 8*ly + lx) = loop_k + iy + i < args.ne00 ? (S1) *((device T1 *) y + i) : 0;\n+            }\n+        } else {\n+            const short sx = (tiitg%NL1);\n+            const short sy = (tiitg/NL1)/8;\n+\n+            const short dx = sx;\n+            const short dy = sy;\n+\n+            const short ly = (tiitg/NL1)%8;\n+\n+            const short ib = 4*sx + sy;\n+\n+            *(threadgroup S1_2x4 *)(sb + 64*ib + 8*ly) = (S1_2x4)(*((device T1_2x4 *) y));\n+        }\n+#else\n         // load data and store to threadgroup memory\n         if (is_same<T0_4x4, block_q>::value && FC_mul_mm_bc_inp) {\n             threadgroup_barrier(mem_flags::mem_threadgroup);\n \n             // no need for dequantization\n             for (short i = 0; i < 16; i++) {\n-                *(sa + SG_MAT_SIZE * ((tiitg/THREAD_PER_ROW/8) \\\n-                +                     (tiitg%THREAD_PER_ROW)*16 + (i/8)*8) \\\n-                +                     (tiitg/THREAD_PER_ROW)%8  + (i&7)*8) = loop_k + 16*il + i < args.ne00 ? ((device T0 *) x)[i] : 0;\n+                const short sx = 2*il0 + i/8;\n+                const short sy = (tiitg/NL0)/8;\n+\n+                const short lx = i%8;\n+                const short ly = (tiitg/NL0)%8;\n+                //const short lx = (tiitg/NL0)%8;\n+                //const short ly = i%8;\n+\n+                *(sa + NK*(8*sy + ly) + 8*sx + lx) = loop_k + 16*il + i < args.ne00 ? *((device T0 *) x + i) : 0;\n             }\n         } else {\n             S0_4x4 temp_a;\n@@ -8468,85 +8704,120 @@ kernel void kernel_mul_mm_id(\n             threadgroup_barrier(mem_flags::mem_threadgroup);\n \n             FOR_UNROLL (short i = 0; i < 16; i++) {\n-                *(sa + SG_MAT_SIZE * ((tiitg/THREAD_PER_ROW/8) \\\n-                +                     (tiitg%THREAD_PER_ROW)*16 + (i/8)*8) \\\n-                +                     (tiitg/THREAD_PER_ROW)%8  + (i&7)*8) = temp_a[i/4][i%4];\n+                const short sx = 2*il0 + i/8;\n+                const short sy = (tiitg/NL0)/8;\n+\n+                const short lx = i%8;\n+                const short ly = (tiitg/NL0)%8;\n+                //const short lx = (tiitg/NL0)%8;\n+                //const short ly = i%8;\n+\n+                *(sa + NK*(8*sy + ly) + 8*sx + lx) = temp_a[i/4][i%4];\n             }\n         }\n \n         if (FC_mul_mm_bc_inp) {\n             for (short i = 0; i < 8; ++i) {\n-                sb[32*8*(tiitg%THREAD_PER_COL) + 8*(tiitg/THREAD_PER_COL) + i] = loop_k + iy + i < args.ne00 ? (S1) ((device T1 *) y)[i] : 0;\n+                const short sx = (tiitg%NL1);\n+                const short sy = (tiitg/NL1)/8;\n+\n+                const short lx = i;\n+                const short ly = (tiitg/NL1)%8;\n+                //const short lx = (tiitg/NL1)%8;\n+                //const short ly = i;\n+\n+                *(sb + NK*(8*sy + ly) + 8*sx + lx) = loop_k + iy + i < args.ne00 ? (S1) *((device T1 *) y + i) : 0;\n             }\n         } else {\n-            *(threadgroup S1_2x4 *)(sb + 32*8*(tiitg%THREAD_PER_COL) + 8*(tiitg/THREAD_PER_COL)) = (S1_2x4)(*((device T1_2x4 *) y));\n+            const short sx = (tiitg%NL1);\n+            const short sy = (tiitg/NL1)/8;\n+\n+            //const short lx = i;\n+            const short ly = (tiitg/NL1)%8;\n+            //const short lx = (tiitg/NL1)%8;\n+            //const short ly = i;\n+\n+            *(threadgroup S1_2x4 *)(sb + NK*(8*sy + ly) + 8*sx) = (S1_2x4)(*((device T1_2x4 *) y));\n         }\n+#endif\n \n         il = (il + 2 < nl) ? il + 2 : il % 2;\n         x  = (il < 2) ? x + (2 + nl - 1)/nl : x;\n-        y += BLOCK_SIZE_K;\n+\n+        y += NK;\n \n         threadgroup_barrier(mem_flags::mem_threadgroup);\n \n+#ifndef GGML_METAL_HAS_TENSOR\n         // load matrices from threadgroup memory and conduct outer products\n-        threadgroup const S0 * lsma = (sa + THREAD_MAT_M*SG_MAT_SIZE*(sgitg%2));\n-        threadgroup const S1 * lsmb = (sb + THREAD_MAT_N*SG_MAT_SIZE*(sgitg/2));\n-\n-        #pragma unroll(4)\n-        for (short ik = 0; ik < BLOCK_SIZE_K/8; ik++) {\n-            #pragma unroll(4)\n-            for (short i = 0; i < 4; i++) {\n-                simdgroup_load(ma[i], lsma + SG_MAT_SIZE * i);\n+        threadgroup const S0 * lsma = (sa + 4*64*(sgitg%2));\n+        threadgroup const S1 * lsmb = (sb + 2*64*(sgitg/2));\n+\n+        FOR_UNROLL (short ik = 0; ik < NK/8; ik++) {\n+            simdgroup_barrier(mem_flags::mem_none);\n+\n+            FOR_UNROLL (short i = 0; i < 4; i++) {\n+                simdgroup_load(ma[i], lsma + 64*i, 8, 0, false);\n             }\n \n             simdgroup_barrier(mem_flags::mem_none);\n \n-            #pragma unroll(2)\n-            for (short i = 0; i < 2; i++) {\n-                simdgroup_load(mb[i], lsmb + SG_MAT_SIZE * i);\n+            FOR_UNROLL (short i = 0; i < 2; i++) {\n+                simdgroup_load(mb[i], lsmb + 64*i, 8, 0, false);\n             }\n \n-            #pragma unroll(8)\n-            for (short i = 0; i < 8; i++){\n+            simdgroup_barrier(mem_flags::mem_none);\n+\n+            FOR_UNROLL (short i = 0; i < 8; i++){\n                 simdgroup_multiply_accumulate(mc[i], mb[i/4], ma[i%4], mc[i]);\n             }\n \n-            lsma += (BLOCK_SIZE_M/SG_MAT_ROW)*SG_MAT_SIZE;\n-            lsmb += (BLOCK_SIZE_N/SG_MAT_ROW)*SG_MAT_SIZE;\n+            lsma += 8*64;\n+            lsmb += 4*64;\n         }\n+#else\n+        auto sA = tA.slice(0, 0);\n+        auto sB = tB.slice(0, 0);\n+\n+        mm.run(sB, sA, cT);\n+#endif\n     }\n \n+    // block is smaller than 64x32, we should avoid writing data outside of the matrix\n     threadgroup_barrier(mem_flags::mem_threadgroup);\n \n-    threadgroup float * temp_str = ((threadgroup float *) shmem) \\\n-                                 + 32*(sgitg&1) + (16*(sgitg >> 1))*BLOCK_SIZE_M;\n+#ifdef GGML_METAL_HAS_TENSOR\n+    auto tC = tensor<threadgroup float, dextents<int32_t, 2>, tensor_inline>(sc, dextents<int32_t, 2>(NR0, NR1));\n+    cT.store(tC);\n+#else\n+    threadgroup float * temp_str = ((threadgroup float *) shmem) + 32*(sgitg&1) + (16*(sgitg >> 1))*NR0;\n \n-    #pragma unroll(8)\n     for (short i = 0; i < 8; i++) {\n-        simdgroup_store(mc[i], temp_str + 8*(i%4) + 8*BLOCK_SIZE_M*(i/4), BLOCK_SIZE_M);\n+        simdgroup_store(mc[i], temp_str + 8*(i%4) + 8*NR0*(i/4), NR0, 0, false);\n     }\n+#endif\n \n     threadgroup_barrier(mem_flags::mem_threadgroup);\n \n-    for (short j = sgitg; j < n_cols; j += 4) {\n-        const int id = ids_i32[im*args.ne21 + r1*BLOCK_SIZE_N + j];\n+    for (short j = sgitg; j < nr1; j += 4) {\n+        const int id = ids_i32[im*args.ne21 + r1 + j];\n \n         const short ide = id % args.ne20;\n         const short idt = id / args.ne20;\n \n-        device float  * D  = (device float  *) dst + (r0*BLOCK_SIZE_M) + ide*args.ne0 + idt*args.ne1*args.ne0;\n+        device float  * D  = (device float  *) dst + r0 + ide*args.ne0 + idt*args.ne1*args.ne0;\n         device float4 * D4 = (device float4 *) D;\n \n-        threadgroup float  * C  = (threadgroup float  *) shmem + (j*BLOCK_SIZE_M);\n+        threadgroup float  * C  = (threadgroup float  *) shmem + j*NR0;\n         threadgroup float4 * C4 = (threadgroup float4 *) C;\n \n         int i = tiisg;\n-        for (; i < n_rows/4; i += 32) {\n+        for (; i < nr0/4; i += 32) {\n             *(D4 + i) = *(C4 + i);\n         }\n \n-        i = (4*(n_rows/4)) + tiisg;\n-        for (; i < n_rows; i += 32) {\n+        i = (4*(nr0/4)) + tiisg;\n+        for (; i < nr0; i += 32) {\n             *(D + i) = *(C + i);\n         }\n     }"
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T23:29:53.546939"
  },
  {
    "pr_number": 16619,
    "title": "Import/Export UX improvements",
    "body": "Related to https://github.com/ggml-org/llama.cpp/pull/16282#discussion_r2432990884, this PR moves the Import/Export action buttons to Settings Dialog and additionally introduces dedicated UI with selectable list of conversations which we want to import/export.\r\n\r\n### Screenshots\r\n\r\n<img width=\"907\" height=\"569\" alt=\"Zrzut_ekranu_2025-10-16_o_16 25 21\" src=\"https://github.com/user-attachments/assets/c1128b3e-32f2-4876-8c45-b49c40cd396d\" />\r\n\r\n<img width=\"895\" height=\"1056\" alt=\"Zrzut_ekranu_2025-10-16_o_16 25 38\" src=\"https://github.com/user-attachments/assets/72c569ca-7f67-49aa-8f51-1f5803f5ea15\" />\r\n",
    "html_url": "https://github.com/ggml-org/llama.cpp/pull/16619",
    "created_at": "2025-10-16T17:18:24Z",
    "merged_at": "2025-10-20T11:29:14Z",
    "merge_commit_sha": "0e4a0cf2fae667d3efcf52f2f52398779d986b1d",
    "base_ref": "master",
    "head_sha": "94f5596551cc1a4c9dbfceb938a478bd852b8d67",
    "user": "allozaur",
    "files": [
      {
        "filename": "tools/server/public/index.html.gz",
        "status": "modified",
        "additions": 0,
        "deletions": 0,
        "changes": 0,
        "patch": ""
      },
      {
        "filename": "tools/server/webui/src/lib/components/app/chat/ChatSettings/ChatSettingsDialog.svelte",
        "status": "modified",
        "additions": 21,
        "deletions": 10,
        "changes": 31,
        "patch": "@@ -9,9 +9,11 @@\n \t\tSun,\n \t\tMoon,\n \t\tChevronLeft,\n-\t\tChevronRight\n+\t\tChevronRight,\n+\t\tDatabase\n \t} from '@lucide/svelte';\n \timport { ChatSettingsFooter, ChatSettingsFields } from '$lib/components/app';\n+\timport ImportExportTab from './ImportExportTab.svelte';\n \timport * as Dialog from '$lib/components/ui/dialog';\n \timport { ScrollArea } from '$lib/components/ui/scroll-area';\n \timport { config, updateMultipleConfig } from '$lib/stores/settings.svelte';\n@@ -205,6 +207,11 @@\n \t\t\t\t}\n \t\t\t]\n \t\t},\n+\t\t{\n+\t\t\ttitle: 'Import/Export',\n+\t\t\ticon: Database,\n+\t\t\tfields: []\n+\t\t},\n \t\t{\n \t\t\ttitle: 'Developer',\n \t\t\ticon: Code,\n@@ -455,21 +462,25 @@\n \n \t\t\t<ScrollArea class=\"max-h-[calc(100dvh-13.5rem)] flex-1 md:max-h-[calc(100vh-13.5rem)]\">\n \t\t\t\t<div class=\"space-y-6 p-4 md:p-6\">\n-\t\t\t\t\t<div>\n+\t\t\t\t\t<div class=\"grid\">\n \t\t\t\t\t\t<div class=\"mb-6 flex hidden items-center gap-2 border-b border-border/30 pb-6 md:flex\">\n \t\t\t\t\t\t\t<currentSection.icon class=\"h-5 w-5\" />\n \n \t\t\t\t\t\t\t<h3 class=\"text-lg font-semibold\">{currentSection.title}</h3>\n \t\t\t\t\t\t</div>\n \n-\t\t\t\t\t\t<div class=\"space-y-6\">\n-\t\t\t\t\t\t\t<ChatSettingsFields\n-\t\t\t\t\t\t\t\tfields={currentSection.fields}\n-\t\t\t\t\t\t\t\t{localConfig}\n-\t\t\t\t\t\t\t\tonConfigChange={handleConfigChange}\n-\t\t\t\t\t\t\t\tonThemeChange={handleThemeChange}\n-\t\t\t\t\t\t\t/>\n-\t\t\t\t\t\t</div>\n+\t\t\t\t\t\t{#if currentSection.title === 'Import/Export'}\n+\t\t\t\t\t\t\t<ImportExportTab />\n+\t\t\t\t\t\t{:else}\n+\t\t\t\t\t\t\t<div class=\"space-y-6\">\n+\t\t\t\t\t\t\t\t<ChatSettingsFields\n+\t\t\t\t\t\t\t\t\tfields={currentSection.fields}\n+\t\t\t\t\t\t\t\t\t{localConfig}\n+\t\t\t\t\t\t\t\t\tonConfigChange={handleConfigChange}\n+\t\t\t\t\t\t\t\t\tonThemeChange={handleThemeChange}\n+\t\t\t\t\t\t\t\t/>\n+\t\t\t\t\t\t\t</div>\n+\t\t\t\t\t\t{/if}\n \t\t\t\t\t</div>\n \n \t\t\t\t\t<div class=\"mt-8 border-t pt-6\">"
      },
      {
        "filename": "tools/server/webui/src/lib/components/app/chat/ChatSettings/ConversationSelectionDialog.svelte",
        "status": "added",
        "additions": 249,
        "deletions": 0,
        "changes": 249,
        "patch": "@@ -0,0 +1,249 @@\n+<script lang=\"ts\">\n+\timport { Search, X } from '@lucide/svelte';\n+\timport * as Dialog from '$lib/components/ui/dialog';\n+\timport { Button } from '$lib/components/ui/button';\n+\timport { Input } from '$lib/components/ui/input';\n+\timport { Checkbox } from '$lib/components/ui/checkbox';\n+\timport { ScrollArea } from '$lib/components/ui/scroll-area';\n+\timport { SvelteSet } from 'svelte/reactivity';\n+\n+\tinterface Props {\n+\t\tconversations: DatabaseConversation[];\n+\t\tmessageCountMap?: Map<string, number>;\n+\t\tmode: 'export' | 'import';\n+\t\tonCancel: () => void;\n+\t\tonConfirm: (selectedConversations: DatabaseConversation[]) => void;\n+\t\topen?: boolean;\n+\t}\n+\n+\tlet {\n+\t\tconversations,\n+\t\tmessageCountMap = new Map(),\n+\t\tmode,\n+\t\tonCancel,\n+\t\tonConfirm,\n+\t\topen = $bindable(false)\n+\t}: Props = $props();\n+\n+\tlet searchQuery = $state('');\n+\tlet selectedIds = $state.raw<SvelteSet<string>>(new SvelteSet(conversations.map((c) => c.id)));\n+\tlet lastClickedId = $state<string | null>(null);\n+\n+\tlet filteredConversations = $derived(\n+\t\tconversations.filter((conv) => {\n+\t\t\tconst name = conv.name || 'Untitled conversation';\n+\t\t\treturn name.toLowerCase().includes(searchQuery.toLowerCase());\n+\t\t})\n+\t);\n+\n+\tlet allSelected = $derived(\n+\t\tfilteredConversations.length > 0 &&\n+\t\t\tfilteredConversations.every((conv) => selectedIds.has(conv.id))\n+\t);\n+\n+\tlet someSelected = $derived(\n+\t\tfilteredConversations.some((conv) => selectedIds.has(conv.id)) && !allSelected\n+\t);\n+\n+\tfunction toggleConversation(id: string, shiftKey: boolean = false) {\n+\t\tconst newSet = new SvelteSet(selectedIds);\n+\n+\t\tif (shiftKey && lastClickedId !== null) {\n+\t\t\tconst lastIndex = filteredConversations.findIndex((c) => c.id === lastClickedId);\n+\t\t\tconst currentIndex = filteredConversations.findIndex((c) => c.id === id);\n+\n+\t\t\tif (lastIndex !== -1 && currentIndex !== -1) {\n+\t\t\t\tconst start = Math.min(lastIndex, currentIndex);\n+\t\t\t\tconst end = Math.max(lastIndex, currentIndex);\n+\n+\t\t\t\tconst shouldSelect = !newSet.has(id);\n+\n+\t\t\t\tfor (let i = start; i <= end; i++) {\n+\t\t\t\t\tif (shouldSelect) {\n+\t\t\t\t\t\tnewSet.add(filteredConversations[i].id);\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tnewSet.delete(filteredConversations[i].id);\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\n+\t\t\t\tselectedIds = newSet;\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t}\n+\n+\t\tif (newSet.has(id)) {\n+\t\t\tnewSet.delete(id);\n+\t\t} else {\n+\t\t\tnewSet.add(id);\n+\t\t}\n+\n+\t\tselectedIds = newSet;\n+\t\tlastClickedId = id;\n+\t}\n+\n+\tfunction toggleAll() {\n+\t\tif (allSelected) {\n+\t\t\tconst newSet = new SvelteSet(selectedIds);\n+\n+\t\t\tfilteredConversations.forEach((conv) => newSet.delete(conv.id));\n+\t\t\tselectedIds = newSet;\n+\t\t} else {\n+\t\t\tconst newSet = new SvelteSet(selectedIds);\n+\n+\t\t\tfilteredConversations.forEach((conv) => newSet.add(conv.id));\n+\t\t\tselectedIds = newSet;\n+\t\t}\n+\t}\n+\n+\tfunction handleConfirm() {\n+\t\tconst selected = conversations.filter((conv) => selectedIds.has(conv.id));\n+\t\tonConfirm(selected);\n+\t}\n+\n+\tfunction handleCancel() {\n+\t\tselectedIds = new SvelteSet(conversations.map((c) => c.id));\n+\t\tsearchQuery = '';\n+\t\tlastClickedId = null;\n+\n+\t\tonCancel();\n+\t}\n+\n+\tlet previousOpen = $state(false);\n+\n+\t$effect(() => {\n+\t\tif (open && !previousOpen) {\n+\t\t\tselectedIds = new SvelteSet(conversations.map((c) => c.id));\n+\t\t\tsearchQuery = '';\n+\t\t\tlastClickedId = null;\n+\t\t} else if (!open && previousOpen) {\n+\t\t\tonCancel();\n+\t\t}\n+\n+\t\tpreviousOpen = open;\n+\t});\n+</script>\n+\n+<Dialog.Root bind:open>\n+\t<Dialog.Portal>\n+\t\t<Dialog.Overlay class=\"z-[1000000]\" />\n+\n+\t\t<Dialog.Content class=\"z-[1000001] max-w-2xl\">\n+\t\t\t<Dialog.Header>\n+\t\t\t\t<Dialog.Title>\n+\t\t\t\t\tSelect Conversations to {mode === 'export' ? 'Export' : 'Import'}\n+\t\t\t\t</Dialog.Title>\n+\n+\t\t\t\t<Dialog.Description>\n+\t\t\t\t\t{#if mode === 'export'}\n+\t\t\t\t\t\tChoose which conversations you want to export. Selected conversations will be downloaded\n+\t\t\t\t\t\tas a JSON file.\n+\t\t\t\t\t{:else}\n+\t\t\t\t\t\tChoose which conversations you want to import. Selected conversations will be merged\n+\t\t\t\t\t\twith your existing conversations.\n+\t\t\t\t\t{/if}\n+\t\t\t\t</Dialog.Description>\n+\t\t\t</Dialog.Header>\n+\n+\t\t\t<div class=\"space-y-4\">\n+\t\t\t\t<div class=\"relative\">\n+\t\t\t\t\t<Search class=\"absolute top-1/2 left-3 h-4 w-4 -translate-y-1/2 text-muted-foreground\" />\n+\n+\t\t\t\t\t<Input bind:value={searchQuery} placeholder=\"Search conversations...\" class=\"pr-9 pl-9\" />\n+\n+\t\t\t\t\t{#if searchQuery}\n+\t\t\t\t\t\t<button\n+\t\t\t\t\t\t\tclass=\"absolute top-1/2 right-3 -translate-y-1/2 text-muted-foreground hover:text-foreground\"\n+\t\t\t\t\t\t\tonclick={() => (searchQuery = '')}\n+\t\t\t\t\t\t\ttype=\"button\"\n+\t\t\t\t\t\t>\n+\t\t\t\t\t\t\t<X class=\"h-4 w-4\" />\n+\t\t\t\t\t\t</button>\n+\t\t\t\t\t{/if}\n+\t\t\t\t</div>\n+\n+\t\t\t\t<div class=\"flex items-center justify-between text-sm text-muted-foreground\">\n+\t\t\t\t\t<span>\n+\t\t\t\t\t\t{selectedIds.size} of {conversations.length} selected\n+\t\t\t\t\t\t{#if searchQuery}\n+\t\t\t\t\t\t\t({filteredConversations.length} shown)\n+\t\t\t\t\t\t{/if}\n+\t\t\t\t\t</span>\n+\t\t\t\t</div>\n+\n+\t\t\t\t<div class=\"overflow-hidden rounded-md border\">\n+\t\t\t\t\t<ScrollArea class=\"h-[400px]\">\n+\t\t\t\t\t\t<table class=\"w-full\">\n+\t\t\t\t\t\t\t<thead class=\"sticky top-0 z-10 bg-muted\">\n+\t\t\t\t\t\t\t\t<tr class=\"border-b\">\n+\t\t\t\t\t\t\t\t\t<th class=\"w-12 p-3 text-left\">\n+\t\t\t\t\t\t\t\t\t\t<Checkbox\n+\t\t\t\t\t\t\t\t\t\t\tchecked={allSelected}\n+\t\t\t\t\t\t\t\t\t\t\tindeterminate={someSelected}\n+\t\t\t\t\t\t\t\t\t\t\tonCheckedChange={toggleAll}\n+\t\t\t\t\t\t\t\t\t\t/>\n+\t\t\t\t\t\t\t\t\t</th>\n+\n+\t\t\t\t\t\t\t\t\t<th class=\"p-3 text-left text-sm font-medium\">Conversation Name</th>\n+\n+\t\t\t\t\t\t\t\t\t<th class=\"w-32 p-3 text-left text-sm font-medium\">Messages</th>\n+\t\t\t\t\t\t\t\t</tr>\n+\t\t\t\t\t\t\t</thead>\n+\t\t\t\t\t\t\t<tbody>\n+\t\t\t\t\t\t\t\t{#if filteredConversations.length === 0}\n+\t\t\t\t\t\t\t\t\t<tr>\n+\t\t\t\t\t\t\t\t\t\t<td colspan=\"3\" class=\"p-8 text-center text-sm text-muted-foreground\">\n+\t\t\t\t\t\t\t\t\t\t\t{#if searchQuery}\n+\t\t\t\t\t\t\t\t\t\t\t\tNo conversations found matching \"{searchQuery}\"\n+\t\t\t\t\t\t\t\t\t\t\t{:else}\n+\t\t\t\t\t\t\t\t\t\t\t\tNo conversations available\n+\t\t\t\t\t\t\t\t\t\t\t{/if}\n+\t\t\t\t\t\t\t\t\t\t</td>\n+\t\t\t\t\t\t\t\t\t</tr>\n+\t\t\t\t\t\t\t\t{:else}\n+\t\t\t\t\t\t\t\t\t{#each filteredConversations as conv (conv.id)}\n+\t\t\t\t\t\t\t\t\t\t<tr\n+\t\t\t\t\t\t\t\t\t\t\tclass=\"cursor-pointer border-b transition-colors hover:bg-muted/50\"\n+\t\t\t\t\t\t\t\t\t\t\tonclick={(e) => toggleConversation(conv.id, e.shiftKey)}\n+\t\t\t\t\t\t\t\t\t\t>\n+\t\t\t\t\t\t\t\t\t\t\t<td class=\"p-3\">\n+\t\t\t\t\t\t\t\t\t\t\t\t<Checkbox\n+\t\t\t\t\t\t\t\t\t\t\t\t\tchecked={selectedIds.has(conv.id)}\n+\t\t\t\t\t\t\t\t\t\t\t\t\tonclick={(e) => {\n+\t\t\t\t\t\t\t\t\t\t\t\t\t\te.preventDefault();\n+\t\t\t\t\t\t\t\t\t\t\t\t\t\te.stopPropagation();\n+\t\t\t\t\t\t\t\t\t\t\t\t\t\ttoggleConversation(conv.id, e.shiftKey);\n+\t\t\t\t\t\t\t\t\t\t\t\t\t}}\n+\t\t\t\t\t\t\t\t\t\t\t\t/>\n+\t\t\t\t\t\t\t\t\t\t\t</td>\n+\n+\t\t\t\t\t\t\t\t\t\t\t<td class=\"p-3 text-sm\">\n+\t\t\t\t\t\t\t\t\t\t\t\t<div\n+\t\t\t\t\t\t\t\t\t\t\t\t\tclass=\"max-w-[17rem] truncate\"\n+\t\t\t\t\t\t\t\t\t\t\t\t\ttitle={conv.name || 'Untitled conversation'}\n+\t\t\t\t\t\t\t\t\t\t\t\t>\n+\t\t\t\t\t\t\t\t\t\t\t\t\t{conv.name || 'Untitled conversation'}\n+\t\t\t\t\t\t\t\t\t\t\t\t</div>\n+\t\t\t\t\t\t\t\t\t\t\t</td>\n+\n+\t\t\t\t\t\t\t\t\t\t\t<td class=\"p-3 text-sm text-muted-foreground\">\n+\t\t\t\t\t\t\t\t\t\t\t\t{messageCountMap.get(conv.id) ?? 0}\n+\t\t\t\t\t\t\t\t\t\t\t</td>\n+\t\t\t\t\t\t\t\t\t\t</tr>\n+\t\t\t\t\t\t\t\t\t{/each}\n+\t\t\t\t\t\t\t\t{/if}\n+\t\t\t\t\t\t\t</tbody>\n+\t\t\t\t\t\t</table>\n+\t\t\t\t\t</ScrollArea>\n+\t\t\t\t</div>\n+\t\t\t</div>\n+\n+\t\t\t<Dialog.Footer>\n+\t\t\t\t<Button variant=\"outline\" onclick={handleCancel}>Cancel</Button>\n+\n+\t\t\t\t<Button onclick={handleConfirm} disabled={selectedIds.size === 0}>\n+\t\t\t\t\t{mode === 'export' ? 'Export' : 'Import'} ({selectedIds.size})\n+\t\t\t\t</Button>\n+\t\t\t</Dialog.Footer>\n+\t\t</Dialog.Content>\n+\t</Dialog.Portal>\n+</Dialog.Root>"
      },
      {
        "filename": "tools/server/webui/src/lib/components/app/chat/ChatSettings/ImportExportTab.svelte",
        "status": "added",
        "additions": 255,
        "deletions": 0,
        "changes": 255,
        "patch": "@@ -0,0 +1,255 @@\n+<script lang=\"ts\">\n+\timport { Download, Upload } from '@lucide/svelte';\n+\timport { Button } from '$lib/components/ui/button';\n+\timport ConversationSelectionDialog from './ConversationSelectionDialog.svelte';\n+\timport { DatabaseStore } from '$lib/stores/database';\n+\timport type { ExportedConversations } from '$lib/types/database';\n+\timport { createMessageCountMap } from '$lib/utils/conversation-utils';\n+\timport { chatStore } from '$lib/stores/chat.svelte';\n+\n+\tlet exportedConversations = $state<DatabaseConversation[]>([]);\n+\tlet importedConversations = $state<DatabaseConversation[]>([]);\n+\tlet showExportSummary = $state(false);\n+\tlet showImportSummary = $state(false);\n+\n+\tlet showExportDialog = $state(false);\n+\tlet showImportDialog = $state(false);\n+\tlet availableConversations = $state<DatabaseConversation[]>([]);\n+\tlet messageCountMap = $state<Map<string, number>>(new Map());\n+\tlet fullImportData = $state<Array<{ conv: DatabaseConversation; messages: DatabaseMessage[] }>>(\n+\t\t[]\n+\t);\n+\n+\tasync function handleExportClick() {\n+\t\ttry {\n+\t\t\tconst allConversations = await DatabaseStore.getAllConversations();\n+\t\t\tif (allConversations.length === 0) {\n+\t\t\t\talert('No conversations to export');\n+\t\t\t\treturn;\n+\t\t\t}\n+\n+\t\t\tconst conversationsWithMessages = await Promise.all(\n+\t\t\t\tallConversations.map(async (conv) => {\n+\t\t\t\t\tconst messages = await DatabaseStore.getConversationMessages(conv.id);\n+\t\t\t\t\treturn { conv, messages };\n+\t\t\t\t})\n+\t\t\t);\n+\n+\t\t\tmessageCountMap = createMessageCountMap(conversationsWithMessages);\n+\t\t\tavailableConversations = allConversations;\n+\t\t\tshowExportDialog = true;\n+\t\t} catch (err) {\n+\t\t\tconsole.error('Failed to load conversations:', err);\n+\t\t\talert('Failed to load conversations');\n+\t\t}\n+\t}\n+\n+\tasync function handleExportConfirm(selectedConversations: DatabaseConversation[]) {\n+\t\ttry {\n+\t\t\tconst allData: ExportedConversations = await Promise.all(\n+\t\t\t\tselectedConversations.map(async (conv) => {\n+\t\t\t\t\tconst messages = await DatabaseStore.getConversationMessages(conv.id);\n+\t\t\t\t\treturn { conv: $state.snapshot(conv), messages: $state.snapshot(messages) };\n+\t\t\t\t})\n+\t\t\t);\n+\n+\t\t\tconst blob = new Blob([JSON.stringify(allData, null, 2)], {\n+\t\t\t\ttype: 'application/json'\n+\t\t\t});\n+\t\t\tconst url = URL.createObjectURL(blob);\n+\t\t\tconst a = document.createElement('a');\n+\n+\t\t\ta.href = url;\n+\t\t\ta.download = `conversations_${new Date().toISOString().split('T')[0]}.json`;\n+\t\t\tdocument.body.appendChild(a);\n+\t\t\ta.click();\n+\t\t\tdocument.body.removeChild(a);\n+\t\t\tURL.revokeObjectURL(url);\n+\n+\t\t\texportedConversations = selectedConversations;\n+\t\t\tshowExportSummary = true;\n+\t\t\tshowImportSummary = false;\n+\t\t\tshowExportDialog = false;\n+\t\t} catch (err) {\n+\t\t\tconsole.error('Export failed:', err);\n+\t\t\talert('Failed to export conversations');\n+\t\t}\n+\t}\n+\n+\tasync function handleImportClick() {\n+\t\ttry {\n+\t\t\tconst input = document.createElement('input');\n+\n+\t\t\tinput.type = 'file';\n+\t\t\tinput.accept = '.json';\n+\n+\t\t\tinput.onchange = async (e) => {\n+\t\t\t\tconst file = (e.target as HTMLInputElement)?.files?.[0];\n+\t\t\t\tif (!file) return;\n+\n+\t\t\t\ttry {\n+\t\t\t\t\tconst text = await file.text();\n+\t\t\t\t\tconst parsedData = JSON.parse(text);\n+\t\t\t\t\tlet importedData: ExportedConversations;\n+\n+\t\t\t\t\tif (Array.isArray(parsedData)) {\n+\t\t\t\t\t\timportedData = parsedData;\n+\t\t\t\t\t} else if (\n+\t\t\t\t\t\tparsedData &&\n+\t\t\t\t\t\ttypeof parsedData === 'object' &&\n+\t\t\t\t\t\t'conv' in parsedData &&\n+\t\t\t\t\t\t'messages' in parsedData\n+\t\t\t\t\t) {\n+\t\t\t\t\t\t// Single conversation object\n+\t\t\t\t\t\timportedData = [parsedData];\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tthrow new Error(\n+\t\t\t\t\t\t\t'Invalid file format: expected array of conversations or single conversation object'\n+\t\t\t\t\t\t);\n+\t\t\t\t\t}\n+\n+\t\t\t\t\tfullImportData = importedData;\n+\t\t\t\t\tavailableConversations = importedData.map(\n+\t\t\t\t\t\t(item: { conv: DatabaseConversation; messages: DatabaseMessage[] }) => item.conv\n+\t\t\t\t\t);\n+\t\t\t\t\tmessageCountMap = createMessageCountMap(importedData);\n+\t\t\t\t\tshowImportDialog = true;\n+\t\t\t\t} catch (err: unknown) {\n+\t\t\t\t\tconst message = err instanceof Error ? err.message : 'Unknown error';\n+\n+\t\t\t\t\tconsole.error('Failed to parse file:', err);\n+\t\t\t\t\talert(`Failed to parse file: ${message}`);\n+\t\t\t\t}\n+\t\t\t};\n+\n+\t\t\tinput.click();\n+\t\t} catch (err) {\n+\t\t\tconsole.error('Import failed:', err);\n+\t\t\talert('Failed to import conversations');\n+\t\t}\n+\t}\n+\n+\tasync function handleImportConfirm(selectedConversations: DatabaseConversation[]) {\n+\t\ttry {\n+\t\t\tconst selectedIds = new Set(selectedConversations.map((c) => c.id));\n+\t\t\tconst selectedData = $state\n+\t\t\t\t.snapshot(fullImportData)\n+\t\t\t\t.filter((item) => selectedIds.has(item.conv.id));\n+\n+\t\t\tawait DatabaseStore.importConversations(selectedData);\n+\n+\t\t\tawait chatStore.loadConversations();\n+\n+\t\t\timportedConversations = selectedConversations;\n+\t\t\tshowImportSummary = true;\n+\t\t\tshowExportSummary = false;\n+\t\t\tshowImportDialog = false;\n+\t\t} catch (err) {\n+\t\t\tconsole.error('Import failed:', err);\n+\t\t\talert('Failed to import conversations. Please check the file format.');\n+\t\t}\n+\t}\n+</script>\n+\n+<div class=\"space-y-6\">\n+\t<div class=\"space-y-4\">\n+\t\t<div class=\"grid\">\n+\t\t\t<h4 class=\"mb-2 text-sm font-medium\">Export Conversations</h4>\n+\n+\t\t\t<p class=\"mb-4 text-sm text-muted-foreground\">\n+\t\t\t\tDownload all your conversations as a JSON file. This includes all messages, attachments, and\n+\t\t\t\tconversation history.\n+\t\t\t</p>\n+\n+\t\t\t<Button\n+\t\t\t\tclass=\"w-full justify-start justify-self-start md:w-auto\"\n+\t\t\t\tonclick={handleExportClick}\n+\t\t\t\tvariant=\"outline\"\n+\t\t\t>\n+\t\t\t\t<Download class=\"mr-2 h-4 w-4\" />\n+\n+\t\t\t\tExport conversations\n+\t\t\t</Button>\n+\n+\t\t\t{#if showExportSummary && exportedConversations.length > 0}\n+\t\t\t\t<div class=\"mt-4 grid overflow-x-auto rounded-lg border border-border/50 bg-muted/30 p-4\">\n+\t\t\t\t\t<h5 class=\"mb-2 text-sm font-medium\">\n+\t\t\t\t\t\tExported {exportedConversations.length} conversation{exportedConversations.length === 1\n+\t\t\t\t\t\t\t? ''\n+\t\t\t\t\t\t\t: 's'}\n+\t\t\t\t\t</h5>\n+\n+\t\t\t\t\t<ul class=\"space-y-1 text-sm text-muted-foreground\">\n+\t\t\t\t\t\t{#each exportedConversations.slice(0, 10) as conv (conv.id)}\n+\t\t\t\t\t\t\t<li class=\"truncate\">\u2022 {conv.name || 'Untitled conversation'}</li>\n+\t\t\t\t\t\t{/each}\n+\n+\t\t\t\t\t\t{#if exportedConversations.length > 10}\n+\t\t\t\t\t\t\t<li class=\"italic\">\n+\t\t\t\t\t\t\t\t... and {exportedConversations.length - 10} more\n+\t\t\t\t\t\t\t</li>\n+\t\t\t\t\t\t{/if}\n+\t\t\t\t\t</ul>\n+\t\t\t\t</div>\n+\t\t\t{/if}\n+\t\t</div>\n+\n+\t\t<div class=\"grid border-t border-border/30 pt-4\">\n+\t\t\t<h4 class=\"mb-2 text-sm font-medium\">Import Conversations</h4>\n+\n+\t\t\t<p class=\"mb-4 text-sm text-muted-foreground\">\n+\t\t\t\tImport one or more conversations from a previously exported JSON file. This will merge with\n+\t\t\t\tyour existing conversations.\n+\t\t\t</p>\n+\n+\t\t\t<Button\n+\t\t\t\tclass=\"w-full justify-start justify-self-start md:w-auto\"\n+\t\t\t\tonclick={handleImportClick}\n+\t\t\t\tvariant=\"outline\"\n+\t\t\t>\n+\t\t\t\t<Upload class=\"mr-2 h-4 w-4\" />\n+\t\t\t\tImport conversations\n+\t\t\t</Button>\n+\n+\t\t\t{#if showImportSummary && importedConversations.length > 0}\n+\t\t\t\t<div class=\"mt-4 grid overflow-x-auto rounded-lg border border-border/50 bg-muted/30 p-4\">\n+\t\t\t\t\t<h5 class=\"mb-2 text-sm font-medium\">\n+\t\t\t\t\t\tImported {importedConversations.length} conversation{importedConversations.length === 1\n+\t\t\t\t\t\t\t? ''\n+\t\t\t\t\t\t\t: 's'}\n+\t\t\t\t\t</h5>\n+\n+\t\t\t\t\t<ul class=\"space-y-1 text-sm text-muted-foreground\">\n+\t\t\t\t\t\t{#each importedConversations.slice(0, 10) as conv (conv.id)}\n+\t\t\t\t\t\t\t<li class=\"truncate\">\u2022 {conv.name || 'Untitled conversation'}</li>\n+\t\t\t\t\t\t{/each}\n+\n+\t\t\t\t\t\t{#if importedConversations.length > 10}\n+\t\t\t\t\t\t\t<li class=\"italic\">\n+\t\t\t\t\t\t\t\t... and {importedConversations.length - 10} more\n+\t\t\t\t\t\t\t</li>\n+\t\t\t\t\t\t{/if}\n+\t\t\t\t\t</ul>\n+\t\t\t\t</div>\n+\t\t\t{/if}\n+\t\t</div>\n+\t</div>\n+</div>\n+\n+<ConversationSelectionDialog\n+\tconversations={availableConversations}\n+\t{messageCountMap}\n+\tmode=\"export\"\n+\tbind:open={showExportDialog}\n+\tonCancel={() => (showExportDialog = false)}\n+\tonConfirm={handleExportConfirm}\n+/>\n+\n+<ConversationSelectionDialog\n+\tconversations={availableConversations}\n+\t{messageCountMap}\n+\tmode=\"import\"\n+\tbind:open={showImportDialog}\n+\tonCancel={() => (showImportDialog = false)}\n+\tonConfirm={handleImportConfirm}\n+/>"
      },
      {
        "filename": "tools/server/webui/src/lib/components/app/chat/ChatSidebar/ChatSidebarActions.svelte",
        "status": "modified",
        "additions": 1,
        "deletions": 31,
        "changes": 32,
        "patch": "@@ -1,9 +1,8 @@\n <script lang=\"ts\">\n-\timport { Search, SquarePen, X, Download, Upload } from '@lucide/svelte';\n+\timport { Search, SquarePen, X } from '@lucide/svelte';\n \timport { KeyboardShortcutInfo } from '$lib/components/app';\n \timport { Button } from '$lib/components/ui/button';\n \timport { Input } from '$lib/components/ui/input';\n-\timport { exportAllConversations, importConversations } from '$lib/stores/chat.svelte';\n \n \tinterface Props {\n \t\thandleMobileSidebarItemClick: () => void;\n@@ -78,34 +77,5 @@\n \n \t\t\t<KeyboardShortcutInfo keys={['cmd', 'k']} />\n \t\t</Button>\n-\n-\t\t<Button\n-\t\t\tclass=\"w-full justify-start text-sm\"\n-\t\t\tonclick={() => {\n-\t\t\t\timportConversations().catch((err) => {\n-\t\t\t\t\tconsole.error('Import failed:', err);\n-\t\t\t\t\t// Optional: show toast or dialog\n-\t\t\t\t});\n-\t\t\t}}\n-\t\t\tvariant=\"ghost\"\n-\t\t>\n-\t\t\t<div class=\"flex items-center gap-2\">\n-\t\t\t\t<Upload class=\"h-4 w-4\" />\n-\t\t\t\tImport conversations\n-\t\t\t</div>\n-\t\t</Button>\n-\n-\t\t<Button\n-\t\t\tclass=\"w-full justify-start text-sm\"\n-\t\t\tonclick={() => {\n-\t\t\t\texportAllConversations();\n-\t\t\t}}\n-\t\t\tvariant=\"ghost\"\n-\t\t>\n-\t\t\t<div class=\"flex items-center gap-2\">\n-\t\t\t\t<Download class=\"h-4 w-4\" />\n-\t\t\t\tExport all conversations\n-\t\t\t</div>\n-\t\t</Button>\n \t{/if}\n </div>"
      },
      {
        "filename": "tools/server/webui/src/lib/components/app/index.ts",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -25,6 +25,8 @@ export { default as ChatScreen } from './chat/ChatScreen/ChatScreen.svelte';\n export { default as ChatSettingsDialog } from './chat/ChatSettings/ChatSettingsDialog.svelte';\n export { default as ChatSettingsFooter } from './chat/ChatSettings/ChatSettingsFooter.svelte';\n export { default as ChatSettingsFields } from './chat/ChatSettings/ChatSettingsFields.svelte';\n+export { default as ImportExportTab } from './chat/ChatSettings/ImportExportTab.svelte';\n+export { default as ConversationSelectionDialog } from './chat/ChatSettings/ConversationSelectionDialog.svelte';\n export { default as ParameterSourceIndicator } from './chat/ChatSettings/ParameterSourceIndicator.svelte';\n \n export { default as ChatSidebar } from './chat/ChatSidebar/ChatSidebar.svelte';"
      },
      {
        "filename": "tools/server/webui/src/lib/stores/chat.svelte.ts",
        "status": "modified",
        "additions": 8,
        "deletions": 3,
        "changes": 11,
        "patch": "@@ -1040,8 +1040,9 @@ class ChatStore {\n \n \t/**\n \t * Exports all conversations with their messages as a JSON file\n+\t * Returns the list of exported conversations\n \t */\n-\tasync exportAllConversations(): Promise<void> {\n+\tasync exportAllConversations(): Promise<DatabaseConversation[]> {\n \t\ttry {\n \t\t\tconst allConversations = await DatabaseStore.getAllConversations();\n \t\t\tif (allConversations.length === 0) {\n@@ -1068,6 +1069,7 @@ class ChatStore {\n \t\t\tURL.revokeObjectURL(url);\n \n \t\t\ttoast.success(`All conversations (${allConversations.length}) prepared for download`);\n+\t\t\treturn allConversations;\n \t\t} catch (err) {\n \t\t\tconsole.error('Failed to export conversations:', err);\n \t\t\tthrow err;\n@@ -1078,8 +1080,9 @@ class ChatStore {\n \t * Imports conversations from a JSON file.\n \t * Supports both single conversation (object) and multiple conversations (array).\n \t * Uses DatabaseStore for safe, encapsulated data access\n+\t * Returns the list of imported conversations\n \t */\n-\tasync importConversations(): Promise<void> {\n+\tasync importConversations(): Promise<DatabaseConversation[]> {\n \t\treturn new Promise((resolve, reject) => {\n \t\t\tconst input = document.createElement('input');\n \t\t\tinput.type = 'file';\n@@ -1120,7 +1123,9 @@ class ChatStore {\n \n \t\t\t\t\ttoast.success(`Imported ${result.imported} conversation(s), skipped ${result.skipped}`);\n \n-\t\t\t\t\tresolve(undefined);\n+\t\t\t\t\t// Extract the conversation objects from imported data\n+\t\t\t\t\tconst importedConversations = importedData.map((item) => item.conv);\n+\t\t\t\t\tresolve(importedConversations);\n \t\t\t\t} catch (err: unknown) {\n \t\t\t\t\tconst message = err instanceof Error ? err.message : 'Unknown error';\n \t\t\t\t\tconsole.error('Failed to import conversations:', err);"
      },
      {
        "filename": "tools/server/webui/src/lib/utils/conversation-utils.ts",
        "status": "added",
        "additions": 30,
        "deletions": 0,
        "changes": 30,
        "patch": "@@ -0,0 +1,30 @@\n+/**\n+ * Utility functions for conversation data manipulation\n+ */\n+\n+/**\n+ * Creates a map of conversation IDs to their message counts from exported conversation data\n+ * @param exportedData - Array of exported conversations with their messages\n+ * @returns Map of conversation ID to message count\n+ */\n+export function createMessageCountMap(\n+\texportedData: Array<{ conv: DatabaseConversation; messages: DatabaseMessage[] }>\n+): Map<string, number> {\n+\tconst countMap = new Map<string, number>();\n+\n+\tfor (const item of exportedData) {\n+\t\tcountMap.set(item.conv.id, item.messages.length);\n+\t}\n+\n+\treturn countMap;\n+}\n+\n+/**\n+ * Gets the message count for a specific conversation from the count map\n+ * @param conversationId - The ID of the conversation\n+ * @param countMap - Map of conversation IDs to message counts\n+ * @returns The message count, or 0 if not found\n+ */\n+export function getMessageCount(conversationId: string, countMap: Map<string, number>): number {\n+\treturn countMap.get(conversationId) ?? 0;\n+}"
      }
    ],
    "num_files": 8,
    "scraped_at": "2025-11-16T23:29:55.236790"
  },
  {
    "pr_number": 16618,
    "title": "webui: add OAI-Compat Harmony tool-call streaming visualization and persistence in chat UI",
    "body": "    - Purely visual and diagnostic change, no effect on model context, prompt\r\n      construction, or inference behavior\r\n\r\n    - Captured assistant tool call payloads during streaming and non-streaming\r\n      completions, and persisted them in chat state and storage for downstream use\r\n\r\n    - Exposed parsed tool call labels beneath the assistant's model info line\r\n      with graceful fallback when parsing fails\r\n\r\n    - Added tool call badges beneath assistant responses that expose JSON tooltips\r\n      and copy their payloads when clicked, matching the existing model badge styling\r\n\r\n    - Added a user-facing setting to toggle tool call visibility to the Developer\r\n      settings section directly under the model selector option\r\n\r\n<img width=\"1701\" height=\"772\" alt=\"1\" src=\"https://github.com/user-attachments/assets/e5e3ad12-809f-4cb5-90e8-7c5199989afb\" />\r\n\r\n<img width=\"815\" height=\"535\" alt=\"2\" src=\"https://github.com/user-attachments/assets/0538d0da-1593-4b31-adbf-d08b859c4ccb\" />\r\n\r\nClose https://github.com/ggml-org/llama.cpp/issues/16597\r\n",
    "html_url": "https://github.com/ggml-org/llama.cpp/pull/16618",
    "created_at": "2025-10-16T17:17:13Z",
    "merged_at": "2025-11-15T20:09:32Z",
    "merge_commit_sha": "1411d9275ad7d2af44543fb9c1e64eea1e1c8de7",
    "base_ref": "master",
    "head_sha": "b1b7ecfd7283a539b8e7da9f643c07fa47708533",
    "user": "ServeurpersoCom",
    "files": [
      {
        "filename": "tools/server/public/index.html.gz",
        "status": "modified",
        "additions": 0,
        "deletions": 0,
        "changes": 0,
        "patch": ""
      },
      {
        "filename": "tools/server/webui/src/lib/components/app/chat/ChatForm/ChatFormModelSelector.svelte",
        "status": "modified",
        "additions": 1,
        "deletions": 7,
        "changes": 8,
        "patch": "@@ -72,12 +72,6 @@\n \t\t}\n \t}\n \n-\tfunction handleScroll() {\n-\t\tif (isOpen) {\n-\t\t\tupdateMenuPosition();\n-\t\t}\n-\t}\n-\n \tasync function handleSelect(value: string | undefined) {\n \t\tif (!value) return;\n \n@@ -259,7 +253,7 @@\n \t}\n </script>\n \n-<svelte:window onresize={handleResize} onscroll={handleScroll} />\n+<svelte:window onresize={handleResize} />\n \n <svelte:document onpointerdown={handlePointerDown} onkeydown={handleKeydown} />\n "
      },
      {
        "filename": "tools/server/webui/src/lib/components/app/chat/ChatMessages/ChatMessage.svelte",
        "status": "modified",
        "additions": 25,
        "deletions": 0,
        "changes": 25,
        "patch": "@@ -2,6 +2,7 @@\n \timport { getDeletionInfo } from '$lib/stores/chat.svelte';\n \timport { copyToClipboard } from '$lib/utils/copy';\n \timport { isIMEComposing } from '$lib/utils/is-ime-composing';\n+\timport type { ApiChatCompletionToolCall } from '$lib/types/api';\n \timport ChatMessageAssistant from './ChatMessageAssistant.svelte';\n \timport ChatMessageUser from './ChatMessageUser.svelte';\n \n@@ -54,6 +55,29 @@\n \t\treturn null;\n \t});\n \n+\tlet toolCallContent = $derived.by((): ApiChatCompletionToolCall[] | string | null => {\n+\t\tif (message.role === 'assistant') {\n+\t\t\tconst trimmedToolCalls = message.toolCalls?.trim();\n+\n+\t\t\tif (!trimmedToolCalls) {\n+\t\t\t\treturn null;\n+\t\t\t}\n+\n+\t\t\ttry {\n+\t\t\t\tconst parsed = JSON.parse(trimmedToolCalls);\n+\n+\t\t\t\tif (Array.isArray(parsed)) {\n+\t\t\t\t\treturn parsed as ApiChatCompletionToolCall[];\n+\t\t\t\t}\n+\t\t\t} catch {\n+\t\t\t\t// Harmony-only path: fall back to the raw string so issues surface visibly.\n+\t\t\t}\n+\n+\t\t\treturn trimmedToolCalls;\n+\t\t}\n+\t\treturn null;\n+\t});\n+\n \tfunction handleCancelEdit() {\n \t\tisEditing = false;\n \t\teditedContent = message.content;\n@@ -171,5 +195,6 @@\n \t\t{showDeleteDialog}\n \t\t{siblingInfo}\n \t\t{thinkingContent}\n+\t\t{toolCallContent}\n \t/>\n {/if}"
      },
      {
        "filename": "tools/server/webui/src/lib/components/app/chat/ChatMessages/ChatMessageAssistant.svelte",
        "status": "modified",
        "additions": 117,
        "deletions": 2,
        "changes": 119,
        "patch": "@@ -11,7 +11,8 @@\n \t\tGauge,\n \t\tClock,\n \t\tWholeWord,\n-\t\tChartNoAxesColumn\n+\t\tChartNoAxesColumn,\n+\t\tWrench\n \t} from '@lucide/svelte';\n \timport { Button } from '$lib/components/ui/button';\n \timport { Checkbox } from '$lib/components/ui/checkbox';\n@@ -21,6 +22,7 @@\n \timport { config } from '$lib/stores/settings.svelte';\n \timport { modelName as serverModelName } from '$lib/stores/server.svelte';\n \timport { copyToClipboard } from '$lib/utils/copy';\n+\timport type { ApiChatCompletionToolCall } from '$lib/types/api';\n \n \tinterface Props {\n \t\tclass?: string;\n@@ -51,6 +53,7 @@\n \t\tsiblingInfo?: ChatMessageSiblingInfo | null;\n \t\ttextareaElement?: HTMLTextAreaElement;\n \t\tthinkingContent: string | null;\n+\t\ttoolCallContent: ApiChatCompletionToolCall[] | string | null;\n \t}\n \n \tlet {\n@@ -76,9 +79,15 @@\n \t\tshouldBranchAfterEdit = false,\n \t\tsiblingInfo = null,\n \t\ttextareaElement = $bindable(),\n-\t\tthinkingContent\n+\t\tthinkingContent,\n+\t\ttoolCallContent = null\n \t}: Props = $props();\n \n+\tconst toolCalls = $derived(\n+\t\tArray.isArray(toolCallContent) ? (toolCallContent as ApiChatCompletionToolCall[]) : null\n+\t);\n+\tconst fallbackToolCalls = $derived(typeof toolCallContent === 'string' ? toolCallContent : null);\n+\n \tconst processingState = useProcessingState();\n \tlet currentConfig = $derived(config());\n \tlet serverModel = $derived(serverModelName());\n@@ -97,6 +106,58 @@\n \n \t\tvoid copyToClipboard(model ?? '');\n \t}\n+\n+\tfunction formatToolCallBadge(toolCall: ApiChatCompletionToolCall, index: number) {\n+\t\tconst callNumber = index + 1;\n+\t\tconst functionName = toolCall.function?.name?.trim();\n+\t\tconst label = functionName || `Call #${callNumber}`;\n+\n+\t\tconst payload: Record<string, unknown> = {};\n+\n+\t\tconst id = toolCall.id?.trim();\n+\t\tif (id) {\n+\t\t\tpayload.id = id;\n+\t\t}\n+\n+\t\tconst type = toolCall.type?.trim();\n+\t\tif (type) {\n+\t\t\tpayload.type = type;\n+\t\t}\n+\n+\t\tif (toolCall.function) {\n+\t\t\tconst fnPayload: Record<string, unknown> = {};\n+\n+\t\t\tconst name = toolCall.function.name?.trim();\n+\t\t\tif (name) {\n+\t\t\t\tfnPayload.name = name;\n+\t\t\t}\n+\n+\t\t\tconst rawArguments = toolCall.function.arguments?.trim();\n+\t\t\tif (rawArguments) {\n+\t\t\t\ttry {\n+\t\t\t\t\tfnPayload.arguments = JSON.parse(rawArguments);\n+\t\t\t\t} catch {\n+\t\t\t\t\tfnPayload.arguments = rawArguments;\n+\t\t\t\t}\n+\t\t\t}\n+\n+\t\t\tif (Object.keys(fnPayload).length > 0) {\n+\t\t\t\tpayload.function = fnPayload;\n+\t\t\t}\n+\t\t}\n+\n+\t\tconst formattedPayload = JSON.stringify(payload, null, 2);\n+\n+\t\treturn {\n+\t\t\tlabel,\n+\t\t\ttooltip: formattedPayload,\n+\t\t\tcopyValue: formattedPayload\n+\t\t};\n+\t}\n+\n+\tfunction handleCopyToolCall(payload: string) {\n+\t\tvoid copyToClipboard(payload, 'Tool call copied to clipboard');\n+\t}\n </script>\n \n <div\n@@ -189,6 +250,47 @@\n \t\t\t</span>\n \t\t{/if}\n \n+\t\t{#if config().showToolCalls}\n+\t\t\t{#if (toolCalls && toolCalls.length > 0) || fallbackToolCalls}\n+\t\t\t\t<span class=\"inline-flex flex-wrap items-center gap-2 text-xs text-muted-foreground\">\n+\t\t\t\t\t<span class=\"inline-flex items-center gap-1\">\n+\t\t\t\t\t\t<Wrench class=\"h-3.5 w-3.5\" />\n+\n+\t\t\t\t\t\t<span>Tool calls:</span>\n+\t\t\t\t\t</span>\n+\n+\t\t\t\t\t{#if toolCalls && toolCalls.length > 0}\n+\t\t\t\t\t\t{#each toolCalls as toolCall, index (toolCall.id ?? `${index}`)}\n+\t\t\t\t\t\t\t{@const badge = formatToolCallBadge(toolCall, index)}\n+\t\t\t\t\t\t\t<button\n+\t\t\t\t\t\t\t\ttype=\"button\"\n+\t\t\t\t\t\t\t\tclass=\"tool-call-badge inline-flex cursor-pointer items-center gap-1 rounded-sm bg-muted-foreground/15 px-1.5 py-0.75\"\n+\t\t\t\t\t\t\t\ttitle={badge.tooltip}\n+\t\t\t\t\t\t\t\taria-label={`Copy tool call ${badge.label}`}\n+\t\t\t\t\t\t\t\tonclick={() => handleCopyToolCall(badge.copyValue)}\n+\t\t\t\t\t\t\t>\n+\t\t\t\t\t\t\t\t{badge.label}\n+\n+\t\t\t\t\t\t\t\t<Copy class=\"ml-1 h-3 w-3\" />\n+\t\t\t\t\t\t\t</button>\n+\t\t\t\t\t\t{/each}\n+\t\t\t\t\t{:else if fallbackToolCalls}\n+\t\t\t\t\t\t<button\n+\t\t\t\t\t\t\ttype=\"button\"\n+\t\t\t\t\t\t\tclass=\"tool-call-badge tool-call-badge--fallback inline-flex cursor-pointer items-center gap-1 rounded-sm bg-muted-foreground/15 px-1.5 py-0.75\"\n+\t\t\t\t\t\t\ttitle={fallbackToolCalls}\n+\t\t\t\t\t\t\taria-label=\"Copy tool call payload\"\n+\t\t\t\t\t\t\tonclick={() => handleCopyToolCall(fallbackToolCalls)}\n+\t\t\t\t\t\t>\n+\t\t\t\t\t\t\t{fallbackToolCalls}\n+\n+\t\t\t\t\t\t\t<Copy class=\"ml-1 h-3 w-3\" />\n+\t\t\t\t\t\t</button>\n+\t\t\t\t\t{/if}\n+\t\t\t\t</span>\n+\t\t\t{/if}\n+\t\t{/if}\n+\n \t\t{#if currentConfig.showMessageStats && message.timings && message.timings.predicted_n && message.timings.predicted_ms}\n \t\t\t{@const tokensPerSecond = (message.timings.predicted_n / message.timings.predicted_ms) * 1000}\n \t\t\t<span class=\"inline-flex items-center gap-2 text-xs text-muted-foreground\">\n@@ -287,4 +389,17 @@\n \t\twhite-space: pre-wrap;\n \t\tword-break: break-word;\n \t}\n+\n+\t.tool-call-badge {\n+\t\tmax-width: 12rem;\n+\t\twhite-space: nowrap;\n+\t\toverflow: hidden;\n+\t\ttext-overflow: ellipsis;\n+\t}\n+\n+\t.tool-call-badge--fallback {\n+\t\tmax-width: 20rem;\n+\t\twhite-space: normal;\n+\t\tword-break: break-word;\n+\t}\n </style>"
      },
      {
        "filename": "tools/server/webui/src/lib/components/app/chat/ChatSettings/ChatSettingsDialog.svelte",
        "status": "modified",
        "additions": 5,
        "deletions": 0,
        "changes": 5,
        "patch": "@@ -226,6 +226,11 @@\n \t\t\t\t\tlabel: 'Enable model selector',\n \t\t\t\t\ttype: 'checkbox'\n \t\t\t\t},\n+\t\t\t\t{\n+\t\t\t\t\tkey: 'showToolCalls',\n+\t\t\t\t\tlabel: 'Show tool call labels',\n+\t\t\t\t\ttype: 'checkbox'\n+\t\t\t\t},\n \t\t\t\t{\n \t\t\t\t\tkey: 'disableReasoningFormat',\n \t\t\t\t\tlabel: 'Show raw LLM output',"
      },
      {
        "filename": "tools/server/webui/src/lib/constants/settings-config.ts",
        "status": "modified",
        "additions": 3,
        "deletions": 0,
        "changes": 3,
        "patch": "@@ -6,6 +6,7 @@ export const SETTING_CONFIG_DEFAULT: Record<string, string | number | boolean> =\n \ttheme: 'system',\n \tshowTokensPerSecond: false,\n \tshowThoughtInProgress: false,\n+\tshowToolCalls: false,\n \tdisableReasoningFormat: false,\n \tkeepStatsVisible: false,\n \tshowMessageStats: true,\n@@ -80,6 +81,8 @@ export const SETTING_CONFIG_INFO: Record<string, string> = {\n \tcustom: 'Custom JSON parameters to send to the API. Must be valid JSON format.',\n \tshowTokensPerSecond: 'Display generation speed in tokens per second during streaming.',\n \tshowThoughtInProgress: 'Expand thought process by default when generating messages.',\n+\tshowToolCalls:\n+\t\t'Display tool call labels and payloads from Harmony-compatible delta.tool_calls data below assistant messages.',\n \tdisableReasoningFormat:\n \t\t'Show raw LLM output without backend parsing and frontend Markdown rendering to inspect streaming across different models.',\n \tkeepStatsVisible: 'Keep processing statistics visible after generation finishes.',"
      },
      {
        "filename": "tools/server/webui/src/lib/services/chat.ts",
        "status": "modified",
        "additions": 161,
        "deletions": 7,
        "changes": 168,
        "patch": "@@ -1,6 +1,25 @@\n import { config } from '$lib/stores/settings.svelte';\n import { selectedModelName } from '$lib/stores/models.svelte';\n import { slotsService } from './slots';\n+import type {\n+\tApiChatCompletionRequest,\n+\tApiChatCompletionResponse,\n+\tApiChatCompletionStreamChunk,\n+\tApiChatCompletionToolCall,\n+\tApiChatCompletionToolCallDelta,\n+\tApiChatMessageData\n+} from '$lib/types/api';\n+import type {\n+\tDatabaseMessage,\n+\tDatabaseMessageExtra,\n+\tDatabaseMessageExtraAudioFile,\n+\tDatabaseMessageExtraImageFile,\n+\tDatabaseMessageExtraLegacyContext,\n+\tDatabaseMessageExtraPdfFile,\n+\tDatabaseMessageExtraTextFile\n+} from '$lib/types/database';\n+import type { ChatMessagePromptProgress, ChatMessageTimings } from '$lib/types/chat';\n+import type { SettingsChatServiceOptions } from '$lib/types/settings';\n /**\n  * ChatService - Low-level API communication layer for llama.cpp server interactions\n  *\n@@ -53,6 +72,7 @@ export class ChatService {\n \t\t\tonComplete,\n \t\t\tonError,\n \t\t\tonReasoningChunk,\n+\t\t\tonToolCallChunk,\n \t\t\tonModel,\n \t\t\tonFirstValidChunk,\n \t\t\t// Generation parameters\n@@ -201,14 +221,21 @@ export class ChatService {\n \t\t\t\t\tonComplete,\n \t\t\t\t\tonError,\n \t\t\t\t\tonReasoningChunk,\n+\t\t\t\t\tonToolCallChunk,\n \t\t\t\t\tonModel,\n \t\t\t\t\tonFirstValidChunk,\n \t\t\t\t\tconversationId,\n \t\t\t\t\tabortController.signal\n \t\t\t\t);\n \t\t\t\treturn;\n \t\t\t} else {\n-\t\t\t\treturn this.handleNonStreamResponse(response, onComplete, onError, onModel);\n+\t\t\t\treturn this.handleNonStreamResponse(\n+\t\t\t\t\tresponse,\n+\t\t\t\t\tonComplete,\n+\t\t\t\t\tonError,\n+\t\t\t\t\tonToolCallChunk,\n+\t\t\t\t\tonModel\n+\t\t\t\t);\n \t\t\t}\n \t\t} catch (error) {\n \t\t\tif (error instanceof Error && error.name === 'AbortError') {\n@@ -264,10 +291,12 @@ export class ChatService {\n \t\tonComplete?: (\n \t\t\tresponse: string,\n \t\t\treasoningContent?: string,\n-\t\t\ttimings?: ChatMessageTimings\n+\t\t\ttimings?: ChatMessageTimings,\n+\t\t\ttoolCalls?: string\n \t\t) => void,\n \t\tonError?: (error: Error) => void,\n \t\tonReasoningChunk?: (chunk: string) => void,\n+\t\tonToolCallChunk?: (chunk: string) => void,\n \t\tonModel?: (model: string) => void,\n \t\tonFirstValidChunk?: () => void,\n \t\tconversationId?: string,\n@@ -282,11 +311,53 @@ export class ChatService {\n \t\tconst decoder = new TextDecoder();\n \t\tlet aggregatedContent = '';\n \t\tlet fullReasoningContent = '';\n+\t\tlet aggregatedToolCalls: ApiChatCompletionToolCall[] = [];\n \t\tlet hasReceivedData = false;\n \t\tlet lastTimings: ChatMessageTimings | undefined;\n \t\tlet streamFinished = false;\n \t\tlet modelEmitted = false;\n \t\tlet firstValidChunkEmitted = false;\n+\t\tlet toolCallIndexOffset = 0;\n+\t\tlet hasOpenToolCallBatch = false;\n+\n+\t\tconst finalizeOpenToolCallBatch = () => {\n+\t\t\tif (!hasOpenToolCallBatch) {\n+\t\t\t\treturn;\n+\t\t\t}\n+\n+\t\t\ttoolCallIndexOffset = aggregatedToolCalls.length;\n+\t\t\thasOpenToolCallBatch = false;\n+\t\t};\n+\n+\t\tconst processToolCallDelta = (toolCalls?: ApiChatCompletionToolCallDelta[]) => {\n+\t\t\tif (!toolCalls || toolCalls.length === 0) {\n+\t\t\t\treturn;\n+\t\t\t}\n+\n+\t\t\taggregatedToolCalls = this.mergeToolCallDeltas(\n+\t\t\t\taggregatedToolCalls,\n+\t\t\t\ttoolCalls,\n+\t\t\t\ttoolCallIndexOffset\n+\t\t\t);\n+\n+\t\t\tif (aggregatedToolCalls.length === 0) {\n+\t\t\t\treturn;\n+\t\t\t}\n+\n+\t\t\thasOpenToolCallBatch = true;\n+\n+\t\t\tconst serializedToolCalls = JSON.stringify(aggregatedToolCalls);\n+\n+\t\t\tif (!serializedToolCalls) {\n+\t\t\t\treturn;\n+\t\t\t}\n+\n+\t\t\thasReceivedData = true;\n+\n+\t\t\tif (!abortSignal?.aborted) {\n+\t\t\t\tonToolCallChunk?.(serializedToolCalls);\n+\t\t\t}\n+\t\t};\n \n \t\ttry {\n \t\t\tlet chunk = '';\n@@ -325,6 +396,7 @@ export class ChatService {\n \n \t\t\t\t\t\t\tconst content = parsed.choices[0]?.delta?.content;\n \t\t\t\t\t\t\tconst reasoningContent = parsed.choices[0]?.delta?.reasoning_content;\n+\t\t\t\t\t\t\tconst toolCalls = parsed.choices[0]?.delta?.tool_calls;\n \t\t\t\t\t\t\tconst timings = parsed.timings;\n \t\t\t\t\t\t\tconst promptProgress = parsed.prompt_progress;\n \n@@ -342,6 +414,7 @@ export class ChatService {\n \t\t\t\t\t\t\t}\n \n \t\t\t\t\t\t\tif (content) {\n+\t\t\t\t\t\t\t\tfinalizeOpenToolCallBatch();\n \t\t\t\t\t\t\t\thasReceivedData = true;\n \t\t\t\t\t\t\t\taggregatedContent += content;\n \t\t\t\t\t\t\t\tif (!abortSignal?.aborted) {\n@@ -350,12 +423,15 @@ export class ChatService {\n \t\t\t\t\t\t\t}\n \n \t\t\t\t\t\t\tif (reasoningContent) {\n+\t\t\t\t\t\t\t\tfinalizeOpenToolCallBatch();\n \t\t\t\t\t\t\t\thasReceivedData = true;\n \t\t\t\t\t\t\t\tfullReasoningContent += reasoningContent;\n \t\t\t\t\t\t\t\tif (!abortSignal?.aborted) {\n \t\t\t\t\t\t\t\t\tonReasoningChunk?.(reasoningContent);\n \t\t\t\t\t\t\t\t}\n \t\t\t\t\t\t\t}\n+\n+\t\t\t\t\t\t\tprocessToolCallDelta(toolCalls);\n \t\t\t\t\t\t} catch (e) {\n \t\t\t\t\t\t\tconsole.error('Error parsing JSON chunk:', e);\n \t\t\t\t\t\t}\n@@ -368,12 +444,26 @@ export class ChatService {\n \t\t\tif (abortSignal?.aborted) return;\n \n \t\t\tif (streamFinished) {\n-\t\t\t\tif (!hasReceivedData && aggregatedContent.length === 0) {\n+\t\t\t\tfinalizeOpenToolCallBatch();\n+\n+\t\t\t\tif (\n+\t\t\t\t\t!hasReceivedData &&\n+\t\t\t\t\taggregatedContent.length === 0 &&\n+\t\t\t\t\taggregatedToolCalls.length === 0\n+\t\t\t\t) {\n \t\t\t\t\tconst noResponseError = new Error('No response received from server. Please try again.');\n \t\t\t\t\tthrow noResponseError;\n \t\t\t\t}\n \n-\t\t\t\tonComplete?.(aggregatedContent, fullReasoningContent || undefined, lastTimings);\n+\t\t\t\tconst finalToolCalls =\n+\t\t\t\t\taggregatedToolCalls.length > 0 ? JSON.stringify(aggregatedToolCalls) : undefined;\n+\n+\t\t\t\tonComplete?.(\n+\t\t\t\t\taggregatedContent,\n+\t\t\t\t\tfullReasoningContent || undefined,\n+\t\t\t\t\tlastTimings,\n+\t\t\t\t\tfinalToolCalls\n+\t\t\t\t);\n \t\t\t}\n \t\t} catch (error) {\n \t\t\tconst err = error instanceof Error ? error : new Error('Stream error');\n@@ -386,6 +476,54 @@ export class ChatService {\n \t\t}\n \t}\n \n+\tprivate mergeToolCallDeltas(\n+\t\texisting: ApiChatCompletionToolCall[],\n+\t\tdeltas: ApiChatCompletionToolCallDelta[],\n+\t\tindexOffset = 0\n+\t): ApiChatCompletionToolCall[] {\n+\t\tconst result = existing.map((call) => ({\n+\t\t\t...call,\n+\t\t\tfunction: call.function ? { ...call.function } : undefined\n+\t\t}));\n+\n+\t\tfor (const delta of deltas) {\n+\t\t\tconst index =\n+\t\t\t\ttypeof delta.index === 'number' && delta.index >= 0\n+\t\t\t\t\t? delta.index + indexOffset\n+\t\t\t\t\t: result.length;\n+\n+\t\t\twhile (result.length <= index) {\n+\t\t\t\tresult.push({ function: undefined });\n+\t\t\t}\n+\n+\t\t\tconst target = result[index]!;\n+\n+\t\t\tif (delta.id) {\n+\t\t\t\ttarget.id = delta.id;\n+\t\t\t}\n+\n+\t\t\tif (delta.type) {\n+\t\t\t\ttarget.type = delta.type;\n+\t\t\t}\n+\n+\t\t\tif (delta.function) {\n+\t\t\t\tconst fn = target.function ? { ...target.function } : {};\n+\n+\t\t\t\tif (delta.function.name) {\n+\t\t\t\t\tfn.name = delta.function.name;\n+\t\t\t\t}\n+\n+\t\t\t\tif (delta.function.arguments) {\n+\t\t\t\t\tfn.arguments = (fn.arguments ?? '') + delta.function.arguments;\n+\t\t\t\t}\n+\n+\t\t\t\ttarget.function = fn;\n+\t\t\t}\n+\t\t}\n+\n+\t\treturn result;\n+\t}\n+\n \t/**\n \t * Handles non-streaming response from the chat completion API.\n \t * Parses the JSON response and extracts the generated content.\n@@ -401,9 +539,11 @@ export class ChatService {\n \t\tonComplete?: (\n \t\t\tresponse: string,\n \t\t\treasoningContent?: string,\n-\t\t\ttimings?: ChatMessageTimings\n+\t\t\ttimings?: ChatMessageTimings,\n+\t\t\ttoolCalls?: string\n \t\t) => void,\n \t\tonError?: (error: Error) => void,\n+\t\tonToolCallChunk?: (chunk: string) => void,\n \t\tonModel?: (model: string) => void\n \t): Promise<string> {\n \t\ttry {\n@@ -423,17 +563,31 @@ export class ChatService {\n \n \t\t\tconst content = data.choices[0]?.message?.content || '';\n \t\t\tconst reasoningContent = data.choices[0]?.message?.reasoning_content;\n+\t\t\tconst toolCalls = data.choices[0]?.message?.tool_calls;\n \n \t\t\tif (reasoningContent) {\n \t\t\t\tconsole.log('Full reasoning content:', reasoningContent);\n \t\t\t}\n \n-\t\t\tif (!content.trim()) {\n+\t\t\tlet serializedToolCalls: string | undefined;\n+\n+\t\t\tif (toolCalls && toolCalls.length > 0) {\n+\t\t\t\tconst mergedToolCalls = this.mergeToolCallDeltas([], toolCalls);\n+\n+\t\t\t\tif (mergedToolCalls.length > 0) {\n+\t\t\t\t\tserializedToolCalls = JSON.stringify(mergedToolCalls);\n+\t\t\t\t\tif (serializedToolCalls) {\n+\t\t\t\t\t\tonToolCallChunk?.(serializedToolCalls);\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\n+\t\t\tif (!content.trim() && !serializedToolCalls) {\n \t\t\t\tconst noResponseError = new Error('No response received from server. Please try again.');\n \t\t\t\tthrow noResponseError;\n \t\t\t}\n \n-\t\t\tonComplete?.(content, reasoningContent);\n+\t\t\tonComplete?.(content, reasoningContent, undefined, serializedToolCalls);\n \n \t\t\treturn content;\n \t\t} catch (error) {"
      },
      {
        "filename": "tools/server/webui/src/lib/stores/chat.svelte.ts",
        "status": "modified",
        "additions": 34,
        "deletions": 2,
        "changes": 36,
        "patch": "@@ -205,6 +205,7 @@ class ChatStore {\n \t\t\t\t\ttype,\n \t\t\t\t\ttimestamp: Date.now(),\n \t\t\t\t\tthinking: '',\n+\t\t\t\t\ttoolCalls: '',\n \t\t\t\t\tchildren: [],\n \t\t\t\t\textra: extras\n \t\t\t\t},\n@@ -360,6 +361,7 @@ class ChatStore {\n \t): Promise<void> {\n \t\tlet streamedContent = '';\n \t\tlet streamedReasoningContent = '';\n+\t\tlet streamedToolCallContent = '';\n \n \t\tlet resolvedModel: string | null = null;\n \t\tlet modelPersisted = false;\n@@ -468,25 +470,42 @@ class ChatStore {\n \t\t\t\t\tthis.updateMessageAtIndex(messageIndex, { thinking: streamedReasoningContent });\n \t\t\t\t},\n \n+\t\t\t\tonToolCallChunk: (toolCallChunk: string) => {\n+\t\t\t\t\tconst chunk = toolCallChunk.trim();\n+\n+\t\t\t\t\tif (!chunk) {\n+\t\t\t\t\t\treturn;\n+\t\t\t\t\t}\n+\n+\t\t\t\t\tstreamedToolCallContent = chunk;\n+\n+\t\t\t\t\tconst messageIndex = this.findMessageIndex(assistantMessage.id);\n+\n+\t\t\t\t\tthis.updateMessageAtIndex(messageIndex, { toolCalls: streamedToolCallContent });\n+\t\t\t\t},\n+\n \t\t\t\tonModel: (modelName: string) => {\n \t\t\t\t\trecordModel(modelName);\n \t\t\t\t},\n \n \t\t\t\tonComplete: async (\n \t\t\t\t\tfinalContent?: string,\n \t\t\t\t\treasoningContent?: string,\n-\t\t\t\t\ttimings?: ChatMessageTimings\n+\t\t\t\t\ttimings?: ChatMessageTimings,\n+\t\t\t\t\ttoolCallContent?: string\n \t\t\t\t) => {\n \t\t\t\t\tslotsService.stopStreaming();\n \n \t\t\t\t\tconst updateData: {\n \t\t\t\t\t\tcontent: string;\n \t\t\t\t\t\tthinking: string;\n+\t\t\t\t\t\ttoolCalls: string;\n \t\t\t\t\t\ttimings?: ChatMessageTimings;\n \t\t\t\t\t\tmodel?: string;\n \t\t\t\t\t} = {\n \t\t\t\t\t\tcontent: finalContent || streamedContent,\n \t\t\t\t\t\tthinking: reasoningContent || streamedReasoningContent,\n+\t\t\t\t\t\ttoolCalls: toolCallContent || streamedToolCallContent,\n \t\t\t\t\t\ttimings: timings\n \t\t\t\t\t};\n \n@@ -499,14 +518,22 @@ class ChatStore {\n \n \t\t\t\t\tconst messageIndex = this.findMessageIndex(assistantMessage.id);\n \n-\t\t\t\t\tconst localUpdateData: { timings?: ChatMessageTimings; model?: string } = {\n+\t\t\t\t\tconst localUpdateData: {\n+\t\t\t\t\t\ttimings?: ChatMessageTimings;\n+\t\t\t\t\t\tmodel?: string;\n+\t\t\t\t\t\ttoolCalls?: string;\n+\t\t\t\t\t} = {\n \t\t\t\t\t\ttimings: timings\n \t\t\t\t\t};\n \n \t\t\t\t\tif (updateData.model) {\n \t\t\t\t\t\tlocalUpdateData.model = updateData.model;\n \t\t\t\t\t}\n \n+\t\t\t\t\tif (updateData.toolCalls !== undefined) {\n+\t\t\t\t\t\tlocalUpdateData.toolCalls = updateData.toolCalls;\n+\t\t\t\t\t}\n+\n \t\t\t\t\tthis.updateMessageAtIndex(messageIndex, localUpdateData);\n \n \t\t\t\t\tawait DatabaseStore.updateCurrentNode(assistantMessage.convId, assistantMessage.id);\n@@ -620,6 +647,7 @@ class ChatStore {\n \t\t\t\tcontent: '',\n \t\t\t\ttimestamp: Date.now(),\n \t\t\t\tthinking: '',\n+\t\t\t\ttoolCalls: '',\n \t\t\t\tchildren: [],\n \t\t\t\tmodel: null\n \t\t\t},\n@@ -1443,6 +1471,7 @@ class ChatStore {\n \t\t\t\t\t\trole: messageToEdit.role,\n \t\t\t\t\t\tcontent: newContent,\n \t\t\t\t\t\tthinking: messageToEdit.thinking || '',\n+\t\t\t\t\t\ttoolCalls: messageToEdit.toolCalls || '',\n \t\t\t\t\t\tchildren: [],\n \t\t\t\t\t\tmodel: messageToEdit.model // Preserve original model info when branching\n \t\t\t\t\t},\n@@ -1518,6 +1547,7 @@ class ChatStore {\n \t\t\t\t\trole: messageToEdit.role,\n \t\t\t\t\tcontent: newContent,\n \t\t\t\t\tthinking: messageToEdit.thinking || '',\n+\t\t\t\t\ttoolCalls: messageToEdit.toolCalls || '',\n \t\t\t\t\tchildren: [],\n \t\t\t\t\textra: messageToEdit.extra ? JSON.parse(JSON.stringify(messageToEdit.extra)) : undefined,\n \t\t\t\t\tmodel: messageToEdit.model // Preserve original model info when branching\n@@ -1589,6 +1619,7 @@ class ChatStore {\n \t\t\t\t\trole: 'assistant',\n \t\t\t\t\tcontent: '',\n \t\t\t\t\tthinking: '',\n+\t\t\t\t\ttoolCalls: '',\n \t\t\t\t\tchildren: [],\n \t\t\t\t\tmodel: null\n \t\t\t\t},\n@@ -1647,6 +1678,7 @@ class ChatStore {\n \t\t\t\t\trole: 'assistant',\n \t\t\t\t\tcontent: '',\n \t\t\t\t\tthinking: '',\n+\t\t\t\t\ttoolCalls: '',\n \t\t\t\t\tchildren: [],\n \t\t\t\t\tmodel: null\n \t\t\t\t},"
      },
      {
        "filename": "tools/server/webui/src/lib/stores/database.ts",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -114,6 +114,7 @@ export class DatabaseStore {\n \t\t\t\t...message,\n \t\t\t\tid: uuid(),\n \t\t\t\tparent: parentId,\n+\t\t\t\ttoolCalls: message.toolCalls ?? '',\n \t\t\t\tchildren: []\n \t\t\t};\n \n@@ -154,6 +155,7 @@ export class DatabaseStore {\n \t\t\tcontent: '',\n \t\t\tparent: null,\n \t\t\tthinking: '',\n+\t\t\ttoolCalls: '',\n \t\t\tchildren: []\n \t\t};\n "
      },
      {
        "filename": "tools/server/webui/src/lib/types/api.d.ts",
        "status": "modified",
        "additions": 19,
        "deletions": 0,
        "changes": 19,
        "patch": "@@ -183,6 +183,23 @@ export interface ApiChatCompletionRequest {\n \tsamplers?: string[];\n \t// Custom parameters (JSON string)\n \tcustom?: Record<string, unknown>;\n+\ttimings_per_token?: boolean;\n+}\n+\n+export interface ApiChatCompletionToolCallFunctionDelta {\n+\tname?: string;\n+\targuments?: string;\n+}\n+\n+export interface ApiChatCompletionToolCallDelta {\n+\tindex?: number;\n+\tid?: string;\n+\ttype?: string;\n+\tfunction?: ApiChatCompletionToolCallFunctionDelta;\n+}\n+\n+export interface ApiChatCompletionToolCall extends ApiChatCompletionToolCallDelta {\n+\tfunction?: ApiChatCompletionToolCallFunctionDelta & { arguments?: string };\n }\n \n export interface ApiChatCompletionStreamChunk {\n@@ -195,6 +212,7 @@ export interface ApiChatCompletionStreamChunk {\n \t\t\tcontent?: string;\n \t\t\treasoning_content?: string;\n \t\t\tmodel?: string;\n+\t\t\ttool_calls?: ApiChatCompletionToolCallDelta[];\n \t\t};\n \t}>;\n \ttimings?: {\n@@ -216,6 +234,7 @@ export interface ApiChatCompletionResponse {\n \t\t\tcontent: string;\n \t\t\treasoning_content?: string;\n \t\t\tmodel?: string;\n+\t\t\ttool_calls?: ApiChatCompletionToolCallDelta[];\n \t\t};\n \t}>;\n }"
      },
      {
        "filename": "tools/server/webui/src/lib/types/database.d.ts",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -60,6 +60,7 @@ export interface DatabaseMessage {\n \tcontent: string;\n \tparent: string;\n \tthinking: string;\n+\ttoolCalls?: string;\n \tchildren: string[];\n \textra?: DatabaseMessageExtra[];\n \ttimings?: ChatMessageTimings;"
      },
      {
        "filename": "tools/server/webui/src/lib/types/settings.d.ts",
        "status": "modified",
        "additions": 8,
        "deletions": 1,
        "changes": 9,
        "patch": "@@ -38,12 +38,19 @@ export interface SettingsChatServiceOptions {\n \tsamplers?: string | string[];\n \t// Custom parameters\n \tcustom?: string;\n+\ttimings_per_token?: boolean;\n \t// Callbacks\n \tonChunk?: (chunk: string) => void;\n \tonReasoningChunk?: (chunk: string) => void;\n+\tonToolCallChunk?: (chunk: string) => void;\n \tonModel?: (model: string) => void;\n \tonFirstValidChunk?: () => void;\n-\tonComplete?: (response: string, reasoningContent?: string, timings?: ChatMessageTimings) => void;\n+\tonComplete?: (\n+\t\tresponse: string,\n+\t\treasoningContent?: string,\n+\t\ttimings?: ChatMessageTimings,\n+\t\ttoolCalls?: string\n+\t) => void;\n \tonError?: (error: Error) => void;\n }\n "
      }
    ],
    "num_files": 12,
    "scraped_at": "2025-11-16T23:29:55.481447"
  },
  {
    "pr_number": 16613,
    "title": "SYCL: Add support for FLOOR,CEIL,ROUND and TRUNC unary operators",
    "body": "This PR adds support for the following unary operators in the SYCL backend:\r\n\r\nFLOOR, CEIL, ROUND, TRUNC.\r\n\r\nChanges include:\r\n\r\nImplementation in element_wise.cpp and element_wise.hpp\r\n\r\nHeader updates in ggml.h\r\n\r\nRegistration in SYCL.csv\r\n\r\nDocumentation updates in docs/ops.md\r\n\r\nTest coverage in test-backend-ops.cpp\r\n\r\nThis SYCL implementation was added following the prior addition of the same operators in the CPU backend.\r\n",
    "html_url": "https://github.com/ggml-org/llama.cpp/pull/16613",
    "created_at": "2025-10-16T11:02:59Z",
    "merged_at": "2025-10-20T08:08:32Z",
    "merge_commit_sha": "2330de7b847ca84eac766df372c604c26db72747",
    "base_ref": "master",
    "head_sha": "47aa3a47980c2e0d5b2fee6e93eedaeeffaed9e5",
    "user": "safranowith",
    "files": [
      {
        "filename": "docs/ops.md",
        "status": "modified",
        "additions": 4,
        "deletions": 4,
        "changes": 8,
        "patch": "@@ -22,7 +22,7 @@ Legend:\n |                           ARANGE | \u274c | \u2705 | \u2705 | \u2705 | \u2705 | \u274c | \u274c | \u274c | \u274c |\n |                           ARGMAX | \u274c | \u2705 | \u2705 | \u2705 | \u2705 | \u274c | \u2705 | \u2705 | \u274c |\n |                          ARGSORT | \u274c | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u274c |\n-|                             CEIL | \u274c | \u274c | \u2705 | \u274c | \u274c | \u274c | \u274c | \u274c | \u274c |\n+|                             CEIL | \u274c | \u274c | \u2705 | \u274c | \u274c | \u274c | \u2705 | \u274c | \u274c |\n |                            CLAMP | \u274c | \u2705 | \u2705 | \u2705 | \ud83d\udfe1 | \ud83d\udfe1 | \u2705 | \ud83d\udfe1 | \u274c |\n |                           CONCAT | \u274c | \u2705 | \u2705 | \ud83d\udfe1 | \u2705 | \ud83d\udfe1 | \ud83d\udfe1 | \u2705 | \u274c |\n |                             CONT | \u274c | \ud83d\udfe1 | \u2705 | \u2705 | \u2705 | \ud83d\udfe1 | \ud83d\udfe1 | \ud83d\udfe1 | \u274c |\n@@ -42,7 +42,7 @@ Legend:\n |                              ELU | \u274c | \u2705 | \u2705 | \ud83d\udfe1 | \ud83d\udfe1 | \u274c | \ud83d\udfe1 | \u274c | \u274c |\n |                              EXP | \u274c | \u2705 | \u2705 | \ud83d\udfe1 | \ud83d\udfe1 | \u274c | \ud83d\udfe1 | \u274c | \u274c |\n |                   FLASH_ATTN_EXT | \u274c | \ud83d\udfe1 | \u2705 | \ud83d\udfe1 | \ud83d\udfe1 | \u274c | \u274c | \ud83d\udfe1 | \u274c |\n-|                            FLOOR | \u274c | \u274c | \u2705 | \u274c | \u274c | \u274c | \u274c | \u274c | \u274c |\n+|                            FLOOR | \u274c | \u274c | \u2705 | \u274c | \u274c | \u274c | \u2705 | \u274c | \u274c |\n |                GATED_LINEAR_ATTN | \u274c | \u274c | \u2705 | \u2705 | \u274c | \u274c | \u2705 | \u274c | \u274c |\n |                            GEGLU | \u274c | \u2705 | \u2705 | \u2705 | \ud83d\udfe1 | \u2705 | \u2705 | \ud83d\udfe1 | \u274c |\n |                        GEGLU_ERF | \u274c | \u2705 | \u2705 | \u2705 | \ud83d\udfe1 | \u2705 | \u2705 | \ud83d\udfe1 | \u274c |\n@@ -84,7 +84,7 @@ Legend:\n |                             ROLL | \u274c | \u274c | \u2705 | \u274c | \u274c | \u274c | \u274c | \u2705 | \u274c |\n |                             ROPE | \u274c | \ud83d\udfe1 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u274c |\n |                        ROPE_BACK | \u274c | \u274c | \u2705 | \u2705 | \u274c | \u274c | \u274c | \u2705 | \u274c |\n-|                            ROUND | \u274c | \u274c | \u2705 | \u274c | \u274c | \u274c | \u274c | \u274c | \u274c |\n+|                            ROUND | \u274c | \u274c | \u2705 | \u274c | \u274c | \u274c | \u2705 | \u274c | \u274c |\n |                        RWKV_WKV6 | \u274c | \u274c | \u2705 | \u2705 | \u2705 | \u274c | \u2705 | \u2705 | \u274c |\n |                        RWKV_WKV7 | \u274c | \u274c | \u2705 | \u2705 | \u2705 | \u274c | \u2705 | \u2705 | \u274c |\n |                            SCALE | \u274c | \ud83d\udfe1 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u274c |\n@@ -111,6 +111,6 @@ Legend:\n |                             TANH | \u274c | \u2705 | \u2705 | \ud83d\udfe1 | \ud83d\udfe1 | \u2705 | \ud83d\udfe1 | \ud83d\udfe1 | \u274c |\n |               TIMESTEP_EMBEDDING | \u274c | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u274c |\n |                         TOPK_MOE | \u274c | \u274c | \u274c | \u274c | \u274c | \u274c | \u2705 | \u274c | \u274c |\n-|                            TRUNC | \u274c | \u274c | \u2705 | \u274c | \u274c | \u274c | \u274c | \u274c | \u274c |\n+|                            TRUNC | \u274c | \u274c | \u2705 | \u274c | \u274c | \u274c | \u2705 | \u274c | \u274c |\n |                          UPSCALE | \u274c | \ud83d\udfe1 | \u2705 | \u2705 | \ud83d\udfe1 | \u2705 | \ud83d\udfe1 | \u2705 | \u274c |\n |                            XIELU | \u274c | \u274c | \u274c | \u274c | \u274c | \u274c | \u274c | \u274c | \u274c |"
      },
      {
        "filename": "docs/ops/SYCL.csv",
        "status": "modified",
        "additions": 16,
        "deletions": 0,
        "changes": 16,
        "patch": "@@ -31,6 +31,14 @@\n \"SYCL0\",\"GELU_ERF\",\"type=f16,ne_a=[5,7,11,13],v=0\",\"support\",\"1\",\"yes\",\"SYCL\"\n \"SYCL0\",\"XIELU\",\"type=f16,ne_a=[128,2,2,2],v=0\",\"support\",\"0\",\"no\",\"SYCL\"\n \"SYCL0\",\"XIELU\",\"type=f16,ne_a=[5,7,11,13],v=0\",\"support\",\"0\",\"no\",\"SYCL\"\n+\"SYCL0\",\"FLOOR\",\"type=f16,ne_a=[128,2,2,2],v=0\",\"support\",\"1\",\"yes\",\"SYCL\"\n+\"SYCL0\",\"FLOOR\",\"type=f16,ne_a=[5,7,11,13],v=0\",\"support\",\"1\",\"yes\",\"SYCL\"\n+\"SYCL0\",\"CEIL\",\"type=f16,ne_a=[128,2,2,2],v=0\",\"support\",\"1\",\"yes\",\"SYCL\"\n+\"SYCL0\",\"CEIL\",\"type=f16,ne_a=[5,7,11,13],v=0\",\"support\",\"1\",\"yes\",\"SYCL\"\n+\"SYCL0\",\"ROUND\",\"type=f16,ne_a=[128,2,2,2],v=0\",\"support\",\"1\",\"yes\",\"SYCL\"\n+\"SYCL0\",\"ROUND\",\"type=f16,ne_a=[5,7,11,13],v=0\",\"support\",\"1\",\"yes\",\"SYCL\"\n+\"SYCL0\",\"TRUNC\",\"type=f16,ne_a=[128,2,2,2],v=0\",\"support\",\"1\",\"yes\",\"SYCL\"\n+\"SYCL0\",\"TRUNC\",\"type=f16,ne_a=[5,7,11,13],v=0\",\"support\",\"1\",\"yes\",\"SYCL\"\n \"SYCL0\",\"ABS\",\"type=f16,ne_a=[128,2,2,2],v=1\",\"support\",\"0\",\"no\",\"SYCL\"\n \"SYCL0\",\"ABS\",\"type=f16,ne_a=[5,7,11,13],v=1\",\"support\",\"0\",\"no\",\"SYCL\"\n \"SYCL0\",\"SGN\",\"type=f16,ne_a=[128,2,2,2],v=1\",\"support\",\"0\",\"no\",\"SYCL\"\n@@ -95,6 +103,14 @@\n \"SYCL0\",\"GELU_ERF\",\"type=f32,ne_a=[5,7,11,13],v=0\",\"support\",\"1\",\"yes\",\"SYCL\"\n \"SYCL0\",\"XIELU\",\"type=f32,ne_a=[128,2,2,2],v=0\",\"support\",\"0\",\"no\",\"SYCL\"\n \"SYCL0\",\"XIELU\",\"type=f32,ne_a=[5,7,11,13],v=0\",\"support\",\"0\",\"no\",\"SYCL\"\n+\"SYCL0\",\"FLOOR\",\"type=f32,ne_a=[128,2,2,2],v=0\",\"support\",\"1\",\"yes\",\"SYCL\"\n+\"SYCL0\",\"FLOOR\",\"type=f32,ne_a=[5,7,11,13],v=0\",\"support\",\"1\",\"yes\",\"SYCL\"\n+\"SYCL0\",\"CEIL\",\"type=f32,ne_a=[128,2,2,2],v=0\",\"support\",\"1\",\"yes\",\"SYCL\"\n+\"SYCL0\",\"CEIL\",\"type=f32,ne_a=[5,7,11,13],v=0\",\"support\",\"1\",\"yes\",\"SYCL\"\n+\"SYCL0\",\"ROUND\",\"type=f32,ne_a=[128,2,2,2],v=0\",\"support\",\"1\",\"yes\",\"SYCL\"\n+\"SYCL0\",\"ROUND\",\"type=f32,ne_a=[5,7,11,13],v=0\",\"support\",\"1\",\"yes\",\"SYCL\"\n+\"SYCL0\",\"TRUNC\",\"type=f32,ne_a=[128,2,2,2],v=0\",\"support\",\"1\",\"yes\",\"SYCL\"\n+\"SYCL0\",\"TRUNC\",\"type=f32,ne_a=[5,7,11,13],v=0\",\"support\",\"1\",\"yes\",\"SYCL\"\n \"SYCL0\",\"ABS\",\"type=f32,ne_a=[128,2,2,2],v=1\",\"support\",\"0\",\"no\",\"SYCL\"\n \"SYCL0\",\"ABS\",\"type=f32,ne_a=[5,7,11,13],v=1\",\"support\",\"0\",\"no\",\"SYCL\"\n \"SYCL0\",\"SGN\",\"type=f32,ne_a=[128,2,2,2],v=1\",\"support\",\"0\",\"no\",\"SYCL\""
      },
      {
        "filename": "docs/ops/Vulkan.csv",
        "status": "modified",
        "additions": 21,
        "deletions": 21,
        "changes": 42,
        "patch": "@@ -3263,27 +3263,27 @@\n \"Vulkan0\",\"RMS_NORM_MUL_ADD\",\"type=f32,ne=[64,5,4,3],eps=1.000000,broadcast=0\",\"support\",\"1\",\"yes\",\"Vulkan\"\n \"Vulkan0\",\"RMS_NORM_MUL_ADD\",\"type=f32,ne=[64,5,4,3],eps=1.000000,broadcast=1\",\"support\",\"1\",\"yes\",\"Vulkan\"\n \"Vulkan0\",\"L2_NORM\",\"type=f32,ne=[64,5,4,3]\",\"support\",\"1\",\"yes\",\"Vulkan\"\n-\"Vulkan0\",\"SSM_CONV\",\"type=f32,ne_a=[4,1024,1,1],ne_b=[3,1024,1,1]\",\"support\",\"0\",\"no\",\"Vulkan\"\n-\"Vulkan0\",\"SSM_CONV\",\"type=f32,ne_a=[8,1024,1,1],ne_b=[3,1024,1,1]\",\"support\",\"0\",\"no\",\"Vulkan\"\n-\"Vulkan0\",\"SSM_CONV\",\"type=f32,ne_a=[4,1024,4,1],ne_b=[3,1024,1,1]\",\"support\",\"0\",\"no\",\"Vulkan\"\n-\"Vulkan0\",\"SSM_CONV\",\"type=f32,ne_a=[4,1536,1,1],ne_b=[3,1536,1,1]\",\"support\",\"0\",\"no\",\"Vulkan\"\n-\"Vulkan0\",\"SSM_CONV\",\"type=f32,ne_a=[8,1536,1,1],ne_b=[3,1536,1,1]\",\"support\",\"0\",\"no\",\"Vulkan\"\n-\"Vulkan0\",\"SSM_CONV\",\"type=f32,ne_a=[4,1536,4,1],ne_b=[3,1536,1,1]\",\"support\",\"0\",\"no\",\"Vulkan\"\n-\"Vulkan0\",\"SSM_CONV\",\"type=f32,ne_a=[4,2048,1,1],ne_b=[3,2048,1,1]\",\"support\",\"0\",\"no\",\"Vulkan\"\n-\"Vulkan0\",\"SSM_CONV\",\"type=f32,ne_a=[8,2048,1,1],ne_b=[3,2048,1,1]\",\"support\",\"0\",\"no\",\"Vulkan\"\n-\"Vulkan0\",\"SSM_CONV\",\"type=f32,ne_a=[4,2048,4,1],ne_b=[3,2048,1,1]\",\"support\",\"0\",\"no\",\"Vulkan\"\n-\"Vulkan0\",\"SSM_CONV\",\"type=f32,ne_a=[4,1024,1,1],ne_b=[4,1024,1,1]\",\"support\",\"0\",\"no\",\"Vulkan\"\n-\"Vulkan0\",\"SSM_CONV\",\"type=f32,ne_a=[8,1024,1,1],ne_b=[4,1024,1,1]\",\"support\",\"0\",\"no\",\"Vulkan\"\n-\"Vulkan0\",\"SSM_CONV\",\"type=f32,ne_a=[4,1024,4,1],ne_b=[4,1024,1,1]\",\"support\",\"0\",\"no\",\"Vulkan\"\n-\"Vulkan0\",\"SSM_CONV\",\"type=f32,ne_a=[4,1536,1,1],ne_b=[4,1536,1,1]\",\"support\",\"0\",\"no\",\"Vulkan\"\n-\"Vulkan0\",\"SSM_CONV\",\"type=f32,ne_a=[8,1536,1,1],ne_b=[4,1536,1,1]\",\"support\",\"0\",\"no\",\"Vulkan\"\n-\"Vulkan0\",\"SSM_CONV\",\"type=f32,ne_a=[4,1536,4,1],ne_b=[4,1536,1,1]\",\"support\",\"0\",\"no\",\"Vulkan\"\n-\"Vulkan0\",\"SSM_CONV\",\"type=f32,ne_a=[4,2048,1,1],ne_b=[4,2048,1,1]\",\"support\",\"0\",\"no\",\"Vulkan\"\n-\"Vulkan0\",\"SSM_CONV\",\"type=f32,ne_a=[8,2048,1,1],ne_b=[4,2048,1,1]\",\"support\",\"0\",\"no\",\"Vulkan\"\n-\"Vulkan0\",\"SSM_CONV\",\"type=f32,ne_a=[4,2048,4,1],ne_b=[4,2048,1,1]\",\"support\",\"0\",\"no\",\"Vulkan\"\n-\"Vulkan0\",\"SSM_SCAN\",\"type=f32,d_state=16,head_dim=1,n_head=1024,n_group=1,n_seq_tokens=32,n_seqs=4\",\"support\",\"0\",\"no\",\"Vulkan\"\n-\"Vulkan0\",\"SSM_SCAN\",\"type=f32,d_state=128,head_dim=64,n_head=16,n_group=2,n_seq_tokens=32,n_seqs=4\",\"support\",\"0\",\"no\",\"Vulkan\"\n-\"Vulkan0\",\"SSM_SCAN\",\"type=f32,d_state=256,head_dim=64,n_head=8,n_group=2,n_seq_tokens=32,n_seqs=4\",\"support\",\"0\",\"no\",\"Vulkan\"\n+\"Vulkan0\",\"SSM_CONV\",\"type=f32,ne_a=[4,1024,1,1],ne_b=[3,1024,1,1]\",\"support\",\"1\",\"yes\",\"Vulkan\"\n+\"Vulkan0\",\"SSM_CONV\",\"type=f32,ne_a=[8,1024,1,1],ne_b=[3,1024,1,1]\",\"support\",\"1\",\"yes\",\"Vulkan\"\n+\"Vulkan0\",\"SSM_CONV\",\"type=f32,ne_a=[4,1024,4,1],ne_b=[3,1024,1,1]\",\"support\",\"1\",\"yes\",\"Vulkan\"\n+\"Vulkan0\",\"SSM_CONV\",\"type=f32,ne_a=[4,1536,1,1],ne_b=[3,1536,1,1]\",\"support\",\"1\",\"yes\",\"Vulkan\"\n+\"Vulkan0\",\"SSM_CONV\",\"type=f32,ne_a=[8,1536,1,1],ne_b=[3,1536,1,1]\",\"support\",\"1\",\"yes\",\"Vulkan\"\n+\"Vulkan0\",\"SSM_CONV\",\"type=f32,ne_a=[4,1536,4,1],ne_b=[3,1536,1,1]\",\"support\",\"1\",\"yes\",\"Vulkan\"\n+\"Vulkan0\",\"SSM_CONV\",\"type=f32,ne_a=[4,2048,1,1],ne_b=[3,2048,1,1]\",\"support\",\"1\",\"yes\",\"Vulkan\"\n+\"Vulkan0\",\"SSM_CONV\",\"type=f32,ne_a=[8,2048,1,1],ne_b=[3,2048,1,1]\",\"support\",\"1\",\"yes\",\"Vulkan\"\n+\"Vulkan0\",\"SSM_CONV\",\"type=f32,ne_a=[4,2048,4,1],ne_b=[3,2048,1,1]\",\"support\",\"1\",\"yes\",\"Vulkan\"\n+\"Vulkan0\",\"SSM_CONV\",\"type=f32,ne_a=[4,1024,1,1],ne_b=[4,1024,1,1]\",\"support\",\"1\",\"yes\",\"Vulkan\"\n+\"Vulkan0\",\"SSM_CONV\",\"type=f32,ne_a=[8,1024,1,1],ne_b=[4,1024,1,1]\",\"support\",\"1\",\"yes\",\"Vulkan\"\n+\"Vulkan0\",\"SSM_CONV\",\"type=f32,ne_a=[4,1024,4,1],ne_b=[4,1024,1,1]\",\"support\",\"1\",\"yes\",\"Vulkan\"\n+\"Vulkan0\",\"SSM_CONV\",\"type=f32,ne_a=[4,1536,1,1],ne_b=[4,1536,1,1]\",\"support\",\"1\",\"yes\",\"Vulkan\"\n+\"Vulkan0\",\"SSM_CONV\",\"type=f32,ne_a=[8,1536,1,1],ne_b=[4,1536,1,1]\",\"support\",\"1\",\"yes\",\"Vulkan\"\n+\"Vulkan0\",\"SSM_CONV\",\"type=f32,ne_a=[4,1536,4,1],ne_b=[4,1536,1,1]\",\"support\",\"1\",\"yes\",\"Vulkan\"\n+\"Vulkan0\",\"SSM_CONV\",\"type=f32,ne_a=[4,2048,1,1],ne_b=[4,2048,1,1]\",\"support\",\"1\",\"yes\",\"Vulkan\"\n+\"Vulkan0\",\"SSM_CONV\",\"type=f32,ne_a=[8,2048,1,1],ne_b=[4,2048,1,1]\",\"support\",\"1\",\"yes\",\"Vulkan\"\n+\"Vulkan0\",\"SSM_CONV\",\"type=f32,ne_a=[4,2048,4,1],ne_b=[4,2048,1,1]\",\"support\",\"1\",\"yes\",\"Vulkan\"\n+\"Vulkan0\",\"SSM_SCAN\",\"type=f32,d_state=16,head_dim=1,n_head=1024,n_group=1,n_seq_tokens=32,n_seqs=4\",\"support\",\"1\",\"yes\",\"Vulkan\"\n+\"Vulkan0\",\"SSM_SCAN\",\"type=f32,d_state=128,head_dim=64,n_head=16,n_group=2,n_seq_tokens=32,n_seqs=4\",\"support\",\"1\",\"yes\",\"Vulkan\"\n+\"Vulkan0\",\"SSM_SCAN\",\"type=f32,d_state=256,head_dim=64,n_head=8,n_group=2,n_seq_tokens=32,n_seqs=4\",\"support\",\"1\",\"yes\",\"Vulkan\"\n \"Vulkan0\",\"RWKV_WKV6\",\"type=f32,head_count=32,head_size=64,n_seq_tokens=1,n_seqs=1\",\"support\",\"1\",\"yes\",\"Vulkan\"\n \"Vulkan0\",\"RWKV_WKV6\",\"type=f32,head_count=32,head_size=64,n_seq_tokens=32,n_seqs=1\",\"support\",\"1\",\"yes\",\"Vulkan\"\n \"Vulkan0\",\"RWKV_WKV6\",\"type=f32,head_count=32,head_size=64,n_seq_tokens=32,n_seqs=4\",\"support\",\"1\",\"yes\",\"Vulkan\""
      },
      {
        "filename": "ggml/src/ggml-sycl/element_wise.cpp",
        "status": "modified",
        "additions": 120,
        "deletions": 0,
        "changes": 120,
        "patch": "@@ -150,6 +150,26 @@ static __dpct_inline__ T op_clamp(T x, float min_val, float max_val) {\n     return x < static_cast<T>(min_val) ? static_cast<T>(min_val) : (x > static_cast<T>(max_val) ? static_cast<T>(max_val) : x);\n }\n \n+template<typename T>\n+static __dpct_inline__ T op_floor(T x) {\n+    return sycl::floor(x);\n+}\n+\n+template<typename T>\n+static __dpct_inline__ T op_ceil(T x) {\n+    return sycl::ceil(x);\n+}\n+\n+template<typename T>\n+static __dpct_inline__ T op_round(T x) {\n+    return sycl::round(x);\n+}\n+\n+template<typename T>\n+static __dpct_inline__ T op_trunc(T x) {\n+    return sycl::trunc(x);\n+}\n+\n template<typename T>\n static void unary_op_sgn_kernel(const T * x, T * dst, const int k, const sycl::nd_item<1> &item_ct1) {\n     SYCL_GLOBAL_ID_LOOP(k, item_ct1) {\n@@ -304,6 +324,34 @@ static void unary_op_clamp_kernel(const T * x, T * dst, const int k, const sycl:\n     }\n }\n \n+template<typename T>\n+static void unary_op_floor_kernel(const T * x, T * dst, const int k, const sycl::nd_item<1> &item_ct1) {\n+    SYCL_GLOBAL_ID_LOOP(k, item_ct1) {\n+        dst[i] = op_floor(x[i]);\n+    }\n+}\n+\n+template<typename T>\n+static void unary_op_ceil_kernel(const T * x, T * dst, const int k, const sycl::nd_item<1> &item_ct1) {\n+    SYCL_GLOBAL_ID_LOOP(k, item_ct1) {\n+        dst[i] = op_ceil(x[i]);\n+    }\n+}\n+\n+template<typename T>\n+static void unary_op_round_kernel(const T * x, T * dst, const int k, const sycl::nd_item<1> &item_ct1) {\n+    SYCL_GLOBAL_ID_LOOP(k, item_ct1) {\n+        dst[i] = op_round(x[i]);\n+    }\n+}\n+\n+template<typename T>\n+static void unary_op_trunc_kernel(const T * x, T * dst, const int k, const sycl::nd_item<1> &item_ct1) {\n+    SYCL_GLOBAL_ID_LOOP(k, item_ct1) {\n+        dst[i] = op_trunc(x[i]);\n+    }\n+}\n+\n template<typename  T>\n static void upscale(const T  *x, T *dst, const int nb00, const int nb01,\n                         const int nb02, const int nb03, const int ne10, const int ne11,\n@@ -897,6 +945,58 @@ static inline void ggml_sycl_op_clamp(ggml_backend_sycl_context & ctx, ggml_tens\n         }, min_val, max_val);\n }\n \n+static inline void ggml_sycl_op_floor(ggml_backend_sycl_context & ctx, ggml_tensor * dst) {\n+    ggml_sycl_detail::dispatch_ggml_sycl_op_unary(ctx, dst,\n+        [](const auto* src, auto* dst_ptr, int k_elements, queue_ptr stream) {\n+            const int num_blocks = ceil_div(k_elements, 256);\n+            stream->parallel_for(\n+                sycl::nd_range<1>(sycl::range<1>(num_blocks) * sycl::range<1>(256),\n+                                  sycl::range<1>(256)),\n+                [=](sycl::nd_item<1> item_ct1) {\n+                    unary_op_floor_kernel(src, dst_ptr, k_elements, item_ct1);\n+                });\n+        });\n+}\n+\n+static inline void ggml_sycl_op_ceil(ggml_backend_sycl_context & ctx, ggml_tensor * dst) {\n+    ggml_sycl_detail::dispatch_ggml_sycl_op_unary(ctx, dst,\n+        [](const auto* src, auto* dst_ptr, int k_elements, queue_ptr stream) {\n+            const int num_blocks = ceil_div(k_elements, 256);\n+            stream->parallel_for(\n+                sycl::nd_range<1>(sycl::range<1>(num_blocks) * sycl::range<1>(256),\n+                                  sycl::range<1>(256)),\n+                [=](sycl::nd_item<1> item_ct1) {\n+                    unary_op_ceil_kernel(src, dst_ptr, k_elements, item_ct1);\n+                });\n+        });\n+}\n+\n+static inline void ggml_sycl_op_round(ggml_backend_sycl_context & ctx, ggml_tensor * dst) {\n+    ggml_sycl_detail::dispatch_ggml_sycl_op_unary(ctx, dst,\n+        [](const auto* src, auto* dst_ptr, int k_elements, queue_ptr stream) {\n+            const int num_blocks = ceil_div(k_elements, 256);\n+            stream->parallel_for(\n+                sycl::nd_range<1>(sycl::range<1>(num_blocks) * sycl::range<1>(256),\n+                                  sycl::range<1>(256)),\n+                [=](sycl::nd_item<1> item_ct1) {\n+                    unary_op_round_kernel(src, dst_ptr, k_elements, item_ct1);\n+                });\n+        });\n+}\n+\n+static inline void ggml_sycl_op_trunc(ggml_backend_sycl_context & ctx, ggml_tensor * dst) {\n+    ggml_sycl_detail::dispatch_ggml_sycl_op_unary(ctx, dst,\n+        [](const auto* src, auto* dst_ptr, int k_elements, queue_ptr stream) {\n+            const int num_blocks = ceil_div(k_elements, 256);\n+            stream->parallel_for(\n+                sycl::nd_range<1>(sycl::range<1>(num_blocks) * sycl::range<1>(256),\n+                                  sycl::range<1>(256)),\n+                [=](sycl::nd_item<1> item_ct1) {\n+                    unary_op_trunc_kernel(src, dst_ptr, k_elements, item_ct1);\n+                });\n+        });\n+}\n+\n static inline void ggml_sycl_op_acc(ggml_backend_sycl_context & ctx, ggml_tensor *dst) {\n     GGML_ASSERT(dst->src[0]->type == GGML_TYPE_F32);\n     GGML_ASSERT(dst->src[1]->type == GGML_TYPE_F32);\n@@ -1122,3 +1222,23 @@ void ggml_sycl_arange(ggml_backend_sycl_context & ctx, ggml_tensor * dst) {\n     scope_op_debug_print scope_dbg_print(__func__, dst, /*num_src=*/0);\n     ggml_sycl_detail::ggml_sycl_op_arange(ctx, dst);\n }\n+\n+void ggml_sycl_floor(ggml_backend_sycl_context & ctx, ggml_tensor * dst) {\n+    scope_op_debug_print scope_dbg_print(__func__, dst, /*num_src=*/1);\n+    ggml_sycl_op_floor(ctx, dst);\n+}\n+\n+void ggml_sycl_ceil(ggml_backend_sycl_context & ctx, ggml_tensor * dst) {\n+    scope_op_debug_print scope_dbg_print(__func__, dst, /*num_src=*/1);\n+    ggml_sycl_op_ceil(ctx, dst);\n+}\n+\n+void ggml_sycl_round(ggml_backend_sycl_context & ctx, ggml_tensor * dst) {\n+    scope_op_debug_print scope_dbg_print(__func__, dst, /*num_src=*/1);\n+    ggml_sycl_op_round(ctx, dst);\n+}\n+\n+void ggml_sycl_trunc(ggml_backend_sycl_context & ctx, ggml_tensor * dst) {\n+    scope_op_debug_print scope_dbg_print(__func__, dst, /*num_src=*/1);\n+    ggml_sycl_op_trunc(ctx, dst);\n+}"
      },
      {
        "filename": "ggml/src/ggml-sycl/element_wise.hpp",
        "status": "modified",
        "additions": 4,
        "deletions": 0,
        "changes": 4,
        "patch": "@@ -80,6 +80,10 @@ void ggml_sycl_reglu(ggml_backend_sycl_context & ctx, ggml_tensor * dst);\n void ggml_sycl_swiglu(ggml_backend_sycl_context & ctx, ggml_tensor * dst);\n void ggml_sycl_geglu_erf(ggml_backend_sycl_context & ctx, ggml_tensor * dst);\n void ggml_sycl_geglu_quick(ggml_backend_sycl_context & ctx, ggml_tensor * dst);\n+void ggml_sycl_floor(ggml_backend_sycl_context & ctx, ggml_tensor * dst);\n+void ggml_sycl_ceil(ggml_backend_sycl_context & ctx, ggml_tensor * dst);\n+void ggml_sycl_round(ggml_backend_sycl_context & ctx, ggml_tensor * dst);\n+void ggml_sycl_trunc(ggml_backend_sycl_context & ctx, ggml_tensor * dst);\n \n void ggml_sycl_arange(ggml_backend_sycl_context & ctx, ggml_tensor * dst);\n "
      },
      {
        "filename": "ggml/src/ggml-sycl/ggml-sycl.cpp",
        "status": "modified",
        "additions": 16,
        "deletions": 0,
        "changes": 16,
        "patch": "@@ -3698,6 +3698,18 @@ static bool ggml_sycl_compute_forward(ggml_backend_sycl_context & ctx, struct gg\n                 case GGML_UNARY_OP_ELU:\n                     ggml_sycl_elu(ctx, dst);\n                     break;\n+                case GGML_UNARY_OP_FLOOR:\n+                    ggml_sycl_floor(ctx, dst);\n+                    break;\n+                case GGML_UNARY_OP_CEIL:\n+                    ggml_sycl_ceil(ctx, dst);\n+                    break;\n+                case GGML_UNARY_OP_ROUND:\n+                    ggml_sycl_round(ctx, dst);\n+                    break;\n+                case GGML_UNARY_OP_TRUNC:\n+                    ggml_sycl_trunc(ctx, dst);\n+                    break;\n                 default:\n                     return false;\n             }\n@@ -4262,6 +4274,10 @@ static bool ggml_backend_sycl_device_supports_op(ggml_backend_dev_t dev, const g\n                 case GGML_UNARY_OP_SGN:\n                 case GGML_UNARY_OP_ABS:\n                 case GGML_UNARY_OP_ELU:\n+                case GGML_UNARY_OP_FLOOR:\n+                case GGML_UNARY_OP_CEIL:\n+                case GGML_UNARY_OP_ROUND:\n+                case GGML_UNARY_OP_TRUNC:\n #if defined (GGML_SYCL_F16)\n                     return ggml_is_contiguous(op->src[0]) && (op->type == op->src[0]->type);\n #else"
      },
      {
        "filename": "tests/test-backend-ops.cpp",
        "status": "modified",
        "additions": 132,
        "deletions": 0,
        "changes": 132,
        "patch": "@@ -3759,6 +3759,130 @@ struct test_clamp : public test_case {\n     }\n };\n \n+// GGML_OP_FLOOR\n+struct test_floor : public test_case {\n+    const ggml_type type;\n+    const std::array<int64_t, 4> ne;\n+\n+    std::string vars() override {\n+        return VARS_TO_STR2(type, ne);\n+    }\n+\n+    test_floor(ggml_type type = GGML_TYPE_F32,\n+               std::array<int64_t, 4> ne = {10, 2, 2, 2})\n+        : type(type), ne(ne) {}\n+\n+    ggml_tensor * build_graph(ggml_context * ctx) override {\n+        ggml_tensor * a = ggml_new_tensor(ctx, type, 4, ne.data());\n+        ggml_set_param(a);\n+        ggml_set_name(a, \"a\");\n+\n+        ggml_tensor * out = ggml_floor(ctx, a);\n+        ggml_set_name(out, \"out\");\n+\n+        return out;\n+    }\n+\n+    void initialize_tensors(ggml_context * ctx) override {\n+        for (ggml_tensor * t = ggml_get_first_tensor(ctx); t != NULL; t = ggml_get_next_tensor(ctx, t)) {\n+            init_tensor_uniform(t, -10.0f, 10.0f);\n+        }\n+    }\n+};\n+\n+// GGML_OP_CEIL\n+struct test_ceil : public test_case {\n+    const ggml_type type;\n+    const std::array<int64_t, 4> ne;\n+\n+    std::string vars() override {\n+        return VARS_TO_STR2(type, ne);\n+    }\n+\n+    test_ceil(ggml_type type = GGML_TYPE_F32,\n+              std::array<int64_t, 4> ne = {10, 2, 2, 2})\n+        : type(type), ne(ne) {}\n+\n+    ggml_tensor * build_graph(ggml_context * ctx) override {\n+        ggml_tensor * a = ggml_new_tensor(ctx, type, 4, ne.data());\n+        ggml_set_param(a);\n+        ggml_set_name(a, \"a\");\n+\n+        ggml_tensor * out = ggml_ceil(ctx, a);\n+        ggml_set_name(out, \"out\");\n+\n+        return out;\n+    }\n+\n+    void initialize_tensors(ggml_context * ctx) override {\n+        for (ggml_tensor * t = ggml_get_first_tensor(ctx); t != NULL; t = ggml_get_next_tensor(ctx, t)) {\n+            init_tensor_uniform(t, -10.0f, 10.0f);\n+        }\n+    }\n+};\n+\n+// GGML_OP_ROUND\n+struct test_round : public test_case {\n+    const ggml_type type;\n+    const std::array<int64_t, 4> ne;\n+\n+    std::string vars() override {\n+        return VARS_TO_STR2(type, ne);\n+    }\n+\n+    test_round(ggml_type type = GGML_TYPE_F32,\n+               std::array<int64_t, 4> ne = {10, 2, 2, 2})\n+        : type(type), ne(ne) {}\n+\n+    ggml_tensor * build_graph(ggml_context * ctx) override {\n+        ggml_tensor * a = ggml_new_tensor(ctx, type, 4, ne.data());\n+        ggml_set_param(a);\n+        ggml_set_name(a, \"a\");\n+\n+        ggml_tensor * out = ggml_round(ctx, a);\n+        ggml_set_name(out, \"out\");\n+\n+        return out;\n+    }\n+\n+    void initialize_tensors(ggml_context * ctx) override {\n+        for (ggml_tensor * t = ggml_get_first_tensor(ctx); t != NULL; t = ggml_get_next_tensor(ctx, t)) {\n+            init_tensor_uniform(t, -10.0f, 10.0f);\n+        }\n+    }\n+};\n+\n+// GGML_OP_TRUNC\n+struct test_trunc : public test_case {\n+    const ggml_type type;\n+    const std::array<int64_t, 4> ne;\n+\n+    std::string vars() override {\n+        return VARS_TO_STR2(type, ne);\n+    }\n+\n+    test_trunc(ggml_type type = GGML_TYPE_F32,\n+               std::array<int64_t, 4> ne = {10, 2, 2, 2})\n+        : type(type), ne(ne) {}\n+\n+    ggml_tensor * build_graph(ggml_context * ctx) override {\n+        ggml_tensor * a = ggml_new_tensor(ctx, type, 4, ne.data());\n+        ggml_set_param(a);\n+        ggml_set_name(a, \"a\");\n+\n+        ggml_tensor * out = ggml_trunc(ctx, a);\n+        ggml_set_name(out, \"out\");\n+\n+        return out;\n+    }\n+\n+    void initialize_tensors(ggml_context * ctx) override {\n+        for (ggml_tensor * t = ggml_get_first_tensor(ctx); t != NULL; t = ggml_get_next_tensor(ctx, t)) {\n+            init_tensor_uniform(t, -10.0f, 10.0f);\n+        }\n+    }\n+};\n+\n // GGML_OP_DIAG_MASK_INF\n struct test_diag_mask_inf : public test_case {\n     const ggml_type type;\n@@ -6585,13 +6709,21 @@ static std::vector<std::unique_ptr<test_case>> make_test_cases_eval() {\n         test_cases.emplace_back(new test_cos       (type));\n         test_cases.emplace_back(new test_clamp     (type));\n         test_cases.emplace_back(new test_leaky_relu(type));\n+        test_cases.emplace_back(new test_floor     (type));\n+        test_cases.emplace_back(new test_ceil      (type));\n+        test_cases.emplace_back(new test_round     (type));\n+        test_cases.emplace_back(new test_trunc     (type));\n         test_cases.emplace_back(new test_sqr       (type, {7, 1, 5, 3}));\n         test_cases.emplace_back(new test_sqrt      (type, {7, 1, 5, 3}));\n         test_cases.emplace_back(new test_log       (type, {7, 1, 5, 3}));\n         test_cases.emplace_back(new test_sin       (type, {7, 1, 5, 3}));\n         test_cases.emplace_back(new test_cos       (type, {7, 1, 5, 3}));\n         test_cases.emplace_back(new test_clamp     (type, {7, 1, 5, 3}));\n         test_cases.emplace_back(new test_leaky_relu(type, {7, 1, 5, 3}));\n+        test_cases.emplace_back(new test_floor     (type, {7, 1, 5, 3}));\n+        test_cases.emplace_back(new test_ceil      (type, {7, 1, 5, 3}));\n+        test_cases.emplace_back(new test_round     (type, {7, 1, 5, 3}));\n+        test_cases.emplace_back(new test_trunc     (type, {7, 1, 5, 3}));\n     }\n \n     test_cases.emplace_back(new test_diag_mask_inf(GGML_TYPE_F32, {10, 10, 1, 1}, 5));"
      }
    ],
    "num_files": 7,
    "scraped_at": "2025-11-16T23:29:56.360911"
  },
  {
    "pr_number": 16602,
    "title": "opencl: transposed gemm/gemv moe kernel with mxfp4,f32",
    "body": "Added redesigned moe-mxfp4 kernels optimized for Adreno:\r\n\r\n- Added pref-transpose for experts, fused with SOA convert kernel.\r\n- Preprocess router table on CPU side.\r\n- Separated decoding and prefill new kernels for MoE-mxfp4.\r\n\r\nAchieved large perf uplift for prefill, especially for long prompts.\r\n",
    "html_url": "https://github.com/ggml-org/llama.cpp/pull/16602",
    "created_at": "2025-10-15T23:22:03Z",
    "merged_at": "2025-10-18T00:55:32Z",
    "merge_commit_sha": "81387858f1fbcc1acedbd308486e1016618ca8f8",
    "base_ref": "master",
    "head_sha": "0ccc26213ab09e38285fc97f46b532e2c30fdd72",
    "user": "shawngu-quic",
    "files": [
      {
        "filename": "ggml/src/ggml-opencl/CMakeLists.txt",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -91,6 +91,8 @@ set(GGML_OPENCL_KERNELS\n     mul_mv_id_q8_0_f32_flat\n     mul_mv_id_mxfp4_f32\n     mul_mv_id_mxfp4_f32_flat\n+    gemm_moe_mxfp4_f32\n+    gemv_moe_mxfp4_f32\n     mul_mm_f32_f32_l4_lm\n     mul_mm_f16_f32_l4_lm\n     mul_mm_q8_0_f32_l4_lm"
      },
      {
        "filename": "ggml/src/ggml-opencl/ggml-opencl.cpp",
        "status": "modified",
        "additions": 205,
        "deletions": 8,
        "changes": 213,
        "patch": "@@ -402,6 +402,7 @@ struct ggml_backend_opencl_context {\n     cl_program program_conv_2d_f32;\n     cl_program program_conv_2d_f16_f32;\n     cl_program program_tsembd;\n+    cl_program program_gemv_moe_mxfp4_f32, program_gemm_moe_mxfp4_f32;\n     cl_program program_mul_mv_id_q4_0_f32_8x_flat;\n     cl_program program_mul_mv_id_q8_0_f32, program_mul_mv_id_q8_0_f32_flat;\n     cl_program program_mul_mv_id_mxfp4_f32;\n@@ -452,7 +453,7 @@ struct ggml_backend_opencl_context {\n     cl_kernel kernel_mul_mat_f16_f32_tiled;\n     cl_kernel kernel_mul_mat_q4_0_f32, kernel_mul_mat_q4_0_f32_v;\n     cl_kernel kernel_convert_block_q4_0, kernel_restore_block_q4_0;\n-    cl_kernel kernel_convert_block_mxfp4, kernel_restore_block_mxfp4;\n+    cl_kernel kernel_convert_block_mxfp4, kernel_convert_block_mxfp4_trans, kernel_restore_block_mxfp4, kernel_restore_block_mxfp4_trans;\n     cl_kernel kernel_convert_block_q8_0, kernel_restore_block_q8_0;\n     cl_kernel kernel_mul_mat_q4_0_f32_8x_flat;\n     cl_kernel kernel_convert_block_q4_0_noshuffle;\n@@ -475,6 +476,7 @@ struct ggml_backend_opencl_context {\n     cl_kernel kernel_conv_2d_f32;\n     cl_kernel kernel_conv_2d_f16_f32;\n     cl_kernel kernel_timestep_embedding;\n+    cl_kernel kernel_gemv_moe_mxfp4_f32, kernel_gemm_moe_mxfp4_f32;\n     cl_kernel kernel_mul_mv_id_q4_0_f32_8x_flat;\n     cl_kernel kernel_mul_mv_id_q8_0_f32, kernel_mul_mv_id_q8_0_f32_flat;\n     cl_kernel kernel_mul_mv_id_mxfp4_f32;\n@@ -559,14 +561,14 @@ struct ggml_backend_opencl_context {\n \n         fprintf(ftrace, \"[\\n\");\n         for (const ProfilingInfo & info : profiling_info) {\n-            fprintf(ftrace, \"{\\\"name\\\": \\\"%s\\\", \\\"cat\\\": \\\"OpenCL\\\", \\\"ph\\\": \\\"B\\\", \\\"ts\\\": %lu, \\\"pid\\\": \\\"\\\", \\\"tid\\\": \\\"Host\\\"},\\n\",\n+            fprintf(ftrace, \"{\\\"name\\\": \\\"%s\\\", \\\"cat\\\": \\\"OpenCL\\\", \\\"ph\\\": \\\"B\\\", \\\"ts\\\": %llu, \\\"pid\\\": \\\"\\\", \\\"tid\\\": \\\"Host\\\"},\\n\",\n                 info.kernel_name.c_str(), info.cmd_queued/1000);\n-            fprintf(ftrace, \"{\\\"name\\\": \\\"%s\\\", \\\"cat\\\": \\\"OpenCL\\\", \\\"ph\\\": \\\"E\\\", \\\"ts\\\": %lu, \\\"pid\\\": \\\"\\\", \\\"tid\\\": \\\"Host\\\"},\\n\",\n+            fprintf(ftrace, \"{\\\"name\\\": \\\"%s\\\", \\\"cat\\\": \\\"OpenCL\\\", \\\"ph\\\": \\\"E\\\", \\\"ts\\\": %llu, \\\"pid\\\": \\\"\\\", \\\"tid\\\": \\\"Host\\\"},\\n\",\n                 info.kernel_name.c_str(), info.cmd_submit/1000);\n \n-            fprintf(ftrace, \"{\\\"name\\\": \\\"%s\\\", \\\"cat\\\": \\\"OpenCL\\\", \\\"ph\\\": \\\"B\\\", \\\"ts\\\": %lu, \\\"pid\\\": \\\"\\\", \\\"tid\\\": \\\"Device\\\"},\\n\",\n+            fprintf(ftrace, \"{\\\"name\\\": \\\"%s\\\", \\\"cat\\\": \\\"OpenCL\\\", \\\"ph\\\": \\\"B\\\", \\\"ts\\\": %llu, \\\"pid\\\": \\\"\\\", \\\"tid\\\": \\\"Device\\\"},\\n\",\n                 info.kernel_name.c_str(), info.cmd_start/1000);\n-            fprintf(ftrace, \"{\\\"name\\\": \\\"%s\\\", \\\"cat\\\": \\\"OpenCL\\\", \\\"ph\\\": \\\"E\\\", \\\"ts\\\": %lu, \\\"pid\\\": \\\"\\\", \\\"tid\\\": \\\"Device\\\"},\\n\",\n+            fprintf(ftrace, \"{\\\"name\\\": \\\"%s\\\", \\\"cat\\\": \\\"OpenCL\\\", \\\"ph\\\": \\\"E\\\", \\\"ts\\\": %llu, \\\"pid\\\": \\\"\\\", \\\"tid\\\": \\\"Device\\\"},\\n\",\n                 info.kernel_name.c_str(), info.cmd_end/1000);\n         }\n         fclose(ftrace);\n@@ -777,6 +779,8 @@ static void load_cl_kernels(ggml_backend_opencl_context *backend_ctx, ggml_cl_ve\n         CL_CHECK((backend_ctx->kernel_convert_block_q4_0  = clCreateKernel(backend_ctx->program_cvt, \"kernel_convert_block_q4_0\", &err), err));\n         CL_CHECK((backend_ctx->kernel_restore_block_q4_0  = clCreateKernel(backend_ctx->program_cvt, \"kernel_restore_block_q4_0\", &err), err));\n         CL_CHECK((backend_ctx->kernel_convert_block_mxfp4 = clCreateKernel(backend_ctx->program_cvt, \"kernel_convert_block_mxfp4\", &err), err));\n+        CL_CHECK((backend_ctx->kernel_convert_block_mxfp4_trans = clCreateKernel(backend_ctx->program_cvt, \"kernel_convert_block_mxfp4_trans\", &err), err));\n+        CL_CHECK((backend_ctx->kernel_restore_block_mxfp4_trans = clCreateKernel(backend_ctx->program_cvt, \"kernel_restore_block_mxfp4_trans\", &err), err));\n         CL_CHECK((backend_ctx->kernel_restore_block_mxfp4 = clCreateKernel(backend_ctx->program_cvt, \"kernel_restore_block_mxfp4\", &err), err));\n         CL_CHECK((backend_ctx->kernel_convert_block_q8_0  = clCreateKernel(backend_ctx->program_cvt, \"kernel_convert_block_q8_0\", &err), err));\n         CL_CHECK((backend_ctx->kernel_restore_block_q8_0  = clCreateKernel(backend_ctx->program_cvt, \"kernel_restore_block_q8_0\", &err), err));\n@@ -1991,6 +1995,42 @@ static void load_cl_kernels(ggml_backend_opencl_context *backend_ctx, ggml_cl_ve\n         CL_CHECK((backend_ctx->CL_mul_mat_Ab_Bi_8x4 = clCreateKernel(backend_ctx->program_CL_gemm, \"kernel_mul_mat_Ab_Bi_8x4\", &err), err));\n         GGML_LOG_CONT(\".\");\n     }\n+\n+    std::string CL_moe_compile_opts = std::string(\"-cl-std=\") + opencl_c_std +\n+            \" -cl-mad-enable \"\n+            \" -cl-fast-relaxed-math\";\n+\n+    // gemv_moe_mxfp4_f32\n+    {\n+#ifdef GGML_OPENCL_EMBED_KERNELS\n+        const std::string kernel_src {\n+            #include \"gemv_moe_mxfp4_f32.cl.h\"\n+        };\n+#else\n+        const std::string kernel_src = read_file(\"gemv_moe_mxfp4_f32.cl\");\n+#endif\n+        backend_ctx->program_gemv_moe_mxfp4_f32 =\n+            build_program_from_source(backend_ctx->context, backend_ctx->device, kernel_src.c_str(), CL_moe_compile_opts);\n+\n+        CL_CHECK((backend_ctx->kernel_gemv_moe_mxfp4_f32 = clCreateKernel(backend_ctx->program_gemv_moe_mxfp4_f32, \"kernel_gemv_moe_mxfp4_f32\", &err), err));\n+        GGML_LOG_CONT(\".\");\n+    }\n+\n+    // gemm_moe_mxfp4_f32\n+    {\n+#ifdef GGML_OPENCL_EMBED_KERNELS\n+        const std::string kernel_src {\n+            #include \"gemm_moe_mxfp4_f32.cl.h\"\n+        };\n+#else\n+        const std::string kernel_src = read_file(\"gemm_moe_mxfp4_f32.cl\");\n+#endif\n+        backend_ctx->program_gemm_moe_mxfp4_f32 =\n+            build_program_from_source(backend_ctx->context, backend_ctx->device, kernel_src.c_str(), CL_moe_compile_opts);\n+\n+        CL_CHECK((backend_ctx->kernel_gemm_moe_mxfp4_f32 = clCreateKernel(backend_ctx->program_gemm_moe_mxfp4_f32, \"kernel_gemm_moe_mxfp4_f32\", &err), err));\n+        GGML_LOG_CONT(\".\");\n+    }\n #endif // GGML_OPENCL_USE_ADRENO_KERNELS\n     GGML_LOG_CONT(\"\\n\");\n }\n@@ -3299,6 +3339,12 @@ inline bool use_adreno_kernels(const ggml_backend_opencl_context *backend_ctx, c\n             tensor->ne[2] == 1 && tensor->ne[3] == 1;\n }\n \n+inline bool use_adreno_moe_kernels(const ggml_backend_opencl_context *backend_ctx, const ggml_tensor *tensor) {\n+    GGML_UNUSED(backend_ctx);\n+    int ne01 = tensor->ne[1];\n+    return ((strstr(tensor->name, \"ffn\") != NULL) || (strstr(tensor->name, \"as\") != NULL)) && (ne01 % 64 == 0);\n+}\n+\n static void ggml_backend_opencl_buffer_set_tensor(ggml_backend_buffer_t buffer, ggml_tensor * tensor, const void * data, size_t offset, size_t size) {\n     ggml_backend_opencl_context *backend_ctx = ggml_cl2_init(buffer->buft->device);\n \n@@ -3601,14 +3647,39 @@ static void ggml_backend_opencl_buffer_set_tensor(ggml_backend_buffer_t buffer,\n             CL_BUFFER_CREATE_TYPE_REGION, &region, &err);\n         CL_CHECK(err);\n \n+#ifdef GGML_OPENCL_USE_ADRENO_KERNELS\n+        if (use_adreno_moe_kernels(backend_ctx, tensor)) {\n+            cl_kernel kernel = backend_ctx->kernel_convert_block_mxfp4_trans;\n+\n+            int ne00 = tensor->ne[0];\n+            int ne01 = tensor->ne[1];\n+            int ne02 = tensor->ne[2];\n+            CL_CHECK(clSetKernelArg(kernel, 0, sizeof(cl_mem), &data_device));\n+            CL_CHECK(clSetKernelArg(kernel, 1, sizeof(cl_mem), &extra->q));\n+            CL_CHECK(clSetKernelArg(kernel, 2, sizeof(cl_mem), &extra->e));\n+            CL_CHECK(clSetKernelArg(kernel, 3, sizeof(int), &ne00));\n+            CL_CHECK(clSetKernelArg(kernel, 4, sizeof(int), &ne01));\n+\n+            size_t global_work_size[3] = {static_cast<size_t>(((ne01 + 63) / 64) * 64), static_cast<size_t>(ne00 / 32), static_cast<size_t>(ne02)};\n+            size_t local_work_size[3] = {64, 2, 1};\n+\n+            cl_event evt;\n+            CL_CHECK(clEnqueueNDRangeKernel(queue, kernel, 3, NULL, global_work_size, local_work_size, 0, NULL, &evt));\n+            CL_CHECK(clWaitForEvents(1, &evt));\n+            CL_CHECK(clReleaseMemObject(data_device));\n+            tensor->extra = extra;\n+\n+            return;\n+        }\n+#endif\n         cl_kernel kernel = backend_ctx->kernel_convert_block_mxfp4;\n \n         CL_CHECK(clSetKernelArg(kernel, 0, sizeof(cl_mem), &data_device));\n         CL_CHECK(clSetKernelArg(kernel, 1, sizeof(cl_mem), &extra->q));\n         CL_CHECK(clSetKernelArg(kernel, 2, sizeof(cl_mem), &extra->e));\n \n-        size_t global_work_size[] = {(size_t)ggml_nelements(tensor)/ggml_blck_size(tensor->type), 1, 1};\n-        size_t local_work_size[] = {64, 1, 1};\n+        size_t global_work_size[3] = {(size_t)ggml_nelements(tensor)/ggml_blck_size(tensor->type), 1, 1};\n+        size_t local_work_size[3] = {64, 1, 1};\n \n         cl_event evt;\n         CL_CHECK(clEnqueueNDRangeKernel(queue, kernel, 3, NULL, global_work_size, local_work_size, 0, NULL, &evt));\n@@ -3624,7 +3695,6 @@ static void ggml_backend_opencl_buffer_set_tensor(ggml_backend_buffer_t buffer,\n             { extra->q }\n         };\n         extra->q_img = clCreateImage(context, CL_MEM_READ_ONLY, &img_format_q, &img_desc_q, NULL, &err);\n-\n         tensor->extra = extra;\n \n         return;\n@@ -3751,6 +3821,33 @@ static void ggml_backend_opencl_buffer_get_tensor(ggml_backend_buffer_t buffer,\n             ggml_nbytes(tensor), NULL, &err);\n         CL_CHECK(err);\n \n+#ifdef GGML_OPENCL_USE_ADRENO_KERNELS\n+        if (use_adreno_moe_kernels(backend_ctx, tensor)) {\n+            cl_kernel kernel = backend_ctx->kernel_restore_block_mxfp4_trans;\n+\n+            int ne00 = tensor->ne[0];\n+            int ne01 = tensor->ne[1];\n+            int ne02 = tensor->ne[2];\n+            CL_CHECK(clSetKernelArg(kernel, 0, sizeof(cl_mem), &extra->q));\n+            CL_CHECK(clSetKernelArg(kernel, 1, sizeof(cl_mem), &extra->e));\n+            CL_CHECK(clSetKernelArg(kernel, 2, sizeof(cl_mem), &data_device));\n+            CL_CHECK(clSetKernelArg(kernel, 3, sizeof(cl_int), &ne00));\n+            CL_CHECK(clSetKernelArg(kernel, 4, sizeof(cl_int), &ne01));\n+\n+            size_t global_work_size[3] = {static_cast<size_t>(((ne01 + 63) / 64) * 64), static_cast<size_t>(ne00 / 32), static_cast<size_t>(ne02)};\n+            size_t local_work_size[3] = {64, 2, 1};\n+\n+            cl_event evt;\n+            CL_CHECK(clEnqueueNDRangeKernel(queue, kernel, 3, NULL,\n+                global_work_size, local_work_size, 0, NULL, &evt));\n+            CL_CHECK(clWaitForEvents(1, &evt));\n+            CL_CHECK(clEnqueueReadBuffer(\n+                queue, data_device, CL_TRUE, offset,\n+                size, data, 0, NULL, NULL));\n+            CL_CHECK(clReleaseMemObject(data_device));\n+            return;\n+        }\n+#endif\n         cl_kernel kernel = backend_ctx->kernel_restore_block_mxfp4;\n         CL_CHECK(clSetKernelArg(kernel, 0, sizeof(cl_mem), &extra->q));\n         CL_CHECK(clSetKernelArg(kernel, 1, sizeof(cl_mem), &extra->e));\n@@ -7553,6 +7650,7 @@ static void ggml_cl_mul_mat_id(ggml_backend_t backend, const ggml_tensor * src0,\n     const int ne21 = src2->ne[1];\n \n     const cl_ulong nb21 = src2->nb[1];\n+    const cl_ulong nb20 = src2->nb[0];\n \n     const int ne0 = dst->ne[0];\n     const int ne1 = dst->ne[1];\n@@ -7692,6 +7790,105 @@ static void ggml_cl_mul_mat_id(ggml_backend_t backend, const ggml_tensor * src0,\n             break;\n         }\n         case GGML_TYPE_MXFP4: {\n+#ifdef GGML_OPENCL_USE_ADRENO_KERNELS\n+            if (use_adreno_moe_kernels(backend_ctx, src0)) {\n+                cl_int status;\n+\n+                size_t local_size[3] = {64, 2, 1};\n+                size_t global_size[3] = {64, 2, 1};\n+\n+                cl_mem src1_sub_buffer, buf_src1_image, buf_src2;\n+\n+                int tile_size = 320;\n+                if (ne12 == 1) { // for gemv\n+                    kernel = backend_ctx->kernel_gemv_moe_mxfp4_f32;\n+\n+                    // create a sub_buffer for src2\n+                    cl_buffer_region region;\n+                    region.origin = offset2;\n+                    region.size = ne20 * ne21 * sizeof(int);\n+                    buf_src2 = clCreateSubBuffer(extra2->data_device, 0, CL_BUFFER_CREATE_TYPE_REGION, &region, &status);\n+                    CL_CHECK(status);\n+\n+                    // set thread grid\n+                    global_size[0] = static_cast<size_t>(ne01);\n+                    global_size[1] = 4;\n+                    global_size[2] = static_cast<size_t>(ne20);\n+                    local_size[1] = 4;\n+                } else { // for gemm\n+                    kernel = backend_ctx->kernel_gemm_moe_mxfp4_f32;\n+\n+                    // preprocess router table\n+                    int num_tiles_per_expert = (ne01 + tile_size - 1) / tile_size;\n+                    void * host_src2_reorder = malloc(ne20 * ne21 * 4 * num_tiles_per_expert * sizeof(short));\n+                    void * host_src2 = malloc(ne21 * nb21);\n+                    CL_CHECK(clEnqueueReadBuffer(backend_ctx->queue, extra2->data_device, CL_TRUE, offset2, ne21 * nb21, host_src2, 0, NULL, NULL));\n+                    int total_experts = nb21 / nb20;\n+                    int out_idx = 0;\n+                    for (int i_expert = 0; i_expert < ne02; i_expert++) {\n+                        for (int i_tile = 0; i_tile < num_tiles_per_expert; i_tile++) {\n+                            for (int j = 0; j < ne21; j++) {\n+                                for (int i = 0; i < ne20; i++) {\n+                                    int expert = ((int *)host_src2)[j * total_experts + i];\n+                                    if (i_expert == expert) {\n+                                        ((short *)host_src2_reorder)[out_idx] = static_cast<short>(expert);\n+                                        ((short *)host_src2_reorder)[out_idx + 1] = static_cast<short>(j * ne11 + (i % ne11));\n+                                        ((short *)host_src2_reorder)[out_idx + 2] = static_cast<short>(j * ne20 + i);\n+                                        ((short *)host_src2_reorder)[out_idx + 3] = static_cast<short>(i_tile);\n+                                        out_idx += 4;\n+                                    }\n+                                }\n+                            }\n+                        }\n+                    }\n+                    buf_src2 = clCreateBuffer(backend_ctx->context, CL_MEM_READ_ONLY | CL_MEM_COPY_HOST_PTR, ne20 * ne21 * 4 * num_tiles_per_expert * sizeof(short), host_src2_reorder, &status);\n+                    CL_CHECK(status);\n+\n+                    // set thread grid\n+                    global_size[0] = static_cast<size_t>(tile_size);\n+                    global_size[2] = static_cast<size_t>(ne20 * ne21 * num_tiles_per_expert);\n+                }\n+\n+                // create a sub_buffer for src1\n+                cl_buffer_region region;\n+                region.origin = offset1;\n+                region.size = ne10 * ne11 * ne12 * sizeof(float);\n+                src1_sub_buffer = clCreateSubBuffer(extra1->data_device, 0, CL_BUFFER_CREATE_TYPE_REGION, &region, &status);\n+                CL_CHECK(status);\n+\n+                // create image for src1\n+                cl_image_format image_format_buf_src1 = {CL_RGBA, CL_FLOAT};\n+                cl_image_desc image_desc_buf_src1 = {CL_MEM_OBJECT_IMAGE1D_BUFFER, static_cast<size_t>(ne10 * ne11 * ne12 / 4), 0,0,0,0,0,0,0, {src1_sub_buffer}};\n+                buf_src1_image = clCreateImage(backend_ctx->context, CL_MEM_READ_ONLY, &image_format_buf_src1, &image_desc_buf_src1, NULL, &status);\n+                CL_CHECK(status);\n+\n+                // Set kernel args\n+                int arg_idx = 0;\n+                CL_CHECK(clSetKernelArg(kernel, arg_idx++, sizeof(cl_mem),    &extra0_mxfp4->q));\n+                CL_CHECK(clSetKernelArg(kernel, arg_idx++, sizeof(cl_mem),    &extra0_mxfp4->e));\n+                CL_CHECK(clSetKernelArg(kernel, arg_idx++, sizeof(cl_mem),    &buf_src1_image));\n+                CL_CHECK(clSetKernelArg(kernel, arg_idx++, sizeof(cl_mem),    &buf_src2));\n+                CL_CHECK(clSetKernelArg(kernel, arg_idx++, sizeof(cl_mem),    &extrad->data_device));\n+                CL_CHECK(clSetKernelArg(kernel, arg_idx++, sizeof(cl_ulong),  &offsetd));\n+                CL_CHECK(clSetKernelArg(kernel, arg_idx++, sizeof(int),       &ne00));\n+                CL_CHECK(clSetKernelArg(kernel, arg_idx++, sizeof(int),       &ne01));\n+                if (ne12 == 1) {\n+                    CL_CHECK(clSetKernelArg(kernel, arg_idx++, sizeof(int),       &ne11));\n+                } else {\n+                    CL_CHECK(clSetKernelArg(kernel, arg_idx++, sizeof(int),       &tile_size));\n+                }\n+\n+                // launch kernel\n+                backend_ctx->enqueue_ndrange_kernel(kernel, 3, global_size, local_size, dst);\n+\n+                // deallocate sub buffers and images\n+                CL_CHECK(clReleaseMemObject(src1_sub_buffer));\n+                CL_CHECK(clReleaseMemObject(buf_src1_image));\n+                CL_CHECK(clReleaseMemObject(buf_src2));\n+                return;\n+            } // else fallback to generic kernel\n+#endif // GGML_OPENCL_USE_ADRENO_KERNELS\n+\n #ifdef GGML_OPENCL_SOA_Q\n             kernel = backend_ctx->kernel_mul_mv_id_mxfp4_f32_flat;\n "
      },
      {
        "filename": "ggml/src/ggml-opencl/kernels/cvt.cl",
        "status": "modified",
        "additions": 42,
        "deletions": 0,
        "changes": 42,
        "patch": "@@ -147,6 +147,27 @@ kernel void kernel_convert_block_mxfp4(\n     }\n }\n \n+kernel void kernel_convert_block_mxfp4_trans(\n+    global struct block_mxfp4 * src0,\n+    __global uint4 * dst_q,\n+    __global uchar * dst_e,\n+    uint ne00,\n+    uint ne01\n+) {\n+    int i00 = get_global_id(1);\n+    uint i01 = get_global_id(0);\n+    uint i02 = get_global_id(2);\n+\n+    uint ne00_blk = ne00 / QK_MXFP4;\n+    uint src_blk_offset = i00 + i01 * ne00_blk + i02 * ne00_blk * ne01;\n+    uint dst_blk_offset = i01 + i00 * ne01 + i02 * ne00_blk * ne01;\n+\n+    global struct block_mxfp4 * b = src0 + src_blk_offset;\n+\n+    dst_q[dst_blk_offset] = ((global uint4 *)(&(b->qs[0])))[0];\n+    dst_e[dst_blk_offset] = b->e;\n+}\n+\n kernel void kernel_restore_block_mxfp4(\n     global uchar * src_q,\n     global half  * src_e,\n@@ -162,6 +183,27 @@ kernel void kernel_restore_block_mxfp4(\n     }\n }\n \n+kernel void kernel_restore_block_mxfp4_trans(\n+    __global uint4 * src_q,\n+    __global uchar * src_e,\n+    global struct block_mxfp4 * dst,\n+    uint ne00,\n+    uint ne01\n+) {\n+    int i00 = get_global_id(1);\n+    uint i01 = get_global_id(0);\n+    uint i02 = get_global_id(2);\n+\n+    uint ne00_blk = ne00 / QK_MXFP4;\n+    uint src_blk_offset = i01 + i00 * ne01 + i02 * ne00_blk * ne01;\n+    uint dst_blk_offset = i00 + i01 * ne00_blk + i02 * ne00_blk * ne01;\n+\n+    global struct block_mxfp4 * b = dst + dst_blk_offset;\n+\n+    ((global uint4 *)(&(b->qs[0])))[0] = src_q[src_blk_offset];\n+    b->e = src_e[src_blk_offset];\n+}\n+\n //------------------------------------------------------------------------------\n // block_q8_0\n //------------------------------------------------------------------------------"
      },
      {
        "filename": "ggml/src/ggml-opencl/kernels/gemm_moe_mxfp4_f32.cl",
        "status": "added",
        "additions": 162,
        "deletions": 0,
        "changes": 162,
        "patch": "@@ -0,0 +1,162 @@\n+#pragma OPENCL EXTENSION cl_khr_fp16 : enable\n+#pragma OPENCL EXTENSION cl_khr_subgroups : enable\n+#pragma OPENCL EXTENSION cl_qcom_reqd_sub_group_size : enable\n+\n+#define QK_MXFP4 32\n+#define N_SIMDGROUP 2\n+#define SIMDGROUP_WIDTH 64\n+\n+static inline half8 mxfp4_to_fp16_packed8(ushort2 fp4x8) { //, ushort 0x0E00, ushort 0x8000) {\n+    ushort2 fp16_packed_a_0, fp16_packed_b_0, bias_a, bias_b, sign_a, sign_b;\n+    fp16_packed_a_0.lo = (fp4x8.s0 << 9) & 0x0E00;\n+    fp16_packed_a_0.hi = (fp4x8.s0 << 5) & 0x0E00;\n+    fp16_packed_b_0.lo = (fp4x8.s0 << 1) & 0x0E00;\n+    fp16_packed_b_0.hi = (fp4x8.s0 >> 3) & 0x0E00;\n+\n+    bias_a.lo = (fp16_packed_a_0.lo != 0) ? 0x3800 : 0x0;\n+    bias_a.hi = (fp16_packed_a_0.hi != 0) ? 0x3800 : 0x0;\n+    bias_b.lo = (fp16_packed_b_0.lo != 0) ? 0x3800 : 0x0;\n+    bias_b.hi = (fp16_packed_b_0.hi != 0) ? 0x3800 : 0x0;\n+\n+    fp16_packed_a_0.lo = (fp16_packed_a_0.lo != 0x0200) ? fp16_packed_a_0.lo : 0x0;\n+    fp16_packed_a_0.hi = (fp16_packed_a_0.hi != 0x0200) ? fp16_packed_a_0.hi : 0x0;\n+    fp16_packed_b_0.lo = (fp16_packed_b_0.lo != 0x0200) ? fp16_packed_b_0.lo : 0x0;\n+    fp16_packed_b_0.hi = (fp16_packed_b_0.hi != 0x0200) ? fp16_packed_b_0.hi : 0x0;\n+\n+    sign_a.lo = (fp4x8.s0 << 12) & 0x8000;\n+    sign_a.hi = (fp4x8.s0 << 8) & 0x8000;\n+    sign_b.lo = (fp4x8.s0 << 4) & 0x8000;\n+    sign_b.hi = fp4x8.s0 & 0x8000;\n+\n+    fp16_packed_a_0 = sign_a + bias_a + fp16_packed_a_0;\n+    fp16_packed_b_0 = sign_b + bias_b + fp16_packed_b_0;\n+\n+    ushort2 fp16_packed_a_1, fp16_packed_b_1;\n+    fp16_packed_a_1.lo = (fp4x8.s1 << 9) & 0x0E00;\n+    fp16_packed_a_1.hi = (fp4x8.s1 << 5) & 0x0E00;\n+    fp16_packed_b_1.lo = (fp4x8.s1 << 1) & 0x0E00;\n+    fp16_packed_b_1.hi = (fp4x8.s1 >> 3) & 0x0E00;\n+\n+    bias_a.lo = (fp16_packed_a_1.lo != 0) ? 0x3800 : 0x0;\n+    bias_a.hi = (fp16_packed_a_1.hi != 0) ? 0x3800 : 0x0;\n+    bias_b.lo = (fp16_packed_b_1.lo != 0) ? 0x3800 : 0x0;\n+    bias_b.hi = (fp16_packed_b_1.hi != 0) ? 0x3800 : 0x0;\n+\n+    fp16_packed_a_1.lo = (fp16_packed_a_1.lo != 0x0200) ? fp16_packed_a_1.lo : 0x0;\n+    fp16_packed_a_1.hi = (fp16_packed_a_1.hi != 0x0200) ? fp16_packed_a_1.hi : 0x0;\n+    fp16_packed_b_1.lo = (fp16_packed_b_1.lo != 0x0200) ? fp16_packed_b_1.lo : 0x0;\n+    fp16_packed_b_1.hi = (fp16_packed_b_1.hi != 0x0200) ? fp16_packed_b_1.hi : 0x0;\n+\n+    sign_a.lo = (fp4x8.s1 << 12) & 0x8000;\n+    sign_a.hi = (fp4x8.s1 << 8) & 0x8000;\n+    sign_b.lo = (fp4x8.s1 << 4) & 0x8000;\n+    sign_b.hi = fp4x8.s1 & 0x8000;\n+\n+    fp16_packed_a_1 = sign_a + bias_a + fp16_packed_a_1;\n+    fp16_packed_b_1 = sign_b + bias_b + fp16_packed_b_1;\n+\n+    return as_half8((ushort8)(fp16_packed_a_0, fp16_packed_b_0, fp16_packed_a_1, fp16_packed_b_1));\n+}\n+\n+static inline float e8m0_to_fp32(uchar x) {\n+    int bits;\n+    bits = (x == 0) ? 0x00400000 : ((uint) x << 23);\n+    return as_float(bits);\n+}\n+\n+\n+__attribute__((qcom_reqd_sub_group_size(\"half\")))\n+__kernel void kernel_gemm_moe_mxfp4_f32(\n+    __global uint4 * src0_q,\n+    __global uchar * src0_e,\n+    __read_only image1d_buffer_t src1,\n+    __global ushort4 * src2,\n+    __global float * dst,\n+    ulong         offsetd,\n+    int           ne00,\n+    int           ne01,\n+    int           tile_size\n+) {\n+    uint i01  = get_global_id(0);\n+    uint i20  = get_global_id(2);\n+    uint sgid = get_local_id(1);\n+    uint slid = get_sub_group_local_id();\n+\n+    ushort4 router = src2[i20];\n+    ushort expert_id = router.x;\n+    ushort i11 = router.y;\n+    ushort i1 = router.z;\n+    ushort tile_id = router.w;\n+\n+    if (tile_id * tile_size + i01 >= ne01) { // handle edge case when ne01 is not multiple of tile_size\n+        return;\n+    }\n+\n+    uint expert_offset = expert_id * ne00 * ne01 / 32;\n+    uint tile_offset = expert_offset + tile_id * tile_size + i01;\n+\n+    __private float sum = 0.0f; // each thread calculate partial sum of one output\n+\n+    // loop along ne00 in block granularity, skip 4 blocks every iter\n+    for (uint ib00 = sgid; ib00 < (ne00 / QK_MXFP4); ib00 += N_SIMDGROUP) {\n+        // load one block of q\n+        uint4 regQ = src0_q[tile_offset + ib00 * ne01];\n+        // convert 8 fp4 to fp16\n+        half8 fp16x8 = mxfp4_to_fp16_packed8(as_ushort2(regQ.s0));\n+\n+        uint offset = i11 * ne00 / 4 + ib00 * 8;\n+        float4 shared_y4;\n+        shared_y4 = read_imagef(src1, (offset + 0));\n+        float4 acc = shared_y4 * (float4)(fp16x8.s0, fp16x8.s2, fp16x8.s4, fp16x8.s6);\n+\n+        shared_y4 = read_imagef(src1, (offset + 4));\n+        acc += shared_y4 * (float4)(fp16x8.s1, fp16x8.s3, fp16x8.s5, fp16x8.s7);\n+\n+\n+        fp16x8 = mxfp4_to_fp16_packed8(as_ushort2(regQ.s1));\n+\n+        shared_y4 = read_imagef(src1, (offset + 1));\n+        acc += shared_y4 * (float4)(fp16x8.s0, fp16x8.s2, fp16x8.s4, fp16x8.s6);\n+\n+        shared_y4 = read_imagef(src1, (offset + 5));\n+        acc += shared_y4 * (float4)(fp16x8.s1, fp16x8.s3, fp16x8.s5, fp16x8.s7);\n+\n+\n+        fp16x8 = mxfp4_to_fp16_packed8(as_ushort2(regQ.s2));\n+\n+        shared_y4 = read_imagef(src1, (offset + 2));\n+        acc += shared_y4 * (float4)(fp16x8.s0, fp16x8.s2, fp16x8.s4, fp16x8.s6);\n+\n+        shared_y4 = read_imagef(src1, (offset + 6));\n+        acc += shared_y4 * (float4)(fp16x8.s1, fp16x8.s3, fp16x8.s5, fp16x8.s7);\n+\n+\n+        fp16x8 = mxfp4_to_fp16_packed8(as_ushort2(regQ.s3));\n+\n+        shared_y4 = read_imagef(src1, (offset + 3));\n+        acc += shared_y4 * (float4)(fp16x8.s0, fp16x8.s2, fp16x8.s4, fp16x8.s6);\n+\n+        shared_y4 = read_imagef(src1, (offset + 7));\n+        acc += shared_y4 * (float4)(fp16x8.s1, fp16x8.s3, fp16x8.s5, fp16x8.s7);\n+\n+        uchar regE = src0_e[tile_offset + ib00 * ne01];\n+        sum += e8m0_to_fp32(regE) * ((acc.s0 + acc.s1) + (acc.s2 + acc.s3));\n+    }\n+\n+    // reduction in local memory, assumes #subgroups=4\n+    __local float reduceLM[SIMDGROUP_WIDTH * (N_SIMDGROUP - 1)];\n+    if (sgid == 1) reduceLM[SIMDGROUP_WIDTH * 0 + slid] = sum;\n+    // if (sgid == 2) reduceLM[SIMDGROUP_WIDTH * 1 + slid] = sum;\n+    // if (sgid == 3) reduceLM[SIMDGROUP_WIDTH * 2 + slid] = sum;\n+    barrier(CLK_LOCAL_MEM_FENCE);\n+    if (sgid == 0) sum += reduceLM[SIMDGROUP_WIDTH * 0 + slid];\n+    // if (sgid == 0) sum += reduceLM[SIMDGROUP_WIDTH * 1 + slid];\n+    // if (sgid == 0) sum += reduceLM[SIMDGROUP_WIDTH * 2 + slid];\n+\n+    // 1 outputs per thread in subgroup 0\n+    if (sgid == 0) {\n+        dst = dst + (offsetd >> 2);\n+        dst[i01 + tile_id * tile_size + i1 * ne01] = sum;\n+    }\n+\n+}"
      },
      {
        "filename": "ggml/src/ggml-opencl/kernels/gemv_moe_mxfp4_f32.cl",
        "status": "added",
        "additions": 156,
        "deletions": 0,
        "changes": 156,
        "patch": "@@ -0,0 +1,156 @@\n+#pragma OPENCL EXTENSION cl_khr_fp16 : enable\n+#pragma OPENCL EXTENSION cl_khr_subgroups : enable\n+#pragma OPENCL EXTENSION cl_qcom_reqd_sub_group_size : enable\n+\n+#define QK_MXFP4 32\n+#define N_SIMDGROUP 4\n+#define SIMDGROUP_WIDTH 64\n+\n+static inline half8 mxfp4_to_fp16_packed8(ushort2 fp4x8) { //, ushort 0x0E00, ushort 0x8000) {\n+    ushort2 fp16_packed_a_0, fp16_packed_b_0, bias_a, bias_b, sign_a, sign_b;\n+    fp16_packed_a_0.lo = (fp4x8.s0 << 9) & 0x0E00;\n+    fp16_packed_a_0.hi = (fp4x8.s0 << 5) & 0x0E00;\n+    fp16_packed_b_0.lo = (fp4x8.s0 << 1) & 0x0E00;\n+    fp16_packed_b_0.hi = (fp4x8.s0 >> 3) & 0x0E00;\n+\n+    bias_a.lo = (fp16_packed_a_0.lo != 0) ? 0x3800 : 0x0;\n+    bias_a.hi = (fp16_packed_a_0.hi != 0) ? 0x3800 : 0x0;\n+    bias_b.lo = (fp16_packed_b_0.lo != 0) ? 0x3800 : 0x0;\n+    bias_b.hi = (fp16_packed_b_0.hi != 0) ? 0x3800 : 0x0;\n+\n+    fp16_packed_a_0.lo = (fp16_packed_a_0.lo != 0x0200) ? fp16_packed_a_0.lo : 0x0;\n+    fp16_packed_a_0.hi = (fp16_packed_a_0.hi != 0x0200) ? fp16_packed_a_0.hi : 0x0;\n+    fp16_packed_b_0.lo = (fp16_packed_b_0.lo != 0x0200) ? fp16_packed_b_0.lo : 0x0;\n+    fp16_packed_b_0.hi = (fp16_packed_b_0.hi != 0x0200) ? fp16_packed_b_0.hi : 0x0;\n+\n+    sign_a.lo = (fp4x8.s0 << 12) & 0x8000;\n+    sign_a.hi = (fp4x8.s0 << 8) & 0x8000;\n+    sign_b.lo = (fp4x8.s0 << 4) & 0x8000;\n+    sign_b.hi = fp4x8.s0 & 0x8000;\n+\n+    fp16_packed_a_0 = sign_a + bias_a + fp16_packed_a_0;\n+    fp16_packed_b_0 = sign_b + bias_b + fp16_packed_b_0;\n+\n+    ushort2 fp16_packed_a_1, fp16_packed_b_1;\n+    fp16_packed_a_1.lo = (fp4x8.s1 << 9) & 0x0E00;\n+    fp16_packed_a_1.hi = (fp4x8.s1 << 5) & 0x0E00;\n+    fp16_packed_b_1.lo = (fp4x8.s1 << 1) & 0x0E00;\n+    fp16_packed_b_1.hi = (fp4x8.s1 >> 3) & 0x0E00;\n+\n+    bias_a.lo = (fp16_packed_a_1.lo != 0) ? 0x3800 : 0x0;\n+    bias_a.hi = (fp16_packed_a_1.hi != 0) ? 0x3800 : 0x0;\n+    bias_b.lo = (fp16_packed_b_1.lo != 0) ? 0x3800 : 0x0;\n+    bias_b.hi = (fp16_packed_b_1.hi != 0) ? 0x3800 : 0x0;\n+\n+    fp16_packed_a_1.lo = (fp16_packed_a_1.lo != 0x0200) ? fp16_packed_a_1.lo : 0x0;\n+    fp16_packed_a_1.hi = (fp16_packed_a_1.hi != 0x0200) ? fp16_packed_a_1.hi : 0x0;\n+    fp16_packed_b_1.lo = (fp16_packed_b_1.lo != 0x0200) ? fp16_packed_b_1.lo : 0x0;\n+    fp16_packed_b_1.hi = (fp16_packed_b_1.hi != 0x0200) ? fp16_packed_b_1.hi : 0x0;\n+\n+    sign_a.lo = (fp4x8.s1 << 12) & 0x8000;\n+    sign_a.hi = (fp4x8.s1 << 8) & 0x8000;\n+    sign_b.lo = (fp4x8.s1 << 4) & 0x8000;\n+    sign_b.hi = fp4x8.s1 & 0x8000;\n+\n+    fp16_packed_a_1 = sign_a + bias_a + fp16_packed_a_1;\n+    fp16_packed_b_1 = sign_b + bias_b + fp16_packed_b_1;\n+\n+    return as_half8((ushort8)(fp16_packed_a_0, fp16_packed_b_0, fp16_packed_a_1, fp16_packed_b_1));\n+}\n+\n+static inline float e8m0_to_fp32(uchar x) {\n+    int bits;\n+    bits = (x == 0) ? 0x00400000 : ((uint) x << 23);\n+    return as_float(bits);\n+}\n+\n+\n+__attribute__((qcom_reqd_sub_group_size(\"half\")))\n+__kernel void kernel_gemv_moe_mxfp4_f32(\n+    __global uint4 * src0_q,\n+    __global uchar * src0_e,\n+    __read_only image1d_buffer_t src1,\n+    __global uint * src2,\n+    __global float * dst,\n+    ulong         offsetd,\n+    int           ne00,\n+    int           ne01,\n+    int           ne11\n+) {\n+    uint i01  = get_global_id(0);\n+    uint i20  = get_global_id(2);\n+    uint sgid = get_local_id(1);\n+    uint slid = get_sub_group_local_id();\n+\n+    uint i11 = i20 % ne11;\n+\n+    uint expert_id = src2[i20];\n+    uint expert_offset = expert_id * ne00 * ne01 / 32;\n+\n+    __private float sum = 0.0f; // each thread calculate partial sum of one output\n+\n+    // loop along ne00 in block granularity, skip 4 blocks every iter\n+    for (uint ib00 = sgid; ib00 < (ne00 / QK_MXFP4); ib00 += N_SIMDGROUP) {\n+\n+        // load one block of q\n+        uint4 regQ = src0_q[expert_offset + ib00 * ne01 + i01];\n+\n+        uint offset = i11 * ne00 / 4 + ib00 * 8;\n+\n+        half8 fp16x8 = mxfp4_to_fp16_packed8(as_ushort2(regQ.s0));\n+\n+        float4 shared_y4;\n+        shared_y4 = read_imagef(src1, (offset + 0));\n+        float4 acc = shared_y4 * (float4)(fp16x8.s0, fp16x8.s2, fp16x8.s4, fp16x8.s6);\n+\n+        shared_y4 = read_imagef(src1, (offset + 4));\n+        acc += shared_y4 * (float4)(fp16x8.s1, fp16x8.s3, fp16x8.s5, fp16x8.s7);\n+\n+\n+        fp16x8 = mxfp4_to_fp16_packed8(as_ushort2(regQ.s1));\n+\n+        shared_y4 = read_imagef(src1, (offset + 1));\n+        acc += shared_y4 * (float4)(fp16x8.s0, fp16x8.s2, fp16x8.s4, fp16x8.s6);\n+\n+        shared_y4 = read_imagef(src1, (offset + 5));\n+        acc += shared_y4 * (float4)(fp16x8.s1, fp16x8.s3, fp16x8.s5, fp16x8.s7);\n+\n+\n+        fp16x8 = mxfp4_to_fp16_packed8(as_ushort2(regQ.s2));\n+\n+        shared_y4 = read_imagef(src1, (offset + 2));\n+        acc += shared_y4 * (float4)(fp16x8.s0, fp16x8.s2, fp16x8.s4, fp16x8.s6);\n+\n+        shared_y4 = read_imagef(src1, (offset + 6));\n+        acc += shared_y4 * (float4)(fp16x8.s1, fp16x8.s3, fp16x8.s5, fp16x8.s7);\n+\n+\n+        fp16x8 = mxfp4_to_fp16_packed8(as_ushort2(regQ.s3));\n+\n+        shared_y4 = read_imagef(src1, (offset + 3));\n+        acc += shared_y4 * (float4)(fp16x8.s0, fp16x8.s2, fp16x8.s4, fp16x8.s6);\n+\n+        shared_y4 = read_imagef(src1, (offset + 7));\n+        acc += shared_y4 * (float4)(fp16x8.s1, fp16x8.s3, fp16x8.s5, fp16x8.s7);\n+\n+        uchar regE = src0_e[ib00 * ne01 + i01 + expert_offset];\n+        sum += e8m0_to_fp32(regE) * ((acc.s0 + acc.s1) + (acc.s2 + acc.s3));\n+    }\n+\n+    // reduction in local memory, assumes #subgroups=4\n+    __local float reduceLM[SIMDGROUP_WIDTH * (N_SIMDGROUP - 1)];\n+    if (sgid == 1) reduceLM[SIMDGROUP_WIDTH * 0 + slid] = sum;\n+    if (sgid == 2) reduceLM[SIMDGROUP_WIDTH * 1 + slid] = sum;\n+    if (sgid == 3) reduceLM[SIMDGROUP_WIDTH * 2 + slid] = sum;\n+    barrier(CLK_LOCAL_MEM_FENCE);\n+    if (sgid == 0) sum += reduceLM[SIMDGROUP_WIDTH * 0 + slid];\n+    if (sgid == 0) sum += reduceLM[SIMDGROUP_WIDTH * 1 + slid];\n+    if (sgid == 0) sum += reduceLM[SIMDGROUP_WIDTH * 2 + slid];\n+\n+    // 1 outputs per thread in subgroup 0\n+    if (sgid == 0) {\n+        dst = dst + (offsetd >> 2);\n+        dst[i01 + i20 * ne01] = sum;\n+    }\n+\n+}"
      }
    ],
    "num_files": 5,
    "scraped_at": "2025-11-16T23:29:57.462850"
  },
  {
    "pr_number": 16576,
    "title": "metal : avoid using Metal's gpuAddress property",
    "body": "alt #15985, #16565\r\n\r\nThe [`gpuAddress`](https://developer.apple.com/documentation/metal/mtlbuffer/gpuaddress) property is not always available. Replace it with an atomic counter.",
    "html_url": "https://github.com/ggml-org/llama.cpp/pull/16576",
    "created_at": "2025-10-14T11:17:47Z",
    "merged_at": "2025-10-14T17:33:05Z",
    "merge_commit_sha": "fa882fd2b1bcb663de23af06fdc391489d05b007",
    "base_ref": "master",
    "head_sha": "84e3d8d26961cca81de65b1790506121dda45bf5",
    "user": "ggerganov",
    "files": [
      {
        "filename": "ggml/src/ggml-metal/ggml-metal-device.m",
        "status": "modified",
        "additions": 14,
        "deletions": 10,
        "changes": 24,
        "patch": "@@ -7,6 +7,8 @@\n \n #include <Metal/Metal.h>\n \n+#include <stdatomic.h>\n+\n #ifndef TARGET_OS_VISION\n #define TARGET_OS_VISION 0\n #endif\n@@ -22,6 +24,9 @@\n // overload of MTLGPUFamilyMetal3 (not available in some environments)\n static const NSInteger MTLGPUFamilyMetal3_GGML = 5001;\n \n+// virtual address for GPU memory allocations\n+static atomic_uintptr_t g_addr_device = 0x000000400ULL;\n+\n #if !GGML_METAL_EMBED_LIBRARY\n // Here to assist with NSBundle Path Hack\n @interface GGMLMetalClass : NSObject\n@@ -827,7 +832,7 @@ bool ggml_metal_device_supports_op(ggml_metal_device_t dev, const struct ggml_te\n };\n \n struct ggml_metal_buffer {\n-    void * all_data; // TODO: https://github.com/ggml-org/llama.cpp/pull/15985\n+    void * all_data;\n     size_t all_size;\n \n     // if false, the Metal buffer data is allocated in private GPU memory and is not shared with the host\n@@ -965,14 +970,15 @@ ggml_metal_buffer_t ggml_metal_buffer_init(ggml_metal_device_t dev, size_t size,\n     if (shared) {\n         res->all_data = ggml_metal_host_malloc(size_aligned);\n         res->is_shared = true;\n-        res->owned = true;\n     } else {\n-        // dummy, non-NULL value - we'll populate this after creating the Metal buffer below\n-        res->all_data = (void *) 0x000000400ULL;\n+        // use virtual address from g_addr_device counter\n+        res->all_data = (void *) atomic_fetch_add_explicit(&g_addr_device, size_aligned, memory_order_relaxed);\n         res->is_shared = false;\n     }\n     res->all_size = size_aligned;\n \n+    res->owned = true;\n+\n     res->device = ggml_metal_device_get_obj(dev);\n     res->queue  = ggml_metal_device_get_queue(dev);\n \n@@ -983,15 +989,13 @@ ggml_metal_buffer_t ggml_metal_buffer_init(ggml_metal_device_t dev, size_t size,\n         res->buffers[0].metal = nil;\n \n         if (size_aligned > 0) {\n-            if (props_dev->use_shared_buffers &&shared) {\n+            if (props_dev->use_shared_buffers && shared) {\n                 res->buffers[0].metal = [res->device newBufferWithBytesNoCopy:res->all_data\n                                                                   length:size_aligned\n                                                                  options:MTLResourceStorageModeShared\n                                                              deallocator:nil];\n             } else {\n                 res->buffers[0].metal = [res->device newBufferWithLength:size_aligned options:MTLResourceStorageModePrivate];\n-\n-                res->all_data = (void *) (res->buffers[0].metal.gpuAddress);\n             }\n         }\n \n@@ -1139,7 +1143,7 @@ bool ggml_metal_buffer_is_shared(ggml_metal_buffer_t buf) {\n \n void ggml_metal_buffer_memset_tensor(ggml_metal_buffer_t buf, struct ggml_tensor * tensor, uint8_t value, size_t offset, size_t size) {\n     if (buf->is_shared) {\n-        memset((char *)tensor->data + offset, value, size);\n+        memset((char *) tensor->data + offset, value, size);\n         return;\n     }\n \n@@ -1168,7 +1172,7 @@ void ggml_metal_buffer_memset_tensor(ggml_metal_buffer_t buf, struct ggml_tensor\n \n void ggml_metal_buffer_set_tensor(ggml_metal_buffer_t buf, struct ggml_tensor * tensor, const void * data, size_t offset, size_t size) {\n     if (buf->is_shared) {\n-        memcpy((char *)tensor->data + offset, data, size);\n+        memcpy((char *) tensor->data + offset, data, size);\n         return;\n     }\n \n@@ -1223,7 +1227,7 @@ void ggml_metal_buffer_set_tensor(ggml_metal_buffer_t buf, struct ggml_tensor *\n \n void ggml_metal_buffer_get_tensor(ggml_metal_buffer_t buf, const struct ggml_tensor * tensor, void * data, size_t offset, size_t size) {\n     if (buf->is_shared) {\n-        memcpy(data, (const char *)tensor->data + offset, size);\n+        memcpy(data, (const char *) tensor->data + offset, size);\n         return;\n     }\n "
      },
      {
        "filename": "ggml/src/ggml-metal/ggml-metal-impl.h",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -251,6 +251,7 @@ typedef struct {\n     int32_t  sect_1;\n     int32_t  sect_2;\n     int32_t  sect_3;\n+    bool     src2;\n } ggml_metal_kargs_rope;\n \n typedef struct {"
      },
      {
        "filename": "ggml/src/ggml-metal/ggml-metal-ops.cpp",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -2969,6 +2969,7 @@ int ggml_metal_op_rope(ggml_metal_op_t ctx, int idx) {\n         /* sect_1      =*/ sect_1,\n         /* sect_2      =*/ sect_2,\n         /* sect_3      =*/ sect_3,\n+        /* src2        =*/ op->src[2] != nullptr,\n     };\n \n     ggml_metal_pipeline_t pipeline = ggml_metal_library_get_pipeline_rope(lib, op);"
      },
      {
        "filename": "ggml/src/ggml-metal/ggml-metal.metal",
        "status": "modified",
        "additions": 4,
        "deletions": 4,
        "changes": 8,
        "patch": "@@ -3748,7 +3748,7 @@ kernel void kernel_rope_norm(\n \n             const float theta = theta_base * pow(args.freq_base, inv_ndims*i0);\n \n-            const float freq_factor = src2 != src0 ? ((device const float *) src2)[ic] : 1.0f;\n+            const float freq_factor = args.src2 ? ((device const float *) src2)[ic] : 1.0f;\n \n             rope_yarn(theta/freq_factor, args.freq_scale, corr_dims, i0, args.ext_factor, args.attn_factor, &cos_theta, &sin_theta);\n \n@@ -3801,7 +3801,7 @@ kernel void kernel_rope_neox(\n \n             const float theta = theta_base * pow(args.freq_base, inv_ndims*i0);\n \n-            const float freq_factor = src2 != src0 ? ((device const float *) src2)[ic] : 1.0f;\n+            const float freq_factor = args.src2 ? ((device const float *) src2)[ic] : 1.0f;\n \n             rope_yarn(theta/freq_factor, args.freq_scale, corr_dims, i0, args.ext_factor, args.attn_factor, &cos_theta, &sin_theta);\n \n@@ -3872,7 +3872,7 @@ kernel void kernel_rope_multi(\n \n             const float theta = theta_base * pow(args.freq_base, inv_ndims*i0);\n \n-            const float freq_factor = src2 != src0 ? ((device const float *) src2)[ic] : 1.0f;\n+            const float freq_factor = args.src2 ? ((device const float *) src2)[ic] : 1.0f;\n \n             rope_yarn(theta/freq_factor, args.freq_scale, corr_dims, i0, args.ext_factor, args.attn_factor, &cos_theta, &sin_theta);\n \n@@ -3939,7 +3939,7 @@ kernel void kernel_rope_vision(\n             const float theta = theta_base * pow(args.freq_base, 2.0f * inv_ndims * p);\n             // end of mrope\n \n-            const float freq_factor = src2 != src0 ? ((device const float *) src2)[ic] : 1.0f;\n+            const float freq_factor = args.src2 ? ((device const float *) src2)[ic] : 1.0f;\n \n             rope_yarn(theta/freq_factor, args.freq_scale, corr_dims, i0, args.ext_factor, args.attn_factor, &cos_theta, &sin_theta);\n "
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T23:30:00.892780"
  },
  {
    "pr_number": 16559,
    "title": "metal: optimise `GGML_OP_SUM`",
    "body": "This PR optimises the `GGML_OP_SUM` operation, as implemented in https://github.com/ggml-org/llama.cpp/pull/16539.\r\n\r\nThe original implementation performed the sum op on one thread as follows:\r\n\r\n```cpp\r\n    ggml_metal_encoder_dispatch_threadgroups(enc, 1, 1, 1, 1, 1, 1);\r\n```\r\n\r\nresulting the following `./test-backend-ops perf -o SUM` log:\r\n\r\n```\r\n./test-backend-ops perf -o SUM\r\nggml_metal_library_init: using embedded metal library\r\nggml_metal_library_init: loaded in 0.007 sec\r\nggml_metal_device_init: GPU name:   Apple M1 Pro\r\nggml_metal_device_init: GPU family: MTLGPUFamilyApple7  (1007)\r\nggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)\r\nggml_metal_device_init: GPU family: MTLGPUFamilyMetal4  (5002)\r\nggml_metal_device_init: simdgroup reduction   = true\r\nggml_metal_device_init: simdgroup matrix mul. = true\r\nggml_metal_device_init: has unified memory    = true\r\nggml_metal_device_init: has bfloat            = true\r\nggml_metal_device_init: use residency sets    = true\r\nggml_metal_device_init: use shared buffers    = true\r\nggml_metal_device_init: recommendedMaxWorkingSetSize  = 12713.12 MB\r\nTesting 3 devices\r\n\r\nggml_metal_init: allocating\r\nggml_metal_init: found device: Apple M1 Pro\r\nggml_metal_init: picking default device: Apple M1 Pro\r\nggml_metal_init: use bfloat         = true\r\nggml_metal_init: use fusion         = true\r\nggml_metal_init: use concurrency    = true\r\nggml_metal_init: use graph optimize = true\r\nBackend 1/3: Metal\r\n  Device description: Apple M1 Pro\r\n  Device memory: 12124 MB (12123 MB free)\r\n\r\nggml_metal_library_compile_pipeline: compiling pipeline: base = 'kernel_op_sum_f32', name = 'kernel_op_sum_f32'\r\nggml_metal_library_compile_pipeline: loaded kernel_op_sum_f32                             0x102efc6b0 | th_max = 1024 | th_width =   32\r\n  SUM(type=f32,ne=[8192,1,1,1]):              163820 runs -     6.25 us/run -       32 kB/run -    0.02 GB/s\r\n```\r\n\r\n### Implementation\r\n\r\nTo fix this, I've modified the host-side code to launch `nth` threads in one threadgroup, so that each thread sums a strided chunk (similar to `op_sum_rows`).\r\n\r\n- `simd_sum(sumf)` is used to perform a partial sum within each SIMD group\r\n- A second `simd_sum(v)`is used to do a reduction across SIMD groups (only SIMD group 0 is involved here)\r\n- The `nth` value is calculated similarly to `op_sum_rows`\r\n\r\nResulting in the following performance log via `./test-backend-ops perf -o SUM`:\r\n\r\n```\r\nggml_metal_library_compile_pipeline: compiling pipeline: base = 'kernel_op_sum_f32', name = 'kernel_op_sum_f32'\r\nggml_metal_library_compile_pipeline: loaded kernel_op_sum_f32                             0x102efc6b0 | th_max = 1024 | th_width =   32\r\n  SUM(type=f32,ne=[8192,1,1,1]):              163820 runs -     6.25 us/run -       32 kB/run -    4.88 GB/s\r\n  SUM(type=f32,ne=[8192,8192,1,1]):                      128 runs - 23911.46 us/run -   262144 kB/run -   10.54 GB/s\r\n  SUM(type=f32,ne=[128,8192,1,1]):              8191 runs -   296.56 us/run -     4096 kB/run -   13.17 GB/s\r\n  Backend Metal: OK\r\nggml_metal_free: deallocating\r\nBackend 2/3: BLAS\r\n  Device description: Accelerate\r\n  Device memory: 0 MB (0 MB free)\r\n\r\n  SUM(type=f32,ne=[8192,1,1,1]): not supported\r\n  SUM(type=f32,ne=[8192,8192,1,1]): not supported\r\n  SUM(type=f32,ne=[128,8192,1,1]): not supported\r\n  Backend BLAS: OK\r\nBackend 3/3: CPU\r\n  Skipping CPU backend\r\n3/3 backends passed\r\nOK\r\n```\r\n\r\nNote that this is still quite a bit slower than `SUM_ROWS`,\r\n\r\n```\r\nggml_metal_library_compile_pipeline: compiling pipeline: base = 'kernel_sum_rows_f32', name = 'kernel_sum_rows_f32'\r\nggml_metal_library_compile_pipeline: loaded kernel_sum_rows_f32                           0x10552e7f0 | th_max = 1024 | th_width =   32\r\n  SUM_ROWS(type=f32,ne=[8192,1,1,1],permute=0,slice=0):               147438 runs -     6.84 us/run -       32 kB/run -    4.46 GB/s\r\n  SUM_ROWS(type=f32,ne=[8192,8192,1,1],permute=0,slice=0):               768 runs -  1467.22 us/run -   262176 kB/run -  171.74 GB/s\r\n  SUM_ROWS(type=f32,ne=[128,8192,1,1],permute=0,slice=0):              16258 runs -    80.57 us/run -     4128 kB/run -   48.87 GB/s\r\n  Backend Metal: OK\r\n```\r\n\r\nSo there may be room for improvement in the current kernel implementation.",
    "html_url": "https://github.com/ggml-org/llama.cpp/pull/16559",
    "created_at": "2025-10-13T09:57:13Z",
    "merged_at": "2025-10-15T14:05:56Z",
    "merge_commit_sha": "f4ce81c45e7bd910e36bf44c253fc5255c49b1e4",
    "base_ref": "master",
    "head_sha": "be1b67036949a8aea4390f7db65dfa3e42e9190b",
    "user": "cern1710",
    "files": [
      {
        "filename": "ggml/src/ggml-cuda/ggml-cuda.cu",
        "status": "modified",
        "additions": 2,
        "deletions": 1,
        "changes": 3,
        "patch": "@@ -3645,9 +3645,10 @@ static bool ggml_backend_cuda_device_supports_op(ggml_backend_dev_t dev, const g\n         case GGML_OP_CONV_2D_DW:\n         case GGML_OP_CONV_TRANSPOSE_2D:\n         case GGML_OP_POOL_2D:\n-        case GGML_OP_SUM:\n         case GGML_OP_ACC:\n             return true;\n+        case GGML_OP_SUM:\n+            return ggml_is_contiguous_rows(op->src[0]);\n         case GGML_OP_ARGSORT:\n             // TODO: Support arbitrary column width\n             return op->src[0]->ne[0] <= 1024;"
      },
      {
        "filename": "ggml/src/ggml-metal/ggml-metal-device.m",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -657,6 +657,7 @@ bool ggml_metal_device_supports_op(ggml_metal_device_t dev, const struct ggml_te\n         case GGML_OP_LOG:\n             return ggml_is_contiguous(op->src[0]) && op->src[0]->type == GGML_TYPE_F32;\n         case GGML_OP_SUM:\n+            return has_simdgroup_reduction && ggml_is_contiguous(op->src[0]);\n         case GGML_OP_SUM_ROWS:\n         case GGML_OP_MEAN:\n         case GGML_OP_SOFT_MAX:"
      },
      {
        "filename": "ggml/src/ggml-metal/ggml-metal-ops.cpp",
        "status": "modified",
        "additions": 14,
        "deletions": 1,
        "changes": 15,
        "patch": "@@ -866,12 +866,25 @@ int ggml_metal_op_sum(ggml_metal_op_t ctx, int idx) {\n \n     ggml_metal_pipeline_t pipeline = ggml_metal_library_get_pipeline_sum(lib, op);\n \n+    int nth = 32; // SIMD width\n+\n+    while (nth < (int) n && nth < ggml_metal_pipeline_max_theads_per_threadgroup(pipeline)) {\n+        nth *= 2;\n+    }\n+\n+    nth = std::min(nth, ggml_metal_pipeline_max_theads_per_threadgroup(pipeline));\n+    nth = std::min(nth, (int) n);\n+\n+    const int nsg = (nth + 31) / 32;\n+\n     ggml_metal_encoder_set_pipeline(enc, pipeline);\n     ggml_metal_encoder_set_bytes   (enc, &args, sizeof(args), 0);\n     ggml_metal_encoder_set_buffer  (enc, ggml_metal_get_buffer_id(op->src[0]), 1);\n     ggml_metal_encoder_set_buffer  (enc, ggml_metal_get_buffer_id(op),         2);\n \n-    ggml_metal_encoder_dispatch_threadgroups(enc, 1, 1, 1, 1, 1, 1);\n+    ggml_metal_encoder_set_threadgroup_memory_size(enc, nsg * sizeof(float), 0);\n+\n+    ggml_metal_encoder_dispatch_threadgroups(enc, 1, 1, 1, nth, 1, 1);\n \n     return 1;\n }"
      },
      {
        "filename": "ggml/src/ggml-metal/ggml-metal.metal",
        "status": "modified",
        "additions": 36,
        "deletions": 6,
        "changes": 42,
        "patch": "@@ -1727,18 +1727,48 @@ kernel void kernel_op_sum_f32(\n         constant ggml_metal_kargs_sum & args,\n         device const float * src0,\n         device       float * dst,\n-        ushort  tiitg[[thread_index_in_threadgroup]]) {\n+        threadgroup  float * shmem_f32 [[threadgroup(0)]],\n+        uint3   tgpig[[threadgroup_position_in_grid]],\n+        ushort3 tpitg[[thread_position_in_threadgroup]],\n+        ushort  sgitg[[simdgroup_index_in_threadgroup]],\n+        ushort  tiisg[[thread_index_in_simdgroup]],\n+        ushort3   ntg[[threads_per_threadgroup]]) {\n \n-    if (tiitg != 0) {\n+    if (args.np == 0) {\n         return;\n     }\n \n-    float acc = 0.0f;\n-    for (ulong i = 0; i < args.np; ++i) {\n-        acc += src0[i];\n+    const uint nsg = (ntg.x + 31) / 32;\n+\n+    float sumf = 0;\n+\n+    for (int64_t i0 = tpitg.x; i0 < args.np; i0 += ntg.x) {\n+        sumf += src0[i0];\n     }\n \n-    dst[0] = acc;\n+    sumf = simd_sum(sumf);\n+\n+    if (tiisg == 0) {\n+        shmem_f32[sgitg] = sumf;\n+    }\n+\n+    threadgroup_barrier(mem_flags::mem_threadgroup);\n+\n+    float total = 0;\n+\n+    if (sgitg == 0) {\n+        float v = 0;\n+\n+        if (tpitg.x < nsg) {\n+            v = shmem_f32[tpitg.x];\n+        }\n+\n+        total = simd_sum(v);\n+\n+        if (tpitg.x == 0) {\n+            dst[0] = total;\n+        }\n+    }\n }\n \n template <bool norm>"
      },
      {
        "filename": "tests/test-backend-ops.cpp",
        "status": "modified",
        "additions": 18,
        "deletions": 3,
        "changes": 21,
        "patch": "@@ -4588,20 +4588,31 @@ struct test_topk_moe: public test_case {\n struct test_sum : public test_case {\n     const ggml_type type;\n     const std::array<int64_t, 4> ne;\n+    const std::array<int64_t, 4> permute;\n+    bool _use_permute;\n \n     std::string vars() override {\n-        return VARS_TO_STR2(type, ne);\n+        std::string v = VARS_TO_STR2(type, ne);\n+        if (_use_permute) v += \",\" + VAR_TO_STR(permute);\n+        return v;\n     }\n \n     test_sum(ggml_type type = GGML_TYPE_F32,\n-            std::array<int64_t, 4> ne = {10, 5, 4, 3})\n-        : type(type), ne(ne) {}\n+            std::array<int64_t, 4> ne = {10, 5, 4, 3},\n+            std::array<int64_t, 4> permute = {0, 0, 0, 0})\n+        : type(type), ne(ne), permute(permute),\n+            _use_permute(permute[0] + permute[1] + permute[2] + permute[3] > 0) {}\n \n     ggml_tensor * build_graph(ggml_context * ctx) override {\n         ggml_tensor * a = ggml_new_tensor(ctx, type, 4, ne.data());\n         ggml_set_param(a);\n         ggml_set_name(a, \"a\");\n \n+        if (_use_permute) {\n+            a = ggml_permute(ctx, a, permute[0], permute[1], permute[2], permute[3]);\n+            ggml_set_name(a, \"a_permuted\");\n+        }\n+\n         ggml_tensor * out = ggml_sum(ctx, a);\n         ggml_set_name(out, \"out\");\n \n@@ -6724,6 +6735,9 @@ static std::vector<std::unique_ptr<test_case>> make_test_cases_eval() {\n \n     test_cases.emplace_back(new test_sum());\n     test_cases.emplace_back(new test_sum_rows());\n+    test_cases.emplace_back(new test_sum(GGML_TYPE_F32, {11, 5, 6, 3}, {0, 2, 1, 3}));  // row-contiguous but non-contiguous\n+    test_cases.emplace_back(new test_sum(GGML_TYPE_F32, {11, 5, 6, 3}, {0, 3, 2, 1}));\n+    test_cases.emplace_back(new test_sum(GGML_TYPE_F32, {11, 5, 6, 3}, {0, 1, 3, 2}));\n     test_cases.emplace_back(new test_sum_rows(GGML_TYPE_F32, { 11, 5, 6, 3 }, true, false));\n     test_cases.emplace_back(new test_sum_rows(GGML_TYPE_F32, { 11, 5, 6, 3 }, false, true));\n     test_cases.emplace_back(new test_sum_rows(GGML_TYPE_F32, { 11, 5, 6, 3 }, true, true));\n@@ -6734,6 +6748,7 @@ static std::vector<std::unique_ptr<test_case>> make_test_cases_eval() {\n     test_cases.emplace_back(new test_sum(GGML_TYPE_F32, { 33, 1024, 1, 1 }));\n     test_cases.emplace_back(new test_sum_rows(GGML_TYPE_F32, { 33, 1024, 1, 1 }));\n     test_cases.emplace_back(new test_sum(GGML_TYPE_F32, { 33, 256, 1, 1 }));\n+    test_cases.emplace_back(new test_sum(GGML_TYPE_F32, { 33, 256, 1, 1 }, { 1, 0, 2, 3 })); // sum dst not-contiguous\n     test_cases.emplace_back(new test_sum_rows(GGML_TYPE_F32, { 33, 256, 1, 1 }));\n     test_cases.emplace_back(new test_mean(GGML_TYPE_F32, { 33, 256, 1, 1 }));\n     test_cases.emplace_back(new test_mean(GGML_TYPE_F32, { 32769, 1, 1, 1 }));"
      }
    ],
    "num_files": 5,
    "scraped_at": "2025-11-16T23:30:02.210250"
  },
  {
    "pr_number": 16542,
    "title": "Add `CONV_TRANSPOSE_2D` for Metal",
    "body": "This PR adds Metal-based implementation of CONV_TRANSPOSE_2D operation (#14909)\r\n\r\nTODO:\r\n\r\n- [x] Tests",
    "html_url": "https://github.com/ggml-org/llama.cpp/pull/16542",
    "created_at": "2025-10-12T19:59:23Z",
    "merged_at": "2025-10-17T06:33:58Z",
    "merge_commit_sha": "9ad4f1931ee0f3b41d9355245ef744786aaae0aa",
    "base_ref": "master",
    "head_sha": "9f3e11c7981f57383d08fa0572c1ac88b30c85e7",
    "user": "iliailmer",
    "files": [
      {
        "filename": "ggml/src/ggml-metal/ggml-metal-device.cpp",
        "status": "modified",
        "additions": 25,
        "deletions": 0,
        "changes": 25,
        "patch": "@@ -1387,6 +1387,31 @@ ggml_metal_pipeline_t ggml_metal_library_get_pipeline_conv_transpose_1d(ggml_met\n     return res;\n }\n \n+ggml_metal_pipeline_t ggml_metal_library_get_pipeline_conv_transpose_2d(ggml_metal_library_t lib, const ggml_tensor * op) {\n+    assert(op->op == GGML_OP_CONV_TRANSPOSE_2D);\n+\n+    GGML_ASSERT(ggml_is_contiguous(op->src[0]));\n+    GGML_ASSERT(ggml_is_contiguous(op->src[1]));\n+    GGML_ASSERT(op->src[0]->type == GGML_TYPE_F16 || op->src[0]->type == GGML_TYPE_F32);\n+    GGML_ASSERT(op->src[1]->type == GGML_TYPE_F32);\n+    GGML_ASSERT(op->type         == GGML_TYPE_F32);\n+\n+    char base[256];\n+    char name[256];\n+\n+    snprintf(base, 256, \"kernel_conv_transpose_2d_%s_%s\", ggml_type_name(op->src[0]->type), ggml_type_name(op->src[1]->type));\n+    snprintf(name, 256, \"%s\", base);\n+\n+    ggml_metal_pipeline_t res = ggml_metal_library_get_pipeline(lib, name);\n+    if (res) {\n+        return res;\n+    }\n+\n+    res = ggml_metal_library_compile_pipeline(lib, base, name, nullptr);\n+\n+    return res;\n+}\n+\n ggml_metal_pipeline_t ggml_metal_library_get_pipeline_upscale(ggml_metal_library_t lib, const ggml_tensor * op) {\n     assert(op->op == GGML_OP_UPSCALE);\n "
      },
      {
        "filename": "ggml/src/ggml-metal/ggml-metal-device.h",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -129,6 +129,7 @@ ggml_metal_pipeline_t ggml_metal_library_get_pipeline_norm              (ggml_me\n ggml_metal_pipeline_t ggml_metal_library_get_pipeline_rope              (ggml_metal_library_t lib, const struct ggml_tensor * op);\n ggml_metal_pipeline_t ggml_metal_library_get_pipeline_im2col            (ggml_metal_library_t lib, const struct ggml_tensor * op);\n ggml_metal_pipeline_t ggml_metal_library_get_pipeline_conv_transpose_1d (ggml_metal_library_t lib, const struct ggml_tensor * op);\n+ggml_metal_pipeline_t ggml_metal_library_get_pipeline_conv_transpose_2d (ggml_metal_library_t lib, const struct ggml_tensor * op);\n ggml_metal_pipeline_t ggml_metal_library_get_pipeline_upscale           (ggml_metal_library_t lib, const struct ggml_tensor * op);\n ggml_metal_pipeline_t ggml_metal_library_get_pipeline_pad               (ggml_metal_library_t lib, const struct ggml_tensor * op);\n ggml_metal_pipeline_t ggml_metal_library_get_pipeline_pad_reflect_1d    (ggml_metal_library_t lib, const struct ggml_tensor * op);"
      },
      {
        "filename": "ggml/src/ggml-metal/ggml-metal-device.m",
        "status": "modified",
        "additions": 5,
        "deletions": 0,
        "changes": 5,
        "patch": "@@ -648,6 +648,11 @@ bool ggml_metal_device_supports_op(ggml_metal_device_t dev, const struct ggml_te\n         case GGML_OP_SCALE:\n         case GGML_OP_CONV_TRANSPOSE_1D:\n             return true;\n+        case GGML_OP_CONV_TRANSPOSE_2D:\n+            return ggml_is_contiguous(op->src[0]) && ggml_is_contiguous(op->src[1]) &&\n+                (op->src[0]->type == GGML_TYPE_F16 || op->src[0]->type == GGML_TYPE_F32) &&\n+                op->src[1]->type == GGML_TYPE_F32 &&\n+                op->type == GGML_TYPE_F32;\n         case GGML_OP_CLAMP:\n             return op->src[0]->type == GGML_TYPE_F32;\n         case GGML_OP_SQR:"
      },
      {
        "filename": "ggml/src/ggml-metal/ggml-metal-impl.h",
        "status": "modified",
        "additions": 13,
        "deletions": 0,
        "changes": 13,
        "patch": "@@ -513,6 +513,19 @@ typedef struct {\n     uint64_t nb1;\n } ggml_metal_kargs_conv_transpose_1d;\n \n+typedef struct {\n+    int32_t  IC;\n+    int32_t  IH;\n+    int32_t  IW;\n+    int32_t  KH;\n+    int32_t  KW;\n+    int32_t  OC;\n+    int32_t  s0;\n+    uint64_t nb0;\n+    uint64_t nb1;\n+    uint64_t nb2;\n+} ggml_metal_kargs_conv_transpose_2d;\n+\n typedef struct {\n     uint64_t  ofs0;\n     uint64_t  ofs1;"
      },
      {
        "filename": "ggml/src/ggml-metal/ggml-metal-ops.cpp",
        "status": "modified",
        "additions": 60,
        "deletions": 0,
        "changes": 60,
        "patch": "@@ -364,6 +364,10 @@ static int ggml_metal_op_encode_impl(ggml_metal_op_t ctx, int idx) {\n             {\n                 n_fuse = ggml_metal_op_conv_transpose_1d(ctx, idx);\n             } break;\n+        case GGML_OP_CONV_TRANSPOSE_2D:\n+            {\n+                n_fuse = ggml_metal_op_conv_transpose_2d(ctx, idx);\n+            } break;\n         case GGML_OP_UPSCALE:\n             {\n                 n_fuse = ggml_metal_op_upscale(ctx, idx);\n@@ -3068,6 +3072,62 @@ int ggml_metal_op_conv_transpose_1d(ggml_metal_op_t ctx, int idx) {\n     return 1;\n }\n \n+int ggml_metal_op_conv_transpose_2d(ggml_metal_op_t ctx, int idx) {\n+    ggml_tensor * op = ctx->node(idx);\n+\n+    ggml_metal_library_t lib = ctx->lib;\n+    ggml_metal_encoder_t enc = ctx->enc;\n+\n+    GGML_TENSOR_LOCALS( int32_t, ne0, op->src[0], ne);\n+    GGML_TENSOR_LOCALS(uint64_t, nb0, op->src[0], nb);\n+    GGML_TENSOR_LOCALS( int32_t, ne1, op->src[1], ne);\n+    GGML_TENSOR_LOCALS(uint64_t, nb1, op->src[1], nb);\n+    GGML_TENSOR_LOCALS( int32_t, ne,  op,         ne);\n+    GGML_TENSOR_LOCALS(uint32_t, nb,  op,         nb);\n+\n+    const int32_t s0 = ((const int32_t *)(op->op_params))[0];\n+\n+    const int32_t IC = op->src[1]->ne[2];\n+    const int32_t IH = op->src[1]->ne[1];\n+    const int32_t IW = op->src[1]->ne[0];\n+\n+    const int32_t KH = op->src[0]->ne[1];\n+    const int32_t KW = op->src[0]->ne[0];\n+\n+    const int32_t OW = op->ne[0];\n+    const int32_t OH = op->ne[1];\n+    const int32_t OC = op->ne[2];\n+\n+    ggml_metal_kargs_conv_transpose_2d args = {\n+        /*.IC  =*/ IC,\n+        /*.IH  =*/ IH,\n+        /*.IW  =*/ IW,\n+        /*.KH  =*/ KH,\n+        /*.KW  =*/ KW,\n+        /*.OC  =*/ OC,\n+        /*.s0  =*/ s0,\n+        /*.nb0 =*/ nb0,\n+        /*.nb1 =*/ nb1,\n+        /*.nb2 =*/ nb2,\n+    };\n+\n+    ggml_metal_pipeline_t pipeline = ggml_metal_library_get_pipeline_conv_transpose_2d(lib, op);\n+\n+    ggml_metal_encoder_set_pipeline(enc, pipeline);\n+    ggml_metal_encoder_set_bytes   (enc, &args, sizeof(args), 0);\n+    ggml_metal_encoder_set_buffer  (enc, ggml_metal_get_buffer_id(op->src[0]), 1);\n+    ggml_metal_encoder_set_buffer  (enc, ggml_metal_get_buffer_id(op->src[1]), 2);\n+    ggml_metal_encoder_set_buffer  (enc, ggml_metal_get_buffer_id(op),         3);\n+\n+    // Metal requires buffer size to be multiple of 16 bytes\n+    const size_t smem = GGML_PAD(KW * KH * sizeof(float), 16);\n+    ggml_metal_encoder_set_threadgroup_memory_size(enc, smem, 0);\n+\n+    ggml_metal_encoder_dispatch_threadgroups(enc, OW, OH, OC, KW, KH, 1);\n+\n+    return 1;\n+}\n+\n int ggml_metal_op_upscale(ggml_metal_op_t ctx, int idx) {\n     ggml_tensor * op = ctx->node(idx);\n "
      },
      {
        "filename": "ggml/src/ggml-metal/ggml-metal-ops.h",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -70,6 +70,7 @@ int ggml_metal_op_norm              (ggml_metal_op_t ctx, int idx);\n int ggml_metal_op_rope              (ggml_metal_op_t ctx, int idx);\n int ggml_metal_op_im2col            (ggml_metal_op_t ctx, int idx);\n int ggml_metal_op_conv_transpose_1d (ggml_metal_op_t ctx, int idx);\n+int ggml_metal_op_conv_transpose_2d (ggml_metal_op_t ctx, int idx);\n int ggml_metal_op_upscale           (ggml_metal_op_t ctx, int idx);\n int ggml_metal_op_pad               (ggml_metal_op_t ctx, int idx);\n int ggml_metal_op_pad_reflect_1d    (ggml_metal_op_t ctx, int idx);"
      },
      {
        "filename": "ggml/src/ggml-metal/ggml-metal.metal",
        "status": "modified",
        "additions": 91,
        "deletions": 0,
        "changes": 91,
        "patch": "@@ -4131,6 +4131,97 @@ kernel void kernel_conv_transpose_1d<half>(\n     uint3   tgpig[[threadgroup_position_in_grid]],\n     uint3    tgpg[[threadgroups_per_grid]]);\n \n+\n+typedef void (conv_transpose_2d_t)(\n+        constant ggml_metal_kargs_conv_transpose_2d & args,\n+        device const float * src0,\n+        device const float * src1,\n+        device        char * dst,\n+        uint3   tgpig[[threadgroup_position_in_grid]],\n+        uint3    tgpg[[threadgroups_per_grid]]);\n+\n+template <typename T>\n+kernel void kernel_conv_transpose_2d(\n+        constant ggml_metal_kargs_conv_transpose_2d & args,\n+        device const T * src0,\n+        device const float * src1,\n+        device        char * dst,\n+        threadgroup float * shared_sum [[threadgroup(0)]],\n+        uint3   tgpig[[threadgroup_position_in_grid]],\n+        uint3   tpitg[[thread_position_in_threadgroup]],\n+        uint3     ntg[[threads_per_threadgroup]]) {\n+\n+    const int64_t out_x = tgpig[0];\n+    const int64_t out_y = tgpig[1];\n+    const int64_t out_c = tgpig[2];\n+\n+    const int64_t kw = tpitg[0];\n+    const int64_t kh = tpitg[1];\n+\n+    float v = 0.0f;\n+\n+    for (int64_t in_c = 0; in_c < args.IC; in_c++) {\n+        int64_t in_y = out_y - kh;\n+\n+        if (in_y < 0 || in_y % args.s0) continue;\n+\n+        in_y /= args.s0;\n+\n+        if (in_y >= args.IH) continue;\n+\n+        int64_t in_x = out_x - kw;\n+\n+        if (in_x < 0 || in_x % args.s0) continue;\n+\n+        in_x /= args.s0;\n+\n+        if (in_x >= args.IW) continue;\n+\n+        const int64_t input_idx = (args.IW * args.IH) * in_c + (args.IW) * in_y + in_x;\n+        const int64_t kernel_idx = (args.KH * args.KW * args.OC) * in_c + (args.KH * args.KW) * out_c + (args.KW) * kh + kw;\n+\n+        v += (float)src0[kernel_idx] * src1[input_idx];\n+    }\n+\n+    const uint tid = tpitg.y * ntg.x + tpitg.x;\n+    shared_sum[tid] = v;\n+\n+    threadgroup_barrier(mem_flags::mem_threadgroup);\n+\n+    if (tid == 0) {\n+        float total = 0.0f;\n+        const uint num_threads = ntg.x * ntg.y;\n+        for (uint i = 0; i < num_threads; i++) {\n+            total += shared_sum[i];\n+        }\n+\n+        device float * dst_ptr = (device float *) (dst + out_x*args.nb0 + out_y * args.nb1 + out_c*args.nb2);\n+        dst_ptr[0] = total;\n+    }\n+}\n+\n+template [[host_name(\"kernel_conv_transpose_2d_f32_f32\")]]\n+kernel void kernel_conv_transpose_2d<float>(\n+    constant ggml_metal_kargs_conv_transpose_2d & args,\n+    device const float * src0,\n+    device const float * src1,\n+    device        char * dst,\n+    threadgroup float * shared_sum [[threadgroup(0)]],\n+    uint3   tgpig[[threadgroup_position_in_grid]],\n+    uint3   tpitg[[thread_position_in_threadgroup]],\n+    uint3     ntg[[threads_per_threadgroup]]);\n+\n+template [[host_name(\"kernel_conv_transpose_2d_f16_f32\")]]\n+kernel void kernel_conv_transpose_2d<half>(\n+    constant ggml_metal_kargs_conv_transpose_2d & args,\n+    device const half  * src0,\n+    device const float * src1,\n+    device        char * dst,\n+    threadgroup float * shared_sum [[threadgroup(0)]],\n+    uint3   tgpig[[threadgroup_position_in_grid]],\n+    uint3   tpitg[[thread_position_in_threadgroup]],\n+    uint3     ntg[[threads_per_threadgroup]]);\n+\n kernel void kernel_upscale_f32(\n     constant ggml_metal_kargs_upscale & args,\n     device  const char * src0,"
      },
      {
        "filename": "tests/test-backend-ops.cpp",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -6952,6 +6952,8 @@ static std::vector<std::unique_ptr<test_case>> make_test_cases_perf() {\n     test_cases.emplace_back(new test_conv_2d_dw({512, 512, 256, 1}, {3, 3, 1, 256}, 1, 1, 1, true));\n \n     test_cases.emplace_back(new test_conv_transpose_2d({256, 256, 256, 1}, {3, 3, 16, 256}, 1));\n+    test_cases.emplace_back(new test_conv_transpose_2d({16, 16, 16, 1}, {3, 3, 8, 16}, 1));\n+    test_cases.emplace_back(new test_conv_transpose_2d({10, 10, 9, 1}, {3, 3, 1, 9}, 2));\n \n     test_cases.emplace_back(new test_mean(GGML_TYPE_F32, {256, 256, 3, 1}));\n "
      }
    ],
    "num_files": 8,
    "scraped_at": "2025-11-16T23:30:04.871521"
  },
  {
    "pr_number": 16539,
    "title": "metal: add support for opt_step_sgd",
    "body": "This PR adds Metal backend for the `OPT_STEP_SGD` operator.\r\n\r\n### Implementation\r\n\r\n- Added `kernel_opt_step_sgd_f32` kernel in `ggml-metal.metal`\r\n- Implemented argument struct `ggml_metal_kargs_opt_step_sgd`\r\n- Storing`np` in the struct as a passed `constant & args`\r\n- Threadgroup size calculation, similar to `ggml_metal_op_opt_step_sgd`\r\n\r\nI've ran `./test-opt` and `./test-backend-ops` to verify the validity of this implementation.\r\n\r\n**Note**: The implementation is mostly identical to https://github.com/ggml-org/llama.cpp/pull/16529, with the only major difference being the kernel itself.",
    "html_url": "https://github.com/ggml-org/llama.cpp/pull/16539",
    "created_at": "2025-10-12T19:19:14Z",
    "merged_at": "2025-10-13T08:25:02Z",
    "merge_commit_sha": "3f750f8d760ab5a61491e6a9409072dfeee4b4d7",
    "base_ref": "master",
    "head_sha": "248d9c56131bf293d954d36131b1a1f9a186f292",
    "user": "cern1710",
    "files": [
      {
        "filename": "ggml/src/ggml-metal/ggml-metal-device.cpp",
        "status": "modified",
        "additions": 19,
        "deletions": 0,
        "changes": 19,
        "patch": "@@ -1519,3 +1519,22 @@ ggml_metal_pipeline_t ggml_metal_library_get_pipeline_opt_step_adamw(ggml_metal_\n \n     return res;\n }\n+\n+ggml_metal_pipeline_t ggml_metal_library_get_pipeline_opt_step_sgd(ggml_metal_library_t lib, const ggml_tensor * op) {\n+    assert(op->op == GGML_OP_OPT_STEP_SGD);\n+\n+    char base[256];\n+    char name[256];\n+\n+    snprintf(base, 256, \"kernel_opt_step_sgd_%s\", ggml_type_name(op->src[0]->type));\n+    snprintf(name, 256, \"%s\", base);\n+\n+    ggml_metal_pipeline_t res = ggml_metal_library_get_pipeline(lib, name);\n+    if (res) {\n+        return res;\n+    }\n+\n+    res = ggml_metal_library_compile_pipeline(lib, base, name, nullptr);\n+\n+    return res;\n+}"
      },
      {
        "filename": "ggml/src/ggml-metal/ggml-metal-device.h",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -136,6 +136,7 @@ ggml_metal_pipeline_t ggml_metal_library_get_pipeline_pad_reflect_1d    (ggml_me\n ggml_metal_pipeline_t ggml_metal_library_get_pipeline_arange            (ggml_metal_library_t lib, const struct ggml_tensor * op);\n ggml_metal_pipeline_t ggml_metal_library_get_pipeline_timestep_embedding(ggml_metal_library_t lib, const struct ggml_tensor * op);\n ggml_metal_pipeline_t ggml_metal_library_get_pipeline_opt_step_adamw    (ggml_metal_library_t lib, const struct ggml_tensor * op);\n+ggml_metal_pipeline_t ggml_metal_library_get_pipeline_opt_step_sgd      (ggml_metal_library_t lib, const struct ggml_tensor * op);\n \n ggml_metal_pipeline_t ggml_metal_library_get_pipeline_flash_attn_ext_pad(\n         ggml_metal_library_t lib,"
      },
      {
        "filename": "ggml/src/ggml-metal/ggml-metal-device.m",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -800,6 +800,7 @@ bool ggml_metal_device_supports_op(ggml_metal_device_t dev, const struct ggml_te\n                 };\n             }\n         case GGML_OP_OPT_STEP_ADAMW:\n+        case GGML_OP_OPT_STEP_SGD:\n             return has_simdgroup_reduction;\n         default:\n             return false;"
      },
      {
        "filename": "ggml/src/ggml-metal/ggml-metal-impl.h",
        "status": "modified",
        "additions": 4,
        "deletions": 0,
        "changes": 4,
        "patch": "@@ -781,4 +781,8 @@ typedef struct {\n     int64_t  np;\n } ggml_metal_kargs_opt_step_adamw;\n \n+typedef struct {\n+    int64_t  np;\n+} ggml_metal_kargs_opt_step_sgd;\n+\n #endif // GGML_METAL_IMPL"
      },
      {
        "filename": "ggml/src/ggml-metal/ggml-metal-ops.cpp",
        "status": "modified",
        "additions": 38,
        "deletions": 0,
        "changes": 38,
        "patch": "@@ -418,6 +418,10 @@ static int ggml_metal_op_encode_impl(ggml_metal_op_t ctx, int idx) {\n             {\n                 n_fuse = ggml_metal_op_opt_step_adamw(ctx, idx);\n             } break;\n+        case GGML_OP_OPT_STEP_SGD:\n+            {\n+                n_fuse = ggml_metal_op_opt_step_sgd(ctx, idx);\n+            } break;\n        default:\n             {\n                 GGML_LOG_ERROR(\"%s: error: node %3d, op = %8s not implemented\\n\", __func__, idx, ggml_op_name(node->op));\n@@ -3469,3 +3473,37 @@ int ggml_metal_op_opt_step_adamw(ggml_metal_op_t ctx, int idx) {\n \n     return 1;\n }\n+\n+int ggml_metal_op_opt_step_sgd(ggml_metal_op_t ctx, int idx) {\n+    ggml_tensor * op = ctx->node(idx);\n+\n+    ggml_metal_library_t lib = ctx->lib;\n+    ggml_metal_encoder_t enc = ctx->enc;\n+\n+    GGML_TENSOR_LOCALS( int32_t, ne0, op->src[0], ne);\n+    GGML_TENSOR_LOCALS(uint64_t, nb0, op->src[0], nb);\n+    GGML_TENSOR_LOCALS( int32_t, ne,  op,         ne);\n+    GGML_TENSOR_LOCALS(uint32_t, nb,  op,         nb);\n+\n+    ggml_metal_pipeline_t pipeline = ggml_metal_library_get_pipeline_opt_step_sgd(lib, op);\n+\n+    const int64_t np = ggml_nelements(op->src[0]);\n+    ggml_metal_kargs_opt_step_sgd args = {\n+        /*.np =*/ np,\n+    };\n+\n+    int ida = 0;\n+\n+    ggml_metal_encoder_set_pipeline(enc, pipeline);\n+    ggml_metal_encoder_set_bytes   (enc, &args, sizeof(args), ida++);\n+    ggml_metal_encoder_set_buffer  (enc, ggml_metal_get_buffer_id(op->src[0]), ida++);\n+    ggml_metal_encoder_set_buffer  (enc, ggml_metal_get_buffer_id(op->src[1]), ida++);\n+    ggml_metal_encoder_set_buffer  (enc, ggml_metal_get_buffer_id(op->src[2]), ida++);\n+\n+    const int nth = std::min(ggml_metal_pipeline_max_theads_per_threadgroup(pipeline), ne0);\n+    const int64_t n = (np + nth - 1) / nth;\n+\n+    ggml_metal_encoder_dispatch_threadgroups(enc, n, 1, 1, nth, 1, 1);\n+\n+    return 1;\n+}"
      },
      {
        "filename": "ggml/src/ggml-metal/ggml-metal-ops.h",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -80,6 +80,7 @@ int ggml_metal_op_argmax            (ggml_metal_op_t ctx, int idx);\n int ggml_metal_op_argsort           (ggml_metal_op_t ctx, int idx);\n int ggml_metal_op_leaky_relu        (ggml_metal_op_t ctx, int idx);\n int ggml_metal_op_opt_step_adamw    (ggml_metal_op_t ctx, int idx);\n+int ggml_metal_op_opt_step_sgd      (ggml_metal_op_t ctx, int idx);\n \n #ifdef __cplusplus\n }"
      },
      {
        "filename": "ggml/src/ggml-metal/ggml-metal.metal",
        "status": "modified",
        "additions": 14,
        "deletions": 0,
        "changes": 14,
        "patch": "@@ -8806,3 +8806,17 @@ kernel void kernel_opt_step_adamw_f32(\n \n     x[gid] = x[gid] * (1.0f - alpha * wd) - alpha * mh / vh;\n }\n+\n+kernel void kernel_opt_step_sgd_f32(\n+        constant    ggml_metal_kargs_opt_step_sgd & args,\n+        device       float * x,\n+        device const float * g,\n+        device const float * pars,\n+        uint        gid[[thread_position_in_grid]]) {\n+\n+    if (gid >= args.np) {\n+        return;\n+    }\n+\n+    x[gid] = x[gid] * (1.0f - pars[0] * pars[1]) - pars[0] * g[gid];\n+}"
      }
    ],
    "num_files": 7,
    "scraped_at": "2025-11-16T23:30:05.675227"
  },
  {
    "pr_number": 16536,
    "title": "Vulkan MMQ Integer Dot Refactor and K-Quant support",
    "body": "This heavily refactors the caching structure of the MMQ shader and also makes it more modular, to work with other kinds of quants.\r\n\r\nBasically instead of turning the quants into 8-bit integers during load to shared memory, the quant structs now get copied through shared memory into registers and only reshaped into 8-bit integers directly before the integer dot operation. This saves on shared memory and on registers.\r\n\r\nTODO:\r\n- [x] Q2_K\r\n- [x] Q3_K\r\n- [x] Q4_K\r\n- [x] Q5_K\r\n- [x] Q6_K\r\n\r\nQ2_K performance is not that good yet. Mapping the 256-wide quant structure to 32-wide Q8_1 structures is not that easy to do efficiently, so I'm still trying to find the best way to do that. @jeffbolznv Let me know if you see any obvious issues with the implementation.",
    "html_url": "https://github.com/ggml-org/llama.cpp/pull/16536",
    "created_at": "2025-10-12T16:58:40Z",
    "merged_at": "2025-10-29T13:39:03Z",
    "merge_commit_sha": "bcf5bda6f5df559565d11d7c8e8295c1159a85ec",
    "base_ref": "master",
    "head_sha": "3b5a6f8e07c7a2739ca15ceaf9f4c750e445e8b8",
    "user": "0cc4m",
    "files": [
      {
        "filename": "ggml/src/ggml-vulkan/ggml-vulkan.cpp",
        "status": "modified",
        "additions": 140,
        "deletions": 25,
        "changes": 165,
        "patch": "@@ -486,6 +486,7 @@ struct vk_device_struct {\n     vk_matmul_pipeline2 pipeline_matmul_id_f16_f32;\n \n     vk_matmul_pipeline2 pipeline_dequant_mul_mat_mat_id[GGML_TYPE_COUNT];\n+    vk_matmul_pipeline2 pipeline_dequant_mul_mat_mat_id_q8_1[GGML_TYPE_COUNT];\n \n     vk_pipeline pipeline_matmul_split_k_reduce;\n     vk_pipeline pipeline_quantize_q8_1;\n@@ -2448,8 +2449,11 @@ static void ggml_vk_load_shaders(vk_device& device) {\n                           l_warptile_id, m_warptile_id, s_warptile_id,\n                           l_warptile_mmq, m_warptile_mmq, s_warptile_mmq,\n                           l_warptile_mmq_int, m_warptile_mmq_int, s_warptile_mmq_int,\n+                          l_warptile_mmq_int_k, m_warptile_mmq_int_k, s_warptile_mmq_int_k,\n                           l_warptile_mmq_k, m_warptile_mmq_k, s_warptile_mmq_k,\n-                          l_warptile_mmqid, m_warptile_mmqid, s_warptile_mmqid;\n+                          l_warptile_mmqid, m_warptile_mmqid, s_warptile_mmqid,\n+                          l_warptile_mmqid_int, m_warptile_mmqid_int, s_warptile_mmqid_int,\n+                          l_warptile_mmqid_int_k, m_warptile_mmqid_int_k, s_warptile_mmqid_int_k;\n     std::array<uint32_t, 3> l_wg_denoms, m_wg_denoms, s_wg_denoms,\n                             l_mmq_wg_denoms, m_mmq_wg_denoms, s_mmq_wg_denoms,\n                             l_mmq_wg_denoms_k, m_mmq_wg_denoms_k, s_mmq_wg_denoms_k,\n@@ -2512,10 +2516,16 @@ static void ggml_vk_load_shaders(vk_device& device) {\n         m_warptile_mmq = { 128,  64,  64, 32, subgroup_size_8, 32, 2, tm_m, tn_m, tk_m, subgroup_size_8 };\n         s_warptile_mmq = { subgroup_size_32, 32, 32, 32, 32, 32, 2, tm_s, tn_s, tk_s, subgroup_size_8 };\n \n+        // Integer MMQ has a smaller shared memory profile, but heavier register use\n         l_warptile_mmq_int = { 128, 128, 128, 32, subgroup_size_8 * 2, 64, 2, 4, 4, 1, subgroup_size_8 };\n         m_warptile_mmq_int = { 128,  64,  64, 32, subgroup_size_8,     32, 2, 2, 2, 1, subgroup_size_8 };\n         s_warptile_mmq_int = { subgroup_size_32, 32, 32, 32, 32,       32, 2, 2, 1, 1, subgroup_size_8 };\n \n+        // K-quants use even more registers, mitigate by setting WMITER to 1\n+        l_warptile_mmq_int_k = { 128, 128, 128, 32, subgroup_size_8 * 2, 64, 1, 4, 4, 1, subgroup_size_8 };\n+        m_warptile_mmq_int_k = { 128,  64,  64, 32, subgroup_size_8,     32, 1, 2, 2, 1, subgroup_size_8 };\n+        s_warptile_mmq_int_k = { subgroup_size_32, 32, 32, 32, 32,       32, 1, 2, 1, 1, subgroup_size_8 };\n+\n         l_warptile_id = { 128, 128, 128, 16, mul_mat_subgroup_size_16 * 2, 64, 2, tm_l, tn_l, tk_l, mul_mat_subgroup_size_16 };\n         m_warptile_id = { 128,  64,  64, 16, mul_mat_subgroup_size_16, 32, 2, tm_m, tn_m, tk_m, mul_mat_subgroup_size_16 };\n         s_warptile_id = { mul_mat_subgroup_size_16, 32, 32, 16, 32, 32, 2, tm_s, tn_s, tk_s, mul_mat_subgroup_size_16 };\n@@ -2524,10 +2534,18 @@ static void ggml_vk_load_shaders(vk_device& device) {\n         m_warptile_mmqid = { 128,  64,  64, 32, mul_mat_subgroup_size_8, 32, 2, tm_m, tn_m, tk_m, mul_mat_subgroup_size_8 };\n         s_warptile_mmqid = { mul_mat_subgroup_size_32, 32, 32, 32, 32, 32, 2, tm_s, tn_s, tk_s, mul_mat_subgroup_size_8 };\n \n+        l_warptile_mmqid_int = { 128, 128, 128, 32, mul_mat_subgroup_size_8 * 2, 64, 2, 4, 4, 1, mul_mat_subgroup_size_8 };\n+        m_warptile_mmqid_int = { 128,  64,  64, 32, mul_mat_subgroup_size_8,     32, 2, 2, 2, 1, mul_mat_subgroup_size_8 };\n+        s_warptile_mmqid_int = { mul_mat_subgroup_size_32, 32, 32, 32, 32,       32, 2, 2, 1, 1, mul_mat_subgroup_size_8 };\n+\n+        l_warptile_mmqid_int_k = { 128, 128, 128, 32, mul_mat_subgroup_size_16 * 2, 64, 1, 4, 4, 1, mul_mat_subgroup_size_16 };\n+        m_warptile_mmqid_int_k = { 128,  64,  64, 32, mul_mat_subgroup_size_16,     32, 1, 2, 2, 1, mul_mat_subgroup_size_16 };\n+        s_warptile_mmqid_int_k = { mul_mat_subgroup_size_32, 32, 32, 32, 32,       32, 1, 2, 1, 1, mul_mat_subgroup_size_16 };\n+\n         // chip specific tuning\n         if ((device->architecture == AMD_GCN) && (device->driver_id != vk::DriverId::eAmdProprietary)) {\n             m_warptile_mmq = m_warptile_mmq_int = { 256, 64, 64, 32, 16, 16, 2, 2, 2, 1, 16 };\n-            m_warptile_mmqid = { 256, 64, 64, 32, 16, 16, 2, 2, 2, 1, 16 };\n+            m_warptile_mmqid = m_warptile_mmqid_int = { 256, 64, 64, 32, 16, 16, 2, 2, 2, 1, 16 };\n         }\n \n         l_mmq_wg_denoms = l_wg_denoms = {128, 128, 1 };\n@@ -2912,18 +2930,15 @@ static void ggml_vk_load_shaders(vk_device& device) {\n         if (device->mul_mat ## ID ## _s[TYPE]) \\\n             ggml_vk_create_pipeline(device, device-> PIPELINE_NAME ->a_s, #NAMELC #F16ACC \"_aligned_s\", NAMELC ## _aligned ## F16ACC ## _len, NAMELC ## _aligned ## F16ACC ## _data, \"main\", PARAMCOUNT, sizeof(PUSHCONST), s_ ## WG_DENOMS, s_ ## WARPTILE, s_align, false, REQSUBGROUPSIZE > 0, REQSUBGROUPSIZE);   \\\n \n-#define CREATE_MMQ(TYPE, PIPELINE_NAME, NAMELC, WG_DENOMS, WARPTILE, PUSHCONST, PARAMCOUNT, ID) \\\n+#define CREATE_MMQ(TYPE, PIPELINE_NAME, NAMELC, WG_DENOMS, WARPTILE, PUSHCONST, PARAMCOUNT, ID, REQSUBGROUPSIZE) \\\n         if (device->mul_mat ## ID ## _l[TYPE]) { \\\n-            ggml_vk_create_pipeline(device, device-> PIPELINE_NAME .f16acc->l, #NAMELC \"_f16acc_l\", NAMELC ## _f16acc_len, NAMELC ##  _f16acc_data, \"main\", PARAMCOUNT, sizeof(PUSHCONST), l_ ## WG_DENOMS, l_ ## WARPTILE, 1);   \\\n-            ggml_vk_create_pipeline(device, device-> PIPELINE_NAME .f32acc->l, #NAMELC        \"_l\", NAMELC ## _len,        NAMELC ##  _data,        \"main\", PARAMCOUNT, sizeof(PUSHCONST), l_ ## WG_DENOMS, l_ ## WARPTILE, 1);   \\\n+            ggml_vk_create_pipeline(device, device-> PIPELINE_NAME .f32acc->l, #NAMELC        \"_l\", NAMELC ## _len,        NAMELC ##  _data,        \"main\", PARAMCOUNT, sizeof(PUSHCONST), l_ ## WG_DENOMS, l_ ## WARPTILE, 1, false, REQSUBGROUPSIZE > 0, REQSUBGROUPSIZE);   \\\n         } \\\n         if (device->mul_mat ## ID ## _m[TYPE]) { \\\n-            ggml_vk_create_pipeline(device, device-> PIPELINE_NAME .f16acc->m, #NAMELC \"_f16acc_m\", NAMELC ## _f16acc_len, NAMELC ##  _f16acc_data, \"main\", PARAMCOUNT, sizeof(PUSHCONST), m_ ## WG_DENOMS, m_ ## WARPTILE, 1);   \\\n-            ggml_vk_create_pipeline(device, device-> PIPELINE_NAME .f32acc->m, #NAMELC        \"_m\", NAMELC ## _len,        NAMELC ##  _data,        \"main\", PARAMCOUNT, sizeof(PUSHCONST), m_ ## WG_DENOMS, m_ ## WARPTILE, 1);   \\\n+            ggml_vk_create_pipeline(device, device-> PIPELINE_NAME .f32acc->m, #NAMELC        \"_m\", NAMELC ## _len,        NAMELC ##  _data,        \"main\", PARAMCOUNT, sizeof(PUSHCONST), m_ ## WG_DENOMS, m_ ## WARPTILE, 1, false, REQSUBGROUPSIZE > 0, REQSUBGROUPSIZE);   \\\n         } \\\n         if (device->mul_mat ## ID ## _s[TYPE]) { \\\n-            ggml_vk_create_pipeline(device, device-> PIPELINE_NAME .f16acc->s, #NAMELC \"_f16acc_s\", NAMELC ## _f16acc_len, NAMELC ##  _f16acc_data, \"main\", PARAMCOUNT, sizeof(PUSHCONST), s_ ## WG_DENOMS, s_ ## WARPTILE, 1);   \\\n-            ggml_vk_create_pipeline(device, device-> PIPELINE_NAME .f32acc->s, #NAMELC        \"_s\", NAMELC ## _len,        NAMELC ##  _data,        \"main\", PARAMCOUNT, sizeof(PUSHCONST), s_ ## WG_DENOMS, s_ ## WARPTILE, 1);   \\\n+            ggml_vk_create_pipeline(device, device-> PIPELINE_NAME .f32acc->s, #NAMELC        \"_s\", NAMELC ## _len,        NAMELC ##  _data,        \"main\", PARAMCOUNT, sizeof(PUSHCONST), s_ ## WG_DENOMS, s_ ## WARPTILE, 1, false, REQSUBGROUPSIZE > 0, REQSUBGROUPSIZE);   \\\n         } \\\n \n         // Create 2 variants, {f16,f32} accumulator\n@@ -2962,11 +2977,19 @@ static void ggml_vk_load_shaders(vk_device& device) {\n \n #if defined(GGML_VULKAN_INTEGER_DOT_GLSLC_SUPPORT)\n         if (device->integer_dot_product) {\n-            CREATE_MMQ(GGML_TYPE_Q4_0, pipeline_dequant_mul_mat_mat_q8_1[GGML_TYPE_Q4_0], matmul_q4_0_q8_1, mmq_wg_denoms, warptile_mmq_int, vk_mat_mat_push_constants, 3, );\n-            CREATE_MMQ(GGML_TYPE_Q4_1, pipeline_dequant_mul_mat_mat_q8_1[GGML_TYPE_Q4_1], matmul_q4_1_q8_1, mmq_wg_denoms, warptile_mmq_int, vk_mat_mat_push_constants, 3, );\n-            CREATE_MMQ(GGML_TYPE_Q5_0, pipeline_dequant_mul_mat_mat_q8_1[GGML_TYPE_Q5_0], matmul_q5_0_q8_1, mmq_wg_denoms, warptile_mmq_int, vk_mat_mat_push_constants, 3, );\n-            CREATE_MMQ(GGML_TYPE_Q5_1, pipeline_dequant_mul_mat_mat_q8_1[GGML_TYPE_Q5_1], matmul_q5_1_q8_1, mmq_wg_denoms, warptile_mmq_int, vk_mat_mat_push_constants, 3, );\n-            CREATE_MMQ(GGML_TYPE_Q8_0, pipeline_dequant_mul_mat_mat_q8_1[GGML_TYPE_Q8_0], matmul_q8_0_q8_1, mmq_wg_denoms, warptile_mmq_int, vk_mat_mat_push_constants, 3, );\n+            CREATE_MMQ(GGML_TYPE_Q4_0, pipeline_dequant_mul_mat_mat_q8_1[GGML_TYPE_Q4_0], matmul_q4_0_q8_1, mmq_wg_denoms, warptile_mmq_int, vk_mat_mat_push_constants, 3, , 0);\n+            CREATE_MMQ(GGML_TYPE_Q4_1, pipeline_dequant_mul_mat_mat_q8_1[GGML_TYPE_Q4_1], matmul_q4_1_q8_1, mmq_wg_denoms, warptile_mmq_int, vk_mat_mat_push_constants, 3, , 0);\n+            CREATE_MMQ(GGML_TYPE_Q5_0, pipeline_dequant_mul_mat_mat_q8_1[GGML_TYPE_Q5_0], matmul_q5_0_q8_1, mmq_wg_denoms, warptile_mmq_int, vk_mat_mat_push_constants, 3, , 0);\n+            CREATE_MMQ(GGML_TYPE_Q5_1, pipeline_dequant_mul_mat_mat_q8_1[GGML_TYPE_Q5_1], matmul_q5_1_q8_1, mmq_wg_denoms, warptile_mmq_int, vk_mat_mat_push_constants, 3, , 0);\n+            CREATE_MMQ(GGML_TYPE_Q8_0, pipeline_dequant_mul_mat_mat_q8_1[GGML_TYPE_Q8_0], matmul_q8_0_q8_1, mmq_wg_denoms, warptile_mmq_int, vk_mat_mat_push_constants, 3, , 0);\n+\n+            CREATE_MMQ(GGML_TYPE_MXFP4, pipeline_dequant_mul_mat_mat_q8_1[GGML_TYPE_MXFP4], matmul_mxfp4_q8_1, mmq_wg_denoms, warptile_mmq_int, vk_mat_mat_push_constants, 3, , 0);\n+\n+            CREATE_MMQ(GGML_TYPE_Q2_K, pipeline_dequant_mul_mat_mat_q8_1[GGML_TYPE_Q2_K], matmul_q2_k_q8_1, mmq_wg_denoms, warptile_mmq_int_k, vk_mat_mat_push_constants, 3, , 0);\n+            CREATE_MMQ(GGML_TYPE_Q3_K, pipeline_dequant_mul_mat_mat_q8_1[GGML_TYPE_Q3_K], matmul_q3_k_q8_1, mmq_wg_denoms, warptile_mmq_int_k, vk_mat_mat_push_constants, 3, , 0);\n+            CREATE_MMQ(GGML_TYPE_Q4_K, pipeline_dequant_mul_mat_mat_q8_1[GGML_TYPE_Q4_K], matmul_q4_k_q8_1, mmq_wg_denoms, warptile_mmq_int_k, vk_mat_mat_push_constants, 3, , 0);\n+            CREATE_MMQ(GGML_TYPE_Q5_K, pipeline_dequant_mul_mat_mat_q8_1[GGML_TYPE_Q5_K], matmul_q5_k_q8_1, mmq_wg_denoms, warptile_mmq_int_k, vk_mat_mat_push_constants, 3, , 0);\n+            CREATE_MMQ(GGML_TYPE_Q6_K, pipeline_dequant_mul_mat_mat_q8_1[GGML_TYPE_Q6_K], matmul_q6_k_q8_1, mmq_wg_denoms, warptile_mmq_int_k, vk_mat_mat_push_constants, 3, , 0);\n         }\n #endif\n \n@@ -2996,6 +3019,24 @@ static void ggml_vk_load_shaders(vk_device& device) {\n             CREATE_MM2(GGML_TYPE_IQ4_XS,  pipeline_dequant_mul_mat_mat_id[GGML_TYPE_IQ4_XS],  matmul_id_subgroup_iq4_xs_f32,  mmq_wg_denoms, warptile_mmqid, vk_mat_mat_id_push_constants, 4, _id, mul_mat_subgroup_size);\n             CREATE_MM2(GGML_TYPE_IQ4_NL,  pipeline_dequant_mul_mat_mat_id[GGML_TYPE_IQ4_NL],  matmul_id_subgroup_iq4_nl_f32,  mmq_wg_denoms, warptile_mmqid, vk_mat_mat_id_push_constants, 4, _id, mul_mat_subgroup_size);\n             CREATE_MM2(GGML_TYPE_MXFP4,   pipeline_dequant_mul_mat_mat_id[GGML_TYPE_MXFP4],   matmul_id_subgroup_mxfp4_f32,   mmq_wg_denoms, warptile_mmqid, vk_mat_mat_id_push_constants, 4, _id, mul_mat_subgroup_size);\n+\n+#if defined(GGML_VULKAN_INTEGER_DOT_GLSLC_SUPPORT)\n+            if (device->integer_dot_product) {\n+                CREATE_MMQ(GGML_TYPE_Q4_0, pipeline_dequant_mul_mat_mat_id_q8_1[GGML_TYPE_Q4_0], matmul_id_subgroup_q4_0_q8_1, mmq_wg_denoms, warptile_mmqid_int,   vk_mat_mat_id_push_constants, 4, _id, mul_mat_subgroup_size);\n+                CREATE_MMQ(GGML_TYPE_Q4_1, pipeline_dequant_mul_mat_mat_id_q8_1[GGML_TYPE_Q4_1], matmul_id_subgroup_q4_1_q8_1, mmq_wg_denoms, warptile_mmqid_int,   vk_mat_mat_id_push_constants, 4, _id, mul_mat_subgroup_size);\n+                CREATE_MMQ(GGML_TYPE_Q5_0, pipeline_dequant_mul_mat_mat_id_q8_1[GGML_TYPE_Q5_0], matmul_id_subgroup_q5_0_q8_1, mmq_wg_denoms, warptile_mmqid_int,   vk_mat_mat_id_push_constants, 4, _id, mul_mat_subgroup_size);\n+                CREATE_MMQ(GGML_TYPE_Q5_1, pipeline_dequant_mul_mat_mat_id_q8_1[GGML_TYPE_Q5_1], matmul_id_subgroup_q5_1_q8_1, mmq_wg_denoms, warptile_mmqid_int,   vk_mat_mat_id_push_constants, 4, _id, mul_mat_subgroup_size);\n+                CREATE_MMQ(GGML_TYPE_Q8_0, pipeline_dequant_mul_mat_mat_id_q8_1[GGML_TYPE_Q8_0], matmul_id_subgroup_q8_0_q8_1, mmq_wg_denoms, warptile_mmqid_int,   vk_mat_mat_id_push_constants, 4, _id, mul_mat_subgroup_size);\n+\n+                CREATE_MMQ(GGML_TYPE_MXFP4, pipeline_dequant_mul_mat_mat_id_q8_1[GGML_TYPE_MXFP4], matmul_id_subgroup_mxfp4_q8_1, mmq_wg_denoms, warptile_mmqid_int,   vk_mat_mat_id_push_constants, 4, _id, mul_mat_subgroup_size);\n+\n+                CREATE_MMQ(GGML_TYPE_Q2_K, pipeline_dequant_mul_mat_mat_id_q8_1[GGML_TYPE_Q2_K], matmul_id_subgroup_q2_k_q8_1, mmq_wg_denoms, warptile_mmqid_int_k, vk_mat_mat_id_push_constants, 4, _id, mul_mat_subgroup_size_16);\n+                CREATE_MMQ(GGML_TYPE_Q3_K, pipeline_dequant_mul_mat_mat_id_q8_1[GGML_TYPE_Q3_K], matmul_id_subgroup_q3_k_q8_1, mmq_wg_denoms, warptile_mmqid_int_k, vk_mat_mat_id_push_constants, 4, _id, mul_mat_subgroup_size_16);\n+                CREATE_MMQ(GGML_TYPE_Q4_K, pipeline_dequant_mul_mat_mat_id_q8_1[GGML_TYPE_Q4_K], matmul_id_subgroup_q4_k_q8_1, mmq_wg_denoms, warptile_mmqid_int_k, vk_mat_mat_id_push_constants, 4, _id, mul_mat_subgroup_size_16);\n+                CREATE_MMQ(GGML_TYPE_Q5_K, pipeline_dequant_mul_mat_mat_id_q8_1[GGML_TYPE_Q5_K], matmul_id_subgroup_q5_k_q8_1, mmq_wg_denoms, warptile_mmqid_int_k, vk_mat_mat_id_push_constants, 4, _id, mul_mat_subgroup_size_16);\n+                CREATE_MMQ(GGML_TYPE_Q6_K, pipeline_dequant_mul_mat_mat_id_q8_1[GGML_TYPE_Q6_K], matmul_id_subgroup_q6_k_q8_1, mmq_wg_denoms, warptile_mmqid_int_k, vk_mat_mat_id_push_constants, 4, _id, mul_mat_subgroup_size_16);\n+            }\n+#endif\n         } else {\n             CREATE_MM(GGML_TYPE_F32, pipeline_matmul_id_f32, matmul_id_f32_f32, , wg_denoms, warptile, vk_mat_mat_push_constants, 4, _id, 0);\n             CREATE_MM2(GGML_TYPE_F16, pipeline_matmul_id_f16, matmul_id_f16, wg_denoms, warptile, vk_mat_mat_push_constants, 4, _id, 0);\n@@ -3022,6 +3063,24 @@ static void ggml_vk_load_shaders(vk_device& device) {\n             CREATE_MM2(GGML_TYPE_IQ4_XS,  pipeline_dequant_mul_mat_mat_id[GGML_TYPE_IQ4_XS],  matmul_id_iq4_xs_f32,  mmq_wg_denoms, warptile_mmqid, vk_mat_mat_id_push_constants, 4, _id, 0);\n             CREATE_MM2(GGML_TYPE_IQ4_NL,  pipeline_dequant_mul_mat_mat_id[GGML_TYPE_IQ4_NL],  matmul_id_iq4_nl_f32,  mmq_wg_denoms, warptile_mmqid, vk_mat_mat_id_push_constants, 4, _id, 0);\n             CREATE_MM2(GGML_TYPE_MXFP4,   pipeline_dequant_mul_mat_mat_id[GGML_TYPE_MXFP4],   matmul_id_mxfp4_f32,   mmq_wg_denoms, warptile_mmqid, vk_mat_mat_id_push_constants, 4, _id, 0);\n+\n+#if defined(GGML_VULKAN_INTEGER_DOT_GLSLC_SUPPORT)\n+            if (device->integer_dot_product) {\n+                CREATE_MMQ(GGML_TYPE_Q4_0, pipeline_dequant_mul_mat_mat_id_q8_1[GGML_TYPE_Q4_0], matmul_id_q4_0_q8_1, mmq_wg_denoms, warptile_mmqid_int,   vk_mat_mat_id_push_constants, 4, _id, 0);\n+                CREATE_MMQ(GGML_TYPE_Q4_1, pipeline_dequant_mul_mat_mat_id_q8_1[GGML_TYPE_Q4_1], matmul_id_q4_1_q8_1, mmq_wg_denoms, warptile_mmqid_int,   vk_mat_mat_id_push_constants, 4, _id, 0);\n+                CREATE_MMQ(GGML_TYPE_Q5_0, pipeline_dequant_mul_mat_mat_id_q8_1[GGML_TYPE_Q5_0], matmul_id_q5_0_q8_1, mmq_wg_denoms, warptile_mmqid_int,   vk_mat_mat_id_push_constants, 4, _id, 0);\n+                CREATE_MMQ(GGML_TYPE_Q5_1, pipeline_dequant_mul_mat_mat_id_q8_1[GGML_TYPE_Q5_1], matmul_id_q5_1_q8_1, mmq_wg_denoms, warptile_mmqid_int,   vk_mat_mat_id_push_constants, 4, _id, 0);\n+                CREATE_MMQ(GGML_TYPE_Q8_0, pipeline_dequant_mul_mat_mat_id_q8_1[GGML_TYPE_Q8_0], matmul_id_q8_0_q8_1, mmq_wg_denoms, warptile_mmqid_int,   vk_mat_mat_id_push_constants, 4, _id, 0);\n+\n+                CREATE_MMQ(GGML_TYPE_MXFP4, pipeline_dequant_mul_mat_mat_id_q8_1[GGML_TYPE_MXFP4], matmul_id_mxfp4_q8_1, mmq_wg_denoms, warptile_mmqid_int,   vk_mat_mat_id_push_constants, 4, _id, 0);\n+\n+                CREATE_MMQ(GGML_TYPE_Q2_K, pipeline_dequant_mul_mat_mat_id_q8_1[GGML_TYPE_Q2_K], matmul_id_q2_k_q8_1, mmq_wg_denoms, warptile_mmqid_int_k, vk_mat_mat_id_push_constants, 4, _id, 0);\n+                CREATE_MMQ(GGML_TYPE_Q3_K, pipeline_dequant_mul_mat_mat_id_q8_1[GGML_TYPE_Q3_K], matmul_id_q3_k_q8_1, mmq_wg_denoms, warptile_mmqid_int_k, vk_mat_mat_id_push_constants, 4, _id, 0);\n+                CREATE_MMQ(GGML_TYPE_Q4_K, pipeline_dequant_mul_mat_mat_id_q8_1[GGML_TYPE_Q4_K], matmul_id_q4_k_q8_1, mmq_wg_denoms, warptile_mmqid_int_k, vk_mat_mat_id_push_constants, 4, _id, 0);\n+                CREATE_MMQ(GGML_TYPE_Q5_K, pipeline_dequant_mul_mat_mat_id_q8_1[GGML_TYPE_Q5_K], matmul_id_q5_k_q8_1, mmq_wg_denoms, warptile_mmqid_int_k, vk_mat_mat_id_push_constants, 4, _id, 0);\n+                CREATE_MMQ(GGML_TYPE_Q6_K, pipeline_dequant_mul_mat_mat_id_q8_1[GGML_TYPE_Q6_K], matmul_id_q6_k_q8_1, mmq_wg_denoms, warptile_mmqid_int_k, vk_mat_mat_id_push_constants, 4, _id, 0);\n+            }\n+#endif\n         }\n #undef CREATE_MM2\n #undef CREATE_MMQ\n@@ -3086,6 +3145,12 @@ static void ggml_vk_load_shaders(vk_device& device) {\n             CREATE_MMQ(GGML_TYPE_Q5_0, pipeline_dequant_mul_mat_mat_q8_1[GGML_TYPE_Q5_0].f32acc, matmul_q5_0_q8_1, mmq_wg_denoms, warptile_mmq_int, vk_mat_mat_push_constants, 3, );\n             CREATE_MMQ(GGML_TYPE_Q5_1, pipeline_dequant_mul_mat_mat_q8_1[GGML_TYPE_Q5_1].f32acc, matmul_q5_1_q8_1, mmq_wg_denoms, warptile_mmq_int, vk_mat_mat_push_constants, 3, );\n             CREATE_MMQ(GGML_TYPE_Q8_0, pipeline_dequant_mul_mat_mat_q8_1[GGML_TYPE_Q8_0].f32acc, matmul_q8_0_q8_1, mmq_wg_denoms, warptile_mmq_int, vk_mat_mat_push_constants, 3, );\n+\n+            CREATE_MMQ(GGML_TYPE_Q2_K, pipeline_dequant_mul_mat_mat_q8_1[GGML_TYPE_Q2_K].f32acc, matmul_q2_k_q8_1, mmq_wg_denoms, warptile_mmq_int_k, vk_mat_mat_push_constants, 3, );\n+            CREATE_MMQ(GGML_TYPE_Q3_K, pipeline_dequant_mul_mat_mat_q8_1[GGML_TYPE_Q3_K].f32acc, matmul_q3_k_q8_1, mmq_wg_denoms, warptile_mmq_int_k, vk_mat_mat_push_constants, 3, );\n+            CREATE_MMQ(GGML_TYPE_Q4_K, pipeline_dequant_mul_mat_mat_q8_1[GGML_TYPE_Q4_K].f32acc, matmul_q4_k_q8_1, mmq_wg_denoms, warptile_mmq_int_k, vk_mat_mat_push_constants, 3, );\n+            CREATE_MMQ(GGML_TYPE_Q5_K, pipeline_dequant_mul_mat_mat_q8_1[GGML_TYPE_Q5_K].f32acc, matmul_q5_k_q8_1, mmq_wg_denoms, warptile_mmq_int_k, vk_mat_mat_push_constants, 3, );\n+            CREATE_MMQ(GGML_TYPE_Q6_K, pipeline_dequant_mul_mat_mat_q8_1[GGML_TYPE_Q6_K].f32acc, matmul_q6_k_q8_1, mmq_wg_denoms, warptile_mmq_int_k, vk_mat_mat_push_constants, 3, );\n         }\n #endif\n \n@@ -3145,7 +3210,7 @@ static void ggml_vk_load_shaders(vk_device& device) {\n     }\n     // reusing CREATE_MM from the fp32 path\n     if ((device->coopmat2 || device->coopmat_support)\n-#if defined(GGML_VULKAN_INTEGER_DOT_GLSLC_SUPPORT)\n+#if defined(GGML_VULKAN_BFLOAT16_GLSLC_SUPPORT)\n         && !device->coopmat_bf16_support\n #endif\n         ) {\n@@ -4928,7 +4993,7 @@ static vk_matmul_pipeline ggml_vk_get_mul_mat_mat_pipeline(ggml_backend_vk_conte\n \n     // MMQ\n     if (src1_type == GGML_TYPE_Q8_1) {\n-        vk_matmul_pipeline pipelines = (ctx->device->fp16 && prec == GGML_PREC_DEFAULT) ? ctx->device->pipeline_dequant_mul_mat_mat_q8_1[src0_type].f16acc : ctx->device->pipeline_dequant_mul_mat_mat_q8_1[src0_type].f32acc;\n+        vk_matmul_pipeline pipelines = ctx->device->pipeline_dequant_mul_mat_mat_q8_1[src0_type].f32acc;\n \n         if (pipelines->s == nullptr && pipelines->m == nullptr && pipelines->l == nullptr) {\n             return nullptr;\n@@ -5075,6 +5140,17 @@ static vk_matmul_pipeline ggml_vk_get_mul_mat_mat_id_pipeline(ggml_backend_vk_co\n         }\n     }\n \n+    // MMQ\n+    if (src1_type == GGML_TYPE_Q8_1) {\n+        vk_matmul_pipeline pipelines = ctx->device->pipeline_dequant_mul_mat_mat_id_q8_1[src0_type].f32acc;\n+\n+        if (pipelines->s == nullptr && pipelines->m == nullptr && pipelines->l == nullptr) {\n+            return nullptr;\n+        }\n+\n+        return pipelines;\n+    }\n+\n     GGML_ASSERT(src1_type == GGML_TYPE_F32 || (ctx->device->coopmat2 && src1_type == GGML_TYPE_F16));\n \n     switch (src0_type) {\n@@ -6880,10 +6956,19 @@ static void ggml_vk_mul_mat_id_q_f16(ggml_backend_vk_context * ctx, vk_context&\n \n     const bool y_f32_kernel = src1->type == GGML_TYPE_F32 && !y_non_contig;\n \n-    vk_matmul_pipeline mmp = ggml_vk_get_mul_mat_mat_id_pipeline(ctx, src0->type, y_non_contig ? f16_type : src1->type, (ggml_prec)dst->op_params[0]);\n+    bool quantize_y = ctx->device->integer_dot_product && src1->type == GGML_TYPE_F32 && ggml_is_contiguous(src1) && (ne11 * ne10) % 4 == 0;\n+\n+    // Check for mmq first\n+    vk_matmul_pipeline mmp = quantize_y ? ggml_vk_get_mul_mat_mat_id_pipeline(ctx, src0->type, GGML_TYPE_Q8_1, (ggml_prec)dst->op_params[0]) : nullptr;\n+\n+    if (mmp == nullptr) {\n+        // Fall back to f16 dequant mul mat\n+        mmp = ggml_vk_get_mul_mat_mat_id_pipeline(ctx, src0->type, y_non_contig ? f16_type : src1->type, (ggml_prec)dst->op_params[0]);\n+        quantize_y = false;\n+    }\n \n     const bool qx_needs_dequant = mmp == nullptr || x_non_contig;\n-    const bool qy_needs_dequant = (src1->type != f16_type && !y_f32_kernel) || y_non_contig;\n+    const bool qy_needs_dequant = !quantize_y && ((src1->type != f16_type && !y_f32_kernel) || y_non_contig);\n \n     if (qx_needs_dequant) {\n         // Fall back to dequant + f16 mulmat\n@@ -6893,8 +6978,8 @@ static void ggml_vk_mul_mat_id_q_f16(ggml_backend_vk_context * ctx, vk_context&\n     // Not implemented\n     GGML_ASSERT(y_non_contig || !qy_needs_dequant);  // NOLINT\n \n-    const uint32_t kpad = ggml_vk_align_size(ne10, ggml_vk_guess_matmul_id_pipeline_align(ctx, mmp, ne01, nei1, qx_needs_dequant ? f16_type : src0->type));\n-    const bool aligned = ne10 == kpad && ne01 > 8 && nei1 > 8;\n+    const uint32_t kpad = quantize_y ? 0 : ggml_vk_align_size(ne10, ggml_vk_guess_matmul_id_pipeline_align(ctx, mmp, ne01, nei1, qx_needs_dequant ? f16_type : src0->type));\n+    const bool aligned = !quantize_y && ne10 == kpad && ne01 > 8 && nei1 > 8;\n \n     vk_pipeline pipeline = ggml_vk_guess_matmul_id_pipeline(ctx, mmp, ne01, nei1, aligned, qx_needs_dequant ? f16_type : src0->type);\n \n@@ -6907,12 +6992,13 @@ static void ggml_vk_mul_mat_id_q_f16(ggml_backend_vk_context * ctx, vk_context&\n     const uint64_t qx_sz = ggml_type_size(src0->type) * x_ne / ggml_blck_size(src0->type);\n     const uint64_t qy_sz = ggml_type_size(src1->type) * y_ne / ggml_blck_size(src1->type);\n     const uint64_t x_sz = !qx_needs_dequant ? qx_sz : sizeof(ggml_fp16_t) * x_ne;\n-    const uint64_t y_sz = y_f32_kernel ? sizeof(float) * y_ne : sizeof(ggml_fp16_t) * y_ne;\n+    const uint64_t y_sz = quantize_y ? (y_ne * ggml_type_size(GGML_TYPE_Q8_1) / ggml_blck_size(GGML_TYPE_Q8_1)) : (y_f32_kernel ? sizeof(float) * y_ne : sizeof(ggml_fp16_t) * y_ne);\n     const uint64_t ids_sz = nbi2;\n     const uint64_t d_sz = sizeof(float) * d_ne;\n \n     vk_pipeline to_fp16_vk_0 = nullptr;\n     vk_pipeline to_fp16_vk_1 = nullptr;\n+    vk_pipeline to_q8_1 = nullptr;\n \n     if (x_non_contig) {\n         to_fp16_vk_0 = ggml_vk_get_cpy_pipeline(ctx, src0, nullptr, f16_type);\n@@ -6927,9 +7013,16 @@ static void ggml_vk_mul_mat_id_q_f16(ggml_backend_vk_context * ctx, vk_context&\n     GGML_ASSERT(!qx_needs_dequant || to_fp16_vk_0 != nullptr);  // NOLINT\n     GGML_ASSERT(!qy_needs_dequant || to_fp16_vk_1 != nullptr);  // NOLINT\n \n+    if (quantize_y) {\n+        to_q8_1 = ggml_vk_get_quantize_pipeline(ctx, GGML_TYPE_Q8_1, true);\n+    }\n+\n     if (dryrun) {\n         const uint64_t x_sz_upd = x_sz * ne02 * ne03;\n-        const uint64_t y_sz_upd = y_sz * ne12 * ne13;\n+        uint64_t y_sz_upd = y_sz * ne12 * ne13;\n+        if (quantize_y) {\n+            y_sz_upd = CEIL_DIV(y_sz_upd, 144) * 144;\n+        }\n         if (\n                 (qx_needs_dequant && x_sz_upd > ctx->device->properties.limits.maxStorageBufferRange) ||\n                 (qy_needs_dequant && y_sz_upd > ctx->device->properties.limits.maxStorageBufferRange)) {\n@@ -6938,7 +7031,7 @@ static void ggml_vk_mul_mat_id_q_f16(ggml_backend_vk_context * ctx, vk_context&\n         if (qx_needs_dequant && ctx->prealloc_size_x < x_sz_upd) {\n             ctx->prealloc_size_x = x_sz_upd;\n         }\n-        if (qy_needs_dequant && ctx->prealloc_size_y < y_sz_upd) {\n+        if ((qy_needs_dequant || quantize_y) && ctx->prealloc_size_y < y_sz_upd) {\n             ctx->prealloc_size_y = y_sz_upd;\n         }\n \n@@ -6950,6 +7043,9 @@ static void ggml_vk_mul_mat_id_q_f16(ggml_backend_vk_context * ctx, vk_context&\n         if (qy_needs_dequant) {\n             ggml_pipeline_request_descriptor_sets(ctx, to_fp16_vk_1, 1);\n         }\n+        if (quantize_y) {\n+            ggml_pipeline_request_descriptor_sets(ctx, to_q8_1, 1);\n+        }\n         return;\n     }\n \n@@ -6986,6 +7082,9 @@ static void ggml_vk_mul_mat_id_q_f16(ggml_backend_vk_context * ctx, vk_context&\n     if (qy_needs_dequant) {\n         d_Y = ctx->prealloc_y;\n         GGML_ASSERT(d_Y->size >= y_sz * ne12 * ne13);\n+    } else if (quantize_y) {\n+        d_Y = ctx->prealloc_y;\n+        GGML_ASSERT(d_Y->size >= CEIL_DIV(y_sz * ne12 * ne13, 144) * 144);\n     } else {\n         d_Y = d_Qy;\n         y_buf_offset = qy_buf_offset;\n@@ -7017,6 +7116,17 @@ static void ggml_vk_mul_mat_id_q_f16(ggml_backend_vk_context * ctx, vk_context&\n             ctx->prealloc_y_last_tensor_used = src1;\n         }\n     }\n+    if (quantize_y) {\n+        if (ctx->prealloc_y_last_pipeline_used != to_q8_1.get() ||\n+            ctx->prealloc_y_last_tensor_used != src1) {\n+            if (ctx->prealloc_y_need_sync) {\n+                ggml_vk_sync_buffers(ctx, subctx);\n+            }\n+            ggml_vk_quantize_q8_1(ctx, subctx, ggml_vk_subbuffer(ctx, d_Qy, qy_buf_offset), ggml_vk_subbuffer(ctx, d_Y, 0), y_ne * ne12 * ne13, true);\n+            ctx->prealloc_y_last_pipeline_used = to_q8_1.get();\n+            ctx->prealloc_y_last_tensor_used = src1;\n+        }\n+    }\n \n     uint32_t stride_batch_x = ne00*ne01;\n     uint32_t stride_batch_y = ne10*ne11;\n@@ -7025,14 +7135,19 @@ static void ggml_vk_mul_mat_id_q_f16(ggml_backend_vk_context * ctx, vk_context&\n         stride_batch_x = src0->nb[0] / ggml_type_size(src0->type);\n     }\n \n-    if (!ggml_vk_dim01_contiguous(src1) && !qy_needs_dequant) {\n+    if (!ggml_vk_dim01_contiguous(src1) && !qy_needs_dequant && !quantize_y) {\n         stride_batch_y = src1->nb[0] / ggml_type_size(src1->type);\n     }\n \n+    uint32_t y_sz_total = y_sz * ne12 * ne13;\n+    if (quantize_y) {\n+        y_sz_total = CEIL_DIV(y_sz_total, 144) * 144;\n+    }\n+\n     // compute\n     ggml_vk_matmul_id(\n         ctx, subctx, pipeline,\n-        { d_X, x_buf_offset, x_sz * ne02 * ne03 }, { d_Y, y_buf_offset, y_sz * ne12 * ne13 },\n+        { d_X, x_buf_offset, x_sz * ne02 * ne03 }, { d_Y, y_buf_offset, y_sz_total },\n         { d_D, d_buf_offset, d_sz * ne22 * ne23 }, { d_ids, ids_buf_offset, ids_sz },\n         ne01, ne21, ne10, ne10, ne10, ne01,\n         stride_batch_x, stride_batch_y, ne20*ne21,"
      },
      {
        "filename": "ggml/src/ggml-vulkan/vulkan-shaders/dequant_funcs.glsl",
        "status": "modified",
        "additions": 5,
        "deletions": 5,
        "changes": 10,
        "patch": "@@ -437,7 +437,7 @@ vec4 dequantize4(uint ib, uint iqs, uint a_offset) {\n #if defined(DATA_A_MXFP4)\n vec2 dequantize(uint ib, uint iqs, uint a_offset) {\n     const uint vui = uint(data_a[a_offset + ib].qs[iqs]);\n-    return vec2(kvalues_mxfp4[vui & 0xF], kvalues_mxfp4[vui >> 4]);\n+    return vec2(kvalues_mxfp4[vui & 0xF], kvalues_mxfp4[vui >> 4]) * 0.5;\n }\n vec4 dequantize4(uint ib, uint iqs, uint a_offset) {\n     vec2 v0 = dequantize(ib, iqs, a_offset);\n@@ -488,9 +488,9 @@ vec2 dequantize(uint ib, uint iqs, uint a_offset) {\n \n     const uvec2 qs = uvec2(data_a[a_offset + ib].qs[qsi], data_a[a_offset + ib].qs[qsi + 1]);\n     const uint scales = data_a[a_offset + ib].scales[scalesi];\n-    const vec2 d = vec2(data_a[a_offset + ib].d);\n+    const vec2 dm = vec2(data_a[a_offset + ib].dm);\n \n-    return d.x * float(scales & 0xF) * vec2((qs >> qsshift) & 3) - d.y * float(scales >> 4);\n+    return dm.x * float(scales & 0xF) * vec2((qs >> qsshift) & 3) - dm.y * float(scales >> 4);\n }\n vec2 get_dm(uint ib, uint a_offset) {\n     return vec2(1, 0);\n@@ -529,7 +529,7 @@ vec2 dequantize(uint ib, uint iqs, uint a_offset) {\n     const uint is = 2 * n + b;                 // 0..7\n     const uint qsi = n * 32 + (iqs % 16) * 2;  // 0,2,4..126\n \n-    const vec2 loadd = vec2(data_a[a_offset + ib].d);\n+    const vec2 loadd = vec2(data_a[a_offset + ib].dm);\n \n     const uint scidx0 = (is < 4) ? is : (is + 4);\n     const uint scidx1 = (is < 4) ? is : (is - 4);\n@@ -567,7 +567,7 @@ vec2 dequantize(uint ib, uint iqs, uint a_offset) {\n \n     const uint8_t hm = uint8_t(1 << (iqs / 16));\n \n-    const vec2 loadd = vec2(data_a[a_offset + ib].d);\n+    const vec2 loadd = vec2(data_a[a_offset + ib].dm);\n \n     const uint scidx0 = (is < 4) ? is : (is + 4);\n     const uint scidx1 = (is < 4) ? is : (is - 4);"
      },
      {
        "filename": "ggml/src/ggml-vulkan/vulkan-shaders/dequant_funcs_cm2.glsl",
        "status": "modified",
        "additions": 3,
        "deletions": 3,
        "changes": 6,
        "patch": "@@ -120,7 +120,7 @@ layout(buffer_reference, std430, buffer_reference_align = 16) buffer decodeBufQ2\n float16_t dequantFuncQ2_K(const in decodeBufQ2_K bl, const in uint blockCoords[2], const in uint coordInBlock[2])\n {\n     decodeBufQ2_K_packed16 bl16 = decodeBufQ2_K_packed16(bl);\n-    const f16vec2 d = bl.block.d;\n+    const f16vec2 dm = bl.block.dm;\n     const uint idx = coordInBlock[1];\n \n     const uint scalesi = (idx & 0xF0) >> 4;             // 0..15\n@@ -131,7 +131,7 @@ float16_t dequantFuncQ2_K(const in decodeBufQ2_K bl, const in uint blockCoords[2\n     qs = unpack8(qs)[idx & 1];\n \n     const uint scales = bl.block.scales[scalesi];\n-    float16_t ret = d.x * float16_t(scales & 0xF) * float16_t(qs) - d.y * float16_t(scales >> 4);\n+    float16_t ret = dm.x * float16_t(scales & 0xF) * float16_t(qs) - dm.y * float16_t(scales >> 4);\n     return ret;\n }\n \n@@ -680,7 +680,7 @@ float16_t dequantFuncMXFP4(const in decodeBufMXFP4 bl, const in uint blockCoords\n     uint32_t qs = bl.block.qs[iqs];\n     qs >>= shift;\n     qs &= 0xF;\n-    float16_t ret = float16_t(kvalues_mxfp4[qs] * d);\n+    float16_t ret = float16_t(kvalues_mxfp4[qs] * d * 0.5);\n     return ret;\n }\n #endif"
      },
      {
        "filename": "ggml/src/ggml-vulkan/vulkan-shaders/dequant_mxfp4.comp",
        "status": "modified",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "patch": "@@ -26,7 +26,7 @@ void main() {\n     const float d = e8m0_to_fp32(data_a[ib].e);\n \n     [[unroll]] for (uint l = 0; l < 8; ++l) {\n-        data_b[b_idx + l +  0] = D_TYPE(d * kvalues_mxfp4[data_a[ib].qs[q_idx + l] & 0xF]);\n-        data_b[b_idx + l + 16] = D_TYPE(d * kvalues_mxfp4[data_a[ib].qs[q_idx + l] >>  4]);\n+        data_b[b_idx + l +  0] = D_TYPE(d * 0.5 * float(kvalues_mxfp4[data_a[ib].qs[q_idx + l] & 0xF]));\n+        data_b[b_idx + l + 16] = D_TYPE(d * 0.5 * float(kvalues_mxfp4[data_a[ib].qs[q_idx + l] >>  4]));\n     }\n }"
      },
      {
        "filename": "ggml/src/ggml-vulkan/vulkan-shaders/dequant_q2_k.comp",
        "status": "modified",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "patch": "@@ -24,8 +24,8 @@ void main() {\n         const uint ql_idx = 32 * ip + il;\n         const uint8_t qs = data_a[i].qs[32 * ip + il];\n \n-        FLOAT_TYPE dall = FLOAT_TYPE(data_a[i].d.x);\n-        FLOAT_TYPE dmin = FLOAT_TYPE(data_a[i].d.y);\n+        FLOAT_TYPE dall = FLOAT_TYPE(data_a[i].dm.x);\n+        FLOAT_TYPE dmin = FLOAT_TYPE(data_a[i].dm.y);\n         data_b[y_idx +  0] = D_TYPE(dall * FLOAT_TYPE((data_a[i].scales[is+0] & 0xF) * ((qs >> 0) & 3)) - dmin * FLOAT_TYPE(data_a[i].scales[is+0] >> 4));\n         data_b[y_idx + 32] = D_TYPE(dall * FLOAT_TYPE((data_a[i].scales[is+2] & 0xF) * ((qs >> 2) & 3)) - dmin * FLOAT_TYPE(data_a[i].scales[is+2] >> 4));\n         data_b[y_idx + 64] = D_TYPE(dall * FLOAT_TYPE((data_a[i].scales[is+4] & 0xF) * ((qs >> 4) & 3)) - dmin * FLOAT_TYPE(data_a[i].scales[is+4] >> 4));"
      },
      {
        "filename": "ggml/src/ggml-vulkan/vulkan-shaders/dequant_q4_k.comp",
        "status": "modified",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "patch": "@@ -20,8 +20,8 @@ void main() {\n         const uint is = 2 * il;\n         const uint n = 4;\n \n-        const FLOAT_TYPE dall = FLOAT_TYPE(data_a[ib].d.x);\n-        const FLOAT_TYPE dmin = FLOAT_TYPE(data_a[ib].d.y);\n+        const FLOAT_TYPE dall = FLOAT_TYPE(data_a[ib].dm.x);\n+        const FLOAT_TYPE dmin = FLOAT_TYPE(data_a[ib].dm.y);\n \n         const uint y_idx = ib * QUANT_K + 64 * il + n * ir;\n         const uint qs_idx = 32*il + n * ir;"
      },
      {
        "filename": "ggml/src/ggml-vulkan/vulkan-shaders/dequant_q5_k.comp",
        "status": "modified",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "patch": "@@ -19,8 +19,8 @@ void main() {\n         const uint ir = tid % 16;\n         const uint is = 2 * il;\n \n-        const FLOAT_TYPE dall = FLOAT_TYPE(data_a[ib].d.x);\n-        const FLOAT_TYPE dmin = FLOAT_TYPE(data_a[ib].d.y);\n+        const FLOAT_TYPE dall = FLOAT_TYPE(data_a[ib].dm.x);\n+        const FLOAT_TYPE dmin = FLOAT_TYPE(data_a[ib].dm.y);\n \n         const uint y_idx = ib * QUANT_K + 64 * il + 2 * ir;\n         const uint qs_idx = 32*il + 2 * ir;"
      },
      {
        "filename": "ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_vec_q2_k.comp",
        "status": "modified",
        "additions": 2,
        "deletions": 4,
        "changes": 6,
        "patch": "@@ -41,9 +41,7 @@ void calc_superblock(const uint a_offset, const uint b_offset, const uint itid,\n         const vec4 qs_u32_4 = vec4(unpack8((qs_u32 >> 4) & 0x03030303));\n         const vec4 qs_u32_6 = vec4(unpack8((qs_u32 >> 6) & 0x03030303));\n \n-        vec2 d = vec2(data_a[ib0 + i].d);\n-        const FLOAT_TYPE dall = FLOAT_TYPE(d.x);\n-        const FLOAT_TYPE dmin = FLOAT_TYPE(d.y);\n+        const FLOAT_TYPE_VEC2 dm = vec2(data_a[ib0 + i].dm);\n \n         [[unroll]] for (uint j = 0; j < NUM_COLS; ++j) {\n             vec2 b0 =   vec2(data_b_v2[(j*p.batch_stride_b + b_offset + y_idx) / 2 +  0]);\n@@ -75,7 +73,7 @@ void calc_superblock(const uint a_offset, const uint b_offset, const uint itid,\n                        fma(FLOAT_TYPE(b96[l]),  sccache2[csel][ix][6 + 8*v_im],\n                        fma(FLOAT_TYPE(b112[l]), sccache2[csel][ix][7 + 8*v_im], sum2))))))));\n             }\n-            temp[j][n] = fma(dall, sum1, fma(-dmin, sum2, temp[j][n]));\n+            temp[j][n] = fma(dm.x, sum1, fma(-dm.y, sum2, temp[j][n]));\n         }\n     }\n }"
      },
      {
        "filename": "ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_vec_q4_k.comp",
        "status": "modified",
        "additions": 2,
        "deletions": 4,
        "changes": 6,
        "patch": "@@ -14,9 +14,7 @@ void calc_superblock(const uint a_offset, const uint b_offset, const uint v_im,\n \n     [[unroll]] for (uint n = 0; n < num_rows; ++n) {\n         const uint ib0 = a_offset / QUANT_K + (first_row+n)*num_blocks_per_row;\n-        vec2 d = vec2(data_a[ib0 + i].d);\n-        const FLOAT_TYPE dall = FLOAT_TYPE(d.x);\n-        const FLOAT_TYPE dmin = FLOAT_TYPE(d.y);\n+        const FLOAT_TYPE_VEC2 dm = FLOAT_TYPE_VEC2(data_a[ib0 + i].dm);\n \n         const uint32_t scale0_u32 = data_a_packed16[ib0 + i].scales[v_im    ];\n         const uint32_t scale4_u32 = data_a_packed16[ib0 + i].scales[v_im + 2];\n@@ -81,7 +79,7 @@ void calc_superblock(const uint a_offset, const uint b_offset, const uint v_im,\n                 fma(FLOAT_TYPE(by10.y), sc2, fma(FLOAT_TYPE(by132.y), sc3, fma(FLOAT_TYPE(by20.y), sc6, fma(FLOAT_TYPE(by232.y), sc7,\n                 fma(FLOAT_TYPE(by10.z), sc2, fma(FLOAT_TYPE(by132.z), sc3, fma(FLOAT_TYPE(by20.z), sc6, fma(FLOAT_TYPE(by232.z), sc7,\n                 fma(FLOAT_TYPE(by10.w), sc2, fma(FLOAT_TYPE(by132.w), sc3, fma(FLOAT_TYPE(by20.w), sc6,     FLOAT_TYPE(by232.w) * sc7)))))))))))))));\n-            temp[j][n] = fma(dall, fma(sx, sc0, fma(sy, sc1, fma(sz, sc4, sw * sc5))), fma(-dmin, smin, temp[j][n]));\n+            temp[j][n] = fma(dm.x, fma(sx, sc0, fma(sy, sc1, fma(sz, sc4, sw * sc5))), fma(-dm.y, smin, temp[j][n]));\n         }\n     }\n }"
      },
      {
        "filename": "ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_vec_q5_k.comp",
        "status": "modified",
        "additions": 2,
        "deletions": 4,
        "changes": 6,
        "patch": "@@ -14,9 +14,7 @@ void calc_superblock(const uint a_offset, const uint b_offset, const uint v_im,\n \n     [[unroll]] for (uint n = 0; n < num_rows; ++n) {\n         const uint ib0 = a_offset / QUANT_K + (first_row+n)*num_blocks_per_row;\n-        vec2 d = vec2(data_a[ib0 + i].d);\n-        const FLOAT_TYPE dall = FLOAT_TYPE(d.x);\n-        const FLOAT_TYPE dmin = FLOAT_TYPE(d.y);\n+        const FLOAT_TYPE_VEC2 dm = FLOAT_TYPE_VEC2(data_a[ib0 + i].dm);\n \n         const uint32_t scale0_u32 = data_a_packed16[ib0 + i].scales[v_im    ];\n         const uint32_t scale4_u32 = data_a_packed16[ib0 + i].scales[v_im + 2];\n@@ -113,7 +111,7 @@ void calc_superblock(const uint a_offset, const uint b_offset, const uint v_im,\n               fma(FLOAT_TYPE(by132.x) + FLOAT_TYPE(by132.y) + FLOAT_TYPE(by148.x) + FLOAT_TYPE(by148.y), sc3,\n               fma(FLOAT_TYPE(by20.x) + FLOAT_TYPE(by20.y) + FLOAT_TYPE(by216.x) + FLOAT_TYPE(by216.y), sc6,\n                   (FLOAT_TYPE(by232.x) + FLOAT_TYPE(by232.y) + FLOAT_TYPE(by248.x) + FLOAT_TYPE(by248.y)) * sc7)));\n-            temp[j][n] = fma(dall, fma(sx, sc0, fma(sy, sc1, fma(sz, sc4, sw * sc5))), fma(-dmin, smin, temp[j][n]));\n+            temp[j][n] = fma(dm.x, fma(sx, sc0, fma(sy, sc1, fma(sz, sc4, sw * sc5))), fma(-dm.y, smin, temp[j][n]));\n         }\n     }\n }"
      },
      {
        "filename": "ggml/src/ggml-vulkan/vulkan-shaders/mul_mm.comp",
        "status": "modified",
        "additions": 1,
        "deletions": 71,
        "changes": 72,
        "patch": "@@ -120,81 +120,11 @@ shared FLOAT_TYPE_VEC2 buf_b[BN * SHMEM_STRIDE];\n \n #define NUM_WARPS (BLOCK_SIZE / WARP)\n \n-#ifdef MUL_MAT_ID\n-shared u16vec2 row_ids[BN];\n-uint _ne1;\n-\n-#ifdef MUL_MAT_ID_USE_SUBGROUPS\n-shared uvec4 ballots_sh[NUM_WARPS];\n-\n-void load_row_ids(uint expert_idx, bool nei0_is_pow2, uint ic) {\n-    _ne1 = 0;\n-    uint num_elements = p.nei1 * p.nei0;\n-    uint nei0shift = findLSB(p.nei0);\n-\n-    uint ids[16];\n-    uint iter = 0;\n-\n-    for (uint j = 0; j < num_elements; j += BLOCK_SIZE) {\n-        // prefetch up to 16 elements\n-        if (iter == 0) {\n-            [[unroll]] for (uint k = 0; k < 16; ++k) {\n-                uint i = j + gl_LocalInvocationIndex + k*BLOCK_SIZE;\n-                bool in_range = i < num_elements;\n-                uint ii1;\n-                if (nei0_is_pow2) {\n-                    ii1 = i >> nei0shift;\n-                } else {\n-                    ii1 = i / p.nei0;\n-                }\n-                uint ii0 = i - ii1 * p.nei0;\n-                ids[k] = in_range ? data_ids[ii1*p.nbi1 + ii0] : 0;\n-            }\n-        }\n-        uint i = j + gl_LocalInvocationIndex;\n-        bool in_range = i < num_elements;\n-        uint ii1;\n-        if (nei0_is_pow2) {\n-            ii1 = i >> nei0shift;\n-        } else {\n-            ii1 = i / p.nei0;\n-        }\n-        uint ii0 = i - ii1 * p.nei0;\n-        uint id = ids[iter++];\n-        uvec4 ballot = subgroupBallot(in_range && id == expert_idx);\n-\n-        ballots_sh[gl_SubgroupID] = ballot;\n-        barrier();\n-\n-        uint subgroup_base = 0;\n-        uint total = 0;\n-        for (uint k = 0; k < gl_NumSubgroups; ++k) {\n-            if (k == gl_SubgroupID) {\n-                subgroup_base = total;\n-            }\n-            total += subgroupBallotBitCount(ballots_sh[k]);\n-        }\n-        barrier();\n-\n-        uint idx = subgroup_base + subgroupBallotExclusiveBitCount(ballot);\n-        if (in_range && id == expert_idx && _ne1 + idx >= ic * BN && _ne1 + idx < (ic + 1) * BN) {\n-            row_ids[_ne1 + idx - ic * BN] = u16vec2(ii0, ii1);\n-        }\n-        _ne1 += total;\n-        iter &= 15;\n-        if (_ne1 >= (ic + 1) * BN) {\n-            break;\n-        }\n-    }\n-    barrier();\n-}\n-#endif // MUL_MAT_ID_USE_SUBGROUPS\n-#endif // MUL_MAT_ID\n-\n #ifdef COOPMAT\n shared ACC_TYPE coopmat_stage[TM * TN * NUM_WARPS];\n #endif\n \n+#include \"mul_mm_id_funcs.glsl\"\n #include \"mul_mm_funcs.glsl\"\n \n void main() {"
      },
      {
        "filename": "ggml/src/ggml-vulkan/vulkan-shaders/mul_mm_funcs.glsl",
        "status": "modified",
        "additions": 7,
        "deletions": 7,
        "changes": 14,
        "patch": "@@ -134,15 +134,15 @@ void load_a_to_shmem(const uint pos_a, const uint row, const uint col, const uin\n             const uint ib = idx / 128;                         // 2 values per idx\n             const uint iqs = idx % 128;                        // 0..127\n \n-            const uint qsi = (iqs / 64) * 32 + (iqs % 16) * 2; // 0,2,4..30\n+            const uint qsi = (iqs / 64) * 16 + (iqs % 16);     // 0..15\n             const uint scalesi = iqs / 8;                      // 0..15\n             const uint qsshift = ((iqs % 64) / 16) * 2;        // 0,2,4,6\n \n-            const uvec2 qs = uvec2(data_a[ib].qs[qsi], data_a[ib].qs[qsi + 1]);\n+            const uvec2 qs = uvec2(unpack8(data_a_packed16[ib].qs[qsi]));\n             const uint scales = data_a[ib].scales[scalesi];\n-            const vec2 d = vec2(data_a[ib].d);\n+            const vec2 dm = vec2(data_a[ib].dm);\n \n-            const vec2 v = d.x * float(scales & 0xF) * vec2((qs >> qsshift) & 3) - d.y * float(scales >> 4);\n+            const vec2 v = dm.x * float(scales & 0xF) * vec2((qs >> qsshift) & 3) - dm.y * float(scales >> 4);\n \n             buf_a[buf_idx] = FLOAT_TYPE_VEC2(v.xy);\n #elif defined(DATA_A_Q3_K)\n@@ -179,7 +179,7 @@ void load_a_to_shmem(const uint pos_a, const uint row, const uint col, const uin\n             const uint is = 2 * n + b;                 // 0..7\n             const uint qsi = n * 32 + (iqs % 16) * 2;  // 0,2,4..126\n \n-            const vec2 loadd = vec2(data_a[ib].d);\n+            const vec2 loadd = vec2(data_a[ib].dm);\n \n             const uint scidx0 = (is < 4) ? is : (is + 4);\n             const uint scidx1 = (is < 4) ? is : (is - 4);\n@@ -215,7 +215,7 @@ void load_a_to_shmem(const uint pos_a, const uint row, const uint col, const uin\n \n             const uint8_t hm = uint8_t(1 << (iqs / 16));\n \n-            const vec2 loadd = vec2(data_a[ib].d);\n+            const vec2 loadd = vec2(data_a[ib].dm);\n \n             const uint scidx0 = (is < 4) ? is : (is + 4);\n             const uint scidx1 = (is < 4) ? is : (is - 4);\n@@ -468,7 +468,7 @@ void load_a_to_shmem(const uint pos_a, const uint row, const uint col, const uin\n             const uint ib = idx / 8;\n             const uint iqs = (idx & 0x07) * 2;\n \n-            const float d = e8m0_to_fp32(data_a[ib].e);\n+            const float d = e8m0_to_fp32(data_a[ib].e) * 0.5;\n             const uint vui = uint(data_a[ib].qs[iqs]);\n             const uint vui2 = uint(data_a[ib].qs[iqs+1]);\n "
      },
      {
        "filename": "ggml/src/ggml-vulkan/vulkan-shaders/mul_mm_id_funcs.glsl",
        "status": "added",
        "additions": 70,
        "deletions": 0,
        "changes": 70,
        "patch": "@@ -0,0 +1,70 @@\n+#ifdef MUL_MAT_ID\n+shared u16vec2 row_ids[BN];\n+uint _ne1;\n+\n+#ifdef MUL_MAT_ID_USE_SUBGROUPS\n+shared uvec4 ballots_sh[NUM_WARPS];\n+\n+void load_row_ids(uint expert_idx, bool nei0_is_pow2, uint ic) {\n+    _ne1 = 0;\n+    uint num_elements = p.nei1 * p.nei0;\n+    uint nei0shift = findLSB(p.nei0);\n+\n+    uint ids[16];\n+    uint iter = 0;\n+\n+    for (uint j = 0; j < num_elements; j += BLOCK_SIZE) {\n+        // prefetch up to 16 elements\n+        if (iter == 0) {\n+            [[unroll]] for (uint k = 0; k < 16; ++k) {\n+                uint i = j + gl_LocalInvocationIndex + k*BLOCK_SIZE;\n+                bool in_range = i < num_elements;\n+                uint ii1;\n+                if (nei0_is_pow2) {\n+                    ii1 = i >> nei0shift;\n+                } else {\n+                    ii1 = i / p.nei0;\n+                }\n+                uint ii0 = i - ii1 * p.nei0;\n+                ids[k] = in_range ? data_ids[ii1*p.nbi1 + ii0] : 0;\n+            }\n+        }\n+        uint i = j + gl_LocalInvocationIndex;\n+        bool in_range = i < num_elements;\n+        uint ii1;\n+        if (nei0_is_pow2) {\n+            ii1 = i >> nei0shift;\n+        } else {\n+            ii1 = i / p.nei0;\n+        }\n+        uint ii0 = i - ii1 * p.nei0;\n+        uint id = ids[iter++];\n+        uvec4 ballot = subgroupBallot(in_range && id == expert_idx);\n+\n+        ballots_sh[gl_SubgroupID] = ballot;\n+        barrier();\n+\n+        uint subgroup_base = 0;\n+        uint total = 0;\n+        for (uint k = 0; k < gl_NumSubgroups; ++k) {\n+            if (k == gl_SubgroupID) {\n+                subgroup_base = total;\n+            }\n+            total += subgroupBallotBitCount(ballots_sh[k]);\n+        }\n+        barrier();\n+\n+        uint idx = subgroup_base + subgroupBallotExclusiveBitCount(ballot);\n+        if (in_range && id == expert_idx && _ne1 + idx >= ic * BN && _ne1 + idx < (ic + 1) * BN) {\n+            row_ids[_ne1 + idx - ic * BN] = u16vec2(ii0, ii1);\n+        }\n+        _ne1 += total;\n+        iter &= 15;\n+        if (_ne1 >= (ic + 1) * BN) {\n+            break;\n+        }\n+    }\n+    barrier();\n+}\n+#endif // MUL_MAT_ID_USE_SUBGROUPS\n+#endif // MUL_MAT_ID"
      },
      {
        "filename": "ggml/src/ggml-vulkan/vulkan-shaders/mul_mmq.comp",
        "status": "modified",
        "additions": 69,
        "deletions": 219,
        "changes": 288,
        "patch": "@@ -10,10 +10,9 @@\n #extension GL_EXT_shader_explicit_arithmetic_types_float16 : require\n #endif\n \n-#ifdef COOPMAT\n-#extension GL_KHR_cooperative_matrix : enable\n-#extension GL_KHR_memory_scope_semantics : enable\n+#if defined(MUL_MAT_ID_USE_SUBGROUPS)\n #extension GL_KHR_shader_subgroup_basic : enable\n+#extension GL_KHR_shader_subgroup_ballot : enable\n #endif\n \n #ifdef MUL_MAT_ID\n@@ -24,7 +23,10 @@\n \n layout(local_size_x_id = 0, local_size_y = 1, local_size_z = 1) in;\n \n-layout (binding = 0) readonly buffer A {A_TYPE_PACKED16 data_a[];};\n+layout (binding = 0) readonly buffer A {A_TYPE data_a[];};\n+#if defined(A_TYPE_PACKED16)\n+layout (binding = 0) readonly buffer A_PACKED16 {A_TYPE_PACKED16 data_a_packed16[];};\n+#endif\n #if defined(A_TYPE_PACKED32)\n layout (binding = 0) readonly buffer A_PACKED32 {A_TYPE_PACKED32 data_a_packed32[];};\n #endif\n@@ -76,40 +78,27 @@ layout (constant_id = 10) const uint WARP = 32;\n \n #define BK 32\n \n-#ifdef COOPMAT\n-#define SHMEM_STRIDE (BK / 4 + 4)\n-#else\n-#define SHMEM_STRIDE (BK / 4 + 1)\n-#endif\n+#define MMQ_SHMEM\n \n-shared int32_t buf_a_qs[BM * SHMEM_STRIDE];\n+#include \"mul_mmq_shmem_types.glsl\"\n \n-#ifndef COOPMAT\n-#if QUANT_AUXF == 1\n-shared FLOAT_TYPE buf_a_dm[BM];\n-#else\n-shared FLOAT_TYPE_VEC2 buf_a_dm[BM];\n-#endif\n+#ifndef BK_STEP\n+#define BK_STEP 4\n #endif\n \n-shared int32_t buf_b_qs[BN * SHMEM_STRIDE];\n-#ifndef COOPMAT\n-shared FLOAT_TYPE_VEC2 buf_b_ds[BN];\n-#endif\n+// Shared memory cache\n+shared block_a_cache buf_a[BM * BK_STEP];\n+shared block_b_cache buf_b[BN * BK_STEP];\n+// Register cache\n+block_a_cache cache_a[WMITER * TM];\n+block_b_cache cache_b;\n \n-#define LOAD_VEC_A (4 * QUANT_R)\n+#define LOAD_VEC_A (4 * QUANT_R_MMQ)\n #define LOAD_VEC_B 16\n \n-#ifdef MUL_MAT_ID\n-shared u16vec2 row_ids[4096];\n-#endif // MUL_MAT_ID\n-\n #define NUM_WARPS (BLOCK_SIZE / WARP)\n \n-#ifdef COOPMAT\n-shared ACC_TYPE coopmat_stage[TM * TN * NUM_WARPS];\n-#endif\n-\n+#include \"mul_mm_id_funcs.glsl\"\n #include \"mul_mmq_funcs.glsl\"\n \n void main() {\n@@ -139,26 +128,12 @@ void main() {\n     const uint WNITER = (WM * WN) / (WARP * TM * TN * WMITER);\n     const uint WSUBM = WM / WMITER;\n     const uint WSUBN = WN / WNITER;\n-\n-#ifdef COOPMAT\n-    const uint warp_i = gl_SubgroupID;\n-\n-    const uint tiw = gl_SubgroupInvocationID;\n-\n-    const uint cms_per_row = WM / TM;\n-    const uint cms_per_col = WN / TN;\n-\n-    const uint storestride = WARP / TM;\n-    const uint store_r = tiw % TM;\n-    const uint store_c = tiw / TM;\n-#else\n     const uint warp_i = gl_LocalInvocationID.x / WARP;\n \n     const uint tiw = gl_LocalInvocationID.x % WARP;\n \n     const uint tiwr = tiw % (WSUBM / TM);\n     const uint tiwc = tiw / (WSUBM / TM);\n-#endif\n \n     const uint warp_r = warp_i % (BM / WM);\n     const uint warp_c = warp_i / (BM / WM);\n@@ -172,17 +147,27 @@ void main() {\n     const uint loadstride_b = BLOCK_SIZE * LOAD_VEC_B / BK;\n \n #ifdef MUL_MAT_ID\n-    uint _ne1 = 0;\n-    for (uint ii1 = 0; ii1 < p.nei1; ii1++) {\n-        for (uint ii0 = 0; ii0 < p.nei0; ii0++) {\n+#ifdef MUL_MAT_ID_USE_SUBGROUPS\n+    if (bitCount(p.nei0) == 1) {\n+        load_row_ids(expert_idx, true, ic);\n+    } else {\n+        load_row_ids(expert_idx, false, ic);\n+    }\n+#else\n+    _ne1 = 0;\n+    for (uint ii1 = 0; ii1 < p.nei1 && _ne1 < (ic + 1) * BN; ii1++) {\n+        for (uint ii0 = 0; ii0 < p.nei0 && _ne1 < (ic + 1) * BN; ii0++) {\n             if (data_ids[ii1*p.nbi1 + ii0] == expert_idx) {\n-                row_ids[_ne1] = u16vec2(ii0, ii1);\n+                if (_ne1 >= ic * BN) {\n+                    row_ids[_ne1 - ic * BN] = u16vec2(ii0, ii1);\n+                }\n                 _ne1++;\n             }\n         }\n     }\n \n     barrier();\n+#endif\n \n     // Workgroup has no work\n     if (ic * BN >= _ne1) return;\n@@ -209,159 +194,70 @@ void main() {\n     uint pos_b_ib = (batch_idx * p.batch_stride_b + ic * BN * p.stride_b + start_k) / BK;\n #endif\n \n-#ifdef COOPMAT\n-    coopmat<int8_t, gl_ScopeSubgroup, TM, TK, gl_MatrixUseA> cache_a;\n-    coopmat<int8_t, gl_ScopeSubgroup, TK, TN, gl_MatrixUseB> cache_b;\n-    coopmat<int32_t, gl_ScopeSubgroup, TM, TN, gl_MatrixUseAccumulator> cm_result;\n-\n-    coopmat<ACC_TYPE, gl_ScopeSubgroup, TM, TN, gl_MatrixUseAccumulator> factors[cms_per_row * cms_per_col];\n-\n-    coopmat<ACC_TYPE, gl_ScopeSubgroup, TM, TN, gl_MatrixUseAccumulator> sums[cms_per_row * cms_per_col];\n-\n-    [[unroll]] for (uint i = 0; i < cms_per_row * cms_per_col; i++) {\n-        sums[i] = coopmat<ACC_TYPE, gl_ScopeSubgroup, TM, TN, gl_MatrixUseAccumulator>(0.0f);\n-    }\n-#else\n-    int32_t cache_a_qs[WMITER * TM * BK / 4];\n-\n-    int32_t cache_b_qs[TN * BK / 4];\n-\n     ACC_TYPE sums[WMITER * TM * WNITER * TN];\n \n     [[unroll]] for (uint i = 0; i < WMITER*TM*WNITER*TN; i++) {\n         sums[i] = ACC_TYPE(0.0f);\n     }\n-#endif\n \n-#if QUANT_AUXF == 1\n-    FLOAT_TYPE cache_a_dm[WMITER * TM];\n-#else\n-    FLOAT_TYPE_VEC2 cache_a_dm[WMITER * TM];\n-#endif\n-\n-    FLOAT_TYPE_VEC2 cache_b_ds[TN];\n-\n-    for (uint block = start_k; block < end_k; block += BK) {\n+    for (uint block = start_k; block < end_k; block += BK * BK_STEP) {\n         [[unroll]] for (uint l = 0; loadc_a + l < BM; l += loadstride_a) {\n-            const uint ib = pos_a_ib + (loadc_a + l) * p.stride_a / BK;\n-            const uint iqs = loadr_a;\n             const uint buf_ib = loadc_a + l;\n+            const uint ib = pos_a_ib + buf_ib * p.stride_a / BK;\n+            const uint iqs = loadr_a;\n \n-            if (iqs == 0) {\n-#if QUANT_AUXF == 1\n-                buf_a_dm[buf_ib] = get_d(ib);\n-#else\n-                buf_a_dm[buf_ib] = get_dm(ib);\n-#endif\n+            [[unroll]] for (uint k_step = 0; k_step < BK_STEP; k_step++) {\n+                block_a_to_shmem(k_step * BM + buf_ib, ib + k_step, iqs);\n             }\n-#if QUANT_R == 1\n-            buf_a_qs[buf_ib * SHMEM_STRIDE + iqs] = repack(ib, iqs);\n-#else\n-            const i32vec2 vals = repack(ib, iqs);\n-            buf_a_qs[buf_ib * SHMEM_STRIDE + iqs    ] = vals.x;\n-            buf_a_qs[buf_ib * SHMEM_STRIDE + iqs + 4] = vals.y;\n-#endif\n         }\n         [[unroll]] for (uint l = 0; loadc_b + l < BN; l += loadstride_b) {\n+            const uint buf_ib = loadc_b + l;\n+\n #ifdef MUL_MAT_ID\n-            const u16vec2 row_idx = row_ids[ic * BN + loadc_b + l];\n-            const uint idx = pos_b_ib + row_idx.y * p.batch_stride_b / LOAD_VEC_B + (row_idx.x % p.ne11) * p.stride_b / LOAD_VEC_B + loadr_b;\n-            const uint ib = idx / 8;\n-            const uint iqs = idx & 0x7;\n+            const u16vec2 row_idx = row_ids[buf_ib];\n+            const uint ib = pos_b_ib + row_idx.y * p.batch_stride_b / BK + (row_idx.x % p.ne11) * p.stride_b / BK;\n #else\n-            const uint ib = pos_b_ib + (loadc_b + l) * p.stride_b / BK;\n-            const uint ib_outer = ib / 4;\n-            const uint ib_inner = ib % 4;\n-\n-            const uint iqs = loadr_b;\n+            const uint ib = pos_b_ib + buf_ib * p.stride_b / BK;\n #endif\n+            const uint iqs = loadr_b;\n \n-            const uint buf_ib = loadc_b + l;\n-\n-            if (iqs == 0) {\n-                buf_b_ds[buf_ib] = FLOAT_TYPE_VEC2(data_b[ib_outer].ds[ib_inner]);\n+            [[unroll]] for (uint k_step = 0; k_step < BK_STEP; k_step++) {\n+                block_b_to_shmem(k_step * BN + buf_ib, ib + k_step, iqs);\n             }\n-            const ivec4 values = data_b[ib_outer].qs[ib_inner * 2 + iqs];\n-            buf_b_qs[buf_ib * SHMEM_STRIDE + iqs * 4    ] = values.x;\n-            buf_b_qs[buf_ib * SHMEM_STRIDE + iqs * 4 + 1] = values.y;\n-            buf_b_qs[buf_ib * SHMEM_STRIDE + iqs * 4 + 2] = values.z;\n-            buf_b_qs[buf_ib * SHMEM_STRIDE + iqs * 4 + 3] = values.w;\n         }\n \n         barrier();\n \n-        pos_a_ib += 1;\n-        pos_b_ib += 1;\n+        pos_a_ib += BK_STEP;\n+        pos_b_ib += BK_STEP;\n \n-#ifdef COOPMAT\n-        [[unroll]] for (uint cm_row = 0; cm_row < cms_per_row; cm_row++) {\n-            const uint ib_a = warp_r * WM + cm_row * TM;\n+        for (uint k_step = 0; k_step < BK_STEP; k_step++) {\n             // Load from shared into cache\n-            coopMatLoad(cache_a, buf_a_qs, ib_a * SHMEM_STRIDE, SHMEM_STRIDE, gl_CooperativeMatrixLayoutRowMajor);\n-\n-            // TODO: only cache values that are actually needed\n-            [[unroll]] for (uint t_idx = 0; t_idx < TM; t_idx++) {\n-                cache_a_dm[t_idx] = buf_a_dm[ib_a + t_idx];\n-            }\n-\n-            [[unroll]] for (uint cm_col = 0; cm_col < cms_per_col; cm_col++) {\n-                const uint ib_b = warp_c * WN + cm_col * TN;\n-                coopMatLoad(cache_b, buf_b_qs, ib_b * SHMEM_STRIDE, SHMEM_STRIDE, gl_CooperativeMatrixLayoutColumnMajor);\n-\n-                // TODO: only cache values that are actually needed\n-                [[unroll]] for (uint t_idx = 0; t_idx < TN; t_idx++) {\n-                    cache_b_dm[t_idx] = buf_b_d[ib_b + t_idx];\n-                }\n-\n-                cm_result = coopmat<int32_t, gl_ScopeSubgroup, TM, TN, gl_MatrixUseAccumulator>(0);\n-                cm_result = coopMatMulAdd(cache_a, cache_b, cm_result);\n-\n-                [[unroll]] for (uint col = 0; col < TN; col += storestride) {\n-                    coopmat_stage[warp_i * TM * TN + (store_c + col) * TM + store_r] = ACC_TYPE(float(cache_a_d[store_r]) * float(cache_b_d[store_c + col]));\n-                }\n-\n-                coopMatLoad(factors, coopmat_stage, warp_i * TM * TN, TM, gl_CooperativeMatrixLayoutColumnMajor);\n-                sums[cm_col * cms_per_row + cm_row] += factors * coopmat<ACC_TYPE, gl_ScopeSubgroup, TM, TN, gl_MatrixUseAccumulator>(cm_result);\n-            }\n-        }\n-#else\n-        // Load from shared into cache\n-        [[unroll]] for (uint wsir = 0; wsir < WMITER; wsir++) {\n-            [[unroll]] for (uint cr = 0; cr < TM; cr++) {\n-                const uint ib = warp_r * WM + wsir * WSUBM + tiwr * TM + cr;\n-                cache_a_dm[wsir * TM + cr] = buf_a_dm[ib];\n-                [[unroll]] for (uint idx_k = 0; idx_k < BK / 4; idx_k++) {\n-                    cache_a_qs[(wsir * TM + cr) * (BK / 4) + idx_k] = buf_a_qs[ib * SHMEM_STRIDE + idx_k];\n-                }\n-            }\n-        }\n+            [[unroll]] for (uint wsir = 0; wsir < WMITER; wsir++) {\n+                [[unroll]] for (uint cr = 0; cr < TM; cr++) {\n+                    const uint reg_ib = wsir * TM + cr;\n+                    const uint buf_ib = warp_r * WM + wsir * WSUBM + tiwr * TM + cr;\n \n-        [[unroll]] for (uint wsic = 0; wsic < WNITER; wsic++) {\n-            [[unroll]] for (uint cc = 0; cc < TN; cc++) {\n-                const uint ib = warp_c * WN + wsic * WSUBN + tiwc * TN + cc;\n-                cache_b_ds[cc] = buf_b_ds[ib];\n-                [[unroll]] for (uint idx_k = 0; idx_k < BK / 4; idx_k++) {\n-                    cache_b_qs[cc * (BK / 4) + idx_k] = buf_b_qs[ib * SHMEM_STRIDE + idx_k];\n+                    block_a_to_registers(reg_ib, k_step * BM + buf_ib);\n                 }\n             }\n \n-            [[unroll]] for (uint wsir = 0; wsir < WMITER; wsir++) {\n+            [[unroll]] for (uint wsic = 0; wsic < WNITER; wsic++) {\n                 [[unroll]] for (uint cc = 0; cc < TN; cc++) {\n-                    [[unroll]] for (uint cr = 0; cr < TM; cr++) {\n-                        const uint cache_a_idx = wsir * TM + cr;\n-                        const uint sums_idx = (wsic * TN + cc) * (WMITER * TM) + wsir * TM + cr;\n-                        int32_t q_sum = 0;\n-                        [[unroll]] for (uint idx_k = 0; idx_k < BK / 4; idx_k++) {\n-                            q_sum += dotPacked4x8EXT(cache_a_qs[cache_a_idx * (BK / 4) + idx_k],\n-                                                     cache_b_qs[cc * (BK / 4) + idx_k]);\n-                        }\n+                    const uint ib = k_step * BN + warp_c * WN + wsic * WSUBN + tiwc * TN + cc;\n+                    block_b_to_registers(ib);\n \n-                        sums[sums_idx] += mul_q8_1(q_sum, cache_a_dm[cache_a_idx], cache_b_ds[cc], 1);\n+                    [[unroll]] for (uint wsir = 0; wsir < WMITER; wsir++) {\n+                        [[unroll]] for (uint cr = 0; cr < TM; cr++) {\n+                            const uint cache_a_idx = wsir * TM + cr;\n+                            const uint sums_idx = (wsic * TN + cc) * (WMITER * TM) + wsir * TM + cr;\n+\n+                            sums[sums_idx] += mmq_dot_product(cache_a_idx);\n+                        }\n                     }\n                 }\n             }\n         }\n-#endif\n \n         barrier();\n     }\n@@ -373,54 +269,6 @@ void main() {\n     const uint offsets = batch_idx * p.batch_stride_d + ik * p.batch_stride_d * gl_NumWorkGroups.z;\n #endif\n \n-#ifdef COOPMAT\n-#ifdef MUL_MAT_ID\n-    [[unroll]] for (uint cm_row = 0; cm_row < cms_per_row; cm_row++) {\n-        [[unroll]] for (uint cm_col = 0; cm_col < cms_per_col; cm_col++) {\n-            coopMatStore(sums[cm_col * cms_per_row + cm_row], coopmat_stage, warp_i * TM * TN, TM, gl_CooperativeMatrixLayoutColumnMajor);\n-\n-            [[unroll]] for (uint col = 0; col < BN; col += storestride) {\n-                const uint row_i = dc + cm_col * TN + col + store_c;\n-                if (row_i >= _ne1) break;\n-\n-                const u16vec2 row_idx = row_ids[row_i];\n-\n-                data_d[row_idx.y * p.batch_stride_d + row_idx.x * p.stride_d + dr + cm_row * TM + store_r] = D_TYPE(coopmat_stage[warp_i * TM * TN + (col + store_c) * TM + store_r]);\n-            }\n-        }\n-    }\n-#else\n-    const bool is_aligned = p.stride_d % 4 == 0;  // Assumption: D_TYPE == float\n-\n-    [[unroll]] for (uint cm_row = 0; cm_row < cms_per_row; cm_row++) {\n-        [[unroll]] for (uint cm_col = 0; cm_col < cms_per_col; cm_col++) {\n-            const bool is_in_bounds = dr + (cm_row + 1) * TM <= p.M && dc + (cm_col + 1) * TN <= p.N;\n-\n-            if (is_aligned && is_in_bounds) {\n-                // Full coopMat is within bounds and stride_d is aligned with 16B\n-                coopmat<D_TYPE, gl_ScopeSubgroup, TM, TN, gl_MatrixUseAccumulator> cm_dtype = coopmat<D_TYPE, gl_ScopeSubgroup, TM, TN, gl_MatrixUseAccumulator>(sums[cm_col * cms_per_row + cm_row]);\n-                coopMatStore(cm_dtype, data_d, offsets + (dc + cm_col * TN) * p.stride_d + dr + cm_row * TM, p.stride_d, gl_CooperativeMatrixLayoutColumnMajor);\n-            } else if (is_in_bounds) {\n-                // Full coopMat is within bounds, but stride_d is not aligned\n-                coopMatStore(sums[cm_col * cms_per_row + cm_row], coopmat_stage, warp_i * TM * TN, TM, gl_CooperativeMatrixLayoutColumnMajor);\n-\n-                [[unroll]] for (uint col = 0; col < TN; col += storestride) {\n-                    data_d[offsets + (dc + cm_col * TN + col + store_c) * p.stride_d + dr + cm_row * TM + store_r] = D_TYPE(coopmat_stage[warp_i * TM * TN + (col + store_c) * TM + store_r]);\n-                }\n-            } else if (dr + cm_row * TM < p.M && dc + cm_col * TN < p.N) {\n-                // Partial coopMat is within bounds\n-                coopMatStore(sums[cm_col * cms_per_row + cm_row], coopmat_stage, warp_i * TM * TN, TM, gl_CooperativeMatrixLayoutColumnMajor);\n-\n-                [[unroll]] for (uint col = 0; col < TN; col += storestride) {\n-                    if (dr + cm_row * TM + store_r < p.M && dc + cm_col * TN + col + store_c < p.N) {\n-                        data_d[offsets + (dc + cm_col * TN + col + store_c) * p.stride_d + dr + cm_row * TM + store_r] = D_TYPE(coopmat_stage[warp_i * TM * TN + (col + store_c) * TM + store_r]);\n-                    }\n-                }\n-            }\n-        }\n-    }\n-#endif // MUL_MAT_ID\n-#else\n     [[unroll]] for (uint wsic = 0; wsic < WNITER; wsic++) {\n         [[unroll]] for (uint wsir = 0; wsir < WMITER; wsir++) {\n \n@@ -431,19 +279,21 @@ void main() {\n                 const uint row_i = dc_warp + cc;\n                 if (row_i >= _ne1) break;\n \n-                const u16vec2 row_idx = row_ids[row_i];\n+                const u16vec2 row_idx = row_ids[row_i - ic * BN];\n #endif // MUL_MAT_ID\n                 [[unroll]] for (uint cr = 0; cr < TM; cr++) {\n+                    const uint sums_idx = (wsic * TN + cc) * WMITER * TM + wsir * TM + cr;\n #ifdef MUL_MAT_ID\n-                    data_d[row_idx.y * p.batch_stride_d + row_idx.x * p.stride_d + dr_warp + cr] = D_TYPE(sums[(wsic * TN + cc) * (WMITER * TM) + wsir * TM + cr]);\n+                    if (dr_warp + cr < p.M) {\n+                        data_d[row_idx.y * p.batch_stride_d + row_idx.x * p.stride_d + dr_warp + cr] = D_TYPE(sums[sums_idx].x);\n+                    }\n #else\n                     if (dr_warp + cr < p.M && dc_warp + cc < p.N) {\n-                        data_d[offsets + (dc_warp + cc) * p.stride_d + dr_warp + cr] = D_TYPE(sums[(wsic * TN + cc) * (WMITER * TM) + wsir * TM + cr]);\n+                        data_d[offsets + (dc_warp + cc) * p.stride_d + dr_warp + cr] = D_TYPE(sums[sums_idx].x);\n                     }\n #endif // MUL_MAT_ID\n                 }\n             }\n         }\n     }\n-#endif // COOPMAT\n }"
      },
      {
        "filename": "ggml/src/ggml-vulkan/vulkan-shaders/mul_mmq_funcs.glsl",
        "status": "modified",
        "additions": 505,
        "deletions": 33,
        "changes": 538,
        "patch": "@@ -6,41 +6,89 @@\n \n // Each iqs value maps to a 32-bit integer\n \n-#if defined(DATA_A_Q4_0)\n+#if defined(DATA_A_Q4_0) || defined(DATA_A_Q4_1)\n+// 2-byte loads for Q4_0 blocks (18 bytes)\n+// 4-byte loads for Q4_1 blocks (20 bytes)\n i32vec2 repack(uint ib, uint iqs) {\n-    // Use 2-byte loads since a q4_0 block (18 bytes) is not divisible by 4\n-    const u16vec2 quants = u16vec2(data_a[ib].qs[iqs * 2    ],\n-                                   data_a[ib].qs[iqs * 2 + 1]);\n+#ifdef DATA_A_Q4_0\n+    const u16vec2 quants = u16vec2(data_a_packed16[ib].qs[iqs * 2    ],\n+                                   data_a_packed16[ib].qs[iqs * 2 + 1]);\n     const uint32_t vui = pack32(quants);\n     return i32vec2( vui       & 0x0F0F0F0F,\n                    (vui >> 4) & 0x0F0F0F0F);\n+#else // DATA_A_Q4_1\n+    const uint32_t vui = data_a_packed32[ib].qs[iqs];\n+    return i32vec2( vui       & 0x0F0F0F0F,\n+                   (vui >> 4) & 0x0F0F0F0F);\n+#endif\n }\n \n+#ifdef DATA_A_Q4_0\n ACC_TYPE mul_q8_1(const int32_t q_sum, const float da, const vec2 dsb, const int32_t sum_divisor) {\n     return ACC_TYPE(da * (float(q_sum) * dsb.x - (8 / sum_divisor) * dsb.y));\n }\n+#else // DATA_A_Q4_1\n+ACC_TYPE mul_q8_1(const int32_t q_sum, const vec2 dma, const vec2 dsb, const int32_t sum_divisor) {\n+    return ACC_TYPE(float(q_sum) * dma.x * dsb.x + dma.y * dsb.y / sum_divisor);\n+}\n #endif\n \n-#if defined(DATA_A_Q4_1)\n-i32vec2 repack(uint ib, uint iqs) {\n-    // Use 4-byte loads since a q4_1 block (20 bytes) is divisible by 4\n-    const uint32_t vui = data_a_packed32[ib].qs[iqs];\n-    return i32vec2( vui       & 0x0F0F0F0F,\n-                   (vui >> 4) & 0x0F0F0F0F);\n+#ifdef MMQ_SHMEM\n+void block_a_to_shmem(const uint buf_ib, const uint ib, const uint iqs) {\n+#ifdef DATA_A_Q4_0\n+    buf_a[buf_ib].qs[iqs] = pack32(u16vec2(data_a_packed16[ib].qs[iqs * 2],\n+                                           data_a_packed16[ib].qs[iqs * 2 + 1]));\n+\n+    if (iqs == 0) {\n+        buf_a[buf_ib].dm = FLOAT_TYPE(data_a_packed16[ib].d);\n+    }\n+#else // DATA_A_Q4_1\n+    buf_a[buf_ib].qs[iqs] = data_a_packed32[ib].qs[iqs];\n+\n+    if (iqs == 0) {\n+        buf_a[buf_ib].dm = FLOAT_TYPE_VEC2(data_a_packed32[ib].dm);\n+    }\n+#endif\n }\n \n-ACC_TYPE mul_q8_1(const int32_t q_sum, const vec2 dma, const vec2 dsb, const int32_t sum_divisor) {\n-    return ACC_TYPE(float(q_sum) * dma.x * dsb.x + dma.y * dsb.y / sum_divisor);\n+void block_a_to_registers(const uint reg_ib, const uint buf_ib) {\n+    cache_a[reg_ib].dm = buf_a[buf_ib].dm;\n+\n+    [[unroll]] for (uint iqs = 0; iqs < 4; iqs++) {\n+        cache_a[reg_ib].qs[iqs] = buf_a[buf_ib].qs[iqs];\n+    }\n }\n-#endif\n \n-#if defined(DATA_A_Q5_0)\n+ACC_TYPE mmq_dot_product(const uint ib_a) {\n+    int32_t q_sum = 0;\n+    [[unroll]] for (uint iqs = 0; iqs < 4; iqs++) {\n+        const uint32_t vui = cache_a[ib_a].qs[iqs];\n+        const i32vec2 qs_a = i32vec2( vui       & 0x0F0F0F0F,\n+                                     (vui >> 4) & 0x0F0F0F0F);\n+\n+        const int32_t qs_b0 = cache_b.qs[iqs];\n+        const int32_t qs_b1 = cache_b.qs[iqs + 4];\n+\n+        q_sum += dotPacked4x8EXT(qs_a.x, qs_b0);\n+        q_sum += dotPacked4x8EXT(qs_a.y, qs_b1);\n+    }\n+\n+    return mul_q8_1(q_sum, cache_a[ib_a].dm, cache_b.ds, 1);\n+}\n+#endif // MMQ_SHMEM\n+\n+#elif defined(DATA_A_Q5_0) || defined(DATA_A_Q5_1)\n+// 2-byte loads for Q5_0 blocks (22 bytes)\n+// 4-byte loads for Q5_1 blocks (24 bytes)\n i32vec2 repack(uint ib, uint iqs) {\n-    // Use 2-byte loads since a q5_0 block (22 bytes) is not divisible by 4\n-    const u16vec2 quants = u16vec2(data_a[ib].qs[iqs * 2    ],\n-                                   data_a[ib].qs[iqs * 2 + 1]);\n+    const u16vec2 quants = u16vec2(data_a_packed16[ib].qs[iqs * 2    ],\n+                                   data_a_packed16[ib].qs[iqs * 2 + 1]);\n     const uint32_t vui = pack32(quants);\n-    const int32_t qh = int32_t((uint32_t(data_a[ib].qh[1]) << 16 | data_a[ib].qh[0]) >> (4 * iqs));\n+#ifdef DATA_A_Q5_0\n+    const int32_t qh = int32_t((uint32_t(data_a_packed16[ib].qh[1]) << 16 | data_a_packed16[ib].qh[0]) >> (4 * iqs));\n+#else // DATA_A_Q5_1\n+    const int32_t qh = int32_t(data_a_packed32[ib].qh >> (4 * iqs));\n+#endif\n     const int32_t v0 = int32_t(vui & 0x0F0F0F0F)\n                      | ((qh & 0xF) * 0x02040810) & 0x10101010; // (0,1,2,3) -> (4,12,20,28)\n \n@@ -50,40 +98,457 @@ i32vec2 repack(uint ib, uint iqs) {\n     return i32vec2(v0, v1);\n }\n \n+#ifdef DATA_A_Q5_0\n ACC_TYPE mul_q8_1(const int32_t q_sum, const float da, const vec2 dsb, const int32_t sum_divisor) {\n     return ACC_TYPE(da * (float(q_sum) * dsb.x - (16 / sum_divisor) * dsb.y));\n }\n+#else // DATA_A_Q5_1\n+ACC_TYPE mul_q8_1(const int32_t q_sum, const vec2 dma, const vec2 dsb, const int32_t sum_divisor) {\n+    return ACC_TYPE(float(q_sum) * dma.x * dsb.x + dma.y * dsb.y / sum_divisor);\n+}\n #endif\n \n-#if defined(DATA_A_Q5_1)\n-i32vec2 repack(uint ib, uint iqs) {\n-    // Use 4-byte loads since a q5_1 block (24 bytes) is divisible by 4\n-    const uint32_t vui = data_a_packed32[ib].qs[iqs];\n-    const int32_t qh = int32_t(data_a_packed32[ib].qh >> (4 * iqs));\n-    const int32_t v0 = int32_t(vui & 0x0F0F0F0F)\n-                     | ((qh & 0xF) * 0x02040810) & 0x10101010; // (0,1,2,3) -> (4,12,20,28)\n+#ifdef MMQ_SHMEM\n+void block_a_to_shmem(const uint buf_ib, const uint ib, const uint iqs) {\n+#ifdef DATA_A_Q5_0\n+    buf_a[buf_ib].qs[iqs] = pack32(u16vec2(data_a_packed16[ib].qs[iqs * 2],\n+                                           data_a_packed16[ib].qs[iqs * 2 + 1]));\n \n-    const int32_t v1 = int32_t((vui >> 4) & 0x0F0F0F0F)\n-                     | (((qh >> 16) & 0xF) * 0x02040810) & 0x10101010; // (16,17,18,19) -> (4,12,20,28)\n+    if (iqs == 0) {\n+        buf_a[buf_ib].dm = FLOAT_TYPE(data_a_packed16[ib].d);\n+        buf_a[buf_ib].qh = pack32(u16vec2(data_a_packed16[ib].qh[0], data_a_packed16[ib].qh[1]));\n+    }\n+#else // DATA_A_Q5_1\n+    buf_a[buf_ib].qs[iqs] = data_a_packed32[ib].qs[iqs];\n \n-    return i32vec2(v0, v1);\n+    if (iqs == 0) {\n+        buf_a[buf_ib].dm = FLOAT_TYPE_VEC2(data_a_packed32[ib].dm);\n+        buf_a[buf_ib].qh = data_a_packed32[ib].qh;\n+    }\n+#endif\n }\n \n-ACC_TYPE mul_q8_1(const int32_t q_sum, const vec2 dma, const vec2 dsb, const int32_t sum_divisor) {\n-    return ACC_TYPE(float(q_sum) * dma.x * dsb.x + dma.y * dsb.y / sum_divisor);\n+void block_a_to_registers(const uint reg_ib, const uint buf_ib) {\n+    cache_a[reg_ib].dm = buf_a[buf_ib].dm;\n+    cache_a[reg_ib].qh = buf_a[buf_ib].qh;\n+\n+    [[unroll]] for (uint iqs = 0; iqs < 4; iqs++) {\n+        cache_a[reg_ib].qs[iqs] = buf_a[buf_ib].qs[iqs];\n+    }\n }\n+\n+ACC_TYPE mmq_dot_product(const uint ib_a) {\n+    int32_t q_sum = 0;\n+    [[unroll]] for (uint iqs = 0; iqs < 4; iqs++) {\n+        const uint32_t vui = cache_a[ib_a].qs[iqs];\n+        const int32_t qh = int32_t(cache_a[ib_a].qh >> (4 * iqs));\n+        const int32_t qs_a0 = int32_t(vui & 0x0F0F0F0F)\n+                         | ((qh & 0xF) * 0x02040810) & 0x10101010; // (0,1,2,3) -> (4,12,20,28)\n+        const int32_t qs_a1 = int32_t((vui >> 4) & 0x0F0F0F0F)\n+                         | (((qh >> 16) & 0xF) * 0x02040810) & 0x10101010; // (16,17,18,19) -> (4,12,20,28)\n+\n+        const int32_t qs_b0 = cache_b.qs[iqs];\n+        const int32_t qs_b1 = cache_b.qs[iqs + 4];\n+\n+        q_sum += dotPacked4x8EXT(qs_a0, qs_b0);\n+        q_sum += dotPacked4x8EXT(qs_a1, qs_b1);\n+    }\n+\n+    return mul_q8_1(q_sum, cache_a[ib_a].dm, cache_b.ds, 1);\n+}\n+#endif // MMQ_SHMEM\n #endif\n \n #if defined(DATA_A_Q8_0)\n+// 2-byte loads for Q8_0 blocks (34 bytes)\n int32_t repack(uint ib, uint iqs) {\n-    // Use 2-byte loads since a q8_0 block (34 bytes) is not divisible by 4\n-    return pack32(i16vec2(data_a[ib].qs[iqs * 2    ],\n-                          data_a[ib].qs[iqs * 2 + 1]));\n+    return pack32(i16vec2(data_a_packed16[ib].qs[iqs * 2    ],\n+                          data_a_packed16[ib].qs[iqs * 2 + 1]));\n }\n \n ACC_TYPE mul_q8_1(const int32_t q_sum, const float da, const vec2 dsb, const int32_t sum_divisor) {\n     return ACC_TYPE(float(q_sum) * da * dsb.x);\n }\n+\n+#ifdef MMQ_SHMEM\n+void block_a_to_shmem(const uint buf_ib, const uint ib, const uint iqs) {\n+    buf_a[buf_ib].qs[iqs] = pack32(i16vec2(data_a_packed16[ib].qs[iqs * 2],\n+                                           data_a_packed16[ib].qs[iqs * 2 + 1]));\n+\n+    if (iqs == 0) {\n+        buf_a[buf_ib].dm = FLOAT_TYPE(data_a_packed16[ib].d);\n+    }\n+}\n+\n+void block_a_to_registers(const uint reg_ib, const uint buf_ib) {\n+    cache_a[reg_ib].dm = buf_a[buf_ib].dm;\n+\n+    [[unroll]] for (uint iqs = 0; iqs < 8; iqs++) {\n+        cache_a[reg_ib].qs[iqs] = buf_a[buf_ib].qs[iqs];\n+    }\n+}\n+\n+ACC_TYPE mmq_dot_product(const uint ib_a) {\n+    int32_t q_sum = 0;\n+    [[unroll]] for (uint iqs = 0; iqs < 8; iqs++) {\n+        const int32_t qs_a = cache_a[ib_a].qs[iqs];\n+        const int32_t qs_b = cache_b.qs[iqs];\n+\n+        q_sum += dotPacked4x8EXT(qs_a, qs_b);\n+    }\n+\n+    return mul_q8_1(q_sum, cache_a[ib_a].dm, cache_b.ds, 1);\n+}\n+#endif // MMQ_SHMEM\n+#endif\n+\n+#if defined(DATA_A_MXFP4)\n+// 1-byte loads for mxfp4 blocks (17 bytes)\n+i32vec2 repack(uint ib, uint iqs) {\n+    const uint32_t quants = pack32(u8vec4(data_a[ib].qs[iqs * 4    ],\n+                                          data_a[ib].qs[iqs * 4 + 1],\n+                                          data_a[ib].qs[iqs * 4 + 2],\n+                                          data_a[ib].qs[iqs * 4 + 3]));\n+\n+    return i32vec2( quants       & 0x0F0F0F0F,\n+                   (quants >> 4) & 0x0F0F0F0F);\n+}\n+\n+ACC_TYPE mul_q8_1(const int32_t q_sum, const float da, const vec2 dsb, const int32_t sum_divisor) {\n+    return ACC_TYPE(da * dsb.x * float(q_sum));\n+}\n+\n+#ifdef MMQ_SHMEM\n+void block_a_to_shmem(const uint buf_ib, const uint ib, const uint iqs) {\n+    const uint32_t qs = pack32(u8vec4(data_a[ib].qs[iqs * 4    ],\n+                                      data_a[ib].qs[iqs * 4 + 1],\n+                                      data_a[ib].qs[iqs * 4 + 2],\n+                                      data_a[ib].qs[iqs * 4 + 3]));\n+\n+    const u8vec4 i_a0 = unpack8( qs       & 0x0F0F0F0F);\n+    const u8vec4 i_a1 = unpack8((qs >> 4) & 0x0F0F0F0F);\n+\n+    buf_a[buf_ib].qs[iqs    ] = pack32(i8vec4(kvalues_mxfp4[i_a0.x], kvalues_mxfp4[i_a0.y], kvalues_mxfp4[i_a0.z], kvalues_mxfp4[i_a0.w]));\n+    buf_a[buf_ib].qs[iqs + 4] = pack32(i8vec4(kvalues_mxfp4[i_a1.x], kvalues_mxfp4[i_a1.y], kvalues_mxfp4[i_a1.z], kvalues_mxfp4[i_a1.w]));\n+\n+    if (iqs == 0) {\n+        buf_a[buf_ib].d = FLOAT_TYPE(e8m0_to_fp32(data_a[ib].e) * 0.5);\n+    }\n+}\n+\n+void block_a_to_registers(const uint reg_ib, const uint buf_ib) {\n+    cache_a[reg_ib].d = buf_a[buf_ib].d;\n+\n+    [[unroll]] for (uint iqs = 0; iqs < 8; iqs++) {\n+        cache_a[reg_ib].qs[iqs] = buf_a[buf_ib].qs[iqs];\n+    }\n+}\n+\n+ACC_TYPE mmq_dot_product(const uint ib_a) {\n+    int32_t q_sum = 0;\n+    [[unroll]] for (uint iqs = 0; iqs < 8; iqs++) {\n+        const int32_t qs_a = cache_a[ib_a].qs[iqs];\n+\n+        q_sum += dotPacked4x8EXT(qs_a, cache_b.qs[iqs]);\n+    }\n+\n+    return mul_q8_1(q_sum, cache_a[ib_a].d, cache_b.ds, 1);\n+}\n+#endif // MMQ_SHMEM\n+#endif\n+\n+// For k-quants, ib and iqs still assume 32-wide blocks, but k-quants are 256-wide\n+// iqs still refers to a 32-bit integer, meaning 0..7 for 32-wide quants\n+#if defined(DATA_A_Q2_K)\n+// 4-byte loads for Q2_K blocks (84 bytes)\n+int32_t repack(uint ib, uint iqs) {\n+    const uint ib_k = ib / 8;\n+    const uint iqs_k = (ib % 8) * 8 + iqs;\n+\n+    const uint qs_idx = (iqs_k / 32) * 8 + (iqs_k % 8);\n+    const uint qs_shift = ((iqs_k % 32) / 8) * 2;\n+\n+    return int32_t((data_a_packed32[ib_k].qs[qs_idx] >> qs_shift) & 0x03030303);\n+}\n+\n+uint8_t get_scale(uint ib, uint iqs) {\n+    const uint ib_k = ib / 8;\n+    const uint iqs_k = (ib % 8) * 8 + iqs;\n+\n+    return data_a[ib_k].scales[iqs_k / 4];\n+}\n+\n+ACC_TYPE mul_q8_1(const int32_t sum_d, const int32_t sum_m, const vec2 dma, const vec2 dsb, const int32_t sum_divisor) {\n+    return ACC_TYPE(dsb.x * (dma.x * float(sum_d) - dma.y * float(sum_m)));\n+}\n+\n+#ifdef MMQ_SHMEM\n+void block_a_to_shmem(const uint buf_ib, const uint ib, const uint iqs) {\n+    const uint ib_k = ib / 8;\n+    const uint iqs_k = (ib % 8) * 8 + iqs * QUANT_R_MMQ;\n+\n+    const uint qs_idx = (iqs_k / 32) * 8 + (iqs_k % 8);\n+    const uint qs_shift = ((iqs_k % 32) / 8) * 2;\n+\n+    // Repack 4x4 quants into one int\n+    const uint32_t vals0 = (data_a_packed32[ib_k].qs[qs_idx    ] >> qs_shift) & 0x03030303;\n+    const uint32_t vals1 = (data_a_packed32[ib_k].qs[qs_idx + 1] >> qs_shift) & 0x03030303;\n+    const uint32_t vals2 = (data_a_packed32[ib_k].qs[qs_idx + 2] >> qs_shift) & 0x03030303;\n+    const uint32_t vals3 = (data_a_packed32[ib_k].qs[qs_idx + 3] >> qs_shift) & 0x03030303;\n+\n+    buf_a[buf_ib].qs[iqs] = vals0 | (vals1 << 2) | (vals2 << 4) | (vals3 << 6);\n+\n+    if (iqs == 0) {\n+        buf_a[buf_ib].dm = FLOAT_TYPE_VEC2(data_a_packed32[ib_k].dm);\n+        buf_a[buf_ib].scales = unpack8(data_a_packed16[ib_k].scales[iqs_k / 8]);\n+    }\n+}\n+\n+void block_a_to_registers(const uint reg_ib, const uint buf_ib) {\n+    cache_a[reg_ib].dm = buf_a[buf_ib].dm;\n+    cache_a[reg_ib].scales = buf_a[buf_ib].scales;\n+\n+    [[unroll]] for (uint iqs = 0; iqs < 2; iqs++) {\n+        cache_a[reg_ib].qs[iqs] = buf_a[buf_ib].qs[iqs];\n+    }\n+}\n+\n+ACC_TYPE mmq_dot_product(const uint ib_a) {\n+    int32_t sum_d = 0;\n+    int32_t sum_m = 0;\n+\n+    [[unroll]] for (uint iqs = 0; iqs < 8; iqs++) {\n+        const uint8_t scale = cache_a[ib_a].scales[iqs / 4];\n+        const int32_t scale_m = int32_t(scale >> 4) * 0x01010101; // Duplicate 8-bit value across 32-bits.\n+        const int32_t qs_a = int32_t((cache_a[ib_a].qs[iqs / 4] >> ((iqs % 4) * 2)) & 0x03030303);\n+\n+        sum_d += dotPacked4x8EXT(qs_a, cache_b.qs[iqs]) * (scale & 0xF);\n+        sum_m += dotPacked4x8EXT(scale_m, cache_b.qs[iqs]);\n+    }\n+\n+    return mul_q8_1(sum_d, sum_m, cache_a[ib_a].dm, cache_b.ds, 1);\n+}\n+#endif // MMQ_SHMEM\n+#endif\n+\n+#if defined(DATA_A_Q3_K)\n+// 2-byte loads for Q3_K blocks (110 bytes)\n+#ifdef MMQ_SHMEM\n+void block_a_to_shmem(const uint buf_ib, const uint ib, const uint iqs) {\n+    const uint ib_k = ib / 8;\n+    const uint hm_idx = iqs * QUANT_R_MMQ;\n+    const uint iqs_k = (ib % 8) * 8 + hm_idx;\n+\n+    const uint qs_idx = (iqs_k / 32) * 8 + (iqs_k % 8);\n+    const uint qs_shift = ((iqs_k % 32) / 8) * 2;\n+    const uint hm_shift = iqs_k / 8;\n+\n+    // Repack 2x4 quants into one int\n+    // Add the 3rd bit instead of subtracting it to allow packing the quants\n+    const i8vec2 vals00 = unpack8(int16_t((data_a_packed16[ib_k].qs[qs_idx * 2        ] >> qs_shift) & uint16_t(0x0303))) |\n+                          unpack8(int16_t(((data_a_packed16[ib_k].hmask[hm_idx * 2    ] >> hm_shift) & uint16_t(0x0101)) << 2));\n+    const i8vec2 vals01 = unpack8(int16_t((data_a_packed16[ib_k].qs[qs_idx * 2 + 1    ] >> qs_shift) & uint16_t(0x0303))) |\n+                          unpack8(int16_t(((data_a_packed16[ib_k].hmask[hm_idx * 2 + 1] >> hm_shift) & uint16_t(0x0101)) << 2));\n+    const i8vec2 vals10 = unpack8(int16_t((data_a_packed16[ib_k].qs[qs_idx * 2 + 2    ] >> qs_shift) & uint16_t(0x0303))) |\n+                          unpack8(int16_t(((data_a_packed16[ib_k].hmask[hm_idx * 2 + 2] >> hm_shift) & uint16_t(0x0101)) << 2));\n+    const i8vec2 vals11 = unpack8(int16_t((data_a_packed16[ib_k].qs[qs_idx * 2 + 3    ] >> qs_shift) & uint16_t(0x0303))) |\n+                          unpack8(int16_t(((data_a_packed16[ib_k].hmask[hm_idx * 2 + 3] >> hm_shift) & uint16_t(0x0101)) << 2));\n+    buf_a[buf_ib].qs[iqs] = pack32(u8vec4(vals00.x, vals00.y, vals01.x, vals01.y)) |\n+                           (pack32(u8vec4(vals10.x, vals10.y, vals11.x, vals11.y)) << 4);\n+\n+    if (iqs == 0) {\n+        const uint is = iqs_k / 4;\n+        const i8vec2 scales = i8vec2(unpack8(((data_a_packed16[ib_k].scales[(is % 8      ) / 2] >> (4 * (is / 8))) & 0x0F0F) |\n+                                            (((data_a_packed16[ib_k].scales[(8 + (is % 4)) / 2] >> (2 * (is / 4))) & 0x0303) << 4)));\n+\n+        buf_a[buf_ib].d_scales = FLOAT_TYPE(data_a_packed16[ib_k].d) * FLOAT_TYPE_VEC2(scales - 32);\n+    }\n+}\n+\n+void block_a_to_registers(const uint reg_ib, const uint buf_ib) {\n+    cache_a[reg_ib].d_scales = buf_a[buf_ib].d_scales;\n+\n+    [[unroll]] for (uint iqs = 0; iqs < 4; iqs++) {\n+        cache_a[reg_ib].qs[iqs] = buf_a[buf_ib].qs[iqs];\n+    }\n+}\n+\n+ACC_TYPE mmq_dot_product(const uint ib_a) {\n+    float result = 0.0;\n+    int32_t q_sum = 0;\n+\n+    [[unroll]] for (uint iqs = 0; iqs < 4; iqs++) {\n+        // Subtract 4 from the quants to correct the 3rd bit offset\n+        const int32_t qs_a = pack32(unpack8(int32_t((cache_a[ib_a].qs[iqs / 2] >> ((iqs % 2) * 4)) & 0x0F0F0F0F)) - int8_t(4));\n+\n+        q_sum += dotPacked4x8EXT(qs_a, cache_b.qs[iqs]);\n+    }\n+    result += float(cache_a[ib_a].d_scales[0]) * float(q_sum);\n+    q_sum = 0;\n+\n+    [[unroll]] for (uint iqs = 4; iqs < 8; iqs++) {\n+        const int32_t qs_a = pack32(unpack8(int32_t((cache_a[ib_a].qs[iqs / 2] >> ((iqs % 2) * 4)) & 0x0F0F0F0F)) - int8_t(4));\n+\n+        q_sum += dotPacked4x8EXT(qs_a, cache_b.qs[iqs]);\n+    }\n+    result += float(cache_a[ib_a].d_scales[1]) * float(q_sum);\n+\n+    return ACC_TYPE(cache_b.ds.x * result);\n+}\n+#endif // MMQ_SHMEM\n+#endif\n+\n+#if defined(DATA_A_Q4_K) || defined(DATA_A_Q5_K)\n+// 4-byte loads for Q4_K blocks (144 bytes) and Q5_K blocks (176 bytes)\n+ACC_TYPE mul_q8_1(const int32_t q_sum, const vec2 dma, const vec2 dsb, const int32_t sum_divisor) {\n+    return ACC_TYPE(dsb.x * dma.x * float(q_sum) - dma.y * dsb.y);\n+}\n+\n+#ifdef MMQ_SHMEM\n+void block_a_to_shmem(const uint buf_ib, const uint ib, const uint iqs) {\n+    const uint ib_k = ib / 8;\n+    const uint iqs_k = (ib % 8) * 8 + iqs * QUANT_R_MMQ;\n+\n+    const uint qs_idx = (iqs_k / 16) * 8 + (iqs_k % 8);\n+    const uint qs_shift = ((iqs_k % 16) / 8) * 4;\n+\n+    // Repack 2x4 quants into one int\n+#if defined(DATA_A_Q4_K)\n+    const uint32_t vals0 = (data_a_packed32[ib_k].qs[qs_idx    ] >> qs_shift) & 0x0F0F0F0F;\n+    const uint32_t vals1 = (data_a_packed32[ib_k].qs[qs_idx + 1] >> qs_shift) & 0x0F0F0F0F;\n+\n+    buf_a[buf_ib].qs[iqs] = vals0 | (vals1 << 4);\n+#else // defined(DATA_A_Q5_K)\n+    const uint qh_idx = iqs * QUANT_R_MMQ;\n+    const uint qh_shift = iqs_k / 8;\n+\n+    buf_a[buf_ib].qs[iqs] = int32_t(((data_a_packed32[ib_k].qs[qs_idx] >> qs_shift) & 0x0F0F0F0F) |\n+                                   (((data_a_packed32[ib_k].qh[qh_idx] >> qh_shift) & 0x01010101) << 4));\n+#endif\n+\n+\n+    if (iqs == 0) {\n+        // Scale index\n+        const uint is = iqs_k / 8;\n+        u8vec2 scale_dm;\n+        if (is < 4) {\n+            scale_dm = u8vec2(data_a[ib_k].scales[is] & 0x3F, data_a[ib_k].scales[is + 4] & 0x3F);\n+        } else {\n+            scale_dm = u8vec2((data_a[ib_k].scales[is+4] & 0xF) | ((data_a[ib_k].scales[is-4] & 0xC0) >> 2),\n+                              (data_a[ib_k].scales[is+4] >>  4) | ((data_a[ib_k].scales[is  ] & 0xC0) >> 2));\n+        }\n+\n+        buf_a[buf_ib].dm = FLOAT_TYPE_VEC2(data_a_packed32[ib_k].dm) * FLOAT_TYPE_VEC2(scale_dm);\n+    }\n+}\n+\n+void block_a_to_registers(const uint reg_ib, const uint buf_ib) {\n+    cache_a[reg_ib].dm = buf_a[buf_ib].dm;\n+\n+    [[unroll]] for (uint iqs = 0; iqs < 8 / QUANT_R_MMQ; iqs++) {\n+        cache_a[reg_ib].qs[iqs] = buf_a[buf_ib].qs[iqs];\n+    }\n+}\n+\n+ACC_TYPE mmq_dot_product(const uint ib_a) {\n+    int32_t q_sum = 0;\n+\n+    [[unroll]] for (uint iqs = 0; iqs < 8; iqs++) {\n+#if defined(DATA_A_Q4_K)\n+        const int32_t qs_a = int32_t((cache_a[ib_a].qs[iqs / 2] >> ((iqs % 2) * 4)) & 0x0F0F0F0F);\n+#else // defined(DATA_A_Q5_K)\n+        const int32_t qs_a = cache_a[ib_a].qs[iqs];\n+#endif\n+\n+        q_sum += dotPacked4x8EXT(qs_a, cache_b.qs[iqs]);\n+    }\n+\n+    return mul_q8_1(q_sum, cache_a[ib_a].dm, cache_b.ds, 1);\n+}\n+#endif // MMQ_SHMEM\n+#endif\n+\n+#ifdef MMQ_SHMEM\n+void block_b_to_shmem(const uint buf_ib, const uint ib, const uint iqs) {\n+    const uint ib_outer = ib / 4;\n+    const uint ib_inner = ib % 4;\n+\n+    if (iqs == 0) {\n+        buf_b[buf_ib].ds = FLOAT_TYPE_VEC2(data_b[ib_outer].ds[ib_inner]);\n+    }\n+\n+    const ivec4 values = data_b[ib_outer].qs[ib_inner * 2 + iqs];\n+    buf_b[buf_ib].qs[iqs * 4    ] = values.x;\n+    buf_b[buf_ib].qs[iqs * 4 + 1] = values.y;\n+    buf_b[buf_ib].qs[iqs * 4 + 2] = values.z;\n+    buf_b[buf_ib].qs[iqs * 4 + 3] = values.w;\n+}\n+\n+void block_b_to_registers(const uint ib) {\n+    cache_b.ds = buf_b[ib].ds;\n+    [[unroll]] for (uint iqs = 0; iqs < BK / 4; iqs++) {\n+        cache_b.qs[iqs] = buf_b[ib].qs[iqs];\n+    }\n+}\n+#endif\n+\n+#if defined(DATA_A_Q6_K)\n+// 2-byte loads for Q6_K blocks (210 bytes)\n+#ifdef MMQ_SHMEM\n+void block_a_to_shmem(const uint buf_ib, const uint ib, const uint iqs) {\n+    const uint ib_k = ib / 8;\n+    const uint iqs_k = (ib % 8) * 8 + iqs;\n+\n+    const uint ql_idx = (iqs_k / 32) * 16 + iqs_k % 16;\n+    const uint ql_shift = ((iqs_k % 32) / 16) * 4;\n+\n+    const uint qh_idx = (iqs_k / 32) * 8 + iqs;\n+    const uint qh_shift = ((iqs_k % 32) / 8) * 2;\n+\n+    const i8vec2 vals00 = (unpack8(int16_t((data_a_packed16[ib_k].ql[ql_idx * 2    ] >> ql_shift) & uint16_t(0x0F0F))) |\n+                          unpack8(int16_t(((data_a_packed16[ib_k].qh[qh_idx * 2    ] >> qh_shift) & uint16_t(0x0303)) << 4))) - int8_t(32);\n+    const i8vec2 vals01 = (unpack8(int16_t((data_a_packed16[ib_k].ql[ql_idx * 2 + 1] >> ql_shift) & uint16_t(0x0F0F))) |\n+                          unpack8(int16_t(((data_a_packed16[ib_k].qh[qh_idx * 2 + 1] >> qh_shift) & uint16_t(0x0303)) << 4))) - int8_t(32);\n+    buf_a[buf_ib].qs[iqs] = pack32(i8vec4(vals00.x, vals00.y, vals01.x, vals01.y));\n+\n+    if (iqs == 0) {\n+        const uint is = iqs_k / 4;\n+        const i8vec2 scales = unpack8(data_a_packed16[ib_k].scales[is / 2]);\n+\n+        buf_a[buf_ib].d_scales = FLOAT_TYPE(data_a_packed16[ib_k].d) * FLOAT_TYPE_VEC2(scales);\n+    }\n+}\n+\n+void block_a_to_registers(const uint reg_ib, const uint buf_ib) {\n+    cache_a[reg_ib].d_scales = buf_a[buf_ib].d_scales;\n+\n+    [[unroll]] for (uint iqs = 0; iqs < 8; iqs++) {\n+        cache_a[reg_ib].qs[iqs] = buf_a[buf_ib].qs[iqs];\n+    }\n+}\n+\n+ACC_TYPE mmq_dot_product(const uint ib_a) {\n+    float result = 0.0;\n+    int32_t q_sum = 0;\n+\n+    [[unroll]] for (uint iqs = 0; iqs < 4; iqs++) {\n+        const int32_t qs_a = cache_a[ib_a].qs[iqs];\n+\n+        q_sum += dotPacked4x8EXT(qs_a, cache_b.qs[iqs]);\n+    }\n+    result += float(cache_a[ib_a].d_scales[0]) * float(q_sum);\n+    q_sum = 0;\n+\n+    [[unroll]] for (uint iqs = 4; iqs < 8; iqs++) {\n+        const int32_t qs_a = cache_a[ib_a].qs[iqs];\n+\n+        q_sum += dotPacked4x8EXT(qs_a, cache_b.qs[iqs]);\n+    }\n+    result += float(cache_a[ib_a].d_scales[1]) * float(q_sum);\n+\n+    return ACC_TYPE(cache_b.ds.x * result);\n+}\n+#endif // MMQ_SHMEM\n #endif\n \n #if defined(DATA_A_Q4_0) || defined(DATA_A_Q5_0) || defined(DATA_A_Q8_0) || defined(DATA_A_IQ1_S) || defined(DATA_A_IQ2_XXS) || defined(DATA_A_IQ2_XS) || defined(DATA_A_IQ2_S) || defined(DATA_A_IQ3_XXS) || defined(DATA_A_IQ3_S) || defined(DATA_A_IQ4_XS) || defined(DATA_A_IQ4_NL)\n@@ -103,3 +568,10 @@ FLOAT_TYPE_VEC2 get_dm(uint ib) {\n     return FLOAT_TYPE_VEC2(data_a_packed32[ib].dm);\n }\n #endif\n+\n+#if defined(DATA_A_Q2_K)\n+FLOAT_TYPE_VEC2 get_dm(uint ib) {\n+    const uint ib_k = ib / 8;\n+    return FLOAT_TYPE_VEC2(data_a_packed32[ib_k].dm);\n+}\n+#endif"
      },
      {
        "filename": "ggml/src/ggml-vulkan/vulkan-shaders/mul_mmq_shmem_types.glsl",
        "status": "added",
        "additions": 78,
        "deletions": 0,
        "changes": 78,
        "patch": "@@ -0,0 +1,78 @@\n+#if defined(DATA_A_Q4_0)\n+#define QUANT_R_MMQ 2\n+struct block_a_cache {\n+    uint32_t qs[16/4];\n+    FLOAT_TYPE dm;\n+};\n+#elif defined(DATA_A_Q4_1)\n+#define QUANT_R_MMQ 2\n+struct block_a_cache {\n+    uint32_t qs[16/4];\n+    FLOAT_TYPE_VEC2 dm;\n+};\n+#elif defined(DATA_A_Q5_0)\n+#define QUANT_R_MMQ 2\n+struct block_a_cache {\n+    uint32_t qs[16/4];\n+    uint32_t qh;\n+    FLOAT_TYPE dm;\n+};\n+#elif defined(DATA_A_Q5_1)\n+#define QUANT_R_MMQ 2\n+struct block_a_cache {\n+    uint32_t qs[16/4];\n+    uint32_t qh;\n+    FLOAT_TYPE_VEC2 dm;\n+};\n+#elif defined(DATA_A_Q8_0)\n+#define QUANT_R_MMQ 1\n+// AMD likes 4, Intel likes 1 and Nvidia likes 2\n+#define BK_STEP 1\n+struct block_a_cache {\n+    int32_t qs[32/4];\n+    FLOAT_TYPE dm;\n+};\n+#elif defined(DATA_A_MXFP4)\n+#define QUANT_R_MMQ 2\n+struct block_a_cache {\n+    int32_t qs[8];\n+    FLOAT_TYPE d;\n+};\n+#elif defined(DATA_A_Q2_K)\n+#define QUANT_R_MMQ 4\n+struct block_a_cache {\n+    uint32_t qs[2];\n+    u8vec2 scales;\n+    FLOAT_TYPE_VEC2 dm;\n+};\n+#elif defined(DATA_A_Q3_K)\n+#define QUANT_R_MMQ 2\n+struct block_a_cache {\n+    uint32_t qs[4];\n+    FLOAT_TYPE_VEC2 d_scales;\n+};\n+#elif defined(DATA_A_Q4_K)\n+#define QUANT_R_MMQ 2\n+struct block_a_cache {\n+    uint32_t qs[4];\n+    FLOAT_TYPE_VEC2 dm;\n+};\n+#elif defined(DATA_A_Q5_K)\n+#define QUANT_R_MMQ 1\n+struct block_a_cache {\n+    int32_t qs[8];\n+    FLOAT_TYPE_VEC2 dm;\n+};\n+#elif defined(DATA_A_Q6_K)\n+#define QUANT_R_MMQ 1\n+struct block_a_cache {\n+    int32_t qs[8];\n+    FLOAT_TYPE_VEC2 d_scales;\n+};\n+#endif\n+\n+struct block_b_cache\n+{\n+    int32_t qs[8];\n+    FLOAT_TYPE_VEC2 ds;\n+};"
      },
      {
        "filename": "ggml/src/ggml-vulkan/vulkan-shaders/types.glsl",
        "status": "modified",
        "additions": 33,
        "deletions": 20,
        "changes": 53,
        "patch": "@@ -66,6 +66,7 @@ struct block_q4_0_packed16\n #define QUANT_AUXF 1\n #define A_TYPE block_q4_0\n #define A_TYPE_PACKED16 block_q4_0_packed16\n+#define DATA_A_QUANT_LEGACY\n #endif\n \n #define QUANT_K_Q4_1 32\n@@ -98,6 +99,7 @@ struct block_q4_1_packed32\n #define A_TYPE block_q4_1\n #define A_TYPE_PACKED16 block_q4_1_packed16\n #define A_TYPE_PACKED32 block_q4_1_packed32\n+#define DATA_A_QUANT_LEGACY\n #endif\n \n #define QUANT_K_Q5_0 32\n@@ -123,6 +125,7 @@ struct block_q5_0_packed16\n #define QUANT_AUXF 1\n #define A_TYPE block_q5_0\n #define A_TYPE_PACKED16 block_q5_0_packed16\n+#define DATA_A_QUANT_LEGACY\n #endif\n \n #define QUANT_K_Q5_1 32\n@@ -158,6 +161,7 @@ struct block_q5_1_packed32\n #define A_TYPE block_q5_1\n #define A_TYPE_PACKED16 block_q5_1_packed16\n #define A_TYPE_PACKED32 block_q5_1_packed32\n+#define DATA_A_QUANT_LEGACY\n #endif\n \n #define QUANT_K_Q8_0 32\n@@ -186,6 +190,7 @@ struct block_q8_0_packed32\n #define A_TYPE block_q8_0\n #define A_TYPE_PACKED16 block_q8_0_packed16\n #define A_TYPE_PACKED32 block_q8_0_packed32\n+#define DATA_A_QUANT_LEGACY\n #endif\n \n #define QUANT_K_Q8_1 32\n@@ -226,21 +231,21 @@ struct block_q2_K\n {\n     uint8_t scales[QUANT_K_Q2_K/16];\n     uint8_t qs[QUANT_K_Q2_K/4];\n-    f16vec2 d;\n+    f16vec2 dm;\n };\n \n struct block_q2_K_packed16\n {\n     uint16_t scales[QUANT_K_Q2_K/16/2];\n     uint16_t qs[QUANT_K_Q2_K/4/2];\n-    f16vec2 d;\n+    f16vec2 dm;\n };\n \n struct block_q2_K_packed32\n {\n     uint32_t scales[QUANT_K_Q2_K/16/4];\n     uint32_t qs[QUANT_K_Q2_K/4/4];\n-    f16vec2 d;\n+    f16vec2 dm;\n };\n \n #if defined(DATA_A_Q2_K)\n@@ -249,6 +254,8 @@ struct block_q2_K_packed32\n #define A_TYPE block_q2_K\n #define A_TYPE_PACKED16 block_q2_K_packed16\n #define A_TYPE_PACKED32 block_q2_K_packed32\n+#define SCALES_PER_32 2\n+#define DATA_A_QUANT_K\n #endif\n \n #define QUANT_K_Q3_K 256\n@@ -274,27 +281,28 @@ struct block_q3_K_packed16\n #define QUANT_R 1\n #define A_TYPE block_q3_K\n #define A_TYPE_PACKED16 block_q3_K_packed16\n+#define DATA_A_QUANT_K\n #endif\n \n #define QUANT_K_Q4_K 256\n \n struct block_q4_K\n {\n-    f16vec2 d;\n+    f16vec2 dm;\n     uint8_t scales[3*QUANT_K_Q4_K/64];\n     uint8_t qs[QUANT_K_Q4_K/2];\n };\n \n struct block_q4_K_packed16\n {\n-    f16vec2 d;\n+    f16vec2 dm;\n     uint16_t scales[3*QUANT_K_Q4_K/64/2];\n     uint16_t qs[QUANT_K_Q4_K/2/2];\n };\n \n struct block_q4_K_packed32\n {\n-    f16vec2 d;\n+    f16vec2 dm;\n     uint32_t scales[3*QUANT_K_Q4_K/64/4];\n     uint32_t qs[QUANT_K_Q4_K/2/4];\n };\n@@ -310,26 +318,35 @@ struct block_q4_K_packed128\n #define A_TYPE block_q4_K\n #define A_TYPE_PACKED16 block_q4_K_packed16\n #define A_TYPE_PACKED32 block_q4_K_packed32\n+#define DATA_A_QUANT_K\n #endif\n \n #define QUANT_K_Q5_K 256\n \n struct block_q5_K\n {\n-    f16vec2 d;\n+    f16vec2 dm;\n     uint8_t scales[12];\n     uint8_t qh[QUANT_K_Q5_K/8];\n     uint8_t qs[QUANT_K_Q5_K/2];\n };\n \n struct block_q5_K_packed16\n {\n-    f16vec2 d;\n+    f16vec2 dm;\n     uint16_t scales[12/2];\n     uint16_t qh[QUANT_K_Q5_K/8/2];\n     uint16_t qs[QUANT_K_Q5_K/2/2];\n };\n \n+struct block_q5_K_packed32\n+{\n+    f16vec2 dm;\n+    uint32_t scales[12/4];\n+    uint32_t qh[QUANT_K_Q5_K/8/4];\n+    uint32_t qs[QUANT_K_Q5_K/2/4];\n+};\n+\n struct block_q5_K_packed128\n {\n     uvec4 q5k[11];\n@@ -340,6 +357,8 @@ struct block_q5_K_packed128\n #define QUANT_R 1\n #define A_TYPE block_q5_K\n #define A_TYPE_PACKED16 block_q5_K_packed16\n+#define A_TYPE_PACKED32 block_q5_K_packed32\n+#define DATA_A_QUANT_K\n #endif\n \n #define QUANT_K_Q6_K 256\n@@ -356,7 +375,7 @@ struct block_q6_K_packed16\n {\n     uint16_t ql[QUANT_K_Q6_K/2/2];\n     uint16_t qh[QUANT_K_Q6_K/4/2];\n-    int8_t scales[QUANT_K_Q6_K/16];\n+    int16_t scales[QUANT_K_Q6_K/16/2];\n     float16_t d;\n };\n \n@@ -365,6 +384,7 @@ struct block_q6_K_packed16\n #define QUANT_R 1\n #define A_TYPE block_q6_K\n #define A_TYPE_PACKED16 block_q6_K_packed16\n+#define DATA_A_QUANT_K\n #endif\n \n // IQuants\n@@ -1363,18 +1383,11 @@ struct block_mxfp4\n     uint8_t qs[QUANT_K_MXFP4/2];\n };\n \n-//struct block_mxfp4_packed16\n-//{\n-//    uint8_t e;\n-//    uint16_t qs[QUANT_K_MXFP4/2/2];\n-//};\n-\n #if defined(DATA_A_MXFP4)\n #define QUANT_K QUANT_K_MXFP4\n #define QUANT_R QUANT_R_MXFP4\n #define QUANT_AUXF 1\n #define A_TYPE block_mxfp4\n-//#define A_TYPE_PACKED16 block_mxfp4_packed16\n #endif\n \n #if defined(DATA_A_IQ4_NL) || defined(DATA_A_IQ4_XS)\n@@ -1397,12 +1410,12 @@ void init_iq_shmem(uvec3 wgsize)\n #endif\n \n #if defined(DATA_A_MXFP4)\n-const FLOAT_TYPE kvalues_mxfp4_const[16] = {\n-    FLOAT_TYPE(0.0f), FLOAT_TYPE(0.5f), FLOAT_TYPE(1.0f), FLOAT_TYPE(1.5f), FLOAT_TYPE(2.0f), FLOAT_TYPE(3.0f), FLOAT_TYPE(4.0f), FLOAT_TYPE(6.0f),\n-    FLOAT_TYPE(-0.0f), FLOAT_TYPE(-0.5f), FLOAT_TYPE(-1.0f), FLOAT_TYPE(-1.5f), FLOAT_TYPE(-2.0f), FLOAT_TYPE(-3.0f), FLOAT_TYPE(-4.0f), FLOAT_TYPE(-6.0f)\n+const int8_t kvalues_mxfp4_const[16] = {\n+    int8_t(0), int8_t(1), int8_t(2), int8_t(3), int8_t(4), int8_t(6), int8_t(8), int8_t(12),\n+    int8_t(0), int8_t(-1), int8_t(-2), int8_t(-3), int8_t(-4), int8_t(-6), int8_t(-8), int8_t(-12),\n };\n \n-shared FLOAT_TYPE kvalues_mxfp4[16];\n+shared int8_t kvalues_mxfp4[16];\n \n #define NEEDS_INIT_IQ_SHMEM\n void init_iq_shmem(uvec3 wgsize)"
      },
      {
        "filename": "ggml/src/ggml-vulkan/vulkan-shaders/vulkan-shaders-gen.cpp",
        "status": "modified",
        "additions": 3,
        "deletions": 2,
        "changes": 5,
        "patch": "@@ -566,15 +566,16 @@ void matmul_shaders(bool fp16, MatMulIdType matmul_id_type, bool coopmat, bool c\n         }\n \n #if defined(GGML_VULKAN_INTEGER_DOT_GLSLC_SUPPORT)\n-        if (!coopmat && !coopmat2 && matmul_id_type == MatMulIdType::NONE && is_legacy_quant(tname)) {\n+        // Integer dot mmq performs better with f32 accumulators\n+        if (!f16acc && !coopmat && !coopmat2 && (is_legacy_quant(tname) || is_k_quant(tname) || tname == \"mxfp4\")) {\n             string_to_spv(shader_name + \"_\" + tname + \"_q8_1\", \"mul_mmq.comp\", merge_maps(merge_maps(base_dict, float_type_dict), {{data_a_key, \"1\"}, {\"D_TYPE\", \"float\"},}), fp16, coopmat, coopmat2, f16acc);\n         }\n #endif\n     }\n }\n \n void process_shaders() {\n-    std::map<std::string, std::string> base_dict = {{\"FLOAT_TYPE\", \"float\"}};\n+    std::map<std::string, std::string> base_dict = {{\"FLOAT_TYPE\", \"float\"}, {\"FLOAT_TYPE_VEC2\", \"vec2\"}};\n \n     // matmul\n     for (const MatMulIdType& matmul_id_type : {MatMulIdType::NONE, MatMulIdType::DEFAULT, MatMulIdType::SUBGROUP}) {"
      }
    ],
    "num_files": 18,
    "scraped_at": "2025-11-16T23:30:05.974576"
  },
  {
    "pr_number": 16531,
    "title": "metal : FA support F32 K and V",
    "body": "target #16528 \r\n\r\n- Add Metal FA kernels for F32 K and V\r\n- Add Metal FA kernels for head size 32\r\n- ~Remove K and V casts with cacheless contexts~ (we should keep the casts for now)\r\n\r\nSample command for testing:\r\n\r\n```bash\r\nllama-embedding -hf ggml-org/bge-small-en-v1.5-Q8_0-GGUF -e -p \"$(printf 'hello %.0s' {1..510})\" --pooling cls -c 512 -fa on\r\n```",
    "html_url": "https://github.com/ggml-org/llama.cpp/pull/16531",
    "created_at": "2025-10-12T09:36:31Z",
    "merged_at": "2025-10-13T20:07:57Z",
    "merge_commit_sha": "e60f241eacec42d3bd7c9edd37d236ebf35132a8",
    "base_ref": "master",
    "head_sha": "c6ea64e652e3887446dd5d85f8019ad3ae320143",
    "user": "ggerganov",
    "files": [
      {
        "filename": "ggml/src/ggml-metal/ggml-metal-device.m",
        "status": "modified",
        "additions": 2,
        "deletions": 1,
        "changes": 3,
        "patch": "@@ -693,7 +693,8 @@ bool ggml_metal_device_supports_op(ggml_metal_device_t dev, const struct ggml_te\n             return true;\n         case GGML_OP_FLASH_ATTN_EXT:\n             // for new head sizes, add checks here\n-            if (op->src[0]->ne[0] != 40 &&\n+            if (op->src[0]->ne[0] != 32 &&\n+                op->src[0]->ne[0] != 40 &&\n                 op->src[0]->ne[0] != 64 &&\n                 op->src[0]->ne[0] != 80 &&\n                 op->src[0]->ne[0] != 96 &&"
      },
      {
        "filename": "ggml/src/ggml-metal/ggml-metal.metal",
        "status": "modified",
        "additions": 109,
        "deletions": 55,
        "changes": 164,
        "patch": "@@ -5213,8 +5213,30 @@ kernel void kernel_flash_attn_ext(\n     half,   half4,     simdgroup_half8x8\n     //float,  float4,    simdgroup_float8x8\n \n+#define FA_TYPES_F32 \\\n+    half,   half4,     simdgroup_half8x8,  \\\n+    float,  float4x4,  simdgroup_float8x8, \\\n+    float,  float4x4,  simdgroup_float8x8, \\\n+    float,             simdgroup_float8x8, \\\n+    float,  float2,    simdgroup_float8x8, \\\n+    float,  float4,    simdgroup_float8x8\n+    //half,   half4,     simdgroup_half8x8\n+\n typedef decltype(kernel_flash_attn_ext<FA_TYPES, half4x4, 1, dequantize_f16, half4x4, 1, dequantize_f16, 64, 64>) flash_attn_ext_t;\n \n+template [[host_name(\"kernel_flash_attn_ext_f32_dk32_dv32\"  )]]  kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES_F32, float4x4,   1, dequantize_f32,  float4x4,   1, dequantize_f32,  32,  32>;\n+template [[host_name(\"kernel_flash_attn_ext_f32_dk40_dv40\"  )]]  kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES_F32, float4x4,   1, dequantize_f32,  float4x4,   1, dequantize_f32,  40,  40>;\n+template [[host_name(\"kernel_flash_attn_ext_f32_dk64_dv64\"  )]]  kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES_F32, float4x4,   1, dequantize_f32,  float4x4,   1, dequantize_f32,  64,  64>;\n+template [[host_name(\"kernel_flash_attn_ext_f32_dk80_dv80\"  )]]  kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES_F32, float4x4,   1, dequantize_f32,  float4x4,   1, dequantize_f32,  80,  80>;\n+template [[host_name(\"kernel_flash_attn_ext_f32_dk96_dv96\"  )]]  kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES_F32, float4x4,   1, dequantize_f32,  float4x4,   1, dequantize_f32,  96,  96>;\n+template [[host_name(\"kernel_flash_attn_ext_f32_dk112_dv112\")]]  kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES_F32, float4x4,   1, dequantize_f32,  float4x4,   1, dequantize_f32,  112, 112>;\n+template [[host_name(\"kernel_flash_attn_ext_f32_dk128_dv128\")]]  kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES_F32, float4x4,   1, dequantize_f32,  float4x4,   1, dequantize_f32,  128, 128>;\n+template [[host_name(\"kernel_flash_attn_ext_f32_dk192_dv192\")]]  kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES_F32, float4x4,   1, dequantize_f32,  float4x4,   1, dequantize_f32,  192, 192>;\n+template [[host_name(\"kernel_flash_attn_ext_f32_dk192_dv128\")]]  kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES_F32, float4x4,   1, dequantize_f32,  float4x4,   1, dequantize_f32,  192, 128>;\n+template [[host_name(\"kernel_flash_attn_ext_f32_dk256_dv256\")]]  kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES_F32, float4x4,   1, dequantize_f32,  float4x4,   1, dequantize_f32,  256, 256>;\n+template [[host_name(\"kernel_flash_attn_ext_f32_dk576_dv512\")]]  kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES_F32, float4x4,   1, dequantize_f32,  float4x4,   1, dequantize_f32,  576, 512>;\n+\n+template [[host_name(\"kernel_flash_attn_ext_f16_dk32_dv32\"  )]]  kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    half4x4,    1, dequantize_f16,  half4x4,    1, dequantize_f16,  32,  32>;\n template [[host_name(\"kernel_flash_attn_ext_f16_dk40_dv40\"  )]]  kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    half4x4,    1, dequantize_f16,  half4x4,    1, dequantize_f16,  40,  40>;\n template [[host_name(\"kernel_flash_attn_ext_f16_dk64_dv64\"  )]]  kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    half4x4,    1, dequantize_f16,  half4x4,    1, dequantize_f16,  64,  64>;\n template [[host_name(\"kernel_flash_attn_ext_f16_dk80_dv80\"  )]]  kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    half4x4,    1, dequantize_f16,  half4x4,    1, dequantize_f16,  80,  80>;\n@@ -5227,6 +5249,7 @@ template [[host_name(\"kernel_flash_attn_ext_f16_dk256_dv256\")]]  kernel flash_at\n template [[host_name(\"kernel_flash_attn_ext_f16_dk576_dv512\")]]  kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    half4x4,    1, dequantize_f16,  half4x4,    1, dequantize_f16,  576, 512>;\n \n #if defined(GGML_METAL_HAS_BF16)\n+template [[host_name(\"kernel_flash_attn_ext_bf16_dk32_dv32\"  )]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES_BF, bfloat4x4,  1, dequantize_bf16, bfloat4x4,  1, dequantize_bf16, 32,  32>;\n template [[host_name(\"kernel_flash_attn_ext_bf16_dk40_dv40\"  )]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES_BF, bfloat4x4,  1, dequantize_bf16, bfloat4x4,  1, dequantize_bf16, 40,  40>;\n template [[host_name(\"kernel_flash_attn_ext_bf16_dk64_dv64\"  )]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES_BF, bfloat4x4,  1, dequantize_bf16, bfloat4x4,  1, dequantize_bf16, 64,  64>;\n template [[host_name(\"kernel_flash_attn_ext_bf16_dk80_dv80\"  )]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES_BF, bfloat4x4,  1, dequantize_bf16, bfloat4x4,  1, dequantize_bf16, 80,  80>;\n@@ -5239,6 +5262,7 @@ template [[host_name(\"kernel_flash_attn_ext_bf16_dk256_dv256\")]] kernel flash_at\n template [[host_name(\"kernel_flash_attn_ext_bf16_dk576_dv512\")]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES_BF, bfloat4x4,  1, dequantize_bf16, bfloat4x4,  1, dequantize_bf16, 576, 512>;\n #endif\n \n+template [[host_name(\"kernel_flash_attn_ext_q4_0_dk32_dv32\"  )]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    block_q4_0, 2, dequantize_q4_0, block_q4_0, 2, dequantize_q4_0, 32,  32>;\n template [[host_name(\"kernel_flash_attn_ext_q4_0_dk40_dv40\"  )]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    block_q4_0, 2, dequantize_q4_0, block_q4_0, 2, dequantize_q4_0, 40,  40>;\n template [[host_name(\"kernel_flash_attn_ext_q4_0_dk64_dv64\"  )]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    block_q4_0, 2, dequantize_q4_0, block_q4_0, 2, dequantize_q4_0, 64,  64>;\n template [[host_name(\"kernel_flash_attn_ext_q4_0_dk80_dv80\"  )]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    block_q4_0, 2, dequantize_q4_0, block_q4_0, 2, dequantize_q4_0, 80,  80>;\n@@ -5250,6 +5274,7 @@ template [[host_name(\"kernel_flash_attn_ext_q4_0_dk192_dv128\")]] kernel flash_at\n template [[host_name(\"kernel_flash_attn_ext_q4_0_dk256_dv256\")]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    block_q4_0, 2, dequantize_q4_0, block_q4_0, 2, dequantize_q4_0, 256, 256>;\n template [[host_name(\"kernel_flash_attn_ext_q4_0_dk576_dv512\")]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    block_q4_0, 2, dequantize_q4_0, block_q4_0, 2, dequantize_q4_0, 576, 512>;\n \n+template [[host_name(\"kernel_flash_attn_ext_q4_1_dk32_dv32\"  )]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    block_q4_1, 2, dequantize_q4_1, block_q4_1, 2, dequantize_q4_1, 32,  32>;\n template [[host_name(\"kernel_flash_attn_ext_q4_1_dk40_dv40\"  )]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    block_q4_1, 2, dequantize_q4_1, block_q4_1, 2, dequantize_q4_1, 40,  40>;\n template [[host_name(\"kernel_flash_attn_ext_q4_1_dk64_dv64\"  )]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    block_q4_1, 2, dequantize_q4_1, block_q4_1, 2, dequantize_q4_1, 64,  64>;\n template [[host_name(\"kernel_flash_attn_ext_q4_1_dk80_dv80\"  )]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    block_q4_1, 2, dequantize_q4_1, block_q4_1, 2, dequantize_q4_1, 80,  80>;\n@@ -5261,6 +5286,7 @@ template [[host_name(\"kernel_flash_attn_ext_q4_1_dk192_dv128\")]] kernel flash_at\n template [[host_name(\"kernel_flash_attn_ext_q4_1_dk256_dv256\")]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    block_q4_1, 2, dequantize_q4_1, block_q4_1, 2, dequantize_q4_1, 256, 256>;\n template [[host_name(\"kernel_flash_attn_ext_q4_1_dk576_dv512\")]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    block_q4_1, 2, dequantize_q4_1, block_q4_1, 2, dequantize_q4_1, 576, 512>;\n \n+template [[host_name(\"kernel_flash_attn_ext_q5_0_dk32_dv32\"  )]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    block_q5_0, 2, dequantize_q5_0, block_q5_0, 2, dequantize_q5_0, 32,  32>;\n template [[host_name(\"kernel_flash_attn_ext_q5_0_dk40_dv40\"  )]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    block_q5_0, 2, dequantize_q5_0, block_q5_0, 2, dequantize_q5_0, 40,  40>;\n template [[host_name(\"kernel_flash_attn_ext_q5_0_dk64_dv64\"  )]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    block_q5_0, 2, dequantize_q5_0, block_q5_0, 2, dequantize_q5_0, 64,  64>;\n template [[host_name(\"kernel_flash_attn_ext_q5_0_dk80_dv80\"  )]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    block_q5_0, 2, dequantize_q5_0, block_q5_0, 2, dequantize_q5_0, 80,  80>;\n@@ -5272,6 +5298,7 @@ template [[host_name(\"kernel_flash_attn_ext_q5_0_dk192_dv128\")]] kernel flash_at\n template [[host_name(\"kernel_flash_attn_ext_q5_0_dk256_dv256\")]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    block_q5_0, 2, dequantize_q5_0, block_q5_0, 2, dequantize_q5_0, 256, 256>;\n template [[host_name(\"kernel_flash_attn_ext_q5_0_dk576_dv512\")]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    block_q5_0, 2, dequantize_q5_0, block_q5_0, 2, dequantize_q5_0, 576, 512>;\n \n+template [[host_name(\"kernel_flash_attn_ext_q5_1_dk32_dv32\"  )]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    block_q5_1, 2, dequantize_q5_1, block_q5_1, 2, dequantize_q5_1, 32,  32>;\n template [[host_name(\"kernel_flash_attn_ext_q5_1_dk40_dv40\"  )]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    block_q5_1, 2, dequantize_q5_1, block_q5_1, 2, dequantize_q5_1, 40,  40>;\n template [[host_name(\"kernel_flash_attn_ext_q5_1_dk64_dv64\"  )]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    block_q5_1, 2, dequantize_q5_1, block_q5_1, 2, dequantize_q5_1, 64,  64>;\n template [[host_name(\"kernel_flash_attn_ext_q5_1_dk80_dv80\"  )]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    block_q5_1, 2, dequantize_q5_1, block_q5_1, 2, dequantize_q5_1, 80,  80>;\n@@ -5283,6 +5310,7 @@ template [[host_name(\"kernel_flash_attn_ext_q5_1_dk192_dv128\")]] kernel flash_at\n template [[host_name(\"kernel_flash_attn_ext_q5_1_dk256_dv256\")]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    block_q5_1, 2, dequantize_q5_1, block_q5_1, 2, dequantize_q5_1, 256, 256>;\n template [[host_name(\"kernel_flash_attn_ext_q5_1_dk576_dv512\")]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    block_q5_1, 2, dequantize_q5_1, block_q5_1, 2, dequantize_q5_1, 576, 512>;\n \n+template [[host_name(\"kernel_flash_attn_ext_q8_0_dk32_dv32\"  )]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    block_q8_0, 2, dequantize_q8_0, block_q8_0, 2, dequantize_q8_0, 32,  32>;\n template [[host_name(\"kernel_flash_attn_ext_q8_0_dk40_dv40\"  )]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    block_q8_0, 2, dequantize_q8_0, block_q8_0, 2, dequantize_q8_0, 40,  40>;\n template [[host_name(\"kernel_flash_attn_ext_q8_0_dk64_dv64\"  )]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    block_q8_0, 2, dequantize_q8_0, block_q8_0, 2, dequantize_q8_0, 64,  64>;\n template [[host_name(\"kernel_flash_attn_ext_q8_0_dk80_dv80\"  )]] kernel flash_attn_ext_t kernel_flash_attn_ext<FA_TYPES,    block_q8_0, 2, dequantize_q8_0, block_q8_0, 2, dequantize_q8_0, 80,  80>;\n@@ -5818,77 +5846,103 @@ kernel void kernel_flash_attn_ext_vec(\n     float, float4, \\\n            float4\n \n+#define FA_TYPES_F32 \\\n+           half4,  \\\n+           float4, \\\n+           float4, \\\n+    float,         \\\n+    float, float4, \\\n+           float4\n+\n typedef decltype(kernel_flash_attn_ext_vec<FA_TYPES, half4, 1, dequantize_f16_t4, half4, 1, dequantize_f16_t4, 128, 128, 4>) flash_attn_ext_vec_t;\n \n-template [[host_name(\"kernel_flash_attn_ext_vec_f16_dk64_dv64\")]]    kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES, half4,      1, dequantize_f16_t4,  half4,       1, dequantize_f16_t4,  64, 64, 2>;\n+template [[host_name(\"kernel_flash_attn_ext_vec_f32_dk32_dv32\")]]    kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES_F32, float4,     1, dequantize_f32_t4,  float4,      1, dequantize_f32_t4,  32, 32, 4>;\n+template [[host_name(\"kernel_flash_attn_ext_vec_f16_dk32_dv32\")]]    kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES,     half4,      1, dequantize_f16_t4,  half4,       1, dequantize_f16_t4,  32, 32, 4>;\n #if defined(GGML_METAL_HAS_BF16)\n-template [[host_name(\"kernel_flash_attn_ext_vec_bf16_dk64_dv64\")]]   kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES, bfloat4,    1, dequantize_bf16_t4, bfloat4,     1, dequantize_bf16_t4, 64, 64, 2>;\n+template [[host_name(\"kernel_flash_attn_ext_vec_bf16_dk32_dv32\")]]   kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES,     bfloat4,    1, dequantize_bf16_t4, bfloat4,     1, dequantize_bf16_t4, 32, 32, 4>;\n #endif\n-template [[host_name(\"kernel_flash_attn_ext_vec_q4_0_dk64_dv64\")]]   kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES, block_q4_0, 8, dequantize_q4_0_t4, block_q4_0,  8, dequantize_q4_0_t4, 64, 64, 2>;\n-template [[host_name(\"kernel_flash_attn_ext_vec_q4_1_dk64_dv64\")]]   kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES, block_q4_1, 8, dequantize_q4_1_t4, block_q4_1,  8, dequantize_q4_1_t4, 64, 64, 2>;\n-template [[host_name(\"kernel_flash_attn_ext_vec_q5_0_dk64_dv64\")]]   kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES, block_q5_0, 8, dequantize_q5_0_t4, block_q5_0,  8, dequantize_q5_0_t4, 64, 64, 2>;\n-template [[host_name(\"kernel_flash_attn_ext_vec_q5_1_dk64_dv64\")]]   kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES, block_q5_1, 8, dequantize_q5_1_t4, block_q5_1,  8, dequantize_q5_1_t4, 64, 64, 2>;\n-template [[host_name(\"kernel_flash_attn_ext_vec_q8_0_dk64_dv64\")]]   kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES, block_q8_0, 8, dequantize_q8_0_t4, block_q8_0,  8, dequantize_q8_0_t4, 64, 64, 2>;\n-\n-template [[host_name(\"kernel_flash_attn_ext_vec_f16_dk96_dv96\")]]    kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES, half4,      1, dequantize_f16_t4,  half4,       1, dequantize_f16_t4,  96, 96, 4>;\n+template [[host_name(\"kernel_flash_attn_ext_vec_q4_0_dk32_dv32\")]]   kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES,     block_q4_0, 8, dequantize_q4_0_t4, block_q4_0,  8, dequantize_q4_0_t4, 32, 32, 4>;\n+template [[host_name(\"kernel_flash_attn_ext_vec_q4_1_dk32_dv32\")]]   kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES,     block_q4_1, 8, dequantize_q4_1_t4, block_q4_1,  8, dequantize_q4_1_t4, 32, 32, 4>;\n+template [[host_name(\"kernel_flash_attn_ext_vec_q5_0_dk32_dv32\")]]   kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES,     block_q5_0, 8, dequantize_q5_0_t4, block_q5_0,  8, dequantize_q5_0_t4, 32, 32, 4>;\n+template [[host_name(\"kernel_flash_attn_ext_vec_q5_1_dk32_dv32\")]]   kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES,     block_q5_1, 8, dequantize_q5_1_t4, block_q5_1,  8, dequantize_q5_1_t4, 32, 32, 4>;\n+template [[host_name(\"kernel_flash_attn_ext_vec_q8_0_dk32_dv32\")]]   kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES,     block_q8_0, 8, dequantize_q8_0_t4, block_q8_0,  8, dequantize_q8_0_t4, 32, 32, 4>;\n+\n+template [[host_name(\"kernel_flash_attn_ext_vec_f32_dk64_dv64\")]]    kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES_F32, float4,     1, dequantize_f32_t4,  float4,      1, dequantize_f32_t4,  64, 64, 2>;\n+template [[host_name(\"kernel_flash_attn_ext_vec_f16_dk64_dv64\")]]    kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES,     half4,      1, dequantize_f16_t4,  half4,       1, dequantize_f16_t4,  64, 64, 2>;\n #if defined(GGML_METAL_HAS_BF16)\n-template [[host_name(\"kernel_flash_attn_ext_vec_bf16_dk96_dv96\")]]   kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES, bfloat4,    1, dequantize_bf16_t4, bfloat4,     1, dequantize_bf16_t4, 96, 96, 4>;\n+template [[host_name(\"kernel_flash_attn_ext_vec_bf16_dk64_dv64\")]]   kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES,     bfloat4,    1, dequantize_bf16_t4, bfloat4,     1, dequantize_bf16_t4, 64, 64, 2>;\n #endif\n-template [[host_name(\"kernel_flash_attn_ext_vec_q4_0_dk96_dv96\")]]   kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES, block_q4_0, 8, dequantize_q4_0_t4, block_q4_0,  8, dequantize_q4_0_t4, 96, 96, 4>;\n-template [[host_name(\"kernel_flash_attn_ext_vec_q4_1_dk96_dv96\")]]   kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES, block_q4_1, 8, dequantize_q4_1_t4, block_q4_1,  8, dequantize_q4_1_t4, 96, 96, 4>;\n-template [[host_name(\"kernel_flash_attn_ext_vec_q5_0_dk96_dv96\")]]   kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES, block_q5_0, 8, dequantize_q5_0_t4, block_q5_0,  8, dequantize_q5_0_t4, 96, 96, 4>;\n-template [[host_name(\"kernel_flash_attn_ext_vec_q5_1_dk96_dv96\")]]   kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES, block_q5_1, 8, dequantize_q5_1_t4, block_q5_1,  8, dequantize_q5_1_t4, 96, 96, 4>;\n-template [[host_name(\"kernel_flash_attn_ext_vec_q8_0_dk96_dv96\")]]   kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES, block_q8_0, 8, dequantize_q8_0_t4, block_q8_0,  8, dequantize_q8_0_t4, 96, 96, 4>;\n-\n-template [[host_name(\"kernel_flash_attn_ext_vec_f16_dk128_dv128\")]]  kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES, half4,      1, dequantize_f16_t4,  half4,       1, dequantize_f16_t4,  128, 128, 1>;\n+template [[host_name(\"kernel_flash_attn_ext_vec_q4_0_dk64_dv64\")]]   kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES,     block_q4_0, 8, dequantize_q4_0_t4, block_q4_0,  8, dequantize_q4_0_t4, 64, 64, 2>;\n+template [[host_name(\"kernel_flash_attn_ext_vec_q4_1_dk64_dv64\")]]   kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES,     block_q4_1, 8, dequantize_q4_1_t4, block_q4_1,  8, dequantize_q4_1_t4, 64, 64, 2>;\n+template [[host_name(\"kernel_flash_attn_ext_vec_q5_0_dk64_dv64\")]]   kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES,     block_q5_0, 8, dequantize_q5_0_t4, block_q5_0,  8, dequantize_q5_0_t4, 64, 64, 2>;\n+template [[host_name(\"kernel_flash_attn_ext_vec_q5_1_dk64_dv64\")]]   kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES,     block_q5_1, 8, dequantize_q5_1_t4, block_q5_1,  8, dequantize_q5_1_t4, 64, 64, 2>;\n+template [[host_name(\"kernel_flash_attn_ext_vec_q8_0_dk64_dv64\")]]   kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES,     block_q8_0, 8, dequantize_q8_0_t4, block_q8_0,  8, dequantize_q8_0_t4, 64, 64, 2>;\n+\n+template [[host_name(\"kernel_flash_attn_ext_vec_f32_dk96_dv96\")]]    kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES_F32, float4,     1, dequantize_f32_t4,  float4,      1, dequantize_f32_t4,  96, 96, 4>;\n+template [[host_name(\"kernel_flash_attn_ext_vec_f16_dk96_dv96\")]]    kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES,     half4,      1, dequantize_f16_t4,  half4,       1, dequantize_f16_t4,  96, 96, 4>;\n #if defined(GGML_METAL_HAS_BF16)\n-template [[host_name(\"kernel_flash_attn_ext_vec_bf16_dk128_dv128\")]] kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES, bfloat4,    1, dequantize_bf16_t4, bfloat4,     1, dequantize_bf16_t4, 128, 128, 1>;\n+template [[host_name(\"kernel_flash_attn_ext_vec_bf16_dk96_dv96\")]]   kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES,     bfloat4,    1, dequantize_bf16_t4, bfloat4,     1, dequantize_bf16_t4, 96, 96, 4>;\n #endif\n-template [[host_name(\"kernel_flash_attn_ext_vec_q4_0_dk128_dv128\")]] kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES, block_q4_0, 8, dequantize_q4_0_t4, block_q4_0,  8, dequantize_q4_0_t4, 128, 128, 1>;\n-template [[host_name(\"kernel_flash_attn_ext_vec_q4_1_dk128_dv128\")]] kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES, block_q4_1, 8, dequantize_q4_1_t4, block_q4_1,  8, dequantize_q4_1_t4, 128, 128, 1>;\n-template [[host_name(\"kernel_flash_attn_ext_vec_q5_0_dk128_dv128\")]] kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES, block_q5_0, 8, dequantize_q5_0_t4, block_q5_0,  8, dequantize_q5_0_t4, 128, 128, 1>;\n-template [[host_name(\"kernel_flash_attn_ext_vec_q5_1_dk128_dv128\")]] kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES, block_q5_1, 8, dequantize_q5_1_t4, block_q5_1,  8, dequantize_q5_1_t4, 128, 128, 1>;\n-template [[host_name(\"kernel_flash_attn_ext_vec_q8_0_dk128_dv128\")]] kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES, block_q8_0, 8, dequantize_q8_0_t4, block_q8_0,  8, dequantize_q8_0_t4, 128, 128, 1>;\n-\n-template [[host_name(\"kernel_flash_attn_ext_vec_f16_dk192_dv192\")]]  kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES, half4,      1, dequantize_f16_t4,  half4,       1, dequantize_f16_t4,  192, 192, 2>;\n+template [[host_name(\"kernel_flash_attn_ext_vec_q4_0_dk96_dv96\")]]   kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES,     block_q4_0, 8, dequantize_q4_0_t4, block_q4_0,  8, dequantize_q4_0_t4, 96, 96, 4>;\n+template [[host_name(\"kernel_flash_attn_ext_vec_q4_1_dk96_dv96\")]]   kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES,     block_q4_1, 8, dequantize_q4_1_t4, block_q4_1,  8, dequantize_q4_1_t4, 96, 96, 4>;\n+template [[host_name(\"kernel_flash_attn_ext_vec_q5_0_dk96_dv96\")]]   kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES,     block_q5_0, 8, dequantize_q5_0_t4, block_q5_0,  8, dequantize_q5_0_t4, 96, 96, 4>;\n+template [[host_name(\"kernel_flash_attn_ext_vec_q5_1_dk96_dv96\")]]   kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES,     block_q5_1, 8, dequantize_q5_1_t4, block_q5_1,  8, dequantize_q5_1_t4, 96, 96, 4>;\n+template [[host_name(\"kernel_flash_attn_ext_vec_q8_0_dk96_dv96\")]]   kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES,     block_q8_0, 8, dequantize_q8_0_t4, block_q8_0,  8, dequantize_q8_0_t4, 96, 96, 4>;\n+\n+template [[host_name(\"kernel_flash_attn_ext_vec_f32_dk128_dv128\")]]  kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES_F32, float4,     1, dequantize_f32_t4,  float4,      1, dequantize_f32_t4,  128, 128, 1>;\n+template [[host_name(\"kernel_flash_attn_ext_vec_f16_dk128_dv128\")]]  kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES,     half4,      1, dequantize_f16_t4,  half4,       1, dequantize_f16_t4,  128, 128, 1>;\n #if defined(GGML_METAL_HAS_BF16)\n-template [[host_name(\"kernel_flash_attn_ext_vec_bf16_dk192_dv192\")]] kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES, bfloat4,    1, dequantize_bf16_t4, bfloat4,     1, dequantize_bf16_t4, 192, 192, 2>;\n+template [[host_name(\"kernel_flash_attn_ext_vec_bf16_dk128_dv128\")]] kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES,     bfloat4,    1, dequantize_bf16_t4, bfloat4,     1, dequantize_bf16_t4, 128, 128, 1>;\n #endif\n-template [[host_name(\"kernel_flash_attn_ext_vec_q4_0_dk192_dv192\")]] kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES, block_q4_0, 8, dequantize_q4_0_t4, block_q4_0,  8, dequantize_q4_0_t4, 192, 192, 2>;\n-template [[host_name(\"kernel_flash_attn_ext_vec_q4_1_dk192_dv192\")]] kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES, block_q4_1, 8, dequantize_q4_1_t4, block_q4_1,  8, dequantize_q4_1_t4, 192, 192, 2>;\n-template [[host_name(\"kernel_flash_attn_ext_vec_q5_0_dk192_dv192\")]] kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES, block_q5_0, 8, dequantize_q5_0_t4, block_q5_0,  8, dequantize_q5_0_t4, 192, 192, 2>;\n-template [[host_name(\"kernel_flash_attn_ext_vec_q5_1_dk192_dv192\")]] kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES, block_q5_1, 8, dequantize_q5_1_t4, block_q5_1,  8, dequantize_q5_1_t4, 192, 192, 2>;\n-template [[host_name(\"kernel_flash_attn_ext_vec_q8_0_dk192_dv192\")]] kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES, block_q8_0, 8, dequantize_q8_0_t4, block_q8_0,  8, dequantize_q8_0_t4, 192, 192, 2>;\n-\n-template [[host_name(\"kernel_flash_attn_ext_vec_f16_dk192_dv128\")]]  kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES, half4,      1, dequantize_f16_t4,  half4,       1, dequantize_f16_t4,  192, 128, 2>;\n+template [[host_name(\"kernel_flash_attn_ext_vec_q4_0_dk128_dv128\")]] kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES,     block_q4_0, 8, dequantize_q4_0_t4, block_q4_0,  8, dequantize_q4_0_t4, 128, 128, 1>;\n+template [[host_name(\"kernel_flash_attn_ext_vec_q4_1_dk128_dv128\")]] kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES,     block_q4_1, 8, dequantize_q4_1_t4, block_q4_1,  8, dequantize_q4_1_t4, 128, 128, 1>;\n+template [[host_name(\"kernel_flash_attn_ext_vec_q5_0_dk128_dv128\")]] kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES,     block_q5_0, 8, dequantize_q5_0_t4, block_q5_0,  8, dequantize_q5_0_t4, 128, 128, 1>;\n+template [[host_name(\"kernel_flash_attn_ext_vec_q5_1_dk128_dv128\")]] kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES,     block_q5_1, 8, dequantize_q5_1_t4, block_q5_1,  8, dequantize_q5_1_t4, 128, 128, 1>;\n+template [[host_name(\"kernel_flash_attn_ext_vec_q8_0_dk128_dv128\")]] kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES,     block_q8_0, 8, dequantize_q8_0_t4, block_q8_0,  8, dequantize_q8_0_t4, 128, 128, 1>;\n+\n+template [[host_name(\"kernel_flash_attn_ext_vec_f32_dk192_dv192\")]]  kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES_F32, float4,     1, dequantize_f32_t4,  float4,      1, dequantize_f32_t4,  192, 192, 2>;\n+template [[host_name(\"kernel_flash_attn_ext_vec_f16_dk192_dv192\")]]  kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES,     half4,      1, dequantize_f16_t4,  half4,       1, dequantize_f16_t4,  192, 192, 2>;\n #if defined(GGML_METAL_HAS_BF16)\n-template [[host_name(\"kernel_flash_attn_ext_vec_bf16_dk192_dv128\")]] kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES, bfloat4,    1, dequantize_bf16_t4, bfloat4,     1, dequantize_bf16_t4, 192, 128, 2>;\n+template [[host_name(\"kernel_flash_attn_ext_vec_bf16_dk192_dv192\")]] kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES,     bfloat4,    1, dequantize_bf16_t4, bfloat4,     1, dequantize_bf16_t4, 192, 192, 2>;\n #endif\n-template [[host_name(\"kernel_flash_attn_ext_vec_q4_0_dk192_dv128\")]] kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES, block_q4_0, 8, dequantize_q4_0_t4, block_q4_0,  8, dequantize_q4_0_t4, 192, 128, 2>;\n-template [[host_name(\"kernel_flash_attn_ext_vec_q4_1_dk192_dv128\")]] kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES, block_q4_1, 8, dequantize_q4_1_t4, block_q4_1,  8, dequantize_q4_1_t4, 192, 128, 2>;\n-template [[host_name(\"kernel_flash_attn_ext_vec_q5_0_dk192_dv128\")]] kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES, block_q5_0, 8, dequantize_q5_0_t4, block_q5_0,  8, dequantize_q5_0_t4, 192, 128, 2>;\n-template [[host_name(\"kernel_flash_attn_ext_vec_q5_1_dk192_dv128\")]] kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES, block_q5_1, 8, dequantize_q5_1_t4, block_q5_1,  8, dequantize_q5_1_t4, 192, 128, 2>;\n-template [[host_name(\"kernel_flash_attn_ext_vec_q8_0_dk192_dv128\")]] kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES, block_q8_0, 8, dequantize_q8_0_t4, block_q8_0,  8, dequantize_q8_0_t4, 192, 128, 2>;\n-\n-template [[host_name(\"kernel_flash_attn_ext_vec_f16_dk256_dv256\")]]  kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES, half4,      1, dequantize_f16_t4,  half4,       1, dequantize_f16_t4,  256, 256, 1>;\n+template [[host_name(\"kernel_flash_attn_ext_vec_q4_0_dk192_dv192\")]] kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES,     block_q4_0, 8, dequantize_q4_0_t4, block_q4_0,  8, dequantize_q4_0_t4, 192, 192, 2>;\n+template [[host_name(\"kernel_flash_attn_ext_vec_q4_1_dk192_dv192\")]] kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES,     block_q4_1, 8, dequantize_q4_1_t4, block_q4_1,  8, dequantize_q4_1_t4, 192, 192, 2>;\n+template [[host_name(\"kernel_flash_attn_ext_vec_q5_0_dk192_dv192\")]] kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES,     block_q5_0, 8, dequantize_q5_0_t4, block_q5_0,  8, dequantize_q5_0_t4, 192, 192, 2>;\n+template [[host_name(\"kernel_flash_attn_ext_vec_q5_1_dk192_dv192\")]] kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES,     block_q5_1, 8, dequantize_q5_1_t4, block_q5_1,  8, dequantize_q5_1_t4, 192, 192, 2>;\n+template [[host_name(\"kernel_flash_attn_ext_vec_q8_0_dk192_dv192\")]] kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES,     block_q8_0, 8, dequantize_q8_0_t4, block_q8_0,  8, dequantize_q8_0_t4, 192, 192, 2>;\n+\n+template [[host_name(\"kernel_flash_attn_ext_vec_f32_dk192_dv128\")]]  kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES_F32, float4,     1, dequantize_f32_t4,  float4,      1, dequantize_f32_t4,  192, 128, 2>;\n+template [[host_name(\"kernel_flash_attn_ext_vec_f16_dk192_dv128\")]]  kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES,     half4,      1, dequantize_f16_t4,  half4,       1, dequantize_f16_t4,  192, 128, 2>;\n #if defined(GGML_METAL_HAS_BF16)\n-template [[host_name(\"kernel_flash_attn_ext_vec_bf16_dk256_dv256\")]] kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES, bfloat4,    1, dequantize_bf16_t4, bfloat4,     1, dequantize_bf16_t4, 256, 256, 1>;\n+template [[host_name(\"kernel_flash_attn_ext_vec_bf16_dk192_dv128\")]] kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES,     bfloat4,    1, dequantize_bf16_t4, bfloat4,     1, dequantize_bf16_t4, 192, 128, 2>;\n #endif\n-template [[host_name(\"kernel_flash_attn_ext_vec_q4_0_dk256_dv256\")]] kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES, block_q4_0, 8, dequantize_q4_0_t4, block_q4_0,  8, dequantize_q4_0_t4, 256, 256, 1>;\n-template [[host_name(\"kernel_flash_attn_ext_vec_q4_1_dk256_dv256\")]] kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES, block_q4_1, 8, dequantize_q4_1_t4, block_q4_1,  8, dequantize_q4_1_t4, 256, 256, 1>;\n-template [[host_name(\"kernel_flash_attn_ext_vec_q5_0_dk256_dv256\")]] kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES, block_q5_0, 8, dequantize_q5_0_t4, block_q5_0,  8, dequantize_q5_0_t4, 256, 256, 1>;\n-template [[host_name(\"kernel_flash_attn_ext_vec_q5_1_dk256_dv256\")]] kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES, block_q5_1, 8, dequantize_q5_1_t4, block_q5_1,  8, dequantize_q5_1_t4, 256, 256, 1>;\n-template [[host_name(\"kernel_flash_attn_ext_vec_q8_0_dk256_dv256\")]] kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES, block_q8_0, 8, dequantize_q8_0_t4, block_q8_0,  8, dequantize_q8_0_t4, 256, 256, 1>;\n-\n-template [[host_name(\"kernel_flash_attn_ext_vec_f16_dk576_dv512\")]]  kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES, half4,      1, dequantize_f16_t4,  half4,       1, dequantize_f16_t4,  576, 512, 2>;\n+template [[host_name(\"kernel_flash_attn_ext_vec_q4_0_dk192_dv128\")]] kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES,     block_q4_0, 8, dequantize_q4_0_t4, block_q4_0,  8, dequantize_q4_0_t4, 192, 128, 2>;\n+template [[host_name(\"kernel_flash_attn_ext_vec_q4_1_dk192_dv128\")]] kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES,     block_q4_1, 8, dequantize_q4_1_t4, block_q4_1,  8, dequantize_q4_1_t4, 192, 128, 2>;\n+template [[host_name(\"kernel_flash_attn_ext_vec_q5_0_dk192_dv128\")]] kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES,     block_q5_0, 8, dequantize_q5_0_t4, block_q5_0,  8, dequantize_q5_0_t4, 192, 128, 2>;\n+template [[host_name(\"kernel_flash_attn_ext_vec_q5_1_dk192_dv128\")]] kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES,     block_q5_1, 8, dequantize_q5_1_t4, block_q5_1,  8, dequantize_q5_1_t4, 192, 128, 2>;\n+template [[host_name(\"kernel_flash_attn_ext_vec_q8_0_dk192_dv128\")]] kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES,     block_q8_0, 8, dequantize_q8_0_t4, block_q8_0,  8, dequantize_q8_0_t4, 192, 128, 2>;\n+\n+template [[host_name(\"kernel_flash_attn_ext_vec_f32_dk256_dv256\")]]  kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES_F32, float4,     1, dequantize_f32_t4,  float4,      1, dequantize_f32_t4,  256, 256, 1>;\n+template [[host_name(\"kernel_flash_attn_ext_vec_f16_dk256_dv256\")]]  kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES,     half4,      1, dequantize_f16_t4,  half4,       1, dequantize_f16_t4,  256, 256, 1>;\n+#if defined(GGML_METAL_HAS_BF16)\n+template [[host_name(\"kernel_flash_attn_ext_vec_bf16_dk256_dv256\")]] kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES,     bfloat4,    1, dequantize_bf16_t4, bfloat4,     1, dequantize_bf16_t4, 256, 256, 1>;\n+#endif\n+template [[host_name(\"kernel_flash_attn_ext_vec_q4_0_dk256_dv256\")]] kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES,     block_q4_0, 8, dequantize_q4_0_t4, block_q4_0,  8, dequantize_q4_0_t4, 256, 256, 1>;\n+template [[host_name(\"kernel_flash_attn_ext_vec_q4_1_dk256_dv256\")]] kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES,     block_q4_1, 8, dequantize_q4_1_t4, block_q4_1,  8, dequantize_q4_1_t4, 256, 256, 1>;\n+template [[host_name(\"kernel_flash_attn_ext_vec_q5_0_dk256_dv256\")]] kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES,     block_q5_0, 8, dequantize_q5_0_t4, block_q5_0,  8, dequantize_q5_0_t4, 256, 256, 1>;\n+template [[host_name(\"kernel_flash_attn_ext_vec_q5_1_dk256_dv256\")]] kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES,     block_q5_1, 8, dequantize_q5_1_t4, block_q5_1,  8, dequantize_q5_1_t4, 256, 256, 1>;\n+template [[host_name(\"kernel_flash_attn_ext_vec_q8_0_dk256_dv256\")]] kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES,     block_q8_0, 8, dequantize_q8_0_t4, block_q8_0,  8, dequantize_q8_0_t4, 256, 256, 1>;\n+\n+template [[host_name(\"kernel_flash_attn_ext_vec_f32_dk576_dv512\")]]  kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES_F32, float4,     1, dequantize_f32_t4,  float4,      1, dequantize_f32_t4,  576, 512, 2>;\n+template [[host_name(\"kernel_flash_attn_ext_vec_f16_dk576_dv512\")]]  kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES,     half4,      1, dequantize_f16_t4,  half4,       1, dequantize_f16_t4,  576, 512, 2>;\n #if defined(GGML_METAL_HAS_BF16)\n-template [[host_name(\"kernel_flash_attn_ext_vec_bf16_dk576_dv512\")]] kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES, bfloat4,    1, dequantize_bf16_t4, bfloat4,     1, dequantize_bf16_t4, 576, 512, 2>;\n+template [[host_name(\"kernel_flash_attn_ext_vec_bf16_dk576_dv512\")]] kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES,     bfloat4,    1, dequantize_bf16_t4, bfloat4,     1, dequantize_bf16_t4, 576, 512, 2>;\n #endif\n-template [[host_name(\"kernel_flash_attn_ext_vec_q4_0_dk576_dv512\")]] kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES, block_q4_0, 8, dequantize_q4_0_t4, block_q4_0,  8, dequantize_q4_0_t4, 576, 512, 2>;\n-template [[host_name(\"kernel_flash_attn_ext_vec_q4_1_dk576_dv512\")]] kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES, block_q4_1, 8, dequantize_q4_1_t4, block_q4_1,  8, dequantize_q4_1_t4, 576, 512, 2>;\n-template [[host_name(\"kernel_flash_attn_ext_vec_q5_0_dk576_dv512\")]] kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES, block_q5_0, 8, dequantize_q5_0_t4, block_q5_0,  8, dequantize_q5_0_t4, 576, 512, 2>;\n-template [[host_name(\"kernel_flash_attn_ext_vec_q5_1_dk576_dv512\")]] kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES, block_q5_1, 8, dequantize_q5_1_t4, block_q5_1,  8, dequantize_q5_1_t4, 576, 512, 2>;\n-template [[host_name(\"kernel_flash_attn_ext_vec_q8_0_dk576_dv512\")]] kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES, block_q8_0, 8, dequantize_q8_0_t4, block_q8_0,  8, dequantize_q8_0_t4, 576, 512, 2>;\n+template [[host_name(\"kernel_flash_attn_ext_vec_q4_0_dk576_dv512\")]] kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES,     block_q4_0, 8, dequantize_q4_0_t4, block_q4_0,  8, dequantize_q4_0_t4, 576, 512, 2>;\n+template [[host_name(\"kernel_flash_attn_ext_vec_q4_1_dk576_dv512\")]] kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES,     block_q4_1, 8, dequantize_q4_1_t4, block_q4_1,  8, dequantize_q4_1_t4, 576, 512, 2>;\n+template [[host_name(\"kernel_flash_attn_ext_vec_q5_0_dk576_dv512\")]] kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES,     block_q5_0, 8, dequantize_q5_0_t4, block_q5_0,  8, dequantize_q5_0_t4, 576, 512, 2>;\n+template [[host_name(\"kernel_flash_attn_ext_vec_q5_1_dk576_dv512\")]] kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES,     block_q5_1, 8, dequantize_q5_1_t4, block_q5_1,  8, dequantize_q5_1_t4, 576, 512, 2>;\n+template [[host_name(\"kernel_flash_attn_ext_vec_q8_0_dk576_dv512\")]] kernel flash_attn_ext_vec_t kernel_flash_attn_ext_vec<FA_TYPES,     block_q8_0, 8, dequantize_q8_0_t4, block_q8_0,  8, dequantize_q8_0_t4, 576, 512, 2>;\n \n #undef FA_TYPES\n "
      },
      {
        "filename": "src/llama-graph.cpp",
        "status": "modified",
        "additions": 0,
        "deletions": 1,
        "changes": 1,
        "patch": "@@ -1323,7 +1323,6 @@ ggml_tensor * llm_graph_context::build_attn_mha(\n \n     ggml_tensor * cur;\n \n-    // TODO: replace hardcoded padding with ggml-provided padding\n     if (cparams.flash_attn && kq_b == nullptr) {\n         GGML_ASSERT(kq_b == nullptr && \"Flash attention does not support KQ bias yet\");\n "
      },
      {
        "filename": "tests/test-backend-ops.cpp",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -6779,7 +6779,7 @@ static std::vector<std::unique_ptr<test_case>> make_test_cases_eval() {\n                                             for (int nb : { 1, 3, 32, 35, }) {\n                                                 for (ggml_prec prec : {GGML_PREC_F32, GGML_PREC_DEFAULT}) {\n                                                     if (hsk != 128 && prec == GGML_PREC_DEFAULT) continue;\n-                                                    for (ggml_type type_KV : {GGML_TYPE_F16, GGML_TYPE_BF16, GGML_TYPE_Q8_0, GGML_TYPE_Q4_0}) {\n+                                                    for (ggml_type type_KV : {GGML_TYPE_F32, GGML_TYPE_F16, GGML_TYPE_BF16, GGML_TYPE_Q8_0, GGML_TYPE_Q4_0}) {\n                                                         test_cases.emplace_back(new test_flash_attn_ext(\n                                                                     hsk, hsv, nh, {nr2, nr3}, kv, nb, mask, sinks, max_bias, logit_softcap, prec, type_KV));\n                                                         // run fewer test cases permuted"
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T23:30:06.550085"
  },
  {
    "pr_number": 16529,
    "title": "Metal: add opt_step_adamw and op_sum",
    "body": "Part of https://github.com/ggml-org/llama.cpp/issues/14909:\r\n\r\nThis PR adds Metal backend for the `OPT_STEP_ADAMW` operator.\r\n\r\n### Implementation\r\n\r\n- Added `kernel_opt_step_adamw_f32` kernel in `ggml-metal.metal`\r\n- Implemented argument struct ~~for parameters~~, `ggml_metal_kargs_opt_step_adamw`\r\n- Storing ~~parameters and~~ `np` in the struct as a passed `constant & args`\r\n- Threadgroup size calculation, similar to `ggml_metal_op_upscale`\r\n\r\nI've not written test cases for this operator as one already exists in `test-backend-ops.cpp`.\r\n\r\n### Additional changes\r\n\r\n- Due to `test-opts` failing, the `GGML_OP_SUM` operator is implemented in Metal (currently unoptimised)",
    "html_url": "https://github.com/ggml-org/llama.cpp/pull/16529",
    "created_at": "2025-10-12T08:56:10Z",
    "merged_at": "2025-10-12T18:43:14Z",
    "merge_commit_sha": "a31cf36ad946a13b3a646bf0dadf2a481e89f944",
    "base_ref": "master",
    "head_sha": "01e8f2e35564c89f0d1b4c08eadcdc9f6d04689e",
    "user": "cern1710",
    "files": [
      {
        "filename": "ggml/src/ggml-metal/ggml-metal-device.cpp",
        "status": "modified",
        "additions": 37,
        "deletions": 0,
        "changes": 37,
        "patch": "@@ -268,6 +268,25 @@ ggml_metal_pipeline_t ggml_metal_library_get_pipeline_glu(ggml_metal_library_t l\n     return res;\n }\n \n+ggml_metal_pipeline_t ggml_metal_library_get_pipeline_sum(ggml_metal_library_t lib, const ggml_tensor * op) {\n+    assert(op->op == GGML_OP_SUM);\n+\n+    char base[256];\n+    char name[256];\n+\n+    snprintf(base, 256, \"kernel_op_sum_%s\", ggml_type_name(op->src[0]->type));\n+    snprintf(name, 256, \"%s\", base);\n+\n+    ggml_metal_pipeline_t res = ggml_metal_library_get_pipeline(lib, name);\n+    if (res) {\n+        return res;\n+    }\n+\n+    res = ggml_metal_library_compile_pipeline(lib, base, name, nullptr);\n+\n+    return res;\n+}\n+\n ggml_metal_pipeline_t ggml_metal_library_get_pipeline_sum_rows(ggml_metal_library_t lib, const ggml_tensor * op) {\n     GGML_ASSERT(op->src[0]->nb[0] == ggml_type_size(op->src[0]->type));\n \n@@ -1482,3 +1501,21 @@ ggml_metal_pipeline_t ggml_metal_library_get_pipeline_timestep_embedding(ggml_me\n     return res;\n }\n \n+ggml_metal_pipeline_t ggml_metal_library_get_pipeline_opt_step_adamw(ggml_metal_library_t lib, const ggml_tensor * op) {\n+    assert(op->op == GGML_OP_OPT_STEP_ADAMW);\n+\n+    char base[256];\n+    char name[256];\n+\n+    snprintf(base, 256, \"kernel_opt_step_adamw_%s\", ggml_type_name(op->src[0]->type));\n+    snprintf(name, 256, \"%s\", base);\n+\n+    ggml_metal_pipeline_t res = ggml_metal_library_get_pipeline(lib, name);\n+    if (res) {\n+        return res;\n+    }\n+\n+    res = ggml_metal_library_compile_pipeline(lib, base, name, nullptr);\n+\n+    return res;\n+}"
      },
      {
        "filename": "ggml/src/ggml-metal/ggml-metal-device.h",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -109,6 +109,7 @@ ggml_metal_pipeline_t ggml_metal_library_get_pipeline_set_rows          (ggml_me\n ggml_metal_pipeline_t ggml_metal_library_get_pipeline_repeat            (ggml_metal_library_t lib, enum ggml_type tsrc);\n ggml_metal_pipeline_t ggml_metal_library_get_pipeline_unary             (ggml_metal_library_t lib, const struct ggml_tensor * op);\n ggml_metal_pipeline_t ggml_metal_library_get_pipeline_glu               (ggml_metal_library_t lib, const struct ggml_tensor * op);\n+ggml_metal_pipeline_t ggml_metal_library_get_pipeline_sum               (ggml_metal_library_t lib, const struct ggml_tensor * op);\n ggml_metal_pipeline_t ggml_metal_library_get_pipeline_sum_rows          (ggml_metal_library_t lib, const struct ggml_tensor * op);\n ggml_metal_pipeline_t ggml_metal_library_get_pipeline_soft_max          (ggml_metal_library_t lib, const struct ggml_tensor * op);\n ggml_metal_pipeline_t ggml_metal_library_get_pipeline_ssm_conv          (ggml_metal_library_t lib, const struct ggml_tensor * op);\n@@ -134,6 +135,7 @@ ggml_metal_pipeline_t ggml_metal_library_get_pipeline_pad               (ggml_me\n ggml_metal_pipeline_t ggml_metal_library_get_pipeline_pad_reflect_1d    (ggml_metal_library_t lib, const struct ggml_tensor * op);\n ggml_metal_pipeline_t ggml_metal_library_get_pipeline_arange            (ggml_metal_library_t lib, const struct ggml_tensor * op);\n ggml_metal_pipeline_t ggml_metal_library_get_pipeline_timestep_embedding(ggml_metal_library_t lib, const struct ggml_tensor * op);\n+ggml_metal_pipeline_t ggml_metal_library_get_pipeline_opt_step_adamw    (ggml_metal_library_t lib, const struct ggml_tensor * op);\n \n ggml_metal_pipeline_t ggml_metal_library_get_pipeline_flash_attn_ext_pad(\n         ggml_metal_library_t lib,"
      },
      {
        "filename": "ggml/src/ggml-metal/ggml-metal-device.m",
        "status": "modified",
        "additions": 3,
        "deletions": 0,
        "changes": 3,
        "patch": "@@ -656,6 +656,7 @@ bool ggml_metal_device_supports_op(ggml_metal_device_t dev, const struct ggml_te\n         case GGML_OP_COS:\n         case GGML_OP_LOG:\n             return ggml_is_contiguous(op->src[0]) && op->src[0]->type == GGML_TYPE_F32;\n+        case GGML_OP_SUM:\n         case GGML_OP_SUM_ROWS:\n         case GGML_OP_MEAN:\n         case GGML_OP_SOFT_MAX:\n@@ -798,6 +799,8 @@ bool ggml_metal_device_supports_op(ggml_metal_device_t dev, const struct ggml_te\n                         return false;\n                 };\n             }\n+        case GGML_OP_OPT_STEP_ADAMW:\n+            return has_simdgroup_reduction;\n         default:\n             return false;\n     }"
      },
      {
        "filename": "ggml/src/ggml-metal/ggml-metal-impl.h",
        "status": "modified",
        "additions": 8,
        "deletions": 0,
        "changes": 8,
        "patch": "@@ -544,6 +544,10 @@ typedef struct{\n     float    limit;\n } ggml_metal_kargs_glu;\n \n+typedef struct {\n+    uint64_t np;\n+} ggml_metal_kargs_sum;\n+\n typedef struct {\n     int64_t  ne00;\n     int64_t  ne01;\n@@ -773,4 +777,8 @@ typedef struct {\n     uint64_t nb01;\n } ggml_metal_kargs_argmax;\n \n+typedef struct {\n+    int64_t  np;\n+} ggml_metal_kargs_opt_step_adamw;\n+\n #endif // GGML_METAL_IMPL"
      },
      {
        "filename": "ggml/src/ggml-metal/ggml-metal-ops.cpp",
        "status": "modified",
        "additions": 68,
        "deletions": 0,
        "changes": 68,
        "patch": "@@ -301,6 +301,10 @@ static int ggml_metal_op_encode_impl(ggml_metal_op_t ctx, int idx) {\n             {\n                 n_fuse = ggml_metal_op_glu(ctx, idx);\n             } break;\n+        case GGML_OP_SUM:\n+            {\n+                n_fuse = ggml_metal_op_sum(ctx, idx);\n+            } break;\n         case GGML_OP_SUM_ROWS:\n         case GGML_OP_MEAN:\n             {\n@@ -410,6 +414,10 @@ static int ggml_metal_op_encode_impl(ggml_metal_op_t ctx, int idx) {\n             {\n                 n_fuse = ggml_metal_op_argmax(ctx, idx);\n             } break;\n+        case GGML_OP_OPT_STEP_ADAMW:\n+            {\n+                n_fuse = ggml_metal_op_opt_step_adamw(ctx, idx);\n+            } break;\n        default:\n             {\n                 GGML_LOG_ERROR(\"%s: error: node %3d, op = %8s not implemented\\n\", __func__, idx, ggml_op_name(node->op));\n@@ -840,6 +848,30 @@ int ggml_metal_op_glu(ggml_metal_op_t ctx, int idx) {\n     return 1;\n }\n \n+int ggml_metal_op_sum(ggml_metal_op_t ctx, int idx) {\n+    ggml_tensor * op  = ctx->node(idx);\n+\n+    ggml_metal_library_t lib = ctx->lib;\n+    ggml_metal_encoder_t enc = ctx->enc;\n+\n+    const uint64_t n = (uint64_t) ggml_nelements(op->src[0]);\n+\n+    ggml_metal_kargs_sum args = {\n+        /*.np =*/ n,\n+    };\n+\n+    ggml_metal_pipeline_t pipeline = ggml_metal_library_get_pipeline_sum(lib, op);\n+\n+    ggml_metal_encoder_set_pipeline(enc, pipeline);\n+    ggml_metal_encoder_set_bytes   (enc, &args, sizeof(args), 0);\n+    ggml_metal_encoder_set_buffer  (enc, ggml_metal_get_buffer_id(op->src[0]), 1);\n+    ggml_metal_encoder_set_buffer  (enc, ggml_metal_get_buffer_id(op),         2);\n+\n+    ggml_metal_encoder_dispatch_threadgroups(enc, 1, 1, 1, 1, 1, 1);\n+\n+    return 1;\n+}\n+\n int ggml_metal_op_sum_rows(ggml_metal_op_t ctx, int idx) {\n     ggml_tensor * op = ctx->node(idx);\n \n@@ -3401,3 +3433,39 @@ int ggml_metal_op_leaky_relu(ggml_metal_op_t ctx, int idx) {\n \n     return 1;\n }\n+\n+int ggml_metal_op_opt_step_adamw(ggml_metal_op_t ctx, int idx) {\n+    ggml_tensor * op = ctx->node(idx);\n+\n+    ggml_metal_library_t lib = ctx->lib;\n+    ggml_metal_encoder_t enc = ctx->enc;\n+\n+    GGML_TENSOR_LOCALS( int32_t, ne0, op->src[0], ne);\n+    GGML_TENSOR_LOCALS(uint64_t, nb0, op->src[0], nb);\n+    GGML_TENSOR_LOCALS( int32_t, ne,  op,         ne);\n+    GGML_TENSOR_LOCALS(uint32_t, nb,  op,         nb);\n+\n+    ggml_metal_pipeline_t pipeline = ggml_metal_library_get_pipeline_opt_step_adamw(lib, op);\n+\n+    const int64_t np = ggml_nelements(op->src[0]);\n+    ggml_metal_kargs_opt_step_adamw args = {\n+        /*.np =*/ np,\n+    };\n+\n+    int ida = 0;\n+\n+    ggml_metal_encoder_set_pipeline(enc, pipeline);\n+    ggml_metal_encoder_set_bytes   (enc, &args, sizeof(args), ida++);\n+    ggml_metal_encoder_set_buffer  (enc, ggml_metal_get_buffer_id(op->src[0]), ida++);\n+    ggml_metal_encoder_set_buffer  (enc, ggml_metal_get_buffer_id(op->src[1]), ida++);\n+    ggml_metal_encoder_set_buffer  (enc, ggml_metal_get_buffer_id(op->src[2]), ida++);\n+    ggml_metal_encoder_set_buffer  (enc, ggml_metal_get_buffer_id(op->src[3]), ida++);\n+    ggml_metal_encoder_set_buffer  (enc, ggml_metal_get_buffer_id(op->src[4]), ida++);\n+\n+    const int nth = std::min(ggml_metal_pipeline_max_theads_per_threadgroup(pipeline), ne0);\n+    const int64_t n = (np + nth - 1) / nth;\n+\n+    ggml_metal_encoder_dispatch_threadgroups(enc, n, 1, 1, nth, 1, 1);\n+\n+    return 1;\n+}"
      },
      {
        "filename": "ggml/src/ggml-metal/ggml-metal-ops.h",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -50,6 +50,7 @@ int ggml_metal_op_scale             (ggml_metal_op_t ctx, int idx);\n int ggml_metal_op_clamp             (ggml_metal_op_t ctx, int idx);\n int ggml_metal_op_unary             (ggml_metal_op_t ctx, int idx);\n int ggml_metal_op_glu               (ggml_metal_op_t ctx, int idx);\n+int ggml_metal_op_sum               (ggml_metal_op_t ctx, int idx);\n int ggml_metal_op_sum_rows          (ggml_metal_op_t ctx, int idx);\n int ggml_metal_op_get_rows          (ggml_metal_op_t ctx, int idx);\n int ggml_metal_op_set_rows          (ggml_metal_op_t ctx, int idx);\n@@ -78,6 +79,7 @@ int ggml_metal_op_timestep_embedding(ggml_metal_op_t ctx, int idx);\n int ggml_metal_op_argmax            (ggml_metal_op_t ctx, int idx);\n int ggml_metal_op_argsort           (ggml_metal_op_t ctx, int idx);\n int ggml_metal_op_leaky_relu        (ggml_metal_op_t ctx, int idx);\n+int ggml_metal_op_opt_step_adamw    (ggml_metal_op_t ctx, int idx);\n \n #ifdef __cplusplus\n }"
      },
      {
        "filename": "ggml/src/ggml-metal/ggml-metal.metal",
        "status": "modified",
        "additions": 52,
        "deletions": 0,
        "changes": 52,
        "patch": "@@ -1723,6 +1723,24 @@ kernel void kernel_geglu_quick_f32(\n     }\n }\n \n+kernel void kernel_op_sum_f32(\n+        constant ggml_metal_kargs_sum & args,\n+        device const float * src0,\n+        device       float * dst,\n+        ushort  tiitg[[thread_index_in_threadgroup]]) {\n+\n+    if (tiitg != 0) {\n+        return;\n+    }\n+\n+    float acc = 0.0f;\n+    for (ulong i = 0; i < args.np; ++i) {\n+        acc += src0[i];\n+    }\n+\n+    dst[0] = acc;\n+}\n+\n template <bool norm>\n kernel void kernel_sum_rows(\n         constant ggml_metal_kargs_sum_rows & args,\n@@ -8754,3 +8772,37 @@ kernel void kernel_pool_2d_avg_f32(\n \n     o_ptr[cur_oh * args.OW + cur_ow] = res;\n }\n+\n+kernel void kernel_opt_step_adamw_f32(\n+        constant    ggml_metal_kargs_opt_step_adamw & args,\n+        device       float * x,\n+        device const float * g,\n+        device       float * g_m,\n+        device       float * g_v,\n+        device const float * pars,\n+        uint        gid[[thread_position_in_grid]]) {\n+\n+    if (gid >= args.np) {\n+        return;\n+    }\n+\n+    const float alpha  = pars[0];\n+    const float beta1  = pars[1];\n+    const float beta2  = pars[2];\n+    const float eps    = pars[3];\n+    const float wd     = pars[4];\n+    const float beta1h = pars[5];\n+    const float beta2h = pars[6];\n+\n+    const float gi = g[gid];\n+    const float gmi = g_m[gid] * beta1 +      gi * (1.0f - beta1);\n+    const float gvi = g_v[gid] * beta2 + gi * gi * (1.0f - beta2);\n+\n+    g_m[gid] = gmi;\n+    g_v[gid] = gvi;\n+\n+    const float mh =      gmi * beta1h;\n+    const float vh = sqrt(gvi * beta2h) + eps;\n+\n+    x[gid] = x[gid] * (1.0f - alpha * wd) - alpha * mh / vh;\n+}"
      }
    ],
    "num_files": 7,
    "scraped_at": "2025-11-16T23:30:06.873582"
  },
  {
    "pr_number": 16528,
    "title": "graph : support cacheless embeddings with FA and iSWA",
    "body": "- Support cacheless iSWA such as EmbeddingGemma\r\n- Enable FA for all cacheless models and batch sizes",
    "html_url": "https://github.com/ggml-org/llama.cpp/pull/16528",
    "created_at": "2025-10-12T07:42:07Z",
    "merged_at": "2025-10-13T19:42:37Z",
    "merge_commit_sha": "e38b7c6e9e4453e3b3e96d76e38bc2ccb6bce458",
    "base_ref": "master",
    "head_sha": "5c6583190ee7592df9e78a3d183226e65f96ecd6",
    "user": "ggerganov",
    "files": [
      {
        "filename": "src/llama-graph.cpp",
        "status": "modified",
        "additions": 74,
        "deletions": 42,
        "changes": 116,
        "patch": "@@ -261,12 +261,17 @@ void llm_graph_input_cross_embd::set_input(const llama_ubatch * ubatch) {\n     }\n }\n \n-static void print_mask(float * data, int64_t n_tokens, int64_t n_kv, int64_t n_swa, llama_swa_type swa_type) {\n+static void print_mask(const float * data, int64_t n_tokens, int64_t n_kv, int64_t n_swa, llama_swa_type swa_type) {\n     LLAMA_LOG_DEBUG(\"%s: === Attention mask ===\\n\", __func__);\n-    const char * swa_type_str = (swa_type == LLAMA_SWA_TYPE_NONE) ? \"LLAMA_SWA_TYPE_NONE\" :\n-                          (swa_type == LLAMA_SWA_TYPE_STANDARD) ? \"LLAMA_SWA_TYPE_STANDARD\" :\n-                          (swa_type == LLAMA_SWA_TYPE_CHUNKED) ? \"LLAMA_SWA_TYPE_CHUNKED\" :\n-                          (swa_type == LLAMA_SWA_TYPE_SYMMETRIC) ? \"LLAMA_SWA_TYPE_SYMMETRIC\" : \"unknown\";\n+    const char * swa_type_str = \"unknown\";\n+\n+    switch (swa_type) {\n+        case LLAMA_SWA_TYPE_NONE:      swa_type_str = \"LLAMA_SWA_TYPE_NONE\"; break;\n+        case LLAMA_SWA_TYPE_STANDARD:  swa_type_str = \"LLAMA_SWA_TYPE_STANDARD\"; break;\n+        case LLAMA_SWA_TYPE_CHUNKED:   swa_type_str = \"LLAMA_SWA_TYPE_CHUNKED\"; break;\n+        case LLAMA_SWA_TYPE_SYMMETRIC: swa_type_str = \"LLAMA_SWA_TYPE_SYMMETRIC\"; break;\n+    };\n+\n     LLAMA_LOG_DEBUG(\"%s: n_swa : %d, n_kv: %d, swq_type: %s\\n\", __func__, (int)n_swa, (int)n_kv, swa_type_str);\n     LLAMA_LOG_DEBUG(\"%s: '0' = can attend, '\u221e' = masked\\n\", __func__);\n     LLAMA_LOG_DEBUG(\"%s: Rows = query tokens, Columns = key/value tokens\\n\\n\", __func__);\n@@ -295,50 +300,67 @@ void llm_graph_input_attn_no_cache::set_input(const llama_ubatch * ubatch) {\n     const int64_t n_kv     = ubatch->n_tokens;\n     const int64_t n_tokens = ubatch->n_tokens;\n \n-    GGML_ASSERT(kq_mask);\n-    GGML_ASSERT(ggml_backend_buffer_is_host(kq_mask->buffer));\n-\n-    float * data = (float *) kq_mask->data;\n-\n-    // [TAG_NO_CACHE_ISWA]\n-    GGML_ASSERT(hparams.swa_type == LLAMA_SWA_TYPE_NONE && \"TODO: implement\");\n+    const auto fill_mask = [&](float * data, int n_swa, llama_swa_type swa_type) {\n+        for (int h = 0; h < 1; ++h) {\n+            for (int i1 = 0; i1 < n_tokens; ++i1) {\n+                const llama_seq_id s1 = ubatch->seq_id[i1][0];\n+                const llama_pos    p1 = ubatch->pos[i1];\n \n-    for (int h = 0; h < 1; ++h) {\n-        for (int i1 = 0; i1 < n_tokens; ++i1) {\n-            const llama_seq_id s1 = ubatch->seq_id[i1][0];\n+                const uint64_t idst = h*(n_kv*n_tokens) + i1*n_kv;\n \n-            for (int i0 = 0; i0 < n_tokens; ++i0) {\n-                float f = -INFINITY;\n-\n-                for (int s = 0; s < ubatch->n_seq_id[i0]; ++s) {\n+                for (int i0 = 0; i0 < n_tokens; ++i0) {\n                     const llama_seq_id s0 = ubatch->seq_id[i0][0];\n+                    const llama_pos p0    = ubatch->pos[i0];\n \n+                    // mask different sequences\n                     if (s0 != s1) {\n-                        continue; // skip different sequences\n+                        continue;\n                     }\n \n-                    if (cparams.causal_attn && ubatch->pos[i0] > ubatch->pos[i1]) {\n-                        continue; // skip future tokens for causal attention\n+                    // mask future tokens\n+                    if (cparams.causal_attn && p0 > p1) {\n+                        continue;\n                     }\n \n-                    // TODO: this does not take into account that some layers are SWA and others are note (i.e. iSWA) [TAG_NO_CACHE_ISWA]\n-                    //if (hparams.is_masked_swa(ubatch->pos[i0], ubatch->pos[i1])) {\n-                    //    continue; // skip masked tokens for SWA\n-                    //}\n-\n-                    // TODO: reimplement this like in llama_kv_cache_unified\n-                    if (hparams.use_alibi) {\n-                        f = -std::abs(ubatch->pos[i0] - ubatch->pos[i1]);\n-                    } else {\n-                        f = 0.0f;\n+                    // apply SWA if any\n+                    if (llama_hparams::is_masked_swa(n_swa, swa_type, p0, p1)) {\n+                        continue;\n                     }\n+\n+                    data[idst + i0] = hparams.use_alibi ? -std::abs(p0 - p1) : 0.0f;\n                 }\n-                data[h*(n_kv*n_tokens) + i1*n_kv + i0] = f;\n             }\n         }\n+    };\n+\n+    {\n+        GGML_ASSERT(self_kq_mask);\n+        GGML_ASSERT(ggml_backend_buffer_is_host(self_kq_mask->buffer));\n+\n+        float * data = (float *) self_kq_mask->data;\n+\n+        std::fill(data, data + ggml_nelements(self_kq_mask), -INFINITY);\n+\n+        fill_mask(data, 0, LLAMA_SWA_TYPE_NONE);\n+\n+        if (debug) {\n+            print_mask(data, n_tokens, n_kv, 0, LLAMA_SWA_TYPE_NONE);\n+        }\n     }\n-    if (debug) {\n-        print_mask(data, n_tokens, n_kv, hparams.n_swa, hparams.swa_type);\n+\n+    if (hparams.swa_type != LLAMA_SWA_TYPE_NONE) {\n+        GGML_ASSERT(self_kq_mask_swa);\n+        GGML_ASSERT(ggml_backend_buffer_is_host(self_kq_mask_swa->buffer));\n+\n+        float * data = (float *) self_kq_mask_swa->data;\n+\n+        std::fill(data, data + ggml_nelements(self_kq_mask_swa), -INFINITY);\n+\n+        fill_mask(data, hparams.n_swa, hparams.swa_type);\n+\n+        if (debug) {\n+            print_mask(data, n_tokens, n_kv, hparams.n_swa, hparams.swa_type);\n+        }\n     }\n }\n \n@@ -1299,12 +1321,10 @@ ggml_tensor * llm_graph_context::build_attn_mha(\n     k = ggml_permute(ctx0, k, 0, 2, 1, 3);\n     v = ggml_permute(ctx0, v, 0, 2, 1, 3);\n \n-    const auto n_kv = k->ne[1];\n-\n     ggml_tensor * cur;\n \n     // TODO: replace hardcoded padding with ggml-provided padding\n-    if (cparams.flash_attn && (n_kv % 256 == 0) && kq_b == nullptr) {\n+    if (cparams.flash_attn && kq_b == nullptr) {\n         GGML_ASSERT(kq_b == nullptr && \"Flash attention does not support KQ bias yet\");\n \n         if (v_trans) {\n@@ -1419,10 +1439,20 @@ llm_graph_input_attn_no_cache * llm_graph_context::build_attn_inp_no_cache() con\n     auto inp = std::make_unique<llm_graph_input_attn_no_cache>(hparams, cparams);\n \n     // note: there is no KV cache, so the number of KV values is equal to the number of tokens in the batch\n-    inp->kq_mask = ggml_new_tensor_4d(ctx0, GGML_TYPE_F32, n_tokens, GGML_PAD(n_tokens, GGML_KQ_MASK_PAD), 1, 1);\n-    ggml_set_input(inp->kq_mask);\n+    inp->self_kq_mask = ggml_new_tensor_4d(ctx0, GGML_TYPE_F32, n_tokens, GGML_PAD(n_tokens, GGML_KQ_MASK_PAD), 1, 1);\n+    ggml_set_input(inp->self_kq_mask);\n+\n+    inp->self_kq_mask_cnv = cparams.flash_attn ? ggml_cast(ctx0, inp->self_kq_mask, GGML_TYPE_F16) : inp->self_kq_mask;\n \n-    inp->kq_mask_cnv = cparams.flash_attn ? ggml_cast(ctx0, inp->kq_mask, GGML_TYPE_F16) : inp->kq_mask;\n+    if (hparams.swa_type != LLAMA_SWA_TYPE_NONE) {\n+        inp->self_kq_mask_swa = ggml_new_tensor_4d(ctx0, GGML_TYPE_F32, n_tokens, GGML_PAD(n_tokens, GGML_KQ_MASK_PAD), 1, 1);\n+        ggml_set_input(inp->self_kq_mask_swa);\n+\n+        inp->self_kq_mask_swa_cnv = cparams.flash_attn ? ggml_cast(ctx0, inp->self_kq_mask_swa, GGML_TYPE_F16) : inp->self_kq_mask_swa;\n+    } else {\n+        inp->self_kq_mask_swa     = nullptr;\n+        inp->self_kq_mask_swa_cnv = nullptr;\n+    }\n \n     return (llm_graph_input_attn_no_cache *) res->add_input(std::move(inp));\n }\n@@ -1447,7 +1477,9 @@ ggml_tensor * llm_graph_context::build_attn(\n     ggml_build_forward_expand(gf, k_cur);\n     ggml_build_forward_expand(gf, v_cur);\n \n-    const auto & kq_mask = inp->get_kq_mask();\n+    const bool is_swa = hparams.is_swa(il);\n+\n+    const auto & kq_mask = is_swa ? inp->get_kq_mask_swa() : inp->get_kq_mask();\n \n     // [TAG_NO_CACHE_PAD]\n     // TODO: if ubatch.equal_seqs() == true, we can split the three tensors below into ubatch.n_seqs_unq streams"
      },
      {
        "filename": "src/llama-graph.h",
        "status": "modified",
        "additions": 7,
        "deletions": 3,
        "changes": 10,
        "patch": "@@ -257,10 +257,14 @@ class llm_graph_input_attn_no_cache : public llm_graph_input_i {\n \n     void set_input(const llama_ubatch * ubatch) override;\n \n-    ggml_tensor * get_kq_mask() const { return kq_mask_cnv; }\n+    ggml_tensor * get_kq_mask()     const { return self_kq_mask_cnv; }\n+    ggml_tensor * get_kq_mask_swa() const { return self_kq_mask_swa_cnv; }\n \n-    ggml_tensor * kq_mask     = nullptr; // F32 [n_tokens, n_batch, 1, 1]\n-    ggml_tensor * kq_mask_cnv = nullptr; //     [n_tokens, n_batch, 1, 1]\n+    // n_tokens == n_batch\n+    ggml_tensor * self_kq_mask         = nullptr; // F32 [n_tokens, n_batch/n_stream, 1, n_stream]\n+    ggml_tensor * self_kq_mask_cnv     = nullptr; //     [n_tokens, n_batch/n_stream, 1, n_stream]\n+    ggml_tensor * self_kq_mask_swa     = nullptr; // F32 [n_tokens, n_batch/n_stream, 1, n_stream]\n+    ggml_tensor * self_kq_mask_swa_cnv = nullptr; //     [n_tokens, n_batch/n_stream, 1, n_stream]\n \n     const llama_hparams hparams;\n     const llama_cparams cparams;"
      },
      {
        "filename": "src/llama-model.cpp",
        "status": "modified",
        "additions": 5,
        "deletions": 6,
        "changes": 11,
        "patch": "@@ -11358,8 +11358,8 @@ struct llm_build_gemma3n_iswa : public llm_graph_context {\n     }\n };\n \n-struct llm_build_gemma_embedding_iswa : public llm_graph_context {\n-    llm_build_gemma_embedding_iswa(const llama_model & model, const llm_graph_params & params) : llm_graph_context(params) {\n+struct llm_build_gemma_embedding : public llm_graph_context {\n+    llm_build_gemma_embedding(const llama_model & model, const llm_graph_params & params) : llm_graph_context(params) {\n         const int64_t n_embd_head = hparams.n_embd_head_k;\n \n         ggml_tensor * cur;\n@@ -11376,8 +11376,7 @@ struct llm_build_gemma_embedding_iswa : public llm_graph_context {\n         // inp_pos - contains the positions\n         ggml_tensor * inp_pos = build_inp_pos();\n \n-        // TODO: support cacheless iSWA embeddings [TAG_NO_CACHE_ISWA]\n-        auto * inp_attn = build_attn_inp_kv_iswa();\n+        auto * inp_attn = build_attn_inp_no_cache();\n \n         ggml_tensor * inp_out_ids = build_inp_out_ids();\n \n@@ -19378,7 +19377,7 @@ llama_memory_i * llama_model::create_memory(const llama_memory_params & params,\n         case LLM_ARCH_NOMIC_BERT_MOE:\n         case LLM_ARCH_NEO_BERT:\n         case LLM_ARCH_WAVTOKENIZER_DEC:\n-        //case LLM_ARCH_GEMMA_EMBEDDING: // TODO: disabled until the cacheless SWA logic is fixed [TAG_NO_CACHE_ISWA]\n+        case LLM_ARCH_GEMMA_EMBEDDING:\n         case LLM_ARCH_DREAM:\n         case LLM_ARCH_LLADA:\n         case LLM_ARCH_LLADA_MOE:\n@@ -19671,7 +19670,7 @@ ggml_cgraph * llama_model::build_graph(const llm_graph_params & params) const {\n             } break;\n         case LLM_ARCH_GEMMA_EMBEDDING:\n             {\n-                llm = std::make_unique<llm_build_gemma_embedding_iswa>(*this, params);\n+                llm = std::make_unique<llm_build_gemma_embedding>(*this, params);\n             } break;\n         case LLM_ARCH_STARCODER2:\n             {"
      },
      {
        "filename": "src/llama.cpp",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -312,6 +312,7 @@ struct llama_model * llama_model_load_from_splits(\n         LLAMA_LOG_ERROR(\"%s: list of splits is empty\\n\", __func__);\n         return nullptr;\n     }\n+    splits.reserve(n_paths);\n     for (size_t i = 0; i < n_paths; ++i) {\n         splits.push_back(paths[i]);\n     }"
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T23:30:07.278894"
  },
  {
    "pr_number": 16526,
    "title": "common : handle unicode during partial json parsing",
    "body": "The JSON partial parsing does not handle partial Unicode escape sequences, causing exceptions when streaming. This PR adds logic to handle Unicode that aligns with the current JSON healing implementation.\r\n\r\nFixes #16465\r\n\r\n## Specifics\r\n\r\nA Unicode escape sequence comes in the form `\\uXXXX`. Most models do not have unique tokens for each Unicode code point, so they emit them in succession: `['\\\\', 'u', 'X', 'X', 'X', 'X']`. This breaks the JSON parsing and dumping operations used to form a partial arguments string.\r\n\r\nHere are the rules I apply to fix the generation so we properly apply a healing marker:\r\n\r\n1. If the last token is `\\`, the existing code handles it.\r\n2. If the last escape sequence is any variation of `\\u`, `\\uX`, `\\uXX`, `\\uXXX`, or `\\uXXXX`, I pad it with zeros until it becomes a complete sequence.\r\n3. If the last escape sequence matches a high surrogate (`U+D800-U+DBFF`), either partially or fully, I pad it as above and add a fake low surrogate (`U+DC00`).\r\n4. If a second-to-last high surrogate sequence exists adjacent to the last sequence, I pad the last sequence to form a valid low surrogate (`U+DC00`).\r\n5. There is a special case where a single backslash `\\` may follow a high surrogate. To handle this, the default padding is a valid low surrogate.\r\n6. I add the padding to the marker, so we know where to split the string to not include the padding.\r\n\r\n## Risks\r\n\r\nI tried my best to minimize any impact, since we use this logic through the code base. I only apply the Unicode logic when the parsing of the existing rules fails.\r\n\r\nHowever, I had to change the call to `j.dump()` to set `ensure_ascii = true`, otherwise it won't escape the Unicode sequences. Since Unicode handling is already unstable, I don't think this makes things _worse_.",
    "html_url": "https://github.com/ggml-org/llama.cpp/pull/16526",
    "created_at": "2025-10-12T05:27:02Z",
    "merged_at": "2025-10-12T13:18:47Z",
    "merge_commit_sha": "2c301e91abb92d03c1a682b4b540ba835562a74b",
    "base_ref": "master",
    "head_sha": "a4bfd9e89bbba60496108776bb40c42dd08d5148",
    "user": "aldehir",
    "files": [
      {
        "filename": "common/chat-parser.cpp",
        "status": "modified",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "patch": "@@ -432,7 +432,7 @@ std::optional<common_chat_msg_parser::consume_json_result> common_chat_msg_parse\n         if (is_arguments_path({})) {\n             // Entire JSON is the arguments and was parsed fully.\n             return consume_json_result {\n-                partial->json.dump(),\n+                partial->json.dump(/* indent */ -1, /* indent_char */ ' ', /* ensure_ascii */ true),\n                 /* .is_partial = */ false,\n             };\n         }\n@@ -444,7 +444,7 @@ std::optional<common_chat_msg_parser::consume_json_result> common_chat_msg_parse\n     std::vector<std::string> path;\n     std::function<json(const json &)> remove_unsupported_healings_and_dump_args = [&](const json & j) -> json {\n         if (is_arguments_path(path)) {\n-            auto arguments = j.dump();\n+            auto arguments = j.dump(/* indent */ -1, /* indent_char */ ' ', /* ensure_ascii */ true);\n             if (is_partial() && !partial->healing_marker.marker.empty()) {\n                 auto idx = arguments.find(partial->healing_marker.json_dump_marker);\n                 if (idx != std::string::npos) {"
      },
      {
        "filename": "common/json-partial.cpp",
        "status": "modified",
        "additions": 51,
        "deletions": 0,
        "changes": 51,
        "patch": "@@ -5,6 +5,7 @@\n #include <nlohmann/json.hpp>\n \n #include <string>\n+#include <regex>\n \n using json = nlohmann::ordered_json;\n \n@@ -168,6 +169,47 @@ bool common_json_parse(\n                 }\n             }\n \n+            // Matches a potentially partial unicode escape sequence, e.g. \\u, \\uX, \\uXX, \\uXXX, \\uXXXX\n+            static const std::regex partial_unicode_regex(R\"(\\\\u(?:[0-9a-fA-F](?:[0-9a-fA-F](?:[0-9a-fA-F](?:[0-9a-fA-F])?)?)?)?$)\");\n+\n+            auto is_high_surrogate = [&](const std::string & s) {\n+                // Check if a partial of a high surrogate (U+D800-U+DBFF)\n+                return s.length() >= 4 &&\n+                    s[0] == '\\\\' && s[1] == 'u' &&\n+                    std::tolower(s[2]) == 'd' &&\n+                    (s[3] == '8' || s[3] == '9' || std::tolower(s[3]) == 'a' || std::tolower(s[3]) == 'b');\n+            };\n+\n+            // Initialize the unicode marker to a low surrogate to handle the edge case\n+            // where a high surrogate (U+D800-U+DBFF) is immediately followed by a\n+            // backslash (\\)\n+            std::string unicode_marker_padding = \"udc00\";\n+            std::smatch last_unicode_seq;\n+\n+            if (std::regex_search(str, last_unicode_seq, partial_unicode_regex)) {\n+                std::smatch second_last_seq;\n+                std::string prelude = str.substr(0, last_unicode_seq.position());\n+\n+                // Pad the escape sequence with 0s until it forms a complete sequence of 6 characters\n+                unicode_marker_padding = std::string(6 - last_unicode_seq.length(), '0');\n+\n+                if (is_high_surrogate(last_unicode_seq.str())) {\n+                    // If the sequence is a partial match for a high surrogate, add a low surrogate (U+DC00-U+UDFF)\n+                    unicode_marker_padding += \"\\\\udc00\";\n+                } else if (std::regex_search(prelude, second_last_seq, partial_unicode_regex)) {\n+                    if (is_high_surrogate(second_last_seq.str())) {\n+                        // If this follows a high surrogate, pad it to be a low surrogate\n+                        if (last_unicode_seq.length() == 2) {\n+                            unicode_marker_padding = \"dc00\";\n+                        } else if (last_unicode_seq.length() == 3) {\n+                            unicode_marker_padding = \"c00\";\n+                        } else {\n+                            // The original unicode_marker_padding is already padded with 0s\n+                        }\n+                    }\n+                }\n+            }\n+\n             const auto & magic_seed = out.healing_marker.marker = healing_marker;//\"$llama.cpp.json$\";\n \n             if (err_loc.stack.back().type == COMMON_JSON_STACK_ELEMENT_KEY) {\n@@ -186,6 +228,9 @@ bool common_json_parse(\n                 } else if (str[str.length() - 1] == '\\\\' && can_parse(str + \"\\\\\\\"\" + closing)) {\n                     // Was inside an object value string after an escape\n                     str += (out.healing_marker.json_dump_marker = \"\\\\\" + magic_seed) + \"\\\"\" + closing;\n+                } else if (can_parse(str + unicode_marker_padding + \"\\\"\" + closing)) {\n+                    // Was inside an object value string after a partial unicode escape\n+                    str += (out.healing_marker.json_dump_marker = unicode_marker_padding + magic_seed) + \"\\\"\" + closing;\n                 } else {\n                     // find last :\n                     auto last_pos = str.find_last_of(':');\n@@ -205,6 +250,9 @@ bool common_json_parse(\n                 } else if (str[str.length() - 1] == '\\\\' && can_parse(str + \"\\\\\\\"\" + closing)) {\n                     // Was inside an array value string after an escape\n                     str += (out.healing_marker.json_dump_marker = \"\\\\\" + magic_seed) + \"\\\"\" + closing;\n+                } else if (can_parse(str + unicode_marker_padding + \"\\\"\" + closing)) {\n+                    // Was inside an array value string after a partial unicode escape\n+                    str += (out.healing_marker.json_dump_marker = unicode_marker_padding + magic_seed) + \"\\\"\" + closing;\n                 } else if (!was_maybe_number() && can_parse(str + \", 1\" + closing)) {\n                     // Had just finished a value\n                     str += (out.healing_marker.json_dump_marker = \",\\\"\" + magic_seed) + \"\\\"\" + closing;\n@@ -230,6 +278,9 @@ bool common_json_parse(\n                 } else if (str[str.length() - 1] == '\\\\' && can_parse(str + \"\\\\\\\": 1\" + closing)) {\n                     // Was inside an object key string after an escape\n                     str += (out.healing_marker.json_dump_marker = \"\\\\\" + magic_seed) + \"\\\": 1\" + closing;\n+                } else if (can_parse(str + unicode_marker_padding + \"\\\": 1\" + closing)) {\n+                    // Was inside an object key string after a partial unicode escape\n+                    str += (out.healing_marker.json_dump_marker = unicode_marker_padding + magic_seed) + \"\\\": 1\" + closing;\n                 } else {\n                     auto last_pos = str.find_last_of(':');\n                     if (last_pos == std::string::npos) {"
      },
      {
        "filename": "tests/test-chat-parser.cpp",
        "status": "modified",
        "additions": 58,
        "deletions": 0,
        "changes": 58,
        "patch": "@@ -524,6 +524,64 @@ static void test_json_with_dumped_args() {\n     R\"({\"foo\": \"bar\", \"args\": {\"arg1\": [)\",\n     R\"({\"foo\":\"bar\",\"args\":\"{\\\"arg1\\\":[\"})\"\n   );\n+\n+  // Unicode tests\n+  test_with_args(\n+    R\"({\"foo\": \"bar\", \"args\": {\"arg1\": \"\\u)\",\n+    R\"({\"foo\":\"bar\",\"args\":\"{\\\"arg1\\\":\\\"\\\\u\"})\"\n+  );\n+  test_with_args(\n+    R\"({\"foo\": \"bar\", \"args\": {\"arg1\": \"\\u0)\",\n+    R\"({\"foo\":\"bar\",\"args\":\"{\\\"arg1\\\":\\\"\\\\u0\"})\"\n+  );\n+  test_with_args(\n+    R\"({\"foo\": \"bar\", \"args\": {\"arg1\": \"\\u00)\",\n+    R\"({\"foo\":\"bar\",\"args\":\"{\\\"arg1\\\":\\\"\\\\u00\"})\"\n+  );\n+  test_with_args(\n+    R\"({\"foo\": \"bar\", \"args\": {\"arg1\": \"\\u000)\",\n+    R\"({\"foo\":\"bar\",\"args\":\"{\\\"arg1\\\":\\\"\\\\u000\"})\"\n+  );\n+  test_with_args(\n+    R\"({\"foo\": \"bar\", \"args\": {\"arg1\": \"\\u0000)\",\n+    R\"({\"foo\":\"bar\",\"args\":\"{\\\"arg1\\\":\\\"\\\\u0000\"})\"\n+  );\n+  test_with_args(\n+    R\"({\"foo\": \"bar\", \"args\": {\"arg1\": \"\\ud8)\",\n+    R\"({\"foo\":\"bar\",\"args\":\"{\\\"arg1\\\":\\\"\\\\ud8\"})\"\n+  );\n+  test_with_args(\n+    R\"({\"foo\": \"bar\", \"args\": {\"arg1\": \"\\ud80)\",\n+    R\"({\"foo\":\"bar\",\"args\":\"{\\\"arg1\\\":\\\"\\\\ud80\"})\"\n+  );\n+  test_with_args(\n+    R\"({\"foo\": \"bar\", \"args\": {\"arg1\": \"\\ud800)\",\n+    R\"({\"foo\":\"bar\",\"args\":\"{\\\"arg1\\\":\\\"\\\\ud800\"})\"\n+  );\n+  test_with_args(\n+    R\"({\"foo\": \"bar\", \"args\": {\"arg1\": \"\\ud800\\)\",\n+    R\"({\"foo\":\"bar\",\"args\":\"{\\\"arg1\\\":\\\"\\\\ud800\\\\\"})\"\n+  );\n+  test_with_args(\n+    R\"({\"foo\": \"bar\", \"args\": {\"arg1\": \"\\ud800\\u)\",\n+    R\"({\"foo\":\"bar\",\"args\":\"{\\\"arg1\\\":\\\"\\\\ud800\\\\u\"})\"\n+  );\n+  test_with_args(\n+    R\"({\"foo\": \"bar\", \"args\": {\"arg1\": \"\\ud800\\ud)\",\n+    R\"({\"foo\":\"bar\",\"args\":\"{\\\"arg1\\\":\\\"\\\\ud800\\\\ud\"})\"\n+  );\n+  test_with_args(\n+    R\"({\"foo\": \"bar\", \"args\": {\"arg1\": \"\\ud800\\udc)\",\n+    R\"({\"foo\":\"bar\",\"args\":\"{\\\"arg1\\\":\\\"\\\\ud800\\\\udc\"})\"\n+  );\n+  test_with_args(\n+    R\"({\"foo\": \"bar\", \"args\": {\"arg1\": \"\\ud800\\udc0)\",\n+    R\"({\"foo\":\"bar\",\"args\":\"{\\\"arg1\\\":\\\"\\\\ud800\\\\udc0\"})\"\n+  );\n+  test_with_args(\n+    R\"({\"foo\": \"bar\", \"args\": {\"arg1\": \"\\ud800\\udc00)\",\n+    R\"({\"foo\":\"bar\",\"args\":\"{\\\"arg1\\\":\\\"\\\\ud800\\\\udc00\"})\"\n+  );\n }\n \n static void test_positions() {"
      },
      {
        "filename": "tests/test-json-partial.cpp",
        "status": "modified",
        "additions": 51,
        "deletions": 1,
        "changes": 52,
        "patch": "@@ -58,7 +58,7 @@ static void test_json_healing() {\n       for (const auto & input : inputs) {\n         common_json out;\n         assert_equals(true, common_json_parse(input, \"$foo\", out));\n-        assert_equals<std::string>(expected, out.json.dump());\n+        assert_equals<std::string>(expected, out.json.dump(/* indent */ -1, /* indent_char */ ' ', /* ensure_ascii */ true));\n         assert_equals<std::string>(expected_marker, out.healing_marker.json_dump_marker);\n       }\n   };\n@@ -228,6 +228,56 @@ static void test_json_healing() {\n     R\"({\"key\":\"$foo\"})\",\n     R\"(:\"$foo)\"\n   );\n+  // Test unicode escape sequences\n+  test(\n+    {\n+      R\"({\"a\":\"\\u)\",\n+    },\n+    R\"({\"a\":\"\\u0000$foo\"})\",\n+    R\"(0000$foo)\"\n+  );\n+  test(\n+    {\n+      R\"({\"a\":\"\\u00)\",\n+    },\n+    R\"({\"a\":\"\\u0000$foo\"})\",\n+    R\"(00$foo)\"\n+  );\n+  test(\n+    {\n+      R\"({\"a\":\"\\ud300)\",\n+    },\n+    R\"({\"a\":\"\\ud300$foo\"})\",\n+    R\"($foo)\"\n+  );\n+  test(\n+    {\n+      R\"({\"a\":\"\\ud800)\",\n+    },\n+    R\"({\"a\":\"\\ud800\\udc00$foo\"})\",\n+    R\"(\\udc00$foo)\"\n+  );\n+  test(\n+    {\n+      R\"({\"a\":\"\\ud800\\)\",\n+    },\n+    R\"({\"a\":\"\\ud800\\udc00$foo\"})\",\n+    R\"(udc00$foo)\"\n+  );\n+  test(\n+    {\n+      R\"({\"a\":\"\\ud800\\u)\",\n+    },\n+    R\"({\"a\":\"\\ud800\\udc00$foo\"})\",\n+    R\"(dc00$foo)\"\n+  );\n+  test(\n+    {\n+      R\"({\"a\":\"\\ud800\\udc00)\",\n+    },\n+    R\"({\"a\":\"\\ud800\\udc00$foo\"})\",\n+    R\"($foo)\"\n+  );\n }\n \n int main() {"
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T23:30:07.553901"
  },
  {
    "pr_number": 16521,
    "title": "[SYCL] fix UT fault cases: count-equal, argsort, pad OPs",
    "body": "Fix UT fault cases: \r\n- count-equal\r\n- argsort\r\n- pad\r\nUpdate the OP list of SYCL backend.\r\n\r\nMake all supported UT cases are passed on Arc770 now.\r\nAfter this PR is merged, SYCL backend should have a new good base.\r\n\r\nI will test all PRs by the UT cases on Arc770 in the future.\r\nAny fault case will block the new PR merge or ask to revert the merged PR.\r\n\r\nIn the first half year, we focus on the performance and new technology.\r\nThe result is not good as we expected.\r\nI think we should focus on the quality firstly:\r\n  make a good design base with 100% UT cases.\r\n  track the PR quality: 100% offline UT pass rate.\r\n  implement all unsupported OPs.\r\n  fix the issues reported by users.\r\n",
    "html_url": "https://github.com/ggml-org/llama.cpp/pull/16521",
    "created_at": "2025-10-11T15:48:58Z",
    "merged_at": "2025-10-12T13:53:35Z",
    "merge_commit_sha": "c7be9febcbafa9af7d1b9443f86475c59c9c5f87",
    "base_ref": "master",
    "head_sha": "6650fffe60b5f218a39be4a2be225cf5fa882ef3",
    "user": "NeoZhangJianyu",
    "files": [
      {
        "filename": "docs/ops.md",
        "status": "modified",
        "additions": 10,
        "deletions": 8,
        "changes": 18,
        "patch": "@@ -31,7 +31,7 @@ Legend:\n |                CONV_TRANSPOSE_1D | \u274c | \u2705 | \u2705 | \u2705 | \u2705 | \u274c | \u2705 | \u2705 | \u274c |\n |                CONV_TRANSPOSE_2D | \u274c | \u274c | \u2705 | \u2705 | \u274c | \u274c | \u274c | \u274c | \u274c |\n |                              COS | \u274c | \u2705 | \u2705 | \u2705 | \ud83d\udfe1 | \u274c | \u2705 | \ud83d\udfe1 | \u274c |\n-|                      COUNT_EQUAL | \u274c | \u2705 | \u2705 | \u2705 | \u274c | \u274c | \u274c | \u2705 | \u274c |\n+|                      COUNT_EQUAL | \u274c | \u2705 | \u2705 | \u2705 | \u274c | \u274c | \u2705 | \u2705 | \u274c |\n |                              CPY | \u274c | \ud83d\udfe1 | \ud83d\udfe1 | \ud83d\udfe1 | \ud83d\udfe1 | \ud83d\udfe1 | \ud83d\udfe1 | \ud83d\udfe1 | \u274c |\n |               CROSS_ENTROPY_LOSS | \u274c | \u274c | \u2705 | \u2705 | \u274c | \u274c | \u274c | \u274c | \u274c |\n |          CROSS_ENTROPY_LOSS_BACK | \u274c | \u274c | \u2705 | \u2705 | \u274c | \u274c | \u274c | \u274c | \u274c |\n@@ -51,7 +51,7 @@ Legend:\n |                         GET_ROWS | \u274c | \ud83d\udfe1 | \u2705 | \ud83d\udfe1 | \u2705 | \ud83d\udfe1 | \ud83d\udfe1 | \ud83d\udfe1 | \u274c |\n |                    GET_ROWS_BACK | \u274c | \u274c | \ud83d\udfe1 | \ud83d\udfe1 | \u274c | \u274c | \u274c | \u274c | \u274c |\n |                       GROUP_NORM | \u274c | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u274c |\n-|               GROUP_NORM_MUL_ADD | \u274c | \u274c | \u274c | \u274c | \u274c | \u274c | \u274c | \u274c | \u274c |\n+|               GROUP_NORM_MUL_ADD | \u274c | \u274c | \u274c | \u274c | \u274c | \u274c | \u2705 | \u274c | \u274c |\n |                      HARDSIGMOID | \u274c | \u2705 | \u2705 | \ud83d\udfe1 | \ud83d\udfe1 | \u274c | \ud83d\udfe1 | \u274c | \u274c |\n |                        HARDSWISH | \u274c | \u2705 | \u2705 | \ud83d\udfe1 | \ud83d\udfe1 | \u274c | \ud83d\udfe1 | \u274c | \u274c |\n |                           IM2COL | \u274c | \u2705 | \u2705 | \u2705 | \ud83d\udfe1 | \u2705 | \u2705 | \u2705 | \u274c |\n@@ -65,11 +65,11 @@ Legend:\n |                       MUL_MAT_ID | \u274c | \ud83d\udfe1 | \u2705 | \u2705 | \u2705 | \ud83d\udfe1 | \ud83d\udfe1 | \u2705 | \u274c |\n |                              NEG | \u274c | \u2705 | \u2705 | \ud83d\udfe1 | \ud83d\udfe1 | \u274c | \ud83d\udfe1 | \u274c | \u274c |\n |                             NORM | \u274c | \u2705 | \u2705 | \u2705 | \ud83d\udfe1 | \u2705 | \u2705 | \ud83d\udfe1 | \u274c |\n-|                     NORM_MUL_ADD | \u274c | \u274c | \u274c | \u274c | \u274c | \u274c | \u274c | \u274c | \u274c |\n+|                     NORM_MUL_ADD | \u274c | \u274c | \u274c | \u274c | \u274c | \u274c | \u2705 | \u274c | \u274c |\n |                   OPT_STEP_ADAMW | \u274c | \u274c | \u2705 | \u2705 | \u274c | \u274c | \u274c | \u2705 | \u274c |\n |                     OPT_STEP_SGD | \u274c | \u274c | \u274c | \u274c | \u274c | \u274c | \u274c | \u274c | \u274c |\n |                         OUT_PROD | \ud83d\udfe1 | \u274c | \ud83d\udfe1 | \ud83d\udfe1 | \u274c | \u274c | \ud83d\udfe1 | \u274c | \u274c |\n-|                              PAD | \u274c | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u274c |\n+|                              PAD | \u274c | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \ud83d\udfe1 | \u2705 | \u274c |\n |                   PAD_REFLECT_1D | \u274c | \u2705 | \u2705 | \u274c | \u2705 | \u274c | \u274c | \u274c | \u274c |\n |                          POOL_2D | \u274c | \ud83d\udfe1 | \u2705 | \u2705 | \u2705 | \u274c | \u2705 | \u2705 | \u274c |\n |                            REGLU | \u274c | \u2705 | \u2705 | \u2705 | \ud83d\udfe1 | \u2705 | \u2705 | \ud83d\udfe1 | \u274c |\n@@ -92,19 +92,21 @@ Legend:\n |                             SILU | \u274c | \u2705 | \u2705 | \ud83d\udfe1 | \ud83d\udfe1 | \ud83d\udfe1 | \ud83d\udfe1 | \ud83d\udfe1 | \u274c |\n |                        SILU_BACK | \u274c | \u274c | \u2705 | \u2705 | \u274c | \u274c | \u274c | \u2705 | \u274c |\n |                              SIN | \u274c | \u2705 | \u2705 | \u2705 | \ud83d\udfe1 | \u274c | \u2705 | \ud83d\udfe1 | \u274c |\n-|                          SOFTCAP | \u274c | \u274c | \u274c | \u274c | \u274c | \u274c | \u274c | \u274c | \u274c |\n-|                         SOFT_MAX | \u274c | \ud83d\udfe1 | \u2705 | \u2705 | \u2705 | \u2705 | \ud83d\udfe1 | \u2705 | \u274c |\n-|                    SOFT_MAX_BACK | \u274c | \u274c | \ud83d\udfe1 | \ud83d\udfe1 | \u274c | \u274c | \u274c | \u2705 | \u274c |\n+|                          SOFTCAP | \u274c | \u274c | \u274c | \u274c | \u274c | \u274c | \u2705 | \u274c | \u274c |\n+|                         SOFT_MAX | \u274c | \ud83d\udfe1 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u274c |\n+|                    SOFT_MAX_BACK | \u274c | \u274c | \ud83d\udfe1 | \ud83d\udfe1 | \u274c | \u274c | \ud83d\udfe1 | \u2705 | \u274c |\n |                              SQR | \u274c | \u2705 | \u2705 | \u2705 | \ud83d\udfe1 | \u274c | \u2705 | \ud83d\udfe1 | \u274c |\n |                             SQRT | \u274c | \u2705 | \u2705 | \u2705 | \ud83d\udfe1 | \u274c | \u2705 | \u274c | \u274c |\n |                         SSM_CONV | \u274c | \u274c | \u2705 | \u2705 | \u2705 | \u274c | \u274c | \u274c | \u274c |\n |                         SSM_SCAN | \u274c | \u274c | \u2705 | \u2705 | \u2705 | \u274c | \u274c | \u274c | \u274c |\n |                             STEP | \u274c | \u2705 | \u2705 | \ud83d\udfe1 | \ud83d\udfe1 | \u274c | \ud83d\udfe1 | \u274c | \u274c |\n |                              SUB | \u274c | \u2705 | \u2705 | \u2705 | \ud83d\udfe1 | \ud83d\udfe1 | \u2705 | \u2705 | \u274c |\n |                              SUM | \u274c | \u2705 | \u2705 | \u2705 | \u274c | \u274c | \u2705 | \u2705 | \u274c |\n-|                         SUM_ROWS | \u274c | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u274c |\n+|                         SUM_ROWS | \u274c | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \ud83d\udfe1 | \u2705 | \u274c |\n |                           SWIGLU | \u274c | \u2705 | \u2705 | \u2705 | \ud83d\udfe1 | \u2705 | \u2705 | \ud83d\udfe1 | \u274c |\n |                       SWIGLU_OAI | \u274c | \u274c | \u274c | \u274c | \u274c | \u274c | \u274c | \u274c | \u274c |\n |                             TANH | \u274c | \u2705 | \u2705 | \ud83d\udfe1 | \ud83d\udfe1 | \u2705 | \ud83d\udfe1 | \ud83d\udfe1 | \u274c |\n |               TIMESTEP_EMBEDDING | \u274c | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u274c |\n+|                         TOPK_MOE | \u274c | \u274c | \u274c | \u274c | \u274c | \u274c | \u2705 | \u274c | \u274c |\n |                          UPSCALE | \u274c | \ud83d\udfe1 | \u2705 | \u2705 | \ud83d\udfe1 | \u2705 | \ud83d\udfe1 | \u2705 | \u274c |\n+|                            XIELU | \u274c | \u274c | \u274c | \u274c | \u274c | \u274c | \u274c | \u274c | \u274c |"
      },
      {
        "filename": "docs/ops/SYCL.csv",
        "status": "modified",
        "additions": 12095,
        "deletions": 4249,
        "changes": 16344,
        "patch": ""
      },
      {
        "filename": "ggml/src/ggml-sycl/backend.hpp",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -18,6 +18,7 @@\n #include \"concat.hpp\"\n #include \"conv.hpp\"\n #include \"convert.hpp\"\n+#include \"count-equal.hpp\"\n #include \"cpy.hpp\"\n #include \"dequantize.hpp\"\n #include \"dmmv.hpp\"\n@@ -28,6 +29,7 @@\n #include \"mmvq.hpp\"\n #include \"norm.hpp\"\n #include \"outprod.hpp\"\n+#include \"pad.hpp\"\n #include \"quantize.hpp\"\n #include \"quants.hpp\"\n #include \"rope.hpp\""
      },
      {
        "filename": "ggml/src/ggml-sycl/binbcast.cpp",
        "status": "modified",
        "additions": 0,
        "deletions": 9,
        "changes": 9,
        "patch": "@@ -303,10 +303,6 @@ inline void ggml_sycl_op_sub(ggml_backend_sycl_context & ctx, ggml_tensor *dst)\n     ggml_sycl_op_bin_bcast<bin_bcast_sycl<op_sub>>(ctx, dst->src[0], dst->src[1], dst);\n }\n \n-inline void ggml_sycl_op_count_equal(ggml_backend_sycl_context & ctx, ggml_tensor * dst) {\n-    ggml_sycl_op_bin_bcast<bin_bcast_sycl<op_count_equal>>(ctx, dst->src[0], dst->src[1], dst);\n-}\n-\n inline void ggml_sycl_op_mul(ggml_backend_sycl_context & ctx, ggml_tensor *dst) {\n \n     ggml_sycl_op_bin_bcast<bin_bcast_sycl<op_mul>>(ctx, dst->src[0], dst->src[1], dst);\n@@ -332,11 +328,6 @@ void ggml_sycl_sub(ggml_backend_sycl_context & ctx, ggml_tensor * dst) {\n     ggml_sycl_op_sub(ctx, dst);\n }\n \n-void ggml_sycl_count_equal(ggml_backend_sycl_context & ctx, ggml_tensor * dst) {\n-    scope_op_debug_print scope_dbg_print(__func__, dst, /*num_src=*/2);\n-    ggml_sycl_op_count_equal(ctx, dst);\n-}\n-\n void ggml_sycl_mul(ggml_backend_sycl_context & ctx, ggml_tensor * dst) {\n     scope_op_debug_print scope_dbg_print(__func__, dst, /*num_src=*/2);\n     ggml_sycl_op_mul(ctx, dst);"
      },
      {
        "filename": "ggml/src/ggml-sycl/binbcast.hpp",
        "status": "modified",
        "additions": 0,
        "deletions": 6,
        "changes": 6,
        "patch": "@@ -16,12 +16,6 @@ static __dpct_inline__ float op_sub(const float a, const float b) {\n     return a - b;\n }\n \n-static __dpct_inline__ float op_count_equal(const float a, const float b) {\n-    return (a == b) ? 1.0f : 0.0f;\n-}\n-\n-void ggml_sycl_count_equal(ggml_backend_sycl_context & ctx, ggml_tensor * dst);\n-\n static __dpct_inline__ float op_mul(const float a, const float b) {\n     return a * b;\n }"
      },
      {
        "filename": "ggml/src/ggml-sycl/common.hpp",
        "status": "modified",
        "additions": 2,
        "deletions": 1,
        "changes": 3,
        "patch": "@@ -195,7 +195,8 @@ struct optimize_feature {\n \n struct sycl_device_info {\n     int     cc;                 // compute capability\n-    // int     nsm;                // number of streaming multiprocessors\n+    int nsm; // number of streaming multiprocessors (CUDA) maps to the maximum\n+             // number of compute units on a SYCL device.\n     // size_t  smpb;               // max. shared memory per block\n     size_t  smpbo;              // max. shared memory per block (with opt-in)\n     bool    vmm;                // virtual memory support"
      },
      {
        "filename": "ggml/src/ggml-sycl/count-equal.cpp",
        "status": "added",
        "additions": 79,
        "deletions": 0,
        "changes": 79,
        "patch": "@@ -0,0 +1,79 @@\n+#include \"count-equal.hpp\"\n+\n+#include <cstdint>\n+\n+template <typename T>\n+static void count_equal(const T *__restrict__ x, const T *__restrict__ y,\n+                        int64_t *__restrict__ dst, const int64_t dk,\n+                        const int64_t k) {\n+    auto item_ct1 = sycl::ext::oneapi::this_work_item::get_nd_item<3>();\n+    const int64_t i0 = (int64_t)item_ct1.get_group(2) * dk;\n+    const int64_t i1 = sycl::min(i0 + dk, k);\n+\n+    int nequal = 0;\n+\n+    for (int64_t i = i0 + item_ct1.get_local_id(2); i < i1; i += WARP_SIZE) {\n+        const T xi = x[i];\n+        const T yi = y[i];\n+        nequal += xi == yi;\n+    }\n+\n+    nequal = warp_reduce_sum(nequal);\n+\n+    if (item_ct1.get_local_id(2) != 0) {\n+        return;\n+    }\n+\n+    dpct::atomic_fetch_add<sycl::access::address_space::generic_space>(\n+        (int *)dst, nequal);\n+}\n+\n+void ggml_sycl_count_equal(ggml_backend_sycl_context &ctx, ggml_tensor *dst) {\n+    scope_op_debug_print scope_dbg_print(__func__, dst, /*num_src=*/2);\n+    const ggml_tensor * src0 = dst->src[0];\n+    const ggml_tensor * src1 = dst->src[1];\n+\n+    GGML_ASSERT(src0->type == src1->type);\n+    GGML_ASSERT( dst->type == GGML_TYPE_I64);\n+\n+    GGML_ASSERT(ggml_are_same_shape(src0, src1));\n+    GGML_ASSERT(ggml_is_contiguous(src0));\n+    GGML_ASSERT(ggml_is_contiguous(src1));\n+    GGML_ASSERT(ggml_is_contiguous(dst));\n+\n+    int64_t * dst_d  = (int64_t *) dst->data;\n+\n+    dpct::queue_ptr stream = ctx.stream();\n+    const int id       = get_current_device_id();\n+    const int nsm = ggml_sycl_info().devices[id].nsm;\n+\n+    const int64_t ne = ggml_nelements(src0);\n+    GGML_ASSERT(ne < (1 << 30) && \"atomicAdd implementation only supports int\");\n+    const int64_t dne =\n+        GGML_PAD((ne + 4 * nsm - 1) / (4 * nsm), SYCL_COUNT_EQUAL_CHUNK_SIZE);\n+\n+    SYCL_CHECK(CHECK_TRY_ERROR(stream->memset(dst_d, 0, ggml_nbytes(dst))));\n+\n+    const dpct::dim3 block_dims(WARP_SIZE, 1, 1);\n+    const dpct::dim3 block_nums(\n+        std::min((int64_t)4 * nsm, (ne + SYCL_COUNT_EQUAL_CHUNK_SIZE - 1) /\n+                                       SYCL_COUNT_EQUAL_CHUNK_SIZE),\n+        1, 1);\n+\n+    switch (src0->type) {\n+    case GGML_TYPE_I32: {\n+        const int *src0_d = (const int *)src0->data;\n+        const int *src1_d = (const int *)src1->data;\n+        stream->parallel_for(\n+            sycl::nd_range<3>(block_nums * block_dims, block_dims),\n+            [=](sycl::nd_item<3> item_ct1) {\n+                count_equal(src0_d, src1_d, dst_d, dne, ne);\n+                GGML_UNUSED(item_ct1);\n+            });\n+\n+    } break;\n+    default:\n+        GGML_ASSERT(false);\n+        break;\n+    }\n+}"
      },
      {
        "filename": "ggml/src/ggml-sycl/count-equal.hpp",
        "status": "added",
        "additions": 9,
        "deletions": 0,
        "changes": 9,
        "patch": "@@ -0,0 +1,9 @@\n+#ifndef GGML_SYCL_COUNT_EQUAL_HPP\n+#define GGML_SYCL_COUNT_EQUAL_HPP\n+#include \"common.hpp\"\n+\n+#define SYCL_COUNT_EQUAL_CHUNK_SIZE 128\n+\n+void ggml_sycl_count_equal(ggml_backend_sycl_context & ctx, ggml_tensor * dst);\n+\n+#endif //GGML_SYCL_COUNT_EQUAL_HPP"
      },
      {
        "filename": "ggml/src/ggml-sycl/element_wise.cpp",
        "status": "modified",
        "additions": 0,
        "deletions": 78,
        "changes": 78,
        "patch": "@@ -328,26 +328,6 @@ static void upscale(const T  *x, T *dst, const int nb00, const int nb01,\n     dst[index] = *(const T *)((const char *)x + i03 * nb03 + i02 * nb02 + i01 * nb01 + i00 * nb00);\n }\n \n-template <typename T>\n-static void pad(const T  *x, T *dst, const int ne0, const int ne00, const int ne01, const int ne02,\n-                    const sycl::nd_item<3> &item_ct1) {\n-    int nidx = SYCL_LOCAL_ID_CALC(item_ct1, 2);\n-    if (nidx >= ne0) {\n-        return;\n-    }\n-\n-    // operation\n-    int offset_dst = nidx + item_ct1.get_group(1) * ne0 +\n-                     item_ct1.get_group(0) * ne0 * item_ct1.get_group_range(1);\n-    if (nidx < ne00 && item_ct1.get_group(1) < (size_t) ne01 && item_ct1.get_group(0) < (size_t) ne02) {\n-        int offset_src = nidx + item_ct1.get_group(1) * ne00 +\n-                         item_ct1.get_group(0) * ne00 * ne01;\n-            dst[offset_dst] = x[offset_src];\n-    } else {\n-        dst[offset_dst] = static_cast<T>(0.0f);\n-    }\n-}\n-\n template<typename T>\n static void clamp(const T * x, T * dst, const float min, const float max, const int k,\n                       const sycl::nd_item<1> &item_ct1) {\n@@ -431,18 +411,6 @@ static void upscale_sycl(const T *x, T *dst, const int nb00, const int nb01,\n         });\n }\n \n-template<typename T>\n-static void pad_sycl(const T *x, T *dst, const int ne00,\n-                         const int ne01, const int ne02, const int ne0,\n-                         const int ne1, const int ne2, queue_ptr stream) {\n-    int num_blocks = ceil_div(ne0, SYCL_PAD_BLOCK_SIZE);\n-    sycl::range<3> gridDim(ne2, ne1, num_blocks);\n-    stream->parallel_for(\n-                      sycl::nd_range<3>(gridDim * sycl::range<3>(1, 1, SYCL_PAD_BLOCK_SIZE),\n-                                        sycl::range<3>(1, 1, SYCL_PAD_BLOCK_SIZE)),\n-                      [=](sycl::nd_item<3> item_ct1) { pad(x, dst, ne0, ne00, ne01, ne02, item_ct1); });\n-}\n-\n template<typename KernelInvoker, typename... Args>\n static inline void dispatch_ggml_sycl_op_unary(ggml_backend_sycl_context & ctx, ggml_tensor * dst, KernelInvoker kernel_invoker, Args&&... args) {\n #if defined (GGML_SYCL_F16)\n@@ -596,40 +564,6 @@ static inline void dispatch_ggml_sycl_op_upscale(ggml_backend_sycl_context & ctx\n     }\n }\n \n-template<typename KernelInvoker, typename... Args>\n-static inline void dispatch_ggml_sycl_op_pad(ggml_backend_sycl_context & ctx, ggml_tensor * dst, KernelInvoker kernel_invoker, Args&&... args) {\n-#if defined (GGML_SYCL_F16)\n-    GGML_ASSERT(dst->src[0]->type == GGML_TYPE_F32 || dst->src[0]->type == GGML_TYPE_F16);\n-    GGML_ASSERT(dst->type == GGML_TYPE_F32 || dst->type == GGML_TYPE_F16);\n-#else\n-    GGML_ASSERT(dst->src[0]->type == GGML_TYPE_F32);\n-    GGML_ASSERT(dst->type == GGML_TYPE_F32);\n-#endif\n-    GGML_ASSERT(dst->src[0]->type == dst->type);\n-    GGML_ASSERT(dst->src[0]->ne[3] == 1 && dst->ne[3] == 1); // just 3D tensors\n-    dpct::queue_ptr main_stream = ctx.stream();\n-    SYCL_CHECK(ggml_sycl_set_device(ctx.device));\n-    switch (dst->type) {\n-#if defined (GGML_SYCL_F16)\n-        case GGML_TYPE_F16:\n-            {\n-                auto data_pts = cast_data<sycl::half>(dst);\n-                kernel_invoker(data_pts.src, data_pts.dst, (int)dst->src[0]->ne[0], (int)dst->src[0]->ne[1], (int)dst->src[0]->ne[2], (int)dst->ne[0],\n-                               (int)dst->ne[1], (int)dst->ne[2], main_stream, std::forward<Args>(args)...);\n-                break;\n-            }\n-#endif\n-        case GGML_TYPE_F32:\n-            {\n-                auto data_pts = cast_data<float>(dst);\n-                kernel_invoker(data_pts.src, data_pts.dst, (int)dst->src[0]->ne[0], (int)dst->src[0]->ne[1], (int)dst->src[0]->ne[2], (int)dst->ne[0],\n-                               (int)dst->ne[1], (int)dst->ne[2], main_stream, std::forward<Args>(args)...);\n-                break;\n-            }\n-        default:\n-            GGML_ABORT(\"GGML tensor type not supported!\\n\");\n-    }\n-}\n \n } // namespace ggml_sycl_detail\n \n@@ -919,14 +853,6 @@ static inline void ggml_sycl_op_upscale(ggml_backend_sycl_context & ctx, ggml_te\n         });\n }\n \n-static inline void ggml_sycl_op_pad(ggml_backend_sycl_context & ctx, ggml_tensor * dst) {\n-    ggml_sycl_detail::dispatch_ggml_sycl_op_pad(ctx, dst,\n-        [](const auto* src, auto* dst_ptr, int ne00, int ne01, int ne02, int ne0, int ne1, int ne2,\n-           queue_ptr stream) {\n-            ggml_sycl_detail::pad_sycl(src, dst_ptr, ne00, ne01, ne02, ne0, ne1, ne2, stream);\n-        });\n-}\n-\n static inline void ggml_sycl_op_clamp(ggml_backend_sycl_context & ctx, ggml_tensor * dst) {\n     float min_val;\n     float max_val;\n@@ -1119,10 +1045,6 @@ void ggml_sycl_upscale(ggml_backend_sycl_context & ctx, ggml_tensor * dst) {\n     ggml_sycl_op_upscale(ctx, dst);\n }\n \n-void ggml_sycl_pad(ggml_backend_sycl_context & ctx, ggml_tensor * dst) {\n-    scope_op_debug_print scope_dbg_print(__func__, dst, /*num_src=*/1);\n-    ggml_sycl_op_pad(ctx, dst);\n-}\n \n void ggml_sycl_clamp(ggml_backend_sycl_context & ctx, ggml_tensor * dst) {\n     scope_op_debug_print scope_dbg_print(__func__, dst, /*num_src=*/1);"
      },
      {
        "filename": "ggml/src/ggml-sycl/element_wise.hpp",
        "status": "modified",
        "additions": 0,
        "deletions": 2,
        "changes": 2,
        "patch": "@@ -67,8 +67,6 @@ void ggml_sycl_sqr(ggml_backend_sycl_context & ctx, ggml_tensor * dst);\n \n void ggml_sycl_upscale(ggml_backend_sycl_context & ctx, ggml_tensor * dst);\n \n-void ggml_sycl_pad(ggml_backend_sycl_context & ctx, ggml_tensor * dst);\n-\n void ggml_sycl_clamp(ggml_backend_sycl_context & ctx, ggml_tensor * dst);\n \n void ggml_sycl_sgn(ggml_backend_sycl_context & ctx, ggml_tensor * dst);"
      },
      {
        "filename": "ggml/src/ggml-sycl/ggml-sycl.cpp",
        "status": "modified",
        "additions": 63,
        "deletions": 40,
        "changes": 103,
        "patch": "@@ -85,9 +85,11 @@ static ggml_sycl_device_info ggml_sycl_init() {\n \n         info.devices[i].cc =\n             100 * prop.get_major_version() + 10 * prop.get_minor_version();\n+        info.devices[i].nsm = prop.get_max_compute_units();\n         info.devices[i].opt_feature.reorder = device.ext_oneapi_architecture_is(syclex::arch_category::intel_gpu);\n-        info.max_work_group_sizes[i] = prop.get_max_work_group_size();\n         info.devices[i].smpbo = prop.get_local_mem_size();\n+\n+        info.max_work_group_sizes[i] = prop.get_max_work_group_size();\n     }\n \n     for (int id = 0; id < info.device_count; ++id) {\n@@ -1512,60 +1514,70 @@ static inline void ggml_sycl_swap(T & a, T & b) {\n template <ggml_sort_order order>\n __dpct_inline__ static void\n k_argsort_f32_i32(const float *x, int *dst, const int ncols, int ncols_pad,\n-                  const sycl::nd_item<3> &item_ct1, uint8_t *dpct_local) {\n+                  const int tasks_per_thread, const sycl::nd_item<3> &item_ct1,\n+                  uint8_t *dpct_local) {\n     // bitonic sort\n-    int col = item_ct1.get_local_id(2);\n+    int col_index =  item_ct1.get_local_id(2);\n     int row = item_ct1.get_group(1);\n \n-    if (col >= ncols_pad) {\n-        return;\n+    for (int i = 0; i < tasks_per_thread; i++) {\n+        int col = col_index * tasks_per_thread + i;\n+        if (col >= ncols_pad) {\n+            return;\n+        }\n     }\n \n     const float * x_row = x + row * ncols;\n     auto dst_row = (int *)dpct_local;\n \n     // initialize indices\n-    dst_row[col] = col;\n+    for (int i=0;i<tasks_per_thread;i++){\n+        int col = col_index*tasks_per_thread+i;\n+        dst_row[col] = col;\n+    }\n \n     item_ct1.barrier(sycl::access::fence_space::local_space);\n \n     for (int k = 2; k <= ncols_pad; k *= 2) {\n         for (int j = k / 2; j > 0; j /= 2) {\n-            int ixj = col ^ j;\n-            if (ixj > col) {\n-                if ((col & k) == 0) {\n-                    if (dst_row[col] >= ncols ||\n-                        (dst_row[ixj] < ncols && (order == GGML_SORT_ORDER_ASC ?\n-                            x_row[dst_row[col]] > x_row[dst_row[ixj]] :\n-                            x_row[dst_row[col]] < x_row[dst_row[ixj]]))\n-                    ) {\n-                        ggml_sycl_swap(dst_row[col], dst_row[ixj]);\n-                    }\n-                } else {\n-                    if (dst_row[ixj] >= ncols ||\n-                        (dst_row[col] < ncols && (order == GGML_SORT_ORDER_ASC ?\n-                            x_row[dst_row[col]] < x_row[dst_row[ixj]] :\n-                            x_row[dst_row[col]] > x_row[dst_row[ixj]]))\n-                    ) {\n-                        ggml_sycl_swap(dst_row[col], dst_row[ixj]);\n+            for (int i = 0; i < tasks_per_thread; i++) {\n+                int col = col_index * tasks_per_thread + i;\n+                int ixj = col ^ j;\n+                if (ixj > col) {\n+                    if ((col & k) == 0) {\n+                        if (dst_row[col] >= ncols ||\n+                            (dst_row[ixj] < ncols &&\n+                             (order == GGML_SORT_ORDER_ASC\n+                                  ? x_row[dst_row[col]] > x_row[dst_row[ixj]]\n+                                  : x_row[dst_row[col]] <\n+                                        x_row[dst_row[ixj]]))) {\n+                            ggml_sycl_swap(dst_row[col], dst_row[ixj]);\n+                        }\n+                    } else {\n+                        if (dst_row[ixj] >= ncols ||\n+                            (dst_row[col] < ncols &&\n+                             (order == GGML_SORT_ORDER_ASC\n+                                  ? x_row[dst_row[col]] < x_row[dst_row[ixj]]\n+                                  : x_row[dst_row[col]] >\n+                                        x_row[dst_row[ixj]]))) {\n+                            ggml_sycl_swap(dst_row[col], dst_row[ixj]);\n+                        }\n                     }\n                 }\n+                item_ct1.barrier(sycl::access::fence_space::local_space);\n             }\n-            /*\n-            DPCT1118:1: SYCL group functions and algorithms must be encountered\n-            in converged control flow. You may need to adjust the code.\n-            */\n-            item_ct1.barrier(sycl::access::fence_space::local_space);\n         }\n     }\n \n     // copy the result to dst without the padding\n-    if (col < ncols) {\n-        dst[row * ncols + col] = dst_row[col];\n+    for (int i = 0; i < tasks_per_thread; i++) {\n+        int col = col_index * tasks_per_thread + i;\n+        if (col < ncols) {\n+            dst[row * ncols + col] = dst_row[col];\n+        }\n     }\n }\n \n-\n static void diag_mask_inf_f32(const float * x, float * dst, const int ncols, const int rows_per_channel, const int n_past,\n                               const sycl::nd_item<3> &item_ct1) {\n     const int col = item_ct1.get_local_range(1) * item_ct1.get_group(1) +\n@@ -1738,11 +1750,20 @@ static int next_power_of_2(int x) {\n \n static void argsort_f32_i32_sycl(const float *x, int *dst, const int ncols,\n                                  const int nrows, ggml_sort_order order,\n-                                 queue_ptr stream) {\n+                                 queue_ptr stream, int device) {\n     // bitonic sort requires ncols to be power of 2\n     const int ncols_pad = next_power_of_2(ncols);\n \n-    const sycl::range<3> block_dims(1, 1, ncols_pad);\n+    int nth = 1;\n+    int max_block_size = ggml_sycl_info().max_work_group_sizes[device];\n+    while (nth < ncols_pad && nth < max_block_size)\n+        nth *= 2;\n+    if (nth > max_block_size)\n+        nth = max_block_size;\n+\n+    const int tasks_per_thread = ncols_pad / nth;\n+\n+    const sycl::range<3> block_dims(1, 1, nth);\n     const sycl::range<3> block_nums(1, nrows, 1);\n     const size_t shared_mem = ncols_pad * sizeof(int);\n \n@@ -1755,8 +1776,9 @@ static void argsort_f32_i32_sycl(const float *x, int *dst, const int ncols,\n                 sycl::nd_range<3>(block_nums * block_dims, block_dims),\n                 [=](sycl::nd_item<3> item_ct1) {\n                     k_argsort_f32_i32<GGML_SORT_ORDER_ASC>(\n-                        x, dst, ncols, ncols_pad, item_ct1,\n-                        dpct_local_acc_ct1.get_multi_ptr<sycl::access::decorated::no>()\n+                        x, dst, ncols, ncols_pad, tasks_per_thread, item_ct1,\n+                        dpct_local_acc_ct1\n+                            .get_multi_ptr<sycl::access::decorated::no>()\n                             .get());\n                 });\n         });\n@@ -1769,8 +1791,9 @@ static void argsort_f32_i32_sycl(const float *x, int *dst, const int ncols,\n                 sycl::nd_range<3>(block_nums * block_dims, block_dims),\n                 [=](sycl::nd_item<3> item_ct1) {\n                     k_argsort_f32_i32<GGML_SORT_ORDER_DESC>(\n-                        x, dst, ncols, ncols_pad, item_ct1,\n-                        dpct_local_acc_ct1.get_multi_ptr<sycl::access::decorated::no>()\n+                        x, dst, ncols, ncols_pad, tasks_per_thread, item_ct1,\n+                        dpct_local_acc_ct1\n+                            .get_multi_ptr<sycl::access::decorated::no>()\n                             .get());\n                 });\n         });\n@@ -2142,7 +2165,8 @@ inline void ggml_sycl_op_argsort(ggml_backend_sycl_context & ctx, ggml_tensor *\n \n     enum ggml_sort_order order = (enum ggml_sort_order) dst->op_params[0];\n \n-    argsort_f32_i32_sycl(src0_dd, (int *) dst_dd, ncols, nrows, order, main_stream);\n+    argsort_f32_i32_sycl(src0_dd, (int *)dst_dd, ncols, nrows, order,\n+                         main_stream, ctx.device);\n }\n \n inline void ggml_sycl_op_argmax(ggml_backend_sycl_context & ctx, ggml_tensor * dst) {\n@@ -4413,8 +4437,7 @@ static bool ggml_backend_sycl_device_supports_op(ggml_backend_dev_t dev, const g\n         case GGML_OP_ACC:\n             return true;\n         case GGML_OP_PAD:\n-            return (ggml_get_op_params_i32(op, 0) == 0) && (ggml_get_op_params_i32(op, 2) == 0) &&\n-                   (ggml_get_op_params_i32(op, 4) == 0) && (ggml_get_op_params_i32(op, 6) == 0);\n+            return ggml_is_contiguous(op->src[0]);\n         case GGML_OP_LEAKY_RELU:\n         case GGML_OP_TIMESTEP_EMBEDDING:\n         case GGML_OP_RWKV_WKV6:"
      },
      {
        "filename": "ggml/src/ggml-sycl/pad.cpp",
        "status": "added",
        "additions": 97,
        "deletions": 0,
        "changes": 97,
        "patch": "@@ -0,0 +1,97 @@\n+//\n+// MIT license\n+// Copyright (C) 2025 Intel Corporation\n+// SPDX-License-Identifier: MIT\n+//\n+\n+//\n+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n+// See https://llvm.org/LICENSE.txt for license information.\n+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n+//\n+\n+//#include \"common.hpp\"\n+#include \"pad.hpp\"\n+\n+static void pad_f32(const float * src, float * dst,\n+                               const int lp0, const int rp0, const int lp1, const int rp1,\n+                               const int lp2, const int rp2, const int lp3, const int rp3,\n+                               const int ne0, const int ne1, const int ne2, const int ne3) {\n+    auto item_ct1 = sycl::ext::oneapi::this_work_item::get_nd_item<3>();\n+    int i0 = item_ct1.get_local_id(2) +\n+             item_ct1.get_group(2) * item_ct1.get_local_range(2);\n+    int i1 = item_ct1.get_group(1);\n+    int i2 = item_ct1.get_group(0) % ne2;\n+    int i3 = item_ct1.get_group(0) / ne2;\n+    if (i0 >= ne0 || i1 >= ne1 || i2 >= ne2 || i3 >= ne3) {\n+        return;\n+    }\n+\n+    // operation\n+    const int64_t dst_idx = i3*(ne0*ne1*ne2) + i2*(ne0*ne1) + i1*ne0 + i0;\n+    if ((i0 >= lp0 && i0 < ne0 - rp0) &&\n+        (i1 >= lp1 && i1 < ne1 - rp1) &&\n+        (i2 >= lp2 && i2 < ne2 - rp2) &&\n+        (i3 >= lp3 && i3 < ne3 - rp3)) {\n+        const int64_t i00 = i0 - lp0;\n+        const int64_t i01 = i1 - lp1;\n+        const int64_t i02 = i2 - lp2;\n+        const int64_t i03 = i3 - lp3;\n+        const int64_t ne02 = ne2 - lp2 - rp2;\n+        const int64_t ne01 = ne1 - lp1 - rp1;\n+        const int64_t ne00 = ne0 - lp0 - rp0;\n+\n+        const int64_t src_idx = i03 * (ne00 * ne01 * ne02) +\n+                                i02 * (ne00 * ne01) + i01 * ne00 + i00;\n+\n+        dst[dst_idx] = src[src_idx];\n+    } else {\n+        dst[dst_idx] = 0.0f;\n+    }\n+}\n+\n+static void pad_f32_sycl(const float *src, float *dst, const int lp0,\n+                         const int rp0, const int lp1, const int rp1,\n+                         const int lp2, const int rp2, const int lp3,\n+                         const int rp3, const int ne0, const int ne1,\n+                         const int ne2, const int ne3,\n+                         dpct::queue_ptr stream) {\n+    int num_blocks = (ne0 + SYCL_PAD_BLOCK_SIZE - 1) / SYCL_PAD_BLOCK_SIZE;\n+    dpct::dim3 gridDim(num_blocks, ne1, ne2 * ne3);\n+    stream->parallel_for(\n+        sycl::nd_range<3>(gridDim * sycl::range<3>(1, 1, SYCL_PAD_BLOCK_SIZE),\n+                          sycl::range<3>(1, 1, SYCL_PAD_BLOCK_SIZE)),\n+        [=](sycl::nd_item<3> item_ct1) {\n+            pad_f32(src, dst, lp0, rp0, lp1, rp1, lp2, rp2, lp3, rp3, ne0, ne1,\n+                    ne2, ne3);\n+        });\n+}\n+\n+void ggml_sycl_op_pad(ggml_backend_sycl_context & ctx, ggml_tensor * dst) {\n+    const ggml_tensor * src0 = dst->src[0];\n+    const float * src0_d = (const float *)src0->data;\n+    float * dst_d = (float *)dst->data;\n+    dpct::queue_ptr     stream = ctx.stream();\n+\n+    GGML_ASSERT(src0->type == GGML_TYPE_F32);\n+    GGML_ASSERT(dst->type == GGML_TYPE_F32);\n+    GGML_ASSERT(ggml_is_contiguous(src0));\n+\n+    const int32_t lp0 = ((const int32_t*)(dst->op_params))[0];\n+    const int32_t rp0 = ((const int32_t*)(dst->op_params))[1];\n+    const int32_t lp1 = ((const int32_t*)(dst->op_params))[2];\n+    const int32_t rp1 = ((const int32_t*)(dst->op_params))[3];\n+    const int32_t lp2 = ((const int32_t*)(dst->op_params))[4];\n+    const int32_t rp2 = ((const int32_t*)(dst->op_params))[5];\n+    const int32_t lp3 = ((const int32_t*)(dst->op_params))[6];\n+    const int32_t rp3 = ((const int32_t*)(dst->op_params))[7];\n+\n+    pad_f32_sycl(src0_d, dst_d,\n+                 lp0, rp0, lp1, rp1, lp2, rp2, lp3, rp3,\n+                 dst->ne[0], dst->ne[1], dst->ne[2], dst->ne[3], stream);\n+}\n+\n+void ggml_sycl_pad(ggml_backend_sycl_context & ctx, ggml_tensor * dst) {\n+    scope_op_debug_print scope_dbg_print(__func__, dst, /*num_src=*/1);\n+    ggml_sycl_op_pad(ctx, dst);\n+}"
      },
      {
        "filename": "ggml/src/ggml-sycl/pad.hpp",
        "status": "added",
        "additions": 24,
        "deletions": 0,
        "changes": 24,
        "patch": "@@ -0,0 +1,24 @@\n+//\n+// MIT license\n+// Copyright (C) 2025 Intel Corporation\n+// SPDX-License-Identifier: MIT\n+//\n+\n+//\n+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n+// See https://llvm.org/LICENSE.txt for license information.\n+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n+//\n+\n+#ifndef GGML_SYCL_PAD_HPP\n+#define GGML_SYCL_PAD_HPP\n+\n+#include \"common.hpp\"\n+\n+#define SYCL_PAD_BLOCK_SIZE 256\n+\n+void ggml_sycl_pad(ggml_backend_sycl_context & ctx, ggml_tensor * dst);\n+\n+void ggml_sycl_op_pad(ggml_backend_sycl_context & ctx, ggml_tensor * dst);\n+\n+#endif // GGML_SYCL_PAD_HPP"
      }
    ],
    "num_files": 13,
    "scraped_at": "2025-11-16T23:30:08.149059"
  }
]